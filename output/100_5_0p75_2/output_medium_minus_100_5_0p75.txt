nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [22:47<37:36:40, 1367.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [42:19<34:05:29, 1252.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [1:03:53<34:15:52, 1271.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [1:25:15<34:01:02, 1275.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [1:37:13<28:21:27, 1074.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [1:58:17<29:44:12, 1138.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [2:18:05<29:50:07, 1154.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [2:39:28<30:33:38, 1195.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [2:58:17<29:41:45, 1174.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [3:20:11<30:26:37, 1217.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [3:41:44<30:40:39, 1240.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [4:03:51<30:58:13, 1266.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [4:25:35<30:53:26, 1278.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [4:46:53<30:32:08, 1278.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [5:08:31<30:19:14, 1284.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [5:30:06<30:02:12, 1287.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [5:51:27<29:38:15, 1285.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [6:10:42<28:23:16, 1246.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [6:32:20<28:23:23, 1261.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [6:53:52<28:14:24, 1270.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [7:15:27<28:02:44, 1278.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [7:35:33<27:13:27, 1256.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [7:57:01<27:04:34, 1265.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [8:18:35<26:54:14, 1274.40s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [8:40:04<26:38:27, 1278.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [9:01:33<26:21:05, 1281.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [9:23:26<26:10:45, 1291.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [9:44:02<25:29:42, 1274.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [10:05:59<25:23:28, 1287.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|███       | 30/100 [10:27:27<25:01:57, 1287.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 31%|███       | 31/100 [10:49:02<24:43:16, 1289.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 32%|███▏      | 32/100 [11:10:31<24:21:33, 1289.61s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 33%|███▎      | 33/100 [11:31:58<23:59:15, 1288.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 34%|███▍      | 34/100 [11:53:28<23:38:04, 1289.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 35%|███▌      | 35/100 [12:15:04<23:18:38, 1291.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 36%|███▌      | 36/100 [12:36:51<23:02:13, 1295.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 37%|███▋      | 37/100 [12:58:14<22:36:36, 1292.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 38%|███▊      | 38/100 [13:19:40<22:13:11, 1290.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 39%|███▉      | 39/100 [13:41:06<21:50:23, 1288.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -11231.808830206552
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23108.77929943447
Iteration 100: Loss = -11496.243869007509
Iteration 200: Loss = -11495.065996127258
Iteration 300: Loss = -11491.210235611115
Iteration 400: Loss = -11481.429411090803
Iteration 500: Loss = -11390.436028039632
Iteration 600: Loss = -11306.086561721862
Iteration 700: Loss = -11285.440654599917
Iteration 800: Loss = -11284.574410952086
Iteration 900: Loss = -11284.324216933055
Iteration 1000: Loss = -11284.163353423868
Iteration 1100: Loss = -11284.025656900749
Iteration 1200: Loss = -11283.953376131618
Iteration 1300: Loss = -11283.905300721226
Iteration 1400: Loss = -11283.864313148171
Iteration 1500: Loss = -11283.796447498042
Iteration 1600: Loss = -11282.94131391811
Iteration 1700: Loss = -11282.911020239035
Iteration 1800: Loss = -11282.890097043193
Iteration 1900: Loss = -11282.874424170954
Iteration 2000: Loss = -11282.863301313753
Iteration 2100: Loss = -11282.854431381747
Iteration 2200: Loss = -11282.847154332956
Iteration 2300: Loss = -11282.840491297224
Iteration 2400: Loss = -11282.83474522306
Iteration 2500: Loss = -11282.829344791146
Iteration 2600: Loss = -11282.823733922578
Iteration 2700: Loss = -11282.817697324906
Iteration 2800: Loss = -11282.813274666383
Iteration 2900: Loss = -11282.809835297052
Iteration 3000: Loss = -11282.809031095821
Iteration 3100: Loss = -11282.804122978732
Iteration 3200: Loss = -11282.801569126621
Iteration 3300: Loss = -11282.804764600834
1
Iteration 3400: Loss = -11282.797071975609
Iteration 3500: Loss = -11282.795386711765
Iteration 3600: Loss = -11282.794019391682
Iteration 3700: Loss = -11282.792503781051
Iteration 3800: Loss = -11282.79125852909
Iteration 3900: Loss = -11282.790684058933
Iteration 4000: Loss = -11282.789136463292
Iteration 4100: Loss = -11282.791204676065
1
Iteration 4200: Loss = -11282.787595086142
Iteration 4300: Loss = -11282.785559527956
Iteration 4400: Loss = -11282.609256582187
Iteration 4500: Loss = -11282.607196302306
Iteration 4600: Loss = -11282.603652105048
Iteration 4700: Loss = -11282.60315394252
Iteration 4800: Loss = -11282.60246326113
Iteration 4900: Loss = -11282.609302184715
1
Iteration 5000: Loss = -11282.601428832788
Iteration 5100: Loss = -11282.601325972357
Iteration 5200: Loss = -11282.600458536994
Iteration 5300: Loss = -11282.599994540258
Iteration 5400: Loss = -11282.598620856066
Iteration 5500: Loss = -11282.59490918386
Iteration 5600: Loss = -11282.58851900013
Iteration 5700: Loss = -11282.588042574174
Iteration 5800: Loss = -11282.587575036041
Iteration 5900: Loss = -11282.58709062475
Iteration 6000: Loss = -11282.588820038987
1
Iteration 6100: Loss = -11282.58596779298
Iteration 6200: Loss = -11282.585284033763
Iteration 6300: Loss = -11282.577765253955
Iteration 6400: Loss = -11282.470210089654
Iteration 6500: Loss = -11282.469505204528
Iteration 6600: Loss = -11282.46828290875
Iteration 6700: Loss = -11282.466143626589
Iteration 6800: Loss = -11282.460299886234
Iteration 6900: Loss = -11282.328562378947
Iteration 7000: Loss = -11280.20537109749
Iteration 7100: Loss = -11280.15925024561
Iteration 7200: Loss = -11280.156952581518
Iteration 7300: Loss = -11280.15603018904
Iteration 7400: Loss = -11280.155443925647
Iteration 7500: Loss = -11280.15500982325
Iteration 7600: Loss = -11280.154597706505
Iteration 7700: Loss = -11280.154229666587
Iteration 7800: Loss = -11280.15381784929
Iteration 7900: Loss = -11280.153289918133
Iteration 8000: Loss = -11280.15297437857
Iteration 8100: Loss = -11280.1527945242
Iteration 8200: Loss = -11280.152859556643
Iteration 8300: Loss = -11280.158780153792
1
Iteration 8400: Loss = -11280.158086096835
2
Iteration 8500: Loss = -11280.152333996706
Iteration 8600: Loss = -11280.152376969983
Iteration 8700: Loss = -11280.158995423757
1
Iteration 8800: Loss = -11280.151471295432
Iteration 8900: Loss = -11280.1510508038
Iteration 9000: Loss = -11280.150634523015
Iteration 9100: Loss = -11280.160296101794
1
Iteration 9200: Loss = -11280.150565485397
Iteration 9300: Loss = -11280.150490929733
Iteration 9400: Loss = -11280.204202864345
1
Iteration 9500: Loss = -11280.150386797688
Iteration 9600: Loss = -11280.151852175777
1
Iteration 9700: Loss = -11280.15153209642
2
Iteration 9800: Loss = -11280.15523371204
3
Iteration 9900: Loss = -11280.152628835815
4
Iteration 10000: Loss = -11280.156000298932
5
Iteration 10100: Loss = -11280.1674198733
6
Iteration 10200: Loss = -11280.155936782512
7
Iteration 10300: Loss = -11280.152569357626
8
Iteration 10400: Loss = -11280.150057719495
Iteration 10500: Loss = -11280.150733549806
1
Iteration 10600: Loss = -11280.153949435828
2
Iteration 10700: Loss = -11280.15005273001
Iteration 10800: Loss = -11280.163007396717
1
Iteration 10900: Loss = -11280.15145240571
2
Iteration 11000: Loss = -11280.149958713397
Iteration 11100: Loss = -11280.181808623125
1
Iteration 11200: Loss = -11280.157587276952
2
Iteration 11300: Loss = -11280.196557765905
3
Iteration 11400: Loss = -11280.150809726707
4
Iteration 11500: Loss = -11280.548149602651
5
Iteration 11600: Loss = -11280.149897201693
Iteration 11700: Loss = -11280.150829530301
1
Iteration 11800: Loss = -11280.14989761458
Iteration 11900: Loss = -11280.162405381318
1
Iteration 12000: Loss = -11280.149844198686
Iteration 12100: Loss = -11280.149888752283
Iteration 12200: Loss = -11280.33760760585
1
Iteration 12300: Loss = -11280.149419034653
Iteration 12400: Loss = -11280.19825830914
1
Iteration 12500: Loss = -11280.149431153724
Iteration 12600: Loss = -11280.151303757493
1
Iteration 12700: Loss = -11280.149428140245
Iteration 12800: Loss = -11280.149870750178
1
Iteration 12900: Loss = -11280.149740368952
2
Iteration 13000: Loss = -11280.149727711452
3
Iteration 13100: Loss = -11280.149501681395
Iteration 13200: Loss = -11280.149927415308
1
Iteration 13300: Loss = -11280.154331138798
2
Iteration 13400: Loss = -11280.14935760201
Iteration 13500: Loss = -11280.47308514181
1
Iteration 13600: Loss = -11280.149338739557
Iteration 13700: Loss = -11280.271045158255
1
Iteration 13800: Loss = -11280.14936854119
Iteration 13900: Loss = -11280.181233921065
1
Iteration 14000: Loss = -11280.151032935486
2
Iteration 14100: Loss = -11280.15905063474
3
Iteration 14200: Loss = -11280.151251194988
4
Iteration 14300: Loss = -11280.153589964533
5
Iteration 14400: Loss = -11280.138581630932
Iteration 14500: Loss = -11280.136384099478
Iteration 14600: Loss = -11280.135962466878
Iteration 14700: Loss = -11280.136878620317
1
Iteration 14800: Loss = -11280.140161455964
2
Iteration 14900: Loss = -11280.136029831841
Iteration 15000: Loss = -11280.136596205655
1
Iteration 15100: Loss = -11280.136002368057
Iteration 15200: Loss = -11280.435224114646
1
Iteration 15300: Loss = -11280.135747615845
Iteration 15400: Loss = -11280.169657718534
1
Iteration 15500: Loss = -11280.135873456067
2
Iteration 15600: Loss = -11280.137117961516
3
Iteration 15700: Loss = -11280.18371137566
4
Iteration 15800: Loss = -11280.135749889676
Iteration 15900: Loss = -11280.138211737352
1
Iteration 16000: Loss = -11280.156497601878
2
Iteration 16100: Loss = -11280.136088579617
3
Iteration 16200: Loss = -11280.13555347852
Iteration 16300: Loss = -11280.178381087775
1
Iteration 16400: Loss = -11280.135135699438
Iteration 16500: Loss = -11280.135849296805
1
Iteration 16600: Loss = -11280.135156385742
Iteration 16700: Loss = -11280.1369925911
1
Iteration 16800: Loss = -11280.135152066276
Iteration 16900: Loss = -11280.13588704931
1
Iteration 17000: Loss = -11280.135858009184
2
Iteration 17100: Loss = -11280.135510238253
3
Iteration 17200: Loss = -11280.25022383829
4
Iteration 17300: Loss = -11280.1354065652
5
Iteration 17400: Loss = -11280.135240478143
Iteration 17500: Loss = -11280.139049837397
1
Iteration 17600: Loss = -11280.13512891807
Iteration 17700: Loss = -11280.270165925846
1
Iteration 17800: Loss = -11280.135224000425
Iteration 17900: Loss = -11280.140294405537
1
Iteration 18000: Loss = -11280.193835370193
2
Iteration 18100: Loss = -11280.13534665118
3
Iteration 18200: Loss = -11280.133830627674
Iteration 18300: Loss = -11280.13394117727
1
Iteration 18400: Loss = -11280.144958591442
2
Iteration 18500: Loss = -11280.133772053734
Iteration 18600: Loss = -11280.148204477291
1
Iteration 18700: Loss = -11280.133807340686
Iteration 18800: Loss = -11280.134118473761
1
Iteration 18900: Loss = -11280.145862362688
2
Iteration 19000: Loss = -11280.133794862026
Iteration 19100: Loss = -11280.183055562688
1
Iteration 19200: Loss = -11280.136987961843
2
Iteration 19300: Loss = -11280.134006499662
3
Iteration 19400: Loss = -11280.144662702092
4
Iteration 19500: Loss = -11280.133910279068
5
Iteration 19600: Loss = -11280.134014192141
6
Iteration 19700: Loss = -11280.13388598478
Iteration 19800: Loss = -11280.133873673994
Iteration 19900: Loss = -11280.135832414508
1
pi: tensor([[0.5722, 0.4278],
        [0.3779, 0.6221]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9654, 0.0346], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1868, 0.0865],
         [0.6764, 0.3053]],

        [[0.7282, 0.1078],
         [0.6836, 0.7261]],

        [[0.5215, 0.0964],
         [0.5115, 0.6518]],

        [[0.6945, 0.1030],
         [0.6046, 0.5453]],

        [[0.5543, 0.1042],
         [0.5588, 0.5117]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5467081977706606
Average Adjusted Rand Index: 0.7311658474984629
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20594.020169553693
Iteration 100: Loss = -11495.245971320706
Iteration 200: Loss = -11493.682796908883
Iteration 300: Loss = -11489.936038093523
Iteration 400: Loss = -11489.085938276068
Iteration 500: Loss = -11488.888224557171
Iteration 600: Loss = -11488.642482588168
Iteration 700: Loss = -11488.068481028791
Iteration 800: Loss = -11487.850484439308
Iteration 900: Loss = -11487.70487856911
Iteration 1000: Loss = -11487.633006586786
Iteration 1100: Loss = -11487.601351243919
Iteration 1200: Loss = -11487.584493000206
Iteration 1300: Loss = -11487.571838473434
Iteration 1400: Loss = -11487.478663069454
Iteration 1500: Loss = -11432.445713872852
Iteration 1600: Loss = -11399.23892884698
Iteration 1700: Loss = -11282.473280781362
Iteration 1800: Loss = -11262.254076789386
Iteration 1900: Loss = -11259.26192392939
Iteration 2000: Loss = -11259.048584427948
Iteration 2100: Loss = -11258.858981780597
Iteration 2200: Loss = -11258.841577154979
Iteration 2300: Loss = -11258.829819255527
Iteration 2400: Loss = -11258.820768622574
Iteration 2500: Loss = -11258.784457029366
Iteration 2600: Loss = -11258.568401279488
Iteration 2700: Loss = -11258.568775084157
1
Iteration 2800: Loss = -11258.550841479251
Iteration 2900: Loss = -11258.546951262286
Iteration 3000: Loss = -11258.542750729508
Iteration 3100: Loss = -11258.538318996312
Iteration 3200: Loss = -11258.534322308484
Iteration 3300: Loss = -11258.525158057291
Iteration 3400: Loss = -11258.490378438837
Iteration 3500: Loss = -11258.478055712441
Iteration 3600: Loss = -11258.471817100108
Iteration 3700: Loss = -11258.470653571108
Iteration 3800: Loss = -11258.469529446662
Iteration 3900: Loss = -11258.46857488741
Iteration 4000: Loss = -11258.468191320166
Iteration 4100: Loss = -11258.467243629084
Iteration 4200: Loss = -11258.468377046875
1
Iteration 4300: Loss = -11258.479935459114
2
Iteration 4400: Loss = -11258.413293191523
Iteration 4500: Loss = -11258.410955387115
Iteration 4600: Loss = -11258.41427984153
1
Iteration 4700: Loss = -11258.409429467087
Iteration 4800: Loss = -11258.41102897586
1
Iteration 4900: Loss = -11258.407831815983
Iteration 5000: Loss = -11258.40783437808
Iteration 5100: Loss = -11258.416149552966
1
Iteration 5200: Loss = -11258.406736084327
Iteration 5300: Loss = -11258.406298717873
Iteration 5400: Loss = -11258.405956790193
Iteration 5500: Loss = -11258.40575044488
Iteration 5600: Loss = -11258.405740932216
Iteration 5700: Loss = -11258.405376718989
Iteration 5800: Loss = -11258.407874364622
1
Iteration 5900: Loss = -11258.420704995155
2
Iteration 6000: Loss = -11258.403729991487
Iteration 6100: Loss = -11258.400874321953
Iteration 6200: Loss = -11258.400287382368
Iteration 6300: Loss = -11258.40012944666
Iteration 6400: Loss = -11258.400600814339
1
Iteration 6500: Loss = -11258.400093235565
Iteration 6600: Loss = -11258.399323257649
Iteration 6700: Loss = -11258.404049212664
1
Iteration 6800: Loss = -11258.39893163908
Iteration 6900: Loss = -11258.398840800764
Iteration 7000: Loss = -11258.398366430116
Iteration 7100: Loss = -11258.398173591848
Iteration 7200: Loss = -11258.398491753273
1
Iteration 7300: Loss = -11258.401874164221
2
Iteration 7400: Loss = -11258.390967110785
Iteration 7500: Loss = -11258.362690201935
Iteration 7600: Loss = -11258.31216027708
Iteration 7700: Loss = -11258.309062172211
Iteration 7800: Loss = -11258.315667754021
1
Iteration 7900: Loss = -11258.30854598379
Iteration 8000: Loss = -11258.285790826381
Iteration 8100: Loss = -11258.283000634923
Iteration 8200: Loss = -11258.269447438193
Iteration 8300: Loss = -11258.268903358316
Iteration 8400: Loss = -11258.266445774512
Iteration 8500: Loss = -11258.266312384645
Iteration 8600: Loss = -11258.266279247477
Iteration 8700: Loss = -11258.266183670195
Iteration 8800: Loss = -11258.26611277155
Iteration 8900: Loss = -11258.443107950558
1
Iteration 9000: Loss = -11258.265395325845
Iteration 9100: Loss = -11258.264594514992
Iteration 9200: Loss = -11257.773719891567
Iteration 9300: Loss = -11257.772841919586
Iteration 9400: Loss = -11257.77269300292
Iteration 9500: Loss = -11257.774952411866
1
Iteration 9600: Loss = -11257.269962629207
Iteration 9700: Loss = -11257.270354154858
1
Iteration 9800: Loss = -11257.267544357572
Iteration 9900: Loss = -11257.23679848666
Iteration 10000: Loss = -11257.236338001543
Iteration 10100: Loss = -11257.238402591793
1
Iteration 10200: Loss = -11257.236334215808
Iteration 10300: Loss = -11257.26198806233
1
Iteration 10400: Loss = -11257.233427071473
Iteration 10500: Loss = -11257.233391125714
Iteration 10600: Loss = -11257.300421187592
1
Iteration 10700: Loss = -11257.232751443627
Iteration 10800: Loss = -11257.232721977452
Iteration 10900: Loss = -11257.283570019572
1
Iteration 11000: Loss = -11257.232729679823
Iteration 11100: Loss = -11257.2701922316
1
Iteration 11200: Loss = -11257.232822249636
Iteration 11300: Loss = -11257.23279277519
Iteration 11400: Loss = -11257.233270688352
1
Iteration 11500: Loss = -11257.237060512703
2
Iteration 11600: Loss = -11257.232612181419
Iteration 11700: Loss = -11257.232797167155
1
Iteration 11800: Loss = -11257.233794427153
2
Iteration 11900: Loss = -11257.233132156785
3
Iteration 12000: Loss = -11257.23072367975
Iteration 12100: Loss = -11257.230203706595
Iteration 12200: Loss = -11257.23276180028
1
Iteration 12300: Loss = -11257.229941810474
Iteration 12400: Loss = -11257.23246760855
1
Iteration 12500: Loss = -11257.229932102236
Iteration 12600: Loss = -11257.229993543704
Iteration 12700: Loss = -11257.230521657111
1
Iteration 12800: Loss = -11257.229889180417
Iteration 12900: Loss = -11257.23811181345
1
Iteration 13000: Loss = -11257.229881313973
Iteration 13100: Loss = -11257.234536260807
1
Iteration 13200: Loss = -11257.22980975991
Iteration 13300: Loss = -11257.230080227842
1
Iteration 13400: Loss = -11257.229669884935
Iteration 13500: Loss = -11257.229632542692
Iteration 13600: Loss = -11257.2348817444
1
Iteration 13700: Loss = -11257.229638790206
Iteration 13800: Loss = -11257.23049899948
1
Iteration 13900: Loss = -11257.229871130741
2
Iteration 14000: Loss = -11257.22989664215
3
Iteration 14100: Loss = -11257.229530300301
Iteration 14200: Loss = -11257.229802318561
1
Iteration 14300: Loss = -11257.237097964045
2
Iteration 14400: Loss = -11257.229418289331
Iteration 14500: Loss = -11257.470375202194
1
Iteration 14600: Loss = -11257.229414776939
Iteration 14700: Loss = -11257.297219263057
1
Iteration 14800: Loss = -11257.22927712224
Iteration 14900: Loss = -11257.240386519368
1
Iteration 15000: Loss = -11257.229284182105
Iteration 15100: Loss = -11257.230875474068
1
Iteration 15200: Loss = -11257.229274393216
Iteration 15300: Loss = -11257.230167489377
1
Iteration 15400: Loss = -11257.229453712067
2
Iteration 15500: Loss = -11257.229370349716
Iteration 15600: Loss = -11257.229286174175
Iteration 15700: Loss = -11257.229492592936
1
Iteration 15800: Loss = -11257.230351529712
2
Iteration 15900: Loss = -11257.22940357293
3
Iteration 16000: Loss = -11257.302177358488
4
Iteration 16100: Loss = -11257.229206141325
Iteration 16200: Loss = -11257.229526485395
1
Iteration 16300: Loss = -11257.229221652175
Iteration 16400: Loss = -11257.22938622449
1
Iteration 16500: Loss = -11257.229011553425
Iteration 16600: Loss = -11257.229290359435
1
Iteration 16700: Loss = -11257.228565567671
Iteration 16800: Loss = -11257.228161989084
Iteration 16900: Loss = -11257.241524441391
1
Iteration 17000: Loss = -11257.228128700252
Iteration 17100: Loss = -11257.229437214915
1
Iteration 17200: Loss = -11257.228716877162
2
Iteration 17300: Loss = -11257.227822742841
Iteration 17400: Loss = -11257.235056199972
1
Iteration 17500: Loss = -11257.22785670526
Iteration 17600: Loss = -11257.227808773687
Iteration 17700: Loss = -11257.22816608354
1
Iteration 17800: Loss = -11257.239560617398
2
Iteration 17900: Loss = -11257.227811415372
Iteration 18000: Loss = -11257.228060871405
1
Iteration 18100: Loss = -11257.228096159735
2
Iteration 18200: Loss = -11257.238846974624
3
Iteration 18300: Loss = -11257.224467076901
Iteration 18400: Loss = -11257.253137484862
1
Iteration 18500: Loss = -11257.22448635101
Iteration 18600: Loss = -11257.224489811299
Iteration 18700: Loss = -11257.375725705668
1
Iteration 18800: Loss = -11257.224560971174
Iteration 18900: Loss = -11257.224479964294
Iteration 19000: Loss = -11257.224747283162
1
Iteration 19100: Loss = -11257.231272312567
2
Iteration 19200: Loss = -11257.223902779615
Iteration 19300: Loss = -11257.255319934748
1
Iteration 19400: Loss = -11257.223864156578
Iteration 19500: Loss = -11257.22507937674
1
Iteration 19600: Loss = -11257.222667180582
Iteration 19700: Loss = -11257.222737150676
Iteration 19800: Loss = -11257.22723127666
1
Iteration 19900: Loss = -11257.222487077128
pi: tensor([[0.6310, 0.3690],
        [0.4183, 0.5817]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4628, 0.5372], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2200, 0.0950],
         [0.6410, 0.2841]],

        [[0.5610, 0.1025],
         [0.6255, 0.5352]],

        [[0.5579, 0.0957],
         [0.5205, 0.5510]],

        [[0.5390, 0.0999],
         [0.6459, 0.7189]],

        [[0.6640, 0.0983],
         [0.6674, 0.6578]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448509923071951
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 11
Adjusted Rand Index: 0.6046396195964233
Global Adjusted Rand Index: 0.35871633893324123
Average Adjusted Rand Index: 0.8348662492673122
11231.808830206552
[0.5467081977706606, 0.35871633893324123] [0.7311658474984629, 0.8348662492673122] [11280.137858279846, 11257.230929933103]
-------------------------------------
This iteration is 1
True Objective function: Loss = -11290.2353503145
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22725.410248655404
Iteration 100: Loss = -11589.572462436781
Iteration 200: Loss = -11588.571455998685
Iteration 300: Loss = -11584.010604039728
Iteration 400: Loss = -11580.817668602158
Iteration 500: Loss = -11522.066099652802
Iteration 600: Loss = -11335.236859443927
Iteration 700: Loss = -11318.551146392614
Iteration 800: Loss = -11301.392844399195
Iteration 900: Loss = -11300.735251409189
Iteration 1000: Loss = -11299.121331600038
Iteration 1100: Loss = -11298.863671161884
Iteration 1200: Loss = -11298.761064169677
Iteration 1300: Loss = -11298.690383266194
Iteration 1400: Loss = -11298.637898927942
Iteration 1500: Loss = -11298.596711107251
Iteration 1600: Loss = -11298.562417706442
Iteration 1700: Loss = -11298.529010266364
Iteration 1800: Loss = -11298.473784589878
Iteration 1900: Loss = -11298.421981373016
Iteration 2000: Loss = -11298.385242372708
Iteration 2100: Loss = -11298.307853743447
Iteration 2200: Loss = -11298.283782829829
Iteration 2300: Loss = -11298.258799967467
Iteration 2400: Loss = -11298.009778649159
Iteration 2500: Loss = -11297.947169057163
Iteration 2600: Loss = -11297.85566433527
Iteration 2700: Loss = -11297.724313472992
Iteration 2800: Loss = -11297.634391383403
Iteration 2900: Loss = -11297.526103905764
Iteration 3000: Loss = -11297.300090958654
Iteration 3100: Loss = -11297.159383356515
Iteration 3200: Loss = -11297.09154941812
Iteration 3300: Loss = -11297.043980218898
Iteration 3400: Loss = -11297.013540938633
Iteration 3500: Loss = -11296.73792730514
Iteration 3600: Loss = -11296.713544562357
Iteration 3700: Loss = -11296.668050890257
Iteration 3800: Loss = -11296.58171827443
Iteration 3900: Loss = -11296.574568761223
Iteration 4000: Loss = -11296.568515303134
Iteration 4100: Loss = -11296.569302724294
1
Iteration 4200: Loss = -11296.560779018713
Iteration 4300: Loss = -11296.55831368108
Iteration 4400: Loss = -11296.556443190588
Iteration 4500: Loss = -11296.555366716466
Iteration 4600: Loss = -11296.55759635939
1
Iteration 4700: Loss = -11296.552916992272
Iteration 4800: Loss = -11296.55087813719
Iteration 4900: Loss = -11296.550424185903
Iteration 5000: Loss = -11296.548343274
Iteration 5100: Loss = -11296.546340462943
Iteration 5200: Loss = -11296.544322225353
Iteration 5300: Loss = -11296.540929273013
Iteration 5400: Loss = -11296.54160205268
1
Iteration 5500: Loss = -11296.547932944068
2
Iteration 5600: Loss = -11296.538795903536
Iteration 5700: Loss = -11296.538926584313
1
Iteration 5800: Loss = -11296.537372497842
Iteration 5900: Loss = -11296.54614050731
1
Iteration 6000: Loss = -11296.536443839826
Iteration 6100: Loss = -11296.536168115572
Iteration 6200: Loss = -11296.540315052167
1
Iteration 6300: Loss = -11296.536677781496
2
Iteration 6400: Loss = -11296.535329484761
Iteration 6500: Loss = -11296.535068731397
Iteration 6600: Loss = -11296.534855371621
Iteration 6700: Loss = -11296.5347203585
Iteration 6800: Loss = -11296.534493865012
Iteration 6900: Loss = -11296.53439134467
Iteration 7000: Loss = -11296.534202754216
Iteration 7100: Loss = -11296.53481922773
1
Iteration 7200: Loss = -11296.533899547121
Iteration 7300: Loss = -11296.533815697074
Iteration 7400: Loss = -11296.533620684284
Iteration 7500: Loss = -11296.533531770783
Iteration 7600: Loss = -11296.53409149466
1
Iteration 7700: Loss = -11296.533299134033
Iteration 7800: Loss = -11296.533235362745
Iteration 7900: Loss = -11296.533128509393
Iteration 8000: Loss = -11296.533055534534
Iteration 8100: Loss = -11296.533317839674
1
Iteration 8200: Loss = -11296.533344428955
2
Iteration 8300: Loss = -11296.533221394775
3
Iteration 8400: Loss = -11296.532776781809
Iteration 8500: Loss = -11296.532831674162
Iteration 8600: Loss = -11296.533400258642
1
Iteration 8700: Loss = -11296.535616724337
2
Iteration 8800: Loss = -11296.533501872393
3
Iteration 8900: Loss = -11296.538183072757
4
Iteration 9000: Loss = -11296.53284961168
Iteration 9100: Loss = -11296.562402477346
1
Iteration 9200: Loss = -11296.532909026317
Iteration 9300: Loss = -11296.532329642228
Iteration 9400: Loss = -11296.535501959843
1
Iteration 9500: Loss = -11296.532200246013
Iteration 9600: Loss = -11296.532674732794
1
Iteration 9700: Loss = -11296.5318495049
Iteration 9800: Loss = -11296.53222637488
1
Iteration 9900: Loss = -11296.533614744509
2
Iteration 10000: Loss = -11296.531920236028
Iteration 10100: Loss = -11296.532130118607
1
Iteration 10200: Loss = -11296.532907107081
2
Iteration 10300: Loss = -11296.54451681911
3
Iteration 10400: Loss = -11296.54149700693
4
Iteration 10500: Loss = -11296.531763403114
Iteration 10600: Loss = -11296.531791271182
Iteration 10700: Loss = -11296.533156910935
1
Iteration 10800: Loss = -11296.531546552615
Iteration 10900: Loss = -11296.566300469449
1
Iteration 11000: Loss = -11296.53111046822
Iteration 11100: Loss = -11296.917407271772
1
Iteration 11200: Loss = -11296.531081304365
Iteration 11300: Loss = -11296.531095678014
Iteration 11400: Loss = -11296.531151535874
Iteration 11500: Loss = -11296.531048339742
Iteration 11600: Loss = -11296.540130940757
1
Iteration 11700: Loss = -11296.53104039112
Iteration 11800: Loss = -11296.531172603938
1
Iteration 11900: Loss = -11296.578891794636
2
Iteration 12000: Loss = -11296.533515922456
3
Iteration 12100: Loss = -11296.533391573503
4
Iteration 12200: Loss = -11296.532273346427
5
Iteration 12300: Loss = -11296.53093050228
Iteration 12400: Loss = -11296.538878142874
1
Iteration 12500: Loss = -11296.536742580704
2
Iteration 12600: Loss = -11296.530876586628
Iteration 12700: Loss = -11296.671682748798
1
Iteration 12800: Loss = -11296.530967207073
Iteration 12900: Loss = -11296.531242577314
1
Iteration 13000: Loss = -11296.530860090354
Iteration 13100: Loss = -11296.532225696767
1
Iteration 13200: Loss = -11296.556213240956
2
Iteration 13300: Loss = -11296.542527188147
3
Iteration 13400: Loss = -11296.530784366367
Iteration 13500: Loss = -11296.53079100017
Iteration 13600: Loss = -11296.538027494831
1
Iteration 13700: Loss = -11296.530685670577
Iteration 13800: Loss = -11296.531029655118
1
Iteration 13900: Loss = -11296.539505344324
2
Iteration 14000: Loss = -11296.540521096873
3
Iteration 14100: Loss = -11296.53460352158
4
Iteration 14200: Loss = -11296.531243758276
5
Iteration 14300: Loss = -11296.531459265634
6
Iteration 14400: Loss = -11296.531205297119
7
Iteration 14500: Loss = -11296.530943328997
8
Iteration 14600: Loss = -11296.567988425944
9
Iteration 14700: Loss = -11296.530647569078
Iteration 14800: Loss = -11296.530920038394
1
Iteration 14900: Loss = -11296.533073918143
2
Iteration 15000: Loss = -11296.5369515085
3
Iteration 15100: Loss = -11296.530961318073
4
Iteration 15200: Loss = -11296.531073692931
5
Iteration 15300: Loss = -11296.530809228478
6
Iteration 15400: Loss = -11296.531200316795
7
Iteration 15500: Loss = -11296.659760169126
8
Iteration 15600: Loss = -11296.532639931287
9
Iteration 15700: Loss = -11296.534538288135
10
Iteration 15800: Loss = -11296.531961771914
11
Iteration 15900: Loss = -11296.533435152194
12
Iteration 16000: Loss = -11296.537019057434
13
Iteration 16100: Loss = -11296.53251102905
14
Iteration 16200: Loss = -11296.53249585069
15
Stopping early at iteration 16200 due to no improvement.
pi: tensor([[0.6803, 0.3197],
        [0.1870, 0.8130]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9460, 0.0540], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1892, 0.1367],
         [0.6522, 0.3096]],

        [[0.6265, 0.1041],
         [0.6746, 0.6396]],

        [[0.5019, 0.1062],
         [0.5915, 0.7128]],

        [[0.5548, 0.0863],
         [0.7068, 0.6521]],

        [[0.7145, 0.1053],
         [0.6755, 0.6799]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.6328997026233792
Average Adjusted Rand Index: 0.7603215772770979
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22764.70057009259
Iteration 100: Loss = -11585.052706972276
Iteration 200: Loss = -11581.268262866257
Iteration 300: Loss = -11579.897431159738
Iteration 400: Loss = -11579.450018852753
Iteration 500: Loss = -11579.240244287981
Iteration 600: Loss = -11579.12566936152
Iteration 700: Loss = -11579.060896861403
Iteration 800: Loss = -11579.022417918008
Iteration 900: Loss = -11578.998143056442
Iteration 1000: Loss = -11578.981960708814
Iteration 1100: Loss = -11578.970566529173
Iteration 1200: Loss = -11578.962303923507
Iteration 1300: Loss = -11578.95612049411
Iteration 1400: Loss = -11578.951280606998
Iteration 1500: Loss = -11578.947492610085
Iteration 1600: Loss = -11578.944477281271
Iteration 1700: Loss = -11578.941988621698
Iteration 1800: Loss = -11578.939919152099
Iteration 1900: Loss = -11578.938186007417
Iteration 2000: Loss = -11578.936737259315
Iteration 2100: Loss = -11578.9355039463
Iteration 2200: Loss = -11578.934475532042
Iteration 2300: Loss = -11578.933490929076
Iteration 2400: Loss = -11578.932713686394
Iteration 2500: Loss = -11578.932016646997
Iteration 2600: Loss = -11578.93140745968
Iteration 2700: Loss = -11578.930907281596
Iteration 2800: Loss = -11578.93039424309
Iteration 2900: Loss = -11578.929982393105
Iteration 3000: Loss = -11578.92958961636
Iteration 3100: Loss = -11578.92921617153
Iteration 3200: Loss = -11578.928924327549
Iteration 3300: Loss = -11578.928632566081
Iteration 3400: Loss = -11578.928395616344
Iteration 3500: Loss = -11578.92812913129
Iteration 3600: Loss = -11578.92793894582
Iteration 3700: Loss = -11578.92773526558
Iteration 3800: Loss = -11578.92757184328
Iteration 3900: Loss = -11578.927433270408
Iteration 4000: Loss = -11578.927266289395
Iteration 4100: Loss = -11578.927104820914
Iteration 4200: Loss = -11578.927007398817
Iteration 4300: Loss = -11578.926852195942
Iteration 4400: Loss = -11578.926744109962
Iteration 4500: Loss = -11578.926686857285
Iteration 4600: Loss = -11578.926622962748
Iteration 4700: Loss = -11578.926494845447
Iteration 4800: Loss = -11578.926417366965
Iteration 4900: Loss = -11578.926347651346
Iteration 5000: Loss = -11578.926272968
Iteration 5100: Loss = -11578.926223956674
Iteration 5200: Loss = -11578.926165971297
Iteration 5300: Loss = -11578.926087789992
Iteration 5400: Loss = -11578.92606659471
Iteration 5500: Loss = -11578.927039495007
1
Iteration 5600: Loss = -11578.925969372276
Iteration 5700: Loss = -11578.92595352531
Iteration 5800: Loss = -11578.92592209139
Iteration 5900: Loss = -11578.925856055206
Iteration 6000: Loss = -11578.927917774801
1
Iteration 6100: Loss = -11578.92579477795
Iteration 6200: Loss = -11578.926635331522
1
Iteration 6300: Loss = -11578.925748148444
Iteration 6400: Loss = -11578.92572614824
Iteration 6500: Loss = -11578.925695646858
Iteration 6600: Loss = -11578.93171551324
1
Iteration 6700: Loss = -11578.92564704305
Iteration 6800: Loss = -11578.925668302514
Iteration 6900: Loss = -11578.925600928258
Iteration 7000: Loss = -11578.925602076666
Iteration 7100: Loss = -11578.930511212699
1
Iteration 7200: Loss = -11578.925566604796
Iteration 7300: Loss = -11578.925900057928
1
Iteration 7400: Loss = -11578.92549345606
Iteration 7500: Loss = -11578.925668591508
1
Iteration 7600: Loss = -11578.925512675522
Iteration 7700: Loss = -11578.929158930754
1
Iteration 7800: Loss = -11578.925485402071
Iteration 7900: Loss = -11578.925488844461
Iteration 8000: Loss = -11579.092175051983
1
Iteration 8100: Loss = -11578.925442775193
Iteration 8200: Loss = -11578.925425819712
Iteration 8300: Loss = -11578.925545157863
1
Iteration 8400: Loss = -11578.925412788187
Iteration 8500: Loss = -11579.226645279188
1
Iteration 8600: Loss = -11578.925422467857
Iteration 8700: Loss = -11578.925437106296
Iteration 8800: Loss = -11578.925549623837
1
Iteration 8900: Loss = -11578.925424054476
Iteration 9000: Loss = -11578.925425620442
Iteration 9100: Loss = -11578.931213296166
1
Iteration 9200: Loss = -11578.925384713046
Iteration 9300: Loss = -11578.925420296278
Iteration 9400: Loss = -11578.94567382174
1
Iteration 9500: Loss = -11578.942858740484
2
Iteration 9600: Loss = -11578.93299547582
3
Iteration 9700: Loss = -11578.926739908004
4
Iteration 9800: Loss = -11578.926993712072
5
Iteration 9900: Loss = -11578.926983243602
6
Iteration 10000: Loss = -11578.927185386014
7
Iteration 10100: Loss = -11578.94551333109
8
Iteration 10200: Loss = -11578.926097816713
9
Iteration 10300: Loss = -11578.925574936262
10
Iteration 10400: Loss = -11578.929791288441
11
Iteration 10500: Loss = -11578.988715704287
12
Iteration 10600: Loss = -11579.106086054746
13
Iteration 10700: Loss = -11578.925350581505
Iteration 10800: Loss = -11578.925669166347
1
Iteration 10900: Loss = -11578.92536952958
Iteration 11000: Loss = -11578.92589096185
1
Iteration 11100: Loss = -11578.925369347358
Iteration 11200: Loss = -11578.925961636245
1
Iteration 11300: Loss = -11579.073872897101
2
Iteration 11400: Loss = -11578.928335021039
3
Iteration 11500: Loss = -11578.945967975013
4
Iteration 11600: Loss = -11578.933988436262
5
Iteration 11700: Loss = -11578.942329172756
6
Iteration 11800: Loss = -11578.935315387042
7
Iteration 11900: Loss = -11578.92759126684
8
Iteration 12000: Loss = -11578.92543046554
Iteration 12100: Loss = -11578.927830637127
1
Iteration 12200: Loss = -11578.92539843659
Iteration 12300: Loss = -11578.9254133748
Iteration 12400: Loss = -11578.925361493468
Iteration 12500: Loss = -11578.925920923426
1
Iteration 12600: Loss = -11578.9253324567
Iteration 12700: Loss = -11578.994281087267
1
Iteration 12800: Loss = -11578.925339671141
Iteration 12900: Loss = -11578.925358290251
Iteration 13000: Loss = -11578.930524254554
1
Iteration 13100: Loss = -11578.925325665972
Iteration 13200: Loss = -11578.925346161624
Iteration 13300: Loss = -11578.925970533875
1
Iteration 13400: Loss = -11578.925345051115
Iteration 13500: Loss = -11578.940644352653
1
Iteration 13600: Loss = -11578.925335448876
Iteration 13700: Loss = -11578.925354510991
Iteration 13800: Loss = -11578.94915247349
1
Iteration 13900: Loss = -11578.925302432024
Iteration 14000: Loss = -11578.925328227471
Iteration 14100: Loss = -11578.930458982328
1
Iteration 14200: Loss = -11578.92532393777
Iteration 14300: Loss = -11578.927148869805
1
Iteration 14400: Loss = -11578.930027946371
2
Iteration 14500: Loss = -11578.93486978628
3
Iteration 14600: Loss = -11578.928297355298
4
Iteration 14700: Loss = -11578.925575365736
5
Iteration 14800: Loss = -11578.92597596101
6
Iteration 14900: Loss = -11578.925359996385
Iteration 15000: Loss = -11578.925495771475
1
Iteration 15100: Loss = -11578.925418549621
Iteration 15200: Loss = -11578.938964909798
1
Iteration 15300: Loss = -11578.927055143047
2
Iteration 15400: Loss = -11578.929253803235
3
Iteration 15500: Loss = -11578.925321876099
Iteration 15600: Loss = -11578.926466399651
1
Iteration 15700: Loss = -11578.95296643951
2
Iteration 15800: Loss = -11578.926186137867
3
Iteration 15900: Loss = -11578.925712549746
4
Iteration 16000: Loss = -11578.994180025646
5
Iteration 16100: Loss = -11578.928649026262
6
Iteration 16200: Loss = -11578.925580411547
7
Iteration 16300: Loss = -11578.934805647314
8
Iteration 16400: Loss = -11578.929075101167
9
Iteration 16500: Loss = -11578.926175918534
10
Iteration 16600: Loss = -11578.925363407809
Iteration 16700: Loss = -11578.946631840448
1
Iteration 16800: Loss = -11579.095778825287
2
Iteration 16900: Loss = -11578.943405420154
3
Iteration 17000: Loss = -11578.933225604127
4
Iteration 17100: Loss = -11578.925482467504
5
Iteration 17200: Loss = -11578.92546287575
Iteration 17300: Loss = -11578.988284125826
1
Iteration 17400: Loss = -11578.967690950398
2
Iteration 17500: Loss = -11578.925316712272
Iteration 17600: Loss = -11578.926397059426
1
Iteration 17700: Loss = -11578.928488691563
2
Iteration 17800: Loss = -11578.925410825554
Iteration 17900: Loss = -11578.925627224455
1
Iteration 18000: Loss = -11578.925550242255
2
Iteration 18100: Loss = -11578.929778240848
3
Iteration 18200: Loss = -11578.929226220258
4
Iteration 18300: Loss = -11578.926725875204
5
Iteration 18400: Loss = -11578.930566015384
6
Iteration 18500: Loss = -11579.021604246807
7
Iteration 18600: Loss = -11578.958572295724
8
Iteration 18700: Loss = -11578.92676796004
9
Iteration 18800: Loss = -11578.92544483107
Iteration 18900: Loss = -11578.925520184495
Iteration 19000: Loss = -11579.161109717
1
Iteration 19100: Loss = -11578.925319339956
Iteration 19200: Loss = -11579.215216267685
1
Iteration 19300: Loss = -11578.925338770157
Iteration 19400: Loss = -11578.931198715236
1
Iteration 19500: Loss = -11578.9254052589
Iteration 19600: Loss = -11578.960933452918
1
Iteration 19700: Loss = -11578.925334503692
Iteration 19800: Loss = -11578.928179353283
1
Iteration 19900: Loss = -11578.925347872571
pi: tensor([[0.9809, 0.0191],
        [0.9566, 0.0434]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0122, 0.9878], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1773, 0.3274],
         [0.5068, 0.1693]],

        [[0.7208, 0.1084],
         [0.6558, 0.7116]],

        [[0.5200, 0.2763],
         [0.6692, 0.5687]],

        [[0.7152, 0.3415],
         [0.5947, 0.6668]],

        [[0.6855, 0.2805],
         [0.5503, 0.7045]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
Global Adjusted Rand Index: -0.0003545984953076914
Average Adjusted Rand Index: -0.0019076636919135314
11290.2353503145
[0.6328997026233792, -0.0003545984953076914] [0.7603215772770979, -0.0019076636919135314] [11296.53249585069, 11578.928464001041]
-------------------------------------
This iteration is 2
True Objective function: Loss = -11506.838496510527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21048.811153940744
Iteration 100: Loss = -11861.366527739003
Iteration 200: Loss = -11858.535668742235
Iteration 300: Loss = -11848.157597373849
Iteration 400: Loss = -11794.322083505134
Iteration 500: Loss = -11519.24496834411
Iteration 600: Loss = -11486.986431652227
Iteration 700: Loss = -11485.653693356622
Iteration 800: Loss = -11484.709820851373
Iteration 900: Loss = -11484.265142387725
Iteration 1000: Loss = -11484.176651048778
Iteration 1100: Loss = -11484.116914774562
Iteration 1200: Loss = -11484.072070123251
Iteration 1300: Loss = -11484.032898064032
Iteration 1400: Loss = -11484.006861806558
Iteration 1500: Loss = -11483.986255838989
Iteration 1600: Loss = -11483.9675591902
Iteration 1700: Loss = -11483.944525504498
Iteration 1800: Loss = -11483.820698057365
Iteration 1900: Loss = -11483.753032397726
Iteration 2000: Loss = -11483.73307064862
Iteration 2100: Loss = -11483.720454029302
Iteration 2200: Loss = -11483.64851899318
Iteration 2300: Loss = -11483.307753502688
Iteration 2400: Loss = -11483.280541188013
Iteration 2500: Loss = -11483.275236665033
Iteration 2600: Loss = -11483.272219232827
Iteration 2700: Loss = -11483.267913242427
Iteration 2800: Loss = -11483.26504274888
Iteration 2900: Loss = -11483.263370216757
Iteration 3000: Loss = -11483.260168657194
Iteration 3100: Loss = -11483.25812791543
Iteration 3200: Loss = -11483.26163508585
1
Iteration 3300: Loss = -11483.254526174118
Iteration 3400: Loss = -11483.252983054159
Iteration 3500: Loss = -11483.252333724882
Iteration 3600: Loss = -11483.250035511304
Iteration 3700: Loss = -11483.248656239626
Iteration 3800: Loss = -11483.247071943266
Iteration 3900: Loss = -11483.2450621782
Iteration 4000: Loss = -11483.243811633916
Iteration 4100: Loss = -11483.241391414296
Iteration 4200: Loss = -11483.240210417152
Iteration 4300: Loss = -11483.231669559655
Iteration 4400: Loss = -11483.18159808997
Iteration 4500: Loss = -11483.177765967182
Iteration 4600: Loss = -11483.178028849592
1
Iteration 4700: Loss = -11483.181972403407
2
Iteration 4800: Loss = -11483.172806343346
Iteration 4900: Loss = -11483.064530985135
Iteration 5000: Loss = -11483.057045818237
Iteration 5100: Loss = -11483.043995845053
Iteration 5200: Loss = -11483.043732901495
Iteration 5300: Loss = -11483.043606880914
Iteration 5400: Loss = -11483.042808044187
Iteration 5500: Loss = -11483.060010563364
1
Iteration 5600: Loss = -11483.042209009436
Iteration 5700: Loss = -11483.053152342756
1
Iteration 5800: Loss = -11483.04168557428
Iteration 5900: Loss = -11483.044774248572
1
Iteration 6000: Loss = -11483.04129255167
Iteration 6100: Loss = -11483.043993682011
1
Iteration 6200: Loss = -11483.04090509191
Iteration 6300: Loss = -11483.043111085532
1
Iteration 6400: Loss = -11483.040574720884
Iteration 6500: Loss = -11483.041026474752
1
Iteration 6600: Loss = -11483.040263889554
Iteration 6700: Loss = -11483.071026407877
1
Iteration 6800: Loss = -11483.041053686728
2
Iteration 6900: Loss = -11483.039901139618
Iteration 7000: Loss = -11483.039885058204
Iteration 7100: Loss = -11483.041119483849
1
Iteration 7200: Loss = -11483.042361977896
2
Iteration 7300: Loss = -11483.040773985116
3
Iteration 7400: Loss = -11483.059213832868
4
Iteration 7500: Loss = -11483.039319441692
Iteration 7600: Loss = -11483.03993586908
1
Iteration 7700: Loss = -11483.03918629941
Iteration 7800: Loss = -11483.03923552966
Iteration 7900: Loss = -11483.039655839435
1
Iteration 8000: Loss = -11483.0389630436
Iteration 8100: Loss = -11483.0390161437
Iteration 8200: Loss = -11483.041429310018
1
Iteration 8300: Loss = -11483.03874456916
Iteration 8400: Loss = -11483.18444982588
1
Iteration 8500: Loss = -11483.044721616763
2
Iteration 8600: Loss = -11483.06567400648
3
Iteration 8700: Loss = -11483.027648620218
Iteration 8800: Loss = -11483.030065275123
1
Iteration 8900: Loss = -11483.024176699793
Iteration 9000: Loss = -11483.024325516906
1
Iteration 9100: Loss = -11483.024078977136
Iteration 9200: Loss = -11483.024163270064
Iteration 9300: Loss = -11483.030235536267
1
Iteration 9400: Loss = -11483.024004783523
Iteration 9500: Loss = -11483.027625107265
1
Iteration 9600: Loss = -11483.0239114369
Iteration 9700: Loss = -11483.02636807641
1
Iteration 9800: Loss = -11483.023874251567
Iteration 9900: Loss = -11483.025092959488
1
Iteration 10000: Loss = -11483.066100562388
2
Iteration 10100: Loss = -11483.023774643592
Iteration 10200: Loss = -11483.0237401051
Iteration 10300: Loss = -11483.024126688926
1
Iteration 10400: Loss = -11483.027005282001
2
Iteration 10500: Loss = -11483.02369087418
Iteration 10600: Loss = -11483.026033554203
1
Iteration 10700: Loss = -11483.023661564697
Iteration 10800: Loss = -11483.043916860788
1
Iteration 10900: Loss = -11483.023563143628
Iteration 11000: Loss = -11483.023542492021
Iteration 11100: Loss = -11483.023515199407
Iteration 11200: Loss = -11483.023181836961
Iteration 11300: Loss = -11483.04252314608
1
Iteration 11400: Loss = -11483.023127213857
Iteration 11500: Loss = -11483.023068314533
Iteration 11600: Loss = -11483.041772294824
1
Iteration 11700: Loss = -11483.03683394886
2
Iteration 11800: Loss = -11483.022978193196
Iteration 11900: Loss = -11483.023202291351
1
Iteration 12000: Loss = -11483.245798011476
2
Iteration 12100: Loss = -11483.022908716273
Iteration 12200: Loss = -11483.071356771701
1
Iteration 12300: Loss = -11483.022520097888
Iteration 12400: Loss = -11483.022398066176
Iteration 12500: Loss = -11483.022874323262
1
Iteration 12600: Loss = -11483.022054016146
Iteration 12700: Loss = -11483.060233096692
1
Iteration 12800: Loss = -11483.021986162204
Iteration 12900: Loss = -11483.022242042367
1
Iteration 13000: Loss = -11483.022088262145
2
Iteration 13100: Loss = -11483.023495073356
3
Iteration 13200: Loss = -11483.15979376019
4
Iteration 13300: Loss = -11483.021981083257
Iteration 13400: Loss = -11483.023144430714
1
Iteration 13500: Loss = -11483.021976451919
Iteration 13600: Loss = -11483.022003381388
Iteration 13700: Loss = -11483.022008030597
Iteration 13800: Loss = -11483.021961468236
Iteration 13900: Loss = -11483.021914343435
Iteration 14000: Loss = -11483.022139822433
1
Iteration 14100: Loss = -11483.021949935539
Iteration 14200: Loss = -11483.026207181889
1
Iteration 14300: Loss = -11483.021942906262
Iteration 14400: Loss = -11483.023942873107
1
Iteration 14500: Loss = -11483.0219608029
Iteration 14600: Loss = -11483.021937024443
Iteration 14700: Loss = -11483.02214659304
1
Iteration 14800: Loss = -11483.021906401269
Iteration 14900: Loss = -11483.022390458618
1
Iteration 15000: Loss = -11483.021883067548
Iteration 15100: Loss = -11483.022324795305
1
Iteration 15200: Loss = -11483.02188362018
Iteration 15300: Loss = -11483.022332956494
1
Iteration 15400: Loss = -11483.022579119148
2
Iteration 15500: Loss = -11483.021847776245
Iteration 15600: Loss = -11483.02322746739
1
Iteration 15700: Loss = -11483.02180816909
Iteration 15800: Loss = -11483.021801709447
Iteration 15900: Loss = -11483.021828957766
Iteration 16000: Loss = -11483.021784622482
Iteration 16100: Loss = -11483.022340145293
1
Iteration 16200: Loss = -11483.021765231328
Iteration 16300: Loss = -11483.038076515486
1
Iteration 16400: Loss = -11483.021781454874
Iteration 16500: Loss = -11483.021783750257
Iteration 16600: Loss = -11483.022049988971
1
Iteration 16700: Loss = -11483.021774338044
Iteration 16800: Loss = -11483.201158573238
1
Iteration 16900: Loss = -11483.021776881624
Iteration 17000: Loss = -11483.068555330365
1
Iteration 17100: Loss = -11483.02178115133
Iteration 17200: Loss = -11483.14206140517
1
Iteration 17300: Loss = -11483.021748878646
Iteration 17400: Loss = -11483.021724074846
Iteration 17500: Loss = -11483.067734704244
1
Iteration 17600: Loss = -11483.021741018574
Iteration 17700: Loss = -11483.045842074001
1
Iteration 17800: Loss = -11483.021733284919
Iteration 17900: Loss = -11483.045361288245
1
Iteration 18000: Loss = -11483.021732309331
Iteration 18100: Loss = -11483.054614216915
1
Iteration 18200: Loss = -11483.02173009226
Iteration 18300: Loss = -11483.0756558365
1
Iteration 18400: Loss = -11483.021707124635
Iteration 18500: Loss = -11483.163755802389
1
Iteration 18600: Loss = -11483.021705465404
Iteration 18700: Loss = -11483.037226377353
1
Iteration 18800: Loss = -11483.021662949039
Iteration 18900: Loss = -11483.107717232542
1
Iteration 19000: Loss = -11483.021654160479
Iteration 19100: Loss = -11483.021694246807
Iteration 19200: Loss = -11483.021799734215
1
Iteration 19300: Loss = -11483.022504901888
2
Iteration 19400: Loss = -11483.30331331135
3
Iteration 19500: Loss = -11483.021659196822
Iteration 19600: Loss = -11483.021732060593
Iteration 19700: Loss = -11483.021639806419
Iteration 19800: Loss = -11483.021641413496
Iteration 19900: Loss = -11483.024309430431
1
pi: tensor([[0.7473, 0.2527],
        [0.3158, 0.6842]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5670, 0.4330], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3092, 0.1038],
         [0.5364, 0.1967]],

        [[0.7063, 0.1038],
         [0.6372, 0.7289]],

        [[0.6327, 0.1027],
         [0.5376, 0.5816]],

        [[0.5382, 0.0991],
         [0.6712, 0.5505]],

        [[0.6420, 0.1043],
         [0.6774, 0.7279]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.9291442028568664
Average Adjusted Rand Index: 0.9296075652143092
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23006.068692295183
Iteration 100: Loss = -11861.90016720421
Iteration 200: Loss = -11860.847759677466
Iteration 300: Loss = -11855.81848438695
Iteration 400: Loss = -11825.613452643229
Iteration 500: Loss = -11686.46676638932
Iteration 600: Loss = -11540.326075096824
Iteration 700: Loss = -11511.383992938834
Iteration 800: Loss = -11486.50510515776
Iteration 900: Loss = -11485.239500485095
Iteration 1000: Loss = -11484.865925074997
Iteration 1100: Loss = -11484.596174684079
Iteration 1200: Loss = -11484.392762111636
Iteration 1300: Loss = -11484.301780615779
Iteration 1400: Loss = -11484.215817205824
Iteration 1500: Loss = -11484.14913586362
Iteration 1600: Loss = -11483.52907083845
Iteration 1700: Loss = -11483.473911606305
Iteration 1800: Loss = -11483.442569192637
Iteration 1900: Loss = -11483.405444812752
Iteration 2000: Loss = -11483.374500181048
Iteration 2100: Loss = -11483.246007981408
Iteration 2200: Loss = -11483.202635421301
Iteration 2300: Loss = -11483.168145442229
Iteration 2400: Loss = -11483.128616316635
Iteration 2500: Loss = -11483.118491463674
Iteration 2600: Loss = -11483.110196804126
Iteration 2700: Loss = -11483.103023536012
Iteration 2800: Loss = -11483.097176319958
Iteration 2900: Loss = -11483.09230229821
Iteration 3000: Loss = -11483.087876503472
Iteration 3100: Loss = -11483.084086599942
Iteration 3200: Loss = -11483.081225441474
Iteration 3300: Loss = -11483.077521120736
Iteration 3400: Loss = -11483.07473369777
Iteration 3500: Loss = -11483.08216445169
1
Iteration 3600: Loss = -11483.069663313574
Iteration 3700: Loss = -11483.071515120932
1
Iteration 3800: Loss = -11483.06570708867
Iteration 3900: Loss = -11483.063640457895
Iteration 4000: Loss = -11483.06742853907
1
Iteration 4100: Loss = -11483.060212990637
Iteration 4200: Loss = -11483.06228925916
1
Iteration 4300: Loss = -11483.057029787851
Iteration 4400: Loss = -11483.055372916151
Iteration 4500: Loss = -11483.05459842719
Iteration 4600: Loss = -11483.052280292452
Iteration 4700: Loss = -11483.050923912018
Iteration 4800: Loss = -11483.054142067655
1
Iteration 4900: Loss = -11483.04780790466
Iteration 5000: Loss = -11483.046950902095
Iteration 5100: Loss = -11483.046500898954
Iteration 5200: Loss = -11483.045538336592
Iteration 5300: Loss = -11483.04488866669
Iteration 5400: Loss = -11483.044227085522
Iteration 5500: Loss = -11483.043829918879
Iteration 5600: Loss = -11483.044818304219
1
Iteration 5700: Loss = -11483.043921552971
Iteration 5800: Loss = -11483.041914551468
Iteration 5900: Loss = -11483.042329142721
1
Iteration 6000: Loss = -11483.040596305284
Iteration 6100: Loss = -11483.039599102782
Iteration 6200: Loss = -11483.041268137276
1
Iteration 6300: Loss = -11483.037414382366
Iteration 6400: Loss = -11483.036754778257
Iteration 6500: Loss = -11483.036779195292
Iteration 6600: Loss = -11483.035862336921
Iteration 6700: Loss = -11483.03541536486
Iteration 6800: Loss = -11483.036354636848
1
Iteration 6900: Loss = -11483.032662059262
Iteration 7000: Loss = -11483.031976989747
Iteration 7100: Loss = -11483.060519094703
1
Iteration 7200: Loss = -11483.031174914378
Iteration 7300: Loss = -11483.031171452454
Iteration 7400: Loss = -11483.032984419791
1
Iteration 7500: Loss = -11483.031049465815
Iteration 7600: Loss = -11483.031689180423
1
Iteration 7700: Loss = -11483.03036841597
Iteration 7800: Loss = -11483.030094100897
Iteration 7900: Loss = -11483.030754530104
1
Iteration 8000: Loss = -11483.067850515039
2
Iteration 8100: Loss = -11483.029692526063
Iteration 8200: Loss = -11483.029843329312
1
Iteration 8300: Loss = -11483.029493787233
Iteration 8400: Loss = -11483.031431373232
1
Iteration 8500: Loss = -11483.02932323491
Iteration 8600: Loss = -11483.156388404966
1
Iteration 8700: Loss = -11483.02916572505
Iteration 8800: Loss = -11483.029134997241
Iteration 8900: Loss = -11483.03403817594
1
Iteration 9000: Loss = -11483.028912655484
Iteration 9100: Loss = -11483.028855653301
Iteration 9200: Loss = -11483.030494674098
1
Iteration 9300: Loss = -11483.028774446964
Iteration 9400: Loss = -11483.02866253201
Iteration 9500: Loss = -11483.074917408754
1
Iteration 9600: Loss = -11483.0285252717
Iteration 9700: Loss = -11483.028442905132
Iteration 9800: Loss = -11483.028477184638
Iteration 9900: Loss = -11483.028183886803
Iteration 10000: Loss = -11483.036464954565
1
Iteration 10100: Loss = -11483.028010151775
Iteration 10200: Loss = -11483.17943540981
1
Iteration 10300: Loss = -11483.027022897624
Iteration 10400: Loss = -11483.359151239536
1
Iteration 10500: Loss = -11483.025918783103
Iteration 10600: Loss = -11483.03232157942
1
Iteration 10700: Loss = -11483.025796286462
Iteration 10800: Loss = -11483.025714753538
Iteration 10900: Loss = -11483.030387284212
1
Iteration 11000: Loss = -11483.025121199495
Iteration 11100: Loss = -11483.025017094864
Iteration 11200: Loss = -11483.025154766852
1
Iteration 11300: Loss = -11483.024931224903
Iteration 11400: Loss = -11483.133009349716
1
Iteration 11500: Loss = -11483.024897823725
Iteration 11600: Loss = -11483.024864846444
Iteration 11700: Loss = -11483.033707550965
1
Iteration 11800: Loss = -11483.024844041749
Iteration 11900: Loss = -11483.100326340491
1
Iteration 12000: Loss = -11483.070598919383
2
Iteration 12100: Loss = -11483.030790755838
3
Iteration 12200: Loss = -11483.024764931466
Iteration 12300: Loss = -11483.0253639613
1
Iteration 12400: Loss = -11483.024706368102
Iteration 12500: Loss = -11483.024832632782
1
Iteration 12600: Loss = -11483.024605779467
Iteration 12700: Loss = -11483.038253230661
1
Iteration 12800: Loss = -11483.024552602097
Iteration 12900: Loss = -11483.024563280034
Iteration 13000: Loss = -11483.02494894719
1
Iteration 13100: Loss = -11483.024513128854
Iteration 13200: Loss = -11483.030672065292
1
Iteration 13300: Loss = -11483.024509592648
Iteration 13400: Loss = -11483.323979777158
1
Iteration 13500: Loss = -11483.024447612606
Iteration 13600: Loss = -11483.024428216298
Iteration 13700: Loss = -11483.044488983527
1
Iteration 13800: Loss = -11483.024379737844
Iteration 13900: Loss = -11483.02437180535
Iteration 14000: Loss = -11483.02480896979
1
Iteration 14100: Loss = -11483.024340426175
Iteration 14200: Loss = -11483.027016223752
1
Iteration 14300: Loss = -11483.024358418203
Iteration 14400: Loss = -11483.024591181233
1
Iteration 14500: Loss = -11483.024352818069
Iteration 14600: Loss = -11483.024393548312
Iteration 14700: Loss = -11483.023787199574
Iteration 14800: Loss = -11483.023925691361
1
Iteration 14900: Loss = -11483.023929105097
2
Iteration 15000: Loss = -11483.023747166351
Iteration 15100: Loss = -11483.081335305536
1
Iteration 15200: Loss = -11483.02371493077
Iteration 15300: Loss = -11483.026077515255
1
Iteration 15400: Loss = -11483.023706015443
Iteration 15500: Loss = -11483.023880826004
1
Iteration 15600: Loss = -11483.023696038355
Iteration 15700: Loss = -11483.023992078399
1
Iteration 15800: Loss = -11483.0237038653
Iteration 15900: Loss = -11483.033481119022
1
Iteration 16000: Loss = -11483.023704264706
Iteration 16100: Loss = -11483.050947788211
1
Iteration 16200: Loss = -11483.02374254682
Iteration 16300: Loss = -11483.023725708816
Iteration 16400: Loss = -11483.02501360498
1
Iteration 16500: Loss = -11483.023727369413
Iteration 16600: Loss = -11483.086567277103
1
Iteration 16700: Loss = -11483.02371169863
Iteration 16800: Loss = -11483.023710154761
Iteration 16900: Loss = -11483.025857121584
1
Iteration 17000: Loss = -11483.023728029519
Iteration 17100: Loss = -11483.251893452323
1
Iteration 17200: Loss = -11483.023665286388
Iteration 17300: Loss = -11483.061290400181
1
Iteration 17400: Loss = -11483.02405732258
2
Iteration 17500: Loss = -11483.023702487579
Iteration 17600: Loss = -11483.024590390032
1
Iteration 17700: Loss = -11483.023649797338
Iteration 17800: Loss = -11483.023846318405
1
Iteration 17900: Loss = -11483.02633350853
2
Iteration 18000: Loss = -11483.023634830995
Iteration 18100: Loss = -11483.052300669598
1
Iteration 18200: Loss = -11483.023612935162
Iteration 18300: Loss = -11483.023605908602
Iteration 18400: Loss = -11483.023872581403
1
Iteration 18500: Loss = -11483.023587912783
Iteration 18600: Loss = -11483.042713903067
1
Iteration 18700: Loss = -11483.023474999496
Iteration 18800: Loss = -11483.023491130414
Iteration 18900: Loss = -11483.024110296366
1
Iteration 19000: Loss = -11483.023469062953
Iteration 19100: Loss = -11483.068828131993
1
Iteration 19200: Loss = -11483.023505519748
Iteration 19300: Loss = -11483.023463606558
Iteration 19400: Loss = -11483.028353441643
1
Iteration 19500: Loss = -11483.023469821665
Iteration 19600: Loss = -11483.074218013984
1
Iteration 19700: Loss = -11483.030915052044
2
Iteration 19800: Loss = -11483.02680799169
3
Iteration 19900: Loss = -11483.02230194052
pi: tensor([[0.6842, 0.3158],
        [0.2527, 0.7473]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4331, 0.5669], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1967, 0.1038],
         [0.6115, 0.3092]],

        [[0.5136, 0.1038],
         [0.5067, 0.7295]],

        [[0.5934, 0.1028],
         [0.7196, 0.5048]],

        [[0.6037, 0.0991],
         [0.6924, 0.7156]],

        [[0.6102, 0.1043],
         [0.5785, 0.6268]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.9291442028568664
Average Adjusted Rand Index: 0.9296075652143092
11506.838496510527
[0.9291442028568664, 0.9291442028568664] [0.9296075652143092, 0.9296075652143092] [11483.021644938639, 11483.021706839714]
-------------------------------------
This iteration is 3
True Objective function: Loss = -11345.616796299195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25097.05499562974
Iteration 100: Loss = -11609.148233580529
Iteration 200: Loss = -11606.042544454544
Iteration 300: Loss = -11605.617557262067
Iteration 400: Loss = -11605.388819220614
Iteration 500: Loss = -11605.253383711937
Iteration 600: Loss = -11605.145210920171
Iteration 700: Loss = -11605.021495150844
Iteration 800: Loss = -11604.871718911925
Iteration 900: Loss = -11604.693760270318
Iteration 1000: Loss = -11604.48969872942
Iteration 1100: Loss = -11557.957645563172
Iteration 1200: Loss = -11550.383517714581
Iteration 1300: Loss = -11550.327375529747
Iteration 1400: Loss = -11550.233877753491
Iteration 1500: Loss = -11550.228154049122
Iteration 1600: Loss = -11550.224253554206
Iteration 1700: Loss = -11550.220984957283
Iteration 1800: Loss = -11550.218640102587
Iteration 1900: Loss = -11550.217364298418
Iteration 2000: Loss = -11550.21634633164
Iteration 2100: Loss = -11550.215569679942
Iteration 2200: Loss = -11550.214898060221
Iteration 2300: Loss = -11550.214385191533
Iteration 2400: Loss = -11550.213916668656
Iteration 2500: Loss = -11550.21351877084
Iteration 2600: Loss = -11550.213242123824
Iteration 2700: Loss = -11550.212961863039
Iteration 2800: Loss = -11550.212719879371
Iteration 2900: Loss = -11550.21252285483
Iteration 3000: Loss = -11550.212332940991
Iteration 3100: Loss = -11550.212162708003
Iteration 3200: Loss = -11550.212009099778
Iteration 3300: Loss = -11550.21192065966
Iteration 3400: Loss = -11550.21181864581
Iteration 3500: Loss = -11550.21171361616
Iteration 3600: Loss = -11550.211635504238
Iteration 3700: Loss = -11550.2115616691
Iteration 3800: Loss = -11550.211450596487
Iteration 3900: Loss = -11550.211380484761
Iteration 4000: Loss = -11550.211353922397
Iteration 4100: Loss = -11550.211319565638
Iteration 4200: Loss = -11550.211269690957
Iteration 4300: Loss = -11550.211228616608
Iteration 4400: Loss = -11550.21118437831
Iteration 4500: Loss = -11550.211122756216
Iteration 4600: Loss = -11550.21109725202
Iteration 4700: Loss = -11550.211052650002
Iteration 4800: Loss = -11550.21105737406
Iteration 4900: Loss = -11550.211822810192
1
Iteration 5000: Loss = -11550.210999455812
Iteration 5100: Loss = -11550.210978744544
Iteration 5200: Loss = -11550.214958498373
1
Iteration 5300: Loss = -11550.210970320464
Iteration 5400: Loss = -11550.212609641818
1
Iteration 5500: Loss = -11550.21510564822
2
Iteration 5600: Loss = -11550.21091709442
Iteration 5700: Loss = -11550.211595435661
1
Iteration 5800: Loss = -11550.210903378907
Iteration 5900: Loss = -11550.21089642255
Iteration 6000: Loss = -11550.21088681431
Iteration 6100: Loss = -11550.210855511628
Iteration 6200: Loss = -11550.210875034694
Iteration 6300: Loss = -11550.211133452465
1
Iteration 6400: Loss = -11550.210857343354
Iteration 6500: Loss = -11550.212017726395
1
Iteration 6600: Loss = -11550.210971242046
2
Iteration 6700: Loss = -11550.212757421636
3
Iteration 6800: Loss = -11550.210891736648
Iteration 6900: Loss = -11550.210893004945
Iteration 7000: Loss = -11550.210826575412
Iteration 7100: Loss = -11550.211001592428
1
Iteration 7200: Loss = -11550.210798819882
Iteration 7300: Loss = -11550.210817660936
Iteration 7400: Loss = -11550.211717332615
1
Iteration 7500: Loss = -11550.210819106818
Iteration 7600: Loss = -11550.212503597822
1
Iteration 7700: Loss = -11550.211041040475
2
Iteration 7800: Loss = -11550.21130362883
3
Iteration 7900: Loss = -11550.210808536216
Iteration 8000: Loss = -11550.210817517314
Iteration 8100: Loss = -11550.211036310153
1
Iteration 8200: Loss = -11550.211629432384
2
Iteration 8300: Loss = -11550.210751626082
Iteration 8400: Loss = -11550.212624992917
1
Iteration 8500: Loss = -11550.21077152455
Iteration 8600: Loss = -11550.211687043435
1
Iteration 8700: Loss = -11550.2108058618
Iteration 8800: Loss = -11550.21081488431
Iteration 8900: Loss = -11550.210898547788
Iteration 9000: Loss = -11550.210806059107
Iteration 9100: Loss = -11550.2108177669
Iteration 9200: Loss = -11550.211033463778
1
Iteration 9300: Loss = -11550.210780731593
Iteration 9400: Loss = -11550.210801345684
Iteration 9500: Loss = -11550.211857975932
1
Iteration 9600: Loss = -11550.210785724154
Iteration 9700: Loss = -11550.210744213819
Iteration 9800: Loss = -11550.229695203885
1
Iteration 9900: Loss = -11550.210772023856
Iteration 10000: Loss = -11550.210780741345
Iteration 10100: Loss = -11550.325944751079
1
Iteration 10200: Loss = -11550.210776843645
Iteration 10300: Loss = -11550.210755994145
Iteration 10400: Loss = -11550.23505726376
1
Iteration 10500: Loss = -11550.210798375065
Iteration 10600: Loss = -11550.210775672627
Iteration 10700: Loss = -11550.346970745893
1
Iteration 10800: Loss = -11550.210774171663
Iteration 10900: Loss = -11550.210773147506
Iteration 11000: Loss = -11550.214813736566
1
Iteration 11100: Loss = -11550.210782842167
Iteration 11200: Loss = -11550.2107746905
Iteration 11300: Loss = -11550.265821014751
1
Iteration 11400: Loss = -11550.210783203822
Iteration 11500: Loss = -11550.210764850153
Iteration 11600: Loss = -11550.245269407269
1
Iteration 11700: Loss = -11550.210790103843
Iteration 11800: Loss = -11550.210763116025
Iteration 11900: Loss = -11550.23005473091
1
Iteration 12000: Loss = -11550.210720473087
Iteration 12100: Loss = -11550.258690751261
1
Iteration 12200: Loss = -11550.210755043652
Iteration 12300: Loss = -11550.210770171469
Iteration 12400: Loss = -11550.211171605975
1
Iteration 12500: Loss = -11550.210796828676
Iteration 12600: Loss = -11550.233489244229
1
Iteration 12700: Loss = -11550.21074965743
Iteration 12800: Loss = -11550.21257895289
1
Iteration 12900: Loss = -11550.210821438379
Iteration 13000: Loss = -11550.210787385162
Iteration 13100: Loss = -11550.212572705726
1
Iteration 13200: Loss = -11550.210774681242
Iteration 13300: Loss = -11550.23010729706
1
Iteration 13400: Loss = -11550.21739134368
2
Iteration 13500: Loss = -11550.211079840778
3
Iteration 13600: Loss = -11550.210759646488
Iteration 13700: Loss = -11550.25150995529
1
Iteration 13800: Loss = -11550.21077934105
Iteration 13900: Loss = -11550.332806252207
1
Iteration 14000: Loss = -11550.210764744896
Iteration 14100: Loss = -11550.210735255774
Iteration 14200: Loss = -11550.21076117825
Iteration 14300: Loss = -11550.212499278492
1
Iteration 14400: Loss = -11550.21074331272
Iteration 14500: Loss = -11550.249637668408
1
Iteration 14600: Loss = -11550.210745616798
Iteration 14700: Loss = -11550.232287157334
1
Iteration 14800: Loss = -11550.21076720175
Iteration 14900: Loss = -11550.217983241126
1
Iteration 15000: Loss = -11550.210756728315
Iteration 15100: Loss = -11550.223258732623
1
Iteration 15200: Loss = -11550.210772520897
Iteration 15300: Loss = -11550.210868035827
Iteration 15400: Loss = -11550.210925923093
Iteration 15500: Loss = -11550.210850676087
Iteration 15600: Loss = -11550.5919018583
1
Iteration 15700: Loss = -11550.210758807225
Iteration 15800: Loss = -11550.218563719525
1
Iteration 15900: Loss = -11550.214108126615
2
Iteration 16000: Loss = -11550.21088227534
3
Iteration 16100: Loss = -11550.2114903707
4
Iteration 16200: Loss = -11550.348567075915
5
Iteration 16300: Loss = -11550.21076187988
Iteration 16400: Loss = -11550.213142303028
1
Iteration 16500: Loss = -11550.210759923619
Iteration 16600: Loss = -11550.211066493584
1
Iteration 16700: Loss = -11550.21078461713
Iteration 16800: Loss = -11550.210870240408
Iteration 16900: Loss = -11550.231484338368
1
Iteration 17000: Loss = -11550.210740370107
Iteration 17100: Loss = -11550.440379314774
1
Iteration 17200: Loss = -11550.210746360362
Iteration 17300: Loss = -11550.223202640167
1
Iteration 17400: Loss = -11550.210768613324
Iteration 17500: Loss = -11550.230541426514
1
Iteration 17600: Loss = -11550.21075937888
Iteration 17700: Loss = -11550.210810584655
Iteration 17800: Loss = -11550.210855815929
Iteration 17900: Loss = -11550.218913599641
1
Iteration 18000: Loss = -11550.210741510176
Iteration 18100: Loss = -11550.369488074968
1
Iteration 18200: Loss = -11550.210757147479
Iteration 18300: Loss = -11550.21510268347
1
Iteration 18400: Loss = -11550.210756255357
Iteration 18500: Loss = -11550.210759761567
Iteration 18600: Loss = -11550.21086501915
1
Iteration 18700: Loss = -11550.210752788327
Iteration 18800: Loss = -11550.213275655378
1
Iteration 18900: Loss = -11550.210764065318
Iteration 19000: Loss = -11550.227643315373
1
Iteration 19100: Loss = -11550.21081161787
Iteration 19200: Loss = -11550.210751948305
Iteration 19300: Loss = -11550.211026706718
1
Iteration 19400: Loss = -11550.210765479093
Iteration 19500: Loss = -11550.211651091708
1
Iteration 19600: Loss = -11550.210770174639
Iteration 19700: Loss = -11550.241525430754
1
Iteration 19800: Loss = -11550.260796597731
2
Iteration 19900: Loss = -11550.310587535672
3
pi: tensor([[0.0360, 0.9640],
        [0.0469, 0.9531]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4376, 0.5624], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3321, 0.1017],
         [0.6299, 0.1771]],

        [[0.6205, 0.2523],
         [0.5372, 0.5461]],

        [[0.5781, 0.0980],
         [0.5315, 0.5757]],

        [[0.5956, 0.2333],
         [0.5655, 0.5522]],

        [[0.7149, 0.2878],
         [0.5124, 0.5978]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448543354594036
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
Global Adjusted Rand Index: 0.03764676886844676
Average Adjusted Rand Index: 0.16876268746856676
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22381.578111055493
Iteration 100: Loss = -11610.623456378878
Iteration 200: Loss = -11607.525942979735
Iteration 300: Loss = -11604.537660657103
Iteration 400: Loss = -11602.773450909537
Iteration 500: Loss = -11602.261662674935
Iteration 600: Loss = -11601.954862056313
Iteration 700: Loss = -11601.655864585367
Iteration 800: Loss = -11601.370686826442
Iteration 900: Loss = -11601.120259503388
Iteration 1000: Loss = -11600.879946633126
Iteration 1100: Loss = -11600.63971463409
Iteration 1200: Loss = -11600.412141445326
Iteration 1300: Loss = -11600.234778100208
Iteration 1400: Loss = -11600.133431290438
Iteration 1500: Loss = -11600.089388010096
Iteration 1600: Loss = -11600.07162476929
Iteration 1700: Loss = -11600.06345727713
Iteration 1800: Loss = -11600.058999543668
Iteration 1900: Loss = -11600.056297417197
Iteration 2000: Loss = -11600.054485223945
Iteration 2100: Loss = -11600.053232486676
Iteration 2200: Loss = -11600.052336323099
Iteration 2300: Loss = -11600.051637280092
Iteration 2400: Loss = -11600.05104059133
Iteration 2500: Loss = -11600.050573702949
Iteration 2600: Loss = -11600.05020820076
Iteration 2700: Loss = -11600.049862377411
Iteration 2800: Loss = -11600.049554870986
Iteration 2900: Loss = -11600.049295814135
Iteration 3000: Loss = -11600.049057709313
Iteration 3100: Loss = -11600.048883575451
Iteration 3200: Loss = -11600.048679999776
Iteration 3300: Loss = -11600.04851890457
Iteration 3400: Loss = -11600.0483580214
Iteration 3500: Loss = -11600.048257154882
Iteration 3600: Loss = -11600.048118806772
Iteration 3700: Loss = -11600.048037355316
Iteration 3800: Loss = -11600.047910318564
Iteration 3900: Loss = -11600.047841612793
Iteration 4000: Loss = -11600.047758546581
Iteration 4100: Loss = -11600.047662915125
Iteration 4200: Loss = -11600.047594064092
Iteration 4300: Loss = -11600.047572149791
Iteration 4400: Loss = -11600.047511483786
Iteration 4500: Loss = -11600.047470351417
Iteration 4600: Loss = -11600.047415522613
Iteration 4700: Loss = -11600.04735627468
Iteration 4800: Loss = -11600.047316296927
Iteration 4900: Loss = -11600.047262916043
Iteration 5000: Loss = -11600.04721737779
Iteration 5100: Loss = -11600.047228244921
Iteration 5200: Loss = -11600.047168575089
Iteration 5300: Loss = -11600.047140109866
Iteration 5400: Loss = -11600.047126858524
Iteration 5500: Loss = -11600.047064166003
Iteration 5600: Loss = -11600.047070920515
Iteration 5700: Loss = -11600.050925045483
1
Iteration 5800: Loss = -11600.047049376719
Iteration 5900: Loss = -11600.047028948218
Iteration 6000: Loss = -11600.047101660575
Iteration 6100: Loss = -11600.046963528319
Iteration 6200: Loss = -11600.047001646388
Iteration 6300: Loss = -11600.046950065245
Iteration 6400: Loss = -11600.046956466324
Iteration 6500: Loss = -11600.046971341873
Iteration 6600: Loss = -11600.046930033657
Iteration 6700: Loss = -11600.046924872964
Iteration 6800: Loss = -11600.046933187374
Iteration 6900: Loss = -11600.046980950647
Iteration 7000: Loss = -11600.046919314047
Iteration 7100: Loss = -11600.046959017605
Iteration 7200: Loss = -11600.04690610214
Iteration 7300: Loss = -11600.047098551486
1
Iteration 7400: Loss = -11600.046864278409
Iteration 7500: Loss = -11600.046908304572
Iteration 7600: Loss = -11600.048440034512
1
Iteration 7700: Loss = -11600.046824730875
Iteration 7800: Loss = -11600.090593778676
1
Iteration 7900: Loss = -11600.047243277228
2
Iteration 8000: Loss = -11600.047959942047
3
Iteration 8100: Loss = -11600.140877390035
4
Iteration 8200: Loss = -11600.046844906841
Iteration 8300: Loss = -11600.047314249672
1
Iteration 8400: Loss = -11600.046939081582
Iteration 8500: Loss = -11600.04682571377
Iteration 8600: Loss = -11600.164600477588
1
Iteration 8700: Loss = -11600.046817140454
Iteration 8800: Loss = -11600.04682956034
Iteration 8900: Loss = -11600.57171368109
1
Iteration 9000: Loss = -11600.046791960674
Iteration 9100: Loss = -11600.046792668118
Iteration 9200: Loss = -11600.076637470074
1
Iteration 9300: Loss = -11600.046804611788
Iteration 9400: Loss = -11600.046848639147
Iteration 9500: Loss = -11600.04680173888
Iteration 9600: Loss = -11600.047114116272
1
Iteration 9700: Loss = -11600.048598853533
2
Iteration 9800: Loss = -11600.046830104595
Iteration 9900: Loss = -11600.047720782786
1
Iteration 10000: Loss = -11600.049732020198
2
Iteration 10100: Loss = -11600.046919416747
Iteration 10200: Loss = -11600.15094862636
1
Iteration 10300: Loss = -11600.046829207451
Iteration 10400: Loss = -11600.052507564898
1
Iteration 10500: Loss = -11600.046798129668
Iteration 10600: Loss = -11600.047160920556
1
Iteration 10700: Loss = -11600.207896673763
2
Iteration 10800: Loss = -11600.046780305885
Iteration 10900: Loss = -11600.053123907943
1
Iteration 11000: Loss = -11600.046807238949
Iteration 11100: Loss = -11600.101040330339
1
Iteration 11200: Loss = -11600.04735316898
2
Iteration 11300: Loss = -11600.059955072584
3
Iteration 11400: Loss = -11600.048019228301
4
Iteration 11500: Loss = -11600.052361580081
5
Iteration 11600: Loss = -11600.048037335535
6
Iteration 11700: Loss = -11600.051869447687
7
Iteration 11800: Loss = -11600.050557443732
8
Iteration 11900: Loss = -11600.048409647668
9
Iteration 12000: Loss = -11600.046884879937
Iteration 12100: Loss = -11600.047457003255
1
Iteration 12200: Loss = -11600.060461952115
2
Iteration 12300: Loss = -11600.046812299519
Iteration 12400: Loss = -11600.06814001763
1
Iteration 12500: Loss = -11600.04757457869
2
Iteration 12600: Loss = -11600.050427205748
3
Iteration 12700: Loss = -11600.08140737111
4
Iteration 12800: Loss = -11600.046794547898
Iteration 12900: Loss = -11600.04772772505
1
Iteration 13000: Loss = -11600.062168193306
2
Iteration 13100: Loss = -11600.046849534208
Iteration 13200: Loss = -11600.048358670556
1
Iteration 13300: Loss = -11600.051420566531
2
Iteration 13400: Loss = -11600.047252467257
3
Iteration 13500: Loss = -11600.047293721602
4
Iteration 13600: Loss = -11600.04696649502
5
Iteration 13700: Loss = -11600.066115020656
6
Iteration 13800: Loss = -11600.16286256482
7
Iteration 13900: Loss = -11600.047058179247
8
Iteration 14000: Loss = -11600.048230694127
9
Iteration 14100: Loss = -11600.100772304897
10
Iteration 14200: Loss = -11600.046792429637
Iteration 14300: Loss = -11600.048112663353
1
Iteration 14400: Loss = -11600.046801263385
Iteration 14500: Loss = -11600.046955642194
1
Iteration 14600: Loss = -11600.046769528884
Iteration 14700: Loss = -11600.052396400517
1
Iteration 14800: Loss = -11600.046805036776
Iteration 14900: Loss = -11600.052664708928
1
Iteration 15000: Loss = -11600.046845359548
Iteration 15100: Loss = -11600.047185202282
1
Iteration 15200: Loss = -11600.05041995677
2
Iteration 15300: Loss = -11600.04683802662
Iteration 15400: Loss = -11600.050417110444
1
Iteration 15500: Loss = -11600.046857411126
Iteration 15600: Loss = -11600.048396910426
1
Iteration 15700: Loss = -11600.047515085043
2
Iteration 15800: Loss = -11600.047617409396
3
Iteration 15900: Loss = -11600.0566296955
4
Iteration 16000: Loss = -11600.073480636538
5
Iteration 16100: Loss = -11600.050639808585
6
Iteration 16200: Loss = -11600.048182536268
7
Iteration 16300: Loss = -11600.047217817813
8
Iteration 16400: Loss = -11600.047430764582
9
Iteration 16500: Loss = -11600.04702858954
10
Iteration 16600: Loss = -11600.063300059548
11
Iteration 16700: Loss = -11600.046823315486
Iteration 16800: Loss = -11600.048987666836
1
Iteration 16900: Loss = -11600.047463335446
2
Iteration 17000: Loss = -11600.047080774943
3
Iteration 17100: Loss = -11600.047578563937
4
Iteration 17200: Loss = -11600.048853452117
5
Iteration 17300: Loss = -11600.04844353918
6
Iteration 17400: Loss = -11600.047833462748
7
Iteration 17500: Loss = -11600.053787313596
8
Iteration 17600: Loss = -11600.274033388703
9
Iteration 17700: Loss = -11600.046782550673
Iteration 17800: Loss = -11600.048318018455
1
Iteration 17900: Loss = -11600.046825429614
Iteration 18000: Loss = -11600.048406355994
1
Iteration 18100: Loss = -11600.046806204167
Iteration 18200: Loss = -11600.047258521818
1
Iteration 18300: Loss = -11600.046811468625
Iteration 18400: Loss = -11600.04690113431
Iteration 18500: Loss = -11600.049029500167
1
Iteration 18600: Loss = -11600.046845309686
Iteration 18700: Loss = -11600.050173791313
1
Iteration 18800: Loss = -11600.05267339324
2
Iteration 18900: Loss = -11600.046791475163
Iteration 19000: Loss = -11600.047312408526
1
Iteration 19100: Loss = -11600.060871755979
2
Iteration 19200: Loss = -11600.046855065453
Iteration 19300: Loss = -11600.047000827246
1
Iteration 19400: Loss = -11600.069338194784
2
Iteration 19500: Loss = -11600.047824752479
3
Iteration 19600: Loss = -11600.278444575059
4
Iteration 19700: Loss = -11600.046999693815
5
Iteration 19800: Loss = -11600.047297673398
6
Iteration 19900: Loss = -11600.04745109822
7
pi: tensor([[0.9758, 0.0242],
        [0.7291, 0.2709]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8903, 0.1097], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1819, 0.1376],
         [0.6311, 0.1003]],

        [[0.6498, 0.0899],
         [0.7011, 0.6072]],

        [[0.5037, 0.1095],
         [0.5409, 0.6971]],

        [[0.6988, 0.2672],
         [0.5469, 0.7092]],

        [[0.5771, 0.3111],
         [0.7277, 0.5343]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
Global Adjusted Rand Index: -0.00021500346749357634
Average Adjusted Rand Index: 0.00011276047273844599
11345.616796299195
[0.03764676886844676, -0.00021500346749357634] [0.16876268746856676, 0.00011276047273844599] [11550.21078706038, 11600.090346216948]
-------------------------------------
This iteration is 4
True Objective function: Loss = -11302.18316150153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20930.825756544655
Iteration 100: Loss = -11597.511823526858
Iteration 200: Loss = -11596.807757435663
Iteration 300: Loss = -11592.832032261162
Iteration 400: Loss = -11543.2436487591
Iteration 500: Loss = -11307.938185873923
Iteration 600: Loss = -11275.414892944988
Iteration 700: Loss = -11272.15087858478
Iteration 800: Loss = -11271.773805552733
Iteration 900: Loss = -11271.555580707467
Iteration 1000: Loss = -11271.469095896999
Iteration 1100: Loss = -11271.410329809303
Iteration 1200: Loss = -11271.361789729683
Iteration 1300: Loss = -11271.317839870902
Iteration 1400: Loss = -11270.729214484976
Iteration 1500: Loss = -11270.703414603355
Iteration 1600: Loss = -11270.682767168275
Iteration 1700: Loss = -11270.629134323997
Iteration 1800: Loss = -11270.60671030763
Iteration 1900: Loss = -11270.59554754075
Iteration 2000: Loss = -11270.57614299093
Iteration 2100: Loss = -11270.561638284948
Iteration 2200: Loss = -11270.552688489492
Iteration 2300: Loss = -11270.5558548268
1
Iteration 2400: Loss = -11270.541311782106
Iteration 2500: Loss = -11270.53753576139
Iteration 2600: Loss = -11270.534606391888
Iteration 2700: Loss = -11270.531359818102
Iteration 2800: Loss = -11270.534720853566
1
Iteration 2900: Loss = -11270.525965779263
Iteration 3000: Loss = -11270.53129091888
1
Iteration 3100: Loss = -11270.52000182967
Iteration 3200: Loss = -11270.517298228426
Iteration 3300: Loss = -11270.516173521075
Iteration 3400: Loss = -11270.514825157368
Iteration 3500: Loss = -11270.512567794116
Iteration 3600: Loss = -11270.512901055095
1
Iteration 3700: Loss = -11270.509937697076
Iteration 3800: Loss = -11270.510477704272
1
Iteration 3900: Loss = -11270.507485523047
Iteration 4000: Loss = -11270.509907546822
1
Iteration 4100: Loss = -11270.506648690545
Iteration 4200: Loss = -11270.510291306246
1
Iteration 4300: Loss = -11270.508193834836
2
Iteration 4400: Loss = -11270.50301597423
Iteration 4500: Loss = -11270.501774762804
Iteration 4600: Loss = -11270.51186268088
1
Iteration 4700: Loss = -11270.499398763792
Iteration 4800: Loss = -11270.498648147443
Iteration 4900: Loss = -11270.499183851882
1
Iteration 5000: Loss = -11270.497594693312
Iteration 5100: Loss = -11270.497319253023
Iteration 5200: Loss = -11270.498195385358
1
Iteration 5300: Loss = -11270.49668297728
Iteration 5400: Loss = -11270.496316353327
Iteration 5500: Loss = -11270.505129558085
1
Iteration 5600: Loss = -11270.496475995864
2
Iteration 5700: Loss = -11270.495706605681
Iteration 5800: Loss = -11270.495454313581
Iteration 5900: Loss = -11270.496372996253
1
Iteration 6000: Loss = -11270.495039331847
Iteration 6100: Loss = -11270.498571434946
1
Iteration 6200: Loss = -11270.492315585465
Iteration 6300: Loss = -11270.487966198662
Iteration 6400: Loss = -11270.50220004095
1
Iteration 6500: Loss = -11270.48763926931
Iteration 6600: Loss = -11270.487455218197
Iteration 6700: Loss = -11270.488117595567
1
Iteration 6800: Loss = -11270.490465250276
2
Iteration 6900: Loss = -11270.489049091848
3
Iteration 7000: Loss = -11270.503151206103
4
Iteration 7100: Loss = -11270.490555061902
5
Iteration 7200: Loss = -11270.489603073778
6
Iteration 7300: Loss = -11270.49109912461
7
Iteration 7400: Loss = -11270.489469234748
8
Iteration 7500: Loss = -11270.486768689168
Iteration 7600: Loss = -11270.493794423419
1
Iteration 7700: Loss = -11270.486486481312
Iteration 7800: Loss = -11270.49083727782
1
Iteration 7900: Loss = -11270.487104614735
2
Iteration 8000: Loss = -11270.486607060579
3
Iteration 8100: Loss = -11270.486161914681
Iteration 8200: Loss = -11270.486127933369
Iteration 8300: Loss = -11270.48687752573
1
Iteration 8400: Loss = -11270.4859338593
Iteration 8500: Loss = -11270.586654841805
1
Iteration 8600: Loss = -11270.485470195581
Iteration 8700: Loss = -11270.485282638583
Iteration 8800: Loss = -11270.486110690745
1
Iteration 8900: Loss = -11270.48526522713
Iteration 9000: Loss = -11270.485163224694
Iteration 9100: Loss = -11270.498527478509
1
Iteration 9200: Loss = -11270.484312934796
Iteration 9300: Loss = -11270.48451953762
1
Iteration 9400: Loss = -11270.488270385968
2
Iteration 9500: Loss = -11270.507748431837
3
Iteration 9600: Loss = -11270.50521258898
4
Iteration 9700: Loss = -11270.571680703073
5
Iteration 9800: Loss = -11270.486248639509
6
Iteration 9900: Loss = -11270.557299186625
7
Iteration 10000: Loss = -11270.484507842943
8
Iteration 10100: Loss = -11270.50807276098
9
Iteration 10200: Loss = -11270.484473423461
10
Iteration 10300: Loss = -11270.583669135684
11
Iteration 10400: Loss = -11270.484542797518
12
Iteration 10500: Loss = -11270.50016849988
13
Iteration 10600: Loss = -11270.484477081702
14
Iteration 10700: Loss = -11270.50574585159
15
Stopping early at iteration 10700 due to no improvement.
pi: tensor([[0.7607, 0.2393],
        [0.2899, 0.7101]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5512, 0.4488], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2987, 0.0958],
         [0.6978, 0.1939]],

        [[0.5265, 0.0986],
         [0.6150, 0.7016]],

        [[0.5022, 0.1006],
         [0.5372, 0.5530]],

        [[0.6440, 0.1067],
         [0.6029, 0.6284]],

        [[0.5418, 0.1032],
         [0.6677, 0.6979]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448420005390695
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7368758840005154
Global Adjusted Rand Index: 0.8833641283285453
Average Adjusted Rand Index: 0.8848264772068923
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24295.630541445473
Iteration 100: Loss = -11414.432107895303
Iteration 200: Loss = -11321.53677921847
Iteration 300: Loss = -11320.0744477469
Iteration 400: Loss = -11319.895175317255
Iteration 500: Loss = -11319.823699209028
Iteration 600: Loss = -11319.775031725985
Iteration 700: Loss = -11319.751667129833
Iteration 800: Loss = -11319.735530840857
Iteration 900: Loss = -11319.722506180007
Iteration 1000: Loss = -11319.71099281527
Iteration 1100: Loss = -11319.702749659144
Iteration 1200: Loss = -11319.695696904113
Iteration 1300: Loss = -11319.68800001896
Iteration 1400: Loss = -11319.67732992483
Iteration 1500: Loss = -11319.674480234047
Iteration 1600: Loss = -11319.67213035562
Iteration 1700: Loss = -11319.66922988317
Iteration 1800: Loss = -11319.667212487948
Iteration 1900: Loss = -11319.665937960619
Iteration 2000: Loss = -11319.66487340902
Iteration 2100: Loss = -11319.663986448146
Iteration 2200: Loss = -11319.663161438846
Iteration 2300: Loss = -11319.66282510502
Iteration 2400: Loss = -11319.66172354937
Iteration 2500: Loss = -11319.660708335452
Iteration 2600: Loss = -11319.645978197954
Iteration 2700: Loss = -11319.641948040635
Iteration 2800: Loss = -11319.641504253044
Iteration 2900: Loss = -11319.642612693755
1
Iteration 3000: Loss = -11319.641256209587
Iteration 3100: Loss = -11319.640647506067
Iteration 3200: Loss = -11319.64040778075
Iteration 3300: Loss = -11319.642414845339
1
Iteration 3400: Loss = -11319.639976898547
Iteration 3500: Loss = -11319.640180732842
1
Iteration 3600: Loss = -11319.639959090966
Iteration 3700: Loss = -11319.639362341944
Iteration 3800: Loss = -11319.639706525428
1
Iteration 3900: Loss = -11319.63950891947
2
Iteration 4000: Loss = -11319.638728095342
Iteration 4100: Loss = -11319.638699555411
Iteration 4200: Loss = -11319.638435130688
Iteration 4300: Loss = -11319.638912581335
1
Iteration 4400: Loss = -11319.638423184926
Iteration 4500: Loss = -11319.639705169837
1
Iteration 4600: Loss = -11319.638744560127
2
Iteration 4700: Loss = -11319.64005038914
3
Iteration 4800: Loss = -11319.637897922832
Iteration 4900: Loss = -11319.637937525515
Iteration 5000: Loss = -11319.638380105678
1
Iteration 5100: Loss = -11319.64638723968
2
Iteration 5200: Loss = -11319.63538909729
Iteration 5300: Loss = -11319.635535495254
1
Iteration 5400: Loss = -11319.636564795037
2
Iteration 5500: Loss = -11319.567708487402
Iteration 5600: Loss = -11319.5638226055
Iteration 5700: Loss = -11319.561183062306
Iteration 5800: Loss = -11319.561041439969
Iteration 5900: Loss = -11319.561064150588
Iteration 6000: Loss = -11319.56273834407
1
Iteration 6100: Loss = -11319.561295659836
2
Iteration 6200: Loss = -11319.560988835412
Iteration 6300: Loss = -11319.561433640301
1
Iteration 6400: Loss = -11319.561818832828
2
Iteration 6500: Loss = -11319.565505220275
3
Iteration 6600: Loss = -11319.561410810855
4
Iteration 6700: Loss = -11319.560353713234
Iteration 6800: Loss = -11319.56065861274
1
Iteration 6900: Loss = -11319.560381414878
Iteration 7000: Loss = -11319.560522463466
1
Iteration 7100: Loss = -11319.565182012337
2
Iteration 7200: Loss = -11319.560656489526
3
Iteration 7300: Loss = -11319.559184005499
Iteration 7400: Loss = -11319.561012889713
1
Iteration 7500: Loss = -11319.559293591179
2
Iteration 7600: Loss = -11319.570477460262
3
Iteration 7700: Loss = -11319.561764838603
4
Iteration 7800: Loss = -11319.559326942242
5
Iteration 7900: Loss = -11319.561625642184
6
Iteration 8000: Loss = -11319.560478224299
7
Iteration 8100: Loss = -11319.558894597843
Iteration 8200: Loss = -11319.564415352106
1
Iteration 8300: Loss = -11319.559146476307
2
Iteration 8400: Loss = -11319.563597351462
3
Iteration 8500: Loss = -11319.55946490203
4
Iteration 8600: Loss = -11319.55972123292
5
Iteration 8700: Loss = -11319.57483042518
6
Iteration 8800: Loss = -11319.558733115371
Iteration 8900: Loss = -11319.567696296077
1
Iteration 9000: Loss = -11319.558673016862
Iteration 9100: Loss = -11319.60451052594
1
Iteration 9200: Loss = -11319.558612256349
Iteration 9300: Loss = -11319.555088496742
Iteration 9400: Loss = -11319.554969064624
Iteration 9500: Loss = -11319.55333183559
Iteration 9600: Loss = -11319.592860913022
1
Iteration 9700: Loss = -11319.553270662032
Iteration 9800: Loss = -11319.553225343785
Iteration 9900: Loss = -11319.553422546258
1
Iteration 10000: Loss = -11319.55301675887
Iteration 10100: Loss = -11319.66955218413
1
Iteration 10200: Loss = -11319.55303114566
Iteration 10300: Loss = -11319.553586625632
1
Iteration 10400: Loss = -11319.604860273235
2
Iteration 10500: Loss = -11319.556038847668
3
Iteration 10600: Loss = -11319.555581235612
4
Iteration 10700: Loss = -11319.555546208092
5
Iteration 10800: Loss = -11319.553245699979
6
Iteration 10900: Loss = -11319.558644594084
7
Iteration 11000: Loss = -11319.592612562767
8
Iteration 11100: Loss = -11319.553700761762
9
Iteration 11200: Loss = -11319.636646757659
10
Iteration 11300: Loss = -11319.553230323449
11
Iteration 11400: Loss = -11319.5686586665
12
Iteration 11500: Loss = -11319.561758438795
13
Iteration 11600: Loss = -11319.560202953624
14
Iteration 11700: Loss = -11319.556689998633
15
Stopping early at iteration 11700 due to no improvement.
pi: tensor([[0.6199, 0.3801],
        [0.2898, 0.7102]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5541, 0.4459], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3003, 0.0953],
         [0.7096, 0.2101]],

        [[0.6145, 0.0984],
         [0.6274, 0.6673]],

        [[0.6945, 0.1008],
         [0.7072, 0.6666]],

        [[0.6991, 0.1072],
         [0.6245, 0.5505]],

        [[0.6265, 0.0945],
         [0.7042, 0.5997]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448420005390695
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 84
Adjusted Rand Index: 0.4553781408590172
Global Adjusted Rand Index: 0.3982215042611802
Average Adjusted Rand Index: 0.8285269285785926
11302.18316150153
[0.8833641283285453, 0.3982215042611802] [0.8848264772068923, 0.8285269285785926] [11270.50574585159, 11319.556689998633]
-------------------------------------
This iteration is 5
True Objective function: Loss = -10877.202446465548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25028.448832299462
Iteration 100: Loss = -10989.513623070794
Iteration 200: Loss = -10989.143276215496
Iteration 300: Loss = -10989.067794304867
Iteration 400: Loss = -10989.033354615252
Iteration 500: Loss = -10989.013614649339
Iteration 600: Loss = -10989.00062269726
Iteration 700: Loss = -10988.991125090335
Iteration 800: Loss = -10988.983206629104
Iteration 900: Loss = -10988.975682267275
Iteration 1000: Loss = -10988.967367776211
Iteration 1100: Loss = -10988.95660396791
Iteration 1200: Loss = -10988.939769048644
Iteration 1300: Loss = -10988.904164972313
Iteration 1400: Loss = -10988.741671980139
Iteration 1500: Loss = -10988.066655717177
Iteration 1600: Loss = -10987.76833113309
Iteration 1700: Loss = -10987.623186502055
Iteration 1800: Loss = -10987.516429515546
Iteration 1900: Loss = -10987.419535498417
Iteration 2000: Loss = -10987.328955933623
Iteration 2100: Loss = -10987.239949904215
Iteration 2200: Loss = -10987.153333234126
Iteration 2300: Loss = -10987.070999199123
Iteration 2400: Loss = -10986.996264050382
Iteration 2500: Loss = -10986.924754496065
Iteration 2600: Loss = -10986.854764452515
Iteration 2700: Loss = -10986.785923209995
Iteration 2800: Loss = -10986.719260545193
Iteration 2900: Loss = -10986.65641746041
Iteration 3000: Loss = -10986.598765916977
Iteration 3100: Loss = -10986.547118794144
Iteration 3200: Loss = -10986.50192228087
Iteration 3300: Loss = -10986.462987865274
Iteration 3400: Loss = -10986.423309932663
Iteration 3500: Loss = -10986.306972039189
Iteration 3600: Loss = -10986.266515947444
Iteration 3700: Loss = -10986.226379188005
Iteration 3800: Loss = -10986.177029730316
Iteration 3900: Loss = -10986.09786952075
Iteration 4000: Loss = -10985.987211694051
Iteration 4100: Loss = -10985.837663290577
Iteration 4200: Loss = -10985.740506948076
Iteration 4300: Loss = -10985.667109075324
Iteration 4400: Loss = -10985.609844971319
Iteration 4500: Loss = -10985.56066339553
Iteration 4600: Loss = -10985.524362665126
Iteration 4700: Loss = -10985.494552487751
Iteration 4800: Loss = -10985.469285589435
Iteration 4900: Loss = -10985.448442555611
Iteration 5000: Loss = -10985.430264174693
Iteration 5100: Loss = -10985.415157800502
Iteration 5200: Loss = -10985.40062149285
Iteration 5300: Loss = -10985.387336285015
Iteration 5400: Loss = -10985.377845350935
Iteration 5500: Loss = -10985.369487409685
Iteration 5600: Loss = -10985.36213241002
Iteration 5700: Loss = -10985.355649702246
Iteration 5800: Loss = -10985.349819030956
Iteration 5900: Loss = -10985.344673574196
Iteration 6000: Loss = -10985.340066825163
Iteration 6100: Loss = -10985.335979383473
Iteration 6200: Loss = -10985.33223701294
Iteration 6300: Loss = -10985.328895932798
Iteration 6400: Loss = -10985.32592534932
Iteration 6500: Loss = -10985.32320977326
Iteration 6600: Loss = -10985.320765522089
Iteration 6700: Loss = -10985.318631597
Iteration 6800: Loss = -10985.316612556693
Iteration 6900: Loss = -10985.31480754495
Iteration 7000: Loss = -10985.313174294939
Iteration 7100: Loss = -10985.311719803505
Iteration 7200: Loss = -10985.310387248961
Iteration 7300: Loss = -10985.30918720549
Iteration 7400: Loss = -10985.308079276765
Iteration 7500: Loss = -10985.307152459602
Iteration 7600: Loss = -10985.306247917984
Iteration 7700: Loss = -10985.305448085888
Iteration 7800: Loss = -10985.304742807884
Iteration 7900: Loss = -10985.304068232424
Iteration 8000: Loss = -10985.3035223743
Iteration 8100: Loss = -10985.399381737734
1
Iteration 8200: Loss = -10985.302487671044
Iteration 8300: Loss = -10985.320602970245
1
Iteration 8400: Loss = -10985.301722385942
Iteration 8500: Loss = -10985.301343843608
Iteration 8600: Loss = -10985.302592051718
1
Iteration 8700: Loss = -10985.300802189411
Iteration 8800: Loss = -10985.300617872746
Iteration 8900: Loss = -10985.300451151827
Iteration 9000: Loss = -10985.300172672582
Iteration 9100: Loss = -10985.299990120802
Iteration 9200: Loss = -10985.299870963252
Iteration 9300: Loss = -10985.299779959105
Iteration 9400: Loss = -10985.299637367636
Iteration 9500: Loss = -10985.306445453338
1
Iteration 9600: Loss = -10985.29945760086
Iteration 9700: Loss = -10985.299383354706
Iteration 9800: Loss = -10985.325108815468
1
Iteration 9900: Loss = -10985.299286369722
Iteration 10000: Loss = -10985.299286328373
Iteration 10100: Loss = -10985.3184102332
1
Iteration 10200: Loss = -10985.299206496346
Iteration 10300: Loss = -10985.299174717675
Iteration 10400: Loss = -10985.299147564716
Iteration 10500: Loss = -10985.300126582524
1
Iteration 10600: Loss = -10985.29912877792
Iteration 10700: Loss = -10985.299110450544
Iteration 10800: Loss = -10985.300065835605
1
Iteration 10900: Loss = -10985.299101823895
Iteration 11000: Loss = -10985.29911152198
Iteration 11100: Loss = -10985.665974147509
1
Iteration 11200: Loss = -10985.299109938574
Iteration 11300: Loss = -10985.299105330458
Iteration 11400: Loss = -10985.299095360335
Iteration 11500: Loss = -10985.299413565835
1
Iteration 11600: Loss = -10985.299073570843
Iteration 11700: Loss = -10985.299088737389
Iteration 11800: Loss = -10985.31600696093
1
Iteration 11900: Loss = -10985.2991013118
Iteration 12000: Loss = -10985.29909156055
Iteration 12100: Loss = -10985.398319288064
1
Iteration 12200: Loss = -10985.299121930988
Iteration 12300: Loss = -10985.299110252183
Iteration 12400: Loss = -10985.300601482937
1
Iteration 12500: Loss = -10985.299264969914
2
Iteration 12600: Loss = -10985.299559375106
3
Iteration 12700: Loss = -10985.301236709698
4
Iteration 12800: Loss = -10985.299344399502
5
Iteration 12900: Loss = -10985.299169600747
Iteration 13000: Loss = -10985.299589358257
1
Iteration 13100: Loss = -10985.300615437425
2
Iteration 13200: Loss = -10985.299153736123
Iteration 13300: Loss = -10985.299577255671
1
Iteration 13400: Loss = -10985.306063583837
2
Iteration 13500: Loss = -10985.299362321732
3
Iteration 13600: Loss = -10985.2991606233
Iteration 13700: Loss = -10985.342581121235
1
Iteration 13800: Loss = -10985.307466292528
2
Iteration 13900: Loss = -10985.299068754703
Iteration 14000: Loss = -10985.299105074995
Iteration 14100: Loss = -10985.310324386333
1
Iteration 14200: Loss = -10985.299095080001
Iteration 14300: Loss = -10985.384829562981
1
Iteration 14400: Loss = -10985.299060684407
Iteration 14500: Loss = -10985.299892926867
1
Iteration 14600: Loss = -10985.299170826374
2
Iteration 14700: Loss = -10985.300807174717
3
Iteration 14800: Loss = -10985.300856062424
4
Iteration 14900: Loss = -10985.29916679497
5
Iteration 15000: Loss = -10985.299101937757
Iteration 15100: Loss = -10985.54623876126
1
Iteration 15200: Loss = -10985.299067800048
Iteration 15300: Loss = -10985.420782073432
1
Iteration 15400: Loss = -10985.299096337078
Iteration 15500: Loss = -10985.299217627253
1
Iteration 15600: Loss = -10985.299188438308
Iteration 15700: Loss = -10985.303944053932
1
Iteration 15800: Loss = -10985.299069186523
Iteration 15900: Loss = -10985.302383946877
1
Iteration 16000: Loss = -10985.300698793377
2
Iteration 16100: Loss = -10985.299108557523
Iteration 16200: Loss = -10985.299419952455
1
Iteration 16300: Loss = -10985.300256900797
2
Iteration 16400: Loss = -10985.299044284966
Iteration 16500: Loss = -10985.30644255601
1
Iteration 16600: Loss = -10985.305448911331
2
Iteration 16700: Loss = -10985.301482189514
3
Iteration 16800: Loss = -10985.2990999999
Iteration 16900: Loss = -10985.299319175123
1
Iteration 17000: Loss = -10985.303089646071
2
Iteration 17100: Loss = -10985.299033674986
Iteration 17200: Loss = -10985.299271563295
1
Iteration 17300: Loss = -10985.299820117678
2
Iteration 17400: Loss = -10985.299114651942
Iteration 17500: Loss = -10985.31581932008
1
Iteration 17600: Loss = -10985.299248371763
2
Iteration 17700: Loss = -10985.301122401816
3
Iteration 17800: Loss = -10985.29912280993
Iteration 17900: Loss = -10985.34515426905
1
Iteration 18000: Loss = -10985.299317052286
2
Iteration 18100: Loss = -10985.302253784099
3
Iteration 18200: Loss = -10985.305723944166
4
Iteration 18300: Loss = -10985.29983855088
5
Iteration 18400: Loss = -10985.299391513703
6
Iteration 18500: Loss = -10985.301061411294
7
Iteration 18600: Loss = -10985.305139043701
8
Iteration 18700: Loss = -10985.302070939222
9
Iteration 18800: Loss = -10985.29974408145
10
Iteration 18900: Loss = -10985.301496887443
11
Iteration 19000: Loss = -10985.303462648111
12
Iteration 19100: Loss = -10985.30032466509
13
Iteration 19200: Loss = -10985.383887646669
14
Iteration 19300: Loss = -10985.304517212904
15
Stopping early at iteration 19300 due to no improvement.
pi: tensor([[9.2437e-01, 7.5633e-02],
        [2.4786e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0536, 0.9464], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[7.0431e-04, 1.7998e-01],
         [5.8571e-01, 1.6217e-01]],

        [[5.6004e-01, 2.2344e-01],
         [5.3259e-01, 5.3555e-01]],

        [[7.1825e-01, 1.0484e-01],
         [6.1684e-01, 6.5973e-01]],

        [[5.3206e-01, 1.3608e-01],
         [6.6396e-01, 5.9311e-01]],

        [[5.6528e-01, 2.1099e-01],
         [6.8828e-01, 6.4038e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.012598425196850394
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.0140971543785986
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: -0.010102533172496984
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.006221888713864513
Global Adjusted Rand Index: 0.005300804843876764
Average Adjusted Rand Index: 0.002075640423439016
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22153.419644513862
Iteration 100: Loss = -10989.533969213084
Iteration 200: Loss = -10989.19896686063
Iteration 300: Loss = -10989.087115164819
Iteration 400: Loss = -10989.00528027694
Iteration 500: Loss = -10986.143969495843
Iteration 600: Loss = -10984.024210103918
Iteration 700: Loss = -10983.67405571382
Iteration 800: Loss = -10983.545275429677
Iteration 900: Loss = -10983.48415277212
Iteration 1000: Loss = -10983.448963628183
Iteration 1100: Loss = -10983.425950921961
Iteration 1200: Loss = -10983.409631116116
Iteration 1300: Loss = -10983.397537893494
Iteration 1400: Loss = -10983.388221664325
Iteration 1500: Loss = -10983.38088069673
Iteration 1600: Loss = -10983.374979620843
Iteration 1700: Loss = -10983.370119969084
Iteration 1800: Loss = -10983.366102149263
Iteration 1900: Loss = -10983.362683565709
Iteration 2000: Loss = -10983.359753710056
Iteration 2100: Loss = -10983.35723763259
Iteration 2200: Loss = -10983.355050462082
Iteration 2300: Loss = -10983.353161536503
Iteration 2400: Loss = -10983.351479703868
Iteration 2500: Loss = -10983.350016139551
Iteration 2600: Loss = -10983.348713380861
Iteration 2700: Loss = -10983.347544062479
Iteration 2800: Loss = -10983.34649001425
Iteration 2900: Loss = -10983.345540228684
Iteration 3000: Loss = -10983.344681011817
Iteration 3100: Loss = -10983.343900510044
Iteration 3200: Loss = -10983.343207266775
Iteration 3300: Loss = -10983.342593986848
Iteration 3400: Loss = -10983.342003907917
Iteration 3500: Loss = -10983.341465004482
Iteration 3600: Loss = -10983.340986659314
Iteration 3700: Loss = -10983.34056319453
Iteration 3800: Loss = -10983.340107549653
Iteration 3900: Loss = -10983.339746234038
Iteration 4000: Loss = -10983.339397144935
Iteration 4100: Loss = -10983.339047825202
Iteration 4200: Loss = -10983.33875947878
Iteration 4300: Loss = -10983.338503963829
Iteration 4400: Loss = -10983.338166828296
Iteration 4500: Loss = -10983.337976482551
Iteration 4600: Loss = -10983.337742178377
Iteration 4700: Loss = -10983.337538808744
Iteration 4800: Loss = -10983.337323050824
Iteration 4900: Loss = -10983.33715468502
Iteration 5000: Loss = -10983.336994972504
Iteration 5100: Loss = -10983.336794091041
Iteration 5200: Loss = -10983.336652240112
Iteration 5300: Loss = -10983.336506961434
Iteration 5400: Loss = -10983.336366752088
Iteration 5500: Loss = -10983.336242401208
Iteration 5600: Loss = -10983.336150191244
Iteration 5700: Loss = -10983.33604739732
Iteration 5800: Loss = -10983.335904062227
Iteration 5900: Loss = -10983.335807147934
Iteration 6000: Loss = -10983.335710870922
Iteration 6100: Loss = -10983.335608237492
Iteration 6200: Loss = -10983.335551658454
Iteration 6300: Loss = -10983.335476128881
Iteration 6400: Loss = -10983.335396371076
Iteration 6500: Loss = -10983.335361751195
Iteration 6600: Loss = -10983.335291667523
Iteration 6700: Loss = -10983.335216870182
Iteration 6800: Loss = -10983.335147162627
Iteration 6900: Loss = -10983.335090216911
Iteration 7000: Loss = -10983.335057541068
Iteration 7100: Loss = -10983.334995680538
Iteration 7200: Loss = -10983.334960685577
Iteration 7300: Loss = -10983.334892548914
Iteration 7400: Loss = -10983.334832794177
Iteration 7500: Loss = -10983.334792824957
Iteration 7600: Loss = -10983.334741563327
Iteration 7700: Loss = -10983.3346824202
Iteration 7800: Loss = -10983.334610169493
Iteration 7900: Loss = -10983.334569919922
Iteration 8000: Loss = -10983.334555817864
Iteration 8100: Loss = -10983.334518611036
Iteration 8200: Loss = -10983.334508803282
Iteration 8300: Loss = -10983.334459697398
Iteration 8400: Loss = -10983.334931526011
1
Iteration 8500: Loss = -10983.334522993691
Iteration 8600: Loss = -10983.334463867037
Iteration 8700: Loss = -10983.334488608416
Iteration 8800: Loss = -10983.336768133362
1
Iteration 8900: Loss = -10983.334485424402
Iteration 9000: Loss = -10983.334440520719
Iteration 9100: Loss = -10983.334475401787
Iteration 9200: Loss = -10983.335142903878
1
Iteration 9300: Loss = -10983.334481282564
Iteration 9400: Loss = -10983.334439642018
Iteration 9500: Loss = -10983.334475284342
Iteration 9600: Loss = -10983.336312105283
1
Iteration 9700: Loss = -10983.334427358925
Iteration 9800: Loss = -10983.334462003537
Iteration 9900: Loss = -10983.334384467533
Iteration 10000: Loss = -10983.338178416796
1
Iteration 10100: Loss = -10983.33435556043
Iteration 10200: Loss = -10983.334389508153
Iteration 10300: Loss = -10983.334835728137
1
Iteration 10400: Loss = -10983.335212929916
2
Iteration 10500: Loss = -10983.334318593405
Iteration 10600: Loss = -10983.334317575975
Iteration 10700: Loss = -10983.334395869877
Iteration 10800: Loss = -10983.33429252782
Iteration 10900: Loss = -10983.334245402379
Iteration 11000: Loss = -10983.336501221323
1
Iteration 11100: Loss = -10983.334315895623
Iteration 11200: Loss = -10983.33427118986
Iteration 11300: Loss = -10983.334247804874
Iteration 11400: Loss = -10983.375852309724
1
Iteration 11500: Loss = -10983.334251658807
Iteration 11600: Loss = -10983.334190466756
Iteration 11700: Loss = -10983.334232999268
Iteration 11800: Loss = -10983.336471121309
1
Iteration 11900: Loss = -10983.334195622401
Iteration 12000: Loss = -10983.334258897055
Iteration 12100: Loss = -10983.33420875205
Iteration 12200: Loss = -10983.33418169387
Iteration 12300: Loss = -10983.334146480534
Iteration 12400: Loss = -10983.334157023603
Iteration 12500: Loss = -10983.360925941597
1
Iteration 12600: Loss = -10983.334099328613
Iteration 12700: Loss = -10983.334187522172
Iteration 12800: Loss = -10983.33414759949
Iteration 12900: Loss = -10983.343829543499
1
Iteration 13000: Loss = -10983.334211885875
Iteration 13100: Loss = -10983.334178551448
Iteration 13200: Loss = -10983.334154176371
Iteration 13300: Loss = -10983.335001883308
1
Iteration 13400: Loss = -10983.334149331415
Iteration 13500: Loss = -10983.334131112233
Iteration 13600: Loss = -10983.340353116
1
Iteration 13700: Loss = -10983.334099437412
Iteration 13800: Loss = -10983.333976569804
Iteration 13900: Loss = -10983.333820579861
Iteration 14000: Loss = -10983.333917269449
Iteration 14100: Loss = -10983.333774877516
Iteration 14200: Loss = -10983.333761501925
Iteration 14300: Loss = -10983.333818448675
Iteration 14400: Loss = -10983.33373598366
Iteration 14500: Loss = -10983.333744750524
Iteration 14600: Loss = -10983.333748500329
Iteration 14700: Loss = -10983.333732596911
Iteration 14800: Loss = -10983.333708502216
Iteration 14900: Loss = -10983.334025448423
1
Iteration 15000: Loss = -10983.33401195193
2
Iteration 15100: Loss = -10983.333794623595
Iteration 15200: Loss = -10983.333802190145
Iteration 15300: Loss = -10983.522826965986
1
Iteration 15400: Loss = -10983.333846802681
Iteration 15500: Loss = -10983.333845078208
Iteration 15600: Loss = -10983.333879299269
Iteration 15700: Loss = -10983.34693716399
1
Iteration 15800: Loss = -10983.33418158226
2
Iteration 15900: Loss = -10983.333904974037
Iteration 16000: Loss = -10983.334015708075
1
Iteration 16100: Loss = -10983.333891191656
Iteration 16200: Loss = -10983.333912751692
Iteration 16300: Loss = -10983.371438684235
1
Iteration 16400: Loss = -10983.333902168879
Iteration 16500: Loss = -10983.33390824126
Iteration 16600: Loss = -10983.38230718443
1
Iteration 16700: Loss = -10983.334199068466
2
Iteration 16800: Loss = -10983.333910503443
Iteration 16900: Loss = -10983.334127822516
1
Iteration 17000: Loss = -10983.333918940307
Iteration 17100: Loss = -10983.333990703499
Iteration 17200: Loss = -10983.510801770672
1
Iteration 17300: Loss = -10983.334044199582
Iteration 17400: Loss = -10983.333918008697
Iteration 17500: Loss = -10983.346650647805
1
Iteration 17600: Loss = -10983.333871204493
Iteration 17700: Loss = -10983.33428459535
1
Iteration 17800: Loss = -10983.341107719729
2
Iteration 17900: Loss = -10983.333994840148
3
Iteration 18000: Loss = -10983.33387087939
Iteration 18100: Loss = -10983.35565116646
1
Iteration 18200: Loss = -10983.333840038613
Iteration 18300: Loss = -10983.335412976037
1
Iteration 18400: Loss = -10983.333828456673
Iteration 18500: Loss = -10983.333930081455
1
Iteration 18600: Loss = -10983.333965438136
2
Iteration 18700: Loss = -10983.334601906545
3
Iteration 18800: Loss = -10983.3356381654
4
Iteration 18900: Loss = -10983.33382595179
Iteration 19000: Loss = -10983.334323783569
1
Iteration 19100: Loss = -10983.335175521423
2
Iteration 19200: Loss = -10983.333853187269
Iteration 19300: Loss = -10983.398844652744
1
Iteration 19400: Loss = -10983.334537047393
2
Iteration 19500: Loss = -10983.333799190075
Iteration 19600: Loss = -10983.7833767306
1
Iteration 19700: Loss = -10983.333806211851
Iteration 19800: Loss = -10983.333813018164
Iteration 19900: Loss = -10983.347863103354
1
pi: tensor([[1.0000e+00, 1.4216e-09],
        [1.2587e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9900, 0.0100], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1621, 0.1515],
         [0.5141, 0.2000]],

        [[0.5429, 0.0606],
         [0.5463, 0.6675]],

        [[0.6524, 0.2021],
         [0.6786, 0.5415]],

        [[0.6883, 0.2719],
         [0.5230, 0.6575]],

        [[0.5275, 0.2424],
         [0.5702, 0.7242]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.009987515605493134
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: -0.007201490658206174
Global Adjusted Rand Index: 0.0022019123893674613
Average Adjusted Rand Index: 0.002280158184571799
10877.202446465548
[0.005300804843876764, 0.0022019123893674613] [0.002075640423439016, 0.002280158184571799] [10985.304517212904, 10983.3338143915]
-------------------------------------
This iteration is 6
True Objective function: Loss = -11126.27711673985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22861.960929461664
Iteration 100: Loss = -11413.434478777883
Iteration 200: Loss = -11408.772671822648
Iteration 300: Loss = -11403.119073259042
Iteration 400: Loss = -11371.88371395367
Iteration 500: Loss = -11209.736716087813
Iteration 600: Loss = -11186.843003598382
Iteration 700: Loss = -11180.665844354451
Iteration 800: Loss = -11178.268443201707
Iteration 900: Loss = -11178.078702592367
Iteration 1000: Loss = -11177.98461305561
Iteration 1100: Loss = -11177.917810143303
Iteration 1200: Loss = -11177.866741370179
Iteration 1300: Loss = -11177.828143138035
Iteration 1400: Loss = -11177.795805472471
Iteration 1500: Loss = -11177.768339043852
Iteration 1600: Loss = -11177.743274915287
Iteration 1700: Loss = -11177.6374615536
Iteration 1800: Loss = -11177.622654685652
Iteration 1900: Loss = -11177.609479393117
Iteration 2000: Loss = -11177.595652931746
Iteration 2100: Loss = -11177.57800294255
Iteration 2200: Loss = -11177.543198306636
Iteration 2300: Loss = -11177.449478016752
Iteration 2400: Loss = -11176.872107395699
Iteration 2500: Loss = -11176.584237935325
Iteration 2600: Loss = -11176.506186214508
Iteration 2700: Loss = -11176.465845116682
Iteration 2800: Loss = -11176.416217368855
Iteration 2900: Loss = -11176.419752940858
1
Iteration 3000: Loss = -11176.4017366
Iteration 3100: Loss = -11176.397095069424
Iteration 3200: Loss = -11176.399655610961
1
Iteration 3300: Loss = -11176.400834915212
2
Iteration 3400: Loss = -11176.382263816264
Iteration 3500: Loss = -11176.299147877606
Iteration 3600: Loss = -11176.27977521273
Iteration 3700: Loss = -11176.275076996924
Iteration 3800: Loss = -11176.29468839464
1
Iteration 3900: Loss = -11176.269729358215
Iteration 4000: Loss = -11176.27084558748
1
Iteration 4100: Loss = -11176.266534754586
Iteration 4200: Loss = -11176.19163768212
Iteration 4300: Loss = -11171.073994907616
Iteration 4400: Loss = -11170.330436383736
Iteration 4500: Loss = -11170.290956620223
Iteration 4600: Loss = -11170.162800611371
Iteration 4700: Loss = -11170.16508113816
1
Iteration 4800: Loss = -11170.140760203101
Iteration 4900: Loss = -11170.128129167544
Iteration 5000: Loss = -11170.123554128308
Iteration 5100: Loss = -11170.077396489725
Iteration 5200: Loss = -11170.045903955537
Iteration 5300: Loss = -11170.043711452801
Iteration 5400: Loss = -11170.042448450731
Iteration 5500: Loss = -11170.041666626114
Iteration 5600: Loss = -11169.978243630403
Iteration 5700: Loss = -11169.969637730896
Iteration 5800: Loss = -11169.969076943722
Iteration 5900: Loss = -11169.967709518853
Iteration 6000: Loss = -11169.966850282508
Iteration 6100: Loss = -11169.968781351554
1
Iteration 6200: Loss = -11169.96565550981
Iteration 6300: Loss = -11169.964652906598
Iteration 6400: Loss = -11169.963550307513
Iteration 6500: Loss = -11169.962895067218
Iteration 6600: Loss = -11169.962641188913
Iteration 6700: Loss = -11169.96409146988
1
Iteration 6800: Loss = -11169.964294737776
2
Iteration 6900: Loss = -11169.964349885742
3
Iteration 7000: Loss = -11169.962825796885
4
Iteration 7100: Loss = -11169.961733710508
Iteration 7200: Loss = -11169.97024905503
1
Iteration 7300: Loss = -11169.961162975766
Iteration 7400: Loss = -11169.960282978209
Iteration 7500: Loss = -11169.955995740242
Iteration 7600: Loss = -11169.96420826657
1
Iteration 7700: Loss = -11169.955216467893
Iteration 7800: Loss = -11169.95882582916
1
Iteration 7900: Loss = -11169.95535735084
2
Iteration 8000: Loss = -11170.034938531495
3
Iteration 8100: Loss = -11169.954762541602
Iteration 8200: Loss = -11169.9768410679
1
Iteration 8300: Loss = -11169.952431802683
Iteration 8400: Loss = -11169.969709173647
1
Iteration 8500: Loss = -11169.951518203434
Iteration 8600: Loss = -11169.95140703656
Iteration 8700: Loss = -11169.953504948613
1
Iteration 8800: Loss = -11169.951307199311
Iteration 8900: Loss = -11169.951231795461
Iteration 9000: Loss = -11169.951662268813
1
Iteration 9100: Loss = -11169.95078624944
Iteration 9200: Loss = -11169.956171873395
1
Iteration 9300: Loss = -11169.951345325288
2
Iteration 9400: Loss = -11169.95044613825
Iteration 9500: Loss = -11170.0155549215
1
Iteration 9600: Loss = -11169.950179285664
Iteration 9700: Loss = -11169.952603801104
1
Iteration 9800: Loss = -11169.954295039755
2
Iteration 9900: Loss = -11169.951992285869
3
Iteration 10000: Loss = -11169.950813565507
4
Iteration 10100: Loss = -11169.950010193641
Iteration 10200: Loss = -11169.98658966453
1
Iteration 10300: Loss = -11169.950006222263
Iteration 10400: Loss = -11169.951853736375
1
Iteration 10500: Loss = -11169.949924755756
Iteration 10600: Loss = -11169.950024079319
Iteration 10700: Loss = -11169.949928818909
Iteration 10800: Loss = -11169.95005223789
1
Iteration 10900: Loss = -11169.94992208067
Iteration 11000: Loss = -11169.960135171183
1
Iteration 11100: Loss = -11169.954312473174
2
Iteration 11200: Loss = -11170.021257088736
3
Iteration 11300: Loss = -11169.951711185198
4
Iteration 11400: Loss = -11169.95162721959
5
Iteration 11500: Loss = -11169.956329640883
6
Iteration 11600: Loss = -11169.950476683842
7
Iteration 11700: Loss = -11170.019292585903
8
Iteration 11800: Loss = -11169.949810325405
Iteration 11900: Loss = -11169.951799410865
1
Iteration 12000: Loss = -11169.950657565088
2
Iteration 12100: Loss = -11169.949619354711
Iteration 12200: Loss = -11169.962819778864
1
Iteration 12300: Loss = -11169.948915460363
Iteration 12400: Loss = -11169.949271289108
1
Iteration 12500: Loss = -11169.950684622812
2
Iteration 12600: Loss = -11169.970518668524
3
Iteration 12700: Loss = -11169.949275569914
4
Iteration 12800: Loss = -11169.951119168949
5
Iteration 12900: Loss = -11170.120112813016
6
Iteration 13000: Loss = -11169.948828085113
Iteration 13100: Loss = -11169.948912309033
Iteration 13200: Loss = -11169.948870666898
Iteration 13300: Loss = -11170.07044307093
1
Iteration 13400: Loss = -11169.948848518528
Iteration 13500: Loss = -11169.94942940349
1
Iteration 13600: Loss = -11170.001235389323
2
Iteration 13700: Loss = -11169.948834426434
Iteration 13800: Loss = -11169.954570499363
1
Iteration 13900: Loss = -11169.948815431882
Iteration 14000: Loss = -11169.94884013055
Iteration 14100: Loss = -11169.949295978135
1
Iteration 14200: Loss = -11169.985196543634
2
Iteration 14300: Loss = -11169.948596274711
Iteration 14400: Loss = -11169.951269434292
1
Iteration 14500: Loss = -11169.953697914489
2
Iteration 14600: Loss = -11170.0214913021
3
Iteration 14700: Loss = -11169.96755081335
4
Iteration 14800: Loss = -11170.017179916465
5
Iteration 14900: Loss = -11169.948504190688
Iteration 15000: Loss = -11169.954358422794
1
Iteration 15100: Loss = -11169.94846653696
Iteration 15200: Loss = -11169.954044927601
1
Iteration 15300: Loss = -11170.00337539448
2
Iteration 15400: Loss = -11169.95231627026
3
Iteration 15500: Loss = -11169.94867600182
4
Iteration 15600: Loss = -11169.948521184326
Iteration 15700: Loss = -11169.95992767336
1
Iteration 15800: Loss = -11169.988821164497
2
Iteration 15900: Loss = -11169.948401950624
Iteration 16000: Loss = -11169.948763584995
1
Iteration 16100: Loss = -11169.955132715779
2
Iteration 16200: Loss = -11169.959485859441
3
Iteration 16300: Loss = -11169.949930543731
4
Iteration 16400: Loss = -11169.948606813432
5
Iteration 16500: Loss = -11169.95939985087
6
Iteration 16600: Loss = -11169.950015543527
7
Iteration 16700: Loss = -11169.95107270084
8
Iteration 16800: Loss = -11169.949352556294
9
Iteration 16900: Loss = -11169.953212864046
10
Iteration 17000: Loss = -11169.948408848624
Iteration 17100: Loss = -11169.950727541103
1
Iteration 17200: Loss = -11169.95026768822
2
Iteration 17300: Loss = -11170.147415346753
3
Iteration 17400: Loss = -11169.950542651086
4
Iteration 17500: Loss = -11169.949443744828
5
Iteration 17600: Loss = -11169.951439037586
6
Iteration 17700: Loss = -11169.993978738636
7
Iteration 17800: Loss = -11169.967943765256
8
Iteration 17900: Loss = -11169.9548958432
9
Iteration 18000: Loss = -11170.004845924115
10
Iteration 18100: Loss = -11169.95015636519
11
Iteration 18200: Loss = -11169.948384548523
Iteration 18300: Loss = -11169.952255090417
1
Iteration 18400: Loss = -11169.967088247027
2
Iteration 18500: Loss = -11169.950494772242
3
Iteration 18600: Loss = -11169.949515319133
4
Iteration 18700: Loss = -11169.96439904704
5
Iteration 18800: Loss = -11170.020081337067
6
Iteration 18900: Loss = -11169.956240822536
7
Iteration 19000: Loss = -11169.949087561592
8
Iteration 19100: Loss = -11169.957469191946
9
Iteration 19200: Loss = -11169.948540050407
10
Iteration 19300: Loss = -11169.953276257871
11
Iteration 19400: Loss = -11169.986085542017
12
Iteration 19500: Loss = -11169.948395365775
Iteration 19600: Loss = -11169.948842552692
1
Iteration 19700: Loss = -11169.957824418463
2
Iteration 19800: Loss = -11169.9483264291
Iteration 19900: Loss = -11169.949060242161
1
pi: tensor([[0.5802, 0.4198],
        [0.2831, 0.7169]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7907, 0.2093], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1949, 0.0946],
         [0.6757, 0.2871]],

        [[0.6423, 0.0965],
         [0.6797, 0.7250]],

        [[0.5139, 0.0956],
         [0.5881, 0.6915]],

        [[0.5823, 0.1027],
         [0.5935, 0.5484]],

        [[0.5401, 0.0986],
         [0.5228, 0.5982]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.0702861335289802
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824276204858761
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 3
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.488977909152464
Average Adjusted Rand Index: 0.7284811873040257
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23651.477815063303
Iteration 100: Loss = -11414.550133481896
Iteration 200: Loss = -11411.94017498191
Iteration 300: Loss = -11410.218839386896
Iteration 400: Loss = -11405.118459278003
Iteration 500: Loss = -11386.550957956813
Iteration 600: Loss = -11309.064583372412
Iteration 700: Loss = -11203.746021364363
Iteration 800: Loss = -11189.065671791726
Iteration 900: Loss = -11186.975633673503
Iteration 1000: Loss = -11186.48996550081
Iteration 1100: Loss = -11185.781759272428
Iteration 1200: Loss = -11183.701403110254
Iteration 1300: Loss = -11183.369388801604
Iteration 1400: Loss = -11183.240863424879
Iteration 1500: Loss = -11179.263265226153
Iteration 1600: Loss = -11176.813329359136
Iteration 1700: Loss = -11176.701962689676
Iteration 1800: Loss = -11176.675758044557
Iteration 1900: Loss = -11176.657682833611
Iteration 2000: Loss = -11176.639955625626
Iteration 2100: Loss = -11176.622042373801
Iteration 2200: Loss = -11176.607619062528
Iteration 2300: Loss = -11176.571135914079
Iteration 2400: Loss = -11176.550177547892
Iteration 2500: Loss = -11176.542154461022
Iteration 2600: Loss = -11176.534139551417
Iteration 2700: Loss = -11176.522801506482
Iteration 2800: Loss = -11176.496212382852
Iteration 2900: Loss = -11176.479749612394
Iteration 3000: Loss = -11176.404817709827
Iteration 3100: Loss = -11176.375730610336
Iteration 3200: Loss = -11176.366594623112
Iteration 3300: Loss = -11176.352180644793
Iteration 3400: Loss = -11176.326547048424
Iteration 3500: Loss = -11176.280480465566
Iteration 3600: Loss = -11173.674289014629
Iteration 3700: Loss = -11171.967006498278
Iteration 3800: Loss = -11170.893915860888
Iteration 3900: Loss = -11170.202002962515
Iteration 4000: Loss = -11170.181345438476
Iteration 4100: Loss = -11170.179852810046
Iteration 4200: Loss = -11170.171212287962
Iteration 4300: Loss = -11170.168816769235
Iteration 4400: Loss = -11170.165676737333
Iteration 4500: Loss = -11170.16317098499
Iteration 4600: Loss = -11170.160222384771
Iteration 4700: Loss = -11170.15257781401
Iteration 4800: Loss = -11170.147000219435
Iteration 4900: Loss = -11170.131817790652
Iteration 5000: Loss = -11170.083739961596
Iteration 5100: Loss = -11170.082070590523
Iteration 5200: Loss = -11170.08098806521
Iteration 5300: Loss = -11170.080333033984
Iteration 5400: Loss = -11170.07919796006
Iteration 5500: Loss = -11170.078493692914
Iteration 5600: Loss = -11170.077878345804
Iteration 5700: Loss = -11170.075801680681
Iteration 5800: Loss = -11170.069493477555
Iteration 5900: Loss = -11170.03378575971
Iteration 6000: Loss = -11170.02977280998
Iteration 6100: Loss = -11170.028738156372
Iteration 6200: Loss = -11170.026986011091
Iteration 6300: Loss = -11170.020271304676
Iteration 6400: Loss = -11170.021299374752
1
Iteration 6500: Loss = -11170.01816097315
Iteration 6600: Loss = -11170.017709880525
Iteration 6700: Loss = -11170.017336222798
Iteration 6800: Loss = -11170.016536256198
Iteration 6900: Loss = -11170.013874278544
Iteration 7000: Loss = -11170.01174430021
Iteration 7100: Loss = -11170.016761613106
1
Iteration 7200: Loss = -11170.010772535818
Iteration 7300: Loss = -11170.01272239295
1
Iteration 7400: Loss = -11169.967694819863
Iteration 7500: Loss = -11169.967279841965
Iteration 7600: Loss = -11169.961911322851
Iteration 7700: Loss = -11169.963562010984
1
Iteration 7800: Loss = -11169.961001694688
Iteration 7900: Loss = -11169.969006996274
1
Iteration 8000: Loss = -11169.959716677911
Iteration 8100: Loss = -11169.959151564382
Iteration 8200: Loss = -11169.96327280396
1
Iteration 8300: Loss = -11169.958468665793
Iteration 8400: Loss = -11169.956690797866
Iteration 8500: Loss = -11169.958228047912
1
Iteration 8600: Loss = -11169.955388382035
Iteration 8700: Loss = -11169.95566076255
1
Iteration 8800: Loss = -11169.955055929007
Iteration 8900: Loss = -11169.994427823554
1
Iteration 9000: Loss = -11169.954117231802
Iteration 9100: Loss = -11169.953530689654
Iteration 9200: Loss = -11169.952916769582
Iteration 9300: Loss = -11169.960428469667
1
Iteration 9400: Loss = -11169.987137343012
2
Iteration 9500: Loss = -11169.977159254136
3
Iteration 9600: Loss = -11169.952494954387
Iteration 9700: Loss = -11169.952190328444
Iteration 9800: Loss = -11169.951954231421
Iteration 9900: Loss = -11169.971346857885
1
Iteration 10000: Loss = -11169.951865054445
Iteration 10100: Loss = -11169.951876259007
Iteration 10200: Loss = -11170.041398789637
1
Iteration 10300: Loss = -11169.953921589733
2
Iteration 10400: Loss = -11169.952517048967
3
Iteration 10500: Loss = -11169.964860296703
4
Iteration 10600: Loss = -11169.952895790235
5
Iteration 10700: Loss = -11169.951960043267
Iteration 10800: Loss = -11169.951142382106
Iteration 10900: Loss = -11169.950812904905
Iteration 11000: Loss = -11169.984421652602
1
Iteration 11100: Loss = -11169.950383491296
Iteration 11200: Loss = -11169.953352650738
1
Iteration 11300: Loss = -11169.959455912856
2
Iteration 11400: Loss = -11169.974616253212
3
Iteration 11500: Loss = -11169.949687288925
Iteration 11600: Loss = -11169.954956993108
1
Iteration 11700: Loss = -11169.96953669324
2
Iteration 11800: Loss = -11169.95003483195
3
Iteration 11900: Loss = -11169.949431979136
Iteration 12000: Loss = -11170.014419742472
1
Iteration 12100: Loss = -11169.949341719808
Iteration 12200: Loss = -11169.960294527185
1
Iteration 12300: Loss = -11169.949247431776
Iteration 12400: Loss = -11169.953371289526
1
Iteration 12500: Loss = -11169.949249863928
Iteration 12600: Loss = -11169.951773671413
1
Iteration 12700: Loss = -11169.94923184235
Iteration 12800: Loss = -11170.020665911761
1
Iteration 12900: Loss = -11169.949234447748
Iteration 13000: Loss = -11170.074906443511
1
Iteration 13100: Loss = -11169.950291385589
2
Iteration 13200: Loss = -11169.949248272555
Iteration 13300: Loss = -11169.957203010224
1
Iteration 13400: Loss = -11169.950504335036
2
Iteration 13500: Loss = -11169.949450747617
3
Iteration 13600: Loss = -11169.951720236731
4
Iteration 13700: Loss = -11169.951775735422
5
Iteration 13800: Loss = -11170.005788536711
6
Iteration 13900: Loss = -11169.982774821674
7
Iteration 14000: Loss = -11169.949264774827
Iteration 14100: Loss = -11169.949320148213
Iteration 14200: Loss = -11169.950495668007
1
Iteration 14300: Loss = -11169.949026976992
Iteration 14400: Loss = -11169.958608230245
1
Iteration 14500: Loss = -11169.948730647613
Iteration 14600: Loss = -11169.949931119205
1
Iteration 14700: Loss = -11170.228382138372
2
Iteration 14800: Loss = -11169.949003439908
3
Iteration 14900: Loss = -11169.992229599506
4
Iteration 15000: Loss = -11169.948483320866
Iteration 15100: Loss = -11169.949206737305
1
Iteration 15200: Loss = -11170.081457447859
2
Iteration 15300: Loss = -11169.948417132093
Iteration 15400: Loss = -11169.976750449123
1
Iteration 15500: Loss = -11169.948370190477
Iteration 15600: Loss = -11169.983722348517
1
Iteration 15700: Loss = -11169.94836687972
Iteration 15800: Loss = -11169.951450353317
1
Iteration 15900: Loss = -11169.94856539013
2
Iteration 16000: Loss = -11169.950043004597
3
Iteration 16100: Loss = -11169.950323610203
4
Iteration 16200: Loss = -11170.030337276346
5
Iteration 16300: Loss = -11169.953629628042
6
Iteration 16400: Loss = -11169.954049935925
7
Iteration 16500: Loss = -11169.948648391039
8
Iteration 16600: Loss = -11169.951237061927
9
Iteration 16700: Loss = -11169.96722264885
10
Iteration 16800: Loss = -11169.950499252387
11
Iteration 16900: Loss = -11169.950107360253
12
Iteration 17000: Loss = -11169.954369896177
13
Iteration 17100: Loss = -11169.948566543988
14
Iteration 17200: Loss = -11169.948708696607
15
Stopping early at iteration 17200 due to no improvement.
pi: tensor([[0.5803, 0.4197],
        [0.2830, 0.7170]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7906, 0.2094], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1949, 0.0948],
         [0.7251, 0.2871]],

        [[0.5655, 0.0965],
         [0.7218, 0.5785]],

        [[0.5835, 0.0956],
         [0.6147, 0.5849]],

        [[0.7298, 0.1028],
         [0.6052, 0.5767]],

        [[0.5716, 0.0986],
         [0.5725, 0.6899]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.0702861335289802
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824276204858761
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 3
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.488977909152464
Average Adjusted Rand Index: 0.7284811873040257
11126.27711673985
[0.488977909152464, 0.488977909152464] [0.7284811873040257, 0.7284811873040257] [11169.948295032147, 11169.948708696607]
-------------------------------------
This iteration is 7
True Objective function: Loss = -11158.240635022208
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22668.268396639225
Iteration 100: Loss = -11403.54479646969
Iteration 200: Loss = -11401.98457258832
Iteration 300: Loss = -11400.092816420367
Iteration 400: Loss = -11391.584226661678
Iteration 500: Loss = -11375.115524067105
Iteration 600: Loss = -11288.72563769834
Iteration 700: Loss = -11208.976929558316
Iteration 800: Loss = -11166.472170370484
Iteration 900: Loss = -11156.402830344441
Iteration 1000: Loss = -11149.1943910894
Iteration 1100: Loss = -11147.684232240523
Iteration 1200: Loss = -11143.85672689378
Iteration 1300: Loss = -11136.84922184649
Iteration 1400: Loss = -11136.310291563412
Iteration 1500: Loss = -11131.794484300499
Iteration 1600: Loss = -11131.738765615628
Iteration 1700: Loss = -11131.699529335843
Iteration 1800: Loss = -11131.661508783092
Iteration 1900: Loss = -11131.584660179306
Iteration 2000: Loss = -11128.709861802658
Iteration 2100: Loss = -11128.656070277257
Iteration 2200: Loss = -11128.63420115088
Iteration 2300: Loss = -11128.621815910088
Iteration 2400: Loss = -11128.611804638229
Iteration 2500: Loss = -11128.603312650603
Iteration 2600: Loss = -11128.59516285706
Iteration 2700: Loss = -11128.585362325499
Iteration 2800: Loss = -11128.558120789665
Iteration 2900: Loss = -11128.516392169453
Iteration 3000: Loss = -11128.499173027249
Iteration 3100: Loss = -11128.496170776767
Iteration 3200: Loss = -11128.488694241498
Iteration 3300: Loss = -11128.48441341923
Iteration 3400: Loss = -11128.482833105185
Iteration 3500: Loss = -11128.480771953056
Iteration 3600: Loss = -11128.475872399453
Iteration 3700: Loss = -11128.473396093432
Iteration 3800: Loss = -11128.46845444744
Iteration 3900: Loss = -11128.469900960648
1
Iteration 4000: Loss = -11128.463997686564
Iteration 4100: Loss = -11128.462766947358
Iteration 4200: Loss = -11128.461603889004
Iteration 4300: Loss = -11128.461052710225
Iteration 4400: Loss = -11128.462250299108
1
Iteration 4500: Loss = -11128.45793086383
Iteration 4600: Loss = -11128.456440804015
Iteration 4700: Loss = -11128.454512785374
Iteration 4800: Loss = -11128.452482363993
Iteration 4900: Loss = -11128.449748951774
Iteration 5000: Loss = -11128.444326389605
Iteration 5100: Loss = -11128.439511804381
Iteration 5200: Loss = -11128.439145684824
Iteration 5300: Loss = -11128.438592535927
Iteration 5400: Loss = -11128.43791097255
Iteration 5500: Loss = -11128.44295930519
1
Iteration 5600: Loss = -11128.45083730838
2
Iteration 5700: Loss = -11128.435537802994
Iteration 5800: Loss = -11128.435124867217
Iteration 5900: Loss = -11128.429817717124
Iteration 6000: Loss = -11128.428081872718
Iteration 6100: Loss = -11128.427441725782
Iteration 6200: Loss = -11128.42701646477
Iteration 6300: Loss = -11128.426642600101
Iteration 6400: Loss = -11128.427639949025
1
Iteration 6500: Loss = -11128.425624503674
Iteration 6600: Loss = -11128.424814643578
Iteration 6700: Loss = -11128.424801650119
Iteration 6800: Loss = -11128.424357317828
Iteration 6900: Loss = -11128.424101084243
Iteration 7000: Loss = -11128.424774075398
1
Iteration 7100: Loss = -11128.422748104726
Iteration 7200: Loss = -11128.42176241391
Iteration 7300: Loss = -11128.424684365411
1
Iteration 7400: Loss = -11128.421384558049
Iteration 7500: Loss = -11128.423612422128
1
Iteration 7600: Loss = -11128.421154765823
Iteration 7700: Loss = -11128.421613841618
1
Iteration 7800: Loss = -11128.421280475986
2
Iteration 7900: Loss = -11128.426284294286
3
Iteration 8000: Loss = -11128.420999567803
Iteration 8100: Loss = -11128.421010177664
Iteration 8200: Loss = -11128.420790355616
Iteration 8300: Loss = -11128.42816215003
1
Iteration 8400: Loss = -11128.421516605313
2
Iteration 8500: Loss = -11128.420526485475
Iteration 8600: Loss = -11128.460378069598
1
Iteration 8700: Loss = -11128.439832091723
2
Iteration 8800: Loss = -11128.427126369479
3
Iteration 8900: Loss = -11128.42143980443
4
Iteration 9000: Loss = -11128.420568872558
Iteration 9100: Loss = -11128.420362716524
Iteration 9200: Loss = -11128.420839304414
1
Iteration 9300: Loss = -11128.439350271121
2
Iteration 9400: Loss = -11128.431943618134
3
Iteration 9500: Loss = -11128.461989992662
4
Iteration 9600: Loss = -11128.420199563181
Iteration 9700: Loss = -11128.420173903314
Iteration 9800: Loss = -11128.422435534665
1
Iteration 9900: Loss = -11128.475513800207
2
Iteration 10000: Loss = -11128.419990841598
Iteration 10100: Loss = -11128.422018034527
1
Iteration 10200: Loss = -11128.419942810104
Iteration 10300: Loss = -11128.420300932135
1
Iteration 10400: Loss = -11128.420060540015
2
Iteration 10500: Loss = -11128.419935828762
Iteration 10600: Loss = -11128.4234635801
1
Iteration 10700: Loss = -11128.41984323105
Iteration 10800: Loss = -11128.675068652137
1
Iteration 10900: Loss = -11128.41980522652
Iteration 11000: Loss = -11128.423743699914
1
Iteration 11100: Loss = -11128.422675096843
2
Iteration 11200: Loss = -11128.448027476155
3
Iteration 11300: Loss = -11128.42012043995
4
Iteration 11400: Loss = -11128.432654215672
5
Iteration 11500: Loss = -11128.41964708777
Iteration 11600: Loss = -11128.426980440683
1
Iteration 11700: Loss = -11128.418872849586
Iteration 11800: Loss = -11128.496701322934
1
Iteration 11900: Loss = -11128.413508813044
Iteration 12000: Loss = -11128.42134482416
1
Iteration 12100: Loss = -11128.413483788067
Iteration 12200: Loss = -11128.41484273362
1
Iteration 12300: Loss = -11128.414773721246
2
Iteration 12400: Loss = -11128.413564284512
Iteration 12500: Loss = -11128.413861274163
1
Iteration 12600: Loss = -11128.643919590766
2
Iteration 12700: Loss = -11128.413328820881
Iteration 12800: Loss = -11128.415090190421
1
Iteration 12900: Loss = -11128.593093448055
2
Iteration 13000: Loss = -11128.413161012088
Iteration 13100: Loss = -11128.440548367047
1
Iteration 13200: Loss = -11128.414707357002
2
Iteration 13300: Loss = -11128.416910911605
3
Iteration 13400: Loss = -11128.413190641268
Iteration 13500: Loss = -11128.413863796046
1
Iteration 13600: Loss = -11128.41316610338
Iteration 13700: Loss = -11128.416269062274
1
Iteration 13800: Loss = -11128.41846556328
2
Iteration 13900: Loss = -11128.417194156726
3
Iteration 14000: Loss = -11128.45311250426
4
Iteration 14100: Loss = -11128.420000252177
5
Iteration 14200: Loss = -11128.413078496467
Iteration 14300: Loss = -11128.416066629412
1
Iteration 14400: Loss = -11128.413038393086
Iteration 14500: Loss = -11128.43748725379
1
Iteration 14600: Loss = -11128.413065201776
Iteration 14700: Loss = -11128.413027180419
Iteration 14800: Loss = -11128.413909028019
1
Iteration 14900: Loss = -11128.413053365035
Iteration 15000: Loss = -11128.413046676085
Iteration 15100: Loss = -11128.414692448823
1
Iteration 15200: Loss = -11128.413064424738
Iteration 15300: Loss = -11128.413031051194
Iteration 15400: Loss = -11128.4135987221
1
Iteration 15500: Loss = -11128.497608333779
2
Iteration 15600: Loss = -11128.413494226397
3
Iteration 15700: Loss = -11128.413091948554
Iteration 15800: Loss = -11128.421379489757
1
Iteration 15900: Loss = -11128.413057747357
Iteration 16000: Loss = -11128.414602802995
1
Iteration 16100: Loss = -11128.413027258128
Iteration 16200: Loss = -11128.413469757628
1
Iteration 16300: Loss = -11128.412988713735
Iteration 16400: Loss = -11128.43012855032
1
Iteration 16500: Loss = -11128.41203216317
Iteration 16600: Loss = -11128.425231391873
1
Iteration 16700: Loss = -11128.42470836453
2
Iteration 16800: Loss = -11128.412065117182
Iteration 16900: Loss = -11128.468307740095
1
Iteration 17000: Loss = -11128.4211117037
2
Iteration 17100: Loss = -11128.413468406703
3
Iteration 17200: Loss = -11128.412058415068
Iteration 17300: Loss = -11128.413046074185
1
Iteration 17400: Loss = -11128.429828406763
2
Iteration 17500: Loss = -11128.502794228953
3
Iteration 17600: Loss = -11128.431905231024
4
Iteration 17700: Loss = -11128.412005674469
Iteration 17800: Loss = -11128.412515053362
1
Iteration 17900: Loss = -11128.412012237344
Iteration 18000: Loss = -11128.412025901904
Iteration 18100: Loss = -11128.57854223068
1
Iteration 18200: Loss = -11128.411987725698
Iteration 18300: Loss = -11128.474261098841
1
Iteration 18400: Loss = -11128.41201195639
Iteration 18500: Loss = -11128.419393827406
1
Iteration 18600: Loss = -11128.412655322578
2
Iteration 18700: Loss = -11128.428307299462
3
Iteration 18800: Loss = -11128.414560726887
4
Iteration 18900: Loss = -11128.412004333586
Iteration 19000: Loss = -11128.414184137155
1
Iteration 19100: Loss = -11128.412016315624
Iteration 19200: Loss = -11128.411979125929
Iteration 19300: Loss = -11128.412084847456
1
Iteration 19400: Loss = -11128.41193868826
Iteration 19500: Loss = -11128.413983004784
1
Iteration 19600: Loss = -11128.411955137486
Iteration 19700: Loss = -11128.412308401914
1
Iteration 19800: Loss = -11128.4264268153
2
Iteration 19900: Loss = -11128.41198960581
pi: tensor([[0.7209, 0.2791],
        [0.3002, 0.6998]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5102, 0.4898], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.0984],
         [0.6080, 0.3041]],

        [[0.7102, 0.0988],
         [0.6734, 0.5662]],

        [[0.6377, 0.1068],
         [0.5713, 0.5869]],

        [[0.5468, 0.1016],
         [0.5242, 0.5608]],

        [[0.5509, 0.0960],
         [0.7029, 0.5292]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448543354594036
Global Adjusted Rand Index: 0.9291543041741744
Average Adjusted Rand Index: 0.9292894766479612
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25293.41849759885
Iteration 100: Loss = -11403.082534957186
Iteration 200: Loss = -11401.619537658946
Iteration 300: Loss = -11400.011285444929
Iteration 400: Loss = -11397.8838999336
Iteration 500: Loss = -11396.473937867106
Iteration 600: Loss = -11395.779061869202
Iteration 700: Loss = -11395.369967336388
Iteration 800: Loss = -11395.044809167803
Iteration 900: Loss = -11394.701767464398
Iteration 1000: Loss = -11394.269971461496
Iteration 1100: Loss = -11393.427220647225
Iteration 1200: Loss = -11347.58434255297
Iteration 1300: Loss = -11347.077626435075
Iteration 1400: Loss = -11346.98952867968
Iteration 1500: Loss = -11346.98467430291
Iteration 1600: Loss = -11346.981787695493
Iteration 1700: Loss = -11346.97965713831
Iteration 1800: Loss = -11346.978037074041
Iteration 1900: Loss = -11346.976697082642
Iteration 2000: Loss = -11346.975478797
Iteration 2100: Loss = -11346.974626262116
Iteration 2200: Loss = -11346.973972564438
Iteration 2300: Loss = -11346.973346560828
Iteration 2400: Loss = -11346.972550838022
Iteration 2500: Loss = -11346.970448249365
Iteration 2600: Loss = -11346.956880050315
Iteration 2700: Loss = -11346.95653707394
Iteration 2800: Loss = -11346.956951717435
1
Iteration 2900: Loss = -11346.955921526189
Iteration 3000: Loss = -11346.95546042986
Iteration 3100: Loss = -11346.95574362417
1
Iteration 3200: Loss = -11346.952855247055
Iteration 3300: Loss = -11346.952003224204
Iteration 3400: Loss = -11346.951901950833
Iteration 3500: Loss = -11346.951841058886
Iteration 3600: Loss = -11346.951701861433
Iteration 3700: Loss = -11346.951625226295
Iteration 3800: Loss = -11346.951877068575
1
Iteration 3900: Loss = -11346.951522842204
Iteration 4000: Loss = -11346.951457603516
Iteration 4100: Loss = -11346.951412081917
Iteration 4200: Loss = -11346.951380013528
Iteration 4300: Loss = -11346.951310824874
Iteration 4400: Loss = -11346.951377056626
Iteration 4500: Loss = -11346.951248478414
Iteration 4600: Loss = -11346.951215908413
Iteration 4700: Loss = -11346.95125943356
Iteration 4800: Loss = -11346.951142809825
Iteration 4900: Loss = -11346.951150657
Iteration 5000: Loss = -11346.951143043732
Iteration 5100: Loss = -11346.95110685509
Iteration 5200: Loss = -11346.951054661344
Iteration 5300: Loss = -11346.951130890666
Iteration 5400: Loss = -11346.951068511524
Iteration 5500: Loss = -11346.951180746913
1
Iteration 5600: Loss = -11346.951031824206
Iteration 5700: Loss = -11346.951732107014
1
Iteration 5800: Loss = -11346.950989532805
Iteration 5900: Loss = -11346.957853850294
1
Iteration 6000: Loss = -11346.951036211236
Iteration 6100: Loss = -11346.952121471706
1
Iteration 6200: Loss = -11346.950994271312
Iteration 6300: Loss = -11346.952322088093
1
Iteration 6400: Loss = -11346.950907755863
Iteration 6500: Loss = -11346.950930686724
Iteration 6600: Loss = -11346.950926309284
Iteration 6700: Loss = -11346.953471825494
1
Iteration 6800: Loss = -11346.950888305864
Iteration 6900: Loss = -11346.951021812287
1
Iteration 7000: Loss = -11346.95089921321
Iteration 7100: Loss = -11346.951131857764
1
Iteration 7200: Loss = -11346.950995670386
Iteration 7300: Loss = -11346.950917988148
Iteration 7400: Loss = -11346.95101688986
Iteration 7500: Loss = -11346.954351178867
1
Iteration 7600: Loss = -11346.95155626599
2
Iteration 7700: Loss = -11346.952668444492
3
Iteration 7800: Loss = -11346.950835341639
Iteration 7900: Loss = -11346.951696729326
1
Iteration 8000: Loss = -11346.953007582777
2
Iteration 8100: Loss = -11346.950995711104
3
Iteration 8200: Loss = -11346.9540166714
4
Iteration 8300: Loss = -11346.954591054819
5
Iteration 8400: Loss = -11346.957782980287
6
Iteration 8500: Loss = -11346.950832722494
Iteration 8600: Loss = -11346.950904150364
Iteration 8700: Loss = -11346.951965971615
1
Iteration 8800: Loss = -11346.950820182044
Iteration 8900: Loss = -11346.95228108224
1
Iteration 9000: Loss = -11346.950802964931
Iteration 9100: Loss = -11346.950836142672
Iteration 9200: Loss = -11346.95097528433
1
Iteration 9300: Loss = -11346.950820733004
Iteration 9400: Loss = -11347.045922258416
1
Iteration 9500: Loss = -11346.950811552733
Iteration 9600: Loss = -11346.95080979667
Iteration 9700: Loss = -11346.952895176568
1
Iteration 9800: Loss = -11346.950816939197
Iteration 9900: Loss = -11346.960701289257
1
Iteration 10000: Loss = -11346.950798443084
Iteration 10100: Loss = -11346.9507874253
Iteration 10200: Loss = -11346.952354954467
1
Iteration 10300: Loss = -11346.950798989574
Iteration 10400: Loss = -11346.950783588789
Iteration 10500: Loss = -11346.950994097071
1
Iteration 10600: Loss = -11346.950795483683
Iteration 10700: Loss = -11346.950797677518
Iteration 10800: Loss = -11346.95382438977
1
Iteration 10900: Loss = -11346.950786410258
Iteration 11000: Loss = -11346.950796122528
Iteration 11100: Loss = -11346.954231708107
1
Iteration 11200: Loss = -11346.950776599175
Iteration 11300: Loss = -11346.950801187753
Iteration 11400: Loss = -11346.951160285314
1
Iteration 11500: Loss = -11346.950774456342
Iteration 11600: Loss = -11346.950784974522
Iteration 11700: Loss = -11346.951366466723
1
Iteration 11800: Loss = -11346.950780189283
Iteration 11900: Loss = -11346.950790004481
Iteration 12000: Loss = -11346.951080947403
1
Iteration 12100: Loss = -11346.950836369188
Iteration 12200: Loss = -11346.950854356672
Iteration 12300: Loss = -11346.950868389711
Iteration 12400: Loss = -11346.950795837347
Iteration 12500: Loss = -11347.00222391945
1
Iteration 12600: Loss = -11346.950795461471
Iteration 12700: Loss = -11346.950784885794
Iteration 12800: Loss = -11346.950888144098
1
Iteration 12900: Loss = -11346.950797987012
Iteration 13000: Loss = -11346.950800572922
Iteration 13100: Loss = -11346.950945867344
1
Iteration 13200: Loss = -11346.950810299531
Iteration 13300: Loss = -11346.950817601308
Iteration 13400: Loss = -11346.954657684362
1
Iteration 13500: Loss = -11346.950823904535
Iteration 13600: Loss = -11346.950779273056
Iteration 13700: Loss = -11346.983542648391
1
Iteration 13800: Loss = -11346.950780884015
Iteration 13900: Loss = -11347.275136348311
1
Iteration 14000: Loss = -11346.95080311468
Iteration 14100: Loss = -11346.950819556563
Iteration 14200: Loss = -11346.952052115621
1
Iteration 14300: Loss = -11346.950791098308
Iteration 14400: Loss = -11347.01382204976
1
Iteration 14500: Loss = -11346.950765836358
Iteration 14600: Loss = -11346.951018705591
1
Iteration 14700: Loss = -11346.95079046379
Iteration 14800: Loss = -11346.950882222103
Iteration 14900: Loss = -11346.950798165632
Iteration 15000: Loss = -11346.951048306975
1
Iteration 15100: Loss = -11346.950774181054
Iteration 15200: Loss = -11346.95164550408
1
Iteration 15300: Loss = -11346.950776246103
Iteration 15400: Loss = -11347.366949003776
1
Iteration 15500: Loss = -11346.950806103421
Iteration 15600: Loss = -11346.950807150093
Iteration 15700: Loss = -11346.951719664916
1
Iteration 15800: Loss = -11346.950798617958
Iteration 15900: Loss = -11346.95109585134
1
Iteration 16000: Loss = -11346.950839911671
Iteration 16100: Loss = -11346.95079324342
Iteration 16200: Loss = -11346.951002139054
1
Iteration 16300: Loss = -11346.952133876748
2
Iteration 16400: Loss = -11346.992977474967
3
Iteration 16500: Loss = -11346.950794419205
Iteration 16600: Loss = -11346.951873088257
1
Iteration 16700: Loss = -11346.950831974611
Iteration 16800: Loss = -11346.950797194413
Iteration 16900: Loss = -11346.960996112097
1
Iteration 17000: Loss = -11346.95081366566
Iteration 17100: Loss = -11346.983368606174
1
Iteration 17200: Loss = -11346.950792229061
Iteration 17300: Loss = -11346.994410570685
1
Iteration 17400: Loss = -11346.951298814627
2
Iteration 17500: Loss = -11346.950801774408
Iteration 17600: Loss = -11346.981075172798
1
Iteration 17700: Loss = -11346.950788884293
Iteration 17800: Loss = -11347.012557702416
1
Iteration 17900: Loss = -11347.008785480053
2
Iteration 18000: Loss = -11346.950874561653
Iteration 18100: Loss = -11346.950938208838
Iteration 18200: Loss = -11346.9509079868
Iteration 18300: Loss = -11346.951118206236
1
Iteration 18400: Loss = -11346.996916099879
2
Iteration 18500: Loss = -11346.950833483545
Iteration 18600: Loss = -11346.95799297226
1
Iteration 18700: Loss = -11346.9507831475
Iteration 18800: Loss = -11347.128658259462
1
Iteration 18900: Loss = -11346.950801090743
Iteration 19000: Loss = -11346.950780614032
Iteration 19100: Loss = -11346.951590211282
1
Iteration 19200: Loss = -11346.950794342289
Iteration 19300: Loss = -11347.181958798401
1
Iteration 19400: Loss = -11346.950785560151
Iteration 19500: Loss = -11346.950794442912
Iteration 19600: Loss = -11347.031431191892
1
Iteration 19700: Loss = -11346.950773509308
Iteration 19800: Loss = -11346.950787466423
Iteration 19900: Loss = -11346.95126254673
1
pi: tensor([[0.0456, 0.9544],
        [0.0278, 0.9722]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4881, 0.5119], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3038, 0.0994],
         [0.5695, 0.1742]],

        [[0.5190, 0.2486],
         [0.5453, 0.7248]],

        [[0.6909, 0.0717],
         [0.5500, 0.6248]],

        [[0.7106, 0.0760],
         [0.6497, 0.7175]],

        [[0.6865, 0.2715],
         [0.5328, 0.6427]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
Global Adjusted Rand Index: 0.04529541799814218
Average Adjusted Rand Index: 0.1843991979044205
11158.240635022208
[0.9291543041741744, 0.04529541799814218] [0.9292894766479612, 0.1843991979044205] [11128.41206922064, 11346.950808662396]
-------------------------------------
This iteration is 8
True Objective function: Loss = -11099.438229028518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24668.621847707374
Iteration 100: Loss = -11333.916822234698
Iteration 200: Loss = -11332.785039955745
Iteration 300: Loss = -11332.214437821089
Iteration 400: Loss = -11331.994158818887
Iteration 500: Loss = -11331.849334393046
Iteration 600: Loss = -11331.286134463915
Iteration 700: Loss = -11330.783423239372
Iteration 800: Loss = -11330.509014661024
Iteration 900: Loss = -11330.362031510045
Iteration 1000: Loss = -11330.263475939604
Iteration 1100: Loss = -11330.1944773193
Iteration 1200: Loss = -11330.14353661375
Iteration 1300: Loss = -11330.104353411045
Iteration 1400: Loss = -11330.072951124705
Iteration 1500: Loss = -11330.048004205319
Iteration 1600: Loss = -11330.027743394836
Iteration 1700: Loss = -11330.010816453547
Iteration 1800: Loss = -11329.996488063342
Iteration 1900: Loss = -11329.983982588645
Iteration 2000: Loss = -11329.972947160168
Iteration 2100: Loss = -11329.962992118815
Iteration 2200: Loss = -11329.954327649748
Iteration 2300: Loss = -11329.946948628818
Iteration 2400: Loss = -11329.940584591444
Iteration 2500: Loss = -11329.934865980129
Iteration 2600: Loss = -11329.929788294974
Iteration 2700: Loss = -11329.925224852906
Iteration 2800: Loss = -11329.921105646843
Iteration 2900: Loss = -11329.91731465649
Iteration 3000: Loss = -11329.913896566473
Iteration 3100: Loss = -11329.91092058832
Iteration 3200: Loss = -11329.908435633755
Iteration 3300: Loss = -11329.90631748245
Iteration 3400: Loss = -11329.904536069182
Iteration 3500: Loss = -11329.9029183746
Iteration 3600: Loss = -11329.901522957514
Iteration 3700: Loss = -11329.900299607309
Iteration 3800: Loss = -11329.899190410832
Iteration 3900: Loss = -11329.898185085287
Iteration 4000: Loss = -11329.897298035092
Iteration 4100: Loss = -11329.896477909762
Iteration 4200: Loss = -11329.895680308246
Iteration 4300: Loss = -11329.895014033425
Iteration 4400: Loss = -11329.89431170683
Iteration 4500: Loss = -11329.89373502738
Iteration 4600: Loss = -11329.893203388698
Iteration 4700: Loss = -11329.892670566776
Iteration 4800: Loss = -11329.892232997969
Iteration 4900: Loss = -11329.89184296208
Iteration 5000: Loss = -11329.891477518819
Iteration 5100: Loss = -11329.891098953465
Iteration 5200: Loss = -11329.890767927314
Iteration 5300: Loss = -11329.890428673887
Iteration 5400: Loss = -11329.89020518194
Iteration 5500: Loss = -11329.889913886225
Iteration 5600: Loss = -11329.889648598297
Iteration 5700: Loss = -11329.88940146552
Iteration 5800: Loss = -11329.889203693216
Iteration 5900: Loss = -11329.888973531404
Iteration 6000: Loss = -11329.888783279614
Iteration 6100: Loss = -11329.888580660176
Iteration 6200: Loss = -11329.888369790413
Iteration 6300: Loss = -11329.888258264928
Iteration 6400: Loss = -11329.888042826788
Iteration 6500: Loss = -11329.88788522685
Iteration 6600: Loss = -11329.887768425453
Iteration 6700: Loss = -11329.88761339953
Iteration 6800: Loss = -11329.887537956281
Iteration 6900: Loss = -11329.887379839029
Iteration 7000: Loss = -11329.887274392077
Iteration 7100: Loss = -11329.887168170811
Iteration 7200: Loss = -11329.88704833654
Iteration 7300: Loss = -11329.886960196556
Iteration 7400: Loss = -11329.88687643195
Iteration 7500: Loss = -11329.886780724917
Iteration 7600: Loss = -11329.886687350805
Iteration 7700: Loss = -11329.886635284656
Iteration 7800: Loss = -11329.886532550752
Iteration 7900: Loss = -11329.886543939863
Iteration 8000: Loss = -11329.886423380367
Iteration 8100: Loss = -11329.886543050125
1
Iteration 8200: Loss = -11329.886337615684
Iteration 8300: Loss = -11329.886245346534
Iteration 8400: Loss = -11329.90897447594
1
Iteration 8500: Loss = -11329.893380022806
2
Iteration 8600: Loss = -11329.89000606942
3
Iteration 8700: Loss = -11329.8976174071
4
Iteration 8800: Loss = -11329.887351890344
5
Iteration 8900: Loss = -11329.885990056473
Iteration 9000: Loss = -11329.90541142524
1
Iteration 9100: Loss = -11329.885902396667
Iteration 9200: Loss = -11329.896938168731
1
Iteration 9300: Loss = -11329.885793318475
Iteration 9400: Loss = -11329.88587829267
Iteration 9500: Loss = -11329.885882724131
Iteration 9600: Loss = -11329.885745010599
Iteration 9700: Loss = -11329.885794476975
Iteration 9800: Loss = -11329.885792320913
Iteration 9900: Loss = -11329.885696015028
Iteration 10000: Loss = -11329.885694724295
Iteration 10100: Loss = -11329.89300204512
1
Iteration 10200: Loss = -11329.88559376702
Iteration 10300: Loss = -11329.885552694275
Iteration 10400: Loss = -11329.885537766657
Iteration 10500: Loss = -11329.885509461677
Iteration 10600: Loss = -11329.885438114405
Iteration 10700: Loss = -11329.88546188085
Iteration 10800: Loss = -11329.888486972832
1
Iteration 10900: Loss = -11329.885410085642
Iteration 11000: Loss = -11329.885389284758
Iteration 11100: Loss = -11329.886193702761
1
Iteration 11200: Loss = -11329.885355603483
Iteration 11300: Loss = -11329.885384490544
Iteration 11400: Loss = -11329.885400335048
Iteration 11500: Loss = -11330.020061246707
1
Iteration 11600: Loss = -11329.885447968505
Iteration 11700: Loss = -11329.885419712662
Iteration 11800: Loss = -11329.890904445982
1
Iteration 11900: Loss = -11329.885481785537
Iteration 12000: Loss = -11329.885431143326
Iteration 12100: Loss = -11329.885427821953
Iteration 12200: Loss = -11329.886625326637
1
Iteration 12300: Loss = -11329.885406666504
Iteration 12400: Loss = -11329.88538678911
Iteration 12500: Loss = -11330.005503986533
1
Iteration 12600: Loss = -11329.885383815847
Iteration 12700: Loss = -11329.885354498268
Iteration 12800: Loss = -11329.885328800434
Iteration 12900: Loss = -11329.885343133172
Iteration 13000: Loss = -11329.885302155966
Iteration 13100: Loss = -11329.885348282693
Iteration 13200: Loss = -11330.013093430498
1
Iteration 13300: Loss = -11329.88528474097
Iteration 13400: Loss = -11329.88530523292
Iteration 13500: Loss = -11329.894697329879
1
Iteration 13600: Loss = -11329.885318286448
Iteration 13700: Loss = -11329.88527530444
Iteration 13800: Loss = -11329.885268872511
Iteration 13900: Loss = -11329.88880109655
1
Iteration 14000: Loss = -11329.885288315225
Iteration 14100: Loss = -11329.885254923121
Iteration 14200: Loss = -11329.907443859445
1
Iteration 14300: Loss = -11329.885243044777
Iteration 14400: Loss = -11329.885235400783
Iteration 14500: Loss = -11329.885241851374
Iteration 14600: Loss = -11329.885488053575
1
Iteration 14700: Loss = -11329.885229068268
Iteration 14800: Loss = -11329.885245627334
Iteration 14900: Loss = -11329.892391275665
1
Iteration 15000: Loss = -11329.885227962475
Iteration 15100: Loss = -11329.885203386097
Iteration 15200: Loss = -11330.014838399875
1
Iteration 15300: Loss = -11329.885216802342
Iteration 15400: Loss = -11329.885179481207
Iteration 15500: Loss = -11329.886051086867
1
Iteration 15600: Loss = -11329.885273731412
Iteration 15700: Loss = -11329.88518931076
Iteration 15800: Loss = -11329.885191603595
Iteration 15900: Loss = -11329.892794203768
1
Iteration 16000: Loss = -11329.88521229548
Iteration 16100: Loss = -11329.88520494292
Iteration 16200: Loss = -11329.978951512889
1
Iteration 16300: Loss = -11329.885202680796
Iteration 16400: Loss = -11329.885192815038
Iteration 16500: Loss = -11329.885193885752
Iteration 16600: Loss = -11329.8854989814
1
Iteration 16700: Loss = -11329.885187856271
Iteration 16800: Loss = -11329.885172405593
Iteration 16900: Loss = -11329.925625878937
1
Iteration 17000: Loss = -11329.885152865321
Iteration 17100: Loss = -11329.885187942016
Iteration 17200: Loss = -11330.043632902794
1
Iteration 17300: Loss = -11329.88515815996
Iteration 17400: Loss = -11329.885128506785
Iteration 17500: Loss = -11330.022855097102
1
Iteration 17600: Loss = -11329.885161658236
Iteration 17700: Loss = -11329.885158147592
Iteration 17800: Loss = -11329.899070126647
1
Iteration 17900: Loss = -11329.88514872474
Iteration 18000: Loss = -11329.885181556172
Iteration 18100: Loss = -11329.885322837783
1
Iteration 18200: Loss = -11329.88516870452
Iteration 18300: Loss = -11330.084714253371
1
Iteration 18400: Loss = -11329.885177351869
Iteration 18500: Loss = -11329.885168352053
Iteration 18600: Loss = -11329.901869054616
1
Iteration 18700: Loss = -11329.885177214705
Iteration 18800: Loss = -11329.885808770629
1
Iteration 18900: Loss = -11329.885172876373
Iteration 19000: Loss = -11330.267512310847
1
Iteration 19100: Loss = -11329.88516197525
Iteration 19200: Loss = -11329.88518663065
Iteration 19300: Loss = -11329.94983974278
1
Iteration 19400: Loss = -11329.885168336443
Iteration 19500: Loss = -11329.885186065554
Iteration 19600: Loss = -11329.900411090464
1
Iteration 19700: Loss = -11329.885190440298
Iteration 19800: Loss = -11329.885161366368
Iteration 19900: Loss = -11329.887189950454
1
pi: tensor([[1.0000e+00, 1.0037e-06],
        [5.7305e-03, 9.9427e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.5226e-09, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5005, 0.1574],
         [0.6447, 0.1698]],

        [[0.6440, 0.2276],
         [0.5969, 0.5167]],

        [[0.6342, 0.1549],
         [0.6965, 0.5152]],

        [[0.6854, 0.2522],
         [0.5913, 0.5023]],

        [[0.6515, 0.2582],
         [0.5618, 0.6848]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 0.0002834017387138837
Average Adjusted Rand Index: -0.00016701260812182435
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20812.65294327251
Iteration 100: Loss = -11333.17670950145
Iteration 200: Loss = -11332.818603498785
Iteration 300: Loss = -11332.648787537002
Iteration 400: Loss = -11332.425629997535
Iteration 500: Loss = -11332.109170544438
Iteration 600: Loss = -11331.753980283504
Iteration 700: Loss = -11331.30277491426
Iteration 800: Loss = -11330.610191366877
Iteration 900: Loss = -11329.626430044516
Iteration 1000: Loss = -11323.132457737856
Iteration 1100: Loss = -11291.73253077648
Iteration 1200: Loss = -11291.527282968664
Iteration 1300: Loss = -11291.463294004352
Iteration 1400: Loss = -11291.433353947057
Iteration 1500: Loss = -11291.416621072027
Iteration 1600: Loss = -11291.406326212364
Iteration 1700: Loss = -11291.399560975919
Iteration 1800: Loss = -11291.394806942302
Iteration 1900: Loss = -11291.39131653533
Iteration 2000: Loss = -11291.388630923375
Iteration 2100: Loss = -11291.386522063882
Iteration 2200: Loss = -11291.384841015008
Iteration 2300: Loss = -11291.383488125464
Iteration 2400: Loss = -11291.382393928414
Iteration 2500: Loss = -11291.38143289935
Iteration 2600: Loss = -11291.38061661973
Iteration 2700: Loss = -11291.379982103923
Iteration 2800: Loss = -11291.379396225951
Iteration 2900: Loss = -11291.378903275141
Iteration 3000: Loss = -11291.3784473164
Iteration 3100: Loss = -11291.378060697107
Iteration 3200: Loss = -11291.37772182511
Iteration 3300: Loss = -11291.377447676285
Iteration 3400: Loss = -11291.37720433135
Iteration 3500: Loss = -11291.37696086052
Iteration 3600: Loss = -11291.376751844216
Iteration 3700: Loss = -11291.376550663983
Iteration 3800: Loss = -11291.376370250986
Iteration 3900: Loss = -11291.376248249793
Iteration 4000: Loss = -11291.37611688086
Iteration 4100: Loss = -11291.37597862146
Iteration 4200: Loss = -11291.375882960745
Iteration 4300: Loss = -11291.375783366571
Iteration 4400: Loss = -11291.375703765058
Iteration 4500: Loss = -11291.37562302383
Iteration 4600: Loss = -11291.37553072846
Iteration 4700: Loss = -11291.375508099603
Iteration 4800: Loss = -11291.37540648441
Iteration 4900: Loss = -11291.37834647595
1
Iteration 5000: Loss = -11291.375310446261
Iteration 5100: Loss = -11291.375274505914
Iteration 5200: Loss = -11291.375313776196
Iteration 5300: Loss = -11291.375176853393
Iteration 5400: Loss = -11291.377120493651
1
Iteration 5500: Loss = -11291.375133566355
Iteration 5600: Loss = -11291.375082799603
Iteration 5700: Loss = -11291.375039974675
Iteration 5800: Loss = -11291.375028385377
Iteration 5900: Loss = -11291.375041976491
Iteration 6000: Loss = -11291.374988780637
Iteration 6100: Loss = -11291.374973815751
Iteration 6200: Loss = -11291.375474401037
1
Iteration 6300: Loss = -11291.374940342934
Iteration 6400: Loss = -11291.375005082928
Iteration 6500: Loss = -11291.374914377058
Iteration 6600: Loss = -11291.375473438464
1
Iteration 6700: Loss = -11291.376490965844
2
Iteration 6800: Loss = -11291.374871701555
Iteration 6900: Loss = -11291.374879825848
Iteration 7000: Loss = -11291.383041121464
1
Iteration 7100: Loss = -11291.37483144418
Iteration 7200: Loss = -11291.375026887066
1
Iteration 7300: Loss = -11291.402380101292
2
Iteration 7400: Loss = -11291.374899225713
Iteration 7500: Loss = -11291.374823543281
Iteration 7600: Loss = -11291.375014466385
1
Iteration 7700: Loss = -11291.375231122041
2
Iteration 7800: Loss = -11291.374839457447
Iteration 7900: Loss = -11291.37506562368
1
Iteration 8000: Loss = -11291.413142128442
2
Iteration 8100: Loss = -11291.374785215092
Iteration 8200: Loss = -11291.374809627978
Iteration 8300: Loss = -11291.374801206981
Iteration 8400: Loss = -11291.374784410222
Iteration 8500: Loss = -11291.389607976149
1
Iteration 8600: Loss = -11291.374793180645
Iteration 8700: Loss = -11291.37475935325
Iteration 8800: Loss = -11291.381738315886
1
Iteration 8900: Loss = -11291.374802933262
Iteration 9000: Loss = -11291.374767336401
Iteration 9100: Loss = -11291.401431900566
1
Iteration 9200: Loss = -11291.374764571558
Iteration 9300: Loss = -11291.37475871674
Iteration 9400: Loss = -11291.374914848959
1
Iteration 9500: Loss = -11291.374763171161
Iteration 9600: Loss = -11291.374773721409
Iteration 9700: Loss = -11291.692758461637
1
Iteration 9800: Loss = -11291.374772221627
Iteration 9900: Loss = -11291.374736475384
Iteration 10000: Loss = -11291.37640924005
1
Iteration 10100: Loss = -11291.374753984166
Iteration 10200: Loss = -11291.374777896428
Iteration 10300: Loss = -11291.374779156617
Iteration 10400: Loss = -11291.374756988409
Iteration 10500: Loss = -11291.601421047357
1
Iteration 10600: Loss = -11291.374759749064
Iteration 10700: Loss = -11291.378603901567
1
Iteration 10800: Loss = -11291.38622139504
2
Iteration 10900: Loss = -11291.37498001251
3
Iteration 11000: Loss = -11291.381228129892
4
Iteration 11100: Loss = -11291.388288805338
5
Iteration 11200: Loss = -11291.374811684658
Iteration 11300: Loss = -11291.375013880683
1
Iteration 11400: Loss = -11291.38088143245
2
Iteration 11500: Loss = -11291.377068728374
3
Iteration 11600: Loss = -11291.37935948915
4
Iteration 11700: Loss = -11291.387880199221
5
Iteration 11800: Loss = -11291.379534831873
6
Iteration 11900: Loss = -11291.37530394757
7
Iteration 12000: Loss = -11291.38061354185
8
Iteration 12100: Loss = -11291.380405457345
9
Iteration 12200: Loss = -11291.509113462227
10
Iteration 12300: Loss = -11291.375497142539
11
Iteration 12400: Loss = -11291.375149366599
12
Iteration 12500: Loss = -11291.3748637004
Iteration 12600: Loss = -11291.375805246365
1
Iteration 12700: Loss = -11291.377923517075
2
Iteration 12800: Loss = -11291.600163693241
3
Iteration 12900: Loss = -11291.374763120135
Iteration 13000: Loss = -11291.400153295346
1
Iteration 13100: Loss = -11291.374744021294
Iteration 13200: Loss = -11291.519087662191
1
Iteration 13300: Loss = -11291.37474717092
Iteration 13400: Loss = -11291.375156134
1
Iteration 13500: Loss = -11291.374808254177
Iteration 13600: Loss = -11291.378983330314
1
Iteration 13700: Loss = -11291.387223000811
2
Iteration 13800: Loss = -11291.394368860741
3
Iteration 13900: Loss = -11291.375961774118
4
Iteration 14000: Loss = -11291.386025407723
5
Iteration 14100: Loss = -11291.464021920596
6
Iteration 14200: Loss = -11291.423654249646
7
Iteration 14300: Loss = -11291.379710032323
8
Iteration 14400: Loss = -11291.375118473023
9
Iteration 14500: Loss = -11291.37549798786
10
Iteration 14600: Loss = -11291.380010125644
11
Iteration 14700: Loss = -11291.37737310691
12
Iteration 14800: Loss = -11291.376515218504
13
Iteration 14900: Loss = -11291.376004504067
14
Iteration 15000: Loss = -11291.375364120098
15
Stopping early at iteration 15000 due to no improvement.
pi: tensor([[0.9476, 0.0524],
        [0.8818, 0.1182]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5417, 0.4583], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1707, 0.1084],
         [0.5224, 0.3095]],

        [[0.7120, 0.2077],
         [0.6225, 0.5208]],

        [[0.7149, 0.1886],
         [0.6516, 0.5851]],

        [[0.6866, 0.0807],
         [0.6060, 0.5489]],

        [[0.5863, 0.2199],
         [0.6236, 0.5353]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.022328462449853457
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 0.06696442424750493
Average Adjusted Rand Index: 0.17218165483042283
11099.438229028518
[0.0002834017387138837, 0.06696442424750493] [-0.00016701260812182435, 0.17218165483042283] [11329.885187550837, 11291.375364120098]
-------------------------------------
This iteration is 9
True Objective function: Loss = -11282.234734599544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19637.02966931147
Iteration 100: Loss = -11576.849483120322
Iteration 200: Loss = -11570.6508136025
Iteration 300: Loss = -11555.831493506363
Iteration 400: Loss = -11500.090699847902
Iteration 500: Loss = -11476.29047755338
Iteration 600: Loss = -11271.082208583244
Iteration 700: Loss = -11252.920301867178
Iteration 800: Loss = -11251.1525240704
Iteration 900: Loss = -11250.87730806246
Iteration 1000: Loss = -11249.85552960929
Iteration 1100: Loss = -11249.747446646776
Iteration 1200: Loss = -11249.695068395034
Iteration 1300: Loss = -11249.659270128475
Iteration 1400: Loss = -11249.631140623007
Iteration 1500: Loss = -11249.596583386014
Iteration 1600: Loss = -11249.523223387345
Iteration 1700: Loss = -11249.478208125543
Iteration 1800: Loss = -11249.448329686273
Iteration 1900: Loss = -11249.437054325317
Iteration 2000: Loss = -11249.427422654064
Iteration 2100: Loss = -11249.417539421567
Iteration 2200: Loss = -11249.408244981714
Iteration 2300: Loss = -11249.401545617808
Iteration 2400: Loss = -11249.408249571967
1
Iteration 2500: Loss = -11249.38865819461
Iteration 2600: Loss = -11249.382900737925
Iteration 2700: Loss = -11249.392898723798
1
Iteration 2800: Loss = -11249.374598753378
Iteration 2900: Loss = -11249.36899949971
Iteration 3000: Loss = -11249.364973288857
Iteration 3100: Loss = -11249.360775629537
Iteration 3200: Loss = -11249.357697384818
Iteration 3300: Loss = -11249.355441180218
Iteration 3400: Loss = -11249.358151890496
1
Iteration 3500: Loss = -11249.345162874346
Iteration 3600: Loss = -11249.343594713397
Iteration 3700: Loss = -11249.34526846883
1
Iteration 3800: Loss = -11249.342378002757
Iteration 3900: Loss = -11249.340802622282
Iteration 4000: Loss = -11249.347699933433
1
Iteration 4100: Loss = -11249.338438478231
Iteration 4200: Loss = -11249.344172964298
1
Iteration 4300: Loss = -11249.336722635884
Iteration 4400: Loss = -11249.337080070318
1
Iteration 4500: Loss = -11249.33493938444
Iteration 4600: Loss = -11249.364840375523
1
Iteration 4700: Loss = -11249.333054494216
Iteration 4800: Loss = -11249.3315000774
Iteration 4900: Loss = -11249.332555291456
1
Iteration 5000: Loss = -11249.329866027556
Iteration 5100: Loss = -11249.329229566969
Iteration 5200: Loss = -11249.328870466918
Iteration 5300: Loss = -11249.328367007995
Iteration 5400: Loss = -11249.327942497162
Iteration 5500: Loss = -11249.330919287666
1
Iteration 5600: Loss = -11249.327314839291
Iteration 5700: Loss = -11249.329101823118
1
Iteration 5800: Loss = -11249.327779841618
2
Iteration 5900: Loss = -11249.32816923265
3
Iteration 6000: Loss = -11249.32650728784
Iteration 6100: Loss = -11249.326132022845
Iteration 6200: Loss = -11249.326819619284
1
Iteration 6300: Loss = -11249.328373155056
2
Iteration 6400: Loss = -11249.325392178089
Iteration 6500: Loss = -11249.325713431015
1
Iteration 6600: Loss = -11249.328468864533
2
Iteration 6700: Loss = -11249.324803761372
Iteration 6800: Loss = -11249.324717143472
Iteration 6900: Loss = -11249.322999378992
Iteration 7000: Loss = -11249.322943531608
Iteration 7100: Loss = -11249.322614967723
Iteration 7200: Loss = -11249.322612247382
Iteration 7300: Loss = -11249.329087678436
1
Iteration 7400: Loss = -11249.322193522765
Iteration 7500: Loss = -11249.322345576953
1
Iteration 7600: Loss = -11249.326117619865
2
Iteration 7700: Loss = -11249.32161451667
Iteration 7800: Loss = -11249.322010296946
1
Iteration 7900: Loss = -11249.318382010366
Iteration 8000: Loss = -11249.318207636361
Iteration 8100: Loss = -11249.31870112725
1
Iteration 8200: Loss = -11249.317926902022
Iteration 8300: Loss = -11249.330921828354
1
Iteration 8400: Loss = -11249.317674402631
Iteration 8500: Loss = -11249.317686057559
Iteration 8600: Loss = -11249.31933459893
1
Iteration 8700: Loss = -11249.317463759517
Iteration 8800: Loss = -11249.32259718814
1
Iteration 8900: Loss = -11249.31738924399
Iteration 9000: Loss = -11249.517103100117
1
Iteration 9100: Loss = -11249.31728019277
Iteration 9200: Loss = -11249.321665876041
1
Iteration 9300: Loss = -11249.316374049458
Iteration 9400: Loss = -11249.317852340058
1
Iteration 9500: Loss = -11249.352628635676
2
Iteration 9600: Loss = -11249.317546691509
3
Iteration 9700: Loss = -11249.709352573831
4
Iteration 9800: Loss = -11249.316055142835
Iteration 9900: Loss = -11249.31521431048
Iteration 10000: Loss = -11249.580120321472
1
Iteration 10100: Loss = -11249.312078337118
Iteration 10200: Loss = -11249.315548794078
1
Iteration 10300: Loss = -11249.312979040431
2
Iteration 10400: Loss = -11249.312361767501
3
Iteration 10500: Loss = -11249.312039841481
Iteration 10600: Loss = -11249.320592405804
1
Iteration 10700: Loss = -11249.31199340066
Iteration 10800: Loss = -11249.313539454628
1
Iteration 10900: Loss = -11249.311945544461
Iteration 11000: Loss = -11249.316654198401
1
Iteration 11100: Loss = -11249.31192252023
Iteration 11200: Loss = -11249.37245507122
1
Iteration 11300: Loss = -11249.311876117763
Iteration 11400: Loss = -11249.312220490945
1
Iteration 11500: Loss = -11249.311749612021
Iteration 11600: Loss = -11249.390645813699
1
Iteration 11700: Loss = -11249.311679019716
Iteration 11800: Loss = -11249.320158323992
1
Iteration 11900: Loss = -11249.311568639861
Iteration 12000: Loss = -11249.311924473848
1
Iteration 12100: Loss = -11249.315286926902
2
Iteration 12200: Loss = -11249.313853524589
3
Iteration 12300: Loss = -11249.311715459173
4
Iteration 12400: Loss = -11249.31171641942
5
Iteration 12500: Loss = -11249.312155979918
6
Iteration 12600: Loss = -11249.313751851056
7
Iteration 12700: Loss = -11249.314530458048
8
Iteration 12800: Loss = -11249.311505716696
Iteration 12900: Loss = -11249.31411311392
1
Iteration 13000: Loss = -11249.311618246684
2
Iteration 13100: Loss = -11249.331153631127
3
Iteration 13200: Loss = -11249.311492162868
Iteration 13300: Loss = -11249.311504625754
Iteration 13400: Loss = -11249.312112498506
1
Iteration 13500: Loss = -11249.311507275308
Iteration 13600: Loss = -11249.312610863937
1
Iteration 13700: Loss = -11249.311513625924
Iteration 13800: Loss = -11249.312320789175
1
Iteration 13900: Loss = -11249.311603184691
Iteration 14000: Loss = -11249.311719584555
1
Iteration 14100: Loss = -11249.311391889076
Iteration 14200: Loss = -11249.311531611467
1
Iteration 14300: Loss = -11249.311360987194
Iteration 14400: Loss = -11249.31926492968
1
Iteration 14500: Loss = -11249.32301282736
2
Iteration 14600: Loss = -11249.34606523393
3
Iteration 14700: Loss = -11249.311623179598
4
Iteration 14800: Loss = -11249.311402356285
Iteration 14900: Loss = -11249.311622812647
1
Iteration 15000: Loss = -11249.573540845817
2
Iteration 15100: Loss = -11249.311378904462
Iteration 15200: Loss = -11249.353508827562
1
Iteration 15300: Loss = -11249.311408722306
Iteration 15400: Loss = -11249.543379542058
1
Iteration 15500: Loss = -11249.311543108619
2
Iteration 15600: Loss = -11249.319294978908
3
Iteration 15700: Loss = -11249.311783175235
4
Iteration 15800: Loss = -11249.311663375593
5
Iteration 15900: Loss = -11249.560507563945
6
Iteration 16000: Loss = -11249.310847001305
Iteration 16100: Loss = -11249.384694579287
1
Iteration 16200: Loss = -11249.310792384353
Iteration 16300: Loss = -11249.311200039669
1
Iteration 16400: Loss = -11249.31077421901
Iteration 16500: Loss = -11249.311509181185
1
Iteration 16600: Loss = -11249.310804081722
Iteration 16700: Loss = -11249.310877173284
Iteration 16800: Loss = -11249.310900730467
Iteration 16900: Loss = -11249.310768200126
Iteration 17000: Loss = -11249.310760551629
Iteration 17100: Loss = -11249.328745301384
1
Iteration 17200: Loss = -11249.310743889848
Iteration 17300: Loss = -11249.310730000405
Iteration 17400: Loss = -11249.314460569736
1
Iteration 17500: Loss = -11249.311403688154
2
Iteration 17600: Loss = -11249.32790695385
3
Iteration 17700: Loss = -11249.310431907748
Iteration 17800: Loss = -11249.317792681668
1
Iteration 17900: Loss = -11249.310387106227
Iteration 18000: Loss = -11249.335131151052
1
Iteration 18100: Loss = -11249.310587760252
2
Iteration 18200: Loss = -11249.310386917128
Iteration 18300: Loss = -11249.340257983931
1
Iteration 18400: Loss = -11249.310294720746
Iteration 18500: Loss = -11249.34184996733
1
Iteration 18600: Loss = -11249.310302264328
Iteration 18700: Loss = -11249.346085073981
1
Iteration 18800: Loss = -11249.310293933011
Iteration 18900: Loss = -11249.310290267753
Iteration 19000: Loss = -11249.335529952026
1
Iteration 19100: Loss = -11249.310089985382
Iteration 19200: Loss = -11249.310082481283
Iteration 19300: Loss = -11249.311500290725
1
Iteration 19400: Loss = -11249.31007196375
Iteration 19500: Loss = -11249.310075364474
Iteration 19600: Loss = -11249.311362598444
1
Iteration 19700: Loss = -11249.31004427373
Iteration 19800: Loss = -11249.310234793878
1
Iteration 19900: Loss = -11249.311886364665
2
pi: tensor([[0.6921, 0.3079],
        [0.2830, 0.7170]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6015, 0.3985], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3068, 0.0957],
         [0.6118, 0.1958]],

        [[0.7285, 0.1076],
         [0.6134, 0.6179]],

        [[0.6745, 0.0981],
         [0.5268, 0.6187]],

        [[0.5807, 0.0992],
         [0.6290, 0.5748]],

        [[0.7044, 0.1026],
         [0.6896, 0.5019]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9206887570795217
time is 1
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9446733085256775
Average Adjusted Rand Index: 0.9446196932408041
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22340.61321863453
Iteration 100: Loss = -11577.808209054338
Iteration 200: Loss = -11576.598872654708
Iteration 300: Loss = -11574.021864514902
Iteration 400: Loss = -11560.10454725661
Iteration 500: Loss = -11534.390311162937
Iteration 600: Loss = -11378.299809634314
Iteration 700: Loss = -11286.587354976798
Iteration 800: Loss = -11257.072479796421
Iteration 900: Loss = -11252.162233550567
Iteration 1000: Loss = -11251.191695336829
Iteration 1100: Loss = -11250.967496711668
Iteration 1200: Loss = -11250.841002337756
Iteration 1300: Loss = -11250.754419960733
Iteration 1400: Loss = -11250.693529569664
Iteration 1500: Loss = -11250.59431416184
Iteration 1600: Loss = -11250.541882861758
Iteration 1700: Loss = -11250.509791721788
Iteration 1800: Loss = -11250.472722842587
Iteration 1900: Loss = -11250.057507859814
Iteration 2000: Loss = -11250.041668599435
Iteration 2100: Loss = -11250.029195767353
Iteration 2200: Loss = -11250.018077372102
Iteration 2300: Loss = -11250.007282391165
Iteration 2400: Loss = -11249.986187116183
Iteration 2500: Loss = -11249.942103425574
Iteration 2600: Loss = -11249.936046162013
Iteration 2700: Loss = -11249.929929301086
Iteration 2800: Loss = -11249.921440561851
Iteration 2900: Loss = -11249.541759300966
Iteration 3000: Loss = -11249.468058697768
Iteration 3100: Loss = -11249.463477629683
Iteration 3200: Loss = -11249.458279202101
Iteration 3300: Loss = -11249.440471976495
Iteration 3400: Loss = -11249.40273098289
Iteration 3500: Loss = -11249.400243520648
Iteration 3600: Loss = -11249.397121226564
Iteration 3700: Loss = -11249.394934147036
Iteration 3800: Loss = -11249.392437932205
Iteration 3900: Loss = -11249.367149024807
Iteration 4000: Loss = -11249.362320352304
Iteration 4100: Loss = -11249.361142642076
Iteration 4200: Loss = -11249.36157129417
1
Iteration 4300: Loss = -11249.357157560313
Iteration 4400: Loss = -11249.35580254061
Iteration 4500: Loss = -11249.356714066396
1
Iteration 4600: Loss = -11249.353199810956
Iteration 4700: Loss = -11249.362777053853
1
Iteration 4800: Loss = -11249.350884631876
Iteration 4900: Loss = -11249.348941468681
Iteration 5000: Loss = -11249.324553600161
Iteration 5100: Loss = -11249.324473365614
Iteration 5200: Loss = -11249.325235042
1
Iteration 5300: Loss = -11249.322577758065
Iteration 5400: Loss = -11249.323047686947
1
Iteration 5500: Loss = -11249.327640136524
2
Iteration 5600: Loss = -11249.321203987554
Iteration 5700: Loss = -11249.322484182769
1
Iteration 5800: Loss = -11249.320495613007
Iteration 5900: Loss = -11249.320452433052
Iteration 6000: Loss = -11249.320095513707
Iteration 6100: Loss = -11249.321107061323
1
Iteration 6200: Loss = -11249.319582583621
Iteration 6300: Loss = -11249.319119591204
Iteration 6400: Loss = -11249.320468887823
1
Iteration 6500: Loss = -11249.318470813378
Iteration 6600: Loss = -11249.321662946431
1
Iteration 6700: Loss = -11249.317949629456
Iteration 6800: Loss = -11249.317710823265
Iteration 6900: Loss = -11249.32787050028
1
Iteration 7000: Loss = -11249.317596850886
Iteration 7100: Loss = -11249.317903625533
1
Iteration 7200: Loss = -11249.31776372344
2
Iteration 7300: Loss = -11249.316861125608
Iteration 7400: Loss = -11249.316700042247
Iteration 7500: Loss = -11249.361742762969
1
Iteration 7600: Loss = -11249.316450492668
Iteration 7700: Loss = -11249.320400880775
1
Iteration 7800: Loss = -11249.316264491084
Iteration 7900: Loss = -11249.321998535832
1
Iteration 8000: Loss = -11249.316046420618
Iteration 8100: Loss = -11249.316984080397
1
Iteration 8200: Loss = -11249.315861975512
Iteration 8300: Loss = -11249.315793718615
Iteration 8400: Loss = -11249.316208801243
1
Iteration 8500: Loss = -11249.315642167276
Iteration 8600: Loss = -11249.315572416794
Iteration 8700: Loss = -11249.315418680158
Iteration 8800: Loss = -11249.319028680093
1
Iteration 8900: Loss = -11249.315249535664
Iteration 9000: Loss = -11249.330695905739
1
Iteration 9100: Loss = -11249.315171544587
Iteration 9200: Loss = -11249.315092059138
Iteration 9300: Loss = -11249.315217927915
1
Iteration 9400: Loss = -11249.315041161315
Iteration 9500: Loss = -11249.399762180057
1
Iteration 9600: Loss = -11249.314931094432
Iteration 9700: Loss = -11249.314825015092
Iteration 9800: Loss = -11249.31488160149
Iteration 9900: Loss = -11249.314311776494
Iteration 10000: Loss = -11249.314562360883
1
Iteration 10100: Loss = -11249.314352822934
Iteration 10200: Loss = -11249.314199553442
Iteration 10300: Loss = -11249.316676346245
1
Iteration 10400: Loss = -11249.314270153716
Iteration 10500: Loss = -11249.341287805719
1
Iteration 10600: Loss = -11249.314109040644
Iteration 10700: Loss = -11249.318191007627
1
Iteration 10800: Loss = -11249.314061948162
Iteration 10900: Loss = -11249.314112499049
Iteration 11000: Loss = -11249.313922661222
Iteration 11100: Loss = -11249.313130567934
Iteration 11200: Loss = -11249.31203908796
Iteration 11300: Loss = -11249.314569469132
1
Iteration 11400: Loss = -11249.311997522589
Iteration 11500: Loss = -11249.314941027857
1
Iteration 11600: Loss = -11249.311974096654
Iteration 11700: Loss = -11249.311969497448
Iteration 11800: Loss = -11249.311992360925
Iteration 11900: Loss = -11249.311825836608
Iteration 12000: Loss = -11249.31795130346
1
Iteration 12100: Loss = -11249.311775261298
Iteration 12200: Loss = -11249.311831980704
Iteration 12300: Loss = -11249.311626321505
Iteration 12400: Loss = -11249.311474961632
Iteration 12500: Loss = -11249.452458840455
1
Iteration 12600: Loss = -11249.311225198879
Iteration 12700: Loss = -11249.311224572213
Iteration 12800: Loss = -11249.877474499099
1
Iteration 12900: Loss = -11249.31119282965
Iteration 13000: Loss = -11249.311192482395
Iteration 13100: Loss = -11249.36829666437
1
Iteration 13200: Loss = -11249.311174735103
Iteration 13300: Loss = -11249.311161238258
Iteration 13400: Loss = -11249.311047644627
Iteration 13500: Loss = -11249.31094103705
Iteration 13600: Loss = -11249.310803939748
Iteration 13700: Loss = -11249.310804796582
Iteration 13800: Loss = -11249.311289057283
1
Iteration 13900: Loss = -11249.314673597959
2
Iteration 14000: Loss = -11249.312279458609
3
Iteration 14100: Loss = -11249.310861737053
Iteration 14200: Loss = -11249.310926125332
Iteration 14300: Loss = -11249.3125647255
1
Iteration 14400: Loss = -11249.311552846877
2
Iteration 14500: Loss = -11249.442069675366
3
Iteration 14600: Loss = -11249.3112246733
4
Iteration 14700: Loss = -11249.39350262887
5
Iteration 14800: Loss = -11249.312030674428
6
Iteration 14900: Loss = -11249.312025887468
7
Iteration 15000: Loss = -11249.31434878639
8
Iteration 15100: Loss = -11249.311541857869
9
Iteration 15200: Loss = -11249.383154392028
10
Iteration 15300: Loss = -11249.31061852161
Iteration 15400: Loss = -11249.403024995026
1
Iteration 15500: Loss = -11249.310649687703
Iteration 15600: Loss = -11249.326793795632
1
Iteration 15700: Loss = -11249.327677202053
2
Iteration 15800: Loss = -11249.31172380357
3
Iteration 15900: Loss = -11249.310997234888
4
Iteration 16000: Loss = -11249.310609274335
Iteration 16100: Loss = -11249.31510624723
1
Iteration 16200: Loss = -11249.32156676707
2
Iteration 16300: Loss = -11249.320186462759
3
Iteration 16400: Loss = -11249.310604375314
Iteration 16500: Loss = -11249.31201031797
1
Iteration 16600: Loss = -11249.356414633912
2
Iteration 16700: Loss = -11249.311324558468
3
Iteration 16800: Loss = -11249.314307930936
4
Iteration 16900: Loss = -11249.314761677579
5
Iteration 17000: Loss = -11249.310697768007
Iteration 17100: Loss = -11249.492821028904
1
Iteration 17200: Loss = -11249.310655825482
Iteration 17300: Loss = -11249.311304605186
1
Iteration 17400: Loss = -11249.31061029902
Iteration 17500: Loss = -11249.310905112907
1
Iteration 17600: Loss = -11249.438368319741
2
Iteration 17700: Loss = -11249.310691829156
Iteration 17800: Loss = -11249.38418607023
1
Iteration 17900: Loss = -11249.31059602279
Iteration 18000: Loss = -11249.312417162342
1
Iteration 18100: Loss = -11249.335754194877
2
Iteration 18200: Loss = -11249.310643717048
Iteration 18300: Loss = -11249.314043476745
1
Iteration 18400: Loss = -11249.322766150226
2
Iteration 18500: Loss = -11249.31060564403
Iteration 18600: Loss = -11249.321862240655
1
Iteration 18700: Loss = -11249.31061103427
Iteration 18800: Loss = -11249.311454997294
1
Iteration 18900: Loss = -11249.366919267195
2
Iteration 19000: Loss = -11249.311209223059
3
Iteration 19100: Loss = -11249.31272131822
4
Iteration 19200: Loss = -11249.40749900309
5
Iteration 19300: Loss = -11249.310606669571
Iteration 19400: Loss = -11249.311558386215
1
Iteration 19500: Loss = -11249.31041872683
Iteration 19600: Loss = -11249.310522617827
1
Iteration 19700: Loss = -11249.338996653594
2
Iteration 19800: Loss = -11249.309814861426
Iteration 19900: Loss = -11249.309867588718
pi: tensor([[0.7169, 0.2831],
        [0.3079, 0.6921]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3984, 0.6016], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1958, 0.0957],
         [0.6238, 0.3068]],

        [[0.6596, 0.1076],
         [0.5902, 0.6456]],

        [[0.5164, 0.0981],
         [0.6484, 0.5283]],

        [[0.5363, 0.0992],
         [0.6462, 0.6651]],

        [[0.7252, 0.1025],
         [0.6913, 0.5355]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9206887570795217
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9446733085256775
Average Adjusted Rand Index: 0.9446196932408041
11282.234734599544
[0.9446733085256775, 0.9446733085256775] [0.9446196932408041, 0.9446196932408041] [11249.310054452, 11249.309886582254]
-------------------------------------
This iteration is 10
True Objective function: Loss = -11109.97145267735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23260.65054641679
Iteration 100: Loss = -11414.302190524862
Iteration 200: Loss = -11412.087789268337
Iteration 300: Loss = -11406.714105916411
Iteration 400: Loss = -11399.546369862499
Iteration 500: Loss = -11340.244176752043
Iteration 600: Loss = -11135.989967234169
Iteration 700: Loss = -11099.399177734393
Iteration 800: Loss = -11097.784058899944
Iteration 900: Loss = -11093.046550097048
Iteration 1000: Loss = -11090.163535247866
Iteration 1100: Loss = -11089.978078527922
Iteration 1200: Loss = -11089.875405829729
Iteration 1300: Loss = -11089.672000647339
Iteration 1400: Loss = -11089.634475975034
Iteration 1500: Loss = -11089.60561261096
Iteration 1600: Loss = -11089.589886249574
Iteration 1700: Loss = -11089.563472621605
Iteration 1800: Loss = -11089.548665025506
Iteration 1900: Loss = -11089.536464790006
Iteration 2000: Loss = -11089.525895043857
Iteration 2100: Loss = -11089.515374983366
Iteration 2200: Loss = -11089.477894797474
Iteration 2300: Loss = -11089.417685024502
Iteration 2400: Loss = -11089.412160079819
Iteration 2500: Loss = -11089.407204431594
Iteration 2600: Loss = -11089.403175924184
Iteration 2700: Loss = -11089.39971453082
Iteration 2800: Loss = -11089.397282263155
Iteration 2900: Loss = -11089.393905023879
Iteration 3000: Loss = -11089.391173609776
Iteration 3100: Loss = -11089.388690234839
Iteration 3200: Loss = -11089.40130892034
1
Iteration 3300: Loss = -11089.384593302482
Iteration 3400: Loss = -11089.382773323281
Iteration 3500: Loss = -11089.381162434487
Iteration 3600: Loss = -11089.379198288994
Iteration 3700: Loss = -11089.377698279757
Iteration 3800: Loss = -11089.37627480966
Iteration 3900: Loss = -11089.374924360223
Iteration 4000: Loss = -11089.372767421746
Iteration 4100: Loss = -11089.325822632614
Iteration 4200: Loss = -11089.321216595827
Iteration 4300: Loss = -11089.3198949871
Iteration 4400: Loss = -11089.320700389599
1
Iteration 4500: Loss = -11089.309398217621
Iteration 4600: Loss = -11089.26754490317
Iteration 4700: Loss = -11089.267777538802
1
Iteration 4800: Loss = -11089.268241399757
2
Iteration 4900: Loss = -11089.264849525804
Iteration 5000: Loss = -11089.264360116596
Iteration 5100: Loss = -11089.263075512004
Iteration 5200: Loss = -11089.261345715046
Iteration 5300: Loss = -11089.260867521134
Iteration 5400: Loss = -11089.269640815137
1
Iteration 5500: Loss = -11089.259874778076
Iteration 5600: Loss = -11089.26216167958
1
Iteration 5700: Loss = -11089.259213612206
Iteration 5800: Loss = -11089.2595604202
1
Iteration 5900: Loss = -11089.258707660383
Iteration 6000: Loss = -11089.262576513276
1
Iteration 6100: Loss = -11089.258092209007
Iteration 6200: Loss = -11089.257794461746
Iteration 6300: Loss = -11089.257984826618
1
Iteration 6400: Loss = -11089.259811492539
2
Iteration 6500: Loss = -11089.254366115258
Iteration 6600: Loss = -11089.255514590592
1
Iteration 6700: Loss = -11089.25386946494
Iteration 6800: Loss = -11089.25383633558
Iteration 6900: Loss = -11089.253532327375
Iteration 7000: Loss = -11089.255000809197
1
Iteration 7100: Loss = -11089.25309626382
Iteration 7200: Loss = -11089.271723496106
1
Iteration 7300: Loss = -11089.234515569886
Iteration 7400: Loss = -11089.234598458788
Iteration 7500: Loss = -11089.236825733724
1
Iteration 7600: Loss = -11089.23552370135
2
Iteration 7700: Loss = -11089.234254445217
Iteration 7800: Loss = -11089.234129784067
Iteration 7900: Loss = -11089.233258074912
Iteration 8000: Loss = -11089.233364828526
1
Iteration 8100: Loss = -11089.233093686069
Iteration 8200: Loss = -11089.233494670147
1
Iteration 8300: Loss = -11089.233828007367
2
Iteration 8400: Loss = -11089.237309874292
3
Iteration 8500: Loss = -11089.232855466882
Iteration 8600: Loss = -11089.242013220804
1
Iteration 8700: Loss = -11089.23494076934
2
Iteration 8800: Loss = -11089.232695596176
Iteration 8900: Loss = -11089.300091730442
1
Iteration 9000: Loss = -11089.232524057928
Iteration 9100: Loss = -11089.233712369418
1
Iteration 9200: Loss = -11089.232443175604
Iteration 9300: Loss = -11089.232466085972
Iteration 9400: Loss = -11089.232301628015
Iteration 9500: Loss = -11089.232576594517
1
Iteration 9600: Loss = -11089.249975772127
2
Iteration 9700: Loss = -11089.324765677276
3
Iteration 9800: Loss = -11089.234976629945
4
Iteration 9900: Loss = -11089.232006470002
Iteration 10000: Loss = -11089.232021591812
Iteration 10100: Loss = -11089.23246781274
1
Iteration 10200: Loss = -11089.232697657273
2
Iteration 10300: Loss = -11089.234591062588
3
Iteration 10400: Loss = -11089.267663888233
4
Iteration 10500: Loss = -11089.340790709597
5
Iteration 10600: Loss = -11089.238485237667
6
Iteration 10700: Loss = -11089.230862057691
Iteration 10800: Loss = -11089.230761878725
Iteration 10900: Loss = -11089.242529118439
1
Iteration 11000: Loss = -11089.232329610732
2
Iteration 11100: Loss = -11089.231569030795
3
Iteration 11200: Loss = -11089.261379022662
4
Iteration 11300: Loss = -11089.234509521884
5
Iteration 11400: Loss = -11089.233386756754
6
Iteration 11500: Loss = -11089.23067989877
Iteration 11600: Loss = -11089.230902924473
1
Iteration 11700: Loss = -11089.231874329294
2
Iteration 11800: Loss = -11089.234541906491
3
Iteration 11900: Loss = -11089.242293044774
4
Iteration 12000: Loss = -11089.287961688544
5
Iteration 12100: Loss = -11089.236427230155
6
Iteration 12200: Loss = -11089.234406694366
7
Iteration 12300: Loss = -11089.2367295976
8
Iteration 12400: Loss = -11089.252608407454
9
Iteration 12500: Loss = -11089.230597634541
Iteration 12600: Loss = -11089.231056405166
1
Iteration 12700: Loss = -11089.231227125212
2
Iteration 12800: Loss = -11089.238990301645
3
Iteration 12900: Loss = -11089.231569714102
4
Iteration 13000: Loss = -11089.231307038406
5
Iteration 13100: Loss = -11089.23439937028
6
Iteration 13200: Loss = -11089.230734760069
7
Iteration 13300: Loss = -11089.230508202587
Iteration 13400: Loss = -11089.25907925544
1
Iteration 13500: Loss = -11089.247289035076
2
Iteration 13600: Loss = -11089.232410375313
3
Iteration 13700: Loss = -11089.261357167192
4
Iteration 13800: Loss = -11089.230533897233
Iteration 13900: Loss = -11089.23052505634
Iteration 14000: Loss = -11089.232648005416
1
Iteration 14100: Loss = -11089.232015310923
2
Iteration 14200: Loss = -11089.230645056288
3
Iteration 14300: Loss = -11089.239731691288
4
Iteration 14400: Loss = -11089.233052083293
5
Iteration 14500: Loss = -11089.23052103708
Iteration 14600: Loss = -11089.230694271531
1
Iteration 14700: Loss = -11089.30796339081
2
Iteration 14800: Loss = -11089.230399167503
Iteration 14900: Loss = -11089.23108341284
1
Iteration 15000: Loss = -11089.230490366039
Iteration 15100: Loss = -11089.231307425072
1
Iteration 15200: Loss = -11089.234765994725
2
Iteration 15300: Loss = -11089.23822118864
3
Iteration 15400: Loss = -11089.230985118777
4
Iteration 15500: Loss = -11089.2304895689
Iteration 15600: Loss = -11089.231447965682
1
Iteration 15700: Loss = -11089.2354248592
2
Iteration 15800: Loss = -11089.271779489112
3
Iteration 15900: Loss = -11089.232501889317
4
Iteration 16000: Loss = -11089.230696856665
5
Iteration 16100: Loss = -11089.230321447623
Iteration 16200: Loss = -11089.227915744887
Iteration 16300: Loss = -11089.227543991748
Iteration 16400: Loss = -11089.237089559332
1
Iteration 16500: Loss = -11089.230576543572
2
Iteration 16600: Loss = -11089.271444023776
3
Iteration 16700: Loss = -11089.22736271697
Iteration 16800: Loss = -11089.235480314752
1
Iteration 16900: Loss = -11089.235008258172
2
Iteration 17000: Loss = -11089.227302963882
Iteration 17100: Loss = -11089.227405032787
1
Iteration 17200: Loss = -11089.234082958816
2
Iteration 17300: Loss = -11089.227279320023
Iteration 17400: Loss = -11089.27634378538
1
Iteration 17500: Loss = -11089.22725976869
Iteration 17600: Loss = -11089.235872803534
1
Iteration 17700: Loss = -11089.229502532404
2
Iteration 17800: Loss = -11089.231441984397
3
Iteration 17900: Loss = -11089.237468866853
4
Iteration 18000: Loss = -11089.229191908353
5
Iteration 18100: Loss = -11089.228650166126
6
Iteration 18200: Loss = -11089.230428318713
7
Iteration 18300: Loss = -11089.22778757851
8
Iteration 18400: Loss = -11089.227346752225
Iteration 18500: Loss = -11089.230217739778
1
Iteration 18600: Loss = -11089.230387764941
2
Iteration 18700: Loss = -11089.22771071243
3
Iteration 18800: Loss = -11089.227306465495
Iteration 18900: Loss = -11089.246826697992
1
Iteration 19000: Loss = -11089.274103861486
2
Iteration 19100: Loss = -11089.227709273951
3
Iteration 19200: Loss = -11089.22736852532
Iteration 19300: Loss = -11089.236019903528
1
Iteration 19400: Loss = -11089.444811750403
2
Iteration 19500: Loss = -11089.227311544768
Iteration 19600: Loss = -11089.236812485391
1
Iteration 19700: Loss = -11089.227264373052
Iteration 19800: Loss = -11089.229306871019
1
Iteration 19900: Loss = -11089.227629311632
2
pi: tensor([[0.7522, 0.2478],
        [0.2499, 0.7501]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5513, 0.4487], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1921, 0.1000],
         [0.5893, 0.3138]],

        [[0.6945, 0.1002],
         [0.7190, 0.5482]],

        [[0.7171, 0.0913],
         [0.5000, 0.6395]],

        [[0.5262, 0.1041],
         [0.6546, 0.6503]],

        [[0.5466, 0.1042],
         [0.7062, 0.6437]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080682750429576
time is 4
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368977891035916
Average Adjusted Rand Index: 0.9376130281052909
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25808.690822523447
Iteration 100: Loss = -11416.29502853405
Iteration 200: Loss = -11414.062129346452
Iteration 300: Loss = -11412.98026098197
Iteration 400: Loss = -11410.702557742612
Iteration 500: Loss = -11404.29964335474
Iteration 600: Loss = -11399.32456087055
Iteration 700: Loss = -11355.434413024035
Iteration 800: Loss = -11167.326636120784
Iteration 900: Loss = -11142.919376946016
Iteration 1000: Loss = -11137.624385870175
Iteration 1100: Loss = -11127.441569169676
Iteration 1200: Loss = -11127.38323943734
Iteration 1300: Loss = -11127.360586509118
Iteration 1400: Loss = -11124.5027860552
Iteration 1500: Loss = -11124.463905120427
Iteration 1600: Loss = -11124.451909404463
Iteration 1700: Loss = -11124.386291752495
Iteration 1800: Loss = -11124.319998379417
Iteration 1900: Loss = -11124.31720868419
Iteration 2000: Loss = -11124.313721297207
Iteration 2100: Loss = -11124.31002097488
Iteration 2200: Loss = -11124.303270668306
Iteration 2300: Loss = -11124.271519502365
Iteration 2400: Loss = -11114.662668529603
Iteration 2500: Loss = -11114.601546150712
Iteration 2600: Loss = -11114.590002660598
Iteration 2700: Loss = -11114.586846368216
Iteration 2800: Loss = -11114.584037849958
Iteration 2900: Loss = -11114.582158589557
Iteration 3000: Loss = -11114.580984310533
Iteration 3100: Loss = -11114.580378555922
Iteration 3200: Loss = -11114.584912550172
1
Iteration 3300: Loss = -11114.579415960188
Iteration 3400: Loss = -11114.578962720541
Iteration 3500: Loss = -11114.578426769094
Iteration 3600: Loss = -11113.398291209176
Iteration 3700: Loss = -11111.05968992512
Iteration 3800: Loss = -11111.059914620933
1
Iteration 3900: Loss = -11111.05901667336
Iteration 4000: Loss = -11111.059017532765
Iteration 4100: Loss = -11111.057361334362
Iteration 4200: Loss = -11111.056829980847
Iteration 4300: Loss = -11111.056373341042
Iteration 4400: Loss = -11111.055844184359
Iteration 4500: Loss = -11098.876970042531
Iteration 4600: Loss = -11098.383239448522
Iteration 4700: Loss = -11098.391005279987
1
Iteration 4800: Loss = -11098.381373323022
Iteration 4900: Loss = -11098.381136696245
Iteration 5000: Loss = -11098.382614066446
1
Iteration 5100: Loss = -11098.380693949843
Iteration 5200: Loss = -11098.381987703113
1
Iteration 5300: Loss = -11098.383217137838
2
Iteration 5400: Loss = -11098.380092583822
Iteration 5500: Loss = -11098.380019268081
Iteration 5600: Loss = -11098.380875558676
1
Iteration 5700: Loss = -11098.379756388676
Iteration 5800: Loss = -11098.380172549185
1
Iteration 5900: Loss = -11098.3795882531
Iteration 6000: Loss = -11098.396254039184
1
Iteration 6100: Loss = -11098.379439810904
Iteration 6200: Loss = -11098.379644502615
1
Iteration 6300: Loss = -11098.379247047882
Iteration 6400: Loss = -11098.378865651574
Iteration 6500: Loss = -11098.351585520893
Iteration 6600: Loss = -11098.343331847354
Iteration 6700: Loss = -11098.328808634216
Iteration 6800: Loss = -11098.326940400964
Iteration 6900: Loss = -11098.32368744033
Iteration 7000: Loss = -11098.323667059882
Iteration 7100: Loss = -11098.32360192648
Iteration 7200: Loss = -11098.323535711641
Iteration 7300: Loss = -11098.323465681973
Iteration 7400: Loss = -11098.32346157271
Iteration 7500: Loss = -11098.32334664093
Iteration 7600: Loss = -11098.318601980196
Iteration 7700: Loss = -11098.318266376247
Iteration 7800: Loss = -11098.318653145889
1
Iteration 7900: Loss = -11098.313848771144
Iteration 8000: Loss = -11098.313943607289
Iteration 8100: Loss = -11098.323232501043
1
Iteration 8200: Loss = -11098.31519837548
2
Iteration 8300: Loss = -11098.315103484647
3
Iteration 8400: Loss = -11098.313831800566
Iteration 8500: Loss = -11098.316324819703
1
Iteration 8600: Loss = -11098.31427426448
2
Iteration 8700: Loss = -11098.315737915716
3
Iteration 8800: Loss = -11098.320136139682
4
Iteration 8900: Loss = -11098.319713619208
5
Iteration 9000: Loss = -11098.312764916252
Iteration 9100: Loss = -11098.3126891348
Iteration 9200: Loss = -11098.313348444502
1
Iteration 9300: Loss = -11098.315712221798
2
Iteration 9400: Loss = -11098.3118233308
Iteration 9500: Loss = -11098.314791513267
1
Iteration 9600: Loss = -11098.315987900667
2
Iteration 9700: Loss = -11098.330724905647
3
Iteration 9800: Loss = -11098.318931664431
4
Iteration 9900: Loss = -11098.365980434945
5
Iteration 10000: Loss = -11098.313122198304
6
Iteration 10100: Loss = -11098.311774990827
Iteration 10200: Loss = -11098.31563354637
1
Iteration 10300: Loss = -11098.31405223624
2
Iteration 10400: Loss = -11098.31201142123
3
Iteration 10500: Loss = -11098.31172838962
Iteration 10600: Loss = -11098.312019561105
1
Iteration 10700: Loss = -11098.514871062991
2
Iteration 10800: Loss = -11089.321225975955
Iteration 10900: Loss = -11089.345441838692
1
Iteration 11000: Loss = -11089.32042043488
Iteration 11100: Loss = -11089.321373461358
1
Iteration 11200: Loss = -11089.321736842607
2
Iteration 11300: Loss = -11089.320220125444
Iteration 11400: Loss = -11089.320422361649
1
Iteration 11500: Loss = -11089.3232477742
2
Iteration 11600: Loss = -11089.320650451322
3
Iteration 11700: Loss = -11089.320454903136
4
Iteration 11800: Loss = -11089.321410372622
5
Iteration 11900: Loss = -11089.320397480553
6
Iteration 12000: Loss = -11089.321626498828
7
Iteration 12100: Loss = -11089.323655096714
8
Iteration 12200: Loss = -11089.3431974114
9
Iteration 12300: Loss = -11089.320260130913
Iteration 12400: Loss = -11089.320552839286
1
Iteration 12500: Loss = -11089.325752624363
2
Iteration 12600: Loss = -11089.327012580521
3
Iteration 12700: Loss = -11089.320240233834
Iteration 12800: Loss = -11089.320472879403
1
Iteration 12900: Loss = -11089.44989916144
2
Iteration 13000: Loss = -11089.32041861182
3
Iteration 13100: Loss = -11089.31822771952
Iteration 13200: Loss = -11089.307970633974
Iteration 13300: Loss = -11089.304950830461
Iteration 13400: Loss = -11089.306302797357
1
Iteration 13500: Loss = -11089.335510593193
2
Iteration 13600: Loss = -11089.304702291327
Iteration 13700: Loss = -11089.304918311804
1
Iteration 13800: Loss = -11089.325731138228
2
Iteration 13900: Loss = -11089.315578430453
3
Iteration 14000: Loss = -11089.311603207492
4
Iteration 14100: Loss = -11089.304381263782
Iteration 14200: Loss = -11089.306574503345
1
Iteration 14300: Loss = -11089.304326158137
Iteration 14400: Loss = -11089.403261145748
1
Iteration 14500: Loss = -11089.455972270996
2
Iteration 14600: Loss = -11089.297585689903
Iteration 14700: Loss = -11089.299806531275
1
Iteration 14800: Loss = -11089.297403278446
Iteration 14900: Loss = -11089.339656921164
1
Iteration 15000: Loss = -11089.297420725537
Iteration 15100: Loss = -11089.297331646685
Iteration 15200: Loss = -11089.297502717665
1
Iteration 15300: Loss = -11089.290594081613
Iteration 15400: Loss = -11089.291015076655
1
Iteration 15500: Loss = -11089.290908034605
2
Iteration 15600: Loss = -11089.291890252684
3
Iteration 15700: Loss = -11089.289207663303
Iteration 15800: Loss = -11089.301100048799
1
Iteration 15900: Loss = -11089.420573384232
2
Iteration 16000: Loss = -11089.303156740623
3
Iteration 16100: Loss = -11089.292612108346
4
Iteration 16200: Loss = -11089.28887167969
Iteration 16300: Loss = -11089.288970580707
Iteration 16400: Loss = -11089.29253329955
1
Iteration 16500: Loss = -11089.288798175434
Iteration 16600: Loss = -11089.289085719409
1
Iteration 16700: Loss = -11089.288789026361
Iteration 16800: Loss = -11089.289262562083
1
Iteration 16900: Loss = -11089.288777794947
Iteration 17000: Loss = -11089.291262951601
1
Iteration 17100: Loss = -11089.420537122347
2
Iteration 17200: Loss = -11089.296506402356
3
Iteration 17300: Loss = -11089.2842283909
Iteration 17400: Loss = -11089.28415642266
Iteration 17500: Loss = -11089.28864189072
1
Iteration 17600: Loss = -11089.321668933575
2
Iteration 17700: Loss = -11089.305002991048
3
Iteration 17800: Loss = -11089.28392957488
Iteration 17900: Loss = -11089.283897163934
Iteration 18000: Loss = -11089.321231385034
1
Iteration 18100: Loss = -11089.289510529716
2
Iteration 18200: Loss = -11089.283915344648
Iteration 18300: Loss = -11089.284665428611
1
Iteration 18400: Loss = -11089.284137325049
2
Iteration 18500: Loss = -11089.285216630551
3
Iteration 18600: Loss = -11089.295377941522
4
Iteration 18700: Loss = -11089.283696046847
Iteration 18800: Loss = -11089.283694198346
Iteration 18900: Loss = -11089.285115338405
1
Iteration 19000: Loss = -11089.2839729964
2
Iteration 19100: Loss = -11089.283871728065
3
Iteration 19200: Loss = -11089.285557250778
4
Iteration 19300: Loss = -11089.283546665001
Iteration 19400: Loss = -11089.287824262234
1
Iteration 19500: Loss = -11089.288739454485
2
Iteration 19600: Loss = -11089.28364134299
Iteration 19700: Loss = -11089.28360136337
Iteration 19800: Loss = -11089.283661496873
Iteration 19900: Loss = -11089.283084671939
pi: tensor([[0.7511, 0.2489],
        [0.2495, 0.7505]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5513, 0.4487], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1919, 0.0999],
         [0.5441, 0.3141]],

        [[0.6564, 0.1000],
         [0.6265, 0.5132]],

        [[0.6679, 0.0913],
         [0.7088, 0.6773]],

        [[0.5491, 0.1044],
         [0.5133, 0.5709]],

        [[0.6837, 0.1045],
         [0.5947, 0.5072]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080682750429576
time is 4
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368977891035916
Average Adjusted Rand Index: 0.9376130281052909
11109.97145267735
[0.9368977891035916, 0.9368977891035916] [0.9376130281052909, 0.9376130281052909] [11089.238415029238, 11089.28782708213]
-------------------------------------
This iteration is 11
True Objective function: Loss = -10855.009497567185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23764.088473739994
Iteration 100: Loss = -10886.183557465025
Iteration 200: Loss = -10880.661514967871
Iteration 300: Loss = -10879.960959830818
Iteration 400: Loss = -10879.288213866377
Iteration 500: Loss = -10839.088646989634
Iteration 600: Loss = -10822.161448448733
Iteration 700: Loss = -10822.041279274783
Iteration 800: Loss = -10822.013133848011
Iteration 900: Loss = -10821.998078020895
Iteration 1000: Loss = -10821.988597654461
Iteration 1100: Loss = -10821.982102234771
Iteration 1200: Loss = -10821.97742993901
Iteration 1300: Loss = -10821.973959138491
Iteration 1400: Loss = -10821.971272567635
Iteration 1500: Loss = -10821.969125164173
Iteration 1600: Loss = -10821.967368326372
Iteration 1700: Loss = -10821.965872566849
Iteration 1800: Loss = -10821.96458971619
Iteration 1900: Loss = -10821.968993331951
1
Iteration 2000: Loss = -10821.962226965106
Iteration 2100: Loss = -10821.961044424292
Iteration 2200: Loss = -10821.959866117939
Iteration 2300: Loss = -10821.959129275121
Iteration 2400: Loss = -10821.958432357764
Iteration 2500: Loss = -10821.957961085887
Iteration 2600: Loss = -10821.96174546476
1
Iteration 2700: Loss = -10821.957233522677
Iteration 2800: Loss = -10821.957443574014
1
Iteration 2900: Loss = -10821.956723765037
Iteration 3000: Loss = -10821.956671183641
Iteration 3100: Loss = -10821.956582742645
Iteration 3200: Loss = -10821.956039168703
Iteration 3300: Loss = -10821.956176262673
1
Iteration 3400: Loss = -10821.956106284922
Iteration 3500: Loss = -10821.955626709614
Iteration 3600: Loss = -10821.955993024772
1
Iteration 3700: Loss = -10821.956718825755
2
Iteration 3800: Loss = -10821.955302810484
Iteration 3900: Loss = -10821.963461415737
1
Iteration 4000: Loss = -10821.955352937795
Iteration 4100: Loss = -10821.955025174078
Iteration 4200: Loss = -10821.955024837005
Iteration 4300: Loss = -10821.954901336927
Iteration 4400: Loss = -10821.954852680718
Iteration 4500: Loss = -10821.954797417007
Iteration 4600: Loss = -10821.954720147563
Iteration 4700: Loss = -10821.954695652506
Iteration 4800: Loss = -10821.954721204322
Iteration 4900: Loss = -10821.95705302898
1
Iteration 5000: Loss = -10821.954675377781
Iteration 5100: Loss = -10821.95454038399
Iteration 5200: Loss = -10821.954575383486
Iteration 5300: Loss = -10821.956389446199
1
Iteration 5400: Loss = -10821.962632451734
2
Iteration 5500: Loss = -10821.95743418984
3
Iteration 5600: Loss = -10821.956442856426
4
Iteration 5700: Loss = -10821.954403263837
Iteration 5800: Loss = -10821.954521662437
1
Iteration 5900: Loss = -10821.954322993579
Iteration 6000: Loss = -10821.954295829548
Iteration 6100: Loss = -10821.9556973676
1
Iteration 6200: Loss = -10821.954318360535
Iteration 6300: Loss = -10821.956066118879
1
Iteration 6400: Loss = -10821.956429944506
2
Iteration 6500: Loss = -10821.954251220814
Iteration 6600: Loss = -10821.954255714389
Iteration 6700: Loss = -10821.954828327822
1
Iteration 6800: Loss = -10821.954383795679
2
Iteration 6900: Loss = -10821.956362351397
3
Iteration 7000: Loss = -10821.954230515224
Iteration 7100: Loss = -10821.954168812912
Iteration 7200: Loss = -10821.954181610223
Iteration 7300: Loss = -10821.954223798954
Iteration 7400: Loss = -10821.954162019549
Iteration 7500: Loss = -10821.95412052571
Iteration 7600: Loss = -10821.954247783022
1
Iteration 7700: Loss = -10821.954543277132
2
Iteration 7800: Loss = -10821.954183756874
Iteration 7900: Loss = -10821.954124213735
Iteration 8000: Loss = -10821.95411940179
Iteration 8100: Loss = -10821.96570278197
1
Iteration 8200: Loss = -10821.954082309809
Iteration 8300: Loss = -10821.9556120162
1
Iteration 8400: Loss = -10821.958819085728
2
Iteration 8500: Loss = -10821.954913637765
3
Iteration 8600: Loss = -10821.954094230246
Iteration 8700: Loss = -10821.955017838685
1
Iteration 8800: Loss = -10821.954055606358
Iteration 8900: Loss = -10821.954070713626
Iteration 9000: Loss = -10821.980040164404
1
Iteration 9100: Loss = -10821.954103999946
Iteration 9200: Loss = -10821.954077346609
Iteration 9300: Loss = -10821.95904775872
1
Iteration 9400: Loss = -10821.954048226717
Iteration 9500: Loss = -10821.954086884618
Iteration 9600: Loss = -10821.954094036062
Iteration 9700: Loss = -10821.95409267575
Iteration 9800: Loss = -10821.954039055616
Iteration 9900: Loss = -10821.95420511668
1
Iteration 10000: Loss = -10821.954052824673
Iteration 10100: Loss = -10821.95768683402
1
Iteration 10200: Loss = -10821.954027569678
Iteration 10300: Loss = -10821.954018035118
Iteration 10400: Loss = -10821.95508817947
1
Iteration 10500: Loss = -10821.95401883284
Iteration 10600: Loss = -10822.09155032246
1
Iteration 10700: Loss = -10821.954035314986
Iteration 10800: Loss = -10821.954284126661
1
Iteration 10900: Loss = -10822.220944405588
2
Iteration 11000: Loss = -10821.954027354876
Iteration 11100: Loss = -10822.27730655568
1
Iteration 11200: Loss = -10821.954021820826
Iteration 11300: Loss = -10821.976740795535
1
Iteration 11400: Loss = -10821.954679870238
2
Iteration 11500: Loss = -10821.954036939538
Iteration 11600: Loss = -10821.955868206122
1
Iteration 11700: Loss = -10821.95404079929
Iteration 11800: Loss = -10821.954585387022
1
Iteration 11900: Loss = -10821.95407945234
Iteration 12000: Loss = -10821.954063987634
Iteration 12100: Loss = -10821.95970963695
1
Iteration 12200: Loss = -10821.954038678921
Iteration 12300: Loss = -10822.224316249793
1
Iteration 12400: Loss = -10821.954018514909
Iteration 12500: Loss = -10821.965344690021
1
Iteration 12600: Loss = -10821.954029784705
Iteration 12700: Loss = -10821.954014680761
Iteration 12800: Loss = -10821.957180592963
1
Iteration 12900: Loss = -10821.95400568064
Iteration 13000: Loss = -10821.954947899249
1
Iteration 13100: Loss = -10821.96767779352
2
Iteration 13200: Loss = -10822.045211799767
3
Iteration 13300: Loss = -10821.95421589256
4
Iteration 13400: Loss = -10821.954051542612
Iteration 13500: Loss = -10822.042330650178
1
Iteration 13600: Loss = -10821.953998209214
Iteration 13700: Loss = -10821.953987835095
Iteration 13800: Loss = -10821.955461703026
1
Iteration 13900: Loss = -10821.954014608806
Iteration 14000: Loss = -10821.981560566299
1
Iteration 14100: Loss = -10821.954021923431
Iteration 14200: Loss = -10822.05292584923
1
Iteration 14300: Loss = -10821.954045897957
Iteration 14400: Loss = -10821.976919090333
1
Iteration 14500: Loss = -10821.954037252883
Iteration 14600: Loss = -10821.954558917654
1
Iteration 14700: Loss = -10821.959738139067
2
Iteration 14800: Loss = -10821.954016500165
Iteration 14900: Loss = -10821.955104028366
1
Iteration 15000: Loss = -10821.9540056988
Iteration 15100: Loss = -10821.955140374504
1
Iteration 15200: Loss = -10821.954032235739
Iteration 15300: Loss = -10821.954340690172
1
Iteration 15400: Loss = -10822.055589352789
2
Iteration 15500: Loss = -10821.954041684383
Iteration 15600: Loss = -10822.137648998865
1
Iteration 15700: Loss = -10821.954015219113
Iteration 15800: Loss = -10821.953992765324
Iteration 15900: Loss = -10821.954787800689
1
Iteration 16000: Loss = -10821.954012558044
Iteration 16100: Loss = -10821.954981354827
1
Iteration 16200: Loss = -10821.954019322871
Iteration 16300: Loss = -10821.954025077985
Iteration 16400: Loss = -10821.95483521715
1
Iteration 16500: Loss = -10821.954007856397
Iteration 16600: Loss = -10821.959887050181
1
Iteration 16700: Loss = -10821.974617271231
2
Iteration 16800: Loss = -10821.968418165583
3
Iteration 16900: Loss = -10822.182219386015
4
Iteration 17000: Loss = -10821.954019196224
Iteration 17100: Loss = -10821.97451348765
1
Iteration 17200: Loss = -10821.954023884935
Iteration 17300: Loss = -10821.953986420478
Iteration 17400: Loss = -10821.954203724345
1
Iteration 17500: Loss = -10821.953999245747
Iteration 17600: Loss = -10821.984153395983
1
Iteration 17700: Loss = -10821.954014085466
Iteration 17800: Loss = -10821.953993200888
Iteration 17900: Loss = -10822.242388513474
1
Iteration 18000: Loss = -10821.953995646316
Iteration 18100: Loss = -10821.95971160699
1
Iteration 18200: Loss = -10821.98679283477
2
Iteration 18300: Loss = -10821.964009404914
3
Iteration 18400: Loss = -10821.954025603629
Iteration 18500: Loss = -10821.954018063694
Iteration 18600: Loss = -10821.954087766402
Iteration 18700: Loss = -10821.953951494952
Iteration 18800: Loss = -10821.956044968843
1
Iteration 18900: Loss = -10821.953980526865
Iteration 19000: Loss = -10821.968041598831
1
Iteration 19100: Loss = -10821.954010282629
Iteration 19200: Loss = -10821.953980630917
Iteration 19300: Loss = -10821.959376390378
1
Iteration 19400: Loss = -10821.954003644798
Iteration 19500: Loss = -10821.953981372606
Iteration 19600: Loss = -10821.954426855998
1
Iteration 19700: Loss = -10821.9539741671
Iteration 19800: Loss = -10821.954044474627
Iteration 19900: Loss = -10821.954031339877
pi: tensor([[0.7612, 0.2388],
        [0.2555, 0.7445]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5332, 0.4668], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1877, 0.1024],
         [0.5790, 0.2944]],

        [[0.5554, 0.0961],
         [0.6946, 0.5737]],

        [[0.5584, 0.0817],
         [0.5682, 0.5685]],

        [[0.5094, 0.0984],
         [0.6771, 0.6570]],

        [[0.5944, 0.0995],
         [0.5151, 0.6993]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9291543655029207
Average Adjusted Rand Index: 0.9297736877925246
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21236.00889680048
Iteration 100: Loss = -11104.042646180655
Iteration 200: Loss = -11099.562433713796
Iteration 300: Loss = -11098.395681777805
Iteration 400: Loss = -11097.624888844557
Iteration 500: Loss = -11097.168463709282
Iteration 600: Loss = -11096.96654614119
Iteration 700: Loss = -11096.86740593818
Iteration 800: Loss = -11096.823345049386
Iteration 900: Loss = -11096.444495894108
Iteration 1000: Loss = -11091.795388692826
Iteration 1100: Loss = -10964.602659479177
Iteration 1200: Loss = -10961.501548632586
Iteration 1300: Loss = -10961.333386675407
Iteration 1400: Loss = -10953.339558366313
Iteration 1500: Loss = -10952.187210166292
Iteration 1600: Loss = -10949.215222926872
Iteration 1700: Loss = -10943.954082484455
Iteration 1800: Loss = -10925.919324108108
Iteration 1900: Loss = -10923.521110350892
Iteration 2000: Loss = -10923.418121842062
Iteration 2100: Loss = -10923.402057027257
Iteration 2200: Loss = -10923.365450834499
Iteration 2300: Loss = -10923.360120850248
Iteration 2400: Loss = -10923.358438721816
Iteration 2500: Loss = -10923.314270609697
Iteration 2600: Loss = -10920.16454669473
Iteration 2700: Loss = -10920.09013584612
Iteration 2800: Loss = -10920.08812723044
Iteration 2900: Loss = -10920.063725181095
Iteration 3000: Loss = -10920.06293083585
Iteration 3100: Loss = -10920.054626899087
Iteration 3200: Loss = -10920.052839063164
Iteration 3300: Loss = -10920.057201260412
1
Iteration 3400: Loss = -10920.050855442112
Iteration 3500: Loss = -10920.050571294709
Iteration 3600: Loss = -10920.057263189834
1
Iteration 3700: Loss = -10920.056674529027
2
Iteration 3800: Loss = -10920.020237321203
Iteration 3900: Loss = -10920.018676843805
Iteration 4000: Loss = -10920.020713199337
1
Iteration 4100: Loss = -10920.01923830061
2
Iteration 4200: Loss = -10920.018511800004
Iteration 4300: Loss = -10920.018542343061
Iteration 4400: Loss = -10920.019739957637
1
Iteration 4500: Loss = -10920.020567505235
2
Iteration 4600: Loss = -10920.020755373085
3
Iteration 4700: Loss = -10920.021446362869
4
Iteration 4800: Loss = -10920.022739287311
5
Iteration 4900: Loss = -10920.017627771864
Iteration 5000: Loss = -10920.019270681318
1
Iteration 5100: Loss = -10920.01739750991
Iteration 5200: Loss = -10920.016998797257
Iteration 5300: Loss = -10920.018293992125
1
Iteration 5400: Loss = -10920.016604330005
Iteration 5500: Loss = -10920.014721674384
Iteration 5600: Loss = -10919.997501750817
Iteration 5700: Loss = -10919.996996163969
Iteration 5800: Loss = -10919.996836137936
Iteration 5900: Loss = -10919.99956867356
1
Iteration 6000: Loss = -10919.99610428648
Iteration 6100: Loss = -10919.993936678506
Iteration 6200: Loss = -10919.993446682327
Iteration 6300: Loss = -10919.992561220353
Iteration 6400: Loss = -10919.992996361823
1
Iteration 6500: Loss = -10919.998683579664
2
Iteration 6600: Loss = -10919.993709901608
3
Iteration 6700: Loss = -10919.997168955846
4
Iteration 6800: Loss = -10919.993920462799
5
Iteration 6900: Loss = -10919.99289715593
6
Iteration 7000: Loss = -10919.992785654027
7
Iteration 7100: Loss = -10919.992189620982
Iteration 7200: Loss = -10920.002159554026
1
Iteration 7300: Loss = -10919.992038845825
Iteration 7400: Loss = -10919.993386772367
1
Iteration 7500: Loss = -10919.992032761254
Iteration 7600: Loss = -10919.994773083152
1
Iteration 7700: Loss = -10919.991952529826
Iteration 7800: Loss = -10919.991910843612
Iteration 7900: Loss = -10919.992804562491
1
Iteration 8000: Loss = -10919.991828697199
Iteration 8100: Loss = -10919.99178298778
Iteration 8200: Loss = -10919.991715736454
Iteration 8300: Loss = -10919.991669311325
Iteration 8400: Loss = -10919.99160879285
Iteration 8500: Loss = -10920.002593846022
1
Iteration 8600: Loss = -10919.991526632995
Iteration 8700: Loss = -10919.991486557697
Iteration 8800: Loss = -10919.991437820143
Iteration 8900: Loss = -10919.991531203195
Iteration 9000: Loss = -10919.991436976972
Iteration 9100: Loss = -10919.991423368994
Iteration 9200: Loss = -10919.991982092106
1
Iteration 9300: Loss = -10919.991439762442
Iteration 9400: Loss = -10919.991416970925
Iteration 9500: Loss = -10919.991399071636
Iteration 9600: Loss = -10919.990170772027
Iteration 9700: Loss = -10920.088585768026
1
Iteration 9800: Loss = -10919.989609633001
Iteration 9900: Loss = -10919.989610366822
Iteration 10000: Loss = -10919.991983106996
1
Iteration 10100: Loss = -10919.989591466265
Iteration 10200: Loss = -10920.023628324407
1
Iteration 10300: Loss = -10919.991608228866
2
Iteration 10400: Loss = -10919.991617902393
3
Iteration 10500: Loss = -10919.987416957742
Iteration 10600: Loss = -10920.06217171435
1
Iteration 10700: Loss = -10919.986752749446
Iteration 10800: Loss = -10919.987186688926
1
Iteration 10900: Loss = -10919.986845145957
Iteration 11000: Loss = -10919.987120430773
1
Iteration 11100: Loss = -10919.988943626802
2
Iteration 11200: Loss = -10919.992349567932
3
Iteration 11300: Loss = -10919.993779472026
4
Iteration 11400: Loss = -10919.990548079744
5
Iteration 11500: Loss = -10919.987552185807
6
Iteration 11600: Loss = -10920.015292113021
7
Iteration 11700: Loss = -10920.00255594295
8
Iteration 11800: Loss = -10919.987224123004
9
Iteration 11900: Loss = -10919.990259294664
10
Iteration 12000: Loss = -10919.990657120063
11
Iteration 12100: Loss = -10919.986728855796
Iteration 12200: Loss = -10919.99210902328
1
Iteration 12300: Loss = -10920.17641317859
2
Iteration 12400: Loss = -10919.98720024467
3
Iteration 12500: Loss = -10920.047185227715
4
Iteration 12600: Loss = -10919.986210132287
Iteration 12700: Loss = -10919.98752311936
1
Iteration 12800: Loss = -10919.996573052202
2
Iteration 12900: Loss = -10920.054312543483
3
Iteration 13000: Loss = -10919.98619072735
Iteration 13100: Loss = -10919.985857216248
Iteration 13200: Loss = -10920.14997372967
1
Iteration 13300: Loss = -10919.983290663997
Iteration 13400: Loss = -10920.085027528792
1
Iteration 13500: Loss = -10919.98328837419
Iteration 13600: Loss = -10920.301236053685
1
Iteration 13700: Loss = -10919.98319253526
Iteration 13800: Loss = -10919.983272761707
Iteration 13900: Loss = -10919.982036815096
Iteration 14000: Loss = -10920.049650106268
1
Iteration 14100: Loss = -10919.982279575059
2
Iteration 14200: Loss = -10919.98422237014
3
Iteration 14300: Loss = -10919.982791489834
4
Iteration 14400: Loss = -10920.028664568044
5
Iteration 14500: Loss = -10919.992617520711
6
Iteration 14600: Loss = -10920.0059948119
7
Iteration 14700: Loss = -10919.996616812234
8
Iteration 14800: Loss = -10920.18843454376
9
Iteration 14900: Loss = -10919.981678430695
Iteration 15000: Loss = -10920.013155780998
1
Iteration 15100: Loss = -10919.981712708925
Iteration 15200: Loss = -10919.94830149844
Iteration 15300: Loss = -10919.949219521928
1
Iteration 15400: Loss = -10919.952960766728
2
Iteration 15500: Loss = -10919.94959188981
3
Iteration 15600: Loss = -10919.95067323196
4
Iteration 15700: Loss = -10920.013138135091
5
Iteration 15800: Loss = -10919.955467714044
6
Iteration 15900: Loss = -10919.947933355777
Iteration 16000: Loss = -10919.960822178835
1
Iteration 16100: Loss = -10919.950616609485
2
Iteration 16200: Loss = -10919.948437945923
3
Iteration 16300: Loss = -10919.947666135278
Iteration 16400: Loss = -10919.94780976262
1
Iteration 16500: Loss = -10919.985240172291
2
Iteration 16600: Loss = -10919.947516655853
Iteration 16700: Loss = -10919.948158154279
1
Iteration 16800: Loss = -10919.94911357418
2
Iteration 16900: Loss = -10919.965255136927
3
Iteration 17000: Loss = -10919.947615279978
Iteration 17100: Loss = -10919.949346742707
1
Iteration 17200: Loss = -10919.947487219146
Iteration 17300: Loss = -10919.949522090466
1
Iteration 17400: Loss = -10920.067607336816
2
Iteration 17500: Loss = -10919.967472481638
3
Iteration 17600: Loss = -10919.947547580887
Iteration 17700: Loss = -10919.951953589605
1
Iteration 17800: Loss = -10919.951890618559
2
Iteration 17900: Loss = -10920.069813186405
3
Iteration 18000: Loss = -10920.00684011396
4
Iteration 18100: Loss = -10919.959460276998
5
Iteration 18200: Loss = -10919.967096844332
6
Iteration 18300: Loss = -10919.962874106248
7
Iteration 18400: Loss = -10920.078774263482
8
Iteration 18500: Loss = -10919.955410561954
9
Iteration 18600: Loss = -10919.947487511463
Iteration 18700: Loss = -10919.95025997576
1
Iteration 18800: Loss = -10919.947337841002
Iteration 18900: Loss = -10919.947995778417
1
Iteration 19000: Loss = -10919.952094067145
2
Iteration 19100: Loss = -10919.958740827846
3
Iteration 19200: Loss = -10919.959184735028
4
Iteration 19300: Loss = -10919.947420611437
Iteration 19400: Loss = -10919.948602855417
1
Iteration 19500: Loss = -10919.965103692697
2
Iteration 19600: Loss = -10919.95344777492
3
Iteration 19700: Loss = -10919.947943310468
4
Iteration 19800: Loss = -10919.94847872967
5
Iteration 19900: Loss = -10919.959546620357
6
pi: tensor([[0.4028, 0.5972],
        [0.6556, 0.3444]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5677, 0.4323], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2183, 0.1021],
         [0.5814, 0.2599]],

        [[0.5146, 0.0950],
         [0.7308, 0.6818]],

        [[0.6966, 0.0794],
         [0.6032, 0.6965]],

        [[0.7099, 0.0973],
         [0.6914, 0.5079]],

        [[0.6505, 0.0985],
         [0.5005, 0.5959]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 11
Adjusted Rand Index: 0.6044583919171261
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721069260785004
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 9
Adjusted Rand Index: 0.6691181331636993
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.04827131196052927
Average Adjusted Rand Index: 0.7470719047230798
10855.009497567185
[0.9291543655029207, 0.04827131196052927] [0.9297736877925246, 0.7470719047230798] [10821.95398517705, 10919.953121318315]
-------------------------------------
This iteration is 12
True Objective function: Loss = -11013.013650103168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22409.47046316541
Iteration 100: Loss = -11265.561102999814
Iteration 200: Loss = -11264.32375297592
Iteration 300: Loss = -11263.49001518966
Iteration 400: Loss = -11262.76988421082
Iteration 500: Loss = -11262.192113045967
Iteration 600: Loss = -11261.378684412693
Iteration 700: Loss = -11260.505025016442
Iteration 800: Loss = -11259.851852664222
Iteration 900: Loss = -11259.048312272469
Iteration 1000: Loss = -11252.951058804118
Iteration 1100: Loss = -11110.223357645094
Iteration 1200: Loss = -11004.21943552504
Iteration 1300: Loss = -10996.89678118758
Iteration 1400: Loss = -10996.500057768997
Iteration 1500: Loss = -10996.306404487856
Iteration 1600: Loss = -10996.230569084673
Iteration 1700: Loss = -10996.169740185316
Iteration 1800: Loss = -10996.088962026506
Iteration 1900: Loss = -10989.327777619023
Iteration 2000: Loss = -10989.130875835479
Iteration 2100: Loss = -10988.97984075287
Iteration 2200: Loss = -10988.42077003222
Iteration 2300: Loss = -10988.40878336017
Iteration 2400: Loss = -10988.396193009587
Iteration 2500: Loss = -10988.383364554771
Iteration 2600: Loss = -10988.376115122275
Iteration 2700: Loss = -10988.368638479267
Iteration 2800: Loss = -10988.351842536857
Iteration 2900: Loss = -10988.342265356996
Iteration 3000: Loss = -10988.339110887742
Iteration 3100: Loss = -10988.336422807013
Iteration 3200: Loss = -10988.334199110095
Iteration 3300: Loss = -10988.33134882709
Iteration 3400: Loss = -10988.335552129869
1
Iteration 3500: Loss = -10988.319764834672
Iteration 3600: Loss = -10988.319062856825
Iteration 3700: Loss = -10988.32756593931
1
Iteration 3800: Loss = -10988.314222387151
Iteration 3900: Loss = -10988.31005765053
Iteration 4000: Loss = -10988.30793695144
Iteration 4100: Loss = -10988.30747415596
Iteration 4200: Loss = -10988.305654639005
Iteration 4300: Loss = -10988.3048484807
Iteration 4400: Loss = -10988.304002275845
Iteration 4500: Loss = -10988.303099995943
Iteration 4600: Loss = -10988.30216982393
Iteration 4700: Loss = -10988.301123643621
Iteration 4800: Loss = -10988.300235025214
Iteration 4900: Loss = -10988.299029301006
Iteration 5000: Loss = -10988.257217997383
Iteration 5100: Loss = -10988.247585660338
Iteration 5200: Loss = -10988.247151997108
Iteration 5300: Loss = -10988.247546261047
1
Iteration 5400: Loss = -10988.246401644714
Iteration 5500: Loss = -10988.24608012181
Iteration 5600: Loss = -10988.24599170968
Iteration 5700: Loss = -10988.245382709223
Iteration 5800: Loss = -10988.245068069382
Iteration 5900: Loss = -10988.244549336272
Iteration 6000: Loss = -10988.243258696953
Iteration 6100: Loss = -10988.24252904273
Iteration 6200: Loss = -10988.24165982875
Iteration 6300: Loss = -10988.24162693925
Iteration 6400: Loss = -10988.24148817127
Iteration 6500: Loss = -10988.2410484339
Iteration 6600: Loss = -10988.240932876794
Iteration 6700: Loss = -10988.245373739283
1
Iteration 6800: Loss = -10988.241034407076
2
Iteration 6900: Loss = -10988.243423482447
3
Iteration 7000: Loss = -10988.241126659295
4
Iteration 7100: Loss = -10988.243715919623
5
Iteration 7200: Loss = -10988.240140705046
Iteration 7300: Loss = -10988.240040125327
Iteration 7400: Loss = -10988.240271041777
1
Iteration 7500: Loss = -10988.240455473056
2
Iteration 7600: Loss = -10988.239734788465
Iteration 7700: Loss = -10988.392627955252
1
Iteration 7800: Loss = -10988.226620884088
Iteration 7900: Loss = -10988.107713399317
Iteration 8000: Loss = -10988.103140679152
Iteration 8100: Loss = -10988.090959603865
Iteration 8200: Loss = -10988.091447313871
1
Iteration 8300: Loss = -10988.088116869902
Iteration 8400: Loss = -10988.15418982251
1
Iteration 8500: Loss = -10988.083896964778
Iteration 8600: Loss = -10988.082906717453
Iteration 8700: Loss = -10988.085223399023
1
Iteration 8800: Loss = -10988.082557902591
Iteration 8900: Loss = -10988.082402537788
Iteration 9000: Loss = -10988.082969490912
1
Iteration 9100: Loss = -10988.082325499767
Iteration 9200: Loss = -10988.082276856907
Iteration 9300: Loss = -10988.082179851704
Iteration 9400: Loss = -10988.082080109774
Iteration 9500: Loss = -10988.082049362674
Iteration 9600: Loss = -10988.082103539751
Iteration 9700: Loss = -10988.081979770875
Iteration 9800: Loss = -10988.112595641767
1
Iteration 9900: Loss = -10988.083599603739
2
Iteration 10000: Loss = -10988.08177365248
Iteration 10100: Loss = -10988.081500752749
Iteration 10200: Loss = -10988.084005636127
1
Iteration 10300: Loss = -10988.083012277084
2
Iteration 10400: Loss = -10988.081800439573
3
Iteration 10500: Loss = -10988.081398876333
Iteration 10600: Loss = -10988.081810126225
1
Iteration 10700: Loss = -10988.312239635072
2
Iteration 10800: Loss = -10988.081220051565
Iteration 10900: Loss = -10988.100410578598
1
Iteration 11000: Loss = -10988.081201473226
Iteration 11100: Loss = -10988.139282104821
1
Iteration 11200: Loss = -10988.081171504198
Iteration 11300: Loss = -10988.081176448677
Iteration 11400: Loss = -10988.081893987348
1
Iteration 11500: Loss = -10988.08115215983
Iteration 11600: Loss = -10988.083738591391
1
Iteration 11700: Loss = -10988.081139465294
Iteration 11800: Loss = -10988.084982713315
1
Iteration 11900: Loss = -10988.09896103368
2
Iteration 12000: Loss = -10988.082357830992
3
Iteration 12100: Loss = -10988.081166933964
Iteration 12200: Loss = -10988.091218549334
1
Iteration 12300: Loss = -10988.090473367292
2
Iteration 12400: Loss = -10988.08108523678
Iteration 12500: Loss = -10988.082138634663
1
Iteration 12600: Loss = -10988.096960448715
2
Iteration 12700: Loss = -10988.087152378504
3
Iteration 12800: Loss = -10988.081000667902
Iteration 12900: Loss = -10988.08100363771
Iteration 13000: Loss = -10988.093578731081
1
Iteration 13100: Loss = -10988.08082742045
Iteration 13200: Loss = -10988.082721732742
1
Iteration 13300: Loss = -10988.080832494301
Iteration 13400: Loss = -10988.087301754955
1
Iteration 13500: Loss = -10988.080781557694
Iteration 13600: Loss = -10988.209448311183
1
Iteration 13700: Loss = -10988.080771147683
Iteration 13800: Loss = -10988.080767644711
Iteration 13900: Loss = -10988.085094750353
1
Iteration 14000: Loss = -10988.08074836879
Iteration 14100: Loss = -10988.312359389896
1
Iteration 14200: Loss = -10988.080712077599
Iteration 14300: Loss = -10988.08212161591
1
Iteration 14400: Loss = -10988.081637617084
2
Iteration 14500: Loss = -10988.08330615743
3
Iteration 14600: Loss = -10988.080748963226
Iteration 14700: Loss = -10988.080877195122
1
Iteration 14800: Loss = -10988.082281509061
2
Iteration 14900: Loss = -10988.082506653125
3
Iteration 15000: Loss = -10988.080705924109
Iteration 15100: Loss = -10988.105353196159
1
Iteration 15200: Loss = -10988.083771916918
2
Iteration 15300: Loss = -10988.080532401396
Iteration 15400: Loss = -10988.080858626057
1
Iteration 15500: Loss = -10988.080524748337
Iteration 15600: Loss = -10988.080702087955
1
Iteration 15700: Loss = -10988.080277295383
Iteration 15800: Loss = -10988.08035365887
Iteration 15900: Loss = -10988.08027133061
Iteration 16000: Loss = -10988.080571590128
1
Iteration 16100: Loss = -10988.080272387046
Iteration 16200: Loss = -10988.08070024575
1
Iteration 16300: Loss = -10988.080330318815
Iteration 16400: Loss = -10988.080391599671
Iteration 16500: Loss = -10988.107542530168
1
Iteration 16600: Loss = -10988.109498698823
2
Iteration 16700: Loss = -10988.08027401505
Iteration 16800: Loss = -10988.080745240877
1
Iteration 16900: Loss = -10988.080289757156
Iteration 17000: Loss = -10988.080527613389
1
Iteration 17100: Loss = -10988.080264016033
Iteration 17200: Loss = -10988.115706549263
1
Iteration 17300: Loss = -10988.080261185532
Iteration 17400: Loss = -10988.080739651994
1
Iteration 17500: Loss = -10988.080320336017
Iteration 17600: Loss = -10988.115917713114
1
Iteration 17700: Loss = -10988.080318048438
Iteration 17800: Loss = -10988.092867618056
1
Iteration 17900: Loss = -10988.08721110869
2
Iteration 18000: Loss = -10988.080311931324
Iteration 18100: Loss = -10988.20769847356
1
Iteration 18200: Loss = -10988.08027676327
Iteration 18300: Loss = -10988.095623736775
1
Iteration 18400: Loss = -10988.132817489244
2
Iteration 18500: Loss = -10988.089248612228
3
Iteration 18600: Loss = -10988.080643002077
4
Iteration 18700: Loss = -10988.08351660139
5
Iteration 18800: Loss = -10988.113735851191
6
Iteration 18900: Loss = -10988.080412047502
7
Iteration 19000: Loss = -10988.080333568167
Iteration 19100: Loss = -10988.165245057293
1
Iteration 19200: Loss = -10988.080170154866
Iteration 19300: Loss = -10988.12690694839
1
Iteration 19400: Loss = -10988.08016900415
Iteration 19500: Loss = -10988.187980748484
1
Iteration 19600: Loss = -10988.080177259255
Iteration 19700: Loss = -10988.152420195738
1
Iteration 19800: Loss = -10988.110697419119
2
Iteration 19900: Loss = -10988.080291205079
3
pi: tensor([[0.7679, 0.2321],
        [0.2083, 0.7917]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5272, 0.4728], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2824, 0.0905],
         [0.5058, 0.2037]],

        [[0.5953, 0.1038],
         [0.6768, 0.6482]],

        [[0.6140, 0.0945],
         [0.6289, 0.5137]],

        [[0.5745, 0.0982],
         [0.5889, 0.6581]],

        [[0.7070, 0.0975],
         [0.6667, 0.5448]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.936897594475247
Average Adjusted Rand Index: 0.9371292951290593
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23891.641411501238
Iteration 100: Loss = -11263.086431004553
Iteration 200: Loss = -11260.88748144145
Iteration 300: Loss = -11259.951847223483
Iteration 400: Loss = -11258.926505468306
Iteration 500: Loss = -11251.863077092845
Iteration 600: Loss = -11026.455636046689
Iteration 700: Loss = -10996.4094835986
Iteration 800: Loss = -10988.677316375908
Iteration 900: Loss = -10988.539592449697
Iteration 1000: Loss = -10988.473528110258
Iteration 1100: Loss = -10988.418588322193
Iteration 1200: Loss = -10988.37809938598
Iteration 1300: Loss = -10988.361015785524
Iteration 1400: Loss = -10988.348593280229
Iteration 1500: Loss = -10988.340258131671
Iteration 1600: Loss = -10988.332842183061
Iteration 1700: Loss = -10988.323840568677
Iteration 1800: Loss = -10988.263650983465
Iteration 1900: Loss = -10988.26681118351
1
Iteration 2000: Loss = -10988.255915263364
Iteration 2100: Loss = -10988.2531603719
Iteration 2200: Loss = -10988.251100291045
Iteration 2300: Loss = -10988.246748185366
Iteration 2400: Loss = -10988.244183763636
Iteration 2500: Loss = -10988.242623181963
Iteration 2600: Loss = -10988.241481330766
Iteration 2700: Loss = -10988.240546631487
Iteration 2800: Loss = -10988.23973176315
Iteration 2900: Loss = -10988.238936387135
Iteration 3000: Loss = -10988.238323711257
Iteration 3100: Loss = -10988.237706346395
Iteration 3200: Loss = -10988.23718734768
Iteration 3300: Loss = -10988.23670204316
Iteration 3400: Loss = -10988.236250417243
Iteration 3500: Loss = -10988.235928703316
Iteration 3600: Loss = -10988.235423085758
Iteration 3700: Loss = -10988.235042142913
Iteration 3800: Loss = -10988.234651470022
Iteration 3900: Loss = -10988.2353834135
1
Iteration 4000: Loss = -10988.231794783387
Iteration 4100: Loss = -10988.232413512038
1
Iteration 4200: Loss = -10988.229948443568
Iteration 4300: Loss = -10988.22896752534
Iteration 4400: Loss = -10988.228588395097
Iteration 4500: Loss = -10988.228988009769
1
Iteration 4600: Loss = -10988.223149299089
Iteration 4700: Loss = -10988.222869998253
Iteration 4800: Loss = -10988.224199526057
1
Iteration 4900: Loss = -10988.22232627145
Iteration 5000: Loss = -10988.222264759837
Iteration 5100: Loss = -10988.222103865219
Iteration 5200: Loss = -10988.222003676374
Iteration 5300: Loss = -10988.222270326647
1
Iteration 5400: Loss = -10988.222160188347
2
Iteration 5500: Loss = -10988.221814724744
Iteration 5600: Loss = -10988.221871928328
Iteration 5700: Loss = -10988.221521683894
Iteration 5800: Loss = -10988.227663743575
1
Iteration 5900: Loss = -10988.22209552587
2
Iteration 6000: Loss = -10988.221499656553
Iteration 6100: Loss = -10988.221120063408
Iteration 6200: Loss = -10988.22508334622
1
Iteration 6300: Loss = -10988.230589755247
2
Iteration 6400: Loss = -10988.220401786011
Iteration 6500: Loss = -10988.225288658612
1
Iteration 6600: Loss = -10988.221123741787
2
Iteration 6700: Loss = -10988.221422817343
3
Iteration 6800: Loss = -10988.22204210608
4
Iteration 6900: Loss = -10988.220849468315
5
Iteration 7000: Loss = -10988.22466214159
6
Iteration 7100: Loss = -10988.222253424203
7
Iteration 7200: Loss = -10988.223731804692
8
Iteration 7300: Loss = -10988.210514559141
Iteration 7400: Loss = -10988.08899402228
Iteration 7500: Loss = -10988.09011236168
1
Iteration 7600: Loss = -10988.088993428622
Iteration 7700: Loss = -10988.088703464342
Iteration 7800: Loss = -10988.088676981752
Iteration 7900: Loss = -10988.123444013221
1
Iteration 8000: Loss = -10988.081985374381
Iteration 8100: Loss = -10988.097376465752
1
Iteration 8200: Loss = -10988.0819565784
Iteration 8300: Loss = -10988.081906859336
Iteration 8400: Loss = -10988.08481735984
1
Iteration 8500: Loss = -10988.081895575204
Iteration 8600: Loss = -10988.08186825918
Iteration 8700: Loss = -10988.081879450849
Iteration 8800: Loss = -10988.081622681231
Iteration 8900: Loss = -10988.081591188828
Iteration 9000: Loss = -10988.08170650916
1
Iteration 9100: Loss = -10988.081384186453
Iteration 9200: Loss = -10988.081349347545
Iteration 9300: Loss = -10988.081711271301
1
Iteration 9400: Loss = -10988.081324121997
Iteration 9500: Loss = -10988.081314749068
Iteration 9600: Loss = -10988.081586267506
1
Iteration 9700: Loss = -10988.081206547557
Iteration 9800: Loss = -10988.081219085525
Iteration 9900: Loss = -10988.082617213111
1
Iteration 10000: Loss = -10988.080743031787
Iteration 10100: Loss = -10988.08646452029
1
Iteration 10200: Loss = -10988.080727909317
Iteration 10300: Loss = -10988.080838134347
1
Iteration 10400: Loss = -10988.08664017902
2
Iteration 10500: Loss = -10988.242318536762
3
Iteration 10600: Loss = -10988.080737680883
Iteration 10700: Loss = -10988.080973652577
1
Iteration 10800: Loss = -10988.095996426344
2
Iteration 10900: Loss = -10988.167317240297
3
Iteration 11000: Loss = -10988.080661947057
Iteration 11100: Loss = -10988.083510553444
1
Iteration 11200: Loss = -10988.082244951487
2
Iteration 11300: Loss = -10988.082091279739
3
Iteration 11400: Loss = -10988.155375299386
4
Iteration 11500: Loss = -10988.103303351509
5
Iteration 11600: Loss = -10988.080694632985
Iteration 11700: Loss = -10988.083289514409
1
Iteration 11800: Loss = -10988.080979599781
2
Iteration 11900: Loss = -10988.08040771479
Iteration 12000: Loss = -10988.084084314287
1
Iteration 12100: Loss = -10988.086934981135
2
Iteration 12200: Loss = -10988.129121687974
3
Iteration 12300: Loss = -10988.080337347472
Iteration 12400: Loss = -10988.080950548841
1
Iteration 12500: Loss = -10988.080200492743
Iteration 12600: Loss = -10988.080294609777
Iteration 12700: Loss = -10988.191333733414
1
Iteration 12800: Loss = -10988.08013589504
Iteration 12900: Loss = -10988.081801207603
1
Iteration 13000: Loss = -10988.08012669831
Iteration 13100: Loss = -10988.080360294884
1
Iteration 13200: Loss = -10988.08009680221
Iteration 13300: Loss = -10988.08561960899
1
Iteration 13400: Loss = -10988.08008738077
Iteration 13500: Loss = -10988.083727446012
1
Iteration 13600: Loss = -10988.080107085618
Iteration 13700: Loss = -10988.080047484587
Iteration 13800: Loss = -10988.087705961778
1
Iteration 13900: Loss = -10988.080084600011
Iteration 14000: Loss = -10988.089039606148
1
Iteration 14100: Loss = -10988.120243371919
2
Iteration 14200: Loss = -10988.080121155544
Iteration 14300: Loss = -10988.080165063138
Iteration 14400: Loss = -10988.08048300452
1
Iteration 14500: Loss = -10988.086184128273
2
Iteration 14600: Loss = -10988.080081943466
Iteration 14700: Loss = -10988.080175287892
Iteration 14800: Loss = -10988.08542608043
1
Iteration 14900: Loss = -10988.082721514449
2
Iteration 15000: Loss = -10988.360176215387
3
Iteration 15100: Loss = -10988.080080636812
Iteration 15200: Loss = -10988.141141064718
1
Iteration 15300: Loss = -10988.080086107753
Iteration 15400: Loss = -10988.299700723464
1
Iteration 15500: Loss = -10988.08006173279
Iteration 15600: Loss = -10988.085595188231
1
Iteration 15700: Loss = -10988.080079468527
Iteration 15800: Loss = -10988.081948250554
1
Iteration 15900: Loss = -10988.091613143464
2
Iteration 16000: Loss = -10988.082691262487
3
Iteration 16100: Loss = -10988.081002494593
4
Iteration 16200: Loss = -10988.081585410346
5
Iteration 16300: Loss = -10988.08270352055
6
Iteration 16400: Loss = -10988.263168219171
7
Iteration 16500: Loss = -10988.079983343085
Iteration 16600: Loss = -10988.088231376812
1
Iteration 16700: Loss = -10988.080001378987
Iteration 16800: Loss = -10988.088475868188
1
Iteration 16900: Loss = -10988.07999055928
Iteration 17000: Loss = -10988.079982960942
Iteration 17100: Loss = -10988.079965656403
Iteration 17200: Loss = -10988.079960884637
Iteration 17300: Loss = -10988.250219862366
1
Iteration 17400: Loss = -10988.083577360734
2
Iteration 17500: Loss = -10988.0799291089
Iteration 17600: Loss = -10988.079956078845
Iteration 17700: Loss = -10988.098645340602
1
Iteration 17800: Loss = -10988.108375725766
2
Iteration 17900: Loss = -10988.079932768584
Iteration 18000: Loss = -10988.082275479635
1
Iteration 18100: Loss = -10988.080823592465
2
Iteration 18200: Loss = -10988.079948722121
Iteration 18300: Loss = -10988.177181834504
1
Iteration 18400: Loss = -10988.079911384675
Iteration 18500: Loss = -10988.296688143791
1
Iteration 18600: Loss = -10988.079933631625
Iteration 18700: Loss = -10988.079922539073
Iteration 18800: Loss = -10988.08016181685
1
Iteration 18900: Loss = -10988.079916208137
Iteration 19000: Loss = -10988.167227753189
1
Iteration 19100: Loss = -10988.079921423627
Iteration 19200: Loss = -10988.079927104904
Iteration 19300: Loss = -10988.0801967273
1
Iteration 19400: Loss = -10988.07995731644
Iteration 19500: Loss = -10988.080798186329
1
Iteration 19600: Loss = -10988.079809083507
Iteration 19700: Loss = -10988.080157257085
1
Iteration 19800: Loss = -10988.079803970055
Iteration 19900: Loss = -10988.079983150055
1
pi: tensor([[0.7677, 0.2323],
        [0.2082, 0.7918]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5271, 0.4729], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2825, 0.0906],
         [0.7029, 0.2037]],

        [[0.6720, 0.1038],
         [0.6167, 0.5988]],

        [[0.5157, 0.0945],
         [0.6975, 0.6390]],

        [[0.6157, 0.0982],
         [0.5394, 0.5837]],

        [[0.6084, 0.0976],
         [0.6569, 0.5028]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.936897594475247
Average Adjusted Rand Index: 0.9371292951290593
11013.013650103168
[0.936897594475247, 0.936897594475247] [0.9371292951290593, 0.9371292951290593] [10988.080228624622, 10988.079742947386]
-------------------------------------
This iteration is 13
True Objective function: Loss = -11044.688064837863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22755.299563779558
Iteration 100: Loss = -11318.003725751872
Iteration 200: Loss = -11316.823822173857
Iteration 300: Loss = -11316.32747045625
Iteration 400: Loss = -11316.091383495244
Iteration 500: Loss = -11315.951309203938
Iteration 600: Loss = -11315.854399262596
Iteration 700: Loss = -11315.78028404296
Iteration 800: Loss = -11315.715709196533
Iteration 900: Loss = -11315.645677948165
Iteration 1000: Loss = -11315.552082545137
Iteration 1100: Loss = -11315.36202217457
Iteration 1200: Loss = -11314.664036827808
Iteration 1300: Loss = -11313.932184280007
Iteration 1400: Loss = -11313.61591809565
Iteration 1500: Loss = -11312.522083867289
Iteration 1600: Loss = -11308.295332995747
Iteration 1700: Loss = -11196.135329113331
Iteration 1800: Loss = -11110.569544180016
Iteration 1900: Loss = -11102.202437231992
Iteration 2000: Loss = -11094.181447570922
Iteration 2100: Loss = -11092.869802090983
Iteration 2200: Loss = -11090.965426083016
Iteration 2300: Loss = -11090.802529434342
Iteration 2400: Loss = -11089.533763265239
Iteration 2500: Loss = -11089.339367209564
Iteration 2600: Loss = -11089.336724369632
Iteration 2700: Loss = -11089.333151568948
Iteration 2800: Loss = -11089.228414331237
Iteration 2900: Loss = -11089.215642957059
Iteration 3000: Loss = -11088.979229142673
Iteration 3100: Loss = -11088.971334899781
Iteration 3200: Loss = -11087.990193994952
Iteration 3300: Loss = -11087.98633646975
Iteration 3400: Loss = -11087.983795069535
Iteration 3500: Loss = -11087.98211606453
Iteration 3600: Loss = -11087.944586404383
Iteration 3700: Loss = -11087.941176004098
Iteration 3800: Loss = -11087.93826179406
Iteration 3900: Loss = -11087.923234463891
Iteration 4000: Loss = -11087.912067466019
Iteration 4100: Loss = -11087.913981597374
1
Iteration 4200: Loss = -11087.906868680866
Iteration 4300: Loss = -11087.902401081172
Iteration 4400: Loss = -11087.89765606523
Iteration 4500: Loss = -11087.66233969265
Iteration 4600: Loss = -11087.663084624208
1
Iteration 4700: Loss = -11087.659560340247
Iteration 4800: Loss = -11087.650410879565
Iteration 4900: Loss = -11087.650509339948
Iteration 5000: Loss = -11087.665404564044
1
Iteration 5100: Loss = -11087.649408785548
Iteration 5200: Loss = -11087.648582485037
Iteration 5300: Loss = -11087.56581134349
Iteration 5400: Loss = -11087.47485288954
Iteration 5500: Loss = -11087.472847670637
Iteration 5600: Loss = -11085.438114276549
Iteration 5700: Loss = -11085.41478273757
Iteration 5800: Loss = -11085.415688598696
1
Iteration 5900: Loss = -11085.414227357442
Iteration 6000: Loss = -11085.414551886393
1
Iteration 6100: Loss = -11085.416480014472
2
Iteration 6200: Loss = -11085.413769441984
Iteration 6300: Loss = -11085.413708182563
Iteration 6400: Loss = -11085.41839775237
1
Iteration 6500: Loss = -11085.414154972152
2
Iteration 6600: Loss = -11085.413300389828
Iteration 6700: Loss = -11085.412846554043
Iteration 6800: Loss = -11085.408269716343
Iteration 6900: Loss = -11085.327049674619
Iteration 7000: Loss = -11085.327263395371
1
Iteration 7100: Loss = -11085.326405755744
Iteration 7200: Loss = -11085.326415394009
Iteration 7300: Loss = -11085.334860501362
1
Iteration 7400: Loss = -11085.326197933318
Iteration 7500: Loss = -11085.326860969206
1
Iteration 7600: Loss = -11085.32629606669
Iteration 7700: Loss = -11085.32610982159
Iteration 7800: Loss = -11085.329926456274
1
Iteration 7900: Loss = -11085.325998698763
Iteration 8000: Loss = -11085.337779309917
1
Iteration 8100: Loss = -11085.29143227944
Iteration 8200: Loss = -11085.291175797043
Iteration 8300: Loss = -11085.29072750286
Iteration 8400: Loss = -11085.273853045413
Iteration 8500: Loss = -11085.20928858791
Iteration 8600: Loss = -11084.888018769285
Iteration 8700: Loss = -11084.91163606922
1
Iteration 8800: Loss = -11084.887285161276
Iteration 8900: Loss = -11084.888158208181
1
Iteration 9000: Loss = -11084.887214257094
Iteration 9100: Loss = -11084.6761640163
Iteration 9200: Loss = -11084.668883633505
Iteration 9300: Loss = -11084.668393326292
Iteration 9400: Loss = -11084.66839564532
Iteration 9500: Loss = -11084.732050960187
1
Iteration 9600: Loss = -11084.668303383309
Iteration 9700: Loss = -11084.668475272714
1
Iteration 9800: Loss = -11084.668159550265
Iteration 9900: Loss = -11084.649342904724
Iteration 10000: Loss = -11084.649199602129
Iteration 10100: Loss = -11084.686427239752
1
Iteration 10200: Loss = -11084.647249938169
Iteration 10300: Loss = -11084.640075509134
Iteration 10400: Loss = -11084.640277941402
1
Iteration 10500: Loss = -11084.639909630856
Iteration 10600: Loss = -11084.640467512705
1
Iteration 10700: Loss = -11084.644942563777
2
Iteration 10800: Loss = -11084.639673131398
Iteration 10900: Loss = -11084.639199751206
Iteration 11000: Loss = -11084.638960794884
Iteration 11100: Loss = -11084.637015362176
Iteration 11200: Loss = -11084.637539903004
1
Iteration 11300: Loss = -11084.684407454612
2
Iteration 11400: Loss = -11084.639486808474
3
Iteration 11500: Loss = -11084.622190049831
Iteration 11600: Loss = -11084.62080762829
Iteration 11700: Loss = -11084.77169545972
1
Iteration 11800: Loss = -11084.620145172168
Iteration 11900: Loss = -11084.881969635331
1
Iteration 12000: Loss = -11084.620138434924
Iteration 12100: Loss = -11084.620553319966
1
Iteration 12200: Loss = -11084.620270708585
2
Iteration 12300: Loss = -11084.619697035438
Iteration 12400: Loss = -11084.612038050174
Iteration 12500: Loss = -11084.615774755815
1
Iteration 12600: Loss = -11084.610264049545
Iteration 12700: Loss = -11084.610912267826
1
Iteration 12800: Loss = -11084.611339651228
2
Iteration 12900: Loss = -11084.824610902206
3
Iteration 13000: Loss = -11084.612826470935
4
Iteration 13100: Loss = -11084.609672615783
Iteration 13200: Loss = -11084.787368041856
1
Iteration 13300: Loss = -11084.607990283102
Iteration 13400: Loss = -11084.60814211671
1
Iteration 13500: Loss = -11084.609761012858
2
Iteration 13600: Loss = -11084.605683820486
Iteration 13700: Loss = -11084.618258685521
1
Iteration 13800: Loss = -11084.678078688941
2
Iteration 13900: Loss = -11084.594406620632
Iteration 14000: Loss = -11084.595464136914
1
Iteration 14100: Loss = -11084.537655393617
Iteration 14200: Loss = -11084.531163396186
Iteration 14300: Loss = -11084.525520542384
Iteration 14400: Loss = -11084.529510229508
1
Iteration 14500: Loss = -11084.530729595948
2
Iteration 14600: Loss = -11084.509454931846
Iteration 14700: Loss = -11084.522917477536
1
Iteration 14800: Loss = -11084.514782880455
2
Iteration 14900: Loss = -11084.508199313232
Iteration 15000: Loss = -11084.51448934005
1
Iteration 15100: Loss = -11084.508382728782
2
Iteration 15200: Loss = -11084.508056341454
Iteration 15300: Loss = -11084.532350576606
1
Iteration 15400: Loss = -11084.517657703844
2
Iteration 15500: Loss = -11084.59701469637
3
Iteration 15600: Loss = -11084.510961900298
4
Iteration 15700: Loss = -11084.508106292396
Iteration 15800: Loss = -11084.511353115955
1
Iteration 15900: Loss = -11084.511975372377
2
Iteration 16000: Loss = -11084.506623074527
Iteration 16100: Loss = -11084.506588949584
Iteration 16200: Loss = -11084.507728323451
1
Iteration 16300: Loss = -11084.628814268943
2
Iteration 16400: Loss = -11084.50651909443
Iteration 16500: Loss = -11084.5065510784
Iteration 16600: Loss = -11084.513553537498
1
Iteration 16700: Loss = -11083.99513809306
Iteration 16800: Loss = -11083.995101706996
Iteration 16900: Loss = -11083.998800631505
1
Iteration 17000: Loss = -11084.011017330842
2
Iteration 17100: Loss = -11083.992607104788
Iteration 17200: Loss = -11083.993654600758
1
Iteration 17300: Loss = -11083.996907967392
2
Iteration 17400: Loss = -11083.992470316805
Iteration 17500: Loss = -11083.971049543752
Iteration 17600: Loss = -11083.974807712459
1
Iteration 17700: Loss = -11084.18957866775
2
Iteration 17800: Loss = -11083.972386663205
3
Iteration 17900: Loss = -11083.970330007345
Iteration 18000: Loss = -11083.979298360377
1
Iteration 18100: Loss = -11084.018266623198
2
Iteration 18200: Loss = -11084.017770169616
3
Iteration 18300: Loss = -11083.97085496736
4
Iteration 18400: Loss = -11083.96961814669
Iteration 18500: Loss = -11083.981690154837
1
Iteration 18600: Loss = -11083.98171410853
2
Iteration 18700: Loss = -11083.966034158855
Iteration 18800: Loss = -11083.973698708385
1
Iteration 18900: Loss = -11083.95964056409
Iteration 19000: Loss = -11083.957687339698
Iteration 19100: Loss = -11083.956663997498
Iteration 19200: Loss = -11083.959925021858
1
Iteration 19300: Loss = -11083.959695606887
2
Iteration 19400: Loss = -11083.968535525659
3
Iteration 19500: Loss = -11083.957073393716
4
Iteration 19600: Loss = -11083.956423047835
Iteration 19700: Loss = -11083.95653608544
1
Iteration 19800: Loss = -11084.018795856311
2
Iteration 19900: Loss = -11083.956369269297
pi: tensor([[0.2651, 0.7349],
        [0.6989, 0.3011]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5248, 0.4752], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2447, 0.0966],
         [0.5320, 0.2492]],

        [[0.6814, 0.0948],
         [0.6558, 0.6960]],

        [[0.6553, 0.0979],
         [0.6941, 0.5885]],

        [[0.6038, 0.0873],
         [0.6093, 0.6010]],

        [[0.6074, 0.1027],
         [0.6613, 0.5398]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721545392564556
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.02628828177260988
Average Adjusted Rand Index: 0.8608508178628455
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23643.57920760952
Iteration 100: Loss = -11319.650855906491
Iteration 200: Loss = -11318.20399052853
Iteration 300: Loss = -11317.64356016193
Iteration 400: Loss = -11317.115390494038
Iteration 500: Loss = -11316.582285152906
Iteration 600: Loss = -11316.033129262352
Iteration 700: Loss = -11315.499280721371
Iteration 800: Loss = -11314.766488583204
Iteration 900: Loss = -11304.714062715424
Iteration 1000: Loss = -11131.579528943557
Iteration 1100: Loss = -11086.3998751619
Iteration 1200: Loss = -11085.493124997996
Iteration 1300: Loss = -11084.952206676895
Iteration 1400: Loss = -11082.077655138231
Iteration 1500: Loss = -11081.067496687032
Iteration 1600: Loss = -11080.597044582422
Iteration 1700: Loss = -11080.507006037531
Iteration 1800: Loss = -11080.453149861487
Iteration 1900: Loss = -11079.573095959915
Iteration 2000: Loss = -11079.343323142059
Iteration 2100: Loss = -11078.862632736487
Iteration 2200: Loss = -11078.779820828702
Iteration 2300: Loss = -11078.633099590244
Iteration 2400: Loss = -11078.59146174143
Iteration 2500: Loss = -11078.57452009081
Iteration 2600: Loss = -11078.56045604411
Iteration 2700: Loss = -11078.522433193635
Iteration 2800: Loss = -11078.409206688018
Iteration 2900: Loss = -11078.400958324486
Iteration 3000: Loss = -11078.394814409388
Iteration 3100: Loss = -11078.383984490967
Iteration 3200: Loss = -11078.361942278338
Iteration 3300: Loss = -11078.348781731258
Iteration 3400: Loss = -11078.337845879763
Iteration 3500: Loss = -11078.302125707516
Iteration 3600: Loss = -11078.301679196993
Iteration 3700: Loss = -11078.291693806044
Iteration 3800: Loss = -11078.288316086968
Iteration 3900: Loss = -11078.295827651145
1
Iteration 4000: Loss = -11078.27967806521
Iteration 4100: Loss = -11078.27807160145
Iteration 4200: Loss = -11078.274698390032
Iteration 4300: Loss = -11078.28415040108
1
Iteration 4400: Loss = -11078.269952516233
Iteration 4500: Loss = -11078.26657326459
Iteration 4600: Loss = -11078.264630206144
Iteration 4700: Loss = -11078.250043258056
Iteration 4800: Loss = -11078.247664402323
Iteration 4900: Loss = -11078.244180021975
Iteration 5000: Loss = -11078.203066368616
Iteration 5100: Loss = -11078.179986834595
Iteration 5200: Loss = -11078.188525001571
1
Iteration 5300: Loss = -11078.17651451518
Iteration 5400: Loss = -11078.174730185461
Iteration 5500: Loss = -11078.173965825425
Iteration 5600: Loss = -11078.172703325663
Iteration 5700: Loss = -11078.17182459362
Iteration 5800: Loss = -11078.173922440268
1
Iteration 5900: Loss = -11078.170620138666
Iteration 6000: Loss = -11078.170097544114
Iteration 6100: Loss = -11078.168784938875
Iteration 6200: Loss = -11078.164925131478
Iteration 6300: Loss = -11078.153003761628
Iteration 6400: Loss = -11078.150438130067
Iteration 6500: Loss = -11078.14956683335
Iteration 6600: Loss = -11078.14892713047
Iteration 6700: Loss = -11078.148825715974
Iteration 6800: Loss = -11078.147028161648
Iteration 6900: Loss = -11078.143050232396
Iteration 7000: Loss = -11078.13808676152
Iteration 7100: Loss = -11078.132244566134
Iteration 7200: Loss = -11078.129387383273
Iteration 7300: Loss = -11078.129004708719
Iteration 7400: Loss = -11078.128700062538
Iteration 7500: Loss = -11078.158864444737
1
Iteration 7600: Loss = -11078.128191904358
Iteration 7700: Loss = -11078.127722219017
Iteration 7800: Loss = -11078.129808918513
1
Iteration 7900: Loss = -11078.125604162753
Iteration 8000: Loss = -11078.124786984532
Iteration 8100: Loss = -11078.122923438414
Iteration 8200: Loss = -11078.122281240034
Iteration 8300: Loss = -11078.122084514454
Iteration 8400: Loss = -11078.121950001994
Iteration 8500: Loss = -11078.12171895443
Iteration 8600: Loss = -11078.121548937128
Iteration 8700: Loss = -11078.121418396962
Iteration 8800: Loss = -11078.121368953925
Iteration 8900: Loss = -11078.12553031266
1
Iteration 9000: Loss = -11078.120542986326
Iteration 9100: Loss = -11078.120427227437
Iteration 9200: Loss = -11078.12038403396
Iteration 9300: Loss = -11078.119642794321
Iteration 9400: Loss = -11078.113604992557
Iteration 9500: Loss = -11078.113951098107
1
Iteration 9600: Loss = -11078.113314196004
Iteration 9700: Loss = -11078.113262923407
Iteration 9800: Loss = -11078.11316603085
Iteration 9900: Loss = -11078.115748349044
1
Iteration 10000: Loss = -11078.160260929408
2
Iteration 10100: Loss = -11078.106711365428
Iteration 10200: Loss = -11078.106580493915
Iteration 10300: Loss = -11078.106612776477
Iteration 10400: Loss = -11078.114849182684
1
Iteration 10500: Loss = -11078.106939852561
2
Iteration 10600: Loss = -11078.105426160482
Iteration 10700: Loss = -11078.105530254867
1
Iteration 10800: Loss = -11078.109952777797
2
Iteration 10900: Loss = -11078.10538953627
Iteration 11000: Loss = -11078.105596018244
1
Iteration 11100: Loss = -11078.138699754863
2
Iteration 11200: Loss = -11078.128533191806
3
Iteration 11300: Loss = -11078.276158557861
4
Iteration 11400: Loss = -11078.105163174821
Iteration 11500: Loss = -11078.105507393402
1
Iteration 11600: Loss = -11078.11228278186
2
Iteration 11700: Loss = -11078.10510789457
Iteration 11800: Loss = -11078.419196684452
1
Iteration 11900: Loss = -11078.105006539974
Iteration 12000: Loss = -11078.114524319886
1
Iteration 12100: Loss = -11078.104962835985
Iteration 12200: Loss = -11078.104965941191
Iteration 12300: Loss = -11078.105596472136
1
Iteration 12400: Loss = -11078.104912900259
Iteration 12500: Loss = -11078.473803144598
1
Iteration 12600: Loss = -11078.104947983318
Iteration 12700: Loss = -11078.104896580733
Iteration 12800: Loss = -11078.106210410057
1
Iteration 12900: Loss = -11078.10485616801
Iteration 13000: Loss = -11078.209967484341
1
Iteration 13100: Loss = -11078.104851872253
Iteration 13200: Loss = -11078.108798717121
1
Iteration 13300: Loss = -11078.219327672241
2
Iteration 13400: Loss = -11078.245516396122
3
Iteration 13500: Loss = -11078.104983344176
4
Iteration 13600: Loss = -11078.104716684382
Iteration 13700: Loss = -11078.127555198611
1
Iteration 13800: Loss = -11078.104697809964
Iteration 13900: Loss = -11078.129431231431
1
Iteration 14000: Loss = -11078.104709614663
Iteration 14100: Loss = -11078.10486726655
1
Iteration 14200: Loss = -11078.104718570137
Iteration 14300: Loss = -11078.104683912945
Iteration 14400: Loss = -11078.114317111738
1
Iteration 14500: Loss = -11078.104690053526
Iteration 14600: Loss = -11078.104703184616
Iteration 14700: Loss = -11078.105043261603
1
Iteration 14800: Loss = -11078.104694144413
Iteration 14900: Loss = -11078.121858023944
1
Iteration 15000: Loss = -11078.104699863028
Iteration 15100: Loss = -11078.106664417712
1
Iteration 15200: Loss = -11078.106432447492
2
Iteration 15300: Loss = -11078.114064871557
3
Iteration 15400: Loss = -11078.179228079847
4
Iteration 15500: Loss = -11078.104645577909
Iteration 15600: Loss = -11078.10853057099
1
Iteration 15700: Loss = -11078.104588490885
Iteration 15800: Loss = -11078.104879573284
1
Iteration 15900: Loss = -11078.106655929072
2
Iteration 16000: Loss = -11078.118459521578
3
Iteration 16100: Loss = -11078.104621510991
Iteration 16200: Loss = -11078.164767516773
1
Iteration 16300: Loss = -11078.104615087996
Iteration 16400: Loss = -11078.106748381291
1
Iteration 16500: Loss = -11078.111282403688
2
Iteration 16600: Loss = -11078.104601243516
Iteration 16700: Loss = -11078.107201514102
1
Iteration 16800: Loss = -11078.104635106709
Iteration 16900: Loss = -11078.10840307196
1
Iteration 17000: Loss = -11078.104623694526
Iteration 17100: Loss = -11078.10843652509
1
Iteration 17200: Loss = -11078.104626223885
Iteration 17300: Loss = -11078.157125533951
1
Iteration 17400: Loss = -11078.104621857006
Iteration 17500: Loss = -11078.125844369872
1
Iteration 17600: Loss = -11078.104607822603
Iteration 17700: Loss = -11078.117873271196
1
Iteration 17800: Loss = -11078.124961973504
2
Iteration 17900: Loss = -11078.105591698564
3
Iteration 18000: Loss = -11078.104613670466
Iteration 18100: Loss = -11078.108330471514
1
Iteration 18200: Loss = -11078.104574786577
Iteration 18300: Loss = -11078.111747873592
1
Iteration 18400: Loss = -11078.104595363004
Iteration 18500: Loss = -11078.10442662701
Iteration 18600: Loss = -11078.10470270521
1
Iteration 18700: Loss = -11078.104382072552
Iteration 18800: Loss = -11078.17275229883
1
Iteration 18900: Loss = -11078.104392985022
Iteration 19000: Loss = -11078.11130556839
1
Iteration 19100: Loss = -11078.107801208213
2
Iteration 19200: Loss = -11078.109788592832
3
Iteration 19300: Loss = -11078.104789930845
4
Iteration 19400: Loss = -11078.122906537743
5
Iteration 19500: Loss = -11078.104429136856
Iteration 19600: Loss = -11078.10444058346
Iteration 19700: Loss = -11078.104452624777
Iteration 19800: Loss = -11078.104405043929
Iteration 19900: Loss = -11078.120533299518
1
pi: tensor([[0.7256, 0.2744],
        [0.3178, 0.6822]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0771, 0.9229], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3060, 0.0998],
         [0.5061, 0.1928]],

        [[0.5080, 0.0932],
         [0.7037, 0.5111]],

        [[0.6589, 0.1005],
         [0.5748, 0.6028]],

        [[0.5904, 0.0885],
         [0.6534, 0.7301]],

        [[0.6439, 0.1016],
         [0.5266, 0.6264]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.017780852685396243
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.6585464801051422
Average Adjusted Rand Index: 0.78044304579265
11044.688064837863
[0.02628828177260988, 0.6585464801051422] [0.8608508178628455, 0.78044304579265] [11084.007697819654, 11078.104428778572]
-------------------------------------
This iteration is 14
True Objective function: Loss = -11272.630714460178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21332.200649272894
Iteration 100: Loss = -11595.562770810446
Iteration 200: Loss = -11591.95733998148
Iteration 300: Loss = -11587.028815228987
Iteration 400: Loss = -11568.090815362431
Iteration 500: Loss = -11418.256843708872
Iteration 600: Loss = -11369.386914140867
Iteration 700: Loss = -11335.186737448736
Iteration 800: Loss = -11334.354000420153
Iteration 900: Loss = -11329.825866571304
Iteration 1000: Loss = -11329.682339306753
Iteration 1100: Loss = -11328.375874642523
Iteration 1200: Loss = -11328.264206535527
Iteration 1300: Loss = -11327.79810323119
Iteration 1400: Loss = -11321.102018207654
Iteration 1500: Loss = -11319.25009130088
Iteration 1600: Loss = -11319.094349665798
Iteration 1700: Loss = -11319.074569771043
Iteration 1800: Loss = -11319.060705104326
Iteration 1900: Loss = -11319.048589921145
Iteration 2000: Loss = -11319.034680935922
Iteration 2100: Loss = -11319.019557651018
Iteration 2200: Loss = -11319.01392898608
Iteration 2300: Loss = -11319.008756105728
Iteration 2400: Loss = -11319.002330514117
Iteration 2500: Loss = -11318.960545534837
Iteration 2600: Loss = -11318.946004060772
Iteration 2700: Loss = -11318.94239482179
Iteration 2800: Loss = -11318.939964172028
Iteration 2900: Loss = -11318.943123416608
1
Iteration 3000: Loss = -11318.935710565494
Iteration 3100: Loss = -11318.933846393813
Iteration 3200: Loss = -11318.93222516198
Iteration 3300: Loss = -11318.930231065835
Iteration 3400: Loss = -11318.929518461107
Iteration 3500: Loss = -11314.039494245824
Iteration 3600: Loss = -11314.01995657839
Iteration 3700: Loss = -11314.017918559019
Iteration 3800: Loss = -11314.016041609915
Iteration 3900: Loss = -11310.738835381206
Iteration 4000: Loss = -11307.625373051926
Iteration 4100: Loss = -11307.62177645343
Iteration 4200: Loss = -11307.607980707013
Iteration 4300: Loss = -11307.605318956645
Iteration 4400: Loss = -11307.612273399469
1
Iteration 4500: Loss = -11307.602393197305
Iteration 4600: Loss = -11307.61364464344
1
Iteration 4700: Loss = -11307.600121359359
Iteration 4800: Loss = -11307.599753202701
Iteration 4900: Loss = -11307.599413984693
Iteration 5000: Loss = -11307.597907872536
Iteration 5100: Loss = -11307.593935949522
Iteration 5200: Loss = -11307.595198489975
1
Iteration 5300: Loss = -11307.592649487353
Iteration 5400: Loss = -11307.592919137707
1
Iteration 5500: Loss = -11307.591942093164
Iteration 5600: Loss = -11307.591747512739
Iteration 5700: Loss = -11307.591230649072
Iteration 5800: Loss = -11307.590571252462
Iteration 5900: Loss = -11301.57081767623
Iteration 6000: Loss = -11301.55308603723
Iteration 6100: Loss = -11301.552956651993
Iteration 6200: Loss = -11301.552431611119
Iteration 6300: Loss = -11301.552291430908
Iteration 6400: Loss = -11301.552044564782
Iteration 6500: Loss = -11301.55182516638
Iteration 6600: Loss = -11301.552004748184
1
Iteration 6700: Loss = -11301.557976798704
2
Iteration 6800: Loss = -11301.552001266831
3
Iteration 6900: Loss = -11301.550845037746
Iteration 7000: Loss = -11301.550447794947
Iteration 7100: Loss = -11301.550652793589
1
Iteration 7200: Loss = -11301.552877364822
2
Iteration 7300: Loss = -11301.549322318811
Iteration 7400: Loss = -11301.567769888952
1
Iteration 7500: Loss = -11301.429424327169
Iteration 7600: Loss = -11301.429323770239
Iteration 7700: Loss = -11301.436197595907
1
Iteration 7800: Loss = -11301.429080388496
Iteration 7900: Loss = -11301.42907598568
Iteration 8000: Loss = -11301.428955524378
Iteration 8100: Loss = -11301.438466855874
1
Iteration 8200: Loss = -11301.428828424225
Iteration 8300: Loss = -11301.428782180117
Iteration 8400: Loss = -11301.496079004055
1
Iteration 8500: Loss = -11301.428671664526
Iteration 8600: Loss = -11301.428616670486
Iteration 8700: Loss = -11301.42926148596
1
Iteration 8800: Loss = -11301.428510332304
Iteration 8900: Loss = -11301.428409436285
Iteration 9000: Loss = -11301.43708355126
1
Iteration 9100: Loss = -11301.428327043128
Iteration 9200: Loss = -11301.435140341222
1
Iteration 9300: Loss = -11301.428201432595
Iteration 9400: Loss = -11301.436999322197
1
Iteration 9500: Loss = -11301.427988755935
Iteration 9600: Loss = -11301.452447978661
1
Iteration 9700: Loss = -11301.426717891049
Iteration 9800: Loss = -11301.426549735888
Iteration 9900: Loss = -11301.427108240083
1
Iteration 10000: Loss = -11301.426099032107
Iteration 10100: Loss = -11301.4939761011
1
Iteration 10200: Loss = -11301.425390331007
Iteration 10300: Loss = -11301.416966733681
Iteration 10400: Loss = -11301.314792063607
Iteration 10500: Loss = -11301.293170543406
Iteration 10600: Loss = -11301.28660110179
Iteration 10700: Loss = -11301.282251221282
Iteration 10800: Loss = -11301.280427884423
Iteration 10900: Loss = -11301.352422798076
1
Iteration 11000: Loss = -11301.277379857873
Iteration 11100: Loss = -11301.27999403451
1
Iteration 11200: Loss = -11301.276744493423
Iteration 11300: Loss = -11301.275939594076
Iteration 11400: Loss = -11301.280012811369
1
Iteration 11500: Loss = -11301.275915384656
Iteration 11600: Loss = -11301.27588017811
Iteration 11700: Loss = -11301.277389338673
1
Iteration 11800: Loss = -11301.275834462334
Iteration 11900: Loss = -11301.27579347684
Iteration 12000: Loss = -11301.275714733709
Iteration 12100: Loss = -11301.274451366724
Iteration 12200: Loss = -11301.26871016068
Iteration 12300: Loss = -11301.268329620412
Iteration 12400: Loss = -11301.26973270456
1
Iteration 12500: Loss = -11301.268211341125
Iteration 12600: Loss = -11301.268745035657
1
Iteration 12700: Loss = -11301.26764740544
Iteration 12800: Loss = -11301.270838471959
1
Iteration 12900: Loss = -11301.266087943923
Iteration 13000: Loss = -11301.27147222578
1
Iteration 13100: Loss = -11301.265419261468
Iteration 13200: Loss = -11301.279205118459
1
Iteration 13300: Loss = -11301.264067553406
Iteration 13400: Loss = -11301.118369601945
Iteration 13500: Loss = -11301.120182564475
1
Iteration 13600: Loss = -11301.11326198614
Iteration 13700: Loss = -11301.113172847725
Iteration 13800: Loss = -11301.403644047989
1
Iteration 13900: Loss = -11301.11315200215
Iteration 14000: Loss = -11301.27686797687
1
Iteration 14100: Loss = -11301.113115859998
Iteration 14200: Loss = -11301.113059110745
Iteration 14300: Loss = -11301.102685978189
Iteration 14400: Loss = -11301.100760565485
Iteration 14500: Loss = -11301.159441402373
1
Iteration 14600: Loss = -11301.043487732368
Iteration 14700: Loss = -11301.055976165191
1
Iteration 14800: Loss = -11301.04348319834
Iteration 14900: Loss = -11301.059218900937
1
Iteration 15000: Loss = -11301.04345099629
Iteration 15100: Loss = -11301.043720692498
1
Iteration 15200: Loss = -11301.0528526205
2
Iteration 15300: Loss = -11301.04273919797
Iteration 15400: Loss = -11301.051851213015
1
Iteration 15500: Loss = -11301.042680132408
Iteration 15600: Loss = -11301.042856650996
1
Iteration 15700: Loss = -11301.042822673835
2
Iteration 15800: Loss = -11301.042714932664
Iteration 15900: Loss = -11301.042685442222
Iteration 16000: Loss = -11301.049298144873
1
Iteration 16100: Loss = -11300.982752654487
Iteration 16200: Loss = -11300.991071182207
1
Iteration 16300: Loss = -11300.977188052915
Iteration 16400: Loss = -11300.992335951183
1
Iteration 16500: Loss = -11300.986895846769
2
Iteration 16600: Loss = -11300.98066359067
3
Iteration 16700: Loss = -11300.984924380185
4
Iteration 16800: Loss = -11300.978978915333
5
Iteration 16900: Loss = -11301.01021317746
6
Iteration 17000: Loss = -11300.977765987769
7
Iteration 17100: Loss = -11300.974760408832
Iteration 17200: Loss = -11300.88271829798
Iteration 17300: Loss = -11300.882678941593
Iteration 17400: Loss = -11300.883084584215
1
Iteration 17500: Loss = -11300.88246750296
Iteration 17600: Loss = -11300.979072685732
1
Iteration 17700: Loss = -11300.882428542836
Iteration 17800: Loss = -11300.88369097726
1
Iteration 17900: Loss = -11300.882753319585
2
Iteration 18000: Loss = -11300.885224746618
3
Iteration 18100: Loss = -11300.882435477228
Iteration 18200: Loss = -11300.882551212686
1
Iteration 18300: Loss = -11300.878750281585
Iteration 18400: Loss = -11300.878917717271
1
Iteration 18500: Loss = -11300.882460937743
2
Iteration 18600: Loss = -11300.87745349697
Iteration 18700: Loss = -11300.887519522774
1
Iteration 18800: Loss = -11300.90206452783
2
Iteration 18900: Loss = -11300.878139860395
3
Iteration 19000: Loss = -11300.876291410164
Iteration 19100: Loss = -11300.882890409666
1
Iteration 19200: Loss = -11300.876243411672
Iteration 19300: Loss = -11300.88100795853
1
Iteration 19400: Loss = -11300.875610678355
Iteration 19500: Loss = -11300.875937870855
1
Iteration 19600: Loss = -11300.945419937494
2
Iteration 19700: Loss = -11300.875609975392
Iteration 19800: Loss = -11300.881269954572
1
Iteration 19900: Loss = -11300.895354897277
2
pi: tensor([[0.6485, 0.3515],
        [0.2196, 0.7804]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9739, 0.0261], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1863, 0.1226],
         [0.6444, 0.3118]],

        [[0.7189, 0.1091],
         [0.6553, 0.6443]],

        [[0.6935, 0.1011],
         [0.5052, 0.6293]],

        [[0.5334, 0.0967],
         [0.6858, 0.6555]],

        [[0.6718, 0.0930],
         [0.6619, 0.6610]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.5951750308556185
Average Adjusted Rand Index: 0.7372920302769612
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21467.057387599754
Iteration 100: Loss = -11595.24840730757
Iteration 200: Loss = -11592.377898598173
Iteration 300: Loss = -11588.501119259312
Iteration 400: Loss = -11583.657436070464
Iteration 500: Loss = -11434.945393387172
Iteration 600: Loss = -11347.689323531242
Iteration 700: Loss = -11331.991332701618
Iteration 800: Loss = -11323.75343013272
Iteration 900: Loss = -11323.486011268671
Iteration 1000: Loss = -11309.936544304497
Iteration 1100: Loss = -11309.679841432844
Iteration 1200: Loss = -11309.629961909952
Iteration 1300: Loss = -11309.591844478393
Iteration 1400: Loss = -11304.874185930903
Iteration 1500: Loss = -11304.844766178949
Iteration 1600: Loss = -11304.80684174512
Iteration 1700: Loss = -11304.790957403287
Iteration 1800: Loss = -11304.219209565372
Iteration 1900: Loss = -11302.954626396824
Iteration 2000: Loss = -11301.773903031812
Iteration 2100: Loss = -11301.594960214215
Iteration 2200: Loss = -11301.575355834579
Iteration 2300: Loss = -11301.561525473147
Iteration 2400: Loss = -11301.486501800886
Iteration 2500: Loss = -11301.48297628249
Iteration 2600: Loss = -11301.480069170322
Iteration 2700: Loss = -11301.477714183135
Iteration 2800: Loss = -11301.47570974406
Iteration 2900: Loss = -11301.474019303696
Iteration 3000: Loss = -11301.472324142236
Iteration 3100: Loss = -11301.470711932669
Iteration 3200: Loss = -11301.469770054886
Iteration 3300: Loss = -11301.46598622547
Iteration 3400: Loss = -11301.462798641543
Iteration 3500: Loss = -11301.468751258746
1
Iteration 3600: Loss = -11301.460917264963
Iteration 3700: Loss = -11301.461942789465
1
Iteration 3800: Loss = -11301.457080667538
Iteration 3900: Loss = -11301.45480143328
Iteration 4000: Loss = -11301.452413486511
Iteration 4100: Loss = -11301.452191791745
Iteration 4200: Loss = -11301.451135170066
Iteration 4300: Loss = -11301.449357861398
Iteration 4400: Loss = -11301.43847682585
Iteration 4500: Loss = -11301.434549589618
Iteration 4600: Loss = -11301.409450295832
Iteration 4700: Loss = -11301.408785231499
Iteration 4800: Loss = -11301.412836861587
1
Iteration 4900: Loss = -11301.401855994522
Iteration 5000: Loss = -11301.383978121059
Iteration 5100: Loss = -11301.381231102909
Iteration 5200: Loss = -11301.377658539106
Iteration 5300: Loss = -11301.372688070463
Iteration 5400: Loss = -11301.368552270496
Iteration 5500: Loss = -11301.369991417783
1
Iteration 5600: Loss = -11301.366188554292
Iteration 5700: Loss = -11301.35893522813
Iteration 5800: Loss = -11301.35290710006
Iteration 5900: Loss = -11301.336345663596
Iteration 6000: Loss = -11301.303658771209
Iteration 6100: Loss = -11273.54887649317
Iteration 6200: Loss = -11273.292560622062
Iteration 6300: Loss = -11265.756617068984
Iteration 6400: Loss = -11265.499108218111
Iteration 6500: Loss = -11256.675808394508
Iteration 6600: Loss = -11256.625039145069
Iteration 6700: Loss = -11256.579306123916
Iteration 6800: Loss = -11252.781542436353
Iteration 6900: Loss = -11238.835291474636
Iteration 7000: Loss = -11238.77803589322
Iteration 7100: Loss = -11234.875411010868
Iteration 7200: Loss = -11234.874432980123
Iteration 7300: Loss = -11234.873054323742
Iteration 7400: Loss = -11234.874805258149
1
Iteration 7500: Loss = -11234.873441618847
2
Iteration 7600: Loss = -11234.872286133244
Iteration 7700: Loss = -11234.858137288977
Iteration 7800: Loss = -11234.858799554471
1
Iteration 7900: Loss = -11234.860692991804
2
Iteration 8000: Loss = -11234.857562224619
Iteration 8100: Loss = -11234.857810920594
1
Iteration 8200: Loss = -11234.861553044364
2
Iteration 8300: Loss = -11234.857161887932
Iteration 8400: Loss = -11234.855500210073
Iteration 8500: Loss = -11234.854806875159
Iteration 8600: Loss = -11234.870493525315
1
Iteration 8700: Loss = -11234.852097190687
Iteration 8800: Loss = -11234.853115129727
1
Iteration 8900: Loss = -11234.851831287013
Iteration 9000: Loss = -11234.850624981376
Iteration 9100: Loss = -11234.850277260799
Iteration 9200: Loss = -11234.850266428282
Iteration 9300: Loss = -11234.857832380678
1
Iteration 9400: Loss = -11234.849378944225
Iteration 9500: Loss = -11234.845872947615
Iteration 9600: Loss = -11234.8459084648
Iteration 9700: Loss = -11234.84567265959
Iteration 9800: Loss = -11234.885244847885
1
Iteration 9900: Loss = -11234.845077443444
Iteration 10000: Loss = -11234.813307243645
Iteration 10100: Loss = -11234.809347586677
Iteration 10200: Loss = -11234.81125182004
1
Iteration 10300: Loss = -11234.815169122796
2
Iteration 10400: Loss = -11234.81157942645
3
Iteration 10500: Loss = -11234.810369122062
4
Iteration 10600: Loss = -11234.821026360654
5
Iteration 10700: Loss = -11234.809606678702
6
Iteration 10800: Loss = -11234.839716279086
7
Iteration 10900: Loss = -11234.814803674732
8
Iteration 11000: Loss = -11234.830988077214
9
Iteration 11100: Loss = -11234.811329474318
10
Iteration 11200: Loss = -11234.811552572668
11
Iteration 11300: Loss = -11234.80795512052
Iteration 11400: Loss = -11234.809344821397
1
Iteration 11500: Loss = -11234.811454240411
2
Iteration 11600: Loss = -11234.808109223488
3
Iteration 11700: Loss = -11234.81315910572
4
Iteration 11800: Loss = -11234.80824451601
5
Iteration 11900: Loss = -11234.811421593824
6
Iteration 12000: Loss = -11234.789692253067
Iteration 12100: Loss = -11234.787849190663
Iteration 12200: Loss = -11234.797843899756
1
Iteration 12300: Loss = -11234.787743114937
Iteration 12400: Loss = -11234.80276210923
1
Iteration 12500: Loss = -11234.787705855666
Iteration 12600: Loss = -11234.880993096163
1
Iteration 12700: Loss = -11234.787604752173
Iteration 12800: Loss = -11234.787484340279
Iteration 12900: Loss = -11234.845078660603
1
Iteration 13000: Loss = -11234.787610490093
2
Iteration 13100: Loss = -11234.787808777917
3
Iteration 13200: Loss = -11234.81315738171
4
Iteration 13300: Loss = -11234.787375605565
Iteration 13400: Loss = -11234.791046313996
1
Iteration 13500: Loss = -11234.787297820698
Iteration 13600: Loss = -11234.788112998634
1
Iteration 13700: Loss = -11234.819539923616
2
Iteration 13800: Loss = -11234.787243516688
Iteration 13900: Loss = -11234.79042604736
1
Iteration 14000: Loss = -11234.791973595462
2
Iteration 14100: Loss = -11234.787276946452
Iteration 14200: Loss = -11234.80394955341
1
Iteration 14300: Loss = -11234.79068336705
2
Iteration 14400: Loss = -11234.78811638366
3
Iteration 14500: Loss = -11234.807245088708
4
Iteration 14600: Loss = -11234.787295551792
Iteration 14700: Loss = -11234.787734833017
1
Iteration 14800: Loss = -11234.83995267281
2
Iteration 14900: Loss = -11234.790131292513
3
Iteration 15000: Loss = -11234.788813936999
4
Iteration 15100: Loss = -11234.792549516578
5
Iteration 15200: Loss = -11234.787203504193
Iteration 15300: Loss = -11234.790443844448
1
Iteration 15400: Loss = -11234.787177581431
Iteration 15500: Loss = -11234.78854094079
1
Iteration 15600: Loss = -11234.788920962666
2
Iteration 15700: Loss = -11234.794337041392
3
Iteration 15800: Loss = -11234.787629713066
4
Iteration 15900: Loss = -11234.890442583774
5
Iteration 16000: Loss = -11234.790319631464
6
Iteration 16100: Loss = -11234.78805464116
7
Iteration 16200: Loss = -11234.804546743353
8
Iteration 16300: Loss = -11234.787244997995
Iteration 16400: Loss = -11234.787945422962
1
Iteration 16500: Loss = -11234.815238125195
2
Iteration 16600: Loss = -11234.792563005296
3
Iteration 16700: Loss = -11234.792442070593
4
Iteration 16800: Loss = -11234.78906042271
5
Iteration 16900: Loss = -11234.787487943382
6
Iteration 17000: Loss = -11234.780796575254
Iteration 17100: Loss = -11234.77771098774
Iteration 17200: Loss = -11234.774847061308
Iteration 17300: Loss = -11234.788822927874
1
Iteration 17400: Loss = -11234.780059864583
2
Iteration 17500: Loss = -11234.774471560633
Iteration 17600: Loss = -11234.774545716287
Iteration 17700: Loss = -11234.773759979533
Iteration 17800: Loss = -11234.775694799198
1
Iteration 17900: Loss = -11234.7736348674
Iteration 18000: Loss = -11235.004043672427
1
Iteration 18100: Loss = -11234.775861314245
2
Iteration 18200: Loss = -11235.017868840598
3
Iteration 18300: Loss = -11234.773615851018
Iteration 18400: Loss = -11234.863839236421
1
Iteration 18500: Loss = -11234.773516049987
Iteration 18600: Loss = -11234.775095087865
1
Iteration 18700: Loss = -11234.77361893711
2
Iteration 18800: Loss = -11234.774251247087
3
Iteration 18900: Loss = -11234.777035736151
4
Iteration 19000: Loss = -11234.773437835283
Iteration 19100: Loss = -11234.774181001118
1
Iteration 19200: Loss = -11234.774714763604
2
Iteration 19300: Loss = -11234.7835554746
3
Iteration 19400: Loss = -11234.783248030557
4
Iteration 19500: Loss = -11234.77347428263
Iteration 19600: Loss = -11234.816544118263
1
Iteration 19700: Loss = -11234.783148469374
2
Iteration 19800: Loss = -11234.774534217882
3
Iteration 19900: Loss = -11234.776419339349
4
pi: tensor([[0.7897, 0.2103],
        [0.2442, 0.7558]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4881, 0.5119], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3081, 0.1045],
         [0.5075, 0.1962]],

        [[0.6517, 0.1082],
         [0.6687, 0.6088]],

        [[0.6283, 0.1013],
         [0.6513, 0.6895]],

        [[0.6856, 0.0967],
         [0.5792, 0.6663]],

        [[0.6028, 0.0932],
         [0.5609, 0.6815]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844846433231073
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.9291543041741744
Average Adjusted Rand Index: 0.9292912466066904
11272.630714460178
[0.5951750308556185, 0.9291543041741744] [0.7372920302769612, 0.9292912466066904] [11300.884021594216, 11234.77452148967]
-------------------------------------
This iteration is 15
True Objective function: Loss = -11379.809076492535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22949.139783719937
Iteration 100: Loss = -11660.013097387451
Iteration 200: Loss = -11643.266062040613
Iteration 300: Loss = -11629.288675882763
Iteration 400: Loss = -11622.026375004338
Iteration 500: Loss = -11544.658486138203
Iteration 600: Loss = -11445.5431441392
Iteration 700: Loss = -11432.24896238295
Iteration 800: Loss = -11428.421385440892
Iteration 900: Loss = -11428.304257406586
Iteration 1000: Loss = -11428.27170644703
Iteration 1100: Loss = -11428.248728355453
Iteration 1200: Loss = -11428.23459148104
Iteration 1300: Loss = -11428.217898069852
Iteration 1400: Loss = -11428.204864533083
Iteration 1500: Loss = -11428.200865833798
Iteration 1600: Loss = -11428.197786633955
Iteration 1700: Loss = -11428.19514916845
Iteration 1800: Loss = -11428.192303138207
Iteration 1900: Loss = -11427.916869815472
Iteration 2000: Loss = -11427.470348559627
Iteration 2100: Loss = -11427.468331674838
Iteration 2200: Loss = -11427.466059457925
Iteration 2300: Loss = -11427.440187839453
Iteration 2400: Loss = -11427.409251520095
Iteration 2500: Loss = -11427.4076942338
Iteration 2600: Loss = -11427.406629832833
Iteration 2700: Loss = -11427.407743362146
1
Iteration 2800: Loss = -11427.405204189492
Iteration 2900: Loss = -11427.409490434728
1
Iteration 3000: Loss = -11427.403101763952
Iteration 3100: Loss = -11427.402543839335
Iteration 3200: Loss = -11427.403062749965
1
Iteration 3300: Loss = -11427.403388361818
2
Iteration 3400: Loss = -11427.40105408681
Iteration 3500: Loss = -11427.394561061787
Iteration 3600: Loss = -11427.38892197402
Iteration 3700: Loss = -11427.388503809701
Iteration 3800: Loss = -11427.388698914243
1
Iteration 3900: Loss = -11427.406292726693
2
Iteration 4000: Loss = -11427.24613447715
Iteration 4100: Loss = -11427.248465058357
1
Iteration 4200: Loss = -11427.24499100974
Iteration 4300: Loss = -11427.245242154802
1
Iteration 4400: Loss = -11427.24483870178
Iteration 4500: Loss = -11427.245395668873
1
Iteration 4600: Loss = -11427.245406422582
2
Iteration 4700: Loss = -11427.250328378173
3
Iteration 4800: Loss = -11427.250834116887
4
Iteration 4900: Loss = -11427.236767448136
Iteration 5000: Loss = -11427.236712328351
Iteration 5100: Loss = -11427.24067353155
1
Iteration 5200: Loss = -11427.236397214954
Iteration 5300: Loss = -11427.236055964491
Iteration 5400: Loss = -11427.24139468906
1
Iteration 5500: Loss = -11427.235740469667
Iteration 5600: Loss = -11427.236737675163
1
Iteration 5700: Loss = -11427.229654230208
Iteration 5800: Loss = -11427.229618030806
Iteration 5900: Loss = -11427.229517726404
Iteration 6000: Loss = -11427.229649090754
1
Iteration 6100: Loss = -11427.230772677776
2
Iteration 6200: Loss = -11427.232715848424
3
Iteration 6300: Loss = -11426.264054396543
Iteration 6400: Loss = -11425.082197631631
Iteration 6500: Loss = -11425.082092777748
Iteration 6600: Loss = -11425.082063153737
Iteration 6700: Loss = -11425.082045378933
Iteration 6800: Loss = -11425.08211589159
Iteration 6900: Loss = -11425.081219373153
Iteration 7000: Loss = -11425.081163782224
Iteration 7100: Loss = -11425.081353348547
1
Iteration 7200: Loss = -11425.081214321872
Iteration 7300: Loss = -11425.081185071758
Iteration 7400: Loss = -11425.087512403787
1
Iteration 7500: Loss = -11425.080844546645
Iteration 7600: Loss = -11425.080721943317
Iteration 7700: Loss = -11425.080272938807
Iteration 7800: Loss = -11425.007355400823
Iteration 7900: Loss = -11425.006744725435
Iteration 8000: Loss = -11425.006915246588
1
Iteration 8100: Loss = -11425.005365600302
Iteration 8200: Loss = -11424.950888769374
Iteration 8300: Loss = -11421.552158057935
Iteration 8400: Loss = -11421.48508768079
Iteration 8500: Loss = -11421.478763993646
Iteration 8600: Loss = -11421.478710094601
Iteration 8700: Loss = -11421.479901469542
1
Iteration 8800: Loss = -11421.475703747386
Iteration 8900: Loss = -11421.475536728629
Iteration 9000: Loss = -11421.47550339628
Iteration 9100: Loss = -11421.47553328902
Iteration 9200: Loss = -11421.47545172556
Iteration 9300: Loss = -11421.477134513201
1
Iteration 9400: Loss = -11421.475456942344
Iteration 9500: Loss = -11421.476349851728
1
Iteration 9600: Loss = -11421.488030255105
2
Iteration 9700: Loss = -11421.474644002854
Iteration 9800: Loss = -11421.496786318154
1
Iteration 9900: Loss = -11421.471203125777
Iteration 10000: Loss = -11421.527064580188
1
Iteration 10100: Loss = -11421.471035753066
Iteration 10200: Loss = -11421.532793221044
1
Iteration 10300: Loss = -11418.89173454329
Iteration 10400: Loss = -11418.892917529296
1
Iteration 10500: Loss = -11418.890862836231
Iteration 10600: Loss = -11418.891218197175
1
Iteration 10700: Loss = -11418.89083956308
Iteration 10800: Loss = -11418.89095492635
1
Iteration 10900: Loss = -11418.890846871258
Iteration 11000: Loss = -11418.974648948468
1
Iteration 11100: Loss = -11418.890832322573
Iteration 11200: Loss = -11418.891021772157
1
Iteration 11300: Loss = -11418.891653316252
2
Iteration 11400: Loss = -11418.89105154057
3
Iteration 11500: Loss = -11418.89312340122
4
Iteration 11600: Loss = -11418.896003157417
5
Iteration 11700: Loss = -11418.900220821508
6
Iteration 11800: Loss = -11418.890238036369
Iteration 11900: Loss = -11418.890684230719
1
Iteration 12000: Loss = -11418.888870410005
Iteration 12100: Loss = -11418.88893586925
Iteration 12200: Loss = -11418.888296884976
Iteration 12300: Loss = -11418.889801450607
1
Iteration 12400: Loss = -11418.889006818372
2
Iteration 12500: Loss = -11418.899333762361
3
Iteration 12600: Loss = -11418.899411860992
4
Iteration 12700: Loss = -11418.91067244342
5
Iteration 12800: Loss = -11418.888232816034
Iteration 12900: Loss = -11418.999530544703
1
Iteration 13000: Loss = -11418.887980377993
Iteration 13100: Loss = -11418.891553222766
1
Iteration 13200: Loss = -11418.852512976826
Iteration 13300: Loss = -11418.85251755133
Iteration 13400: Loss = -11418.852304563352
Iteration 13500: Loss = -11418.851802830342
Iteration 13600: Loss = -11418.845450562812
Iteration 13700: Loss = -11418.84508575793
Iteration 13800: Loss = -11418.862269548932
1
Iteration 13900: Loss = -11418.851650251365
2
Iteration 14000: Loss = -11418.836673640382
Iteration 14100: Loss = -11419.272019218168
1
Iteration 14200: Loss = -11416.97502258362
Iteration 14300: Loss = -11416.964522538019
Iteration 14400: Loss = -11416.97392168864
1
Iteration 14500: Loss = -11416.974744325886
2
Iteration 14600: Loss = -11416.963452918499
Iteration 14700: Loss = -11416.958991469053
Iteration 14800: Loss = -11417.237117631708
1
Iteration 14900: Loss = -11416.958551327867
Iteration 15000: Loss = -11417.077142690738
1
Iteration 15100: Loss = -11416.95852402971
Iteration 15200: Loss = -11417.057789146826
1
Iteration 15300: Loss = -11416.939748852217
Iteration 15400: Loss = -11416.936758414362
Iteration 15500: Loss = -11416.94028439448
1
Iteration 15600: Loss = -11417.03362233897
2
Iteration 15700: Loss = -11416.956739389883
3
Iteration 15800: Loss = -11416.864663928105
Iteration 15900: Loss = -11416.872538531756
1
Iteration 16000: Loss = -11416.86374365508
Iteration 16100: Loss = -11416.86425301244
1
Iteration 16200: Loss = -11416.86242152991
Iteration 16300: Loss = -11416.856223917257
Iteration 16400: Loss = -11416.85600202097
Iteration 16500: Loss = -11416.85671132727
1
Iteration 16600: Loss = -11416.864424027968
2
Iteration 16700: Loss = -11416.85582288942
Iteration 16800: Loss = -11416.858995949024
1
Iteration 16900: Loss = -11416.856940919653
2
Iteration 17000: Loss = -11416.877145473261
3
Iteration 17100: Loss = -11416.948546244099
4
Iteration 17200: Loss = -11416.856217022178
5
Iteration 17300: Loss = -11416.855468751932
Iteration 17400: Loss = -11416.861928516279
1
Iteration 17500: Loss = -11416.858387091079
2
Iteration 17600: Loss = -11416.856872403208
3
Iteration 17700: Loss = -11416.85759000001
4
Iteration 17800: Loss = -11416.868740960208
5
Iteration 17900: Loss = -11416.855350540529
Iteration 18000: Loss = -11416.856213515526
1
Iteration 18100: Loss = -11416.8681398708
2
Iteration 18200: Loss = -11416.855374205206
Iteration 18300: Loss = -11416.855889656917
1
Iteration 18400: Loss = -11416.855331252711
Iteration 18500: Loss = -11416.855346983652
Iteration 18600: Loss = -11416.856144072543
1
Iteration 18700: Loss = -11416.855269965205
Iteration 18800: Loss = -11416.854980567572
Iteration 18900: Loss = -11416.857160697762
1
Iteration 19000: Loss = -11416.900788066854
2
Iteration 19100: Loss = -11416.855104802704
3
Iteration 19200: Loss = -11416.856527649881
4
Iteration 19300: Loss = -11416.858940472364
5
Iteration 19400: Loss = -11416.853396847948
Iteration 19500: Loss = -11416.852943944168
Iteration 19600: Loss = -11416.85315633498
1
Iteration 19700: Loss = -11416.908496375696
2
Iteration 19800: Loss = -11416.852768188515
Iteration 19900: Loss = -11416.853073207905
1
pi: tensor([[0.3020, 0.6980],
        [0.7205, 0.2795]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4410, 0.5590], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2436, 0.1004],
         [0.6617, 0.2718]],

        [[0.7052, 0.0972],
         [0.7210, 0.7223]],

        [[0.5306, 0.0946],
         [0.5678, 0.5314]],

        [[0.7033, 0.1054],
         [0.5577, 0.5270]],

        [[0.5210, 0.1049],
         [0.5771, 0.5949]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6691997785118287
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.053811365170087146
Average Adjusted Rand Index: 0.8557756719451547
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19987.33129511996
Iteration 100: Loss = -11673.912238540415
Iteration 200: Loss = -11671.109796793631
Iteration 300: Loss = -11665.324436824281
Iteration 400: Loss = -11661.618088489351
Iteration 500: Loss = -11561.798609158286
Iteration 600: Loss = -11447.144731484517
Iteration 700: Loss = -11440.746367398873
Iteration 800: Loss = -11439.413694542402
Iteration 900: Loss = -11438.99988718422
Iteration 1000: Loss = -11436.756946290277
Iteration 1100: Loss = -11431.7444968361
Iteration 1200: Loss = -11431.18660744234
Iteration 1300: Loss = -11427.69438852955
Iteration 1400: Loss = -11427.612833797704
Iteration 1500: Loss = -11425.064650444883
Iteration 1600: Loss = -11425.059309149288
Iteration 1700: Loss = -11425.04634952458
Iteration 1800: Loss = -11425.042919425028
Iteration 1900: Loss = -11425.039856629197
Iteration 2000: Loss = -11425.03522365403
Iteration 2100: Loss = -11425.029694123758
Iteration 2200: Loss = -11425.027059604172
Iteration 2300: Loss = -11424.989722166272
Iteration 2400: Loss = -11424.98437285784
Iteration 2500: Loss = -11424.983006684593
Iteration 2600: Loss = -11424.982986737074
Iteration 2700: Loss = -11424.982058119012
Iteration 2800: Loss = -11424.981236917316
Iteration 2900: Loss = -11424.9808433498
Iteration 3000: Loss = -11424.983722704595
1
Iteration 3100: Loss = -11424.98040437942
Iteration 3200: Loss = -11424.979866471227
Iteration 3300: Loss = -11424.979896933217
Iteration 3400: Loss = -11424.983558518763
1
Iteration 3500: Loss = -11424.968661243991
Iteration 3600: Loss = -11424.969817359317
1
Iteration 3700: Loss = -11424.970518004553
2
Iteration 3800: Loss = -11424.968390234211
Iteration 3900: Loss = -11424.969065010131
1
Iteration 4000: Loss = -11424.968915665682
2
Iteration 4100: Loss = -11424.967011101466
Iteration 4200: Loss = -11424.966509089127
Iteration 4300: Loss = -11424.964771974397
Iteration 4400: Loss = -11424.963752140522
Iteration 4500: Loss = -11424.965401517667
1
Iteration 4600: Loss = -11424.962982248022
Iteration 4700: Loss = -11424.96086361318
Iteration 4800: Loss = -11424.960034998792
Iteration 4900: Loss = -11424.959864353683
Iteration 5000: Loss = -11424.959615712072
Iteration 5100: Loss = -11424.958296263505
Iteration 5200: Loss = -11424.952362537271
Iteration 5300: Loss = -11424.952713740657
1
Iteration 5400: Loss = -11424.952278770166
Iteration 5500: Loss = -11424.950070175364
Iteration 5600: Loss = -11424.95004028249
Iteration 5700: Loss = -11424.949972209028
Iteration 5800: Loss = -11424.95000858488
Iteration 5900: Loss = -11424.949915644764
Iteration 6000: Loss = -11424.95148895477
1
Iteration 6100: Loss = -11424.94984007464
Iteration 6200: Loss = -11424.95160562406
1
Iteration 6300: Loss = -11424.949763511542
Iteration 6400: Loss = -11424.949848468925
Iteration 6500: Loss = -11424.949643167305
Iteration 6600: Loss = -11424.949802876405
1
Iteration 6700: Loss = -11424.949387055989
Iteration 6800: Loss = -11424.950702983233
1
Iteration 6900: Loss = -11424.948556681624
Iteration 7000: Loss = -11424.95101270639
1
Iteration 7100: Loss = -11424.948499935794
Iteration 7200: Loss = -11424.948592326211
Iteration 7300: Loss = -11424.948459482748
Iteration 7400: Loss = -11424.951171621438
1
Iteration 7500: Loss = -11425.033898715319
2
Iteration 7600: Loss = -11424.950541720807
3
Iteration 7700: Loss = -11424.966188576127
4
Iteration 7800: Loss = -11424.957401989792
5
Iteration 7900: Loss = -11424.948157877152
Iteration 8000: Loss = -11424.94842842835
1
Iteration 8100: Loss = -11424.94332841448
Iteration 8200: Loss = -11424.945297404895
1
Iteration 8300: Loss = -11424.943129269079
Iteration 8400: Loss = -11424.94311388132
Iteration 8500: Loss = -11424.94312868945
Iteration 8600: Loss = -11424.943043458548
Iteration 8700: Loss = -11424.983971350108
1
Iteration 8800: Loss = -11424.942628690731
Iteration 8900: Loss = -11424.989052461391
1
Iteration 9000: Loss = -11424.942639306082
Iteration 9100: Loss = -11424.955793568588
1
Iteration 9200: Loss = -11424.94257087628
Iteration 9300: Loss = -11424.942735036298
1
Iteration 9400: Loss = -11424.942555247027
Iteration 9500: Loss = -11424.942706992744
1
Iteration 9600: Loss = -11424.942560875903
Iteration 9700: Loss = -11424.942826725939
1
Iteration 9800: Loss = -11424.941539804602
Iteration 9900: Loss = -11424.958507454277
1
Iteration 10000: Loss = -11424.941544038584
Iteration 10100: Loss = -11424.941523748348
Iteration 10200: Loss = -11424.941750735703
1
Iteration 10300: Loss = -11424.94149662385
Iteration 10400: Loss = -11425.079553853253
1
Iteration 10500: Loss = -11424.941490678433
Iteration 10600: Loss = -11424.956274004504
1
Iteration 10700: Loss = -11424.988704859426
2
Iteration 10800: Loss = -11424.94766895115
3
Iteration 10900: Loss = -11425.156981529279
4
Iteration 11000: Loss = -11424.941090484492
Iteration 11100: Loss = -11424.946304710194
1
Iteration 11200: Loss = -11424.940054232567
Iteration 11300: Loss = -11424.940075558625
Iteration 11400: Loss = -11424.940398395614
1
Iteration 11500: Loss = -11424.940071665565
Iteration 11600: Loss = -11424.94080609164
1
Iteration 11700: Loss = -11424.940108045083
Iteration 11800: Loss = -11424.940062673855
Iteration 11900: Loss = -11424.94012763579
Iteration 12000: Loss = -11424.941150893681
1
Iteration 12100: Loss = -11424.937757988435
Iteration 12200: Loss = -11424.939082131721
1
Iteration 12300: Loss = -11424.975114789842
2
Iteration 12400: Loss = -11424.937485390219
Iteration 12500: Loss = -11424.93803614863
1
Iteration 12600: Loss = -11424.93764595194
2
Iteration 12700: Loss = -11424.937530491641
Iteration 12800: Loss = -11424.93753029275
Iteration 12900: Loss = -11424.937554463137
Iteration 13000: Loss = -11424.937470740426
Iteration 13100: Loss = -11424.942584941418
1
Iteration 13200: Loss = -11424.937449379488
Iteration 13300: Loss = -11424.966004864677
1
Iteration 13400: Loss = -11424.9381868539
2
Iteration 13500: Loss = -11424.937563237816
3
Iteration 13600: Loss = -11424.939590049684
4
Iteration 13700: Loss = -11425.039598402347
5
Iteration 13800: Loss = -11424.937386942453
Iteration 13900: Loss = -11424.93906534407
1
Iteration 14000: Loss = -11424.937279163627
Iteration 14100: Loss = -11424.938074274682
1
Iteration 14200: Loss = -11424.937303508792
Iteration 14300: Loss = -11424.952591812767
1
Iteration 14400: Loss = -11424.937277723699
Iteration 14500: Loss = -11424.938170865353
1
Iteration 14600: Loss = -11424.951272197912
2
Iteration 14700: Loss = -11424.94405163563
3
Iteration 14800: Loss = -11424.937335932826
Iteration 14900: Loss = -11424.937661349408
1
Iteration 15000: Loss = -11425.341816116792
2
Iteration 15100: Loss = -11424.937275600285
Iteration 15200: Loss = -11424.93733823347
Iteration 15300: Loss = -11424.937318837183
Iteration 15400: Loss = -11424.937285926584
Iteration 15500: Loss = -11424.937978854225
1
Iteration 15600: Loss = -11424.937293758532
Iteration 15700: Loss = -11425.402691065972
1
Iteration 15800: Loss = -11424.93726781958
Iteration 15900: Loss = -11424.937264636455
Iteration 16000: Loss = -11425.203158012358
1
Iteration 16100: Loss = -11424.937272021565
Iteration 16200: Loss = -11424.937268236728
Iteration 16300: Loss = -11424.98275667213
1
Iteration 16400: Loss = -11425.087565343156
2
Iteration 16500: Loss = -11424.937398418751
3
Iteration 16600: Loss = -11424.93735477571
Iteration 16700: Loss = -11424.938646829778
1
Iteration 16800: Loss = -11424.938553864042
2
Iteration 16900: Loss = -11424.94586009097
3
Iteration 17000: Loss = -11424.937308493953
Iteration 17100: Loss = -11424.937330466708
Iteration 17200: Loss = -11424.946462833617
1
Iteration 17300: Loss = -11424.965662574703
2
Iteration 17400: Loss = -11424.937271706574
Iteration 17500: Loss = -11425.089886267502
1
Iteration 17600: Loss = -11424.937491369166
2
Iteration 17700: Loss = -11424.937280067714
Iteration 17800: Loss = -11424.95615709225
1
Iteration 17900: Loss = -11424.942381120127
2
Iteration 18000: Loss = -11424.95423138095
3
Iteration 18100: Loss = -11424.937565710261
4
Iteration 18200: Loss = -11424.937292325543
Iteration 18300: Loss = -11425.111285715991
1
Iteration 18400: Loss = -11424.937211313343
Iteration 18500: Loss = -11424.937590898637
1
Iteration 18600: Loss = -11424.93724459904
Iteration 18700: Loss = -11424.93780180676
1
Iteration 18800: Loss = -11424.937229785342
Iteration 18900: Loss = -11424.941327757408
1
Iteration 19000: Loss = -11424.937652838657
2
Iteration 19100: Loss = -11424.936800879685
Iteration 19200: Loss = -11424.960126084958
1
Iteration 19300: Loss = -11424.93678993421
Iteration 19400: Loss = -11424.937268187976
1
Iteration 19500: Loss = -11424.937488587599
2
Iteration 19600: Loss = -11424.93680888808
Iteration 19700: Loss = -11424.936765986591
Iteration 19800: Loss = -11424.936995653252
1
Iteration 19900: Loss = -11424.936783837094
pi: tensor([[0.5928, 0.4072],
        [0.3287, 0.6713]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5441, 0.4559], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2945, 0.1012],
         [0.6211, 0.2193]],

        [[0.6149, 0.1000],
         [0.7195, 0.7141]],

        [[0.6559, 0.0949],
         [0.6870, 0.5963]],

        [[0.6725, 0.1074],
         [0.5338, 0.6695]],

        [[0.6135, 0.1023],
         [0.5272, 0.5067]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 87
Adjusted Rand Index: 0.5418167836748644
Global Adjusted Rand Index: 0.37821352358444194
Average Adjusted Rand Index: 0.8373302112174814
11379.809076492535
[0.053811365170087146, 0.37821352358444194] [0.8557756719451547, 0.8373302112174814] [11416.852934910597, 11424.937536228184]
-------------------------------------
This iteration is 16
True Objective function: Loss = -11277.390685323497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21219.09222346793
Iteration 100: Loss = -11635.549810356099
Iteration 200: Loss = -11634.79272440661
Iteration 300: Loss = -11631.779463748086
Iteration 400: Loss = -11631.18652088546
Iteration 500: Loss = -11630.821703342453
Iteration 600: Loss = -11629.91198562177
Iteration 700: Loss = -11629.55804963009
Iteration 800: Loss = -11629.314002398332
Iteration 900: Loss = -11628.95780422077
Iteration 1000: Loss = -11628.040806544688
Iteration 1100: Loss = -11625.345089787415
Iteration 1200: Loss = -11387.390671991623
Iteration 1300: Loss = -11350.974960739
Iteration 1400: Loss = -11327.46669829348
Iteration 1500: Loss = -11314.95796364422
Iteration 1600: Loss = -11312.061966095895
Iteration 1700: Loss = -11302.145252048254
Iteration 1800: Loss = -11287.38375613136
Iteration 1900: Loss = -11287.294446945243
Iteration 2000: Loss = -11287.285158990888
Iteration 2100: Loss = -11285.121567543363
Iteration 2200: Loss = -11285.112443653372
Iteration 2300: Loss = -11285.087782193747
Iteration 2400: Loss = -11285.081850588718
Iteration 2500: Loss = -11278.91656793558
Iteration 2600: Loss = -11270.480262037685
Iteration 2700: Loss = -11264.472440973856
Iteration 2800: Loss = -11264.470525087041
Iteration 2900: Loss = -11264.46909040426
Iteration 3000: Loss = -11264.467608526671
Iteration 3100: Loss = -11264.468455448787
1
Iteration 3200: Loss = -11264.46382266388
Iteration 3300: Loss = -11264.458791755253
Iteration 3400: Loss = -11264.447758571163
Iteration 3500: Loss = -11264.446378562508
Iteration 3600: Loss = -11264.440856867437
Iteration 3700: Loss = -11257.565435755469
Iteration 3800: Loss = -11257.543877524675
Iteration 3900: Loss = -11257.546922546582
1
Iteration 4000: Loss = -11257.540816662724
Iteration 4100: Loss = -11257.540338561568
Iteration 4200: Loss = -11257.539440771336
Iteration 4300: Loss = -11257.53771548271
Iteration 4400: Loss = -11257.5348155275
Iteration 4500: Loss = -11257.524052537454
Iteration 4600: Loss = -11257.484373008823
Iteration 4700: Loss = -11257.481540029057
Iteration 4800: Loss = -11257.468418779994
Iteration 4900: Loss = -11257.453109766588
Iteration 5000: Loss = -11254.567403954323
Iteration 5100: Loss = -11254.56541829801
Iteration 5200: Loss = -11254.565183234718
Iteration 5300: Loss = -11254.564279437534
Iteration 5400: Loss = -11254.561675889438
Iteration 5500: Loss = -11254.567188294835
1
Iteration 5600: Loss = -11254.55984885088
Iteration 5700: Loss = -11254.559681706127
Iteration 5800: Loss = -11254.562594322659
1
Iteration 5900: Loss = -11254.558900461427
Iteration 6000: Loss = -11254.558230825587
Iteration 6100: Loss = -11254.5580600514
Iteration 6200: Loss = -11254.55788949292
Iteration 6300: Loss = -11254.55812350983
1
Iteration 6400: Loss = -11254.556751083883
Iteration 6500: Loss = -11254.55642572882
Iteration 6600: Loss = -11254.556316370727
Iteration 6700: Loss = -11254.554884575924
Iteration 6800: Loss = -11254.557791699528
1
Iteration 6900: Loss = -11254.554632699419
Iteration 7000: Loss = -11254.569304010492
1
Iteration 7100: Loss = -11254.554246860718
Iteration 7200: Loss = -11253.789296715873
Iteration 7300: Loss = -11253.779644901491
Iteration 7400: Loss = -11253.778798811236
Iteration 7500: Loss = -11253.778278328818
Iteration 7600: Loss = -11253.784093020364
1
Iteration 7700: Loss = -11253.78455949261
2
Iteration 7800: Loss = -11253.766430834461
Iteration 7900: Loss = -11253.751537823797
Iteration 8000: Loss = -11253.75698574063
1
Iteration 8100: Loss = -11253.747084986184
Iteration 8200: Loss = -11253.752491581858
1
Iteration 8300: Loss = -11253.746990912128
Iteration 8400: Loss = -11253.747146919637
1
Iteration 8500: Loss = -11253.746993563844
Iteration 8600: Loss = -11253.74431406173
Iteration 8700: Loss = -11253.744295143733
Iteration 8800: Loss = -11253.74441080894
1
Iteration 8900: Loss = -11253.746011538182
2
Iteration 9000: Loss = -11253.751762094604
3
Iteration 9100: Loss = -11253.752629444514
4
Iteration 9200: Loss = -11253.750557424739
5
Iteration 9300: Loss = -11253.749071183402
6
Iteration 9400: Loss = -11253.839206155395
7
Iteration 9500: Loss = -11253.744474503646
8
Iteration 9600: Loss = -11253.744035894313
Iteration 9700: Loss = -11253.745951341181
1
Iteration 9800: Loss = -11253.744023111372
Iteration 9900: Loss = -11253.74535249118
1
Iteration 10000: Loss = -11253.743986071046
Iteration 10100: Loss = -11253.744478021657
1
Iteration 10200: Loss = -11253.762006841635
2
Iteration 10300: Loss = -11253.74035781754
Iteration 10400: Loss = -11253.740544812757
1
Iteration 10500: Loss = -11253.74314848709
2
Iteration 10600: Loss = -11253.753143951066
3
Iteration 10700: Loss = -11253.739834174394
Iteration 10800: Loss = -11253.74183282002
1
Iteration 10900: Loss = -11253.813938529765
2
Iteration 11000: Loss = -11253.739244731185
Iteration 11100: Loss = -11253.740022445725
1
Iteration 11200: Loss = -11253.741395126639
2
Iteration 11300: Loss = -11253.746501982823
3
Iteration 11400: Loss = -11253.738632941982
Iteration 11500: Loss = -11253.921746124926
1
Iteration 11600: Loss = -11253.738681964094
Iteration 11700: Loss = -11253.738971703255
1
Iteration 11800: Loss = -11253.926254310536
2
Iteration 11900: Loss = -11253.738419749767
Iteration 12000: Loss = -11253.745267138633
1
Iteration 12100: Loss = -11253.745961834844
2
Iteration 12200: Loss = -11253.776514670346
3
Iteration 12300: Loss = -11253.737290164281
Iteration 12400: Loss = -11253.735840794736
Iteration 12500: Loss = -11253.745462594816
1
Iteration 12600: Loss = -11253.738762695899
2
Iteration 12700: Loss = -11253.734878613797
Iteration 12800: Loss = -11253.734719376169
Iteration 12900: Loss = -11253.735752029892
1
Iteration 13000: Loss = -11253.735142881062
2
Iteration 13100: Loss = -11253.741286169277
3
Iteration 13200: Loss = -11253.67378562157
Iteration 13300: Loss = -11253.707641491592
1
Iteration 13400: Loss = -11253.65837894465
Iteration 13500: Loss = -11253.674480332182
1
Iteration 13600: Loss = -11253.653444928557
Iteration 13700: Loss = -11253.652638159736
Iteration 13800: Loss = -11253.653116271
1
Iteration 13900: Loss = -11253.65278594159
2
Iteration 14000: Loss = -11253.651581489667
Iteration 14100: Loss = -11253.650401720723
Iteration 14200: Loss = -11253.652139988462
1
Iteration 14300: Loss = -11253.72475132865
2
Iteration 14400: Loss = -11253.667072544557
3
Iteration 14500: Loss = -11253.650089129927
Iteration 14600: Loss = -11253.65358099569
1
Iteration 14700: Loss = -11253.654952186955
2
Iteration 14800: Loss = -11253.68602597331
3
Iteration 14900: Loss = -11253.653987450412
4
Iteration 15000: Loss = -11253.651500879329
5
Iteration 15100: Loss = -11253.672077880114
6
Iteration 15200: Loss = -11253.658022453266
7
Iteration 15300: Loss = -11253.650296977543
8
Iteration 15400: Loss = -11253.650362683342
9
Iteration 15500: Loss = -11253.650234307115
10
Iteration 15600: Loss = -11253.64998660813
Iteration 15700: Loss = -11253.651659975007
1
Iteration 15800: Loss = -11253.664980916374
2
Iteration 15900: Loss = -11253.672054947445
3
Iteration 16000: Loss = -11253.649877423502
Iteration 16100: Loss = -11253.650343747946
1
Iteration 16200: Loss = -11253.656756588745
2
Iteration 16300: Loss = -11253.646189634317
Iteration 16400: Loss = -11253.650370748433
1
Iteration 16500: Loss = -11253.672185287613
2
Iteration 16600: Loss = -11253.64633497362
3
Iteration 16700: Loss = -11253.646482841432
4
Iteration 16800: Loss = -11253.690633017424
5
Iteration 16900: Loss = -11253.647050671652
6
Iteration 17000: Loss = -11253.646942164587
7
Iteration 17100: Loss = -11253.646328154231
8
Iteration 17200: Loss = -11253.666077744856
9
Iteration 17300: Loss = -11253.66946159467
10
Iteration 17400: Loss = -11253.72913990393
11
Iteration 17500: Loss = -11253.667644274277
12
Iteration 17600: Loss = -11253.649992919256
13
Iteration 17700: Loss = -11253.657887374244
14
Iteration 17800: Loss = -11253.644663917696
Iteration 17900: Loss = -11253.658338152642
1
Iteration 18000: Loss = -11253.648461268436
2
Iteration 18100: Loss = -11253.64427307545
Iteration 18200: Loss = -11253.643991361381
Iteration 18300: Loss = -11253.646258261479
1
Iteration 18400: Loss = -11253.644588130874
2
Iteration 18500: Loss = -11253.657244595726
3
Iteration 18600: Loss = -11253.644974106457
4
Iteration 18700: Loss = -11253.646686112223
5
Iteration 18800: Loss = -11253.644287175479
6
Iteration 18900: Loss = -11253.645555296867
7
Iteration 19000: Loss = -11253.660489874073
8
Iteration 19100: Loss = -11253.644092263035
9
Iteration 19200: Loss = -11253.644242494105
10
Iteration 19300: Loss = -11253.644488508366
11
Iteration 19400: Loss = -11253.670000670218
12
Iteration 19500: Loss = -11253.64393849971
Iteration 19600: Loss = -11253.646093658455
1
Iteration 19700: Loss = -11253.64394162078
Iteration 19800: Loss = -11253.645225743554
1
Iteration 19900: Loss = -11253.649937466014
2
pi: tensor([[0.8010, 0.1990],
        [0.2129, 0.7871]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4580, 0.5420], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2062, 0.1070],
         [0.6964, 0.3071]],

        [[0.6571, 0.1073],
         [0.5979, 0.6061]],

        [[0.6309, 0.0957],
         [0.6585, 0.5193]],

        [[0.7127, 0.0934],
         [0.6078, 0.6705]],

        [[0.6213, 0.0945],
         [0.6001, 0.5323]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080965973782139
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9446734234268186
Average Adjusted Rand Index: 0.9456184418027689
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22538.631055303453
Iteration 100: Loss = -11635.229270256412
Iteration 200: Loss = -11633.440243652478
Iteration 300: Loss = -11632.51968469953
Iteration 400: Loss = -11630.579315493966
Iteration 500: Loss = -11625.616886556367
Iteration 600: Loss = -11592.230844950878
Iteration 700: Loss = -11340.10519045275
Iteration 800: Loss = -11280.86132058068
Iteration 900: Loss = -11260.691851267411
Iteration 1000: Loss = -11259.666951302219
Iteration 1100: Loss = -11255.031570024716
Iteration 1200: Loss = -11254.892712952613
Iteration 1300: Loss = -11254.806900092986
Iteration 1400: Loss = -11254.749234764307
Iteration 1500: Loss = -11254.707655875904
Iteration 1600: Loss = -11254.67566703788
Iteration 1700: Loss = -11254.649613727343
Iteration 1800: Loss = -11254.626855841063
Iteration 1900: Loss = -11254.603350410187
Iteration 2000: Loss = -11254.557579973367
Iteration 2100: Loss = -11254.473526591884
Iteration 2200: Loss = -11254.448055529667
Iteration 2300: Loss = -11254.229570151823
Iteration 2400: Loss = -11253.734843779386
Iteration 2500: Loss = -11253.727872346566
Iteration 2600: Loss = -11253.721926995435
Iteration 2700: Loss = -11253.716633377493
Iteration 2800: Loss = -11253.711697564222
Iteration 2900: Loss = -11253.706859265687
Iteration 3000: Loss = -11253.701220916906
Iteration 3100: Loss = -11253.693862354998
Iteration 3200: Loss = -11253.694969832632
1
Iteration 3300: Loss = -11253.685048921798
Iteration 3400: Loss = -11253.68171442929
Iteration 3500: Loss = -11253.67821589534
Iteration 3600: Loss = -11253.674360866213
Iteration 3700: Loss = -11253.672080365537
Iteration 3800: Loss = -11253.670480226576
Iteration 3900: Loss = -11253.670055937162
Iteration 4000: Loss = -11253.667846344088
Iteration 4100: Loss = -11253.666938292794
Iteration 4200: Loss = -11253.665708099692
Iteration 4300: Loss = -11253.66480080592
Iteration 4400: Loss = -11253.664059673803
Iteration 4500: Loss = -11253.663137795433
Iteration 4600: Loss = -11253.662430899363
Iteration 4700: Loss = -11253.661931666193
Iteration 4800: Loss = -11253.661101225878
Iteration 4900: Loss = -11253.660535201281
Iteration 5000: Loss = -11253.66013795616
Iteration 5100: Loss = -11253.659399406388
Iteration 5200: Loss = -11253.663924253593
1
Iteration 5300: Loss = -11253.65846562909
Iteration 5400: Loss = -11253.667745628638
1
Iteration 5500: Loss = -11253.657607692336
Iteration 5600: Loss = -11253.66176719651
1
Iteration 5700: Loss = -11253.657216112117
Iteration 5800: Loss = -11253.656226551519
Iteration 5900: Loss = -11253.655269576808
Iteration 6000: Loss = -11253.654231533388
Iteration 6100: Loss = -11253.652941158052
Iteration 6200: Loss = -11253.652619949304
Iteration 6300: Loss = -11253.652152324434
Iteration 6400: Loss = -11253.663737835233
1
Iteration 6500: Loss = -11253.651896611385
Iteration 6600: Loss = -11253.652637468069
1
Iteration 6700: Loss = -11253.651073056297
Iteration 6800: Loss = -11253.650906811316
Iteration 6900: Loss = -11253.650682970176
Iteration 7000: Loss = -11253.65338421357
1
Iteration 7100: Loss = -11253.650271414414
Iteration 7200: Loss = -11253.64792308938
Iteration 7300: Loss = -11253.647415854362
Iteration 7400: Loss = -11253.65562347494
1
Iteration 7500: Loss = -11253.646817944276
Iteration 7600: Loss = -11253.646747700117
Iteration 7700: Loss = -11253.646525878137
Iteration 7800: Loss = -11253.646439796576
Iteration 7900: Loss = -11253.646174252957
Iteration 8000: Loss = -11253.64650636842
1
Iteration 8100: Loss = -11253.646664586808
2
Iteration 8200: Loss = -11253.651643178768
3
Iteration 8300: Loss = -11253.64687215859
4
Iteration 8400: Loss = -11253.646345115581
5
Iteration 8500: Loss = -11253.64571800543
Iteration 8600: Loss = -11253.708714084842
1
Iteration 8700: Loss = -11253.645317010081
Iteration 8800: Loss = -11253.645257320048
Iteration 8900: Loss = -11253.645528278757
1
Iteration 9000: Loss = -11253.684998560324
2
Iteration 9100: Loss = -11253.645090015496
Iteration 9200: Loss = -11253.645389442268
1
Iteration 9300: Loss = -11253.64890594669
2
Iteration 9400: Loss = -11253.644826105474
Iteration 9500: Loss = -11253.646224340884
1
Iteration 9600: Loss = -11253.676070150861
2
Iteration 9700: Loss = -11253.653334602024
3
Iteration 9800: Loss = -11253.66309760252
4
Iteration 9900: Loss = -11253.657552501458
5
Iteration 10000: Loss = -11253.764861001751
6
Iteration 10100: Loss = -11253.644581400946
Iteration 10200: Loss = -11253.64546908109
1
Iteration 10300: Loss = -11253.64453602524
Iteration 10400: Loss = -11253.644833892151
1
Iteration 10500: Loss = -11253.64458403958
Iteration 10600: Loss = -11253.65584144422
1
Iteration 10700: Loss = -11253.647599340522
2
Iteration 10800: Loss = -11253.67726270871
3
Iteration 10900: Loss = -11253.644441333234
Iteration 11000: Loss = -11253.64498006844
1
Iteration 11100: Loss = -11253.685133419096
2
Iteration 11200: Loss = -11253.644403800168
Iteration 11300: Loss = -11253.644690173602
1
Iteration 11400: Loss = -11253.71008046635
2
Iteration 11500: Loss = -11253.644604264215
3
Iteration 11600: Loss = -11253.644433356443
Iteration 11700: Loss = -11253.673676894812
1
Iteration 11800: Loss = -11253.644291536237
Iteration 11900: Loss = -11253.64588416086
1
Iteration 12000: Loss = -11253.646636357465
2
Iteration 12100: Loss = -11253.652175838495
3
Iteration 12200: Loss = -11253.64539877071
4
Iteration 12300: Loss = -11253.65017436659
5
Iteration 12400: Loss = -11253.644309863601
Iteration 12500: Loss = -11253.663620087329
1
Iteration 12600: Loss = -11253.657541385915
2
Iteration 12700: Loss = -11253.759218881754
3
Iteration 12800: Loss = -11253.667351013368
4
Iteration 12900: Loss = -11253.710654612994
5
Iteration 13000: Loss = -11253.644084445992
Iteration 13100: Loss = -11253.649198179857
1
Iteration 13200: Loss = -11253.666636366206
2
Iteration 13300: Loss = -11253.64371016949
Iteration 13400: Loss = -11253.652685571045
1
Iteration 13500: Loss = -11253.652364473808
2
Iteration 13600: Loss = -11253.844835416885
3
Iteration 13700: Loss = -11253.644448337765
4
Iteration 13800: Loss = -11253.643659456578
Iteration 13900: Loss = -11253.644130685454
1
Iteration 14000: Loss = -11253.66559263421
2
Iteration 14100: Loss = -11253.646141734715
3
Iteration 14200: Loss = -11253.657164389144
4
Iteration 14300: Loss = -11253.653394893274
5
Iteration 14400: Loss = -11253.645594183528
6
Iteration 14500: Loss = -11253.644253143855
7
Iteration 14600: Loss = -11253.644047130429
8
Iteration 14700: Loss = -11253.648870727631
9
Iteration 14800: Loss = -11253.643996841382
10
Iteration 14900: Loss = -11253.647094741305
11
Iteration 15000: Loss = -11253.643719959984
Iteration 15100: Loss = -11253.645757748756
1
Iteration 15200: Loss = -11253.645435061373
2
Iteration 15300: Loss = -11253.709419108081
3
Iteration 15400: Loss = -11253.64361122898
Iteration 15500: Loss = -11253.64504174522
1
Iteration 15600: Loss = -11253.64565211722
2
Iteration 15700: Loss = -11253.656514895161
3
Iteration 15800: Loss = -11253.657290498235
4
Iteration 15900: Loss = -11253.644082010702
5
Iteration 16000: Loss = -11253.643714996897
6
Iteration 16100: Loss = -11253.644191759779
7
Iteration 16200: Loss = -11253.648497742091
8
Iteration 16300: Loss = -11253.643654842812
Iteration 16400: Loss = -11253.644675513857
1
Iteration 16500: Loss = -11253.643669231033
Iteration 16600: Loss = -11253.644194719438
1
Iteration 16700: Loss = -11253.644863049658
2
Iteration 16800: Loss = -11253.656646153066
3
Iteration 16900: Loss = -11253.711304925891
4
Iteration 17000: Loss = -11253.649856696959
5
Iteration 17100: Loss = -11253.643662259912
Iteration 17200: Loss = -11253.64399637917
1
Iteration 17300: Loss = -11253.644653095827
2
Iteration 17400: Loss = -11253.670021061096
3
Iteration 17500: Loss = -11253.650926348635
4
Iteration 17600: Loss = -11253.64419710157
5
Iteration 17700: Loss = -11253.700892842659
6
Iteration 17800: Loss = -11253.644507585326
7
Iteration 17900: Loss = -11253.65100569036
8
Iteration 18000: Loss = -11253.64355018115
Iteration 18100: Loss = -11253.646578487482
1
Iteration 18200: Loss = -11253.798575136623
2
Iteration 18300: Loss = -11253.643589710771
Iteration 18400: Loss = -11253.645196128662
1
Iteration 18500: Loss = -11253.644401118034
2
Iteration 18600: Loss = -11253.644459491867
3
Iteration 18700: Loss = -11253.657996751605
4
Iteration 18800: Loss = -11253.645637940459
5
Iteration 18900: Loss = -11253.645898094823
6
Iteration 19000: Loss = -11253.646989338462
7
Iteration 19100: Loss = -11253.646939056416
8
Iteration 19200: Loss = -11253.712787330818
9
Iteration 19300: Loss = -11253.64510750797
10
Iteration 19400: Loss = -11253.643925277289
11
Iteration 19500: Loss = -11253.84700681026
12
Iteration 19600: Loss = -11253.652627513307
13
Iteration 19700: Loss = -11253.646497045542
14
Iteration 19800: Loss = -11253.644253274264
15
Stopping early at iteration 19800 due to no improvement.
pi: tensor([[0.7998, 0.2002],
        [0.2143, 0.7857]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4565, 0.5435], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2066, 0.1067],
         [0.5055, 0.3066]],

        [[0.6531, 0.1071],
         [0.6672, 0.6809]],

        [[0.6683, 0.0951],
         [0.5313, 0.6190]],

        [[0.6250, 0.0931],
         [0.5773, 0.5274]],

        [[0.7018, 0.0943],
         [0.6458, 0.5047]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080965973782139
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9446734234268186
Average Adjusted Rand Index: 0.9456184418027689
11277.390685323497
[0.9446734234268186, 0.9446734234268186] [0.9456184418027689, 0.9456184418027689] [11253.664127019703, 11253.644253274264]
-------------------------------------
This iteration is 17
True Objective function: Loss = -10990.515521132842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21739.246653553386
Iteration 100: Loss = -11281.09457381213
Iteration 200: Loss = -11275.821391745654
Iteration 300: Loss = -11273.23208124489
Iteration 400: Loss = -11249.517097974334
Iteration 500: Loss = -11175.04864108469
Iteration 600: Loss = -10988.84117637417
Iteration 700: Loss = -10983.644096506614
Iteration 800: Loss = -10982.552205980233
Iteration 900: Loss = -10980.885200357045
Iteration 1000: Loss = -10980.821930517564
Iteration 1100: Loss = -10980.791244239843
Iteration 1200: Loss = -10980.766810416382
Iteration 1300: Loss = -10980.731611407135
Iteration 1400: Loss = -10980.69543867629
Iteration 1500: Loss = -10980.679550110483
Iteration 1600: Loss = -10980.667140943566
Iteration 1700: Loss = -10980.658301780584
Iteration 1800: Loss = -10980.638266603875
Iteration 1900: Loss = -10980.625965379117
Iteration 2000: Loss = -10980.62172534439
Iteration 2100: Loss = -10980.61629448067
Iteration 2200: Loss = -10980.565035079939
Iteration 2300: Loss = -10974.599397780374
Iteration 2400: Loss = -10974.575713815942
Iteration 2500: Loss = -10974.573592205199
Iteration 2600: Loss = -10974.571151468986
Iteration 2700: Loss = -10974.554498508953
Iteration 2800: Loss = -10974.552723682917
Iteration 2900: Loss = -10974.551244401495
Iteration 3000: Loss = -10974.549601007773
Iteration 3100: Loss = -10974.547408125876
Iteration 3200: Loss = -10974.543385814915
Iteration 3300: Loss = -10974.54137370709
Iteration 3400: Loss = -10974.540280137713
Iteration 3500: Loss = -10974.539516532377
Iteration 3600: Loss = -10974.540749250766
1
Iteration 3700: Loss = -10974.538192988131
Iteration 3800: Loss = -10974.544651049626
1
Iteration 3900: Loss = -10974.536570596294
Iteration 4000: Loss = -10974.506197041695
Iteration 4100: Loss = -10974.507090377921
1
Iteration 4200: Loss = -10974.472857035966
Iteration 4300: Loss = -10974.471995232956
Iteration 4400: Loss = -10974.472104128017
1
Iteration 4500: Loss = -10974.468815942622
Iteration 4600: Loss = -10974.468879856218
Iteration 4700: Loss = -10974.469604987522
1
Iteration 4800: Loss = -10974.46798524659
Iteration 4900: Loss = -10974.468160915247
1
Iteration 5000: Loss = -10974.467505772178
Iteration 5100: Loss = -10974.467223618001
Iteration 5200: Loss = -10974.46704679596
Iteration 5300: Loss = -10974.466926924588
Iteration 5400: Loss = -10974.468632325552
1
Iteration 5500: Loss = -10974.463809964567
Iteration 5600: Loss = -10974.463240856117
Iteration 5700: Loss = -10974.439118984545
Iteration 5800: Loss = -10974.441505149638
1
Iteration 5900: Loss = -10974.440896441065
2
Iteration 6000: Loss = -10974.440078004684
3
Iteration 6100: Loss = -10974.451408556459
4
Iteration 6200: Loss = -10974.43874423869
Iteration 6300: Loss = -10974.438304658297
Iteration 6400: Loss = -10974.439150479358
1
Iteration 6500: Loss = -10974.391876391239
Iteration 6600: Loss = -10974.385755098188
Iteration 6700: Loss = -10974.389154859593
1
Iteration 6800: Loss = -10974.385158116873
Iteration 6900: Loss = -10974.383395155566
Iteration 7000: Loss = -10974.38326837758
Iteration 7100: Loss = -10974.383192395733
Iteration 7200: Loss = -10974.464188398431
1
Iteration 7300: Loss = -10974.383089808349
Iteration 7400: Loss = -10974.407488539968
1
Iteration 7500: Loss = -10974.38288578335
Iteration 7600: Loss = -10974.382805889574
Iteration 7700: Loss = -10974.384028422064
1
Iteration 7800: Loss = -10974.382741844942
Iteration 7900: Loss = -10974.382772932784
Iteration 8000: Loss = -10974.382607992675
Iteration 8100: Loss = -10974.382527747268
Iteration 8200: Loss = -10974.397069928793
1
Iteration 8300: Loss = -10974.381500528143
Iteration 8400: Loss = -10974.380688956735
Iteration 8500: Loss = -10974.380764637968
Iteration 8600: Loss = -10974.380540910304
Iteration 8700: Loss = -10974.015720589965
Iteration 8800: Loss = -10974.033397020508
1
Iteration 8900: Loss = -10974.012343562521
Iteration 9000: Loss = -10974.007562704764
Iteration 9100: Loss = -10974.011260918878
1
Iteration 9200: Loss = -10974.00980813332
2
Iteration 9300: Loss = -10974.016714136007
3
Iteration 9400: Loss = -10974.023609295038
4
Iteration 9500: Loss = -10974.004401684346
Iteration 9600: Loss = -10974.004663164502
1
Iteration 9700: Loss = -10974.007976922618
2
Iteration 9800: Loss = -10974.002242338393
Iteration 9900: Loss = -10974.016133219548
1
Iteration 10000: Loss = -10974.019631881978
2
Iteration 10100: Loss = -10974.011216440562
3
Iteration 10200: Loss = -10974.004543796671
4
Iteration 10300: Loss = -10974.001282534859
Iteration 10400: Loss = -10974.000203302527
Iteration 10500: Loss = -10973.99922874808
Iteration 10600: Loss = -10974.000885283755
1
Iteration 10700: Loss = -10973.99887639884
Iteration 10800: Loss = -10974.000174436485
1
Iteration 10900: Loss = -10973.998872969793
Iteration 11000: Loss = -10974.024995470832
1
Iteration 11100: Loss = -10973.99886111569
Iteration 11200: Loss = -10974.088188813765
1
Iteration 11300: Loss = -10974.011037355507
2
Iteration 11400: Loss = -10974.001552595351
3
Iteration 11500: Loss = -10973.998794505422
Iteration 11600: Loss = -10973.999513243114
1
Iteration 11700: Loss = -10973.998728819242
Iteration 11800: Loss = -10973.997412991293
Iteration 11900: Loss = -10974.010450606185
1
Iteration 12000: Loss = -10974.004433893117
2
Iteration 12100: Loss = -10974.013678845648
3
Iteration 12200: Loss = -10973.997531329653
4
Iteration 12300: Loss = -10973.99989378178
5
Iteration 12400: Loss = -10973.99739806266
Iteration 12500: Loss = -10973.997690888666
1
Iteration 12600: Loss = -10974.015283956847
2
Iteration 12700: Loss = -10973.997603479262
3
Iteration 12800: Loss = -10973.997269113808
Iteration 12900: Loss = -10974.036705453223
1
Iteration 13000: Loss = -10973.99721307425
Iteration 13100: Loss = -10973.997702174413
1
Iteration 13200: Loss = -10973.997236886426
Iteration 13300: Loss = -10973.997463652502
1
Iteration 13400: Loss = -10974.001977361377
2
Iteration 13500: Loss = -10974.06952158498
3
Iteration 13600: Loss = -10974.014318181959
4
Iteration 13700: Loss = -10973.992807582195
Iteration 13800: Loss = -10973.99218859589
Iteration 13900: Loss = -10974.065657090176
1
Iteration 14000: Loss = -10973.992213915593
Iteration 14100: Loss = -10973.997090856703
1
Iteration 14200: Loss = -10973.999199077689
2
Iteration 14300: Loss = -10973.992097366681
Iteration 14400: Loss = -10974.017389898701
1
Iteration 14500: Loss = -10973.99139435128
Iteration 14600: Loss = -10974.241762799636
1
Iteration 14700: Loss = -10973.993271461406
2
Iteration 14800: Loss = -10974.00654969902
3
Iteration 14900: Loss = -10974.012309778893
4
Iteration 15000: Loss = -10974.063844293165
5
Iteration 15100: Loss = -10973.993010648073
6
Iteration 15200: Loss = -10973.994883417476
7
Iteration 15300: Loss = -10974.003354605853
8
Iteration 15400: Loss = -10973.992367157209
9
Iteration 15500: Loss = -10973.993452499777
10
Iteration 15600: Loss = -10973.993360340546
11
Iteration 15700: Loss = -10973.993311225122
12
Iteration 15800: Loss = -10973.992788770674
13
Iteration 15900: Loss = -10973.99159950611
14
Iteration 16000: Loss = -10973.991804947174
15
Stopping early at iteration 16000 due to no improvement.
pi: tensor([[0.7940, 0.2060],
        [0.2620, 0.7380]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4538, 0.5462], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1911, 0.0891],
         [0.6086, 0.2990]],

        [[0.5769, 0.1013],
         [0.5908, 0.6318]],

        [[0.5561, 0.1024],
         [0.6570, 0.5552]],

        [[0.6742, 0.1008],
         [0.5422, 0.5210]],

        [[0.5470, 0.0931],
         [0.6877, 0.5780]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9760961377037042
Average Adjusted Rand Index: 0.9759964884296097
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22004.5617497681
Iteration 100: Loss = -11283.11780990726
Iteration 200: Loss = -11276.847603078879
Iteration 300: Loss = -11262.33455109002
Iteration 400: Loss = -11167.153745395104
Iteration 500: Loss = -10989.297020795353
Iteration 600: Loss = -10977.566251578533
Iteration 700: Loss = -10975.788146649405
Iteration 800: Loss = -10975.108467793025
Iteration 900: Loss = -10974.955410804925
Iteration 1000: Loss = -10974.85222498063
Iteration 1100: Loss = -10974.778236849192
Iteration 1200: Loss = -10974.715667620909
Iteration 1300: Loss = -10974.669039816583
Iteration 1400: Loss = -10974.636297975603
Iteration 1500: Loss = -10974.579337478364
Iteration 1600: Loss = -10974.544322905007
Iteration 1700: Loss = -10974.525758273832
Iteration 1800: Loss = -10974.513387567438
Iteration 1900: Loss = -10974.502555699566
Iteration 2000: Loss = -10974.490530631221
Iteration 2100: Loss = -10974.476543998095
Iteration 2200: Loss = -10974.470247340108
Iteration 2300: Loss = -10974.464971375253
Iteration 2400: Loss = -10974.460896288927
Iteration 2500: Loss = -10974.455760222649
Iteration 2600: Loss = -10974.451214283668
Iteration 2700: Loss = -10974.454580609978
1
Iteration 2800: Loss = -10974.443532449783
Iteration 2900: Loss = -10974.436528837317
Iteration 3000: Loss = -10974.429814667643
Iteration 3100: Loss = -10974.427713787867
Iteration 3200: Loss = -10974.433023028481
1
Iteration 3300: Loss = -10974.424160883997
Iteration 3400: Loss = -10974.422966429762
Iteration 3500: Loss = -10974.42219171226
Iteration 3600: Loss = -10974.419962468035
Iteration 3700: Loss = -10974.41773744766
Iteration 3800: Loss = -10974.416663235354
Iteration 3900: Loss = -10974.41205696037
Iteration 4000: Loss = -10974.411604101622
Iteration 4100: Loss = -10974.41157501624
Iteration 4200: Loss = -10974.409227587475
Iteration 4300: Loss = -10974.408589994688
Iteration 4400: Loss = -10974.408332944648
Iteration 4500: Loss = -10974.407613243162
Iteration 4600: Loss = -10974.406915243235
Iteration 4700: Loss = -10974.406416790149
Iteration 4800: Loss = -10974.405994544039
Iteration 4900: Loss = -10974.406067500064
Iteration 5000: Loss = -10974.405843410355
Iteration 5100: Loss = -10974.416247543228
1
Iteration 5200: Loss = -10974.404480147236
Iteration 5300: Loss = -10974.404465514595
Iteration 5400: Loss = -10974.40388396199
Iteration 5500: Loss = -10974.408745211413
1
Iteration 5600: Loss = -10974.403266601328
Iteration 5700: Loss = -10974.405663548008
1
Iteration 5800: Loss = -10974.402497066125
Iteration 5900: Loss = -10974.40206566843
Iteration 6000: Loss = -10974.400506651276
Iteration 6100: Loss = -10974.406845941117
1
Iteration 6200: Loss = -10974.402560451459
2
Iteration 6300: Loss = -10974.398542244266
Iteration 6400: Loss = -10974.398177436415
Iteration 6500: Loss = -10974.397997434678
Iteration 6600: Loss = -10974.400614402306
1
Iteration 6700: Loss = -10974.397722600715
Iteration 6800: Loss = -10974.397640737445
Iteration 6900: Loss = -10974.397700575842
Iteration 7000: Loss = -10974.412603147444
1
Iteration 7100: Loss = -10974.397841556898
2
Iteration 7200: Loss = -10974.39723693306
Iteration 7300: Loss = -10974.397132219941
Iteration 7400: Loss = -10974.397067655944
Iteration 7500: Loss = -10974.396986274092
Iteration 7600: Loss = -10974.3985203705
1
Iteration 7700: Loss = -10974.399892729996
2
Iteration 7800: Loss = -10974.397643335975
3
Iteration 7900: Loss = -10974.399573033646
4
Iteration 8000: Loss = -10974.39847926787
5
Iteration 8100: Loss = -10974.482519876821
6
Iteration 8200: Loss = -10974.39640138037
Iteration 8300: Loss = -10974.397028193262
1
Iteration 8400: Loss = -10974.396288281612
Iteration 8500: Loss = -10974.396276594263
Iteration 8600: Loss = -10974.396682423363
1
Iteration 8700: Loss = -10974.401372358248
2
Iteration 8800: Loss = -10974.394351702373
Iteration 8900: Loss = -10974.393632067638
Iteration 9000: Loss = -10974.021969462216
Iteration 9100: Loss = -10974.021908980005
Iteration 9200: Loss = -10974.022371587711
1
Iteration 9300: Loss = -10974.024150497484
2
Iteration 9400: Loss = -10974.030767068936
3
Iteration 9500: Loss = -10974.017202047056
Iteration 9600: Loss = -10974.020749401707
1
Iteration 9700: Loss = -10974.017263007723
Iteration 9800: Loss = -10974.016722969924
Iteration 9900: Loss = -10974.016041536976
Iteration 10000: Loss = -10974.020372562101
1
Iteration 10100: Loss = -10974.05200154912
2
Iteration 10200: Loss = -10974.034508811004
3
Iteration 10300: Loss = -10974.015174381428
Iteration 10400: Loss = -10974.015438271254
1
Iteration 10500: Loss = -10974.016424379539
2
Iteration 10600: Loss = -10974.020594653457
3
Iteration 10700: Loss = -10974.021059919975
4
Iteration 10800: Loss = -10974.015059138694
Iteration 10900: Loss = -10974.015032623707
Iteration 11000: Loss = -10974.016056923121
1
Iteration 11100: Loss = -10974.01511374989
Iteration 11200: Loss = -10974.0149491676
Iteration 11300: Loss = -10974.014815296294
Iteration 11400: Loss = -10974.014868271268
Iteration 11500: Loss = -10974.064697390122
1
Iteration 11600: Loss = -10974.014672261153
Iteration 11700: Loss = -10974.014471267054
Iteration 11800: Loss = -10974.035183283955
1
Iteration 11900: Loss = -10974.027446729577
2
Iteration 12000: Loss = -10974.014616246006
3
Iteration 12100: Loss = -10974.014420779571
Iteration 12200: Loss = -10974.016257887431
1
Iteration 12300: Loss = -10974.014363729148
Iteration 12400: Loss = -10974.014485921232
1
Iteration 12500: Loss = -10974.015706689297
2
Iteration 12600: Loss = -10974.014382013433
Iteration 12700: Loss = -10974.014663998758
1
Iteration 12800: Loss = -10974.03251476133
2
Iteration 12900: Loss = -10974.014242666843
Iteration 13000: Loss = -10974.015688797585
1
Iteration 13100: Loss = -10974.016043580152
2
Iteration 13200: Loss = -10974.01437871299
3
Iteration 13300: Loss = -10974.014278575612
Iteration 13400: Loss = -10974.01898793854
1
Iteration 13500: Loss = -10974.023799695957
2
Iteration 13600: Loss = -10974.01447487747
3
Iteration 13700: Loss = -10974.001421406816
Iteration 13800: Loss = -10974.047242141984
1
Iteration 13900: Loss = -10974.001858150097
2
Iteration 14000: Loss = -10974.001628751865
3
Iteration 14100: Loss = -10974.030444759605
4
Iteration 14200: Loss = -10974.001155472399
Iteration 14300: Loss = -10974.001361011193
1
Iteration 14400: Loss = -10974.044450028832
2
Iteration 14500: Loss = -10974.005633358085
3
Iteration 14600: Loss = -10974.005803744736
4
Iteration 14700: Loss = -10974.038352262447
5
Iteration 14800: Loss = -10974.075771992013
6
Iteration 14900: Loss = -10974.001050998235
Iteration 15000: Loss = -10974.001122480497
Iteration 15100: Loss = -10974.001233991135
1
Iteration 15200: Loss = -10973.993780180912
Iteration 15300: Loss = -10974.049210249308
1
Iteration 15400: Loss = -10973.992236493841
Iteration 15500: Loss = -10973.99228432629
Iteration 15600: Loss = -10974.21268511288
1
Iteration 15700: Loss = -10973.992183234082
Iteration 15800: Loss = -10974.004362332858
1
Iteration 15900: Loss = -10973.992184833049
Iteration 16000: Loss = -10974.04941673035
1
Iteration 16100: Loss = -10973.993507951833
2
Iteration 16200: Loss = -10974.054479597507
3
Iteration 16300: Loss = -10973.992139220141
Iteration 16400: Loss = -10973.994007958347
1
Iteration 16500: Loss = -10973.992552693753
2
Iteration 16600: Loss = -10973.993493450947
3
Iteration 16700: Loss = -10974.001376647702
4
Iteration 16800: Loss = -10973.992598334176
5
Iteration 16900: Loss = -10973.993258923918
6
Iteration 17000: Loss = -10974.005745150016
7
Iteration 17100: Loss = -10973.991740066976
Iteration 17200: Loss = -10973.993634354409
1
Iteration 17300: Loss = -10974.001797708066
2
Iteration 17400: Loss = -10974.057498003924
3
Iteration 17500: Loss = -10974.19077355863
4
Iteration 17600: Loss = -10973.992375304246
5
Iteration 17700: Loss = -10973.99257443073
6
Iteration 17800: Loss = -10974.016729483901
7
Iteration 17900: Loss = -10973.991974809122
8
Iteration 18000: Loss = -10974.013793460143
9
Iteration 18100: Loss = -10973.991629301718
Iteration 18200: Loss = -10973.9919899635
1
Iteration 18300: Loss = -10973.991667557373
Iteration 18400: Loss = -10973.991890550627
1
Iteration 18500: Loss = -10973.991620977627
Iteration 18600: Loss = -10973.992619441899
1
Iteration 18700: Loss = -10973.991647650384
Iteration 18800: Loss = -10973.992309176676
1
Iteration 18900: Loss = -10973.992455803405
2
Iteration 19000: Loss = -10973.991638352536
Iteration 19100: Loss = -10973.992675291327
1
Iteration 19200: Loss = -10974.024429413776
2
Iteration 19300: Loss = -10973.992991008958
3
Iteration 19400: Loss = -10974.0082261559
4
Iteration 19500: Loss = -10974.053651941229
5
Iteration 19600: Loss = -10974.012721957139
6
Iteration 19700: Loss = -10973.991915007111
7
Iteration 19800: Loss = -10973.991541825173
Iteration 19900: Loss = -10974.026313640608
1
pi: tensor([[0.7382, 0.2618],
        [0.2059, 0.7941]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5463, 0.4537], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2989, 0.0891],
         [0.6183, 0.1911]],

        [[0.5293, 0.1012],
         [0.5777, 0.5188]],

        [[0.5261, 0.1024],
         [0.6234, 0.7270]],

        [[0.5193, 0.1008],
         [0.5370, 0.5863]],

        [[0.5715, 0.0931],
         [0.7094, 0.6478]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9760961377037042
Average Adjusted Rand Index: 0.9759964884296097
10990.515521132842
[0.9760961377037042, 0.9760961377037042] [0.9759964884296097, 0.9759964884296097] [10973.991804947174, 10973.991826131656]
-------------------------------------
This iteration is 18
True Objective function: Loss = -11229.264400903518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24656.580017543503
Iteration 100: Loss = -11504.430528697936
Iteration 200: Loss = -11500.766061941185
Iteration 300: Loss = -11497.679430039829
Iteration 400: Loss = -11495.349982743763
Iteration 500: Loss = -11459.872534951137
Iteration 600: Loss = -11353.31430992886
Iteration 700: Loss = -11333.68946593403
Iteration 800: Loss = -11330.65190747367
Iteration 900: Loss = -11330.613929760508
Iteration 1000: Loss = -11330.589122881875
Iteration 1100: Loss = -11330.57387319926
Iteration 1200: Loss = -11330.534573487348
Iteration 1300: Loss = -11323.386723903546
Iteration 1400: Loss = -11323.372736010684
Iteration 1500: Loss = -11323.38855695378
1
Iteration 1600: Loss = -11316.687811217631
Iteration 1700: Loss = -11315.256543704229
Iteration 1800: Loss = -11313.817757836543
Iteration 1900: Loss = -11313.819003227445
1
Iteration 2000: Loss = -11313.813129124856
Iteration 2100: Loss = -11313.811815942498
Iteration 2200: Loss = -11313.022758635441
Iteration 2300: Loss = -11313.019200510806
Iteration 2400: Loss = -11313.016937061611
Iteration 2500: Loss = -11313.016009976573
Iteration 2600: Loss = -11313.013766183443
Iteration 2700: Loss = -11312.967856971203
Iteration 2800: Loss = -11312.966972645807
Iteration 2900: Loss = -11312.966216617548
Iteration 3000: Loss = -11312.965402775586
Iteration 3100: Loss = -11312.96209733405
Iteration 3200: Loss = -11312.893121411173
Iteration 3300: Loss = -11307.180702940457
Iteration 3400: Loss = -11307.190880334661
1
Iteration 3500: Loss = -11307.129472622128
Iteration 3600: Loss = -11307.125235695295
Iteration 3700: Loss = -11307.123079207002
Iteration 3800: Loss = -11307.131948788208
1
Iteration 3900: Loss = -11307.122670673478
Iteration 4000: Loss = -11307.122161416244
Iteration 4100: Loss = -11307.121053916417
Iteration 4200: Loss = -11306.923189946947
Iteration 4300: Loss = -11306.923156628965
Iteration 4400: Loss = -11305.138413996523
Iteration 4500: Loss = -11305.124394206676
Iteration 4600: Loss = -11305.124086471304
Iteration 4700: Loss = -11305.138892563353
1
Iteration 4800: Loss = -11305.123251453648
Iteration 4900: Loss = -11305.111752131046
Iteration 5000: Loss = -11287.177937377319
Iteration 5100: Loss = -11280.963036026847
Iteration 5200: Loss = -11280.90769491192
Iteration 5300: Loss = -11280.866300081503
Iteration 5400: Loss = -11277.520525496519
Iteration 5500: Loss = -11275.029617268501
Iteration 5600: Loss = -11273.76536971495
Iteration 5700: Loss = -11267.873802614213
Iteration 5800: Loss = -11267.873981520615
1
Iteration 5900: Loss = -11267.872143430493
Iteration 6000: Loss = -11267.872006907675
Iteration 6100: Loss = -11267.806539966796
Iteration 6200: Loss = -11267.801731831174
Iteration 6300: Loss = -11267.802213193925
1
Iteration 6400: Loss = -11267.80006604952
Iteration 6500: Loss = -11267.815982662638
1
Iteration 6600: Loss = -11267.797843235196
Iteration 6700: Loss = -11267.79787040618
Iteration 6800: Loss = -11267.797742300763
Iteration 6900: Loss = -11267.797654044341
Iteration 7000: Loss = -11267.783779380436
Iteration 7100: Loss = -11267.77940679247
Iteration 7200: Loss = -11267.77894130459
Iteration 7300: Loss = -11267.78085815657
1
Iteration 7400: Loss = -11267.776287445842
Iteration 7500: Loss = -11267.776110007948
Iteration 7600: Loss = -11267.776400669583
1
Iteration 7700: Loss = -11267.775817927868
Iteration 7800: Loss = -11267.77677500113
1
Iteration 7900: Loss = -11267.775433336095
Iteration 8000: Loss = -11267.777197079495
1
Iteration 8100: Loss = -11267.769918390582
Iteration 8200: Loss = -11267.774734668676
1
Iteration 8300: Loss = -11267.775835798084
2
Iteration 8400: Loss = -11267.770144273032
3
Iteration 8500: Loss = -11267.765842620694
Iteration 8600: Loss = -11267.7688273262
1
Iteration 8700: Loss = -11267.801077032442
2
Iteration 8800: Loss = -11267.764775112277
Iteration 8900: Loss = -11267.763757886845
Iteration 9000: Loss = -11267.753062654487
Iteration 9100: Loss = -11267.753074651302
Iteration 9200: Loss = -11267.752964354186
Iteration 9300: Loss = -11267.753314096202
1
Iteration 9400: Loss = -11267.752927060077
Iteration 9500: Loss = -11267.625706916098
Iteration 9600: Loss = -11267.617615282023
Iteration 9700: Loss = -11267.617596896322
Iteration 9800: Loss = -11267.746299610297
1
Iteration 9900: Loss = -11267.617268886612
Iteration 10000: Loss = -11267.690596759565
1
Iteration 10100: Loss = -11267.617117624643
Iteration 10200: Loss = -11267.982032445578
1
Iteration 10300: Loss = -11267.617111858934
Iteration 10400: Loss = -11267.61709860647
Iteration 10500: Loss = -11267.617215593207
1
Iteration 10600: Loss = -11267.61704437821
Iteration 10700: Loss = -11267.6329055133
1
Iteration 10800: Loss = -11267.618437043593
2
Iteration 10900: Loss = -11267.617524034897
3
Iteration 11000: Loss = -11267.617720278797
4
Iteration 11100: Loss = -11267.61703854579
Iteration 11200: Loss = -11267.617684047182
1
Iteration 11300: Loss = -11267.682503797005
2
Iteration 11400: Loss = -11267.616459690586
Iteration 11500: Loss = -11267.616408367661
Iteration 11600: Loss = -11267.616664590949
1
Iteration 11700: Loss = -11267.616353966101
Iteration 11800: Loss = -11267.616195091286
Iteration 11900: Loss = -11267.616400344168
1
Iteration 12000: Loss = -11267.61522237841
Iteration 12100: Loss = -11267.569067014507
Iteration 12200: Loss = -11267.575846384538
1
Iteration 12300: Loss = -11267.562164025378
Iteration 12400: Loss = -11267.560915957138
Iteration 12500: Loss = -11267.65912571529
1
Iteration 12600: Loss = -11267.560908242514
Iteration 12700: Loss = -11267.552257706308
Iteration 12800: Loss = -11267.58132154164
1
Iteration 12900: Loss = -11267.552198034524
Iteration 13000: Loss = -11267.552162443531
Iteration 13100: Loss = -11267.553828798518
1
Iteration 13200: Loss = -11267.55058412331
Iteration 13300: Loss = -11267.550539619351
Iteration 13400: Loss = -11267.550674194526
1
Iteration 13500: Loss = -11267.550387006275
Iteration 13600: Loss = -11267.504597546087
Iteration 13700: Loss = -11267.503338395723
Iteration 13800: Loss = -11267.504588546948
1
Iteration 13900: Loss = -11267.503585680814
2
Iteration 14000: Loss = -11267.520365647333
3
Iteration 14100: Loss = -11267.503299196133
Iteration 14200: Loss = -11267.630962976933
1
Iteration 14300: Loss = -11267.48203748128
Iteration 14400: Loss = -11267.481942369172
Iteration 14500: Loss = -11267.455846645282
Iteration 14600: Loss = -11267.456442968827
1
Iteration 14700: Loss = -11267.457754265466
2
Iteration 14800: Loss = -11267.45498063134
Iteration 14900: Loss = -11267.454609952496
Iteration 15000: Loss = -11267.479532958492
1
Iteration 15100: Loss = -11267.454379484256
Iteration 15200: Loss = -11267.610684344625
1
Iteration 15300: Loss = -11267.454421301058
Iteration 15400: Loss = -11267.454392520016
Iteration 15500: Loss = -11267.45718495207
1
Iteration 15600: Loss = -11267.45529613663
2
Iteration 15700: Loss = -11267.45477001951
3
Iteration 15800: Loss = -11267.454549514398
4
Iteration 15900: Loss = -11267.454967893145
5
Iteration 16000: Loss = -11267.454739110224
6
Iteration 16100: Loss = -11267.454464024846
Iteration 16200: Loss = -11267.45902935968
1
Iteration 16300: Loss = -11267.492772045847
2
Iteration 16400: Loss = -11267.454393433572
Iteration 16500: Loss = -11267.454388944201
Iteration 16600: Loss = -11267.456652275658
1
Iteration 16700: Loss = -11267.454461641602
Iteration 16800: Loss = -11267.454188095198
Iteration 16900: Loss = -11267.454207086665
Iteration 17000: Loss = -11267.454341075989
1
Iteration 17100: Loss = -11267.452069542707
Iteration 17200: Loss = -11267.451390547221
Iteration 17300: Loss = -11267.355903718366
Iteration 17400: Loss = -11267.343062584874
Iteration 17500: Loss = -11267.342044579176
Iteration 17600: Loss = -11267.39614444201
1
Iteration 17700: Loss = -11267.343417966682
2
Iteration 17800: Loss = -11267.341827650149
Iteration 17900: Loss = -11267.380756746299
1
Iteration 18000: Loss = -11267.355045243712
2
Iteration 18100: Loss = -11267.344004971117
3
Iteration 18200: Loss = -11267.341830667428
Iteration 18300: Loss = -11267.332782443325
Iteration 18400: Loss = -11267.33065013952
Iteration 18500: Loss = -11267.329732005317
Iteration 18600: Loss = -11267.329851920573
1
Iteration 18700: Loss = -11267.324704316754
Iteration 18800: Loss = -11267.378990378122
1
Iteration 18900: Loss = -11267.323802158528
Iteration 19000: Loss = -11267.323770629551
Iteration 19100: Loss = -11267.325096255096
1
Iteration 19200: Loss = -11267.324345842839
2
Iteration 19300: Loss = -11267.32346108006
Iteration 19400: Loss = -11267.324563961676
1
Iteration 19500: Loss = -11267.323456339567
Iteration 19600: Loss = -11267.323485394534
Iteration 19700: Loss = -11267.308868347402
Iteration 19800: Loss = -11267.302731633126
Iteration 19900: Loss = -11267.30420853276
1
pi: tensor([[0.4526, 0.5474],
        [0.4945, 0.5055]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4812, 0.5188], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2338, 0.0949],
         [0.5434, 0.2701]],

        [[0.5392, 0.1003],
         [0.6305, 0.6543]],

        [[0.6383, 0.0953],
         [0.6587, 0.5530]],

        [[0.6105, 0.0985],
         [0.6815, 0.5946]],

        [[0.6604, 0.0928],
         [0.5631, 0.5235]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7719210443888802
Global Adjusted Rand Index: 0.29890052417620083
Average Adjusted Rand Index: 0.8377762133489011
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23151.720086842342
Iteration 100: Loss = -11505.326795253455
Iteration 200: Loss = -11503.92529282941
Iteration 300: Loss = -11502.9411929172
Iteration 400: Loss = -11499.526191692417
Iteration 500: Loss = -11495.952666162357
Iteration 600: Loss = -11489.279792001056
Iteration 700: Loss = -11400.292652432461
Iteration 800: Loss = -11321.548900499889
Iteration 900: Loss = -11296.12462502325
Iteration 1000: Loss = -11294.568541023922
Iteration 1100: Loss = -11294.228343071021
Iteration 1200: Loss = -11291.899001897724
Iteration 1300: Loss = -11288.21034104637
Iteration 1400: Loss = -11288.084230987053
Iteration 1500: Loss = -11288.012182245806
Iteration 1600: Loss = -11287.959722582862
Iteration 1700: Loss = -11287.914376344872
Iteration 1800: Loss = -11287.869880375016
Iteration 1900: Loss = -11287.838136850716
Iteration 2000: Loss = -11287.810844555306
Iteration 2100: Loss = -11287.755908898558
Iteration 2200: Loss = -11287.316872857218
Iteration 2300: Loss = -11287.301965516344
Iteration 2400: Loss = -11287.290096293538
Iteration 2500: Loss = -11287.277729870513
Iteration 2600: Loss = -11287.114168454573
Iteration 2700: Loss = -11287.101557109598
Iteration 2800: Loss = -11287.09362258163
Iteration 2900: Loss = -11287.088052134866
Iteration 3000: Loss = -11287.08301400681
Iteration 3100: Loss = -11287.078476381597
Iteration 3200: Loss = -11287.077075275585
Iteration 3300: Loss = -11287.07038797846
Iteration 3400: Loss = -11287.07448190253
1
Iteration 3500: Loss = -11287.063388758066
Iteration 3600: Loss = -11287.061638459556
Iteration 3700: Loss = -11287.057660315202
Iteration 3800: Loss = -11287.053531921976
Iteration 3900: Loss = -11287.050002560462
Iteration 4000: Loss = -11287.046491641919
Iteration 4100: Loss = -11287.043687063346
Iteration 4200: Loss = -11287.042178055446
Iteration 4300: Loss = -11287.038580860863
Iteration 4400: Loss = -11287.037171730615
Iteration 4500: Loss = -11287.036071168024
Iteration 4600: Loss = -11287.035587854887
Iteration 4700: Loss = -11287.036610953579
1
Iteration 4800: Loss = -11287.044181179266
2
Iteration 4900: Loss = -11287.030455105885
Iteration 5000: Loss = -11287.02659423766
Iteration 5100: Loss = -11287.024576457909
Iteration 5200: Loss = -11287.024258016787
Iteration 5300: Loss = -11287.019600913041
Iteration 5400: Loss = -11287.01628020997
Iteration 5500: Loss = -11287.01711049762
1
Iteration 5600: Loss = -11287.014462293166
Iteration 5700: Loss = -11287.0136290623
Iteration 5800: Loss = -11287.012766095202
Iteration 5900: Loss = -11287.012410977073
Iteration 6000: Loss = -11287.005773563911
Iteration 6100: Loss = -11287.00265774973
Iteration 6200: Loss = -11287.003265756834
1
Iteration 6300: Loss = -11286.99993882167
Iteration 6400: Loss = -11286.997681618055
Iteration 6500: Loss = -11286.992137427556
Iteration 6600: Loss = -11286.977904110472
Iteration 6700: Loss = -11285.658221496216
Iteration 6800: Loss = -11281.413134340395
Iteration 6900: Loss = -11278.976968346022
Iteration 7000: Loss = -11271.061443921386
Iteration 7100: Loss = -11265.651099335506
Iteration 7200: Loss = -11258.034825213843
Iteration 7300: Loss = -11258.017478527367
Iteration 7400: Loss = -11258.015833727854
Iteration 7500: Loss = -11258.03834559464
1
Iteration 7600: Loss = -11258.014053296663
Iteration 7700: Loss = -11258.013532217641
Iteration 7800: Loss = -11258.013097730276
Iteration 7900: Loss = -11258.012397475386
Iteration 8000: Loss = -11257.997279165967
Iteration 8100: Loss = -11254.378812031178
Iteration 8200: Loss = -11254.37761111023
Iteration 8300: Loss = -11254.379000570485
1
Iteration 8400: Loss = -11254.376983429094
Iteration 8500: Loss = -11254.37677797036
Iteration 8600: Loss = -11254.377048064673
1
Iteration 8700: Loss = -11254.376218970498
Iteration 8800: Loss = -11254.375775277538
Iteration 8900: Loss = -11254.563208366892
1
Iteration 9000: Loss = -11254.375188493685
Iteration 9100: Loss = -11254.363528100208
Iteration 9200: Loss = -11254.365250360515
1
Iteration 9300: Loss = -11254.359896828468
Iteration 9400: Loss = -11254.584305890976
1
Iteration 9500: Loss = -11254.359676046644
Iteration 9600: Loss = -11254.359956874343
1
Iteration 9700: Loss = -11254.359661062186
Iteration 9800: Loss = -11254.359490469755
Iteration 9900: Loss = -11254.360125916546
1
Iteration 10000: Loss = -11254.359296843384
Iteration 10100: Loss = -11254.38412267513
1
Iteration 10200: Loss = -11254.358717919355
Iteration 10300: Loss = -11254.358143469442
Iteration 10400: Loss = -11254.357712144338
Iteration 10500: Loss = -11254.370355614668
1
Iteration 10600: Loss = -11254.357599502344
Iteration 10700: Loss = -11254.36726056456
1
Iteration 10800: Loss = -11254.366761452347
2
Iteration 10900: Loss = -11254.358104140612
3
Iteration 11000: Loss = -11254.35800225588
4
Iteration 11100: Loss = -11254.359877867508
5
Iteration 11200: Loss = -11254.359631868407
6
Iteration 11300: Loss = -11254.373333752115
7
Iteration 11400: Loss = -11254.358661240132
8
Iteration 11500: Loss = -11254.37265390338
9
Iteration 11600: Loss = -11254.357266006586
Iteration 11700: Loss = -11254.35746290538
1
Iteration 11800: Loss = -11254.357968655808
2
Iteration 11900: Loss = -11254.357467584314
3
Iteration 12000: Loss = -11254.357614074905
4
Iteration 12100: Loss = -11254.362180180588
5
Iteration 12200: Loss = -11254.357281414095
Iteration 12300: Loss = -11254.40703050761
1
Iteration 12400: Loss = -11254.38880441601
2
Iteration 12500: Loss = -11254.371854281993
3
Iteration 12600: Loss = -11254.353698476089
Iteration 12700: Loss = -11254.344731596673
Iteration 12800: Loss = -11254.344748440393
Iteration 12900: Loss = -11254.34507397936
1
Iteration 13000: Loss = -11254.368756028309
2
Iteration 13100: Loss = -11254.347641206035
3
Iteration 13200: Loss = -11254.349261557645
4
Iteration 13300: Loss = -11254.419905261351
5
Iteration 13400: Loss = -11254.357672435126
6
Iteration 13500: Loss = -11254.344772080341
Iteration 13600: Loss = -11254.412098279598
1
Iteration 13700: Loss = -11254.34477970987
Iteration 13800: Loss = -11254.344786748481
Iteration 13900: Loss = -11254.346064536228
1
Iteration 14000: Loss = -11254.377850546547
2
Iteration 14100: Loss = -11254.350897929775
3
Iteration 14200: Loss = -11254.508285183618
4
Iteration 14300: Loss = -11254.34699603615
5
Iteration 14400: Loss = -11254.356003701272
6
Iteration 14500: Loss = -11254.344836177585
Iteration 14600: Loss = -11254.348600302386
1
Iteration 14700: Loss = -11254.428105833129
2
Iteration 14800: Loss = -11254.546693750737
3
Iteration 14900: Loss = -11254.344645067997
Iteration 15000: Loss = -11254.344687387125
Iteration 15100: Loss = -11254.344613853282
Iteration 15200: Loss = -11254.344925832269
1
Iteration 15300: Loss = -11254.345727929833
2
Iteration 15400: Loss = -11254.351799873953
3
Iteration 15500: Loss = -11254.350865375347
4
Iteration 15600: Loss = -11254.747037164801
5
Iteration 15700: Loss = -11254.344290286732
Iteration 15800: Loss = -11254.345319623031
1
Iteration 15900: Loss = -11254.344390967057
2
Iteration 16000: Loss = -11254.344358483308
Iteration 16100: Loss = -11254.346444085593
1
Iteration 16200: Loss = -11254.344254253563
Iteration 16300: Loss = -11254.370201464142
1
Iteration 16400: Loss = -11254.344244247057
Iteration 16500: Loss = -11254.347800599167
1
Iteration 16600: Loss = -11254.35082898005
2
Iteration 16700: Loss = -11254.344180413415
Iteration 16800: Loss = -11254.351268196295
1
Iteration 16900: Loss = -11254.344143163238
Iteration 17000: Loss = -11254.347890512447
1
Iteration 17100: Loss = -11254.344119563168
Iteration 17200: Loss = -11254.345903732368
1
Iteration 17300: Loss = -11254.344758349278
2
Iteration 17400: Loss = -11254.34425056066
3
Iteration 17500: Loss = -11254.344237911973
4
Iteration 17600: Loss = -11254.340493346182
Iteration 17700: Loss = -11254.355690094906
1
Iteration 17800: Loss = -11254.340390371482
Iteration 17900: Loss = -11254.358859256332
1
Iteration 18000: Loss = -11254.36232069805
2
Iteration 18100: Loss = -11254.343152935917
3
Iteration 18200: Loss = -11254.340453498928
Iteration 18300: Loss = -11254.341710759458
1
Iteration 18400: Loss = -11254.34043194633
Iteration 18500: Loss = -11254.34071365865
1
Iteration 18600: Loss = -11254.34047012115
Iteration 18700: Loss = -11254.344271451462
1
Iteration 18800: Loss = -11254.340415993012
Iteration 18900: Loss = -11254.383782290824
1
Iteration 19000: Loss = -11254.34296588651
2
Iteration 19100: Loss = -11254.340706279772
3
Iteration 19200: Loss = -11254.34077085797
4
Iteration 19300: Loss = -11254.340497662017
Iteration 19400: Loss = -11254.343311119566
1
Iteration 19500: Loss = -11254.34038632453
Iteration 19600: Loss = -11254.354540518652
1
Iteration 19700: Loss = -11254.340379644333
Iteration 19800: Loss = -11254.358751633536
1
Iteration 19900: Loss = -11254.340362918156
pi: tensor([[0.5818, 0.4182],
        [0.2981, 0.7019]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6408, 0.3592], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2276, 0.0952],
         [0.5689, 0.2726]],

        [[0.6014, 0.1008],
         [0.5419, 0.6101]],

        [[0.6933, 0.0962],
         [0.6694, 0.6343]],

        [[0.5804, 0.0988],
         [0.5983, 0.6541]],

        [[0.5854, 0.0927],
         [0.6927, 0.5520]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 15
Adjusted Rand Index: 0.4851933910944513
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7719210443888802
Global Adjusted Rand Index: 0.3635434951387375
Average Adjusted Rand Index: 0.7887148234589674
11229.264400903518
[0.29890052417620083, 0.3635434951387375] [0.8377762133489011, 0.7887148234589674] [11267.303122832327, 11254.343973076755]
-------------------------------------
This iteration is 19
True Objective function: Loss = -11148.981464588333
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22361.623016790676
Iteration 100: Loss = -11454.660103127739
Iteration 200: Loss = -11440.517490366328
Iteration 300: Loss = -11337.632932263341
Iteration 400: Loss = -11217.365482118561
Iteration 500: Loss = -11201.596772266874
Iteration 600: Loss = -11192.751455514242
Iteration 700: Loss = -11192.303472407844
Iteration 800: Loss = -11192.183501632784
Iteration 900: Loss = -11192.11339780774
Iteration 1000: Loss = -11192.068152628639
Iteration 1100: Loss = -11192.036269797503
Iteration 1200: Loss = -11192.012298840222
Iteration 1300: Loss = -11191.993048409242
Iteration 1400: Loss = -11191.977074915327
Iteration 1500: Loss = -11191.958844854364
Iteration 1600: Loss = -11191.906787577473
Iteration 1700: Loss = -11191.897315971219
Iteration 1800: Loss = -11191.888964183998
Iteration 1900: Loss = -11191.880665723047
Iteration 2000: Loss = -11191.871618979465
Iteration 2100: Loss = -11191.860409482291
Iteration 2200: Loss = -11191.84265660041
Iteration 2300: Loss = -11191.797915445306
Iteration 2400: Loss = -11191.290944938071
Iteration 2500: Loss = -11190.31867964675
Iteration 2600: Loss = -11190.22999594725
Iteration 2700: Loss = -11190.210825499014
Iteration 2800: Loss = -11189.979018178212
Iteration 2900: Loss = -11189.972263638125
Iteration 3000: Loss = -11189.96815011085
Iteration 3100: Loss = -11189.967645565335
Iteration 3200: Loss = -11189.963366546153
Iteration 3300: Loss = -11189.961859260737
Iteration 3400: Loss = -11189.983467255002
1
Iteration 3500: Loss = -11189.95879450688
Iteration 3600: Loss = -11189.95821647009
Iteration 3700: Loss = -11189.958800658356
1
Iteration 3800: Loss = -11189.954688983295
Iteration 3900: Loss = -11189.95517661647
1
Iteration 4000: Loss = -11189.954146202157
Iteration 4100: Loss = -11189.957711484094
1
Iteration 4200: Loss = -11189.952693668589
Iteration 4300: Loss = -11189.952776905926
Iteration 4400: Loss = -11189.952235537765
Iteration 4500: Loss = -11189.951736122875
Iteration 4600: Loss = -11189.951353152253
Iteration 4700: Loss = -11189.951122181172
Iteration 4800: Loss = -11189.951099203336
Iteration 4900: Loss = -11189.950691703963
Iteration 5000: Loss = -11189.951996252634
1
Iteration 5100: Loss = -11189.950256345293
Iteration 5200: Loss = -11189.950160435223
Iteration 5300: Loss = -11189.949959042513
Iteration 5400: Loss = -11189.951368762697
1
Iteration 5500: Loss = -11189.949636824469
Iteration 5600: Loss = -11189.952655009407
1
Iteration 5700: Loss = -11189.954740364903
2
Iteration 5800: Loss = -11189.950434365508
3
Iteration 5900: Loss = -11189.949179266869
Iteration 6000: Loss = -11189.949156326169
Iteration 6100: Loss = -11189.948906522102
Iteration 6200: Loss = -11189.948873695552
Iteration 6300: Loss = -11189.948449398102
Iteration 6400: Loss = -11189.947450873646
Iteration 6500: Loss = -11189.944490779955
Iteration 6600: Loss = -11189.94437850008
Iteration 6700: Loss = -11189.945392467664
1
Iteration 6800: Loss = -11189.944019112616
Iteration 6900: Loss = -11189.943926540594
Iteration 7000: Loss = -11189.94631494473
1
Iteration 7100: Loss = -11189.943819708758
Iteration 7200: Loss = -11189.944424530233
1
Iteration 7300: Loss = -11189.94372437865
Iteration 7400: Loss = -11189.943729052706
Iteration 7500: Loss = -11189.943828321375
Iteration 7600: Loss = -11189.966038897734
1
Iteration 7700: Loss = -11189.943588828419
Iteration 7800: Loss = -11189.944803296432
1
Iteration 7900: Loss = -11189.943555178435
Iteration 8000: Loss = -11189.943969535601
1
Iteration 8100: Loss = -11189.943849985637
2
Iteration 8200: Loss = -11189.956174305795
3
Iteration 8300: Loss = -11189.943705654496
4
Iteration 8400: Loss = -11189.962258272342
5
Iteration 8500: Loss = -11189.94340171903
Iteration 8600: Loss = -11189.943926401418
1
Iteration 8700: Loss = -11189.943346876347
Iteration 8800: Loss = -11189.94354858248
1
Iteration 8900: Loss = -11189.943451322222
2
Iteration 9000: Loss = -11189.94350414275
3
Iteration 9100: Loss = -11189.949564755823
4
Iteration 9200: Loss = -11189.980110763818
5
Iteration 9300: Loss = -11189.94325064107
Iteration 9400: Loss = -11189.943397918987
1
Iteration 9500: Loss = -11189.946122836463
2
Iteration 9600: Loss = -11189.94342033843
3
Iteration 9700: Loss = -11189.94376877422
4
Iteration 9800: Loss = -11190.000426135311
5
Iteration 9900: Loss = -11189.947243086906
6
Iteration 10000: Loss = -11190.037329977855
7
Iteration 10100: Loss = -11189.942954088325
Iteration 10200: Loss = -11189.947496807737
1
Iteration 10300: Loss = -11189.94294096139
Iteration 10400: Loss = -11189.962755585
1
Iteration 10500: Loss = -11189.94292735318
Iteration 10600: Loss = -11189.959286982707
1
Iteration 10700: Loss = -11189.945811752083
2
Iteration 10800: Loss = -11189.944497084707
3
Iteration 10900: Loss = -11189.942902329241
Iteration 11000: Loss = -11189.94351121614
1
Iteration 11100: Loss = -11189.990990477736
2
Iteration 11200: Loss = -11189.945051939025
3
Iteration 11300: Loss = -11189.94601111319
4
Iteration 11400: Loss = -11189.943396876883
5
Iteration 11500: Loss = -11189.942897549576
Iteration 11600: Loss = -11189.943276208782
1
Iteration 11700: Loss = -11189.992890764555
2
Iteration 11800: Loss = -11189.95183445483
3
Iteration 11900: Loss = -11189.940826855602
Iteration 12000: Loss = -11189.943318057813
1
Iteration 12100: Loss = -11190.006686242205
2
Iteration 12200: Loss = -11189.941012135714
3
Iteration 12300: Loss = -11189.941042787683
4
Iteration 12400: Loss = -11189.975268292903
5
Iteration 12500: Loss = -11189.941127389857
6
Iteration 12600: Loss = -11189.940812380026
Iteration 12700: Loss = -11189.942920905578
1
Iteration 12800: Loss = -11189.949741194954
2
Iteration 12900: Loss = -11189.944435936966
3
Iteration 13000: Loss = -11189.940696473452
Iteration 13100: Loss = -11189.954204538108
1
Iteration 13200: Loss = -11189.941957266281
2
Iteration 13300: Loss = -11189.942117492967
3
Iteration 13400: Loss = -11190.087836269518
4
Iteration 13500: Loss = -11189.945070361418
5
Iteration 13600: Loss = -11189.94063874322
Iteration 13700: Loss = -11189.940987952848
1
Iteration 13800: Loss = -11189.940895409294
2
Iteration 13900: Loss = -11189.977653797034
3
Iteration 14000: Loss = -11189.952612261968
4
Iteration 14100: Loss = -11189.94050215504
Iteration 14200: Loss = -11189.940528215699
Iteration 14300: Loss = -11189.967013760379
1
Iteration 14400: Loss = -11189.989544669259
2
Iteration 14500: Loss = -11189.950580373796
3
Iteration 14600: Loss = -11189.940458682335
Iteration 14700: Loss = -11189.941892341853
1
Iteration 14800: Loss = -11189.951569655981
2
Iteration 14900: Loss = -11189.940792908546
3
Iteration 15000: Loss = -11189.940634419716
4
Iteration 15100: Loss = -11189.967261646545
5
Iteration 15200: Loss = -11189.940454407817
Iteration 15300: Loss = -11189.940551032296
Iteration 15400: Loss = -11189.987351941616
1
Iteration 15500: Loss = -11189.940688524846
2
Iteration 15600: Loss = -11190.011063665683
3
Iteration 15700: Loss = -11189.940536512939
Iteration 15800: Loss = -11189.941109386806
1
Iteration 15900: Loss = -11189.945038310161
2
Iteration 16000: Loss = -11189.940735509252
3
Iteration 16100: Loss = -11189.943524420696
4
Iteration 16200: Loss = -11189.940344241963
Iteration 16300: Loss = -11189.94039183834
Iteration 16400: Loss = -11189.946567518178
1
Iteration 16500: Loss = -11189.940567251522
2
Iteration 16600: Loss = -11189.940397487258
Iteration 16700: Loss = -11189.942343209656
1
Iteration 16800: Loss = -11189.94037900912
Iteration 16900: Loss = -11189.94044809458
Iteration 17000: Loss = -11190.06970272385
1
Iteration 17100: Loss = -11189.943330157108
2
Iteration 17200: Loss = -11189.94208010954
3
Iteration 17300: Loss = -11189.951276897456
4
Iteration 17400: Loss = -11189.946390858579
5
Iteration 17500: Loss = -11189.940697496646
6
Iteration 17600: Loss = -11189.940406024152
Iteration 17700: Loss = -11189.948290502964
1
Iteration 17800: Loss = -11189.94697585972
2
Iteration 17900: Loss = -11189.940494350567
Iteration 18000: Loss = -11189.94149135
1
Iteration 18100: Loss = -11189.942495379082
2
Iteration 18200: Loss = -11189.940515785773
Iteration 18300: Loss = -11189.941721006559
1
Iteration 18400: Loss = -11190.039955694734
2
Iteration 18500: Loss = -11189.94032418078
Iteration 18600: Loss = -11189.94307067172
1
Iteration 18700: Loss = -11189.945598793325
2
Iteration 18800: Loss = -11189.940536947175
3
Iteration 18900: Loss = -11189.94048065892
4
Iteration 19000: Loss = -11190.017920229751
5
Iteration 19100: Loss = -11189.942442434196
6
Iteration 19200: Loss = -11189.94671425862
7
Iteration 19300: Loss = -11189.941848547753
8
Iteration 19400: Loss = -11189.940906596
9
Iteration 19500: Loss = -11189.976309102582
10
Iteration 19600: Loss = -11190.114957556216
11
Iteration 19700: Loss = -11189.94034794087
Iteration 19800: Loss = -11189.940624706052
1
Iteration 19900: Loss = -11189.956683731965
2
pi: tensor([[0.6926, 0.3074],
        [0.1629, 0.8371]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9591, 0.0409], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1842, 0.1103],
         [0.6872, 0.3055]],

        [[0.6910, 0.1053],
         [0.7229, 0.6857]],

        [[0.6252, 0.0955],
         [0.6684, 0.6746]],

        [[0.6152, 0.1053],
         [0.5436, 0.7015]],

        [[0.6072, 0.0976],
         [0.6709, 0.6589]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080863220989386
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.5890249280988601
Average Adjusted Rand Index: 0.7290313068361701
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20208.798267171296
Iteration 100: Loss = -11455.040047200438
Iteration 200: Loss = -11451.174213441882
Iteration 300: Loss = -11442.230898395634
Iteration 400: Loss = -11262.638381642184
Iteration 500: Loss = -11233.21766792627
Iteration 600: Loss = -11219.586493457715
Iteration 700: Loss = -11218.056351557188
Iteration 800: Loss = -11208.847375949825
Iteration 900: Loss = -11202.838524150126
Iteration 1000: Loss = -11202.645412422115
Iteration 1100: Loss = -11202.59129759941
Iteration 1200: Loss = -11200.513730957697
Iteration 1300: Loss = -11200.316076658051
Iteration 1400: Loss = -11200.287118570164
Iteration 1500: Loss = -11200.258999551408
Iteration 1600: Loss = -11195.431238306734
Iteration 1700: Loss = -11195.386777791153
Iteration 1800: Loss = -11193.316066301144
Iteration 1900: Loss = -11193.2198540652
Iteration 2000: Loss = -11193.208701597145
Iteration 2100: Loss = -11193.200448132124
Iteration 2200: Loss = -11193.190720843953
Iteration 2300: Loss = -11193.16545182197
Iteration 2400: Loss = -11191.900597849026
Iteration 2500: Loss = -11191.735168415276
Iteration 2600: Loss = -11191.701630922498
Iteration 2700: Loss = -11191.687407608328
Iteration 2800: Loss = -11190.87786463669
Iteration 2900: Loss = -11190.814670495194
Iteration 3000: Loss = -11190.806570737164
Iteration 3100: Loss = -11190.788240350856
Iteration 3200: Loss = -11190.630638507144
Iteration 3300: Loss = -11190.61608806157
Iteration 3400: Loss = -11190.611062373353
Iteration 3500: Loss = -11190.596973451813
Iteration 3600: Loss = -11190.40083643249
Iteration 3700: Loss = -11190.397576476327
Iteration 3800: Loss = -11190.394556551546
Iteration 3900: Loss = -11190.373078001972
Iteration 4000: Loss = -11190.350416854255
Iteration 4100: Loss = -11190.339553265296
Iteration 4200: Loss = -11190.331729912832
Iteration 4300: Loss = -11190.326149921557
Iteration 4400: Loss = -11190.322706864372
Iteration 4500: Loss = -11190.317126651431
Iteration 4600: Loss = -11190.316253353834
Iteration 4700: Loss = -11190.31422555957
Iteration 4800: Loss = -11190.274800044435
Iteration 4900: Loss = -11190.266868070996
Iteration 5000: Loss = -11190.259554508637
Iteration 5100: Loss = -11190.207234500524
Iteration 5200: Loss = -11190.206692435824
Iteration 5300: Loss = -11190.24054663175
1
Iteration 5400: Loss = -11190.205688229898
Iteration 5500: Loss = -11190.204398355838
Iteration 5600: Loss = -11190.12872119064
Iteration 5700: Loss = -11190.087868443807
Iteration 5800: Loss = -11190.101298386493
1
Iteration 5900: Loss = -11190.086538879483
Iteration 6000: Loss = -11190.086266237058
Iteration 6100: Loss = -11190.085678329788
Iteration 6200: Loss = -11190.095310484136
1
Iteration 6300: Loss = -11190.073580001443
Iteration 6400: Loss = -11190.069702070274
Iteration 6500: Loss = -11190.056509765824
Iteration 6600: Loss = -11190.056040126254
Iteration 6700: Loss = -11190.055088928972
Iteration 6800: Loss = -11190.03914930521
Iteration 6900: Loss = -11190.038867463149
Iteration 7000: Loss = -11190.057248599882
1
Iteration 7100: Loss = -11190.038652816238
Iteration 7200: Loss = -11190.038620166151
Iteration 7300: Loss = -11190.040653058988
1
Iteration 7400: Loss = -11190.038451056558
Iteration 7500: Loss = -11190.038350254197
Iteration 7600: Loss = -11190.038189484638
Iteration 7700: Loss = -11190.038943611713
1
Iteration 7800: Loss = -11190.004258899968
Iteration 7900: Loss = -11189.997386403156
Iteration 8000: Loss = -11189.997479812157
Iteration 8100: Loss = -11190.02898745648
1
Iteration 8200: Loss = -11189.996813666648
Iteration 8300: Loss = -11189.986830211448
Iteration 8400: Loss = -11189.982694195147
Iteration 8500: Loss = -11189.975077885216
Iteration 8600: Loss = -11189.975440436483
1
Iteration 8700: Loss = -11189.961299113087
Iteration 8800: Loss = -11189.961388386628
Iteration 8900: Loss = -11189.960949847396
Iteration 9000: Loss = -11189.96026527123
Iteration 9100: Loss = -11189.959069451777
Iteration 9200: Loss = -11189.957308273164
Iteration 9300: Loss = -11189.974752198721
1
Iteration 9400: Loss = -11189.956392116868
Iteration 9500: Loss = -11189.956112355347
Iteration 9600: Loss = -11189.951024207154
Iteration 9700: Loss = -11190.155278548722
1
Iteration 9800: Loss = -11189.945045523675
Iteration 9900: Loss = -11189.944721139098
Iteration 10000: Loss = -11190.01907183746
1
Iteration 10100: Loss = -11189.944168147747
Iteration 10200: Loss = -11189.944225580144
Iteration 10300: Loss = -11189.944241826464
Iteration 10400: Loss = -11189.944172106323
Iteration 10500: Loss = -11189.953246161189
1
Iteration 10600: Loss = -11189.944111673001
Iteration 10700: Loss = -11189.944454594988
1
Iteration 10800: Loss = -11189.94446461323
2
Iteration 10900: Loss = -11189.946482639449
3
Iteration 11000: Loss = -11189.967251287995
4
Iteration 11100: Loss = -11189.945134333733
5
Iteration 11200: Loss = -11189.944223967925
6
Iteration 11300: Loss = -11189.964252662467
7
Iteration 11400: Loss = -11189.949011164106
8
Iteration 11500: Loss = -11189.943936473794
Iteration 11600: Loss = -11189.943663764618
Iteration 11700: Loss = -11189.955160885598
1
Iteration 11800: Loss = -11189.944762851212
2
Iteration 11900: Loss = -11189.943907402467
3
Iteration 12000: Loss = -11189.949967717625
4
Iteration 12100: Loss = -11190.005454461638
5
Iteration 12200: Loss = -11189.956785943814
6
Iteration 12300: Loss = -11190.005371837891
7
Iteration 12400: Loss = -11189.958837349302
8
Iteration 12500: Loss = -11189.978831891358
9
Iteration 12600: Loss = -11189.947222184386
10
Iteration 12700: Loss = -11189.944836073299
11
Iteration 12800: Loss = -11189.942887214622
Iteration 12900: Loss = -11189.942777032464
Iteration 13000: Loss = -11189.942985639562
1
Iteration 13100: Loss = -11189.946193798334
2
Iteration 13200: Loss = -11189.942862934511
Iteration 13300: Loss = -11189.94947218764
1
Iteration 13400: Loss = -11189.9430217279
2
Iteration 13500: Loss = -11189.942733269274
Iteration 13600: Loss = -11189.946443936318
1
Iteration 13700: Loss = -11189.962523158196
2
Iteration 13800: Loss = -11189.9435188233
3
Iteration 13900: Loss = -11189.941418005586
Iteration 14000: Loss = -11189.941418188506
Iteration 14100: Loss = -11189.94173933078
1
Iteration 14200: Loss = -11189.968370443412
2
Iteration 14300: Loss = -11189.94104008469
Iteration 14400: Loss = -11189.94169787029
1
Iteration 14500: Loss = -11189.940851664795
Iteration 14600: Loss = -11189.942148484362
1
Iteration 14700: Loss = -11189.94353417004
2
Iteration 14800: Loss = -11189.946265645347
3
Iteration 14900: Loss = -11189.949849983048
4
Iteration 15000: Loss = -11189.947123364547
5
Iteration 15100: Loss = -11189.945497475588
6
Iteration 15200: Loss = -11189.940896115608
Iteration 15300: Loss = -11189.946833910115
1
Iteration 15400: Loss = -11189.944711479033
2
Iteration 15500: Loss = -11189.956494645443
3
Iteration 15600: Loss = -11189.94151129502
4
Iteration 15700: Loss = -11189.944633432322
5
Iteration 15800: Loss = -11189.941368959571
6
Iteration 15900: Loss = -11189.940836622927
Iteration 16000: Loss = -11189.973730671125
1
Iteration 16100: Loss = -11190.059347003136
2
Iteration 16200: Loss = -11189.940841723726
Iteration 16300: Loss = -11189.940630243394
Iteration 16400: Loss = -11189.946461038991
1
Iteration 16500: Loss = -11189.966058560585
2
Iteration 16600: Loss = -11189.941907279705
3
Iteration 16700: Loss = -11189.940770337256
4
Iteration 16800: Loss = -11190.078964878565
5
Iteration 16900: Loss = -11189.94153249993
6
Iteration 17000: Loss = -11189.944670180474
7
Iteration 17100: Loss = -11189.941461198086
8
Iteration 17200: Loss = -11189.942013353982
9
Iteration 17300: Loss = -11189.940909803267
10
Iteration 17400: Loss = -11189.947757866754
11
Iteration 17500: Loss = -11189.944275519
12
Iteration 17600: Loss = -11189.940639947701
Iteration 17700: Loss = -11189.943119183485
1
Iteration 17800: Loss = -11189.94347043295
2
Iteration 17900: Loss = -11189.965076672466
3
Iteration 18000: Loss = -11189.940717399804
Iteration 18100: Loss = -11189.940820407948
1
Iteration 18200: Loss = -11189.975295939652
2
Iteration 18300: Loss = -11189.940556539945
Iteration 18400: Loss = -11190.015780323241
1
Iteration 18500: Loss = -11189.940587685676
Iteration 18600: Loss = -11189.967603280515
1
Iteration 18700: Loss = -11189.982973956794
2
Iteration 18800: Loss = -11189.94375124364
3
Iteration 18900: Loss = -11189.941019819202
4
Iteration 19000: Loss = -11189.944716454513
5
Iteration 19100: Loss = -11189.947630638148
6
Iteration 19200: Loss = -11189.943646884529
7
Iteration 19300: Loss = -11189.94096515677
8
Iteration 19400: Loss = -11189.95425932679
9
Iteration 19500: Loss = -11189.955347649156
10
Iteration 19600: Loss = -11189.940737807216
11
Iteration 19700: Loss = -11189.940461101689
Iteration 19800: Loss = -11189.946972026708
1
Iteration 19900: Loss = -11189.940578668946
2
pi: tensor([[0.8380, 0.1620],
        [0.3109, 0.6891]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0406, 0.9594], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3055, 0.1097],
         [0.5406, 0.1839]],

        [[0.6690, 0.1050],
         [0.7207, 0.6607]],

        [[0.6656, 0.0956],
         [0.5278, 0.6566]],

        [[0.6112, 0.1056],
         [0.7243, 0.6393]],

        [[0.7058, 0.0975],
         [0.6977, 0.6872]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080863220989386
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.5890249280988601
Average Adjusted Rand Index: 0.7290313068361701
11148.981464588333
[0.5890249280988601, 0.5890249280988601] [0.7290313068361701, 0.7290313068361701] [11189.940349464207, 11189.958640931922]
-------------------------------------
This iteration is 20
True Objective function: Loss = -11180.596596344172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20607.117496937306
Iteration 100: Loss = -11471.448761349562
Iteration 200: Loss = -11470.180658948695
Iteration 300: Loss = -11458.461091902569
Iteration 400: Loss = -11369.661135813352
Iteration 500: Loss = -11255.34372536639
Iteration 600: Loss = -11169.948512557698
Iteration 700: Loss = -11161.328278047613
Iteration 800: Loss = -11158.632763006064
Iteration 900: Loss = -11157.16588533584
Iteration 1000: Loss = -11157.025356180782
Iteration 1100: Loss = -11156.960707680031
Iteration 1200: Loss = -11156.919129673242
Iteration 1300: Loss = -11156.885079159234
Iteration 1400: Loss = -11156.738790872265
Iteration 1500: Loss = -11156.640801855954
Iteration 1600: Loss = -11156.555435146136
Iteration 1700: Loss = -11156.542315166607
Iteration 1800: Loss = -11156.52748266224
Iteration 1900: Loss = -11156.41534789577
Iteration 2000: Loss = -11156.40603193578
Iteration 2100: Loss = -11156.400302757955
Iteration 2200: Loss = -11156.395527479619
Iteration 2300: Loss = -11156.391386435407
Iteration 2400: Loss = -11156.387793297772
Iteration 2500: Loss = -11156.38467102515
Iteration 2600: Loss = -11156.392243647038
1
Iteration 2700: Loss = -11156.379313219491
Iteration 2800: Loss = -11156.377081785522
Iteration 2900: Loss = -11156.37508617917
Iteration 3000: Loss = -11156.373221758458
Iteration 3100: Loss = -11156.371522569425
Iteration 3200: Loss = -11156.369914498197
Iteration 3300: Loss = -11156.368396233594
Iteration 3400: Loss = -11156.365826018133
Iteration 3500: Loss = -11156.129287701864
Iteration 3600: Loss = -11156.118845884734
Iteration 3700: Loss = -11156.117341229725
Iteration 3800: Loss = -11156.117844948612
1
Iteration 3900: Loss = -11156.109817141425
Iteration 4000: Loss = -11156.106783316865
Iteration 4100: Loss = -11156.103923813607
Iteration 4200: Loss = -11156.10223684754
Iteration 4300: Loss = -11156.100543295997
Iteration 4400: Loss = -11156.09934286311
Iteration 4500: Loss = -11156.097667650718
Iteration 4600: Loss = -11156.096166115276
Iteration 4700: Loss = -11156.090905236795
Iteration 4800: Loss = -11156.0518114549
Iteration 4900: Loss = -11156.049008205893
Iteration 5000: Loss = -11156.045757565855
Iteration 5100: Loss = -11156.044609702478
Iteration 5200: Loss = -11156.043694288994
Iteration 5300: Loss = -11156.043475082715
Iteration 5400: Loss = -11156.043522667935
Iteration 5500: Loss = -11156.042638054416
Iteration 5600: Loss = -11156.042436380663
Iteration 5700: Loss = -11156.041813585462
Iteration 5800: Loss = -11156.029086625309
Iteration 5900: Loss = -11156.027908438213
Iteration 6000: Loss = -11156.031134018776
1
Iteration 6100: Loss = -11156.024969836419
Iteration 6200: Loss = -11156.022598517244
Iteration 6300: Loss = -11156.021070881903
Iteration 6400: Loss = -11156.02112808308
Iteration 6500: Loss = -11156.009039685454
Iteration 6600: Loss = -11156.01117159735
1
Iteration 6700: Loss = -11155.979467051391
Iteration 6800: Loss = -11155.982031664935
1
Iteration 6900: Loss = -11155.978716185797
Iteration 7000: Loss = -11155.757671358135
Iteration 7100: Loss = -11155.737405188154
Iteration 7200: Loss = -11155.735411574
Iteration 7300: Loss = -11155.734926606707
Iteration 7400: Loss = -11155.738127998266
1
Iteration 7500: Loss = -11155.734487269476
Iteration 7600: Loss = -11155.734418782042
Iteration 7700: Loss = -11155.73323580213
Iteration 7800: Loss = -11155.812282409403
1
Iteration 7900: Loss = -11155.732906206395
Iteration 8000: Loss = -11155.732677846201
Iteration 8100: Loss = -11155.864938478258
1
Iteration 8200: Loss = -11155.731054172222
Iteration 8300: Loss = -11155.730915247954
Iteration 8400: Loss = -11155.742689516917
1
Iteration 8500: Loss = -11155.730620770299
Iteration 8600: Loss = -11155.732137803123
1
Iteration 8700: Loss = -11155.767918691145
2
Iteration 8800: Loss = -11155.73416286832
3
Iteration 8900: Loss = -11155.729388470354
Iteration 9000: Loss = -11155.729528894408
1
Iteration 9100: Loss = -11155.7415753746
2
Iteration 9200: Loss = -11155.728364611563
Iteration 9300: Loss = -11155.728855649084
1
Iteration 9400: Loss = -11155.728442696312
Iteration 9500: Loss = -11155.727592012054
Iteration 9600: Loss = -11155.729678689884
1
Iteration 9700: Loss = -11155.725949613774
Iteration 9800: Loss = -11155.725839934381
Iteration 9900: Loss = -11155.784867820936
1
Iteration 10000: Loss = -11155.725739436653
Iteration 10100: Loss = -11155.725711720901
Iteration 10200: Loss = -11155.725823638859
1
Iteration 10300: Loss = -11155.725618064302
Iteration 10400: Loss = -11155.73436456098
1
Iteration 10500: Loss = -11155.718870751742
Iteration 10600: Loss = -11155.718258350507
Iteration 10700: Loss = -11155.717966324344
Iteration 10800: Loss = -11155.71136190583
Iteration 10900: Loss = -11156.03411264408
1
Iteration 11000: Loss = -11155.71125245488
Iteration 11100: Loss = -11155.71097532205
Iteration 11200: Loss = -11155.722132708368
1
Iteration 11300: Loss = -11155.710799175253
Iteration 11400: Loss = -11155.74095409758
1
Iteration 11500: Loss = -11155.710805875293
Iteration 11600: Loss = -11155.822464748358
1
Iteration 11700: Loss = -11155.710773775289
Iteration 11800: Loss = -11155.726013666073
1
Iteration 11900: Loss = -11155.712173807678
2
Iteration 12000: Loss = -11155.718319224903
3
Iteration 12100: Loss = -11155.77733075971
4
Iteration 12200: Loss = -11155.710736793677
Iteration 12300: Loss = -11155.751698160477
1
Iteration 12400: Loss = -11155.707690301235
Iteration 12500: Loss = -11155.759316136304
1
Iteration 12600: Loss = -11155.707639192973
Iteration 12700: Loss = -11155.71357723011
1
Iteration 12800: Loss = -11155.707653514193
Iteration 12900: Loss = -11155.707644795875
Iteration 13000: Loss = -11155.710992916238
1
Iteration 13100: Loss = -11155.706664128165
Iteration 13200: Loss = -11155.706654576887
Iteration 13300: Loss = -11155.706719024382
Iteration 13400: Loss = -11155.706663894114
Iteration 13500: Loss = -11155.707726578536
1
Iteration 13600: Loss = -11155.707172964288
2
Iteration 13700: Loss = -11155.726187760705
3
Iteration 13800: Loss = -11155.707435295233
4
Iteration 13900: Loss = -11155.706670353075
Iteration 14000: Loss = -11155.709216174087
1
Iteration 14100: Loss = -11155.747428024693
2
Iteration 14200: Loss = -11155.704366377158
Iteration 14300: Loss = -11155.712535474155
1
Iteration 14400: Loss = -11155.704392384207
Iteration 14500: Loss = -11155.739090102668
1
Iteration 14600: Loss = -11155.704375092477
Iteration 14700: Loss = -11155.70581670964
1
Iteration 14800: Loss = -11155.70439867424
Iteration 14900: Loss = -11155.704395588142
Iteration 15000: Loss = -11155.70444278232
Iteration 15100: Loss = -11155.714021054942
1
Iteration 15200: Loss = -11155.70819894748
2
Iteration 15300: Loss = -11155.704367438831
Iteration 15400: Loss = -11155.705903930184
1
Iteration 15500: Loss = -11155.704286437296
Iteration 15600: Loss = -11155.699814613265
Iteration 15700: Loss = -11155.70000056907
1
Iteration 15800: Loss = -11155.699771141994
Iteration 15900: Loss = -11155.699724916109
Iteration 16000: Loss = -11155.699915115423
1
Iteration 16100: Loss = -11155.699712696156
Iteration 16200: Loss = -11155.714701899376
1
Iteration 16300: Loss = -11155.699680391263
Iteration 16400: Loss = -11156.049027098687
1
Iteration 16500: Loss = -11155.698899170548
Iteration 16600: Loss = -11155.699174172345
1
Iteration 16700: Loss = -11155.698968682775
Iteration 16800: Loss = -11155.698933993654
Iteration 16900: Loss = -11155.699126296819
1
Iteration 17000: Loss = -11155.73738966776
2
Iteration 17100: Loss = -11155.698734766525
Iteration 17200: Loss = -11155.729868342893
1
Iteration 17300: Loss = -11155.7012022966
2
Iteration 17400: Loss = -11155.698774221697
Iteration 17500: Loss = -11155.69881902428
Iteration 17600: Loss = -11155.698302237623
Iteration 17700: Loss = -11155.71368790476
1
Iteration 17800: Loss = -11155.697446398915
Iteration 17900: Loss = -11155.697596682992
1
Iteration 18000: Loss = -11155.924662514768
2
Iteration 18100: Loss = -11155.69440958001
Iteration 18200: Loss = -11155.741525193593
1
Iteration 18300: Loss = -11155.694308914823
Iteration 18400: Loss = -11155.712758593694
1
Iteration 18500: Loss = -11155.694302469734
Iteration 18600: Loss = -11155.697069440828
1
Iteration 18700: Loss = -11155.69431710604
Iteration 18800: Loss = -11155.694269778523
Iteration 18900: Loss = -11155.694739277285
1
Iteration 19000: Loss = -11155.693320550297
Iteration 19100: Loss = -11155.699438038408
1
Iteration 19200: Loss = -11155.69329220994
Iteration 19300: Loss = -11155.697007183115
1
Iteration 19400: Loss = -11155.693327685898
Iteration 19500: Loss = -11155.9365237046
1
Iteration 19600: Loss = -11155.693276052392
Iteration 19700: Loss = -11155.695190763365
1
Iteration 19800: Loss = -11155.697492401425
2
Iteration 19900: Loss = -11155.693397250388
3
pi: tensor([[0.8050, 0.1950],
        [0.2635, 0.7365]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4261, 0.5739], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1956, 0.0953],
         [0.5431, 0.3088]],

        [[0.6803, 0.0954],
         [0.6267, 0.5725]],

        [[0.7018, 0.1013],
         [0.6592, 0.6693]],

        [[0.5328, 0.1133],
         [0.6368, 0.6786]],

        [[0.6606, 0.1027],
         [0.5473, 0.5219]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207385189720222
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9137635173901196
Average Adjusted Rand Index: 0.9140832463233968
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20958.370052608203
Iteration 100: Loss = -11471.248626297189
Iteration 200: Loss = -11470.118490556344
Iteration 300: Loss = -11459.065107701732
Iteration 400: Loss = -11413.100058944798
Iteration 500: Loss = -11369.748214006351
Iteration 600: Loss = -11229.957257616536
Iteration 700: Loss = -11159.549599848178
Iteration 800: Loss = -11157.175086207662
Iteration 900: Loss = -11156.747264365742
Iteration 1000: Loss = -11156.565324362058
Iteration 1100: Loss = -11156.46583472569
Iteration 1200: Loss = -11156.396971988237
Iteration 1300: Loss = -11156.34069359002
Iteration 1400: Loss = -11156.213976473731
Iteration 1500: Loss = -11156.162110545893
Iteration 1600: Loss = -11156.138926537196
Iteration 1700: Loss = -11156.115714231588
Iteration 1800: Loss = -11155.865918642778
Iteration 1900: Loss = -11155.853491195336
Iteration 2000: Loss = -11155.843406595877
Iteration 2100: Loss = -11155.834778546934
Iteration 2200: Loss = -11155.827110839473
Iteration 2300: Loss = -11155.819733086693
Iteration 2400: Loss = -11155.807494817172
Iteration 2500: Loss = -11155.785396117248
Iteration 2600: Loss = -11155.780809186012
Iteration 2700: Loss = -11155.777013434337
Iteration 2800: Loss = -11155.773638098073
Iteration 2900: Loss = -11155.770564658256
Iteration 3000: Loss = -11155.767811607642
Iteration 3100: Loss = -11155.772296562516
1
Iteration 3200: Loss = -11155.763010490811
Iteration 3300: Loss = -11155.762168602472
Iteration 3400: Loss = -11155.758917652562
Iteration 3500: Loss = -11155.756447602014
Iteration 3600: Loss = -11155.7528104822
Iteration 3700: Loss = -11155.748005217763
Iteration 3800: Loss = -11155.74619240472
Iteration 3900: Loss = -11155.74705449068
1
Iteration 4000: Loss = -11155.739271164148
Iteration 4100: Loss = -11155.737381997571
Iteration 4200: Loss = -11155.736373460852
Iteration 4300: Loss = -11155.735114428353
Iteration 4400: Loss = -11155.739445556022
1
Iteration 4500: Loss = -11155.733079159265
Iteration 4600: Loss = -11155.731802720373
Iteration 4700: Loss = -11155.732074082156
1
Iteration 4800: Loss = -11155.729581736337
Iteration 4900: Loss = -11155.72892198629
Iteration 5000: Loss = -11155.728303539763
Iteration 5100: Loss = -11155.729691077544
1
Iteration 5200: Loss = -11155.728878837515
2
Iteration 5300: Loss = -11155.725884823221
Iteration 5400: Loss = -11155.725067850723
Iteration 5500: Loss = -11155.724820682424
Iteration 5600: Loss = -11155.729218803768
1
Iteration 5700: Loss = -11155.724151257129
Iteration 5800: Loss = -11155.72430722744
1
Iteration 5900: Loss = -11155.723494295435
Iteration 6000: Loss = -11155.72367253061
1
Iteration 6100: Loss = -11155.722914392942
Iteration 6200: Loss = -11155.722668734958
Iteration 6300: Loss = -11155.726666903885
1
Iteration 6400: Loss = -11155.722138089017
Iteration 6500: Loss = -11155.721979266556
Iteration 6600: Loss = -11155.740090678617
1
Iteration 6700: Loss = -11155.72152763819
Iteration 6800: Loss = -11155.72163055909
1
Iteration 6900: Loss = -11155.721161245416
Iteration 7000: Loss = -11155.720966054581
Iteration 7100: Loss = -11155.721189717231
1
Iteration 7200: Loss = -11155.724716208797
2
Iteration 7300: Loss = -11155.72022671986
Iteration 7400: Loss = -11155.719839942065
Iteration 7500: Loss = -11155.719578272052
Iteration 7600: Loss = -11155.721217887794
1
Iteration 7700: Loss = -11155.71855831287
Iteration 7800: Loss = -11155.71777318865
Iteration 7900: Loss = -11155.7168803755
Iteration 8000: Loss = -11155.716815401778
Iteration 8100: Loss = -11155.716579509817
Iteration 8200: Loss = -11155.721363624407
1
Iteration 8300: Loss = -11155.716354889018
Iteration 8400: Loss = -11155.716362932964
Iteration 8500: Loss = -11155.71620564548
Iteration 8600: Loss = -11155.716069235943
Iteration 8700: Loss = -11155.72073578089
1
Iteration 8800: Loss = -11155.715482810447
Iteration 8900: Loss = -11155.712887340109
Iteration 9000: Loss = -11155.708329005547
Iteration 9100: Loss = -11155.70805137663
Iteration 9200: Loss = -11155.713508562187
1
Iteration 9300: Loss = -11155.707407963931
Iteration 9400: Loss = -11155.707700032513
1
Iteration 9500: Loss = -11155.714940085345
2
Iteration 9600: Loss = -11155.707254774246
Iteration 9700: Loss = -11155.707343112734
Iteration 9800: Loss = -11155.7056756726
Iteration 9900: Loss = -11155.702952597168
Iteration 10000: Loss = -11155.702144419356
Iteration 10100: Loss = -11155.702169320159
Iteration 10200: Loss = -11155.701865407504
Iteration 10300: Loss = -11155.70243064759
1
Iteration 10400: Loss = -11155.864563584812
2
Iteration 10500: Loss = -11155.70181307397
Iteration 10600: Loss = -11155.702417576282
1
Iteration 10700: Loss = -11155.698884151017
Iteration 10800: Loss = -11155.698753645778
Iteration 10900: Loss = -11155.698602237457
Iteration 11000: Loss = -11155.698582346664
Iteration 11100: Loss = -11155.692994626703
Iteration 11200: Loss = -11155.724325731284
1
Iteration 11300: Loss = -11155.692925247691
Iteration 11400: Loss = -11155.692895954322
Iteration 11500: Loss = -11155.693022821679
1
Iteration 11600: Loss = -11155.69287374081
Iteration 11700: Loss = -11155.694843101559
1
Iteration 11800: Loss = -11155.69285377031
Iteration 11900: Loss = -11155.692908637147
Iteration 12000: Loss = -11155.791550564898
1
Iteration 12100: Loss = -11155.69272765027
Iteration 12200: Loss = -11155.694391967547
1
Iteration 12300: Loss = -11155.70941679795
2
Iteration 12400: Loss = -11155.689064640039
Iteration 12500: Loss = -11155.689093660269
Iteration 12600: Loss = -11155.81882599918
1
Iteration 12700: Loss = -11155.688910410407
Iteration 12800: Loss = -11155.779873733325
1
Iteration 12900: Loss = -11155.686477447936
Iteration 13000: Loss = -11156.135587543688
1
Iteration 13100: Loss = -11155.686468766311
Iteration 13200: Loss = -11155.686435292686
Iteration 13300: Loss = -11155.688255229707
1
Iteration 13400: Loss = -11155.686435722226
Iteration 13500: Loss = -11155.979672946616
1
Iteration 13600: Loss = -11155.686447775824
Iteration 13700: Loss = -11155.708354491184
1
Iteration 13800: Loss = -11155.685167410866
Iteration 13900: Loss = -11155.687060047378
1
Iteration 14000: Loss = -11155.685382334348
2
Iteration 14100: Loss = -11155.685476099918
3
Iteration 14200: Loss = -11155.686380398281
4
Iteration 14300: Loss = -11155.685208356497
Iteration 14400: Loss = -11155.68513830426
Iteration 14500: Loss = -11155.685490969132
1
Iteration 14600: Loss = -11155.685136345097
Iteration 14700: Loss = -11155.685131507593
Iteration 14800: Loss = -11155.72153235584
1
Iteration 14900: Loss = -11155.685113640502
Iteration 15000: Loss = -11155.685094643335
Iteration 15100: Loss = -11155.685863185696
1
Iteration 15200: Loss = -11155.685081388287
Iteration 15300: Loss = -11155.686163053208
1
Iteration 15400: Loss = -11155.685532063502
2
Iteration 15500: Loss = -11155.694825611661
3
Iteration 15600: Loss = -11155.685081326965
Iteration 15700: Loss = -11155.718772901
1
Iteration 15800: Loss = -11155.68504335083
Iteration 15900: Loss = -11155.684831573546
Iteration 16000: Loss = -11155.684889178636
Iteration 16100: Loss = -11155.684823323696
Iteration 16200: Loss = -11155.702049581549
1
Iteration 16300: Loss = -11155.68471335593
Iteration 16400: Loss = -11155.684728886394
Iteration 16500: Loss = -11155.686043787546
1
Iteration 16600: Loss = -11155.684669276645
Iteration 16700: Loss = -11155.684680282919
Iteration 16800: Loss = -11155.684722665668
Iteration 16900: Loss = -11155.68469043676
Iteration 17000: Loss = -11155.689105476837
1
Iteration 17100: Loss = -11155.684686218372
Iteration 17200: Loss = -11155.686535385275
1
Iteration 17300: Loss = -11155.684667504443
Iteration 17400: Loss = -11155.687825359868
1
Iteration 17500: Loss = -11155.772574114932
2
Iteration 17600: Loss = -11155.828074723073
3
Iteration 17700: Loss = -11155.685207507453
4
Iteration 17800: Loss = -11155.684669192582
Iteration 17900: Loss = -11155.68979449807
1
Iteration 18000: Loss = -11155.685147446398
2
Iteration 18100: Loss = -11155.684621090657
Iteration 18200: Loss = -11155.73424088496
1
Iteration 18300: Loss = -11155.68452918945
Iteration 18400: Loss = -11155.714716934963
1
Iteration 18500: Loss = -11155.68452496178
Iteration 18600: Loss = -11155.684651368425
1
Iteration 18700: Loss = -11155.684580511039
Iteration 18800: Loss = -11155.684499770046
Iteration 18900: Loss = -11155.686178703336
1
Iteration 19000: Loss = -11155.684513585727
Iteration 19100: Loss = -11155.791638468128
1
Iteration 19200: Loss = -11155.684509888853
Iteration 19300: Loss = -11155.684491822074
Iteration 19400: Loss = -11155.684607118117
1
Iteration 19500: Loss = -11155.684490409021
Iteration 19600: Loss = -11155.684890378274
1
Iteration 19700: Loss = -11155.684709280062
2
Iteration 19800: Loss = -11155.699675069662
3
Iteration 19900: Loss = -11155.687123818267
4
pi: tensor([[0.8046, 0.1954],
        [0.2636, 0.7364]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4258, 0.5742], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.0953],
         [0.6453, 0.3087]],

        [[0.6992, 0.0953],
         [0.6683, 0.5722]],

        [[0.6819, 0.1013],
         [0.5438, 0.6656]],

        [[0.6344, 0.1132],
         [0.5757, 0.5080]],

        [[0.5351, 0.1027],
         [0.5188, 0.6902]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207385189720222
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9137635173901196
Average Adjusted Rand Index: 0.9140832463233968
11180.596596344172
[0.9137635173901196, 0.9137635173901196] [0.9140832463233968, 0.9140832463233968] [11155.693189686548, 11155.684903868429]
-------------------------------------
This iteration is 21
True Objective function: Loss = -11248.59547611485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21107.16506206066
Iteration 100: Loss = -11432.116109959414
Iteration 200: Loss = -11431.2736794355
Iteration 300: Loss = -11430.713644734615
Iteration 400: Loss = -11430.031075759001
Iteration 500: Loss = -11429.454351785767
Iteration 600: Loss = -11428.439896344607
Iteration 700: Loss = -11299.853883349018
Iteration 800: Loss = -11272.028209210735
Iteration 900: Loss = -11260.696840956556
Iteration 1000: Loss = -11258.821215319502
Iteration 1100: Loss = -11258.743139623739
Iteration 1200: Loss = -11258.697744424371
Iteration 1300: Loss = -11258.673969946272
Iteration 1400: Loss = -11258.646206803898
Iteration 1500: Loss = -11258.320189242264
Iteration 1600: Loss = -11258.30776561566
Iteration 1700: Loss = -11258.295224358313
Iteration 1800: Loss = -11258.27871049926
Iteration 1900: Loss = -11258.251743909817
Iteration 2000: Loss = -11258.188453348597
Iteration 2100: Loss = -11258.036317898535
Iteration 2200: Loss = -11257.939075086753
Iteration 2300: Loss = -11257.495079064189
Iteration 2400: Loss = -11256.058088569602
Iteration 2500: Loss = -11251.53483791068
Iteration 2600: Loss = -11251.483726747561
Iteration 2700: Loss = -11251.466187809816
Iteration 2800: Loss = -11251.452043720243
Iteration 2900: Loss = -11251.453094741573
1
Iteration 3000: Loss = -11251.439286160305
Iteration 3100: Loss = -11251.132303944909
Iteration 3200: Loss = -11251.113261345767
Iteration 3300: Loss = -11251.110710930247
Iteration 3400: Loss = -11251.105763579944
Iteration 3500: Loss = -11250.905169204818
Iteration 3600: Loss = -11250.897974821353
Iteration 3700: Loss = -11250.896120072808
Iteration 3800: Loss = -11250.894723687112
Iteration 3900: Loss = -11250.892970642983
Iteration 4000: Loss = -11250.885796422015
Iteration 4100: Loss = -11250.885383814471
Iteration 4200: Loss = -11250.884175036108
Iteration 4300: Loss = -11250.880278470202
Iteration 4400: Loss = -11250.838705474887
Iteration 4500: Loss = -11250.840907377675
1
Iteration 4600: Loss = -11250.837972620722
Iteration 4700: Loss = -11250.837639943054
Iteration 4800: Loss = -11250.83691369719
Iteration 4900: Loss = -11250.836393525386
Iteration 5000: Loss = -11250.836545347112
1
Iteration 5100: Loss = -11250.836063900331
Iteration 5200: Loss = -11250.835903279893
Iteration 5300: Loss = -11250.835756888884
Iteration 5400: Loss = -11250.835415520452
Iteration 5500: Loss = -11250.836663095246
1
Iteration 5600: Loss = -11250.830909942139
Iteration 5700: Loss = -11250.818323570657
Iteration 5800: Loss = -11250.816087437877
Iteration 5900: Loss = -11250.818271233942
1
Iteration 6000: Loss = -11250.81584458652
Iteration 6100: Loss = -11250.815696962523
Iteration 6200: Loss = -11250.816619178975
1
Iteration 6300: Loss = -11250.807016665185
Iteration 6400: Loss = -11250.751065907134
Iteration 6500: Loss = -11250.751022567894
Iteration 6600: Loss = -11250.750965780215
Iteration 6700: Loss = -11250.750971871892
Iteration 6800: Loss = -11250.750888401826
Iteration 6900: Loss = -11250.750966712261
Iteration 7000: Loss = -11250.749382041504
Iteration 7100: Loss = -11250.782610472543
1
Iteration 7200: Loss = -11250.748585607964
Iteration 7300: Loss = -11250.748561519147
Iteration 7400: Loss = -11250.75107883852
1
Iteration 7500: Loss = -11250.767949154957
2
Iteration 7600: Loss = -11250.74823179206
Iteration 7700: Loss = -11250.748035139806
Iteration 7800: Loss = -11250.747257193058
Iteration 7900: Loss = -11250.7468714128
Iteration 8000: Loss = -11250.746831339442
Iteration 8100: Loss = -11250.74685452317
Iteration 8200: Loss = -11250.746799108923
Iteration 8300: Loss = -11250.746800945055
Iteration 8400: Loss = -11250.748680787148
1
Iteration 8500: Loss = -11250.752383526687
2
Iteration 8600: Loss = -11250.744098235162
Iteration 8700: Loss = -11250.739888535081
Iteration 8800: Loss = -11250.758871851633
1
Iteration 8900: Loss = -11250.735097951303
Iteration 9000: Loss = -11250.73512209109
Iteration 9100: Loss = -11250.735579200262
1
Iteration 9200: Loss = -11250.74168605573
2
Iteration 9300: Loss = -11250.74502113926
3
Iteration 9400: Loss = -11250.764629820742
4
Iteration 9500: Loss = -11250.74721432522
5
Iteration 9600: Loss = -11250.732297930572
Iteration 9700: Loss = -11250.732728878967
1
Iteration 9800: Loss = -11250.737802120033
2
Iteration 9900: Loss = -11250.739248640562
3
Iteration 10000: Loss = -11250.732273587524
Iteration 10100: Loss = -11250.733359647096
1
Iteration 10200: Loss = -11250.732209862588
Iteration 10300: Loss = -11250.73039443604
Iteration 10400: Loss = -11250.73032954616
Iteration 10500: Loss = -11250.730375500054
Iteration 10600: Loss = -11250.733599200006
1
Iteration 10700: Loss = -11250.733535733474
2
Iteration 10800: Loss = -11250.898539666985
3
Iteration 10900: Loss = -11250.730255051318
Iteration 11000: Loss = -11250.735335396539
1
Iteration 11100: Loss = -11250.730346230688
Iteration 11200: Loss = -11250.88420869677
1
Iteration 11300: Loss = -11250.730262391706
Iteration 11400: Loss = -11250.730576335598
1
Iteration 11500: Loss = -11250.821011467768
2
Iteration 11600: Loss = -11250.730805436608
3
Iteration 11700: Loss = -11250.754980136835
4
Iteration 11800: Loss = -11250.731402501691
5
Iteration 11900: Loss = -11250.743827576036
6
Iteration 12000: Loss = -11250.729899275297
Iteration 12100: Loss = -11250.752199956061
1
Iteration 12200: Loss = -11250.72979223185
Iteration 12300: Loss = -11250.768379796158
1
Iteration 12400: Loss = -11250.729600621367
Iteration 12500: Loss = -11250.736972827926
1
Iteration 12600: Loss = -11250.761697344
2
Iteration 12700: Loss = -11250.732103824383
3
Iteration 12800: Loss = -11250.733523377627
4
Iteration 12900: Loss = -11250.733589357995
5
Iteration 13000: Loss = -11250.729630171256
Iteration 13100: Loss = -11250.729967023852
1
Iteration 13200: Loss = -11250.730085701756
2
Iteration 13300: Loss = -11250.731673237964
3
Iteration 13400: Loss = -11250.736599351103
4
Iteration 13500: Loss = -11250.729617291157
Iteration 13600: Loss = -11250.74654538015
1
Iteration 13700: Loss = -11250.72408293801
Iteration 13800: Loss = -11250.76895548781
1
Iteration 13900: Loss = -11250.72750397562
2
Iteration 14000: Loss = -11250.724641592024
3
Iteration 14100: Loss = -11250.752563977489
4
Iteration 14200: Loss = -11250.723452686578
Iteration 14300: Loss = -11250.728791812959
1
Iteration 14400: Loss = -11250.728904075233
2
Iteration 14500: Loss = -11250.725584359394
3
Iteration 14600: Loss = -11250.722866178325
Iteration 14700: Loss = -11250.723858311027
1
Iteration 14800: Loss = -11250.725751337957
2
Iteration 14900: Loss = -11250.725933128762
3
Iteration 15000: Loss = -11250.725618204711
4
Iteration 15100: Loss = -11250.733199836453
5
Iteration 15200: Loss = -11250.722913145713
Iteration 15300: Loss = -11250.723187945168
1
Iteration 15400: Loss = -11250.722839303467
Iteration 15500: Loss = -11250.722954017094
1
Iteration 15600: Loss = -11250.722840480004
Iteration 15700: Loss = -11250.724545841786
1
Iteration 15800: Loss = -11250.723163532302
2
Iteration 15900: Loss = -11250.724547948588
3
Iteration 16000: Loss = -11250.753415663796
4
Iteration 16100: Loss = -11250.722827326344
Iteration 16200: Loss = -11250.773202223954
1
Iteration 16300: Loss = -11250.733041355557
2
Iteration 16400: Loss = -11250.735168427314
3
Iteration 16500: Loss = -11250.722963164593
4
Iteration 16600: Loss = -11250.735063039258
5
Iteration 16700: Loss = -11250.731239282894
6
Iteration 16800: Loss = -11250.72284428708
Iteration 16900: Loss = -11250.72350237986
1
Iteration 17000: Loss = -11250.727007542959
2
Iteration 17100: Loss = -11250.722942723623
Iteration 17200: Loss = -11250.73770925966
1
Iteration 17300: Loss = -11250.722838475209
Iteration 17400: Loss = -11250.72358722131
1
Iteration 17500: Loss = -11250.723197798
2
Iteration 17600: Loss = -11250.722956602624
3
Iteration 17700: Loss = -11250.722894011951
Iteration 17800: Loss = -11250.72297840951
Iteration 17900: Loss = -11250.722864955578
Iteration 18000: Loss = -11250.727174794807
1
Iteration 18100: Loss = -11250.722854929816
Iteration 18200: Loss = -11250.723487967036
1
Iteration 18300: Loss = -11250.722884272456
Iteration 18400: Loss = -11250.725204631974
1
Iteration 18500: Loss = -11250.724283157973
2
Iteration 18600: Loss = -11250.722843693484
Iteration 18700: Loss = -11250.72591656489
1
Iteration 18800: Loss = -11250.722804676194
Iteration 18900: Loss = -11250.72390287076
1
Iteration 19000: Loss = -11250.722912996574
2
Iteration 19100: Loss = -11250.724034522047
3
Iteration 19200: Loss = -11250.722995755046
4
Iteration 19300: Loss = -11250.821885747435
5
Iteration 19400: Loss = -11250.722827696163
Iteration 19500: Loss = -11250.724655746275
1
Iteration 19600: Loss = -11250.900705765212
2
Iteration 19700: Loss = -11250.72282208681
Iteration 19800: Loss = -11250.723895734922
1
Iteration 19900: Loss = -11250.722992589293
2
pi: tensor([[0.7196, 0.2804],
        [0.2717, 0.7283]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8696, 0.1304], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1911, 0.1126],
         [0.6445, 0.3121]],

        [[0.5077, 0.1202],
         [0.6242, 0.5731]],

        [[0.5094, 0.1091],
         [0.6681, 0.6333]],

        [[0.6781, 0.0941],
         [0.5540, 0.7087]],

        [[0.5208, 0.1075],
         [0.5468, 0.7181]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.01106118216384376
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.58883637938024
Average Adjusted Rand Index: 0.7504308154002158
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22065.856579936277
Iteration 100: Loss = -11433.602887758536
Iteration 200: Loss = -11432.213409032782
Iteration 300: Loss = -11431.710576372612
Iteration 400: Loss = -11431.196920333065
Iteration 500: Loss = -11430.609201308358
Iteration 600: Loss = -11429.650838639676
Iteration 700: Loss = -11426.377838907229
Iteration 800: Loss = -11408.321950465002
Iteration 900: Loss = -11346.551241013318
Iteration 1000: Loss = -11290.988397935058
Iteration 1100: Loss = -11269.405308767604
Iteration 1200: Loss = -11263.728600294397
Iteration 1300: Loss = -11263.214011445747
Iteration 1400: Loss = -11258.870939157561
Iteration 1500: Loss = -11258.6481833345
Iteration 1600: Loss = -11258.379000266788
Iteration 1700: Loss = -11257.812052196403
Iteration 1800: Loss = -11252.225828043043
Iteration 1900: Loss = -11251.406222951631
Iteration 2000: Loss = -11251.33708312793
Iteration 2100: Loss = -11251.288184357063
Iteration 2200: Loss = -11251.190193597471
Iteration 2300: Loss = -11251.007075202016
Iteration 2400: Loss = -11250.961695036698
Iteration 2500: Loss = -11250.942016664416
Iteration 2600: Loss = -11250.926219383775
Iteration 2700: Loss = -11250.912339194954
Iteration 2800: Loss = -11250.906042444878
Iteration 2900: Loss = -11250.901853785439
Iteration 3000: Loss = -11250.898315615848
Iteration 3100: Loss = -11250.894885502647
Iteration 3200: Loss = -11250.891587343704
Iteration 3300: Loss = -11250.889954525375
Iteration 3400: Loss = -11250.886103816129
Iteration 3500: Loss = -11250.884219135432
Iteration 3600: Loss = -11250.88223335445
Iteration 3700: Loss = -11250.880401785087
Iteration 3800: Loss = -11250.878635891353
Iteration 3900: Loss = -11250.87720436423
Iteration 4000: Loss = -11250.875899330822
Iteration 4100: Loss = -11250.875378180916
Iteration 4200: Loss = -11250.87374431483
Iteration 4300: Loss = -11250.872909320538
Iteration 4400: Loss = -11250.871736204683
Iteration 4500: Loss = -11250.871866088217
1
Iteration 4600: Loss = -11250.8694873194
Iteration 4700: Loss = -11250.871052291783
1
Iteration 4800: Loss = -11250.86709577119
Iteration 4900: Loss = -11250.865869743544
Iteration 5000: Loss = -11250.874638382778
1
Iteration 5100: Loss = -11250.842796808804
Iteration 5200: Loss = -11250.764395726881
Iteration 5300: Loss = -11250.763741301604
Iteration 5400: Loss = -11250.764188930258
1
Iteration 5500: Loss = -11250.762782381438
Iteration 5600: Loss = -11250.762711009314
Iteration 5700: Loss = -11250.764341516982
1
Iteration 5800: Loss = -11250.761657946057
Iteration 5900: Loss = -11250.761302578503
Iteration 6000: Loss = -11250.76114935013
Iteration 6100: Loss = -11250.76160231446
1
Iteration 6200: Loss = -11250.760863957072
Iteration 6300: Loss = -11250.760344530443
Iteration 6400: Loss = -11250.75987234433
Iteration 6500: Loss = -11250.759489476102
Iteration 6600: Loss = -11250.759227545741
Iteration 6700: Loss = -11250.758864337831
Iteration 6800: Loss = -11250.758659845407
Iteration 6900: Loss = -11250.75846950534
Iteration 7000: Loss = -11250.758241987243
Iteration 7100: Loss = -11250.762489674618
1
Iteration 7200: Loss = -11250.757783538804
Iteration 7300: Loss = -11250.757691706745
Iteration 7400: Loss = -11250.757511940465
Iteration 7500: Loss = -11250.766058148942
1
Iteration 7600: Loss = -11250.759050809687
2
Iteration 7700: Loss = -11250.757865957341
3
Iteration 7800: Loss = -11250.757042452457
Iteration 7900: Loss = -11250.756892467543
Iteration 8000: Loss = -11250.75684756603
Iteration 8100: Loss = -11250.756190356567
Iteration 8200: Loss = -11250.754593015989
Iteration 8300: Loss = -11250.75372986524
Iteration 8400: Loss = -11250.761457941835
1
Iteration 8500: Loss = -11250.730180266171
Iteration 8600: Loss = -11250.730183350024
Iteration 8700: Loss = -11250.835298328291
1
Iteration 8800: Loss = -11250.730040496617
Iteration 8900: Loss = -11250.732736639098
1
Iteration 9000: Loss = -11250.730297169961
2
Iteration 9100: Loss = -11250.729890561062
Iteration 9200: Loss = -11250.729821970146
Iteration 9300: Loss = -11250.729907014766
Iteration 9400: Loss = -11250.72932863081
Iteration 9500: Loss = -11250.727510366505
Iteration 9600: Loss = -11250.727630052903
1
Iteration 9700: Loss = -11250.727340399395
Iteration 9800: Loss = -11250.727346271551
Iteration 9900: Loss = -11250.757813134294
1
Iteration 10000: Loss = -11250.72728791577
Iteration 10100: Loss = -11250.727257702069
Iteration 10200: Loss = -11250.728476228878
1
Iteration 10300: Loss = -11250.747786310267
2
Iteration 10400: Loss = -11250.727208759146
Iteration 10500: Loss = -11250.774235516343
1
Iteration 10600: Loss = -11250.727140438436
Iteration 10700: Loss = -11250.782818496042
1
Iteration 10800: Loss = -11250.727113694164
Iteration 10900: Loss = -11250.746860601952
1
Iteration 11000: Loss = -11250.727071792197
Iteration 11100: Loss = -11250.763160534692
1
Iteration 11200: Loss = -11250.727022048399
Iteration 11300: Loss = -11250.727461994737
1
Iteration 11400: Loss = -11250.726509235707
Iteration 11500: Loss = -11250.726659013671
1
Iteration 11600: Loss = -11250.726559357548
Iteration 11700: Loss = -11250.736690926622
1
Iteration 11800: Loss = -11250.801488568552
2
Iteration 11900: Loss = -11250.727010176983
3
Iteration 12000: Loss = -11250.72653152224
Iteration 12100: Loss = -11250.736769407074
1
Iteration 12200: Loss = -11250.730207419138
2
Iteration 12300: Loss = -11250.726523877513
Iteration 12400: Loss = -11250.822657805516
1
Iteration 12500: Loss = -11250.726460453003
Iteration 12600: Loss = -11250.726468998724
Iteration 12700: Loss = -11250.726540123884
Iteration 12800: Loss = -11250.862915361971
1
Iteration 12900: Loss = -11250.726414339058
Iteration 13000: Loss = -11250.870451297109
1
Iteration 13100: Loss = -11250.730039742766
2
Iteration 13200: Loss = -11250.789311291506
3
Iteration 13300: Loss = -11250.726439840912
Iteration 13400: Loss = -11250.828705395394
1
Iteration 13500: Loss = -11250.726386383085
Iteration 13600: Loss = -11250.727232829779
1
Iteration 13700: Loss = -11250.726765223912
2
Iteration 13800: Loss = -11250.911349889853
3
Iteration 13900: Loss = -11250.726240357157
Iteration 14000: Loss = -11250.72968284549
1
Iteration 14100: Loss = -11250.726234156788
Iteration 14200: Loss = -11250.72645308907
1
Iteration 14300: Loss = -11250.72624959112
Iteration 14400: Loss = -11250.728756198345
1
Iteration 14500: Loss = -11250.726215144785
Iteration 14600: Loss = -11250.726870959026
1
Iteration 14700: Loss = -11250.724274545511
Iteration 14800: Loss = -11250.726489895827
1
Iteration 14900: Loss = -11250.724225074327
Iteration 15000: Loss = -11250.725220170314
1
Iteration 15100: Loss = -11250.724106584605
Iteration 15200: Loss = -11250.729047819164
1
Iteration 15300: Loss = -11250.724113399498
Iteration 15400: Loss = -11250.742216807083
1
Iteration 15500: Loss = -11250.724118340948
Iteration 15600: Loss = -11250.724248730303
1
Iteration 15700: Loss = -11250.724360169348
2
Iteration 15800: Loss = -11250.72783479665
3
Iteration 15900: Loss = -11250.724107260887
Iteration 16000: Loss = -11250.725098381223
1
Iteration 16100: Loss = -11250.724775875358
2
Iteration 16200: Loss = -11250.724377438732
3
Iteration 16300: Loss = -11250.724661278378
4
Iteration 16400: Loss = -11250.726975936263
5
Iteration 16500: Loss = -11250.724325313326
6
Iteration 16600: Loss = -11250.72434972346
7
Iteration 16700: Loss = -11250.729475226599
8
Iteration 16800: Loss = -11250.739358984005
9
Iteration 16900: Loss = -11250.729190805985
10
Iteration 17000: Loss = -11250.724659699052
11
Iteration 17100: Loss = -11250.72528313771
12
Iteration 17200: Loss = -11250.741776261619
13
Iteration 17300: Loss = -11250.737866205469
14
Iteration 17400: Loss = -11250.733116612419
15
Stopping early at iteration 17400 due to no improvement.
pi: tensor([[0.7195, 0.2805],
        [0.2715, 0.7285]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8696, 0.1304], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1911, 0.1126],
         [0.5747, 0.3121]],

        [[0.6281, 0.1202],
         [0.5118, 0.5662]],

        [[0.7155, 0.1091],
         [0.5171, 0.7046]],

        [[0.5838, 0.0947],
         [0.6794, 0.6333]],

        [[0.6819, 0.1075],
         [0.5356, 0.6508]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.01106118216384376
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.58883637938024
Average Adjusted Rand Index: 0.7504308154002158
11248.59547611485
[0.58883637938024, 0.58883637938024] [0.7504308154002158, 0.7504308154002158] [11250.723475105513, 11250.733116612419]
-------------------------------------
This iteration is 22
True Objective function: Loss = -11086.617957576182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23918.317100619268
Iteration 100: Loss = -11322.147347808208
Iteration 200: Loss = -11316.634311408521
Iteration 300: Loss = -11312.245817744992
Iteration 400: Loss = -11304.592826608488
Iteration 500: Loss = -11249.123635033238
Iteration 600: Loss = -11166.42987861538
Iteration 700: Loss = -11132.594396109653
Iteration 800: Loss = -11126.077710190357
Iteration 900: Loss = -11114.961577502449
Iteration 1000: Loss = -11112.595475474645
Iteration 1100: Loss = -11112.43407063126
Iteration 1200: Loss = -11112.36006407977
Iteration 1300: Loss = -11112.302498363135
Iteration 1400: Loss = -11111.98093939497
Iteration 1500: Loss = -11111.93920104578
Iteration 1600: Loss = -11111.869413624505
Iteration 1700: Loss = -11111.684432786911
Iteration 1800: Loss = -11111.668715823092
Iteration 1900: Loss = -11111.652581258932
Iteration 2000: Loss = -11111.616667594633
Iteration 2100: Loss = -11111.598572472127
Iteration 2200: Loss = -11111.587873567754
Iteration 2300: Loss = -11111.57970459595
Iteration 2400: Loss = -11111.572494278109
Iteration 2500: Loss = -11111.564604823572
Iteration 2600: Loss = -11111.556254872368
Iteration 2700: Loss = -11111.545037486574
Iteration 2800: Loss = -11111.532952507925
Iteration 2900: Loss = -11111.520411440593
Iteration 3000: Loss = -11111.499187694348
Iteration 3100: Loss = -11111.465639410273
Iteration 3200: Loss = -11111.403438957886
Iteration 3300: Loss = -11111.309878845637
Iteration 3400: Loss = -11111.25698201678
Iteration 3500: Loss = -11111.214341812925
Iteration 3600: Loss = -11111.072773066302
Iteration 3700: Loss = -11111.045666432716
Iteration 3800: Loss = -11110.658857890761
Iteration 3900: Loss = -11110.399757154972
Iteration 4000: Loss = -11108.61013591099
Iteration 4100: Loss = -11107.115531187723
Iteration 4200: Loss = -11107.104106007477
Iteration 4300: Loss = -11107.048105523603
Iteration 4400: Loss = -11095.811301794001
Iteration 4500: Loss = -11094.89784182529
Iteration 4600: Loss = -11078.098688577737
Iteration 4700: Loss = -11062.188778523026
Iteration 4800: Loss = -11060.770198059614
Iteration 4900: Loss = -11060.73085249465
Iteration 5000: Loss = -11060.728435895267
Iteration 5100: Loss = -11060.731754106122
1
Iteration 5200: Loss = -11060.72200318851
Iteration 5300: Loss = -11049.65735936572
Iteration 5400: Loss = -11049.628151689869
Iteration 5500: Loss = -11049.626705031782
Iteration 5600: Loss = -11049.626101228816
Iteration 5700: Loss = -11049.625824753854
Iteration 5800: Loss = -11049.625020736632
Iteration 5900: Loss = -11049.625908856133
1
Iteration 6000: Loss = -11049.629097869623
2
Iteration 6100: Loss = -11049.624086098373
Iteration 6200: Loss = -11049.624292674014
1
Iteration 6300: Loss = -11049.623592641092
Iteration 6400: Loss = -11049.62348529208
Iteration 6500: Loss = -11049.623492821192
Iteration 6600: Loss = -11049.638030626735
1
Iteration 6700: Loss = -11049.62299213749
Iteration 6800: Loss = -11049.623155423744
1
Iteration 6900: Loss = -11049.627012738018
2
Iteration 7000: Loss = -11049.624337682526
3
Iteration 7100: Loss = -11049.622598335427
Iteration 7200: Loss = -11049.622536442586
Iteration 7300: Loss = -11049.631973744876
1
Iteration 7400: Loss = -11049.622537869958
Iteration 7500: Loss = -11049.622267825065
Iteration 7600: Loss = -11049.63299212628
1
Iteration 7700: Loss = -11049.62265836969
2
Iteration 7800: Loss = -11049.622241410332
Iteration 7900: Loss = -11049.62211051181
Iteration 8000: Loss = -11049.622386317591
1
Iteration 8100: Loss = -11049.62283441031
2
Iteration 8200: Loss = -11049.64345471648
3
Iteration 8300: Loss = -11049.621596563466
Iteration 8400: Loss = -11049.637945463948
1
Iteration 8500: Loss = -11049.62139246041
Iteration 8600: Loss = -11049.621337403469
Iteration 8700: Loss = -11049.620762311377
Iteration 8800: Loss = -11049.62045823828
Iteration 8900: Loss = -11049.645979587403
1
Iteration 9000: Loss = -11049.620264003835
Iteration 9100: Loss = -11049.620185566397
Iteration 9200: Loss = -11049.620221792393
Iteration 9300: Loss = -11049.62013629414
Iteration 9400: Loss = -11049.629244792768
1
Iteration 9500: Loss = -11049.620140600959
Iteration 9600: Loss = -11049.66092716349
1
Iteration 9700: Loss = -11049.621630152014
2
Iteration 9800: Loss = -11049.62360130383
3
Iteration 9900: Loss = -11049.633301592903
4
Iteration 10000: Loss = -11049.670551124731
5
Iteration 10100: Loss = -11049.623160399526
6
Iteration 10200: Loss = -11049.625641392384
7
Iteration 10300: Loss = -11049.620765312458
8
Iteration 10400: Loss = -11049.61999710572
Iteration 10500: Loss = -11049.619689811243
Iteration 10600: Loss = -11049.729373489723
1
Iteration 10700: Loss = -11049.619636043968
Iteration 10800: Loss = -11049.74614443764
1
Iteration 10900: Loss = -11049.619625738673
Iteration 11000: Loss = -11049.61963201575
Iteration 11100: Loss = -11049.620390335549
1
Iteration 11200: Loss = -11049.619608424564
Iteration 11300: Loss = -11049.623185467064
1
Iteration 11400: Loss = -11049.619722278647
2
Iteration 11500: Loss = -11049.625318776447
3
Iteration 11600: Loss = -11049.624613133008
4
Iteration 11700: Loss = -11049.72151845127
5
Iteration 11800: Loss = -11049.659973800553
6
Iteration 11900: Loss = -11049.619638151846
Iteration 12000: Loss = -11049.621687641906
1
Iteration 12100: Loss = -11049.619630160794
Iteration 12200: Loss = -11049.619686721515
Iteration 12300: Loss = -11049.62667271751
1
Iteration 12400: Loss = -11049.619574800583
Iteration 12500: Loss = -11049.736631867674
1
Iteration 12600: Loss = -11049.619748883966
2
Iteration 12700: Loss = -11049.619592653262
Iteration 12800: Loss = -11049.622671799865
1
Iteration 12900: Loss = -11049.619524685035
Iteration 13000: Loss = -11049.624734245186
1
Iteration 13100: Loss = -11049.634353908072
2
Iteration 13200: Loss = -11049.65957297857
3
Iteration 13300: Loss = -11049.620915456204
4
Iteration 13400: Loss = -11049.619493012806
Iteration 13500: Loss = -11049.64015844036
1
Iteration 13600: Loss = -11049.619485263252
Iteration 13700: Loss = -11049.622099738788
1
Iteration 13800: Loss = -11049.636591791445
2
Iteration 13900: Loss = -11049.619522501842
Iteration 14000: Loss = -11049.634921879653
1
Iteration 14100: Loss = -11049.619013776197
Iteration 14200: Loss = -11049.74145227531
1
Iteration 14300: Loss = -11049.619069188393
Iteration 14400: Loss = -11049.629132173095
1
Iteration 14500: Loss = -11049.623461372506
2
Iteration 14600: Loss = -11049.96990794274
3
Iteration 14700: Loss = -11049.619258126177
4
Iteration 14800: Loss = -11049.626631507803
5
Iteration 14900: Loss = -11049.619046781218
Iteration 15000: Loss = -11049.671500774004
1
Iteration 15100: Loss = -11049.618779452709
Iteration 15200: Loss = -11049.620975989035
1
Iteration 15300: Loss = -11049.622777614659
2
Iteration 15400: Loss = -11049.618762113649
Iteration 15500: Loss = -11049.619111115377
1
Iteration 15600: Loss = -11049.620151532457
2
Iteration 15700: Loss = -11049.649197324943
3
Iteration 15800: Loss = -11049.630045056196
4
Iteration 15900: Loss = -11049.636178229128
5
Iteration 16000: Loss = -11049.618905277888
6
Iteration 16100: Loss = -11049.619396130101
7
Iteration 16200: Loss = -11049.621251091081
8
Iteration 16300: Loss = -11049.619935135268
9
Iteration 16400: Loss = -11049.61869486082
Iteration 16500: Loss = -11049.61893833607
1
Iteration 16600: Loss = -11049.618717500523
Iteration 16700: Loss = -11049.618918668175
1
Iteration 16800: Loss = -11049.68307379917
2
Iteration 16900: Loss = -11049.618746547361
Iteration 17000: Loss = -11049.619678904195
1
Iteration 17100: Loss = -11049.630549641088
2
Iteration 17200: Loss = -11049.619074257625
3
Iteration 17300: Loss = -11049.619131578269
4
Iteration 17400: Loss = -11049.675269194133
5
Iteration 17500: Loss = -11049.618728996818
Iteration 17600: Loss = -11049.61898733391
1
Iteration 17700: Loss = -11049.701124070303
2
Iteration 17800: Loss = -11049.619535594818
3
Iteration 17900: Loss = -11049.646113307512
4
Iteration 18000: Loss = -11049.618729379874
Iteration 18100: Loss = -11049.74619838217
1
Iteration 18200: Loss = -11049.62030992254
2
Iteration 18300: Loss = -11049.618714223343
Iteration 18400: Loss = -11049.643294527128
1
Iteration 18500: Loss = -11049.63177683403
2
Iteration 18600: Loss = -11049.682536254159
3
Iteration 18700: Loss = -11049.618615913043
Iteration 18800: Loss = -11049.619259463308
1
Iteration 18900: Loss = -11049.619214706652
2
Iteration 19000: Loss = -11049.619815367514
3
Iteration 19100: Loss = -11049.61884996912
4
Iteration 19200: Loss = -11049.619321770826
5
Iteration 19300: Loss = -11049.619174747233
6
Iteration 19400: Loss = -11049.618596405679
Iteration 19500: Loss = -11049.619734860102
1
Iteration 19600: Loss = -11049.61925687017
2
Iteration 19700: Loss = -11049.62206677353
3
Iteration 19800: Loss = -11049.618721914088
4
Iteration 19900: Loss = -11049.618945892284
5
pi: tensor([[0.7771, 0.2229],
        [0.2426, 0.7574]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3794, 0.6206], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3005, 0.0947],
         [0.6157, 0.1949]],

        [[0.7126, 0.0958],
         [0.5770, 0.5021]],

        [[0.5590, 0.0973],
         [0.7110, 0.6095]],

        [[0.6153, 0.1034],
         [0.6478, 0.6941]],

        [[0.7264, 0.1112],
         [0.6583, 0.6800]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9206245835695015
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9214426004875488
Average Adjusted Rand Index: 0.9209287121149451
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20092.819839628308
Iteration 100: Loss = -11323.676408866411
Iteration 200: Loss = -11317.76517714708
Iteration 300: Loss = -11308.435539775199
Iteration 400: Loss = -11212.502216759978
Iteration 500: Loss = -11124.349866392487
Iteration 600: Loss = -11112.782381835541
Iteration 700: Loss = -11112.102108006362
Iteration 800: Loss = -11111.879468523915
Iteration 900: Loss = -11111.742498761172
Iteration 1000: Loss = -11111.656972255892
Iteration 1100: Loss = -11111.595674008417
Iteration 1200: Loss = -11111.546149350177
Iteration 1300: Loss = -11111.50473133607
Iteration 1400: Loss = -11111.473552835634
Iteration 1500: Loss = -11111.447667017757
Iteration 1600: Loss = -11111.422224338536
Iteration 1700: Loss = -11111.393446384674
Iteration 1800: Loss = -11111.358307048078
Iteration 1900: Loss = -11111.31052773744
Iteration 2000: Loss = -11111.231964508255
Iteration 2100: Loss = -11111.089865663003
Iteration 2200: Loss = -11110.713544303273
Iteration 2300: Loss = -11110.514531401748
Iteration 2400: Loss = -11110.271126122747
Iteration 2500: Loss = -11095.88561842672
Iteration 2600: Loss = -11071.987242543319
Iteration 2700: Loss = -11049.872475116988
Iteration 2800: Loss = -11049.702134576288
Iteration 2900: Loss = -11049.675474875075
Iteration 3000: Loss = -11049.66278189941
Iteration 3100: Loss = -11049.657502934988
Iteration 3200: Loss = -11049.64896093918
Iteration 3300: Loss = -11049.644697652251
Iteration 3400: Loss = -11049.64162060329
Iteration 3500: Loss = -11049.640135787926
Iteration 3600: Loss = -11049.635367673289
Iteration 3700: Loss = -11049.633450979654
Iteration 3800: Loss = -11049.63207116902
Iteration 3900: Loss = -11049.630669902257
Iteration 4000: Loss = -11049.630231749472
Iteration 4100: Loss = -11049.628797079818
Iteration 4200: Loss = -11049.627800438977
Iteration 4300: Loss = -11049.633274432506
1
Iteration 4400: Loss = -11049.626294223162
Iteration 4500: Loss = -11049.626507324323
1
Iteration 4600: Loss = -11049.63705455035
2
Iteration 4700: Loss = -11049.627841485937
3
Iteration 4800: Loss = -11049.62594989402
Iteration 4900: Loss = -11049.624005055495
Iteration 5000: Loss = -11049.623642701217
Iteration 5100: Loss = -11049.623755529525
1
Iteration 5200: Loss = -11049.623258440617
Iteration 5300: Loss = -11049.623601126845
1
Iteration 5400: Loss = -11049.622331620103
Iteration 5500: Loss = -11049.622629537125
1
Iteration 5600: Loss = -11049.622491064025
2
Iteration 5700: Loss = -11049.621993030594
Iteration 5800: Loss = -11049.62158791845
Iteration 5900: Loss = -11049.621475878861
Iteration 6000: Loss = -11049.62143982591
Iteration 6100: Loss = -11049.62134834366
Iteration 6200: Loss = -11049.620996345564
Iteration 6300: Loss = -11049.620865321524
Iteration 6400: Loss = -11049.62077955227
Iteration 6500: Loss = -11049.62099261136
1
Iteration 6600: Loss = -11049.629318057323
2
Iteration 6700: Loss = -11049.62038868555
Iteration 6800: Loss = -11049.62082851111
1
Iteration 6900: Loss = -11049.621527147432
2
Iteration 7000: Loss = -11049.64804512021
3
Iteration 7100: Loss = -11049.620517788951
4
Iteration 7200: Loss = -11049.6199455504
Iteration 7300: Loss = -11049.620713199558
1
Iteration 7400: Loss = -11049.619801775962
Iteration 7500: Loss = -11049.76189404385
1
Iteration 7600: Loss = -11049.619694907342
Iteration 7700: Loss = -11049.619602149913
Iteration 7800: Loss = -11049.621676779934
1
Iteration 7900: Loss = -11049.619537170367
Iteration 8000: Loss = -11049.619489075734
Iteration 8100: Loss = -11049.620268483995
1
Iteration 8200: Loss = -11049.619450070151
Iteration 8300: Loss = -11049.619355274104
Iteration 8400: Loss = -11049.62042998208
1
Iteration 8500: Loss = -11049.619338428398
Iteration 8600: Loss = -11049.621537809406
1
Iteration 8700: Loss = -11049.683621703687
2
Iteration 8800: Loss = -11049.619378899431
Iteration 8900: Loss = -11049.619309927017
Iteration 9000: Loss = -11049.620583557702
1
Iteration 9100: Loss = -11049.619193323984
Iteration 9200: Loss = -11049.687545193394
1
Iteration 9300: Loss = -11049.619178624234
Iteration 9400: Loss = -11049.622125331389
1
Iteration 9500: Loss = -11049.62703763627
2
Iteration 9600: Loss = -11049.619446420553
3
Iteration 9700: Loss = -11049.619278345788
Iteration 9800: Loss = -11049.804734016068
1
Iteration 9900: Loss = -11049.620265181444
2
Iteration 10000: Loss = -11050.061179193232
3
Iteration 10100: Loss = -11049.618992536596
Iteration 10200: Loss = -11049.619244567892
1
Iteration 10300: Loss = -11049.622207291613
2
Iteration 10400: Loss = -11049.619913584838
3
Iteration 10500: Loss = -11049.622257470011
4
Iteration 10600: Loss = -11049.620756207016
5
Iteration 10700: Loss = -11049.65864960882
6
Iteration 10800: Loss = -11049.619366000365
7
Iteration 10900: Loss = -11049.618988287206
Iteration 11000: Loss = -11049.619128968829
1
Iteration 11100: Loss = -11049.650924670326
2
Iteration 11200: Loss = -11049.618979303075
Iteration 11300: Loss = -11049.720709580879
1
Iteration 11400: Loss = -11049.638537788052
2
Iteration 11500: Loss = -11049.618907166516
Iteration 11600: Loss = -11049.70697648266
1
Iteration 11700: Loss = -11049.618877480458
Iteration 11800: Loss = -11049.65881044221
1
Iteration 11900: Loss = -11049.61886823106
Iteration 12000: Loss = -11049.650005293512
1
Iteration 12100: Loss = -11049.618616254087
Iteration 12200: Loss = -11049.621295667703
1
Iteration 12300: Loss = -11049.621381779487
2
Iteration 12400: Loss = -11049.618688460756
Iteration 12500: Loss = -11049.618783119453
Iteration 12600: Loss = -11049.62830132117
1
Iteration 12700: Loss = -11049.618714436196
Iteration 12800: Loss = -11049.61864585633
Iteration 12900: Loss = -11049.618647048028
Iteration 13000: Loss = -11049.62661799979
1
Iteration 13100: Loss = -11049.648788141545
2
Iteration 13200: Loss = -11049.799887615405
3
Iteration 13300: Loss = -11049.618589250853
Iteration 13400: Loss = -11049.61947844837
1
Iteration 13500: Loss = -11049.618583416293
Iteration 13600: Loss = -11049.61883445292
1
Iteration 13700: Loss = -11049.618574341412
Iteration 13800: Loss = -11049.620406013297
1
Iteration 13900: Loss = -11049.628420092213
2
Iteration 14000: Loss = -11049.617945348178
Iteration 14100: Loss = -11049.62557241846
1
Iteration 14200: Loss = -11049.617927517707
Iteration 14300: Loss = -11049.683566691598
1
Iteration 14400: Loss = -11049.617911136946
Iteration 14500: Loss = -11049.669448213353
1
Iteration 14600: Loss = -11049.61791300081
Iteration 14700: Loss = -11049.617976221765
Iteration 14800: Loss = -11049.87296133284
1
Iteration 14900: Loss = -11049.61788061646
Iteration 15000: Loss = -11049.632518472474
1
Iteration 15100: Loss = -11049.617898978608
Iteration 15200: Loss = -11049.618822385308
1
Iteration 15300: Loss = -11049.617860119188
Iteration 15400: Loss = -11049.638496873426
1
Iteration 15500: Loss = -11049.617888376206
Iteration 15600: Loss = -11049.623126843175
1
Iteration 15700: Loss = -11049.61802656755
2
Iteration 15800: Loss = -11049.61789002207
Iteration 15900: Loss = -11049.619112100774
1
Iteration 16000: Loss = -11049.618700577037
2
Iteration 16100: Loss = -11049.854342011906
3
Iteration 16200: Loss = -11049.618131668602
4
Iteration 16300: Loss = -11049.630006932886
5
Iteration 16400: Loss = -11049.61889676577
6
Iteration 16500: Loss = -11049.620011070923
7
Iteration 16600: Loss = -11049.619105331203
8
Iteration 16700: Loss = -11049.617912579837
Iteration 16800: Loss = -11049.632006678765
1
Iteration 16900: Loss = -11049.618174078889
2
Iteration 17000: Loss = -11049.62013862744
3
Iteration 17100: Loss = -11049.834022684407
4
Iteration 17200: Loss = -11049.618003626012
Iteration 17300: Loss = -11049.626291175273
1
Iteration 17400: Loss = -11049.617940361832
Iteration 17500: Loss = -11049.630922087465
1
Iteration 17600: Loss = -11049.617931438539
Iteration 17700: Loss = -11049.624478874279
1
Iteration 17800: Loss = -11049.617931073624
Iteration 17900: Loss = -11049.621716574065
1
Iteration 18000: Loss = -11049.618133792228
2
Iteration 18100: Loss = -11049.618153349133
3
Iteration 18200: Loss = -11049.621477558017
4
Iteration 18300: Loss = -11049.628331353477
5
Iteration 18400: Loss = -11049.618077947864
6
Iteration 18500: Loss = -11049.65556934871
7
Iteration 18600: Loss = -11049.660783387797
8
Iteration 18700: Loss = -11049.674171716695
9
Iteration 18800: Loss = -11049.617927101754
Iteration 18900: Loss = -11049.61829323319
1
Iteration 19000: Loss = -11049.677674278315
2
Iteration 19100: Loss = -11049.617894339231
Iteration 19200: Loss = -11049.61812479907
1
Iteration 19300: Loss = -11049.619219091623
2
Iteration 19400: Loss = -11049.61793144216
Iteration 19500: Loss = -11049.619013020163
1
Iteration 19600: Loss = -11049.628128072784
2
Iteration 19700: Loss = -11049.617928475585
Iteration 19800: Loss = -11049.647906515558
1
Iteration 19900: Loss = -11049.61789685346
pi: tensor([[0.7770, 0.2230],
        [0.2420, 0.7580]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3784, 0.6216], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3011, 0.0948],
         [0.5665, 0.1944]],

        [[0.6295, 0.0961],
         [0.6360, 0.5853]],

        [[0.6412, 0.0974],
         [0.5296, 0.7192]],

        [[0.5811, 0.1037],
         [0.5583, 0.6717]],

        [[0.7223, 0.1111],
         [0.6970, 0.7109]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9206245835695015
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9214426004875488
Average Adjusted Rand Index: 0.9209287121149451
11086.617957576182
[0.9214426004875488, 0.9214426004875488] [0.9209287121149451, 0.9209287121149451] [11049.634567848834, 11049.624772563031]
-------------------------------------
This iteration is 23
True Objective function: Loss = -11091.000831647678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20048.388600233884
Iteration 100: Loss = -11365.240445661655
Iteration 200: Loss = -11363.835807183319
Iteration 300: Loss = -11360.550809097407
Iteration 400: Loss = -11359.773534469401
Iteration 500: Loss = -11359.098937456949
Iteration 600: Loss = -11357.171202530431
Iteration 700: Loss = -11313.128114053328
Iteration 800: Loss = -11282.401721528893
Iteration 900: Loss = -11107.833192547549
Iteration 1000: Loss = -11058.342982790786
Iteration 1100: Loss = -11057.641188028958
Iteration 1200: Loss = -11057.433466018074
Iteration 1300: Loss = -11057.215929593705
Iteration 1400: Loss = -11057.142263518701
Iteration 1500: Loss = -11057.089826919399
Iteration 1600: Loss = -11056.93402599416
Iteration 1700: Loss = -11056.881178471798
Iteration 1800: Loss = -11056.861591537418
Iteration 1900: Loss = -11056.840360213797
Iteration 2000: Loss = -11056.705451606746
Iteration 2100: Loss = -11056.629544675456
Iteration 2200: Loss = -11056.622055925756
Iteration 2300: Loss = -11056.615307252303
Iteration 2400: Loss = -11056.609805534776
Iteration 2500: Loss = -11056.607508227404
Iteration 2600: Loss = -11056.600654469741
Iteration 2700: Loss = -11056.596495168435
Iteration 2800: Loss = -11056.591469711377
Iteration 2900: Loss = -11056.581455141051
Iteration 3000: Loss = -11056.574667496925
Iteration 3100: Loss = -11056.572211365514
Iteration 3200: Loss = -11056.570215547636
Iteration 3300: Loss = -11056.56842139877
Iteration 3400: Loss = -11056.566831220223
Iteration 3500: Loss = -11056.565376885354
Iteration 3600: Loss = -11056.564054758373
Iteration 3700: Loss = -11056.562831712186
Iteration 3800: Loss = -11056.561835734112
Iteration 3900: Loss = -11056.56067545198
Iteration 4000: Loss = -11056.559741473822
Iteration 4100: Loss = -11056.559679193608
Iteration 4200: Loss = -11056.558060132333
Iteration 4300: Loss = -11056.55811849584
Iteration 4400: Loss = -11056.556432520105
Iteration 4500: Loss = -11056.555680402813
Iteration 4600: Loss = -11056.555101780968
Iteration 4700: Loss = -11056.55448800994
Iteration 4800: Loss = -11056.554613339313
1
Iteration 4900: Loss = -11056.559830418957
2
Iteration 5000: Loss = -11056.553139969159
Iteration 5100: Loss = -11056.558484932239
1
Iteration 5200: Loss = -11056.552117126053
Iteration 5300: Loss = -11056.551621983706
Iteration 5400: Loss = -11056.550347116037
Iteration 5500: Loss = -11056.460151615356
Iteration 5600: Loss = -11056.45977218421
Iteration 5700: Loss = -11056.460227035706
1
Iteration 5800: Loss = -11056.459037481985
Iteration 5900: Loss = -11056.457660844133
Iteration 6000: Loss = -11056.379285816342
Iteration 6100: Loss = -11056.380305282888
1
Iteration 6200: Loss = -11056.386131513727
2
Iteration 6300: Loss = -11056.377624549443
Iteration 6400: Loss = -11056.311883982175
Iteration 6500: Loss = -11056.317368007773
1
Iteration 6600: Loss = -11056.311048463007
Iteration 6700: Loss = -11056.310681277319
Iteration 6800: Loss = -11056.310658301718
Iteration 6900: Loss = -11056.310333383957
Iteration 7000: Loss = -11056.310163110895
Iteration 7100: Loss = -11056.311142820228
1
Iteration 7200: Loss = -11056.31108095321
2
Iteration 7300: Loss = -11056.309611467683
Iteration 7400: Loss = -11056.309483648993
Iteration 7500: Loss = -11056.306903005463
Iteration 7600: Loss = -11056.272481195816
Iteration 7700: Loss = -11056.272954642662
1
Iteration 7800: Loss = -11056.27162109738
Iteration 7900: Loss = -11056.272426628775
1
Iteration 8000: Loss = -11056.271566004889
Iteration 8100: Loss = -11056.272078300459
1
Iteration 8200: Loss = -11056.271461681317
Iteration 8300: Loss = -11056.27131112628
Iteration 8400: Loss = -11056.28484524421
1
Iteration 8500: Loss = -11056.271199737555
Iteration 8600: Loss = -11056.34734391221
1
Iteration 8700: Loss = -11056.2785583005
2
Iteration 8800: Loss = -11056.271093950087
Iteration 8900: Loss = -11056.2729542679
1
Iteration 9000: Loss = -11056.292370069374
2
Iteration 9100: Loss = -11056.270989663739
Iteration 9200: Loss = -11056.271281051824
1
Iteration 9300: Loss = -11056.294115204457
2
Iteration 9400: Loss = -11056.273756612765
3
Iteration 9500: Loss = -11056.27093524252
Iteration 9600: Loss = -11056.27123293824
1
Iteration 9700: Loss = -11056.272253389774
2
Iteration 9800: Loss = -11056.276657629065
3
Iteration 9900: Loss = -11056.287307395674
4
Iteration 10000: Loss = -11056.27075268202
Iteration 10100: Loss = -11056.272052394386
1
Iteration 10200: Loss = -11056.273545006947
2
Iteration 10300: Loss = -11056.274198750489
3
Iteration 10400: Loss = -11056.270611008242
Iteration 10500: Loss = -11056.271653960725
1
Iteration 10600: Loss = -11056.270980235826
2
Iteration 10700: Loss = -11056.286254580025
3
Iteration 10800: Loss = -11056.270475518251
Iteration 10900: Loss = -11056.27037114421
Iteration 11000: Loss = -11056.270235183803
Iteration 11100: Loss = -11056.273115655053
1
Iteration 11200: Loss = -11056.292150088411
2
Iteration 11300: Loss = -11056.270716443973
3
Iteration 11400: Loss = -11056.267518261233
Iteration 11500: Loss = -11056.271726605773
1
Iteration 11600: Loss = -11056.267219087149
Iteration 11700: Loss = -11056.267700042492
1
Iteration 11800: Loss = -11056.26717504295
Iteration 11900: Loss = -11056.268183534346
1
Iteration 12000: Loss = -11056.268569715536
2
Iteration 12100: Loss = -11056.271093194966
3
Iteration 12200: Loss = -11056.268475575469
4
Iteration 12300: Loss = -11056.314912746153
5
Iteration 12400: Loss = -11056.26707634761
Iteration 12500: Loss = -11056.271125933634
1
Iteration 12600: Loss = -11056.267162286662
Iteration 12700: Loss = -11056.267634027896
1
Iteration 12800: Loss = -11056.34969916485
2
Iteration 12900: Loss = -11056.267000921023
Iteration 13000: Loss = -11056.278039598474
1
Iteration 13100: Loss = -11056.271520643742
2
Iteration 13200: Loss = -11056.277616371366
3
Iteration 13300: Loss = -11056.266832288748
Iteration 13400: Loss = -11056.266691755618
Iteration 13500: Loss = -11056.270609444071
1
Iteration 13600: Loss = -11056.272784736619
2
Iteration 13700: Loss = -11056.268313281018
3
Iteration 13800: Loss = -11056.262512901685
Iteration 13900: Loss = -11056.257905258271
Iteration 14000: Loss = -11056.258478155949
1
Iteration 14100: Loss = -11056.262240459759
2
Iteration 14200: Loss = -11056.270970720389
3
Iteration 14300: Loss = -11056.25455645192
Iteration 14400: Loss = -11056.256355872822
1
Iteration 14500: Loss = -11056.254520456732
Iteration 14600: Loss = -11056.256440446863
1
Iteration 14700: Loss = -11056.304155583755
2
Iteration 14800: Loss = -11056.207650116543
Iteration 14900: Loss = -11056.221812411752
1
Iteration 15000: Loss = -11056.324525532793
2
Iteration 15100: Loss = -11056.204639713917
Iteration 15200: Loss = -11056.204651498314
Iteration 15300: Loss = -11056.213883832308
1
Iteration 15400: Loss = -11056.20504635087
2
Iteration 15500: Loss = -11056.207766536
3
Iteration 15600: Loss = -11056.21145745804
4
Iteration 15700: Loss = -11056.259181717634
5
Iteration 15800: Loss = -11056.244830623555
6
Iteration 15900: Loss = -11056.204370695319
Iteration 16000: Loss = -11056.204936394046
1
Iteration 16100: Loss = -11056.221683237922
2
Iteration 16200: Loss = -11056.201749710524
Iteration 16300: Loss = -11056.203291880303
1
Iteration 16400: Loss = -11056.091700052739
Iteration 16500: Loss = -11056.092761191307
1
Iteration 16600: Loss = -11056.105080362153
2
Iteration 16700: Loss = -11056.08060772719
Iteration 16800: Loss = -11056.080356648949
Iteration 16900: Loss = -11056.139998852039
1
Iteration 17000: Loss = -11056.079279104348
Iteration 17100: Loss = -11056.080029532895
1
Iteration 17200: Loss = -11056.110122918482
2
Iteration 17300: Loss = -11056.118958163019
3
Iteration 17400: Loss = -11056.093060364059
4
Iteration 17500: Loss = -11056.08021410372
5
Iteration 17600: Loss = -11056.079185913373
Iteration 17700: Loss = -11056.086536770381
1
Iteration 17800: Loss = -11056.07916030675
Iteration 17900: Loss = -11056.079605887886
1
Iteration 18000: Loss = -11056.0798146936
2
Iteration 18100: Loss = -11056.0840438271
3
Iteration 18200: Loss = -11056.083709828661
4
Iteration 18300: Loss = -11056.07946385989
5
Iteration 18400: Loss = -11056.079682351463
6
Iteration 18500: Loss = -11056.092131175623
7
Iteration 18600: Loss = -11056.078383728833
Iteration 18700: Loss = -11056.078005460706
Iteration 18800: Loss = -11056.154080955821
1
Iteration 18900: Loss = -11056.216577429983
2
Iteration 19000: Loss = -11056.083293752894
3
Iteration 19100: Loss = -11056.078328289614
4
Iteration 19200: Loss = -11056.082486139176
5
Iteration 19300: Loss = -11056.104725518055
6
Iteration 19400: Loss = -11056.07821486633
7
Iteration 19500: Loss = -11056.077861183168
Iteration 19600: Loss = -11056.087685505727
1
Iteration 19700: Loss = -11056.07831288105
2
Iteration 19800: Loss = -11056.08305701946
3
Iteration 19900: Loss = -11056.10344102258
4
pi: tensor([[0.7535, 0.2465],
        [0.1977, 0.8023]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5794, 0.4206], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2890, 0.0948],
         [0.5716, 0.2102]],

        [[0.6538, 0.1039],
         [0.6653, 0.6417]],

        [[0.5686, 0.0863],
         [0.6034, 0.5065]],

        [[0.6270, 0.0967],
         [0.7164, 0.6038]],

        [[0.6575, 0.0954],
         [0.7109, 0.6919]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.882389682918764
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524808956156886
Average Adjusted Rand Index: 0.9526352100050589
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21543.33669603907
Iteration 100: Loss = -11365.180815015001
Iteration 200: Loss = -11362.47284302583
Iteration 300: Loss = -11360.898925603178
Iteration 400: Loss = -11360.133540202864
Iteration 500: Loss = -11359.32607295859
Iteration 600: Loss = -11358.55688096443
Iteration 700: Loss = -11357.703899841235
Iteration 800: Loss = -11341.827462867472
Iteration 900: Loss = -11301.508127922265
Iteration 1000: Loss = -11182.349123297632
Iteration 1100: Loss = -11059.040968905929
Iteration 1200: Loss = -11058.217079179221
Iteration 1300: Loss = -11057.70714155304
Iteration 1400: Loss = -11057.550529226437
Iteration 1500: Loss = -11057.361908636738
Iteration 1600: Loss = -11057.31632824752
Iteration 1700: Loss = -11057.278496502062
Iteration 1800: Loss = -11057.253032040375
Iteration 1900: Loss = -11057.235798721767
Iteration 2000: Loss = -11057.221714295454
Iteration 2100: Loss = -11057.209155653143
Iteration 2200: Loss = -11057.195978201995
Iteration 2300: Loss = -11057.122449347142
Iteration 2400: Loss = -11056.979472861784
Iteration 2500: Loss = -11056.949713464397
Iteration 2600: Loss = -11056.827020020572
Iteration 2700: Loss = -11056.817425568137
Iteration 2800: Loss = -11056.813217918954
Iteration 2900: Loss = -11056.809672612912
Iteration 3000: Loss = -11056.806606304679
Iteration 3100: Loss = -11056.80384702946
Iteration 3200: Loss = -11056.801210223144
Iteration 3300: Loss = -11056.798687235269
Iteration 3400: Loss = -11056.796165858128
Iteration 3500: Loss = -11056.79320501382
Iteration 3600: Loss = -11056.788400321531
Iteration 3700: Loss = -11056.781917378783
Iteration 3800: Loss = -11056.775815266703
Iteration 3900: Loss = -11056.765309298517
Iteration 4000: Loss = -11056.750861586475
Iteration 4100: Loss = -11056.722618767522
Iteration 4200: Loss = -11056.718936320018
Iteration 4300: Loss = -11056.716205125069
Iteration 4400: Loss = -11056.713915682954
Iteration 4500: Loss = -11056.623850069442
Iteration 4600: Loss = -11056.618147833113
Iteration 4700: Loss = -11056.58626567749
Iteration 4800: Loss = -11056.584186386937
Iteration 4900: Loss = -11056.549974564849
Iteration 5000: Loss = -11056.548540585474
Iteration 5100: Loss = -11056.549894787106
1
Iteration 5200: Loss = -11056.547418948032
Iteration 5300: Loss = -11056.547546688325
1
Iteration 5400: Loss = -11056.55143804502
2
Iteration 5500: Loss = -11056.546887953062
Iteration 5600: Loss = -11056.545586511236
Iteration 5700: Loss = -11056.545654421574
Iteration 5800: Loss = -11056.54474049732
Iteration 5900: Loss = -11056.546775417379
1
Iteration 6000: Loss = -11056.54400536858
Iteration 6100: Loss = -11056.544485577288
1
Iteration 6200: Loss = -11056.549833945863
2
Iteration 6300: Loss = -11056.543845940538
Iteration 6400: Loss = -11056.531712023794
Iteration 6500: Loss = -11056.52607229764
Iteration 6600: Loss = -11056.526390400602
1
Iteration 6700: Loss = -11056.53672722037
2
Iteration 6800: Loss = -11056.526765286817
3
Iteration 6900: Loss = -11056.524933722274
Iteration 7000: Loss = -11056.524825451832
Iteration 7100: Loss = -11056.535561008657
1
Iteration 7200: Loss = -11056.524411702128
Iteration 7300: Loss = -11056.52621123748
1
Iteration 7400: Loss = -11056.524072894981
Iteration 7500: Loss = -11056.524104440765
Iteration 7600: Loss = -11056.523923567112
Iteration 7700: Loss = -11056.523944532208
Iteration 7800: Loss = -11056.523808186981
Iteration 7900: Loss = -11056.523659196537
Iteration 8000: Loss = -11056.52440041748
1
Iteration 8100: Loss = -11056.523470514176
Iteration 8200: Loss = -11056.52356472701
Iteration 8300: Loss = -11056.523343555238
Iteration 8400: Loss = -11056.55713464069
1
Iteration 8500: Loss = -11056.523218290085
Iteration 8600: Loss = -11056.525682367297
1
Iteration 8700: Loss = -11056.526087628765
2
Iteration 8800: Loss = -11056.433271492522
Iteration 8900: Loss = -11056.459664865704
1
Iteration 9000: Loss = -11056.453180487979
2
Iteration 9100: Loss = -11056.429599325516
Iteration 9200: Loss = -11056.429654988882
Iteration 9300: Loss = -11056.431784287932
1
Iteration 9400: Loss = -11056.437864901312
2
Iteration 9500: Loss = -11056.429094406181
Iteration 9600: Loss = -11056.427811659925
Iteration 9700: Loss = -11056.428072842056
1
Iteration 9800: Loss = -11056.42765991765
Iteration 9900: Loss = -11056.427832651165
1
Iteration 10000: Loss = -11056.431868717364
2
Iteration 10100: Loss = -11056.440326748216
3
Iteration 10200: Loss = -11056.427664914778
Iteration 10300: Loss = -11056.428418078256
1
Iteration 10400: Loss = -11056.45050990996
2
Iteration 10500: Loss = -11056.419960441826
Iteration 10600: Loss = -11056.420083799194
1
Iteration 10700: Loss = -11056.424887169851
2
Iteration 10800: Loss = -11056.420854747288
3
Iteration 10900: Loss = -11056.445168195056
4
Iteration 11000: Loss = -11056.4202793943
5
Iteration 11100: Loss = -11056.424337797898
6
Iteration 11200: Loss = -11056.42042337016
7
Iteration 11300: Loss = -11056.420573480762
8
Iteration 11400: Loss = -11056.42405559311
9
Iteration 11500: Loss = -11056.470207811944
10
Iteration 11600: Loss = -11056.308431368976
Iteration 11700: Loss = -11056.309705597048
1
Iteration 11800: Loss = -11056.507180398934
2
Iteration 11900: Loss = -11056.30864032412
3
Iteration 12000: Loss = -11056.309218622437
4
Iteration 12100: Loss = -11056.34673482451
5
Iteration 12200: Loss = -11056.312736594244
6
Iteration 12300: Loss = -11056.319653690654
7
Iteration 12400: Loss = -11056.284133140543
Iteration 12500: Loss = -11056.291843534633
1
Iteration 12600: Loss = -11056.280178489109
Iteration 12700: Loss = -11056.29021678344
1
Iteration 12800: Loss = -11056.282081673446
2
Iteration 12900: Loss = -11056.27965069488
Iteration 13000: Loss = -11056.280112167711
1
Iteration 13100: Loss = -11056.35387176782
2
Iteration 13200: Loss = -11056.279525630333
Iteration 13300: Loss = -11056.28870526443
1
Iteration 13400: Loss = -11056.351207325992
2
Iteration 13500: Loss = -11056.279504671133
Iteration 13600: Loss = -11056.280406933123
1
Iteration 13700: Loss = -11056.279494998742
Iteration 13800: Loss = -11056.277378212511
Iteration 13900: Loss = -11056.277014759302
Iteration 14000: Loss = -11056.27722647782
1
Iteration 14100: Loss = -11056.277187751879
2
Iteration 14200: Loss = -11056.278587995152
3
Iteration 14300: Loss = -11056.281130606087
4
Iteration 14400: Loss = -11056.277269133972
5
Iteration 14500: Loss = -11056.275898359863
Iteration 14600: Loss = -11056.280066873996
1
Iteration 14700: Loss = -11056.275820516412
Iteration 14800: Loss = -11056.276262897436
1
Iteration 14900: Loss = -11056.282830990971
2
Iteration 15000: Loss = -11056.301104183196
3
Iteration 15100: Loss = -11056.356958992572
4
Iteration 15200: Loss = -11056.27828218065
5
Iteration 15300: Loss = -11056.274901395758
Iteration 15400: Loss = -11056.27597362474
1
Iteration 15500: Loss = -11056.27469889188
Iteration 15600: Loss = -11056.274533443926
Iteration 15700: Loss = -11056.249182484073
Iteration 15800: Loss = -11056.203107332294
Iteration 15900: Loss = -11056.211780575359
1
Iteration 16000: Loss = -11056.313373988898
2
Iteration 16100: Loss = -11056.197316760277
Iteration 16200: Loss = -11056.19415024425
Iteration 16300: Loss = -11056.197414237586
1
Iteration 16400: Loss = -11056.198533522153
2
Iteration 16500: Loss = -11056.195490951131
3
Iteration 16600: Loss = -11056.193742348929
Iteration 16700: Loss = -11056.19682886225
1
Iteration 16800: Loss = -11056.352461768443
2
Iteration 16900: Loss = -11056.190813381465
Iteration 17000: Loss = -11056.190930684543
1
Iteration 17100: Loss = -11056.19100663393
2
Iteration 17200: Loss = -11056.190662863697
Iteration 17300: Loss = -11056.309775439966
1
Iteration 17400: Loss = -11056.190516653782
Iteration 17500: Loss = -11056.193878460415
1
Iteration 17600: Loss = -11056.211661249235
2
Iteration 17700: Loss = -11056.190459272602
Iteration 17800: Loss = -11056.190298836114
Iteration 17900: Loss = -11056.201973374828
1
Iteration 18000: Loss = -11056.078633555457
Iteration 18100: Loss = -11056.065902075698
Iteration 18200: Loss = -11056.065617041655
Iteration 18300: Loss = -11056.065352887841
Iteration 18400: Loss = -11056.0733745206
1
Iteration 18500: Loss = -11056.065891675502
2
Iteration 18600: Loss = -11056.066465432381
3
Iteration 18700: Loss = -11056.073936443772
4
Iteration 18800: Loss = -11056.075181963442
5
Iteration 18900: Loss = -11056.153051069445
6
Iteration 19000: Loss = -11056.057757386056
Iteration 19100: Loss = -11056.05880748659
1
Iteration 19200: Loss = -11056.060211909227
2
Iteration 19300: Loss = -11056.058017325382
3
Iteration 19400: Loss = -11056.057011990868
Iteration 19500: Loss = -11056.056595433114
Iteration 19600: Loss = -11056.059524054077
1
Iteration 19700: Loss = -11056.201343814071
2
Iteration 19800: Loss = -11056.056893341587
3
Iteration 19900: Loss = -11056.058054338539
4
pi: tensor([[0.7531, 0.2469],
        [0.1980, 0.8020]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5795, 0.4205], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2889, 0.0948],
         [0.6976, 0.2102]],

        [[0.5507, 0.1039],
         [0.6235, 0.6493]],

        [[0.7281, 0.0860],
         [0.5447, 0.6749]],

        [[0.5497, 0.0967],
         [0.7080, 0.5138]],

        [[0.6114, 0.0954],
         [0.5031, 0.6124]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.882389682918764
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524808956156886
Average Adjusted Rand Index: 0.9526352100050589
11091.000831647678
[0.9524808956156886, 0.9524808956156886] [0.9526352100050589, 0.9526352100050589] [11056.077673891112, 11056.058699437768]
-------------------------------------
This iteration is 24
True Objective function: Loss = -10888.313372695342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21018.49702944233
Iteration 100: Loss = -11118.745029742111
Iteration 200: Loss = -11117.723720366543
Iteration 300: Loss = -11117.371450205008
Iteration 400: Loss = -11116.948987557274
Iteration 500: Loss = -11116.342368981179
Iteration 600: Loss = -11115.333299824268
Iteration 700: Loss = -11114.427583192417
Iteration 800: Loss = -11113.33368967578
Iteration 900: Loss = -11110.804692331303
Iteration 1000: Loss = -11051.870287837353
Iteration 1100: Loss = -10980.283011482781
Iteration 1200: Loss = -10946.825279656463
Iteration 1300: Loss = -10942.646805350823
Iteration 1400: Loss = -10941.304555056842
Iteration 1500: Loss = -10937.44578856547
Iteration 1600: Loss = -10937.357213532232
Iteration 1700: Loss = -10937.288648415028
Iteration 1800: Loss = -10935.723460914745
Iteration 1900: Loss = -10935.045711399487
Iteration 2000: Loss = -10935.022857301106
Iteration 2100: Loss = -10935.005741377041
Iteration 2200: Loss = -10934.990957317621
Iteration 2300: Loss = -10934.975574764412
Iteration 2400: Loss = -10934.964198731366
Iteration 2500: Loss = -10934.957154416865
Iteration 2600: Loss = -10934.955556411274
Iteration 2700: Loss = -10934.946297097273
Iteration 2800: Loss = -10934.942604410637
Iteration 2900: Loss = -10934.95418309388
1
Iteration 3000: Loss = -10934.934031561794
Iteration 3100: Loss = -10934.942204105839
1
Iteration 3200: Loss = -10934.927787403163
Iteration 3300: Loss = -10934.92692020421
Iteration 3400: Loss = -10934.921782437286
Iteration 3500: Loss = -10934.919084248615
Iteration 3600: Loss = -10934.915552067956
Iteration 3700: Loss = -10934.911408504962
Iteration 3800: Loss = -10934.907017555466
Iteration 3900: Loss = -10934.905718685754
Iteration 4000: Loss = -10934.904389462086
Iteration 4100: Loss = -10934.902515072414
Iteration 4200: Loss = -10934.894088716377
Iteration 4300: Loss = -10934.886063799107
Iteration 4400: Loss = -10934.884181899974
Iteration 4500: Loss = -10934.86459263886
Iteration 4600: Loss = -10934.831469544479
Iteration 4700: Loss = -10934.764465941627
Iteration 4800: Loss = -10934.648529183405
Iteration 4900: Loss = -10934.590231757616
Iteration 5000: Loss = -10934.491668427456
Iteration 5100: Loss = -10934.437562949664
Iteration 5200: Loss = -10934.366753151306
Iteration 5300: Loss = -10900.80289050944
Iteration 5400: Loss = -10867.480031517018
Iteration 5500: Loss = -10864.645945593684
Iteration 5600: Loss = -10853.51698244378
Iteration 5700: Loss = -10853.506937239374
Iteration 5800: Loss = -10853.482133398167
Iteration 5900: Loss = -10853.480857263268
Iteration 6000: Loss = -10853.479883283591
Iteration 6100: Loss = -10853.479096136945
Iteration 6200: Loss = -10853.478310318744
Iteration 6300: Loss = -10853.477671261284
Iteration 6400: Loss = -10853.477163854594
Iteration 6500: Loss = -10853.47670924084
Iteration 6600: Loss = -10853.47624668833
Iteration 6700: Loss = -10853.475785918865
Iteration 6800: Loss = -10853.47524358433
Iteration 6900: Loss = -10853.474460827198
Iteration 7000: Loss = -10853.473504175563
Iteration 7100: Loss = -10853.440011829935
Iteration 7200: Loss = -10853.476719682236
1
Iteration 7300: Loss = -10853.429927021405
Iteration 7400: Loss = -10853.429222605238
Iteration 7500: Loss = -10853.584406765605
1
Iteration 7600: Loss = -10853.429025087225
Iteration 7700: Loss = -10853.428966774987
Iteration 7800: Loss = -10853.43026879084
1
Iteration 7900: Loss = -10853.428852071977
Iteration 8000: Loss = -10853.428775050363
Iteration 8100: Loss = -10853.429544783947
1
Iteration 8200: Loss = -10853.443773858928
2
Iteration 8300: Loss = -10853.428648826131
Iteration 8400: Loss = -10853.440892176795
1
Iteration 8500: Loss = -10853.430278660862
2
Iteration 8600: Loss = -10853.428567084298
Iteration 8700: Loss = -10853.428542462889
Iteration 8800: Loss = -10853.55634870298
1
Iteration 8900: Loss = -10853.428423459844
Iteration 9000: Loss = -10853.491242645841
1
Iteration 9100: Loss = -10853.428362686149
Iteration 9200: Loss = -10853.42895986934
1
Iteration 9300: Loss = -10853.428358131878
Iteration 9400: Loss = -10853.428435796712
Iteration 9500: Loss = -10853.428383024064
Iteration 9600: Loss = -10853.428527780494
1
Iteration 9700: Loss = -10853.428967414984
2
Iteration 9800: Loss = -10853.449206001053
3
Iteration 9900: Loss = -10853.428238916826
Iteration 10000: Loss = -10853.428339500218
1
Iteration 10100: Loss = -10853.434672826417
2
Iteration 10200: Loss = -10853.428474784527
3
Iteration 10300: Loss = -10853.428511293381
4
Iteration 10400: Loss = -10853.431242377332
5
Iteration 10500: Loss = -10853.438821502208
6
Iteration 10600: Loss = -10853.430773875543
7
Iteration 10700: Loss = -10853.42832234636
Iteration 10800: Loss = -10853.432611485961
1
Iteration 10900: Loss = -10853.428372732722
Iteration 11000: Loss = -10853.428195873306
Iteration 11100: Loss = -10853.42992764872
1
Iteration 11200: Loss = -10853.428091580463
Iteration 11300: Loss = -10853.428286182227
1
Iteration 11400: Loss = -10853.428079681187
Iteration 11500: Loss = -10853.43045694931
1
Iteration 11600: Loss = -10853.428038268923
Iteration 11700: Loss = -10853.431639033228
1
Iteration 11800: Loss = -10853.42807736827
Iteration 11900: Loss = -10853.428095060206
Iteration 12000: Loss = -10853.428060169223
Iteration 12100: Loss = -10853.428290089023
1
Iteration 12200: Loss = -10853.442636799457
2
Iteration 12300: Loss = -10853.431228172913
3
Iteration 12400: Loss = -10853.427910228644
Iteration 12500: Loss = -10853.439342114929
1
Iteration 12600: Loss = -10853.427907130737
Iteration 12700: Loss = -10853.429666751399
1
Iteration 12800: Loss = -10853.427891312518
Iteration 12900: Loss = -10853.429496721705
1
Iteration 13000: Loss = -10853.42786765686
Iteration 13100: Loss = -10853.430803720052
1
Iteration 13200: Loss = -10853.427882708314
Iteration 13300: Loss = -10853.427982450485
Iteration 13400: Loss = -10853.428291170923
1
Iteration 13500: Loss = -10853.451144965657
2
Iteration 13600: Loss = -10853.439122466103
3
Iteration 13700: Loss = -10853.487505945084
4
Iteration 13800: Loss = -10853.464015715208
5
Iteration 13900: Loss = -10853.554946899925
6
Iteration 14000: Loss = -10853.427879936013
Iteration 14100: Loss = -10853.503170749102
1
Iteration 14200: Loss = -10853.42787411984
Iteration 14300: Loss = -10853.427887094764
Iteration 14400: Loss = -10853.427913120488
Iteration 14500: Loss = -10853.427898387761
Iteration 14600: Loss = -10853.460322727662
1
Iteration 14700: Loss = -10853.42784827419
Iteration 14800: Loss = -10853.427865820688
Iteration 14900: Loss = -10853.428060819504
1
Iteration 15000: Loss = -10853.428182612863
2
Iteration 15100: Loss = -10853.436614096705
3
Iteration 15200: Loss = -10853.434670008284
4
Iteration 15300: Loss = -10853.4451752684
5
Iteration 15400: Loss = -10853.429836579817
6
Iteration 15500: Loss = -10853.429882356339
7
Iteration 15600: Loss = -10853.429428489937
8
Iteration 15700: Loss = -10853.44722664447
9
Iteration 15800: Loss = -10853.427924395586
Iteration 15900: Loss = -10853.427973718022
Iteration 16000: Loss = -10853.716506956838
1
Iteration 16100: Loss = -10853.427842786381
Iteration 16200: Loss = -10853.42846139424
1
Iteration 16300: Loss = -10853.42785818927
Iteration 16400: Loss = -10853.427834585798
Iteration 16500: Loss = -10853.429277158291
1
Iteration 16600: Loss = -10853.427818128313
Iteration 16700: Loss = -10853.490506902364
1
Iteration 16800: Loss = -10853.42745988892
Iteration 16900: Loss = -10853.446683052849
1
Iteration 17000: Loss = -10853.427737741174
2
Iteration 17100: Loss = -10853.433955576236
3
Iteration 17200: Loss = -10853.46264184015
4
Iteration 17300: Loss = -10853.429138625763
5
Iteration 17400: Loss = -10853.426743071152
Iteration 17500: Loss = -10853.426946562406
1
Iteration 17600: Loss = -10853.724164562147
2
Iteration 17700: Loss = -10853.426635955419
Iteration 17800: Loss = -10853.435998623456
1
Iteration 17900: Loss = -10853.426641325797
Iteration 18000: Loss = -10853.426606761574
Iteration 18100: Loss = -10853.426825653056
1
Iteration 18200: Loss = -10853.426609068043
Iteration 18300: Loss = -10853.428468026965
1
Iteration 18400: Loss = -10853.426632705246
Iteration 18500: Loss = -10853.433327086981
1
Iteration 18600: Loss = -10853.42660645343
Iteration 18700: Loss = -10853.436504886864
1
Iteration 18800: Loss = -10853.426619038568
Iteration 18900: Loss = -10853.446600942278
1
Iteration 19000: Loss = -10853.42658703474
Iteration 19100: Loss = -10853.433405493402
1
Iteration 19200: Loss = -10853.429754138213
2
Iteration 19300: Loss = -10853.427934673524
3
Iteration 19400: Loss = -10853.426574174022
Iteration 19500: Loss = -10853.429039776789
1
Iteration 19600: Loss = -10853.42657113798
Iteration 19700: Loss = -10853.474271357005
1
Iteration 19800: Loss = -10853.42658191972
Iteration 19900: Loss = -10853.426588309672
pi: tensor([[0.7746, 0.2254],
        [0.2388, 0.7612]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5177, 0.4823], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1971, 0.0937],
         [0.6434, 0.2853]],

        [[0.6550, 0.0947],
         [0.5417, 0.6304]],

        [[0.5173, 0.0966],
         [0.5633, 0.7144]],

        [[0.6364, 0.0920],
         [0.6341, 0.6316]],

        [[0.7234, 0.0977],
         [0.5507, 0.5716]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.9137636548136063
Average Adjusted Rand Index: 0.9137717081430298
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22030.261494537986
Iteration 100: Loss = -11118.435439962677
Iteration 200: Loss = -11117.64188766259
Iteration 300: Loss = -11117.332332047252
Iteration 400: Loss = -11117.150917650302
Iteration 500: Loss = -11117.10501136429
Iteration 600: Loss = -11117.079308834764
Iteration 700: Loss = -11117.06201089623
Iteration 800: Loss = -11117.049156496996
Iteration 900: Loss = -11117.03835231639
Iteration 1000: Loss = -11117.02796988143
Iteration 1100: Loss = -11117.015943831359
Iteration 1200: Loss = -11116.998795197098
Iteration 1300: Loss = -11116.967032328183
Iteration 1400: Loss = -11116.883830362996
Iteration 1500: Loss = -11116.624967032483
Iteration 1600: Loss = -11116.36176785661
Iteration 1700: Loss = -11116.102088514643
Iteration 1800: Loss = -11115.476544770036
Iteration 1900: Loss = -11111.041466380471
Iteration 2000: Loss = -11110.581106117123
Iteration 2100: Loss = -11110.13199651185
Iteration 2200: Loss = -11109.785476361902
Iteration 2300: Loss = -11048.106593071476
Iteration 2400: Loss = -11038.878055191917
Iteration 2500: Loss = -11027.156806472925
Iteration 2600: Loss = -11021.848615767438
Iteration 2700: Loss = -11016.45804282743
Iteration 2800: Loss = -11007.445303534954
Iteration 2900: Loss = -11003.44551014146
Iteration 3000: Loss = -10997.586699008407
Iteration 3100: Loss = -10988.524329938158
Iteration 3200: Loss = -10988.510845087838
Iteration 3300: Loss = -10984.65219146507
Iteration 3400: Loss = -10984.64705131605
Iteration 3500: Loss = -10984.628256717382
Iteration 3600: Loss = -10984.631373672033
1
Iteration 3700: Loss = -10984.623885359462
Iteration 3800: Loss = -10984.630453739665
1
Iteration 3900: Loss = -10984.619287872521
Iteration 4000: Loss = -10984.616244957324
Iteration 4100: Loss = -10984.617727079347
1
Iteration 4200: Loss = -10984.602972294875
Iteration 4300: Loss = -10984.606593488026
1
Iteration 4400: Loss = -10984.529108175679
Iteration 4500: Loss = -10982.22533986423
Iteration 4600: Loss = -10974.676696145467
Iteration 4700: Loss = -10972.872491238155
Iteration 4800: Loss = -10972.62920491154
Iteration 4900: Loss = -10972.57670640764
Iteration 5000: Loss = -10953.613222907883
Iteration 5100: Loss = -10953.464834216444
Iteration 5200: Loss = -10953.448645247528
Iteration 5300: Loss = -10953.401425877935
Iteration 5400: Loss = -10947.834582506177
Iteration 5500: Loss = -10947.821207262383
Iteration 5600: Loss = -10936.224807074335
Iteration 5700: Loss = -10936.186625366276
Iteration 5800: Loss = -10936.127977667042
Iteration 5900: Loss = -10936.128121812195
1
Iteration 6000: Loss = -10936.131155202107
2
Iteration 6100: Loss = -10936.127166876333
Iteration 6200: Loss = -10936.130363495377
1
Iteration 6300: Loss = -10936.127113589824
Iteration 6400: Loss = -10936.126720049877
Iteration 6500: Loss = -10936.131209635909
1
Iteration 6600: Loss = -10936.127740297066
2
Iteration 6700: Loss = -10936.118904241883
Iteration 6800: Loss = -10936.118028160965
Iteration 6900: Loss = -10936.082395775533
Iteration 7000: Loss = -10936.082027662364
Iteration 7100: Loss = -10936.082706186171
1
Iteration 7200: Loss = -10936.088572948078
2
Iteration 7300: Loss = -10936.076800773779
Iteration 7400: Loss = -10935.949270743353
Iteration 7500: Loss = -10935.94930581038
Iteration 7600: Loss = -10935.951709121982
1
Iteration 7700: Loss = -10935.949259122595
Iteration 7800: Loss = -10935.949373214537
1
Iteration 7900: Loss = -10935.952371760846
2
Iteration 8000: Loss = -10935.949172856974
Iteration 8100: Loss = -10935.949907541684
1
Iteration 8200: Loss = -10935.948832711887
Iteration 8300: Loss = -10935.94870648576
Iteration 8400: Loss = -10935.986370308383
1
Iteration 8500: Loss = -10935.947180189398
Iteration 8600: Loss = -10935.965334906326
1
Iteration 8700: Loss = -10935.946538650585
Iteration 8800: Loss = -10935.947839364275
1
Iteration 8900: Loss = -10935.94639728008
Iteration 9000: Loss = -10935.94712045157
1
Iteration 9100: Loss = -10935.946310063056
Iteration 9200: Loss = -10935.969955447426
1
Iteration 9300: Loss = -10935.946121865201
Iteration 9400: Loss = -10935.946079941532
Iteration 9500: Loss = -10935.945814307679
Iteration 9600: Loss = -10935.945644312087
Iteration 9700: Loss = -10935.957520012178
1
Iteration 9800: Loss = -10935.945165146211
Iteration 9900: Loss = -10935.945150407973
Iteration 10000: Loss = -10935.945399459652
1
Iteration 10100: Loss = -10935.945126295841
Iteration 10200: Loss = -10935.945134452337
Iteration 10300: Loss = -10935.945216645496
Iteration 10400: Loss = -10935.950062471542
1
Iteration 10500: Loss = -10935.95019718156
2
Iteration 10600: Loss = -10935.979263001223
3
Iteration 10700: Loss = -10935.945029666444
Iteration 10800: Loss = -10935.94533656098
1
Iteration 10900: Loss = -10935.946373724697
2
Iteration 11000: Loss = -10935.945080958587
Iteration 11100: Loss = -10935.946092403328
1
Iteration 11200: Loss = -10935.944350708378
Iteration 11300: Loss = -10935.93811426494
Iteration 11400: Loss = -10935.942529798045
1
Iteration 11500: Loss = -10935.937615940926
Iteration 11600: Loss = -10935.940308212217
1
Iteration 11700: Loss = -10936.234940829345
2
Iteration 11800: Loss = -10935.93760409846
Iteration 11900: Loss = -10935.941852926755
1
Iteration 12000: Loss = -10935.937624614822
Iteration 12100: Loss = -10935.939686947286
1
Iteration 12200: Loss = -10935.940096591561
2
Iteration 12300: Loss = -10935.937631069257
Iteration 12400: Loss = -10935.934724790044
Iteration 12500: Loss = -10936.275708847217
1
Iteration 12600: Loss = -10935.93455322707
Iteration 12700: Loss = -10935.949463182013
1
Iteration 12800: Loss = -10935.934551157257
Iteration 12900: Loss = -10935.944686800174
1
Iteration 13000: Loss = -10935.934607780571
Iteration 13100: Loss = -10935.935101168454
1
Iteration 13200: Loss = -10936.122685249835
2
Iteration 13300: Loss = -10935.934556472539
Iteration 13400: Loss = -10935.938953709561
1
Iteration 13500: Loss = -10935.940258902645
2
Iteration 13600: Loss = -10935.934421577393
Iteration 13700: Loss = -10935.937108206756
1
Iteration 13800: Loss = -10935.934143271117
Iteration 13900: Loss = -10935.934160477245
Iteration 14000: Loss = -10935.93412654769
Iteration 14100: Loss = -10935.934298998702
1
Iteration 14200: Loss = -10935.934098584301
Iteration 14300: Loss = -10935.935030788494
1
Iteration 14400: Loss = -10935.934094845554
Iteration 14500: Loss = -10936.333769594206
1
Iteration 14600: Loss = -10935.934111150553
Iteration 14700: Loss = -10935.934125704653
Iteration 14800: Loss = -10935.934546704975
1
Iteration 14900: Loss = -10935.9392091654
2
Iteration 15000: Loss = -10936.09872351922
3
Iteration 15100: Loss = -10935.93429528269
4
Iteration 15200: Loss = -10935.975011938941
5
Iteration 15300: Loss = -10935.93397925054
Iteration 15400: Loss = -10935.937746208072
1
Iteration 15500: Loss = -10935.933989701582
Iteration 15600: Loss = -10935.934078636776
Iteration 15700: Loss = -10935.935699726288
1
Iteration 15800: Loss = -10935.934074234185
Iteration 15900: Loss = -10935.964201811728
1
Iteration 16000: Loss = -10935.933968699792
Iteration 16100: Loss = -10935.934044708227
Iteration 16200: Loss = -10935.933978934267
Iteration 16300: Loss = -10935.933961000726
Iteration 16400: Loss = -10935.948655450278
1
Iteration 16500: Loss = -10935.93631461557
2
Iteration 16600: Loss = -10935.93399429644
Iteration 16700: Loss = -10935.934271955679
1
Iteration 16800: Loss = -10935.93399422715
Iteration 16900: Loss = -10935.939306438742
1
Iteration 17000: Loss = -10935.933960868273
Iteration 17100: Loss = -10935.935944397115
1
Iteration 17200: Loss = -10935.9718688744
2
Iteration 17300: Loss = -10935.9340992577
3
Iteration 17400: Loss = -10935.934979462161
4
Iteration 17500: Loss = -10935.933659553632
Iteration 17600: Loss = -10935.933692046396
Iteration 17700: Loss = -10936.032194325111
1
Iteration 17800: Loss = -10935.93364760596
Iteration 17900: Loss = -10935.935388680284
1
Iteration 18000: Loss = -10935.933660810511
Iteration 18100: Loss = -10935.96915017161
1
Iteration 18200: Loss = -10935.933634346022
Iteration 18300: Loss = -10936.07981247337
1
Iteration 18400: Loss = -10935.933642399474
Iteration 18500: Loss = -10935.933645232428
Iteration 18600: Loss = -10935.933773143845
1
Iteration 18700: Loss = -10935.933625769889
Iteration 18800: Loss = -10935.93363905485
Iteration 18900: Loss = -10935.933715255982
Iteration 19000: Loss = -10935.933650720994
Iteration 19100: Loss = -10936.024721949785
1
Iteration 19200: Loss = -10935.933636651933
Iteration 19300: Loss = -10935.933649382281
Iteration 19400: Loss = -10935.934142993065
1
Iteration 19500: Loss = -10935.93369697822
Iteration 19600: Loss = -10935.938216233062
1
Iteration 19700: Loss = -10935.933616114184
Iteration 19800: Loss = -10935.937697101535
1
Iteration 19900: Loss = -10935.933619266954
pi: tensor([[0.3650, 0.6350],
        [0.6358, 0.3642]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5137, 0.4863], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2249, 0.0931],
         [0.6690, 0.2516]],

        [[0.7165, 0.0948],
         [0.7270, 0.7230]],

        [[0.6968, 0.0977],
         [0.6923, 0.5075]],

        [[0.6920, 0.0901],
         [0.6951, 0.7226]],

        [[0.6085, 0.0950],
         [0.6726, 0.5099]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448427857772554
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.736960421744899
Global Adjusted Rand Index: 0.030464969795823957
Average Adjusted Rand Index: 0.8534885302702998
10888.313372695342
[0.9137636548136063, 0.030464969795823957] [0.9137717081430298, 0.8534885302702998] [10853.426809968105, 10935.970844044294]
-------------------------------------
This iteration is 25
True Objective function: Loss = -11017.010330462863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21470.604988267114
Iteration 100: Loss = -11287.08513210265
Iteration 200: Loss = -11286.080766607603
Iteration 300: Loss = -11285.745597243553
Iteration 400: Loss = -11285.335625658103
Iteration 500: Loss = -11283.590015051006
Iteration 600: Loss = -11282.388125528512
Iteration 700: Loss = -11217.630952484846
Iteration 800: Loss = -11200.854038917621
Iteration 900: Loss = -11147.456188898253
Iteration 1000: Loss = -11096.364567873165
Iteration 1100: Loss = -11060.435926946702
Iteration 1200: Loss = -11057.079505440039
Iteration 1300: Loss = -11052.741043150125
Iteration 1400: Loss = -11051.623137928895
Iteration 1500: Loss = -11051.506515573123
Iteration 1600: Loss = -11051.44575817564
Iteration 1700: Loss = -11051.301155636567
Iteration 1800: Loss = -11051.280301949719
Iteration 1900: Loss = -11051.252011870884
Iteration 2000: Loss = -11051.190531816304
Iteration 2100: Loss = -11051.182874146349
Iteration 2200: Loss = -11051.176760104601
Iteration 2300: Loss = -11051.17111565936
Iteration 2400: Loss = -11051.164879378211
Iteration 2500: Loss = -11051.1573580898
Iteration 2600: Loss = -11051.153553912161
Iteration 2700: Loss = -11051.150499255
Iteration 2800: Loss = -11051.148332960727
Iteration 2900: Loss = -11051.146034020632
Iteration 3000: Loss = -11051.144436976061
Iteration 3100: Loss = -11051.142718729694
Iteration 3200: Loss = -11051.14202971979
Iteration 3300: Loss = -11051.140345207048
Iteration 3400: Loss = -11051.138377117191
Iteration 3500: Loss = -11051.134589024039
Iteration 3600: Loss = -11051.003304051135
Iteration 3700: Loss = -11051.002592920757
Iteration 3800: Loss = -11051.001579528347
Iteration 3900: Loss = -11051.001026824564
Iteration 4000: Loss = -11051.001515215417
1
Iteration 4100: Loss = -11050.998849831849
Iteration 4200: Loss = -11050.997615907683
Iteration 4300: Loss = -11051.000697489331
1
Iteration 4400: Loss = -11050.996760887732
Iteration 4500: Loss = -11050.995354764904
Iteration 4600: Loss = -11050.994601042235
Iteration 4700: Loss = -11050.990766662446
Iteration 4800: Loss = -11050.981108316364
Iteration 4900: Loss = -11050.97445461032
Iteration 5000: Loss = -11050.973487233978
Iteration 5100: Loss = -11050.972179165889
Iteration 5200: Loss = -11050.971560363549
Iteration 5300: Loss = -11050.971116804289
Iteration 5400: Loss = -11050.97198068706
1
Iteration 5500: Loss = -11050.967191020227
Iteration 5600: Loss = -11050.966169956635
Iteration 5700: Loss = -11050.96591621735
Iteration 5800: Loss = -11050.968912115815
1
Iteration 5900: Loss = -11050.965318896066
Iteration 6000: Loss = -11050.965203720596
Iteration 6100: Loss = -11050.955453799941
Iteration 6200: Loss = -11050.954331774246
Iteration 6300: Loss = -11050.954186231675
Iteration 6400: Loss = -11050.95409004465
Iteration 6500: Loss = -11050.95477087047
1
Iteration 6600: Loss = -11050.954126345068
Iteration 6700: Loss = -11050.953905846198
Iteration 6800: Loss = -11050.953732863321
Iteration 6900: Loss = -11050.954626159717
1
Iteration 7000: Loss = -11050.953622218605
Iteration 7100: Loss = -11050.955070670236
1
Iteration 7200: Loss = -11050.95345259727
Iteration 7300: Loss = -11050.96461243982
1
Iteration 7400: Loss = -11050.953284712037
Iteration 7500: Loss = -11050.953249055812
Iteration 7600: Loss = -11050.953732751319
1
Iteration 7700: Loss = -11050.959782529364
2
Iteration 7800: Loss = -11050.953049285374
Iteration 7900: Loss = -11051.005085344752
1
Iteration 8000: Loss = -11050.953005943249
Iteration 8100: Loss = -11050.952930338331
Iteration 8200: Loss = -11050.954295566855
1
Iteration 8300: Loss = -11050.952853249628
Iteration 8400: Loss = -11050.952819065455
Iteration 8500: Loss = -11050.95394263369
1
Iteration 8600: Loss = -11050.952774378618
Iteration 8700: Loss = -11050.952720389761
Iteration 8800: Loss = -11050.952835509594
1
Iteration 8900: Loss = -11050.954270952483
2
Iteration 9000: Loss = -11050.952629943858
Iteration 9100: Loss = -11050.957579379616
1
Iteration 9200: Loss = -11050.95259643039
Iteration 9300: Loss = -11050.953057431308
1
Iteration 9400: Loss = -11050.952537524827
Iteration 9500: Loss = -11050.959119874233
1
Iteration 9600: Loss = -11050.952531006862
Iteration 9700: Loss = -11050.952502126887
Iteration 9800: Loss = -11050.952751619361
1
Iteration 9900: Loss = -11050.95244068931
Iteration 10000: Loss = -11050.97991363563
1
Iteration 10100: Loss = -11050.95238994938
Iteration 10200: Loss = -11050.952398853484
Iteration 10300: Loss = -11050.952716328266
1
Iteration 10400: Loss = -11050.962326467636
2
Iteration 10500: Loss = -11050.935403537864
Iteration 10600: Loss = -11050.939537411898
1
Iteration 10700: Loss = -11050.934860208507
Iteration 10800: Loss = -11050.935508559598
1
Iteration 10900: Loss = -11050.905027154928
Iteration 11000: Loss = -11050.904947557025
Iteration 11100: Loss = -11050.97686352942
1
Iteration 11200: Loss = -11050.904752231465
Iteration 11300: Loss = -11050.903113467735
Iteration 11400: Loss = -11050.904672157032
1
Iteration 11500: Loss = -11050.903054835771
Iteration 11600: Loss = -11050.902856915116
Iteration 11700: Loss = -11050.901802995062
Iteration 11800: Loss = -11050.901074478234
Iteration 11900: Loss = -11050.901024018634
Iteration 12000: Loss = -11050.903826097161
1
Iteration 12100: Loss = -11050.901028908365
Iteration 12200: Loss = -11050.933002985576
1
Iteration 12300: Loss = -11050.900988202797
Iteration 12400: Loss = -11050.901634088239
1
Iteration 12500: Loss = -11050.900969605322
Iteration 12600: Loss = -11050.901050829825
Iteration 12700: Loss = -11050.902062729956
1
Iteration 12800: Loss = -11050.900933766618
Iteration 12900: Loss = -11050.891216227525
Iteration 13000: Loss = -11050.890702886703
Iteration 13100: Loss = -11050.892528246073
1
Iteration 13200: Loss = -11050.897699241972
2
Iteration 13300: Loss = -11050.956078564337
3
Iteration 13400: Loss = -11050.890674811697
Iteration 13500: Loss = -11050.891054943124
1
Iteration 13600: Loss = -11050.892176613013
2
Iteration 13700: Loss = -11050.89734958545
3
Iteration 13800: Loss = -11050.996354375095
4
Iteration 13900: Loss = -11050.909883555112
5
Iteration 14000: Loss = -11050.890483109531
Iteration 14100: Loss = -11050.986402518089
1
Iteration 14200: Loss = -11050.890461628618
Iteration 14300: Loss = -11050.946869444606
1
Iteration 14400: Loss = -11050.885977655187
Iteration 14500: Loss = -11050.8857436645
Iteration 14600: Loss = -11050.885060212617
Iteration 14700: Loss = -11050.884722237097
Iteration 14800: Loss = -11050.873774308671
Iteration 14900: Loss = -11050.872742588943
Iteration 15000: Loss = -11050.889926097303
1
Iteration 15100: Loss = -11050.86951668082
Iteration 15200: Loss = -11050.86974305018
1
Iteration 15300: Loss = -11050.881958529548
2
Iteration 15400: Loss = -11050.851658899252
Iteration 15500: Loss = -11050.851404297844
Iteration 15600: Loss = -11050.854435497877
1
Iteration 15700: Loss = -11050.869626562211
2
Iteration 15800: Loss = -11050.847801192245
Iteration 15900: Loss = -11050.851356812022
1
Iteration 16000: Loss = -11050.847713523892
Iteration 16100: Loss = -11050.862257406467
1
Iteration 16200: Loss = -11050.847744081573
Iteration 16300: Loss = -11050.84644718382
Iteration 16400: Loss = -11050.845448887065
Iteration 16500: Loss = -11050.845298525332
Iteration 16600: Loss = -11051.047193804623
1
Iteration 16700: Loss = -11050.845091965712
Iteration 16800: Loss = -11050.845085367597
Iteration 16900: Loss = -11050.862978531613
1
Iteration 17000: Loss = -11050.845075460218
Iteration 17100: Loss = -11050.848786156339
1
Iteration 17200: Loss = -11050.870044633119
2
Iteration 17300: Loss = -11050.846570781487
3
Iteration 17400: Loss = -11050.84476319157
Iteration 17500: Loss = -11050.844992568318
1
Iteration 17600: Loss = -11050.93841502199
2
Iteration 17700: Loss = -11050.844739360971
Iteration 17800: Loss = -11050.926723930206
1
Iteration 17900: Loss = -11050.844713972128
Iteration 18000: Loss = -11050.844691972594
Iteration 18100: Loss = -11050.844685601049
Iteration 18200: Loss = -11050.844658771108
Iteration 18300: Loss = -11050.847119823611
1
Iteration 18400: Loss = -11050.862467794737
2
Iteration 18500: Loss = -11050.844604378544
Iteration 18600: Loss = -11050.84463681012
Iteration 18700: Loss = -11050.865032640448
1
Iteration 18800: Loss = -11050.844213934002
Iteration 18900: Loss = -11050.831387646904
Iteration 19000: Loss = -11050.832230770295
1
Iteration 19100: Loss = -11050.828557774124
Iteration 19200: Loss = -11050.827293725155
Iteration 19300: Loss = -11050.826501390387
Iteration 19400: Loss = -11050.997131695038
1
Iteration 19500: Loss = -11050.826510476087
Iteration 19600: Loss = -11050.973615251234
1
Iteration 19700: Loss = -11050.826507882763
Iteration 19800: Loss = -11050.826476408101
Iteration 19900: Loss = -11050.834247741528
1
pi: tensor([[0.6598, 0.3402],
        [0.4261, 0.5739]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4637, 0.5363], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2149, 0.0934],
         [0.6460, 0.2833]],

        [[0.5626, 0.0888],
         [0.6207, 0.5942]],

        [[0.6439, 0.1081],
         [0.6963, 0.5470]],

        [[0.5307, 0.0966],
         [0.5768, 0.7204]],

        [[0.6891, 0.0900],
         [0.6427, 0.7284]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 1
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080965973782139
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369635135591801
Global Adjusted Rand Index: 0.34915556634966266
Average Adjusted Rand Index: 0.8774931881610015
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19254.224636502695
Iteration 100: Loss = -11285.822276010405
Iteration 200: Loss = -11285.26746123437
Iteration 300: Loss = -11284.166349646852
Iteration 400: Loss = -11283.754993348282
Iteration 500: Loss = -11282.246680709422
Iteration 600: Loss = -11131.518215806997
Iteration 700: Loss = -11057.903733277059
Iteration 800: Loss = -11050.790567049982
Iteration 900: Loss = -11049.898452311589
Iteration 1000: Loss = -11049.385720818678
Iteration 1100: Loss = -11049.278126153089
Iteration 1200: Loss = -11049.264127778048
Iteration 1300: Loss = -11049.254257754857
Iteration 1400: Loss = -11049.247273671073
Iteration 1500: Loss = -11049.242334057406
Iteration 1600: Loss = -11049.23723810046
Iteration 1700: Loss = -11049.223180665782
Iteration 1800: Loss = -11049.205570271291
Iteration 1900: Loss = -11049.224404410474
1
Iteration 2000: Loss = -11049.202094852006
Iteration 2100: Loss = -11049.200384650227
Iteration 2200: Loss = -11049.198055595283
Iteration 2300: Loss = -11049.192017629592
Iteration 2400: Loss = -11049.18752100507
Iteration 2500: Loss = -11049.186543694128
Iteration 2600: Loss = -11049.18561225326
Iteration 2700: Loss = -11049.184891975583
Iteration 2800: Loss = -11049.207701928672
1
Iteration 2900: Loss = -11049.182768336537
Iteration 3000: Loss = -11049.170378854607
Iteration 3100: Loss = -11049.174078017224
1
Iteration 3200: Loss = -11049.168528992297
Iteration 3300: Loss = -11049.167483685083
Iteration 3400: Loss = -11049.154176997867
Iteration 3500: Loss = -11049.167646207019
1
Iteration 3600: Loss = -11049.150997724548
Iteration 3700: Loss = -11049.158691944573
1
Iteration 3800: Loss = -11049.160536393445
2
Iteration 3900: Loss = -11049.151950483692
3
Iteration 4000: Loss = -11049.155181638613
4
Iteration 4100: Loss = -11049.15062466043
Iteration 4200: Loss = -11049.149977731144
Iteration 4300: Loss = -11049.151992738573
1
Iteration 4400: Loss = -11049.149524387029
Iteration 4500: Loss = -11049.149896603229
1
Iteration 4600: Loss = -11049.149236154497
Iteration 4700: Loss = -11049.15216130742
1
Iteration 4800: Loss = -11049.15286538619
2
Iteration 4900: Loss = -11049.148580518731
Iteration 5000: Loss = -11049.149561292888
1
Iteration 5100: Loss = -11049.144606827496
Iteration 5200: Loss = -11049.144099668632
Iteration 5300: Loss = -11049.151097549984
1
Iteration 5400: Loss = -11049.145101053009
2
Iteration 5500: Loss = -11049.15135504073
3
Iteration 5600: Loss = -11049.143124377764
Iteration 5700: Loss = -11049.143170649022
Iteration 5800: Loss = -11049.143042012482
Iteration 5900: Loss = -11049.142892807671
Iteration 6000: Loss = -11049.137682442746
Iteration 6100: Loss = -11049.137570834017
Iteration 6200: Loss = -11049.137331307275
Iteration 6300: Loss = -11049.136751900758
Iteration 6400: Loss = -11049.122235795441
Iteration 6500: Loss = -11049.122562293083
1
Iteration 6600: Loss = -11049.12258456786
2
Iteration 6700: Loss = -11049.129407843053
3
Iteration 6800: Loss = -11049.123635101652
4
Iteration 6900: Loss = -11049.12238461071
5
Iteration 7000: Loss = -11049.122298717892
Iteration 7100: Loss = -11049.122900767261
1
Iteration 7200: Loss = -11049.122006327125
Iteration 7300: Loss = -11049.111578068463
Iteration 7400: Loss = -11049.094676381537
Iteration 7500: Loss = -11049.092679500878
Iteration 7600: Loss = -11049.092430296203
Iteration 7700: Loss = -11049.09237078589
Iteration 7800: Loss = -11049.09956121275
1
Iteration 7900: Loss = -11049.090036735712
Iteration 8000: Loss = -11049.091099554751
1
Iteration 8100: Loss = -11049.073153145055
Iteration 8200: Loss = -11049.223335245604
1
Iteration 8300: Loss = -11049.070299883439
Iteration 8400: Loss = -11049.070280540438
Iteration 8500: Loss = -11049.070397044088
1
Iteration 8600: Loss = -11049.070259392214
Iteration 8700: Loss = -11049.197115317038
1
Iteration 8800: Loss = -11049.068592743171
Iteration 8900: Loss = -11049.066806422132
Iteration 9000: Loss = -11049.131895181275
1
Iteration 9100: Loss = -11049.066614888734
Iteration 9200: Loss = -11049.066445322718
Iteration 9300: Loss = -11049.066915092266
1
Iteration 9400: Loss = -11049.066059521485
Iteration 9500: Loss = -11049.068435587675
1
Iteration 9600: Loss = -11049.06604244393
Iteration 9700: Loss = -11049.066800510098
1
Iteration 9800: Loss = -11049.066053105298
Iteration 9900: Loss = -11049.066914445464
1
Iteration 10000: Loss = -11049.0660173934
Iteration 10100: Loss = -11049.074663235977
1
Iteration 10200: Loss = -11049.08688553031
2
Iteration 10300: Loss = -11049.065916652853
Iteration 10400: Loss = -11049.065753756156
Iteration 10500: Loss = -11049.06562307811
Iteration 10600: Loss = -11049.062755631352
Iteration 10700: Loss = -11049.062648926558
Iteration 10800: Loss = -11049.06012806435
Iteration 10900: Loss = -11049.059954787466
Iteration 11000: Loss = -11049.081431257624
1
Iteration 11100: Loss = -11049.0599950847
Iteration 11200: Loss = -11049.059990882253
Iteration 11300: Loss = -11049.062718437044
1
Iteration 11400: Loss = -11049.06004092679
Iteration 11500: Loss = -11049.060214648236
1
Iteration 11600: Loss = -11049.129113544761
2
Iteration 11700: Loss = -11049.084497977401
3
Iteration 11800: Loss = -11049.179816034219
4
Iteration 11900: Loss = -11049.059969183252
Iteration 12000: Loss = -11049.060983089474
1
Iteration 12100: Loss = -11049.059935679916
Iteration 12200: Loss = -11049.060370786632
1
Iteration 12300: Loss = -11049.061068186986
2
Iteration 12400: Loss = -11049.192895260401
3
Iteration 12500: Loss = -11049.05991256052
Iteration 12600: Loss = -11049.22627125035
1
Iteration 12700: Loss = -11049.060651154532
2
Iteration 12800: Loss = -11049.059950344057
Iteration 12900: Loss = -11049.059871714282
Iteration 13000: Loss = -11049.060637558574
1
Iteration 13100: Loss = -11049.064933516438
2
Iteration 13200: Loss = -11049.05655489781
Iteration 13300: Loss = -11049.056793090635
1
Iteration 13400: Loss = -11049.05329820958
Iteration 13500: Loss = -11049.310389106848
1
Iteration 13600: Loss = -11049.052111790072
Iteration 13700: Loss = -11049.052618862632
1
Iteration 13800: Loss = -11049.055380269188
2
Iteration 13900: Loss = -11049.21212044052
3
Iteration 14000: Loss = -11049.047004353592
Iteration 14100: Loss = -11049.060840693228
1
Iteration 14200: Loss = -11049.046811115131
Iteration 14300: Loss = -11049.047671735985
1
Iteration 14400: Loss = -11049.047040558457
2
Iteration 14500: Loss = -11049.046720055503
Iteration 14600: Loss = -11049.049388611546
1
Iteration 14700: Loss = -11049.046634218552
Iteration 14800: Loss = -11049.047528108857
1
Iteration 14900: Loss = -11049.214752438354
2
Iteration 15000: Loss = -11049.06361821224
3
Iteration 15100: Loss = -11049.046436558074
Iteration 15200: Loss = -11049.048824027375
1
Iteration 15300: Loss = -11049.046523210543
Iteration 15400: Loss = -11049.04823943204
1
Iteration 15500: Loss = -11049.054891085452
2
Iteration 15600: Loss = -11049.046424330429
Iteration 15700: Loss = -11049.05367120248
1
Iteration 15800: Loss = -11049.053591525699
2
Iteration 15900: Loss = -11049.02605781129
Iteration 16000: Loss = -11049.104559719175
1
Iteration 16100: Loss = -11049.05056238273
2
Iteration 16200: Loss = -11049.017240240155
Iteration 16300: Loss = -11049.018283433768
1
Iteration 16400: Loss = -11049.016986489583
Iteration 16500: Loss = -11049.017694694181
1
Iteration 16600: Loss = -11049.01701235333
Iteration 16700: Loss = -11049.018315358217
1
Iteration 16800: Loss = -11049.033141470865
2
Iteration 16900: Loss = -11049.016967980284
Iteration 17000: Loss = -11049.01701350183
Iteration 17100: Loss = -11049.077694264188
1
Iteration 17200: Loss = -11049.008353023448
Iteration 17300: Loss = -11049.009055765175
1
Iteration 17400: Loss = -11049.09496069728
2
Iteration 17500: Loss = -11049.01094277833
3
Iteration 17600: Loss = -11049.010322993867
4
Iteration 17700: Loss = -11049.008322433763
Iteration 17800: Loss = -11049.008427575965
1
Iteration 17900: Loss = -11049.009075880329
2
Iteration 18000: Loss = -11049.008512498982
3
Iteration 18100: Loss = -11049.014396240693
4
Iteration 18200: Loss = -11049.005863323475
Iteration 18300: Loss = -11049.007491281984
1
Iteration 18400: Loss = -11049.006430113779
2
Iteration 18500: Loss = -11049.00584062334
Iteration 18600: Loss = -11049.020010780327
1
Iteration 18700: Loss = -11049.005817982545
Iteration 18800: Loss = -11049.023593000134
1
Iteration 18900: Loss = -11049.005808131698
Iteration 19000: Loss = -11049.005900534008
Iteration 19100: Loss = -11049.013493235294
1
Iteration 19200: Loss = -11049.010589512593
2
Iteration 19300: Loss = -11049.01812794296
3
Iteration 19400: Loss = -11049.005978796018
Iteration 19500: Loss = -11049.00674674158
1
Iteration 19600: Loss = -11049.00644563458
2
Iteration 19700: Loss = -11049.008251930587
3
Iteration 19800: Loss = -11049.005850892772
Iteration 19900: Loss = -11049.006257802466
1
pi: tensor([[0.6584, 0.3416],
        [0.4316, 0.5684]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4886, 0.5114], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2134, 0.0932],
         [0.7207, 0.2870]],

        [[0.6316, 0.0890],
         [0.6915, 0.6224]],

        [[0.6731, 0.1081],
         [0.5155, 0.5910]],

        [[0.7237, 0.0971],
         [0.5319, 0.6125]],

        [[0.7079, 0.0905],
         [0.5542, 0.6688]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080965973782139
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369635135591801
Global Adjusted Rand Index: 0.36352993418727514
Average Adjusted Rand Index: 0.9010091688897987
11017.010330462863
[0.34915556634966266, 0.36352993418727514] [0.8774931881610015, 0.9010091688897987] [11050.826490610403, 11049.030492457945]
-------------------------------------
This iteration is 26
True Objective function: Loss = -11253.205971344172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20491.707929448185
Iteration 100: Loss = -11576.731168313805
Iteration 200: Loss = -11574.608279834963
Iteration 300: Loss = -11568.780702890273
Iteration 400: Loss = -11537.000649729895
Iteration 500: Loss = -11310.599611912816
Iteration 600: Loss = -11250.233442794293
Iteration 700: Loss = -11235.387803183017
Iteration 800: Loss = -11233.416931788024
Iteration 900: Loss = -11232.290372055853
Iteration 1000: Loss = -11232.10168461659
Iteration 1100: Loss = -11232.04664177949
Iteration 1200: Loss = -11232.01459313106
Iteration 1300: Loss = -11231.990203518359
Iteration 1400: Loss = -11231.972923753969
Iteration 1500: Loss = -11231.958749622132
Iteration 1600: Loss = -11231.944299104669
Iteration 1700: Loss = -11231.900189878705
Iteration 1800: Loss = -11231.8433688564
Iteration 1900: Loss = -11231.828688016987
Iteration 2000: Loss = -11231.828442249094
Iteration 2100: Loss = -11231.81780192359
Iteration 2200: Loss = -11231.813923476366
Iteration 2300: Loss = -11231.810711395527
Iteration 2400: Loss = -11231.80767193412
Iteration 2500: Loss = -11231.805150290236
Iteration 2600: Loss = -11231.803019629193
Iteration 2700: Loss = -11231.801062328195
Iteration 2800: Loss = -11231.799321513987
Iteration 2900: Loss = -11231.797703122254
Iteration 3000: Loss = -11231.800290048035
1
Iteration 3100: Loss = -11231.794720669799
Iteration 3200: Loss = -11231.793382386631
Iteration 3300: Loss = -11231.79214801077
Iteration 3400: Loss = -11231.7909956163
Iteration 3500: Loss = -11231.789849974502
Iteration 3600: Loss = -11231.788109895899
Iteration 3700: Loss = -11231.781176063552
Iteration 3800: Loss = -11231.779131945736
Iteration 3900: Loss = -11231.777391937756
Iteration 4000: Loss = -11231.777400446177
Iteration 4100: Loss = -11231.776009350258
Iteration 4200: Loss = -11231.778021239432
1
Iteration 4300: Loss = -11231.774982731018
Iteration 4400: Loss = -11231.776910316587
1
Iteration 4500: Loss = -11231.774176929517
Iteration 4600: Loss = -11231.773929248693
Iteration 4700: Loss = -11231.774134584952
1
Iteration 4800: Loss = -11231.774465968
2
Iteration 4900: Loss = -11231.774210111787
3
Iteration 5000: Loss = -11231.772505008319
Iteration 5100: Loss = -11231.774820754208
1
Iteration 5200: Loss = -11231.771623626932
Iteration 5300: Loss = -11231.771138950484
Iteration 5400: Loss = -11231.7712123685
Iteration 5500: Loss = -11231.772238496636
1
Iteration 5600: Loss = -11231.787030632317
2
Iteration 5700: Loss = -11231.77036261821
Iteration 5800: Loss = -11231.770268960563
Iteration 5900: Loss = -11231.769958146562
Iteration 6000: Loss = -11231.769999772248
Iteration 6100: Loss = -11231.769644147362
Iteration 6200: Loss = -11231.799627033093
1
Iteration 6300: Loss = -11231.85577426737
2
Iteration 6400: Loss = -11231.76915651385
Iteration 6500: Loss = -11231.769057084093
Iteration 6600: Loss = -11231.777009524549
1
Iteration 6700: Loss = -11231.76883173481
Iteration 6800: Loss = -11231.771595740134
1
Iteration 6900: Loss = -11231.768550351642
Iteration 7000: Loss = -11231.7683177017
Iteration 7100: Loss = -11231.767419200622
Iteration 7200: Loss = -11231.769463558912
1
Iteration 7300: Loss = -11231.762134952163
Iteration 7400: Loss = -11231.841355112796
1
Iteration 7500: Loss = -11231.761754185829
Iteration 7600: Loss = -11231.782457518502
1
Iteration 7700: Loss = -11231.765030290208
2
Iteration 7800: Loss = -11231.823811932527
3
Iteration 7900: Loss = -11231.761242071758
Iteration 8000: Loss = -11231.759303610268
Iteration 8100: Loss = -11231.758438821058
Iteration 8200: Loss = -11231.759064146103
1
Iteration 8300: Loss = -11231.758278432866
Iteration 8400: Loss = -11231.77382863354
1
Iteration 8500: Loss = -11231.758193922982
Iteration 8600: Loss = -11231.758135721508
Iteration 8700: Loss = -11231.764884698048
1
Iteration 8800: Loss = -11231.758051942883
Iteration 8900: Loss = -11231.758120206692
Iteration 9000: Loss = -11231.758015048355
Iteration 9100: Loss = -11231.757966240784
Iteration 9200: Loss = -11231.758041064078
Iteration 9300: Loss = -11231.757897408923
Iteration 9400: Loss = -11231.758153554314
1
Iteration 9500: Loss = -11231.757942760194
Iteration 9600: Loss = -11231.75873241371
1
Iteration 9700: Loss = -11231.758169349088
2
Iteration 9800: Loss = -11231.758368435101
3
Iteration 9900: Loss = -11231.761869619608
4
Iteration 10000: Loss = -11231.757832937781
Iteration 10100: Loss = -11231.758234565865
1
Iteration 10200: Loss = -11231.758847944462
2
Iteration 10300: Loss = -11231.803982224854
3
Iteration 10400: Loss = -11231.758167479598
4
Iteration 10500: Loss = -11231.757946097316
5
Iteration 10600: Loss = -11231.769209694501
6
Iteration 10700: Loss = -11231.757619193175
Iteration 10800: Loss = -11231.764910660362
1
Iteration 10900: Loss = -11231.75761593446
Iteration 11000: Loss = -11231.762423491715
1
Iteration 11100: Loss = -11231.757618950985
Iteration 11200: Loss = -11231.85609672489
1
Iteration 11300: Loss = -11231.759064193879
2
Iteration 11400: Loss = -11231.86282517798
3
Iteration 11500: Loss = -11231.757504378043
Iteration 11600: Loss = -11231.768602370854
1
Iteration 11700: Loss = -11231.777638476688
2
Iteration 11800: Loss = -11231.76089073315
3
Iteration 11900: Loss = -11231.757477300818
Iteration 12000: Loss = -11231.757682112686
1
Iteration 12100: Loss = -11231.758028668914
2
Iteration 12200: Loss = -11231.757462908108
Iteration 12300: Loss = -11231.75793127813
1
Iteration 12400: Loss = -11231.757662938937
2
Iteration 12500: Loss = -11231.757623177687
3
Iteration 12600: Loss = -11231.947440145594
4
Iteration 12700: Loss = -11231.760501061777
5
Iteration 12800: Loss = -11231.875741087246
6
Iteration 12900: Loss = -11231.75891354499
7
Iteration 13000: Loss = -11231.787453999521
8
Iteration 13100: Loss = -11231.757872881932
9
Iteration 13200: Loss = -11231.756966030374
Iteration 13300: Loss = -11231.757229461675
1
Iteration 13400: Loss = -11231.756954300987
Iteration 13500: Loss = -11231.764138036755
1
Iteration 13600: Loss = -11231.75692198082
Iteration 13700: Loss = -11231.762463550825
1
Iteration 13800: Loss = -11231.756960799064
Iteration 13900: Loss = -11231.759384295046
1
Iteration 14000: Loss = -11231.755712878765
Iteration 14100: Loss = -11231.755911108607
1
Iteration 14200: Loss = -11231.840484811883
2
Iteration 14300: Loss = -11231.755786212963
Iteration 14400: Loss = -11231.755759027936
Iteration 14500: Loss = -11231.757084386181
1
Iteration 14600: Loss = -11231.779340824032
2
Iteration 14700: Loss = -11231.759559458655
3
Iteration 14800: Loss = -11231.759144718939
4
Iteration 14900: Loss = -11231.762968992607
5
Iteration 15000: Loss = -11231.755629142943
Iteration 15100: Loss = -11231.755595246745
Iteration 15200: Loss = -11231.77265311524
1
Iteration 15300: Loss = -11231.755868595585
2
Iteration 15400: Loss = -11231.764718468297
3
Iteration 15500: Loss = -11231.75536722969
Iteration 15600: Loss = -11231.829987908472
1
Iteration 15700: Loss = -11231.755350078347
Iteration 15800: Loss = -11231.800155925192
1
Iteration 15900: Loss = -11231.756788090801
2
Iteration 16000: Loss = -11231.755996026739
3
Iteration 16100: Loss = -11231.75567895175
4
Iteration 16200: Loss = -11231.755362236585
Iteration 16300: Loss = -11231.755364574523
Iteration 16400: Loss = -11231.755472131534
1
Iteration 16500: Loss = -11231.755349684827
Iteration 16600: Loss = -11231.774202903018
1
Iteration 16700: Loss = -11231.755363049022
Iteration 16800: Loss = -11231.756198220533
1
Iteration 16900: Loss = -11231.754004804046
Iteration 17000: Loss = -11231.754126738211
1
Iteration 17100: Loss = -11231.778426850993
2
Iteration 17200: Loss = -11231.898770182732
3
Iteration 17300: Loss = -11231.75403871917
Iteration 17400: Loss = -11231.753188269588
Iteration 17500: Loss = -11231.927231412179
1
Iteration 17600: Loss = -11231.753078751579
Iteration 17700: Loss = -11231.894843525843
1
Iteration 17800: Loss = -11231.753050678195
Iteration 17900: Loss = -11231.777283303214
1
Iteration 18000: Loss = -11231.752841935993
Iteration 18100: Loss = -11231.755259811307
1
Iteration 18200: Loss = -11231.752689376459
Iteration 18300: Loss = -11231.752629969977
Iteration 18400: Loss = -11231.760302417128
1
Iteration 18500: Loss = -11231.842982319838
2
Iteration 18600: Loss = -11231.752600752612
Iteration 18700: Loss = -11231.753921034402
1
Iteration 18800: Loss = -11231.754128407438
2
Iteration 18900: Loss = -11231.753006316694
3
Iteration 19000: Loss = -11231.756908491685
4
Iteration 19100: Loss = -11231.752215752722
Iteration 19200: Loss = -11231.753575127485
1
Iteration 19300: Loss = -11231.753085434955
2
Iteration 19400: Loss = -11231.752571114137
3
Iteration 19500: Loss = -11231.753330545625
4
Iteration 19600: Loss = -11231.756237759064
5
Iteration 19700: Loss = -11231.75653500692
6
Iteration 19800: Loss = -11231.752206832098
Iteration 19900: Loss = -11231.752495437464
1
pi: tensor([[0.7705, 0.2295],
        [0.2271, 0.7729]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5328, 0.4672], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3068, 0.0955],
         [0.6530, 0.1994]],

        [[0.6554, 0.0928],
         [0.6535, 0.6411]],

        [[0.6258, 0.1085],
         [0.6325, 0.5861]],

        [[0.5430, 0.1015],
         [0.6832, 0.6780]],

        [[0.6750, 0.1079],
         [0.6730, 0.6620]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524809210812679
Average Adjusted Rand Index: 0.9526447772304938
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23020.828030548797
Iteration 100: Loss = -11577.15136067828
Iteration 200: Loss = -11575.518276608711
Iteration 300: Loss = -11570.388624068286
Iteration 400: Loss = -11564.006050209158
Iteration 500: Loss = -11489.79565884211
Iteration 600: Loss = -11285.274856711934
Iteration 700: Loss = -11256.459133382517
Iteration 800: Loss = -11242.58355841841
Iteration 900: Loss = -11241.415270339085
Iteration 1000: Loss = -11234.100838011522
Iteration 1100: Loss = -11233.983877173636
Iteration 1200: Loss = -11233.908233506052
Iteration 1300: Loss = -11233.854622825358
Iteration 1400: Loss = -11233.814311688271
Iteration 1500: Loss = -11233.782127765158
Iteration 1600: Loss = -11233.753912087646
Iteration 1700: Loss = -11233.72458874429
Iteration 1800: Loss = -11233.689724354976
Iteration 1900: Loss = -11233.668134132504
Iteration 2000: Loss = -11233.654076045304
Iteration 2100: Loss = -11233.637522114954
Iteration 2200: Loss = -11233.329585446849
Iteration 2300: Loss = -11231.840674844014
Iteration 2400: Loss = -11231.833559590952
Iteration 2500: Loss = -11231.827727488242
Iteration 2600: Loss = -11231.822603540964
Iteration 2700: Loss = -11231.81794837704
Iteration 2800: Loss = -11231.813518608424
Iteration 2900: Loss = -11231.809145986499
Iteration 3000: Loss = -11231.804688499196
Iteration 3100: Loss = -11231.800503588069
Iteration 3200: Loss = -11231.797430609784
Iteration 3300: Loss = -11231.794952739632
Iteration 3400: Loss = -11231.79283655349
Iteration 3500: Loss = -11231.791306047453
Iteration 3600: Loss = -11231.789667450412
Iteration 3700: Loss = -11231.790604586859
1
Iteration 3800: Loss = -11231.785115932926
Iteration 3900: Loss = -11231.78193704412
Iteration 4000: Loss = -11231.777547338117
Iteration 4100: Loss = -11231.777834445938
1
Iteration 4200: Loss = -11231.775111779016
Iteration 4300: Loss = -11231.77411774925
Iteration 4400: Loss = -11231.773320500792
Iteration 4500: Loss = -11231.772348783454
Iteration 4600: Loss = -11231.773006772673
1
Iteration 4700: Loss = -11231.772469353149
2
Iteration 4800: Loss = -11231.7691260523
Iteration 4900: Loss = -11231.76782447374
Iteration 5000: Loss = -11231.76697265906
Iteration 5100: Loss = -11231.771436576662
1
Iteration 5200: Loss = -11231.7656945346
Iteration 5300: Loss = -11231.765829582075
1
Iteration 5400: Loss = -11231.764688784353
Iteration 5500: Loss = -11231.764424308785
Iteration 5600: Loss = -11231.763860173387
Iteration 5700: Loss = -11231.764587511794
1
Iteration 5800: Loss = -11231.768859317513
2
Iteration 5900: Loss = -11231.765580058185
3
Iteration 6000: Loss = -11231.762506834704
Iteration 6100: Loss = -11231.762252516812
Iteration 6200: Loss = -11231.765766600667
1
Iteration 6300: Loss = -11231.761759356954
Iteration 6400: Loss = -11231.7629651315
1
Iteration 6500: Loss = -11231.761297623512
Iteration 6600: Loss = -11231.761163562325
Iteration 6700: Loss = -11231.761560570392
1
Iteration 6800: Loss = -11231.76061291782
Iteration 6900: Loss = -11231.762607876628
1
Iteration 7000: Loss = -11231.759304746345
Iteration 7100: Loss = -11231.759171064841
Iteration 7200: Loss = -11231.758948121605
Iteration 7300: Loss = -11231.758807162263
Iteration 7400: Loss = -11231.758947400647
1
Iteration 7500: Loss = -11231.758531484742
Iteration 7600: Loss = -11231.75841944944
Iteration 7700: Loss = -11231.758320615723
Iteration 7800: Loss = -11231.75819473086
Iteration 7900: Loss = -11231.760661506765
1
Iteration 8000: Loss = -11231.758030586976
Iteration 8100: Loss = -11231.758667829337
1
Iteration 8200: Loss = -11231.75791103446
Iteration 8300: Loss = -11231.771339571937
1
Iteration 8400: Loss = -11231.757759852842
Iteration 8500: Loss = -11231.757917208291
1
Iteration 8600: Loss = -11231.757613963215
Iteration 8700: Loss = -11231.76094823028
1
Iteration 8800: Loss = -11231.757482145234
Iteration 8900: Loss = -11231.773468574822
1
Iteration 9000: Loss = -11231.757406026758
Iteration 9100: Loss = -11231.75735166356
Iteration 9200: Loss = -11231.757434998593
Iteration 9300: Loss = -11231.75725537342
Iteration 9400: Loss = -11231.770550338486
1
Iteration 9500: Loss = -11231.757172904521
Iteration 9600: Loss = -11231.757121141405
Iteration 9700: Loss = -11231.758744356555
1
Iteration 9800: Loss = -11231.756960202705
Iteration 9900: Loss = -11231.836593464532
1
Iteration 10000: Loss = -11231.75683638112
Iteration 10100: Loss = -11231.758824579372
1
Iteration 10200: Loss = -11231.756784876956
Iteration 10300: Loss = -11231.756824730408
Iteration 10400: Loss = -11231.796571062783
1
Iteration 10500: Loss = -11231.756570868887
Iteration 10600: Loss = -11231.801693771551
1
Iteration 10700: Loss = -11231.755784143337
Iteration 10800: Loss = -11231.755312601752
Iteration 10900: Loss = -11231.784694929476
1
Iteration 11000: Loss = -11231.755283521274
Iteration 11100: Loss = -11231.755268627929
Iteration 11200: Loss = -11231.772363580125
1
Iteration 11300: Loss = -11231.768977420998
2
Iteration 11400: Loss = -11231.931205901292
3
Iteration 11500: Loss = -11231.75524170419
Iteration 11600: Loss = -11231.75524115783
Iteration 11700: Loss = -11231.779113960374
1
Iteration 11800: Loss = -11231.755192032355
Iteration 11900: Loss = -11231.767049224174
1
Iteration 12000: Loss = -11231.755160835273
Iteration 12100: Loss = -11231.755369846218
1
Iteration 12200: Loss = -11231.756160270916
2
Iteration 12300: Loss = -11231.756018809541
3
Iteration 12400: Loss = -11231.782497262278
4
Iteration 12500: Loss = -11231.755127482957
Iteration 12600: Loss = -11231.754100706692
Iteration 12700: Loss = -11231.763004776554
1
Iteration 12800: Loss = -11231.753405547615
Iteration 12900: Loss = -11231.753310535445
Iteration 13000: Loss = -11231.755266935323
1
Iteration 13100: Loss = -11231.753278309958
Iteration 13200: Loss = -11231.754792723827
1
Iteration 13300: Loss = -11231.753316391003
Iteration 13400: Loss = -11231.755594768754
1
Iteration 13500: Loss = -11231.753250421483
Iteration 13600: Loss = -11231.754182615261
1
Iteration 13700: Loss = -11231.757772198038
2
Iteration 13800: Loss = -11231.756103125179
3
Iteration 13900: Loss = -11231.75811599735
4
Iteration 14000: Loss = -11231.753185022282
Iteration 14100: Loss = -11231.760036935368
1
Iteration 14200: Loss = -11231.75292507401
Iteration 14300: Loss = -11231.795991098781
1
Iteration 14400: Loss = -11231.75294678866
Iteration 14500: Loss = -11231.758281031149
1
Iteration 14600: Loss = -11231.799938090313
2
Iteration 14700: Loss = -11231.75286763488
Iteration 14800: Loss = -11231.768486996827
1
Iteration 14900: Loss = -11231.75288949229
Iteration 15000: Loss = -11231.753643129112
1
Iteration 15100: Loss = -11231.757518900295
2
Iteration 15200: Loss = -11231.7529248591
Iteration 15300: Loss = -11231.753619155705
1
Iteration 15400: Loss = -11231.758485916811
2
Iteration 15500: Loss = -11231.753069013428
3
Iteration 15600: Loss = -11231.752943940483
Iteration 15700: Loss = -11231.752780191031
Iteration 15800: Loss = -11231.753425058449
1
Iteration 15900: Loss = -11231.753229991802
2
Iteration 16000: Loss = -11231.76815175166
3
Iteration 16100: Loss = -11231.752542291128
Iteration 16200: Loss = -11231.753346670892
1
Iteration 16300: Loss = -11231.886070532602
2
Iteration 16400: Loss = -11231.752513771939
Iteration 16500: Loss = -11231.752460425994
Iteration 16600: Loss = -11231.871037976303
1
Iteration 16700: Loss = -11231.752901427095
2
Iteration 16800: Loss = -11231.753171567285
3
Iteration 16900: Loss = -11231.765279653508
4
Iteration 17000: Loss = -11231.767518144194
5
Iteration 17100: Loss = -11231.759346953846
6
Iteration 17200: Loss = -11231.752351486228
Iteration 17300: Loss = -11231.752768865672
1
Iteration 17400: Loss = -11231.758281907847
2
Iteration 17500: Loss = -11231.752803107407
3
Iteration 17600: Loss = -11231.753334943622
4
Iteration 17700: Loss = -11231.752403880893
Iteration 17800: Loss = -11231.752554135834
1
Iteration 17900: Loss = -11231.752621197911
2
Iteration 18000: Loss = -11231.752629794004
3
Iteration 18100: Loss = -11231.863858934275
4
Iteration 18200: Loss = -11231.752609501515
5
Iteration 18300: Loss = -11231.766944445053
6
Iteration 18400: Loss = -11231.754824472107
7
Iteration 18500: Loss = -11231.752205397443
Iteration 18600: Loss = -11231.752310121881
1
Iteration 18700: Loss = -11231.755981781997
2
Iteration 18800: Loss = -11231.765600679724
3
Iteration 18900: Loss = -11231.752139311728
Iteration 19000: Loss = -11231.753845171977
1
Iteration 19100: Loss = -11231.75207408684
Iteration 19200: Loss = -11231.752625706953
1
Iteration 19300: Loss = -11231.752065008019
Iteration 19400: Loss = -11231.756535301938
1
Iteration 19500: Loss = -11231.75205878754
Iteration 19600: Loss = -11231.81893646942
1
Iteration 19700: Loss = -11231.752075846582
Iteration 19800: Loss = -11231.841124723891
1
Iteration 19900: Loss = -11231.752058173399
pi: tensor([[0.7721, 0.2279],
        [0.2270, 0.7730]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4664, 0.5336], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.0954],
         [0.6075, 0.3065]],

        [[0.5722, 0.0931],
         [0.5841, 0.5973]],

        [[0.7021, 0.1084],
         [0.7289, 0.7061]],

        [[0.6333, 0.1012],
         [0.6419, 0.6826]],

        [[0.5416, 0.1077],
         [0.5526, 0.5993]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524809210812679
Average Adjusted Rand Index: 0.9526447772304938
11253.205971344172
[0.9524809210812679, 0.9524809210812679] [0.9526447772304938, 0.9526447772304938] [11231.758218865216, 11231.75584024674]
-------------------------------------
This iteration is 27
True Objective function: Loss = -11381.101212346859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24210.32533034904
Iteration 100: Loss = -11701.911117791467
Iteration 200: Loss = -11700.627633750666
Iteration 300: Loss = -11699.174591872103
Iteration 400: Loss = -11694.173506940993
Iteration 500: Loss = -11691.918361294234
Iteration 600: Loss = -11690.74999987251
Iteration 700: Loss = -11690.371056530346
Iteration 800: Loss = -11690.031293414211
Iteration 900: Loss = -11568.184953561467
Iteration 1000: Loss = -11556.65127579519
Iteration 1100: Loss = -11551.010407323844
Iteration 1200: Loss = -11550.684837277297
Iteration 1300: Loss = -11549.99401430318
Iteration 1400: Loss = -11526.55480199233
Iteration 1500: Loss = -11517.757580210966
Iteration 1600: Loss = -11515.785436441834
Iteration 1700: Loss = -11515.72664551096
Iteration 1800: Loss = -11515.65453579155
Iteration 1900: Loss = -11514.86919379158
Iteration 2000: Loss = -11514.675554663138
Iteration 2100: Loss = -11514.646750768377
Iteration 2200: Loss = -11514.425995419717
Iteration 2300: Loss = -11510.44113875209
Iteration 2400: Loss = -11510.415953822485
Iteration 2500: Loss = -11510.37975678555
Iteration 2600: Loss = -11509.096763854366
Iteration 2700: Loss = -11509.09073313434
Iteration 2800: Loss = -11509.087380416806
Iteration 2900: Loss = -11509.069636730073
Iteration 3000: Loss = -11506.923643163118
Iteration 3100: Loss = -11506.915706483582
Iteration 3200: Loss = -11501.799755931064
Iteration 3300: Loss = -11501.164099916377
Iteration 3400: Loss = -11500.94951142165
Iteration 3500: Loss = -11499.493975181174
Iteration 3600: Loss = -11494.702351996777
Iteration 3700: Loss = -11494.03210058179
Iteration 3800: Loss = -11494.02878713506
Iteration 3900: Loss = -11494.02505216719
Iteration 4000: Loss = -11494.000881967337
Iteration 4100: Loss = -11494.004518862108
1
Iteration 4200: Loss = -11493.996961290273
Iteration 4300: Loss = -11493.995276528596
Iteration 4400: Loss = -11493.933640473311
Iteration 4500: Loss = -11489.694915536864
Iteration 4600: Loss = -11485.227319619626
Iteration 4700: Loss = -11485.22519920428
Iteration 4800: Loss = -11485.245915178948
1
Iteration 4900: Loss = -11485.224981270883
Iteration 5000: Loss = -11485.224108854047
Iteration 5100: Loss = -11480.070012313967
Iteration 5200: Loss = -11480.020339101406
Iteration 5300: Loss = -11480.019594039333
Iteration 5400: Loss = -11480.019223921698
Iteration 5500: Loss = -11480.013185891998
Iteration 5600: Loss = -11480.003175370988
Iteration 5700: Loss = -11478.147942509506
Iteration 5800: Loss = -11478.089341142888
Iteration 5900: Loss = -11477.555885058617
Iteration 6000: Loss = -11477.537835929725
Iteration 6100: Loss = -11477.536266405163
Iteration 6200: Loss = -11477.52560880313
Iteration 6300: Loss = -11477.52445047673
Iteration 6400: Loss = -11477.497637156474
Iteration 6500: Loss = -11477.47463149337
Iteration 6600: Loss = -11477.426027402757
Iteration 6700: Loss = -11477.423642710904
Iteration 6800: Loss = -11477.41876199692
Iteration 6900: Loss = -11477.418774761893
Iteration 7000: Loss = -11477.418599904815
Iteration 7100: Loss = -11477.418515012092
Iteration 7200: Loss = -11477.419527801165
1
Iteration 7300: Loss = -11477.418315088971
Iteration 7400: Loss = -11477.43554005614
1
Iteration 7500: Loss = -11477.418042033094
Iteration 7600: Loss = -11477.420646262513
1
Iteration 7700: Loss = -11477.416349020223
Iteration 7800: Loss = -11477.421222708215
1
Iteration 7900: Loss = -11477.415223093694
Iteration 8000: Loss = -11477.415503690434
1
Iteration 8100: Loss = -11477.38621700627
Iteration 8200: Loss = -11477.386002666319
Iteration 8300: Loss = -11476.850527850424
Iteration 8400: Loss = -11476.826489147312
Iteration 8500: Loss = -11476.742764370636
Iteration 8600: Loss = -11476.742139579139
Iteration 8700: Loss = -11476.73950907888
Iteration 8800: Loss = -11471.844359099567
Iteration 8900: Loss = -11471.813861642806
Iteration 9000: Loss = -11470.646416193484
Iteration 9100: Loss = -11470.637527075616
Iteration 9200: Loss = -11470.636540305726
Iteration 9300: Loss = -11470.621231361049
Iteration 9400: Loss = -11470.621224770952
Iteration 9500: Loss = -11470.625350238835
1
Iteration 9600: Loss = -11470.620758728515
Iteration 9700: Loss = -11470.620577124677
Iteration 9800: Loss = -11467.96061122141
Iteration 9900: Loss = -11467.877947083656
Iteration 10000: Loss = -11467.876738175697
Iteration 10100: Loss = -11467.867121137602
Iteration 10200: Loss = -11467.864689716142
Iteration 10300: Loss = -11467.864905845387
1
Iteration 10400: Loss = -11467.866529649003
2
Iteration 10500: Loss = -11467.864360695856
Iteration 10600: Loss = -11467.870690867292
1
Iteration 10700: Loss = -11467.978963883947
2
Iteration 10800: Loss = -11467.86381948263
Iteration 10900: Loss = -11467.86372285465
Iteration 11000: Loss = -11467.86535709929
1
Iteration 11100: Loss = -11467.883978415352
2
Iteration 11200: Loss = -11467.862988630166
Iteration 11300: Loss = -11467.862190097223
Iteration 11400: Loss = -11467.861016958235
Iteration 11500: Loss = -11467.868628136666
1
Iteration 11600: Loss = -11467.89097402967
2
Iteration 11700: Loss = -11467.85995032743
Iteration 11800: Loss = -11467.957423123273
1
Iteration 11900: Loss = -11467.86562960032
2
Iteration 12000: Loss = -11467.859905498688
Iteration 12100: Loss = -11467.874546752053
1
Iteration 12200: Loss = -11467.859867712214
Iteration 12300: Loss = -11467.860170615033
1
Iteration 12400: Loss = -11468.056175537306
2
Iteration 12500: Loss = -11467.860274147846
3
Iteration 12600: Loss = -11467.859927561012
Iteration 12700: Loss = -11467.861278480845
1
Iteration 12800: Loss = -11467.859690707408
Iteration 12900: Loss = -11467.860674624477
1
Iteration 13000: Loss = -11467.85672725072
Iteration 13100: Loss = -11467.861589263606
1
Iteration 13200: Loss = -11468.010807020964
2
Iteration 13300: Loss = -11467.855780148084
Iteration 13400: Loss = -11467.856180482524
1
Iteration 13500: Loss = -11467.856281736755
2
Iteration 13600: Loss = -11467.849224643809
Iteration 13700: Loss = -11467.82920959402
Iteration 13800: Loss = -11467.83985387026
1
Iteration 13900: Loss = -11467.842690910338
2
Iteration 14000: Loss = -11467.829420910592
3
Iteration 14100: Loss = -11467.832571410236
4
Iteration 14200: Loss = -11467.837552160185
5
Iteration 14300: Loss = -11467.865393540016
6
Iteration 14400: Loss = -11467.798909014364
Iteration 14500: Loss = -11467.800804130822
1
Iteration 14600: Loss = -11467.850290123575
2
Iteration 14700: Loss = -11467.79872589113
Iteration 14800: Loss = -11467.80324204322
1
Iteration 14900: Loss = -11467.798246154958
Iteration 15000: Loss = -11467.797618553346
Iteration 15100: Loss = -11467.808513977643
1
Iteration 15200: Loss = -11467.799357411028
2
Iteration 15300: Loss = -11467.798520588036
3
Iteration 15400: Loss = -11467.798541679975
4
Iteration 15500: Loss = -11467.801439496718
5
Iteration 15600: Loss = -11467.803490817232
6
Iteration 15700: Loss = -11467.797063885182
Iteration 15800: Loss = -11467.798313532654
1
Iteration 15900: Loss = -11467.798749403592
2
Iteration 16000: Loss = -11467.949080694443
3
Iteration 16100: Loss = -11467.7967961176
Iteration 16200: Loss = -11467.800165004315
1
Iteration 16300: Loss = -11467.808426904425
2
Iteration 16400: Loss = -11467.796813285964
Iteration 16500: Loss = -11467.794235952942
Iteration 16600: Loss = -11467.792645691772
Iteration 16700: Loss = -11467.79313575944
1
Iteration 16800: Loss = -11467.845321183268
2
Iteration 16900: Loss = -11467.791575040925
Iteration 17000: Loss = -11467.799557870976
1
Iteration 17100: Loss = -11467.80135399348
2
Iteration 17200: Loss = -11467.791695062051
3
Iteration 17300: Loss = -11467.813379197807
4
Iteration 17400: Loss = -11467.800362029566
5
Iteration 17500: Loss = -11467.791861229287
6
Iteration 17600: Loss = -11467.793852809595
7
Iteration 17700: Loss = -11467.794114404971
8
Iteration 17800: Loss = -11467.80898996175
9
Iteration 17900: Loss = -11467.817151905927
10
Iteration 18000: Loss = -11467.881546350956
11
Iteration 18100: Loss = -11467.794408273816
12
Iteration 18200: Loss = -11467.795218196345
13
Iteration 18300: Loss = -11467.792379685086
14
Iteration 18400: Loss = -11467.795665841075
15
Stopping early at iteration 18400 due to no improvement.
pi: tensor([[0.3892, 0.6108],
        [0.6277, 0.3723]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4761, 0.5239], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2545, 0.1011],
         [0.6573, 0.2646]],

        [[0.5370, 0.1014],
         [0.6069, 0.6261]],

        [[0.5117, 0.1087],
         [0.6374, 0.6001]],

        [[0.5937, 0.0956],
         [0.6237, 0.5911]],

        [[0.6197, 0.1001],
         [0.5727, 0.7148]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080702804390127
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
Global Adjusted Rand Index: 0.039695934781989725
Average Adjusted Rand Index: 0.8984060833048074
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21555.71037844357
Iteration 100: Loss = -11703.020671088914
Iteration 200: Loss = -11701.90014880825
Iteration 300: Loss = -11700.19350630581
Iteration 400: Loss = -11699.25635466212
Iteration 500: Loss = -11697.739571875776
Iteration 600: Loss = -11649.266200467657
Iteration 700: Loss = -11483.69599956408
Iteration 800: Loss = -11444.86268390873
Iteration 900: Loss = -11443.325873384583
Iteration 1000: Loss = -11442.884944390797
Iteration 1100: Loss = -11442.587722995086
Iteration 1200: Loss = -11442.31369749881
Iteration 1300: Loss = -11442.197333944292
Iteration 1400: Loss = -11442.120821653196
Iteration 1500: Loss = -11442.037220502083
Iteration 1600: Loss = -11442.003725347688
Iteration 1700: Loss = -11441.979161192468
Iteration 1800: Loss = -11441.96005593049
Iteration 1900: Loss = -11441.944514520248
Iteration 2000: Loss = -11441.931547747095
Iteration 2100: Loss = -11441.920629234779
Iteration 2200: Loss = -11441.911158843386
Iteration 2300: Loss = -11441.90275845782
Iteration 2400: Loss = -11441.895113287761
Iteration 2500: Loss = -11441.887411840913
Iteration 2600: Loss = -11441.87719177201
Iteration 2700: Loss = -11441.868265768728
Iteration 2800: Loss = -11441.863538863596
Iteration 2900: Loss = -11441.85974205297
Iteration 3000: Loss = -11441.85657074842
Iteration 3100: Loss = -11441.855257518202
Iteration 3200: Loss = -11441.85123629449
Iteration 3300: Loss = -11441.857056059915
1
Iteration 3400: Loss = -11441.846942358432
Iteration 3500: Loss = -11441.844938942028
Iteration 3600: Loss = -11441.84316129357
Iteration 3700: Loss = -11441.841484527195
Iteration 3800: Loss = -11441.839985642006
Iteration 3900: Loss = -11441.83835117492
Iteration 4000: Loss = -11441.83679605618
Iteration 4100: Loss = -11441.835207425669
Iteration 4200: Loss = -11441.832865314393
Iteration 4300: Loss = -11441.823166610759
Iteration 4400: Loss = -11441.812425902268
Iteration 4500: Loss = -11441.821486354158
1
Iteration 4600: Loss = -11441.804814250858
Iteration 4700: Loss = -11441.793295977935
Iteration 4800: Loss = -11441.697265016737
Iteration 4900: Loss = -11441.00926558712
Iteration 5000: Loss = -11438.005959844828
Iteration 5100: Loss = -11437.780601380902
Iteration 5200: Loss = -11437.757731971562
Iteration 5300: Loss = -11437.750780367722
Iteration 5400: Loss = -11437.748310405568
Iteration 5500: Loss = -11437.747914831605
Iteration 5600: Loss = -11437.745113083667
Iteration 5700: Loss = -11437.74509038126
Iteration 5800: Loss = -11437.740724514291
Iteration 5900: Loss = -11437.736829993473
Iteration 6000: Loss = -11437.73752727788
1
Iteration 6100: Loss = -11437.73452611992
Iteration 6200: Loss = -11437.738582056643
1
Iteration 6300: Loss = -11437.733115174538
Iteration 6400: Loss = -11437.733907888449
1
Iteration 6500: Loss = -11437.730786501814
Iteration 6600: Loss = -11437.729691812456
Iteration 6700: Loss = -11437.729387695797
Iteration 6800: Loss = -11437.730666073903
1
Iteration 6900: Loss = -11437.735645007444
2
Iteration 7000: Loss = -11437.728459900842
Iteration 7100: Loss = -11437.729144476747
1
Iteration 7200: Loss = -11437.727929259861
Iteration 7300: Loss = -11437.727694229203
Iteration 7400: Loss = -11437.727433243115
Iteration 7500: Loss = -11437.730581884673
1
Iteration 7600: Loss = -11437.727113583927
Iteration 7700: Loss = -11437.738789340887
1
Iteration 7800: Loss = -11437.72728210446
2
Iteration 7900: Loss = -11437.74772066505
3
Iteration 8000: Loss = -11437.726644859387
Iteration 8100: Loss = -11437.726567631444
Iteration 8200: Loss = -11437.739362061966
1
Iteration 8300: Loss = -11437.726346929627
Iteration 8400: Loss = -11437.731585678817
1
Iteration 8500: Loss = -11437.728020549219
2
Iteration 8600: Loss = -11437.726050207833
Iteration 8700: Loss = -11437.728251042248
1
Iteration 8800: Loss = -11437.72553847686
Iteration 8900: Loss = -11437.732824397548
1
Iteration 9000: Loss = -11437.723960104377
Iteration 9100: Loss = -11437.725761293434
1
Iteration 9200: Loss = -11437.723848735197
Iteration 9300: Loss = -11437.723767300286
Iteration 9400: Loss = -11437.723938744659
1
Iteration 9500: Loss = -11437.722999385165
Iteration 9600: Loss = -11437.72259741348
Iteration 9700: Loss = -11437.722806766647
1
Iteration 9800: Loss = -11437.72291530464
2
Iteration 9900: Loss = -11437.72252166306
Iteration 10000: Loss = -11437.72257304943
Iteration 10100: Loss = -11437.72382271197
1
Iteration 10200: Loss = -11437.722738109673
2
Iteration 10300: Loss = -11437.722449069923
Iteration 10400: Loss = -11437.72308111982
1
Iteration 10500: Loss = -11437.991145051345
2
Iteration 10600: Loss = -11437.722383045491
Iteration 10700: Loss = -11437.724257085218
1
Iteration 10800: Loss = -11437.722350421334
Iteration 10900: Loss = -11437.729611529856
1
Iteration 11000: Loss = -11437.72228819768
Iteration 11100: Loss = -11438.106838097574
1
Iteration 11200: Loss = -11437.722080382597
Iteration 11300: Loss = -11437.722069194017
Iteration 11400: Loss = -11437.722505552465
1
Iteration 11500: Loss = -11437.722028085842
Iteration 11600: Loss = -11438.116061559373
1
Iteration 11700: Loss = -11437.722047261586
Iteration 11800: Loss = -11437.722029763394
Iteration 11900: Loss = -11437.723605937097
1
Iteration 12000: Loss = -11437.722032533224
Iteration 12100: Loss = -11437.722089640363
Iteration 12200: Loss = -11437.740049697859
1
Iteration 12300: Loss = -11437.722007539774
Iteration 12400: Loss = -11437.726338791686
1
Iteration 12500: Loss = -11437.721990173874
Iteration 12600: Loss = -11437.721987258345
Iteration 12700: Loss = -11437.722286694607
1
Iteration 12800: Loss = -11437.72196971953
Iteration 12900: Loss = -11437.727189123916
1
Iteration 13000: Loss = -11437.722207167042
2
Iteration 13100: Loss = -11437.722164817234
3
Iteration 13200: Loss = -11437.724701877523
4
Iteration 13300: Loss = -11437.80533538455
5
Iteration 13400: Loss = -11437.721958709893
Iteration 13500: Loss = -11437.766902693724
1
Iteration 13600: Loss = -11437.722188230424
2
Iteration 13700: Loss = -11437.72201800876
Iteration 13800: Loss = -11437.724095461606
1
Iteration 13900: Loss = -11437.721925412
Iteration 14000: Loss = -11437.727775677307
1
Iteration 14100: Loss = -11437.721914979516
Iteration 14200: Loss = -11437.731075673842
1
Iteration 14300: Loss = -11437.735508473095
2
Iteration 14400: Loss = -11437.722083999817
3
Iteration 14500: Loss = -11437.72190284908
Iteration 14600: Loss = -11437.733630798712
1
Iteration 14700: Loss = -11437.721808688748
Iteration 14800: Loss = -11437.722046756611
1
Iteration 14900: Loss = -11437.785847776688
2
Iteration 15000: Loss = -11437.721789381825
Iteration 15100: Loss = -11437.77157429314
1
Iteration 15200: Loss = -11437.72177969992
Iteration 15300: Loss = -11437.80578519195
1
Iteration 15400: Loss = -11437.721772223294
Iteration 15500: Loss = -11437.723318241091
1
Iteration 15600: Loss = -11437.752338716167
2
Iteration 15700: Loss = -11437.72191374967
3
Iteration 15800: Loss = -11437.722090026873
4
Iteration 15900: Loss = -11437.725188705153
5
Iteration 16000: Loss = -11437.721954244415
6
Iteration 16100: Loss = -11437.721868546727
Iteration 16200: Loss = -11437.762399222862
1
Iteration 16300: Loss = -11437.721774596821
Iteration 16400: Loss = -11437.728259885052
1
Iteration 16500: Loss = -11437.721727838863
Iteration 16600: Loss = -11437.721995885397
1
Iteration 16700: Loss = -11437.723722227282
2
Iteration 16800: Loss = -11437.787364132437
3
Iteration 16900: Loss = -11437.721753650447
Iteration 17000: Loss = -11437.725778745413
1
Iteration 17100: Loss = -11437.721617974961
Iteration 17200: Loss = -11437.723783262052
1
Iteration 17300: Loss = -11437.721608302883
Iteration 17400: Loss = -11437.728504837527
1
Iteration 17500: Loss = -11437.721609013211
Iteration 17600: Loss = -11437.721612925627
Iteration 17700: Loss = -11437.738631596405
1
Iteration 17800: Loss = -11437.721605738105
Iteration 17900: Loss = -11437.721594080298
Iteration 18000: Loss = -11437.72180508736
1
Iteration 18100: Loss = -11437.75776076462
2
Iteration 18200: Loss = -11437.72159218898
Iteration 18300: Loss = -11437.729678946382
1
Iteration 18400: Loss = -11437.723568439967
2
Iteration 18500: Loss = -11437.736193255523
3
Iteration 18600: Loss = -11437.721641354294
Iteration 18700: Loss = -11437.721651349526
Iteration 18800: Loss = -11437.75703914342
1
Iteration 18900: Loss = -11437.721609160088
Iteration 19000: Loss = -11437.72779235317
1
Iteration 19100: Loss = -11437.721593663291
Iteration 19200: Loss = -11437.731550651237
1
Iteration 19300: Loss = -11437.721597778445
Iteration 19400: Loss = -11437.756083276523
1
Iteration 19500: Loss = -11437.722447998009
2
Iteration 19600: Loss = -11437.749416209605
3
Iteration 19700: Loss = -11437.721591896609
Iteration 19800: Loss = -11437.721737908294
1
Iteration 19900: Loss = -11437.875238984743
2
pi: tensor([[0.7547, 0.2453],
        [0.3730, 0.6270]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0883, 0.9117], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3069, 0.0949],
         [0.6783, 0.1978]],

        [[0.7114, 0.1024],
         [0.5823, 0.5762]],

        [[0.5294, 0.1107],
         [0.5786, 0.6084]],

        [[0.5203, 0.0974],
         [0.5985, 0.5275]],

        [[0.5875, 0.1024],
         [0.5042, 0.5108]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.02269374787246114
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5828600204236454
Average Adjusted Rand Index: 0.7965379659042215
11381.101212346859
[0.039695934781989725, 0.5828600204236454] [0.8984060833048074, 0.7965379659042215] [11467.795665841075, 11437.721588607075]
-------------------------------------
This iteration is 28
True Objective function: Loss = -11530.046689037514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20871.59628670595
Iteration 100: Loss = -11801.369280524817
Iteration 200: Loss = -11800.665635682437
Iteration 300: Loss = -11799.182771179085
Iteration 400: Loss = -11784.286101868454
Iteration 500: Loss = -11683.68375032865
Iteration 600: Loss = -11576.917718988014
Iteration 700: Loss = -11560.937431351475
Iteration 800: Loss = -11557.652256572203
Iteration 900: Loss = -11556.375727140929
Iteration 1000: Loss = -11555.450218598966
Iteration 1100: Loss = -11555.291550508055
Iteration 1200: Loss = -11555.203259443942
Iteration 1300: Loss = -11555.123274747943
Iteration 1400: Loss = -11554.930912890613
Iteration 1500: Loss = -11553.315398736366
Iteration 1600: Loss = -11552.983439067375
Iteration 1700: Loss = -11552.88711212734
Iteration 1800: Loss = -11552.8328593676
Iteration 1900: Loss = -11552.79500020459
Iteration 2000: Loss = -11552.766260656379
Iteration 2100: Loss = -11552.742594378637
Iteration 2200: Loss = -11552.721076968332
Iteration 2300: Loss = -11552.6934922671
Iteration 2400: Loss = -11552.526598050601
Iteration 2500: Loss = -11552.456493665324
Iteration 2600: Loss = -11552.418568849205
Iteration 2700: Loss = -11552.353189733132
Iteration 2800: Loss = -11552.341474006695
Iteration 2900: Loss = -11552.332106591773
Iteration 3000: Loss = -11552.320583537177
Iteration 3100: Loss = -11552.344224537712
1
Iteration 3200: Loss = -11552.296425357952
Iteration 3300: Loss = -11552.29082522323
Iteration 3400: Loss = -11552.287058811966
Iteration 3500: Loss = -11552.284428685734
Iteration 3600: Loss = -11552.29505350168
1
Iteration 3700: Loss = -11552.279063475607
Iteration 3800: Loss = -11552.277335280056
Iteration 3900: Loss = -11552.275344192249
Iteration 4000: Loss = -11552.273602775856
Iteration 4100: Loss = -11552.271803326928
Iteration 4200: Loss = -11552.269897520913
Iteration 4300: Loss = -11552.268393590424
Iteration 4400: Loss = -11552.267446011265
Iteration 4500: Loss = -11552.266744530625
Iteration 4600: Loss = -11552.265655084211
Iteration 4700: Loss = -11552.26487694965
Iteration 4800: Loss = -11552.268110875642
1
Iteration 4900: Loss = -11552.26343479126
Iteration 5000: Loss = -11552.272593971991
1
Iteration 5100: Loss = -11552.26192082337
Iteration 5200: Loss = -11552.260943063671
Iteration 5300: Loss = -11552.259553075235
Iteration 5400: Loss = -11552.255621533946
Iteration 5500: Loss = -11552.248510137002
Iteration 5600: Loss = -11552.247224716157
Iteration 5700: Loss = -11552.246787142154
Iteration 5800: Loss = -11552.246750529343
Iteration 5900: Loss = -11552.245905920627
Iteration 6000: Loss = -11552.246841248198
1
Iteration 6100: Loss = -11552.245520499355
Iteration 6200: Loss = -11552.245204715928
Iteration 6300: Loss = -11552.244970203352
Iteration 6400: Loss = -11552.244752970166
Iteration 6500: Loss = -11552.245213952172
1
Iteration 6600: Loss = -11552.244383000438
Iteration 6700: Loss = -11552.248126083896
1
Iteration 6800: Loss = -11552.24407665137
Iteration 6900: Loss = -11552.24472590839
1
Iteration 7000: Loss = -11552.243834678982
Iteration 7100: Loss = -11552.243824603722
Iteration 7200: Loss = -11552.24387895807
Iteration 7300: Loss = -11552.243076294792
Iteration 7400: Loss = -11552.228148312526
Iteration 7500: Loss = -11552.22712005924
Iteration 7600: Loss = -11552.227003473528
Iteration 7700: Loss = -11552.226907204455
Iteration 7800: Loss = -11552.226981924387
Iteration 7900: Loss = -11552.226759159703
Iteration 8000: Loss = -11552.23469934706
1
Iteration 8100: Loss = -11552.226595328977
Iteration 8200: Loss = -11552.228183166779
1
Iteration 8300: Loss = -11552.226471539185
Iteration 8400: Loss = -11552.228137724082
1
Iteration 8500: Loss = -11552.226187520982
Iteration 8600: Loss = -11552.28551151723
1
Iteration 8700: Loss = -11552.225369136433
Iteration 8800: Loss = -11552.245056905711
1
Iteration 8900: Loss = -11552.225141843437
Iteration 9000: Loss = -11552.293871542863
1
Iteration 9100: Loss = -11552.223997248442
Iteration 9200: Loss = -11552.22897885301
1
Iteration 9300: Loss = -11552.223868491357
Iteration 9400: Loss = -11552.22547878019
1
Iteration 9500: Loss = -11552.223741589047
Iteration 9600: Loss = -11552.255073699418
1
Iteration 9700: Loss = -11552.223691038771
Iteration 9800: Loss = -11552.224283508172
1
Iteration 9900: Loss = -11552.223680830579
Iteration 10000: Loss = -11552.581996187007
1
Iteration 10100: Loss = -11552.223568031643
Iteration 10200: Loss = -11552.224004488218
1
Iteration 10300: Loss = -11552.223502183715
Iteration 10400: Loss = -11552.22348687142
Iteration 10500: Loss = -11552.230108052525
1
Iteration 10600: Loss = -11552.22293640303
Iteration 10700: Loss = -11552.218366978823
Iteration 10800: Loss = -11552.284261723056
1
Iteration 10900: Loss = -11552.216873408248
Iteration 11000: Loss = -11552.216065449795
Iteration 11100: Loss = -11552.242335790037
1
Iteration 11200: Loss = -11552.215906534755
Iteration 11300: Loss = -11552.215747902002
Iteration 11400: Loss = -11552.215995851211
1
Iteration 11500: Loss = -11552.215679865054
Iteration 11600: Loss = -11552.223816079862
1
Iteration 11700: Loss = -11552.215678072482
Iteration 11800: Loss = -11552.216070181446
1
Iteration 11900: Loss = -11552.216156085147
2
Iteration 12000: Loss = -11552.215663261222
Iteration 12100: Loss = -11552.31601868071
1
Iteration 12200: Loss = -11552.21566485501
Iteration 12300: Loss = -11552.220036871026
1
Iteration 12400: Loss = -11552.215658776915
Iteration 12500: Loss = -11552.21562153386
Iteration 12600: Loss = -11552.222352283707
1
Iteration 12700: Loss = -11552.215627842193
Iteration 12800: Loss = -11552.287906547446
1
Iteration 12900: Loss = -11552.215593165432
Iteration 13000: Loss = -11552.21560155016
Iteration 13100: Loss = -11552.215676123114
Iteration 13200: Loss = -11552.215592583796
Iteration 13300: Loss = -11552.252587629553
1
Iteration 13400: Loss = -11552.215596309634
Iteration 13500: Loss = -11552.215572001482
Iteration 13600: Loss = -11552.221998653858
1
Iteration 13700: Loss = -11552.215548704946
Iteration 13800: Loss = -11552.215563066064
Iteration 13900: Loss = -11552.217003928181
1
Iteration 14000: Loss = -11552.215557725098
Iteration 14100: Loss = -11552.215592292796
Iteration 14200: Loss = -11552.215693394453
1
Iteration 14300: Loss = -11552.215581827797
Iteration 14400: Loss = -11552.21723343614
1
Iteration 14500: Loss = -11552.21553575786
Iteration 14600: Loss = -11552.215547194744
Iteration 14700: Loss = -11552.216618193266
1
Iteration 14800: Loss = -11552.215539537296
Iteration 14900: Loss = -11552.245651160143
1
Iteration 15000: Loss = -11552.215553020957
Iteration 15100: Loss = -11552.218189571866
1
Iteration 15200: Loss = -11552.215587964729
Iteration 15300: Loss = -11552.215565316648
Iteration 15400: Loss = -11552.215750783122
1
Iteration 15500: Loss = -11552.21552744107
Iteration 15600: Loss = -11552.226766442747
1
Iteration 15700: Loss = -11552.215541697791
Iteration 15800: Loss = -11552.215543186017
Iteration 15900: Loss = -11552.215536686686
Iteration 16000: Loss = -11552.21550765919
Iteration 16100: Loss = -11552.21743098786
1
Iteration 16200: Loss = -11552.215536969063
Iteration 16300: Loss = -11552.215599252851
Iteration 16400: Loss = -11552.21556315772
Iteration 16500: Loss = -11552.215497576513
Iteration 16600: Loss = -11552.215588036917
Iteration 16700: Loss = -11552.21556028142
Iteration 16800: Loss = -11552.215510899572
Iteration 16900: Loss = -11552.215515364715
Iteration 17000: Loss = -11552.215798459763
1
Iteration 17100: Loss = -11552.215554076922
Iteration 17200: Loss = -11552.218040414602
1
Iteration 17300: Loss = -11552.217413159035
2
Iteration 17400: Loss = -11552.217423509432
3
Iteration 17500: Loss = -11552.21556710822
Iteration 17600: Loss = -11552.21550201289
Iteration 17700: Loss = -11552.215947828603
1
Iteration 17800: Loss = -11552.215522513909
Iteration 17900: Loss = -11552.704843712707
1
Iteration 18000: Loss = -11552.215542011738
Iteration 18100: Loss = -11552.215521177332
Iteration 18200: Loss = -11552.266624100295
1
Iteration 18300: Loss = -11552.215498528305
Iteration 18400: Loss = -11552.215490048931
Iteration 18500: Loss = -11552.218350428004
1
Iteration 18600: Loss = -11552.215491144661
Iteration 18700: Loss = -11552.326753007552
1
Iteration 18800: Loss = -11552.215495034585
Iteration 18900: Loss = -11552.215488415597
Iteration 19000: Loss = -11552.215537337977
Iteration 19100: Loss = -11552.215495821998
Iteration 19200: Loss = -11552.513786961263
1
Iteration 19300: Loss = -11552.215482976728
Iteration 19400: Loss = -11552.215481664341
Iteration 19500: Loss = -11552.250188902068
1
Iteration 19600: Loss = -11552.215489156775
Iteration 19700: Loss = -11552.21547712298
Iteration 19800: Loss = -11552.22220820231
1
Iteration 19900: Loss = -11552.21546445034
pi: tensor([[0.8037, 0.1963],
        [0.4021, 0.5979]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0240, 0.9760], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2965, 0.0934],
         [0.5198, 0.1883]],

        [[0.6721, 0.1195],
         [0.6333, 0.6368]],

        [[0.5085, 0.0975],
         [0.6117, 0.5206]],

        [[0.5836, 0.1056],
         [0.7133, 0.6892]],

        [[0.7227, 0.1032],
         [0.5827, 0.6178]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448509923071951
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9206925302859384
Global Adjusted Rand Index: 0.5767252643332025
Average Adjusted Rand Index: 0.7219146687262257
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19164.745158902366
Iteration 100: Loss = -11800.941672782305
Iteration 200: Loss = -11800.054648374024
Iteration 300: Loss = -11794.512826844908
Iteration 400: Loss = -11770.528205713033
Iteration 500: Loss = -11638.815994216573
Iteration 600: Loss = -11628.2711735028
Iteration 700: Loss = -11619.577051704278
Iteration 800: Loss = -11607.307845882056
Iteration 900: Loss = -11595.385518193318
Iteration 1000: Loss = -11587.32545620961
Iteration 1100: Loss = -11584.44999435934
Iteration 1200: Loss = -11576.182360541216
Iteration 1300: Loss = -11575.581845834844
Iteration 1400: Loss = -11575.537818461702
Iteration 1500: Loss = -11575.515718657736
Iteration 1600: Loss = -11575.490330886181
Iteration 1700: Loss = -11575.478079277658
Iteration 1800: Loss = -11575.470200446698
Iteration 1900: Loss = -11575.46443767871
Iteration 2000: Loss = -11575.460653778313
Iteration 2100: Loss = -11575.457068381056
Iteration 2200: Loss = -11575.45434005528
Iteration 2300: Loss = -11575.45183430757
Iteration 2400: Loss = -11575.45527240087
1
Iteration 2500: Loss = -11575.447013657851
Iteration 2600: Loss = -11575.436016033547
Iteration 2700: Loss = -11575.403383864654
Iteration 2800: Loss = -11575.381391180783
Iteration 2900: Loss = -11575.379974437636
Iteration 3000: Loss = -11575.378775352525
Iteration 3100: Loss = -11575.37849667904
Iteration 3200: Loss = -11575.382747850466
1
Iteration 3300: Loss = -11575.376733069059
Iteration 3400: Loss = -11575.3765952502
Iteration 3500: Loss = -11575.376673344172
Iteration 3600: Loss = -11575.381038898471
1
Iteration 3700: Loss = -11575.375780557464
Iteration 3800: Loss = -11575.37525546651
Iteration 3900: Loss = -11575.373672636035
Iteration 4000: Loss = -11575.373693867246
Iteration 4100: Loss = -11575.373212633354
Iteration 4200: Loss = -11575.372482360435
Iteration 4300: Loss = -11575.372119540398
Iteration 4400: Loss = -11575.37186890106
Iteration 4500: Loss = -11575.371880964241
Iteration 4600: Loss = -11575.376095204616
1
Iteration 4700: Loss = -11575.389274281904
2
Iteration 4800: Loss = -11575.371087195106
Iteration 4900: Loss = -11575.371498001135
1
Iteration 5000: Loss = -11575.370569960474
Iteration 5100: Loss = -11575.370958007361
1
Iteration 5200: Loss = -11575.373618531807
2
Iteration 5300: Loss = -11575.37397692973
3
Iteration 5400: Loss = -11575.377100015321
4
Iteration 5500: Loss = -11575.371963821723
5
Iteration 5600: Loss = -11575.369113944787
Iteration 5700: Loss = -11575.369637333415
1
Iteration 5800: Loss = -11575.369327243257
2
Iteration 5900: Loss = -11575.374095499807
3
Iteration 6000: Loss = -11575.368913904676
Iteration 6100: Loss = -11575.368653548698
Iteration 6200: Loss = -11575.374901370358
1
Iteration 6300: Loss = -11575.380860567358
2
Iteration 6400: Loss = -11575.383687051566
3
Iteration 6500: Loss = -11575.378291403242
4
Iteration 6600: Loss = -11575.370527873105
5
Iteration 6700: Loss = -11575.36798017617
Iteration 6800: Loss = -11575.367864091517
Iteration 6900: Loss = -11575.421877822235
1
Iteration 7000: Loss = -11575.34483014666
Iteration 7100: Loss = -11575.365029179988
1
Iteration 7200: Loss = -11575.344730712306
Iteration 7300: Loss = -11575.344690326208
Iteration 7400: Loss = -11575.344848551134
1
Iteration 7500: Loss = -11575.344622829864
Iteration 7600: Loss = -11575.350905456427
1
Iteration 7700: Loss = -11575.344558968998
Iteration 7800: Loss = -11575.344535949074
Iteration 7900: Loss = -11575.344474103282
Iteration 8000: Loss = -11575.344521242483
Iteration 8100: Loss = -11575.34420928762
Iteration 8200: Loss = -11575.341200970823
Iteration 8300: Loss = -11575.35069896451
1
Iteration 8400: Loss = -11575.340362923442
Iteration 8500: Loss = -11575.340371613143
Iteration 8600: Loss = -11575.391048175221
1
Iteration 8700: Loss = -11575.34037567549
Iteration 8800: Loss = -11575.340291269351
Iteration 8900: Loss = -11575.3883017179
1
Iteration 9000: Loss = -11575.34029213021
Iteration 9100: Loss = -11575.340895016107
1
Iteration 9200: Loss = -11575.341576592427
2
Iteration 9300: Loss = -11575.34016239446
Iteration 9400: Loss = -11575.3152378437
Iteration 9500: Loss = -11575.318599539445
1
Iteration 9600: Loss = -11575.31514883204
Iteration 9700: Loss = -11575.315859314294
1
Iteration 9800: Loss = -11575.31511235815
Iteration 9900: Loss = -11575.315152112877
Iteration 10000: Loss = -11575.315078126061
Iteration 10100: Loss = -11575.326536717155
1
Iteration 10200: Loss = -11575.315072793459
Iteration 10300: Loss = -11575.315072797097
Iteration 10400: Loss = -11575.325784373905
1
Iteration 10500: Loss = -11575.314916381567
Iteration 10600: Loss = -11575.314885566771
Iteration 10700: Loss = -11575.315310957772
1
Iteration 10800: Loss = -11575.314859240756
Iteration 10900: Loss = -11575.315411966745
1
Iteration 11000: Loss = -11575.31843877281
2
Iteration 11100: Loss = -11575.31544628268
3
Iteration 11200: Loss = -11575.326511166877
4
Iteration 11300: Loss = -11575.314882779807
Iteration 11400: Loss = -11575.315111079448
1
Iteration 11500: Loss = -11575.314854611583
Iteration 11600: Loss = -11575.318258645539
1
Iteration 11700: Loss = -11575.314818212726
Iteration 11800: Loss = -11575.316795833924
1
Iteration 11900: Loss = -11575.314899746685
Iteration 12000: Loss = -11575.31484308848
Iteration 12100: Loss = -11575.318076565714
1
Iteration 12200: Loss = -11575.321654761576
2
Iteration 12300: Loss = -11575.543689077986
3
Iteration 12400: Loss = -11575.314867171612
Iteration 12500: Loss = -11575.319538914366
1
Iteration 12600: Loss = -11575.266205795626
Iteration 12700: Loss = -11575.266127073664
Iteration 12800: Loss = -11575.26941904159
1
Iteration 12900: Loss = -11575.265820274059
Iteration 13000: Loss = -11575.3185102134
1
Iteration 13100: Loss = -11575.265811152181
Iteration 13200: Loss = -11575.265839958207
Iteration 13300: Loss = -11575.265888568665
Iteration 13400: Loss = -11575.265817169298
Iteration 13500: Loss = -11575.361513915885
1
Iteration 13600: Loss = -11575.265819634506
Iteration 13700: Loss = -11575.265808268965
Iteration 13800: Loss = -11575.277637922107
1
Iteration 13900: Loss = -11575.265790262954
Iteration 14000: Loss = -11575.265795088571
Iteration 14100: Loss = -11575.284569887708
1
Iteration 14200: Loss = -11575.265827242401
Iteration 14300: Loss = -11575.265801850823
Iteration 14400: Loss = -11575.300827851712
1
Iteration 14500: Loss = -11575.265794728188
Iteration 14600: Loss = -11575.265810304345
Iteration 14700: Loss = -11575.2686037412
1
Iteration 14800: Loss = -11575.265831449684
Iteration 14900: Loss = -11575.265799395813
Iteration 15000: Loss = -11575.4845010931
1
Iteration 15100: Loss = -11575.265802134933
Iteration 15200: Loss = -11575.265797310534
Iteration 15300: Loss = -11575.304338045935
1
Iteration 15400: Loss = -11575.265810423767
Iteration 15500: Loss = -11575.277671736676
1
Iteration 15600: Loss = -11575.267511745373
2
Iteration 15700: Loss = -11575.378534493037
3
Iteration 15800: Loss = -11575.378613014967
4
Iteration 15900: Loss = -11575.265813961423
Iteration 16000: Loss = -11575.272989751045
1
Iteration 16100: Loss = -11575.26580556835
Iteration 16200: Loss = -11575.269692060281
1
Iteration 16300: Loss = -11575.26577528634
Iteration 16400: Loss = -11575.291565610296
1
Iteration 16500: Loss = -11575.265800593914
Iteration 16600: Loss = -11575.265803606595
Iteration 16700: Loss = -11575.26616675905
1
Iteration 16800: Loss = -11575.265793660381
Iteration 16900: Loss = -11575.388570799141
1
Iteration 17000: Loss = -11575.265789145109
Iteration 17100: Loss = -11575.265812476566
Iteration 17200: Loss = -11575.278907450122
1
Iteration 17300: Loss = -11575.265806891866
Iteration 17400: Loss = -11575.26574299017
Iteration 17500: Loss = -11575.28714812313
1
Iteration 17600: Loss = -11575.265755094975
Iteration 17700: Loss = -11575.265756426175
Iteration 17800: Loss = -11575.30130973825
1
Iteration 17900: Loss = -11575.26575785385
Iteration 18000: Loss = -11575.265757228524
Iteration 18100: Loss = -11575.265730564646
Iteration 18200: Loss = -11575.26968795878
1
Iteration 18300: Loss = -11575.265734650782
Iteration 18400: Loss = -11575.265773604
Iteration 18500: Loss = -11575.265929916643
1
Iteration 18600: Loss = -11575.26902859041
2
Iteration 18700: Loss = -11575.265804582643
Iteration 18800: Loss = -11575.265865456606
Iteration 18900: Loss = -11575.267120864342
1
Iteration 19000: Loss = -11575.26581618702
Iteration 19100: Loss = -11575.35973949044
1
Iteration 19200: Loss = -11575.265787776218
Iteration 19300: Loss = -11575.269597232284
1
Iteration 19400: Loss = -11575.265798946662
Iteration 19500: Loss = -11575.265754742577
Iteration 19600: Loss = -11575.267119889528
1
Iteration 19700: Loss = -11575.265757022087
Iteration 19800: Loss = -11575.265748086931
Iteration 19900: Loss = -11575.265957634565
1
pi: tensor([[0.4033, 0.5967],
        [0.6676, 0.3324]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4456, 0.5544], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2728, 0.1023],
         [0.5638, 0.2425]],

        [[0.5812, 0.1127],
         [0.6969, 0.5221]],

        [[0.5728, 0.0912],
         [0.6325, 0.6155]],

        [[0.5352, 0.1047],
         [0.7033, 0.5651]],

        [[0.6851, 0.1030],
         [0.5792, 0.5105]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7020815530487331
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8822045595164437
Global Adjusted Rand Index: 0.05009088252676396
Average Adjusted Rand Index: 0.8395896755954626
11530.046689037514
[0.5767252643332025, 0.05009088252676396] [0.7219146687262257, 0.8395896755954626] [11552.237886203007, 11575.265768493135]
-------------------------------------
This iteration is 29
True Objective function: Loss = -11025.799598201182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21411.798197724424
Iteration 100: Loss = -11220.047095577393
Iteration 200: Loss = -11219.187566185255
Iteration 300: Loss = -11218.92170674202
Iteration 400: Loss = -11218.666892600322
Iteration 500: Loss = -11217.651122723568
Iteration 600: Loss = -11215.622277563047
Iteration 700: Loss = -11171.411143034498
Iteration 800: Loss = -11170.631807132133
Iteration 900: Loss = -11170.30555560607
Iteration 1000: Loss = -11170.118556595644
Iteration 1100: Loss = -11169.991885118892
Iteration 1200: Loss = -11169.902895370338
Iteration 1300: Loss = -11169.833574584069
Iteration 1400: Loss = -11169.77771331635
Iteration 1500: Loss = -11169.733996968258
Iteration 1600: Loss = -11169.699154029548
Iteration 1700: Loss = -11169.670145333519
Iteration 1800: Loss = -11169.644217377241
Iteration 1900: Loss = -11169.617501218332
Iteration 2000: Loss = -11169.580928224339
Iteration 2100: Loss = -11169.471475104267
Iteration 2200: Loss = -11168.869774717083
Iteration 2300: Loss = -11168.754554174557
Iteration 2400: Loss = -11168.722026363475
Iteration 2500: Loss = -11168.705679339091
Iteration 2600: Loss = -11168.695091912728
Iteration 2700: Loss = -11168.687553933463
Iteration 2800: Loss = -11168.681840696061
Iteration 2900: Loss = -11168.677327825217
Iteration 3000: Loss = -11168.673690451635
Iteration 3100: Loss = -11168.670671221798
Iteration 3200: Loss = -11168.668094228216
Iteration 3300: Loss = -11168.665892996123
Iteration 3400: Loss = -11168.664042616048
Iteration 3500: Loss = -11168.662412964686
Iteration 3600: Loss = -11168.660976390482
Iteration 3700: Loss = -11168.659668678803
Iteration 3800: Loss = -11168.658571339494
Iteration 3900: Loss = -11168.657526875317
Iteration 4000: Loss = -11168.656600824279
Iteration 4100: Loss = -11168.655789691073
Iteration 4200: Loss = -11168.655110068714
Iteration 4300: Loss = -11168.65437692364
Iteration 4400: Loss = -11168.653729154954
Iteration 4500: Loss = -11168.655525123855
1
Iteration 4600: Loss = -11168.652670235819
Iteration 4700: Loss = -11168.652140731394
Iteration 4800: Loss = -11168.658934485271
1
Iteration 4900: Loss = -11168.651303805138
Iteration 5000: Loss = -11168.651053528853
Iteration 5100: Loss = -11168.650595415224
Iteration 5200: Loss = -11168.650933362975
1
Iteration 5300: Loss = -11168.649941798345
Iteration 5400: Loss = -11168.650927429957
1
Iteration 5500: Loss = -11168.64961252597
Iteration 5600: Loss = -11168.65068187303
1
Iteration 5700: Loss = -11168.648918866913
Iteration 5800: Loss = -11168.648725478011
Iteration 5900: Loss = -11168.651748274206
1
Iteration 6000: Loss = -11168.648315110553
Iteration 6100: Loss = -11168.652542003132
1
Iteration 6200: Loss = -11168.64796751934
Iteration 6300: Loss = -11168.650356601282
1
Iteration 6400: Loss = -11168.64768677088
Iteration 6500: Loss = -11168.64755060895
Iteration 6600: Loss = -11168.64742457034
Iteration 6700: Loss = -11168.64730543788
Iteration 6800: Loss = -11168.647369616794
Iteration 6900: Loss = -11168.647086649828
Iteration 7000: Loss = -11168.646927154192
Iteration 7100: Loss = -11168.646888644644
Iteration 7200: Loss = -11168.647401786813
1
Iteration 7300: Loss = -11168.646977618148
Iteration 7400: Loss = -11168.646579735407
Iteration 7500: Loss = -11168.646574336599
Iteration 7600: Loss = -11168.646436268431
Iteration 7700: Loss = -11168.649504387526
1
Iteration 7800: Loss = -11168.647227012792
2
Iteration 7900: Loss = -11168.646356366202
Iteration 8000: Loss = -11168.646244965257
Iteration 8100: Loss = -11168.647586790552
1
Iteration 8200: Loss = -11168.647829117785
2
Iteration 8300: Loss = -11168.665167680396
3
Iteration 8400: Loss = -11168.646063131491
Iteration 8500: Loss = -11168.645934294402
Iteration 8600: Loss = -11168.651011066899
1
Iteration 8700: Loss = -11168.64577967566
Iteration 8800: Loss = -11168.646363163762
1
Iteration 8900: Loss = -11168.645710929863
Iteration 9000: Loss = -11168.646271537636
1
Iteration 9100: Loss = -11168.645616173759
Iteration 9200: Loss = -11168.645609703579
Iteration 9300: Loss = -11168.700956879946
1
Iteration 9400: Loss = -11168.645530493643
Iteration 9500: Loss = -11168.645490386623
Iteration 9600: Loss = -11168.645888820565
1
Iteration 9700: Loss = -11168.645443561922
Iteration 9800: Loss = -11168.645452361498
Iteration 9900: Loss = -11168.717517137484
1
Iteration 10000: Loss = -11168.645534680856
Iteration 10100: Loss = -11168.645451645698
Iteration 10200: Loss = -11168.667047000643
1
Iteration 10300: Loss = -11168.645293517757
Iteration 10400: Loss = -11168.646080406368
1
Iteration 10500: Loss = -11168.645299702028
Iteration 10600: Loss = -11168.665169719
1
Iteration 10700: Loss = -11168.645241756496
Iteration 10800: Loss = -11168.645252057895
Iteration 10900: Loss = -11168.650520627547
1
Iteration 11000: Loss = -11168.645201270707
Iteration 11100: Loss = -11168.645215795934
Iteration 11200: Loss = -11168.647570708665
1
Iteration 11300: Loss = -11168.645179345727
Iteration 11400: Loss = -11168.759572511932
1
Iteration 11500: Loss = -11168.649122372106
2
Iteration 11600: Loss = -11168.646631703505
3
Iteration 11700: Loss = -11168.651729439172
4
Iteration 11800: Loss = -11168.645169912024
Iteration 11900: Loss = -11168.646777943113
1
Iteration 12000: Loss = -11168.645099560727
Iteration 12100: Loss = -11168.645347520349
1
Iteration 12200: Loss = -11168.650174763901
2
Iteration 12300: Loss = -11168.645139411696
Iteration 12400: Loss = -11168.665673968204
1
Iteration 12500: Loss = -11168.645098127317
Iteration 12600: Loss = -11168.670511411117
1
Iteration 12700: Loss = -11168.645094719292
Iteration 12800: Loss = -11168.655253905394
1
Iteration 12900: Loss = -11168.645074202519
Iteration 13000: Loss = -11168.667916428372
1
Iteration 13100: Loss = -11168.645041606305
Iteration 13200: Loss = -11169.05694893563
1
Iteration 13300: Loss = -11168.645079192895
Iteration 13400: Loss = -11168.645070902961
Iteration 13500: Loss = -11168.645349431114
1
Iteration 13600: Loss = -11168.645018200526
Iteration 13700: Loss = -11168.645682955574
1
Iteration 13800: Loss = -11168.64507615878
Iteration 13900: Loss = -11168.645026829141
Iteration 14000: Loss = -11168.691122114345
1
Iteration 14100: Loss = -11168.645020348324
Iteration 14200: Loss = -11168.647849457651
1
Iteration 14300: Loss = -11168.645047619673
Iteration 14400: Loss = -11168.648490204469
1
Iteration 14500: Loss = -11168.644994904354
Iteration 14600: Loss = -11168.659899950393
1
Iteration 14700: Loss = -11168.645006658084
Iteration 14800: Loss = -11168.67862371977
1
Iteration 14900: Loss = -11168.64501916384
Iteration 15000: Loss = -11168.703264597709
1
Iteration 15100: Loss = -11168.64502498075
Iteration 15200: Loss = -11168.64565950385
1
Iteration 15300: Loss = -11168.645651676768
2
Iteration 15400: Loss = -11168.645066785684
Iteration 15500: Loss = -11168.66868582646
1
Iteration 15600: Loss = -11168.64499126659
Iteration 15700: Loss = -11168.685056569811
1
Iteration 15800: Loss = -11168.644957856162
Iteration 15900: Loss = -11168.644960569904
Iteration 16000: Loss = -11168.645204427596
1
Iteration 16100: Loss = -11168.645009023412
Iteration 16200: Loss = -11168.655291326411
1
Iteration 16300: Loss = -11168.644945489279
Iteration 16400: Loss = -11168.7356466662
1
Iteration 16500: Loss = -11168.651978047254
2
Iteration 16600: Loss = -11168.648701565053
3
Iteration 16700: Loss = -11168.651650564643
4
Iteration 16800: Loss = -11168.64496575105
Iteration 16900: Loss = -11168.645668702069
1
Iteration 17000: Loss = -11168.644965184723
Iteration 17100: Loss = -11168.653090898948
1
Iteration 17200: Loss = -11168.81028316408
2
Iteration 17300: Loss = -11168.644978948701
Iteration 17400: Loss = -11168.646430340848
1
Iteration 17500: Loss = -11168.649033505522
2
Iteration 17600: Loss = -11168.645070026796
Iteration 17700: Loss = -11168.645088181995
Iteration 17800: Loss = -11168.646260121524
1
Iteration 17900: Loss = -11168.645282139218
2
Iteration 18000: Loss = -11168.646268988397
3
Iteration 18100: Loss = -11168.672622706614
4
Iteration 18200: Loss = -11168.644982779419
Iteration 18300: Loss = -11168.64703735769
1
Iteration 18400: Loss = -11168.644977785041
Iteration 18500: Loss = -11168.644967170778
Iteration 18600: Loss = -11168.647387481731
1
Iteration 18700: Loss = -11168.64499935184
Iteration 18800: Loss = -11168.64500343714
Iteration 18900: Loss = -11168.645738816258
1
Iteration 19000: Loss = -11168.6450332868
Iteration 19100: Loss = -11168.65102125582
1
Iteration 19200: Loss = -11168.644983833481
Iteration 19300: Loss = -11168.646057554708
1
Iteration 19400: Loss = -11168.645039954064
Iteration 19500: Loss = -11168.646161232622
1
Iteration 19600: Loss = -11168.668956886526
2
Iteration 19700: Loss = -11168.645117527749
Iteration 19800: Loss = -11168.645007891919
Iteration 19900: Loss = -11168.667181418843
1
pi: tensor([[2.6464e-08, 1.0000e+00],
        [5.7320e-02, 9.4268e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4919, 0.5081], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3063, 0.1060],
         [0.6458, 0.1657]],

        [[0.5284, 0.0856],
         [0.5190, 0.5364]],

        [[0.5460, 0.2023],
         [0.6491, 0.6678]],

        [[0.6695, 0.2407],
         [0.5633, 0.7169]],

        [[0.5409, 0.1834],
         [0.5702, 0.6246]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.08228505033038272
Average Adjusted Rand Index: 0.18222882790328562
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21554.60260276934
Iteration 100: Loss = -11333.468543483976
Iteration 200: Loss = -11252.357070063586
Iteration 300: Loss = -11225.386302993606
Iteration 400: Loss = -11221.836709762998
Iteration 500: Loss = -11219.266814520734
Iteration 600: Loss = -11218.593308249141
Iteration 700: Loss = -11217.792839098598
Iteration 800: Loss = -11217.240727468517
Iteration 900: Loss = -11216.876988436106
Iteration 1000: Loss = -11216.624509089132
Iteration 1100: Loss = -11216.403656204158
Iteration 1200: Loss = -11216.14224676578
Iteration 1300: Loss = -11215.29975588187
Iteration 1400: Loss = -11210.605250991226
Iteration 1500: Loss = -11209.818850705125
Iteration 1600: Loss = -11184.526795090143
Iteration 1700: Loss = -11161.175984991001
Iteration 1800: Loss = -11148.715122502012
Iteration 1900: Loss = -11114.154426852121
Iteration 2000: Loss = -11091.945498474264
Iteration 2100: Loss = -11087.363640349393
Iteration 2200: Loss = -11082.205099651172
Iteration 2300: Loss = -11082.016329792294
Iteration 2400: Loss = -11065.488954526787
Iteration 2500: Loss = -11065.433921706695
Iteration 2600: Loss = -11065.421937117451
Iteration 2700: Loss = -11065.412574654827
Iteration 2800: Loss = -11065.40082141883
Iteration 2900: Loss = -11065.391366779977
Iteration 3000: Loss = -11065.374468370084
Iteration 3100: Loss = -11065.349187006057
Iteration 3200: Loss = -11065.344628540608
Iteration 3300: Loss = -11065.340853132077
Iteration 3400: Loss = -11065.333051970367
Iteration 3500: Loss = -11065.312831974514
Iteration 3600: Loss = -11065.297406959377
Iteration 3700: Loss = -11065.296504537486
Iteration 3800: Loss = -11065.29304365009
Iteration 3900: Loss = -11065.291859491703
Iteration 4000: Loss = -11065.290654810406
Iteration 4100: Loss = -11065.289545420143
Iteration 4200: Loss = -11065.288628378628
Iteration 4300: Loss = -11065.287778229986
Iteration 4400: Loss = -11065.287984451696
1
Iteration 4500: Loss = -11065.286212733201
Iteration 4600: Loss = -11065.28551444471
Iteration 4700: Loss = -11065.28488406234
Iteration 4800: Loss = -11065.284238300937
Iteration 4900: Loss = -11065.287836452708
1
Iteration 5000: Loss = -11065.28316859261
Iteration 5100: Loss = -11065.282731993288
Iteration 5200: Loss = -11065.283588584742
1
Iteration 5300: Loss = -11065.290805829769
2
Iteration 5400: Loss = -11065.281149189566
Iteration 5500: Loss = -11065.2803570365
Iteration 5600: Loss = -11065.279960423995
Iteration 5700: Loss = -11065.278896033698
Iteration 5800: Loss = -11065.277846315637
Iteration 5900: Loss = -11065.275484317706
Iteration 6000: Loss = -11065.269225459191
Iteration 6100: Loss = -11064.785886340811
Iteration 6200: Loss = -11063.109480495854
Iteration 6300: Loss = -11062.269352270776
Iteration 6400: Loss = -11062.015024023121
Iteration 6500: Loss = -11061.87602937023
Iteration 6600: Loss = -11057.701058414292
Iteration 6700: Loss = -11053.47660985278
Iteration 6800: Loss = -11053.28049945132
Iteration 6900: Loss = -11053.278147946745
Iteration 7000: Loss = -11053.274775318838
Iteration 7100: Loss = -11053.270844539738
Iteration 7200: Loss = -11052.981128696596
Iteration 7300: Loss = -11049.817670166496
Iteration 7400: Loss = -11049.681795562956
Iteration 7500: Loss = -11049.573091562355
Iteration 7600: Loss = -11049.571567082705
Iteration 7700: Loss = -11049.570966952175
Iteration 7800: Loss = -11049.57159017315
1
Iteration 7900: Loss = -11049.567264600719
Iteration 8000: Loss = -11048.363075364397
Iteration 8100: Loss = -11048.360317194563
Iteration 8200: Loss = -11048.232402467738
Iteration 8300: Loss = -11048.230217226976
Iteration 8400: Loss = -11048.26575339352
1
Iteration 8500: Loss = -11048.216592448485
Iteration 8600: Loss = -11048.226476778005
1
Iteration 8700: Loss = -11048.216053926824
Iteration 8800: Loss = -11048.215980657133
Iteration 8900: Loss = -11048.216176376663
1
Iteration 9000: Loss = -11048.214565902395
Iteration 9100: Loss = -11048.213702471317
Iteration 9200: Loss = -11048.21398068382
1
Iteration 9300: Loss = -11048.212907162557
Iteration 9400: Loss = -11048.212867808785
Iteration 9500: Loss = -11048.230097858108
1
Iteration 9600: Loss = -11048.2127645056
Iteration 9700: Loss = -11048.212751179286
Iteration 9800: Loss = -11048.228347746905
1
Iteration 9900: Loss = -11048.212725287283
Iteration 10000: Loss = -11048.215992118528
1
Iteration 10100: Loss = -11048.214286781422
2
Iteration 10200: Loss = -11048.219658019381
3
Iteration 10300: Loss = -11048.21277925869
Iteration 10400: Loss = -11048.212647022678
Iteration 10500: Loss = -11048.130156381128
Iteration 10600: Loss = -11048.120292305666
Iteration 10700: Loss = -11048.119295514609
Iteration 10800: Loss = -11048.119168380252
Iteration 10900: Loss = -11048.119071035282
Iteration 11000: Loss = -11048.10125507565
Iteration 11100: Loss = -11048.168994760441
1
Iteration 11200: Loss = -11048.09913862566
Iteration 11300: Loss = -11048.130203133642
1
Iteration 11400: Loss = -11048.125823787437
2
Iteration 11500: Loss = -11048.09749727365
Iteration 11600: Loss = -11048.090346669449
Iteration 11700: Loss = -11048.09999019651
1
Iteration 11800: Loss = -11048.134844320375
2
Iteration 11900: Loss = -11048.087764231925
Iteration 12000: Loss = -11048.08960511251
1
Iteration 12100: Loss = -11048.087783417113
Iteration 12200: Loss = -11048.089158612835
1
Iteration 12300: Loss = -11048.087546804107
Iteration 12400: Loss = -11048.087143834935
Iteration 12500: Loss = -11048.088011756985
1
Iteration 12600: Loss = -11048.088799533252
2
Iteration 12700: Loss = -11048.09745961282
3
Iteration 12800: Loss = -11048.086774894513
Iteration 12900: Loss = -11048.086799870309
Iteration 13000: Loss = -11048.235382427394
1
Iteration 13100: Loss = -11048.086652887025
Iteration 13200: Loss = -11048.088213626315
1
Iteration 13300: Loss = -11048.095264210759
2
Iteration 13400: Loss = -11048.12134644048
3
Iteration 13500: Loss = -11048.115872224938
4
Iteration 13600: Loss = -11048.095510933044
5
Iteration 13700: Loss = -11048.07463649241
Iteration 13800: Loss = -11048.074952844754
1
Iteration 13900: Loss = -11048.074183924164
Iteration 14000: Loss = -11048.079083096913
1
Iteration 14100: Loss = -11048.075674055535
2
Iteration 14200: Loss = -11048.072384340658
Iteration 14300: Loss = -11048.072381670738
Iteration 14400: Loss = -11048.105454576464
1
Iteration 14500: Loss = -11048.179147026722
2
Iteration 14600: Loss = -11048.186243127588
3
Iteration 14700: Loss = -11048.071959221452
Iteration 14800: Loss = -11048.071725421256
Iteration 14900: Loss = -11048.104197645118
1
Iteration 15000: Loss = -11048.091069295899
2
Iteration 15100: Loss = -11048.067067865552
Iteration 15200: Loss = -11048.068254627702
1
Iteration 15300: Loss = -11048.074951851191
2
Iteration 15400: Loss = -11048.091097891152
3
Iteration 15500: Loss = -11048.066592084391
Iteration 15600: Loss = -11048.066612307295
Iteration 15700: Loss = -11048.21317106451
1
Iteration 15800: Loss = -11048.066995199215
2
Iteration 15900: Loss = -11048.065429193164
Iteration 16000: Loss = -11048.0655742533
1
Iteration 16100: Loss = -11048.065654131608
2
Iteration 16200: Loss = -11048.066344214376
3
Iteration 16300: Loss = -11048.093870053344
4
Iteration 16400: Loss = -11048.069544652355
5
Iteration 16500: Loss = -11048.068148299819
6
Iteration 16600: Loss = -11048.064380099617
Iteration 16700: Loss = -11048.099141625627
1
Iteration 16800: Loss = -11048.073629599045
2
Iteration 16900: Loss = -11048.063300198874
Iteration 17000: Loss = -11048.062986270093
Iteration 17100: Loss = -11048.062799058596
Iteration 17200: Loss = -11048.067287784806
1
Iteration 17300: Loss = -11048.171072896588
2
Iteration 17400: Loss = -11048.06279248048
Iteration 17500: Loss = -11048.064019134117
1
Iteration 17600: Loss = -11048.062876031747
Iteration 17700: Loss = -11048.062725386273
Iteration 17800: Loss = -11048.062889223671
1
Iteration 17900: Loss = -11048.070761885288
2
Iteration 18000: Loss = -11048.197276822055
3
Iteration 18100: Loss = -11048.06303078488
4
Iteration 18200: Loss = -11048.064473495311
5
Iteration 18300: Loss = -11048.070927123188
6
Iteration 18400: Loss = -11048.062931910721
7
Iteration 18500: Loss = -11048.063696581596
8
Iteration 18600: Loss = -11048.062700750877
Iteration 18700: Loss = -11048.063293172243
1
Iteration 18800: Loss = -11048.122702261378
2
Iteration 18900: Loss = -11048.087230391975
3
Iteration 19000: Loss = -11048.079349214246
4
Iteration 19100: Loss = -11048.100684738545
5
Iteration 19200: Loss = -11048.073059958717
6
Iteration 19300: Loss = -11048.064083464466
7
Iteration 19400: Loss = -11048.106534038225
8
Iteration 19500: Loss = -11048.070598521348
9
Iteration 19600: Loss = -11048.065275563025
10
Iteration 19700: Loss = -11048.056321576827
Iteration 19800: Loss = -11048.055177072423
Iteration 19900: Loss = -11048.056469004845
1
pi: tensor([[0.6721, 0.3279],
        [0.2736, 0.7264]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2725, 0.7275], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2842, 0.0989],
         [0.5462, 0.2069]],

        [[0.5729, 0.0966],
         [0.5122, 0.6689]],

        [[0.6358, 0.1049],
         [0.6210, 0.6725]],

        [[0.6178, 0.1031],
         [0.5067, 0.6644]],

        [[0.6154, 0.0925],
         [0.5441, 0.6282]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 78
Adjusted Rand Index: 0.3080330630864453
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4289646729666215
Average Adjusted Rand Index: 0.8000296324488414
11025.799598201182
[0.08228505033038272, 0.4289646729666215] [0.18222882790328562, 0.8000296324488414] [11168.646979269575, 11048.055094211295]
-------------------------------------
This iteration is 30
True Objective function: Loss = -11194.550431096859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24135.13035320695
Iteration 100: Loss = -11561.080177853864
Iteration 200: Loss = -11560.401908282742
Iteration 300: Loss = -11560.073077902129
Iteration 400: Loss = -11559.427795012145
Iteration 500: Loss = -11558.8019441937
Iteration 600: Loss = -11558.151214364412
Iteration 700: Loss = -11554.294091675843
Iteration 800: Loss = -11456.420964146037
Iteration 900: Loss = -11281.762953990556
Iteration 1000: Loss = -11276.305093460025
Iteration 1100: Loss = -11276.128667582116
Iteration 1200: Loss = -11270.165130058822
Iteration 1300: Loss = -11270.104129353502
Iteration 1400: Loss = -11270.059438369604
Iteration 1500: Loss = -11270.044350152946
Iteration 1600: Loss = -11270.03389310784
Iteration 1700: Loss = -11270.021840179068
Iteration 1800: Loss = -11269.989994999265
Iteration 1900: Loss = -11270.011619844823
1
Iteration 2000: Loss = -11269.977635464185
Iteration 2100: Loss = -11269.9730881499
Iteration 2200: Loss = -11269.966083778767
Iteration 2300: Loss = -11269.951551756036
Iteration 2400: Loss = -11269.949445315984
Iteration 2500: Loss = -11269.947713246376
Iteration 2600: Loss = -11269.945394559325
Iteration 2700: Loss = -11269.941765574034
Iteration 2800: Loss = -11269.940452560864
Iteration 2900: Loss = -11269.94634084425
1
Iteration 3000: Loss = -11269.935624086915
Iteration 3100: Loss = -11269.933973700292
Iteration 3200: Loss = -11269.933294467135
Iteration 3300: Loss = -11269.933691280808
1
Iteration 3400: Loss = -11269.931868418953
Iteration 3500: Loss = -11269.936132086872
1
Iteration 3600: Loss = -11269.916210921789
Iteration 3700: Loss = -11269.911258237582
Iteration 3800: Loss = -11269.9115623879
1
Iteration 3900: Loss = -11269.910808736882
Iteration 4000: Loss = -11269.9137417504
1
Iteration 4100: Loss = -11269.90916576867
Iteration 4200: Loss = -11269.9077416741
Iteration 4300: Loss = -11269.907307180383
Iteration 4400: Loss = -11269.907432768758
1
Iteration 4500: Loss = -11269.904847258815
Iteration 4600: Loss = -11269.900122733852
Iteration 4700: Loss = -11269.898148269673
Iteration 4800: Loss = -11269.89764385105
Iteration 4900: Loss = -11269.897131022502
Iteration 5000: Loss = -11269.907790278952
1
Iteration 5100: Loss = -11269.892585975977
Iteration 5200: Loss = -11269.892108820875
Iteration 5300: Loss = -11269.891543028874
Iteration 5400: Loss = -11269.891023627659
Iteration 5500: Loss = -11269.890515724841
Iteration 5600: Loss = -11269.89040557622
Iteration 5700: Loss = -11269.890845930555
1
Iteration 5800: Loss = -11269.890233305452
Iteration 5900: Loss = -11269.890237349235
Iteration 6000: Loss = -11269.890049535692
Iteration 6100: Loss = -11269.889958865977
Iteration 6200: Loss = -11269.906298553691
1
Iteration 6300: Loss = -11269.88976891586
Iteration 6400: Loss = -11269.890042858608
1
Iteration 6500: Loss = -11269.893032183343
2
Iteration 6600: Loss = -11269.890152664953
3
Iteration 6700: Loss = -11269.889728255857
Iteration 6800: Loss = -11269.893518169692
1
Iteration 6900: Loss = -11269.918506012342
2
Iteration 7000: Loss = -11269.889213700835
Iteration 7100: Loss = -11269.889323351137
1
Iteration 7200: Loss = -11269.88902991652
Iteration 7300: Loss = -11269.889046690843
Iteration 7400: Loss = -11269.88900079027
Iteration 7500: Loss = -11269.901620558872
1
Iteration 7600: Loss = -11269.888958518819
Iteration 7700: Loss = -11269.88884781824
Iteration 7800: Loss = -11269.888930709647
Iteration 7900: Loss = -11269.893754462824
1
Iteration 8000: Loss = -11269.87679713034
Iteration 8100: Loss = -11269.859531647322
Iteration 8200: Loss = -11269.875076400709
1
Iteration 8300: Loss = -11269.85960561509
Iteration 8400: Loss = -11269.860116730808
1
Iteration 8500: Loss = -11269.859757346321
2
Iteration 8600: Loss = -11269.952536850516
3
Iteration 8700: Loss = -11269.859348334248
Iteration 8800: Loss = -11269.86488579107
1
Iteration 8900: Loss = -11269.859353215694
Iteration 9000: Loss = -11269.901453806551
1
Iteration 9100: Loss = -11269.8592997091
Iteration 9200: Loss = -11269.85927597791
Iteration 9300: Loss = -11269.859414571052
1
Iteration 9400: Loss = -11269.8581363023
Iteration 9500: Loss = -11269.969442571311
1
Iteration 9600: Loss = -11269.857590822663
Iteration 9700: Loss = -11269.86561696152
1
Iteration 9800: Loss = -11269.857561601188
Iteration 9900: Loss = -11269.859957040853
1
Iteration 10000: Loss = -11269.69341144289
Iteration 10100: Loss = -11269.692552999335
Iteration 10200: Loss = -11269.693310021954
1
Iteration 10300: Loss = -11269.69283885847
2
Iteration 10400: Loss = -11269.696433678231
3
Iteration 10500: Loss = -11269.692213909604
Iteration 10600: Loss = -11269.692440769071
1
Iteration 10700: Loss = -11269.69494084416
2
Iteration 10800: Loss = -11269.692336596318
3
Iteration 10900: Loss = -11269.695015505495
4
Iteration 11000: Loss = -11269.692928098602
5
Iteration 11100: Loss = -11269.692244165373
Iteration 11200: Loss = -11269.692762474931
1
Iteration 11300: Loss = -11269.82335647179
2
Iteration 11400: Loss = -11269.6942460838
3
Iteration 11500: Loss = -11269.69858020303
4
Iteration 11600: Loss = -11269.691972215302
Iteration 11700: Loss = -11269.691864768582
Iteration 11800: Loss = -11269.714133764403
1
Iteration 11900: Loss = -11269.697467908432
2
Iteration 12000: Loss = -11269.691758734572
Iteration 12100: Loss = -11269.693501981335
1
Iteration 12200: Loss = -11269.746110251654
2
Iteration 12300: Loss = -11269.812889651204
3
Iteration 12400: Loss = -11269.69149665503
Iteration 12500: Loss = -11269.697641919754
1
Iteration 12600: Loss = -11269.689327936354
Iteration 12700: Loss = -11267.623790117488
Iteration 12800: Loss = -11267.646126422787
1
Iteration 12900: Loss = -11267.616024488103
Iteration 13000: Loss = -11267.616972280432
1
Iteration 13100: Loss = -11267.77576473734
2
Iteration 13200: Loss = -11267.618217171352
3
Iteration 13300: Loss = -11267.616572162131
4
Iteration 13400: Loss = -11267.74434195357
5
Iteration 13500: Loss = -11267.619526976512
6
Iteration 13600: Loss = -11267.616399866592
7
Iteration 13700: Loss = -11267.773130033978
8
Iteration 13800: Loss = -11267.62108021723
9
Iteration 13900: Loss = -11267.614722059992
Iteration 14000: Loss = -11267.540484874648
Iteration 14100: Loss = -11267.540926204414
1
Iteration 14200: Loss = -11267.53853890119
Iteration 14300: Loss = -11267.537986614292
Iteration 14400: Loss = -11267.595933667923
1
Iteration 14500: Loss = -11267.548792301497
2
Iteration 14600: Loss = -11267.529427332338
Iteration 14700: Loss = -11267.530976579656
1
Iteration 14800: Loss = -11267.5478335294
2
Iteration 14900: Loss = -11267.529196046826
Iteration 15000: Loss = -11267.53079953386
1
Iteration 15100: Loss = -11267.529594936706
2
Iteration 15200: Loss = -11267.529187486265
Iteration 15300: Loss = -11267.529329544583
1
Iteration 15400: Loss = -11267.53680455042
2
Iteration 15500: Loss = -11267.543966392208
3
Iteration 15600: Loss = -11267.529193246506
Iteration 15700: Loss = -11267.552201847848
1
Iteration 15800: Loss = -11267.529178522484
Iteration 15900: Loss = -11267.52926737208
Iteration 16000: Loss = -11267.533960665154
1
Iteration 16100: Loss = -11267.529127873166
Iteration 16200: Loss = -11267.532772237879
1
Iteration 16300: Loss = -11267.529187893835
Iteration 16400: Loss = -11267.534169750801
1
Iteration 16500: Loss = -11267.546578442507
2
Iteration 16600: Loss = -11267.529352306432
3
Iteration 16700: Loss = -11267.530538739538
4
Iteration 16800: Loss = -11267.547654149463
5
Iteration 16900: Loss = -11267.529074052107
Iteration 17000: Loss = -11267.53368680952
1
Iteration 17100: Loss = -11267.531021480889
2
Iteration 17200: Loss = -11267.551948655577
3
Iteration 17300: Loss = -11267.52924323096
4
Iteration 17400: Loss = -11267.529841192341
5
Iteration 17500: Loss = -11267.593330848862
6
Iteration 17600: Loss = -11267.535940221967
7
Iteration 17700: Loss = -11267.534537556041
8
Iteration 17800: Loss = -11267.531866319247
9
Iteration 17900: Loss = -11267.858253650189
10
Iteration 18000: Loss = -11267.529152297531
Iteration 18100: Loss = -11267.728881680796
1
Iteration 18200: Loss = -11267.529053062744
Iteration 18300: Loss = -11267.552822695663
1
Iteration 18400: Loss = -11267.53533033115
2
Iteration 18500: Loss = -11267.52995589209
3
Iteration 18600: Loss = -11267.530129636205
4
Iteration 18700: Loss = -11267.529149658212
Iteration 18800: Loss = -11267.528472464353
Iteration 18900: Loss = -11267.529897891607
1
Iteration 19000: Loss = -11267.528529028648
Iteration 19100: Loss = -11267.534518787255
1
Iteration 19200: Loss = -11267.52532708265
Iteration 19300: Loss = -11267.525474878192
1
Iteration 19400: Loss = -11267.592267967602
2
Iteration 19500: Loss = -11267.525278539935
Iteration 19600: Loss = -11267.541936823074
1
Iteration 19700: Loss = -11267.525302649105
Iteration 19800: Loss = -11267.525742937263
1
Iteration 19900: Loss = -11267.527432305307
2
pi: tensor([[0.5571, 0.4429],
        [0.4048, 0.5952]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5667, 0.4333], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2911, 0.0963],
         [0.6465, 0.2218]],

        [[0.6516, 0.0926],
         [0.6325, 0.5212]],

        [[0.5174, 0.0992],
         [0.7177, 0.6511]],

        [[0.6588, 0.0938],
         [0.5206, 0.6703]],

        [[0.6796, 0.0906],
         [0.5626, 0.7031]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208065164923572
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 88
Adjusted Rand Index: 0.5727709131139027
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.3881580691297639
Average Adjusted Rand Index: 0.8668752978543883
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19962.260374662983
Iteration 100: Loss = -11559.534123315296
Iteration 200: Loss = -11558.005817825306
Iteration 300: Loss = -11552.753216905949
Iteration 400: Loss = -11441.434203283829
Iteration 500: Loss = -11232.112140777233
Iteration 600: Loss = -11217.13214361566
Iteration 700: Loss = -11189.019069785121
Iteration 800: Loss = -11181.567079156144
Iteration 900: Loss = -11181.367239595964
Iteration 1000: Loss = -11181.284803400511
Iteration 1100: Loss = -11181.227590292514
Iteration 1200: Loss = -11181.178693394537
Iteration 1300: Loss = -11181.0656744454
Iteration 1400: Loss = -11180.983930618739
Iteration 1500: Loss = -11180.965049205051
Iteration 1600: Loss = -11180.95156966264
Iteration 1700: Loss = -11180.940852670888
Iteration 1800: Loss = -11180.932519083119
Iteration 1900: Loss = -11180.924835508278
Iteration 2000: Loss = -11180.917102229312
Iteration 2100: Loss = -11180.908334511423
Iteration 2200: Loss = -11180.89830762225
Iteration 2300: Loss = -11180.892083308165
Iteration 2400: Loss = -11180.888485315761
Iteration 2500: Loss = -11180.884913433834
Iteration 2600: Loss = -11180.878103262517
Iteration 2700: Loss = -11180.853577059692
Iteration 2800: Loss = -11180.841557373004
Iteration 2900: Loss = -11180.839932903533
Iteration 3000: Loss = -11180.837346193104
Iteration 3100: Loss = -11180.836037561545
Iteration 3200: Loss = -11180.835942787231
Iteration 3300: Loss = -11180.833878698293
Iteration 3400: Loss = -11180.832966418815
Iteration 3500: Loss = -11180.83220473365
Iteration 3600: Loss = -11180.831453085611
Iteration 3700: Loss = -11180.831204173619
Iteration 3800: Loss = -11180.833370335053
1
Iteration 3900: Loss = -11180.834654251625
2
Iteration 4000: Loss = -11180.829073661096
Iteration 4100: Loss = -11180.828640863905
Iteration 4200: Loss = -11180.828502380253
Iteration 4300: Loss = -11180.827990122507
Iteration 4400: Loss = -11180.827367130514
Iteration 4500: Loss = -11180.827151437428
Iteration 4600: Loss = -11180.826652243602
Iteration 4700: Loss = -11180.827228509675
1
Iteration 4800: Loss = -11180.830339401335
2
Iteration 4900: Loss = -11180.837463723581
3
Iteration 5000: Loss = -11180.825953722771
Iteration 5100: Loss = -11180.825166772336
Iteration 5200: Loss = -11180.825196421516
Iteration 5300: Loss = -11180.824619177863
Iteration 5400: Loss = -11180.824401395623
Iteration 5500: Loss = -11180.824892219294
1
Iteration 5600: Loss = -11180.823816688779
Iteration 5700: Loss = -11180.823409195538
Iteration 5800: Loss = -11180.823219500086
Iteration 5900: Loss = -11180.815020782946
Iteration 6000: Loss = -11180.811752286341
Iteration 6100: Loss = -11180.811724998694
Iteration 6200: Loss = -11180.811293443505
Iteration 6300: Loss = -11180.811318802493
Iteration 6400: Loss = -11180.863346715398
1
Iteration 6500: Loss = -11180.810863377865
Iteration 6600: Loss = -11180.822460689837
1
Iteration 6700: Loss = -11180.810509765182
Iteration 6800: Loss = -11180.887403965282
1
Iteration 6900: Loss = -11180.810178247468
Iteration 7000: Loss = -11180.810113746167
Iteration 7100: Loss = -11180.810178502541
Iteration 7200: Loss = -11180.809910067366
Iteration 7300: Loss = -11180.911843189344
1
Iteration 7400: Loss = -11180.809813145439
Iteration 7500: Loss = -11180.809739526783
Iteration 7600: Loss = -11180.842088661951
1
Iteration 7700: Loss = -11180.809647328715
Iteration 7800: Loss = -11180.809582108157
Iteration 7900: Loss = -11180.976849821622
1
Iteration 8000: Loss = -11180.809477242452
Iteration 8100: Loss = -11180.809342361938
Iteration 8200: Loss = -11180.812382568414
1
Iteration 8300: Loss = -11180.808655514358
Iteration 8400: Loss = -11180.844660241577
1
Iteration 8500: Loss = -11180.80837447789
Iteration 8600: Loss = -11180.808790763538
1
Iteration 8700: Loss = -11180.814083725998
2
Iteration 8800: Loss = -11180.810968497
3
Iteration 8900: Loss = -11180.72864063107
Iteration 9000: Loss = -11180.73896888713
1
Iteration 9100: Loss = -11180.747749747045
2
Iteration 9200: Loss = -11180.722023861004
Iteration 9300: Loss = -11180.720282822798
Iteration 9400: Loss = -11180.723817539456
1
Iteration 9500: Loss = -11180.720082217918
Iteration 9600: Loss = -11180.748796036629
1
Iteration 9700: Loss = -11180.72946117112
2
Iteration 9800: Loss = -11180.720626625403
3
Iteration 9900: Loss = -11180.720009440714
Iteration 10000: Loss = -11180.7214270967
1
Iteration 10100: Loss = -11180.734489375569
2
Iteration 10200: Loss = -11180.719903977804
Iteration 10300: Loss = -11180.721189476826
1
Iteration 10400: Loss = -11180.719862221238
Iteration 10500: Loss = -11180.71990446928
Iteration 10600: Loss = -11180.719855129919
Iteration 10700: Loss = -11180.818709245836
1
Iteration 10800: Loss = -11180.719799859515
Iteration 10900: Loss = -11180.719761376868
Iteration 11000: Loss = -11181.213460730369
1
Iteration 11100: Loss = -11180.719632359416
Iteration 11200: Loss = -11180.719660535044
Iteration 11300: Loss = -11180.731604722369
1
Iteration 11400: Loss = -11180.719634098186
Iteration 11500: Loss = -11180.71961244684
Iteration 11600: Loss = -11180.720085909043
1
Iteration 11700: Loss = -11180.719618997986
Iteration 11800: Loss = -11180.727782480357
1
Iteration 11900: Loss = -11180.719602878595
Iteration 12000: Loss = -11180.738006699936
1
Iteration 12100: Loss = -11180.719564650035
Iteration 12200: Loss = -11180.72000011394
1
Iteration 12300: Loss = -11180.71960892139
Iteration 12400: Loss = -11180.727213762219
1
Iteration 12500: Loss = -11180.719598823234
Iteration 12600: Loss = -11181.097848614076
1
Iteration 12700: Loss = -11180.719588596881
Iteration 12800: Loss = -11180.719578582986
Iteration 12900: Loss = -11180.72090932757
1
Iteration 13000: Loss = -11180.728808100177
2
Iteration 13100: Loss = -11180.719591496862
Iteration 13200: Loss = -11180.737602446852
1
Iteration 13300: Loss = -11180.719562224464
Iteration 13400: Loss = -11180.73197946139
1
Iteration 13500: Loss = -11180.719609017722
Iteration 13600: Loss = -11180.71956489841
Iteration 13700: Loss = -11180.726249860812
1
Iteration 13800: Loss = -11180.71954503313
Iteration 13900: Loss = -11180.719547138533
Iteration 14000: Loss = -11180.719557614826
Iteration 14100: Loss = -11180.719515683166
Iteration 14200: Loss = -11181.048898889872
1
Iteration 14300: Loss = -11180.719562025557
Iteration 14400: Loss = -11180.724893270472
1
Iteration 14500: Loss = -11180.719561929474
Iteration 14600: Loss = -11180.720792714641
1
Iteration 14700: Loss = -11180.726811609324
2
Iteration 14800: Loss = -11180.722679844845
3
Iteration 14900: Loss = -11180.719570992022
Iteration 15000: Loss = -11180.719746510944
1
Iteration 15100: Loss = -11180.743745207366
2
Iteration 15200: Loss = -11180.719599362625
Iteration 15300: Loss = -11180.720314293643
1
Iteration 15400: Loss = -11180.719678482557
Iteration 15500: Loss = -11180.719762147599
Iteration 15600: Loss = -11180.774640249718
1
Iteration 15700: Loss = -11180.719533630065
Iteration 15800: Loss = -11180.721015736299
1
Iteration 15900: Loss = -11180.719530647217
Iteration 16000: Loss = -11180.72520825242
1
Iteration 16100: Loss = -11180.71953190684
Iteration 16200: Loss = -11180.719696913331
1
Iteration 16300: Loss = -11180.71959073884
Iteration 16400: Loss = -11180.7195347021
Iteration 16500: Loss = -11180.720449152412
1
Iteration 16600: Loss = -11180.719572965854
Iteration 16700: Loss = -11180.719528585625
Iteration 16800: Loss = -11180.724726614199
1
Iteration 16900: Loss = -11180.742501687597
2
Iteration 17000: Loss = -11180.7197943608
3
Iteration 17100: Loss = -11180.71959182947
Iteration 17200: Loss = -11180.71969938175
1
Iteration 17300: Loss = -11180.719532014842
Iteration 17400: Loss = -11180.719545109825
Iteration 17500: Loss = -11180.719786812457
1
Iteration 17600: Loss = -11180.754263326186
2
Iteration 17700: Loss = -11180.749379765222
3
Iteration 17800: Loss = -11180.827539003598
4
Iteration 17900: Loss = -11180.717537505392
Iteration 18000: Loss = -11180.72164596351
1
Iteration 18100: Loss = -11180.717533129791
Iteration 18200: Loss = -11180.717809866961
1
Iteration 18300: Loss = -11180.717523660014
Iteration 18400: Loss = -11180.718478809546
1
Iteration 18500: Loss = -11180.717513838248
Iteration 18600: Loss = -11180.77482492433
1
Iteration 18700: Loss = -11180.717514144992
Iteration 18800: Loss = -11180.717520742835
Iteration 18900: Loss = -11180.719282743452
1
Iteration 19000: Loss = -11180.717529917489
Iteration 19100: Loss = -11180.717526811912
Iteration 19200: Loss = -11180.717765497093
1
Iteration 19300: Loss = -11180.717522870402
Iteration 19400: Loss = -11180.718287509542
1
Iteration 19500: Loss = -11180.732621652014
2
Iteration 19600: Loss = -11180.72088084872
3
Iteration 19700: Loss = -11180.725668030263
4
Iteration 19800: Loss = -11180.72705064512
5
Iteration 19900: Loss = -11180.720454501969
6
pi: tensor([[0.7126, 0.2874],
        [0.2410, 0.7590]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4423, 0.5577], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1999, 0.0972],
         [0.6678, 0.2992]],

        [[0.6322, 0.0927],
         [0.5149, 0.7296]],

        [[0.5280, 0.0989],
         [0.5386, 0.7268]],

        [[0.5779, 0.0974],
         [0.7034, 0.6223]],

        [[0.7055, 0.0905],
         [0.5564, 0.6448]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9603194842709415
Average Adjusted Rand Index: 0.9599978056701757
11194.550431096859
[0.3881580691297639, 0.9603194842709415] [0.8668752978543883, 0.9599978056701757] [11267.528734754458, 11180.71756673747]
-------------------------------------
This iteration is 31
True Objective function: Loss = -11091.093687180504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23629.126427596184
Iteration 100: Loss = -11384.577853929259
Iteration 200: Loss = -11382.155118472878
Iteration 300: Loss = -11379.496644836297
Iteration 400: Loss = -11377.786058205638
Iteration 500: Loss = -11376.203166524267
Iteration 600: Loss = -11374.050504068011
Iteration 700: Loss = -11279.02268554528
Iteration 800: Loss = -11171.8969883677
Iteration 900: Loss = -11149.584862377329
Iteration 1000: Loss = -11142.472984016102
Iteration 1100: Loss = -11139.715268846698
Iteration 1200: Loss = -11138.998673243317
Iteration 1300: Loss = -11138.857480688068
Iteration 1400: Loss = -11138.737927330163
Iteration 1500: Loss = -11138.637013279707
Iteration 1600: Loss = -11138.560019942694
Iteration 1700: Loss = -11137.04279963095
Iteration 1800: Loss = -11136.132151548318
Iteration 1900: Loss = -11136.092828225885
Iteration 2000: Loss = -11136.069474057402
Iteration 2100: Loss = -11136.050746507992
Iteration 2200: Loss = -11136.034875379106
Iteration 2300: Loss = -11136.020228606556
Iteration 2400: Loss = -11136.003415128778
Iteration 2500: Loss = -11135.988287304644
Iteration 2600: Loss = -11135.978650041541
Iteration 2700: Loss = -11135.970137613753
Iteration 2800: Loss = -11135.961753971795
Iteration 2900: Loss = -11135.943443897726
Iteration 3000: Loss = -11135.903137033805
Iteration 3100: Loss = -11135.893527946444
Iteration 3200: Loss = -11135.879002767148
Iteration 3300: Loss = -11135.874178762684
Iteration 3400: Loss = -11135.875051017427
1
Iteration 3500: Loss = -11135.866725719352
Iteration 3600: Loss = -11135.86299877529
Iteration 3700: Loss = -11135.8592309256
Iteration 3800: Loss = -11134.89246865414
Iteration 3900: Loss = -11134.887205827616
Iteration 4000: Loss = -11134.882108698539
Iteration 4100: Loss = -11134.878434149214
Iteration 4200: Loss = -11134.876229343647
Iteration 4300: Loss = -11134.874155596399
Iteration 4400: Loss = -11134.871848253506
Iteration 4500: Loss = -11134.870453233174
Iteration 4600: Loss = -11134.86894284756
Iteration 4700: Loss = -11134.868940520602
Iteration 4800: Loss = -11134.866490100645
Iteration 4900: Loss = -11134.866161839574
Iteration 5000: Loss = -11134.864824660004
Iteration 5100: Loss = -11134.86369167672
Iteration 5200: Loss = -11134.862877612875
Iteration 5300: Loss = -11134.862098331427
Iteration 5400: Loss = -11134.861512366011
Iteration 5500: Loss = -11134.860716068824
Iteration 5600: Loss = -11134.860236531746
Iteration 5700: Loss = -11134.859549813771
Iteration 5800: Loss = -11134.859173357325
Iteration 5900: Loss = -11134.858443842706
Iteration 6000: Loss = -11134.857916733685
Iteration 6100: Loss = -11134.857361454875
Iteration 6200: Loss = -11134.856834690312
Iteration 6300: Loss = -11134.856966208325
1
Iteration 6400: Loss = -11134.855354545667
Iteration 6500: Loss = -11134.856188205042
1
Iteration 6600: Loss = -11134.852075701243
Iteration 6700: Loss = -11134.863738164284
1
Iteration 6800: Loss = -11134.449395422953
Iteration 6900: Loss = -11133.529729757453
Iteration 7000: Loss = -11131.936946087617
Iteration 7100: Loss = -11131.21011039959
Iteration 7200: Loss = -11131.154506906098
Iteration 7300: Loss = -11131.13543807152
Iteration 7400: Loss = -11131.13268752028
Iteration 7500: Loss = -11131.129909344621
Iteration 7600: Loss = -11131.12803598794
Iteration 7700: Loss = -11131.134932217052
1
Iteration 7800: Loss = -11131.12536954998
Iteration 7900: Loss = -11131.123251972918
Iteration 8000: Loss = -11131.108389527099
Iteration 8100: Loss = -11131.106358471141
Iteration 8200: Loss = -11131.10590495844
Iteration 8300: Loss = -11131.106272665284
1
Iteration 8400: Loss = -11131.12856735708
2
Iteration 8500: Loss = -11131.107457151456
3
Iteration 8600: Loss = -11131.154675591495
4
Iteration 8700: Loss = -11131.10493609577
Iteration 8800: Loss = -11131.104865401758
Iteration 8900: Loss = -11131.11649695544
1
Iteration 9000: Loss = -11131.10446111677
Iteration 9100: Loss = -11131.104433710125
Iteration 9200: Loss = -11131.110194867251
1
Iteration 9300: Loss = -11131.10340782693
Iteration 9400: Loss = -11131.103274218676
Iteration 9500: Loss = -11131.180066211411
1
Iteration 9600: Loss = -11131.103033602989
Iteration 9700: Loss = -11131.104454376957
1
Iteration 9800: Loss = -11131.102428644597
Iteration 9900: Loss = -11131.10210599776
Iteration 10000: Loss = -11131.103157414873
1
Iteration 10100: Loss = -11131.102051867236
Iteration 10200: Loss = -11131.215858976238
1
Iteration 10300: Loss = -11131.102729714548
2
Iteration 10400: Loss = -11131.101978607052
Iteration 10500: Loss = -11131.102149007473
1
Iteration 10600: Loss = -11131.102565284902
2
Iteration 10700: Loss = -11131.101927104022
Iteration 10800: Loss = -11131.102410949261
1
Iteration 10900: Loss = -11131.373676659177
2
Iteration 11000: Loss = -11131.101855395373
Iteration 11100: Loss = -11131.382620996917
1
Iteration 11200: Loss = -11131.101830123342
Iteration 11300: Loss = -11131.101872621115
Iteration 11400: Loss = -11131.10226286843
1
Iteration 11500: Loss = -11131.10184097631
Iteration 11600: Loss = -11131.101799086786
Iteration 11700: Loss = -11131.101861121479
Iteration 11800: Loss = -11131.101807692534
Iteration 11900: Loss = -11131.10672258697
1
Iteration 12000: Loss = -11131.101780200472
Iteration 12100: Loss = -11131.109977234373
1
Iteration 12200: Loss = -11131.103378539146
2
Iteration 12300: Loss = -11131.10180669799
Iteration 12400: Loss = -11131.10283071384
1
Iteration 12500: Loss = -11131.10254982023
2
Iteration 12600: Loss = -11131.101755861704
Iteration 12700: Loss = -11131.102347173497
1
Iteration 12800: Loss = -11131.142203563808
2
Iteration 12900: Loss = -11131.128171871282
3
Iteration 13000: Loss = -11131.101819597448
Iteration 13100: Loss = -11131.101744687265
Iteration 13200: Loss = -11131.139901819715
1
Iteration 13300: Loss = -11131.101717830097
Iteration 13400: Loss = -11131.10542339269
1
Iteration 13500: Loss = -11131.137984932926
2
Iteration 13600: Loss = -11131.111128801385
3
Iteration 13700: Loss = -11131.10203201819
4
Iteration 13800: Loss = -11131.101756965374
Iteration 13900: Loss = -11131.104032566334
1
Iteration 14000: Loss = -11131.10207062197
2
Iteration 14100: Loss = -11131.101720889183
Iteration 14200: Loss = -11131.298331590184
1
Iteration 14300: Loss = -11131.101674255206
Iteration 14400: Loss = -11131.102786560057
1
Iteration 14500: Loss = -11131.101647594036
Iteration 14600: Loss = -11131.101934838583
1
Iteration 14700: Loss = -11131.101649561031
Iteration 14800: Loss = -11131.111043435412
1
Iteration 14900: Loss = -11131.101672006007
Iteration 15000: Loss = -11131.101659497232
Iteration 15100: Loss = -11131.101709819899
Iteration 15200: Loss = -11131.101602983512
Iteration 15300: Loss = -11131.103506671147
1
Iteration 15400: Loss = -11131.101602257559
Iteration 15500: Loss = -11131.101556999502
Iteration 15600: Loss = -11131.101578998941
Iteration 15700: Loss = -11131.103038279407
1
Iteration 15800: Loss = -11131.101566504663
Iteration 15900: Loss = -11131.101590913706
Iteration 16000: Loss = -11131.149522792934
1
Iteration 16100: Loss = -11131.101567447042
Iteration 16200: Loss = -11131.101572358428
Iteration 16300: Loss = -11131.1044682298
1
Iteration 16400: Loss = -11131.119604492715
2
Iteration 16500: Loss = -11131.101664839473
Iteration 16600: Loss = -11131.101712435811
Iteration 16700: Loss = -11131.102453936504
1
Iteration 16800: Loss = -11131.101606814393
Iteration 16900: Loss = -11131.101838201534
1
Iteration 17000: Loss = -11131.10158857711
Iteration 17100: Loss = -11131.101689145653
1
Iteration 17200: Loss = -11131.101572969608
Iteration 17300: Loss = -11131.102078117827
1
Iteration 17400: Loss = -11131.101562926531
Iteration 17500: Loss = -11131.10765427794
1
Iteration 17600: Loss = -11131.101569311382
Iteration 17700: Loss = -11131.10280997181
1
Iteration 17800: Loss = -11131.10224145958
2
Iteration 17900: Loss = -11131.14927783455
3
Iteration 18000: Loss = -11131.101567498865
Iteration 18100: Loss = -11131.102070985211
1
Iteration 18200: Loss = -11131.141284383553
2
Iteration 18300: Loss = -11131.101538591678
Iteration 18400: Loss = -11131.104031920575
1
Iteration 18500: Loss = -11131.101568823704
Iteration 18600: Loss = -11131.102347841495
1
Iteration 18700: Loss = -11131.101566700116
Iteration 18800: Loss = -11131.108509083218
1
Iteration 18900: Loss = -11131.101592924639
Iteration 19000: Loss = -11131.106880067558
1
Iteration 19100: Loss = -11131.132991830726
2
Iteration 19200: Loss = -11131.104424697458
3
Iteration 19300: Loss = -11131.103002070482
4
Iteration 19400: Loss = -11131.10219546599
5
Iteration 19500: Loss = -11131.109363408132
6
Iteration 19600: Loss = -11131.110786958146
7
Iteration 19700: Loss = -11131.106721827087
8
Iteration 19800: Loss = -11131.10243401532
9
Iteration 19900: Loss = -11131.101525479822
pi: tensor([[0.7038, 0.2962],
        [0.2303, 0.7697]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9004, 0.0996], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1926, 0.0969],
         [0.5085, 0.3096]],

        [[0.6695, 0.0937],
         [0.7162, 0.6729]],

        [[0.6219, 0.0992],
         [0.5910, 0.5046]],

        [[0.5401, 0.1067],
         [0.6428, 0.6259]],

        [[0.6996, 0.0940],
         [0.6598, 0.5794]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.036554016474926315
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9206925302859384
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
Global Adjusted Rand Index: 0.5349084677026762
Average Adjusted Rand Index: 0.7524163333707294
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20769.46152805214
Iteration 100: Loss = -11382.933657553504
Iteration 200: Loss = -11380.895278850789
Iteration 300: Loss = -11378.722516022874
Iteration 400: Loss = -11376.877551163298
Iteration 500: Loss = -11374.402723350197
Iteration 600: Loss = -11247.184397989873
Iteration 700: Loss = -11148.641396201312
Iteration 800: Loss = -11136.58250017477
Iteration 900: Loss = -11135.39231001048
Iteration 1000: Loss = -11135.227160852186
Iteration 1100: Loss = -11135.132377941745
Iteration 1200: Loss = -11135.070507428205
Iteration 1300: Loss = -11135.027887068789
Iteration 1400: Loss = -11134.997973431993
Iteration 1500: Loss = -11134.967446835879
Iteration 1600: Loss = -11134.947042694166
Iteration 1700: Loss = -11134.931052090724
Iteration 1800: Loss = -11134.915332800503
Iteration 1900: Loss = -11134.893633196121
Iteration 2000: Loss = -11134.880366860025
Iteration 2100: Loss = -11134.869196222842
Iteration 2200: Loss = -11134.856336677794
Iteration 2300: Loss = -11134.839256990605
Iteration 2400: Loss = -11134.808555357247
Iteration 2500: Loss = -11134.706324007167
Iteration 2600: Loss = -11134.351528761737
Iteration 2700: Loss = -11134.121663176255
Iteration 2800: Loss = -11132.85863371549
Iteration 2900: Loss = -11131.416769125855
Iteration 3000: Loss = -11131.162988460213
Iteration 3100: Loss = -11131.136143411422
Iteration 3200: Loss = -11131.122870946674
Iteration 3300: Loss = -11131.119129565746
Iteration 3400: Loss = -11131.117248510562
Iteration 3500: Loss = -11131.11769671437
1
Iteration 3600: Loss = -11131.113464367316
Iteration 3700: Loss = -11131.112354972862
Iteration 3800: Loss = -11131.111500033257
Iteration 3900: Loss = -11131.11073389059
Iteration 4000: Loss = -11131.11004407378
Iteration 4100: Loss = -11131.109470317402
Iteration 4200: Loss = -11131.109332025939
Iteration 4300: Loss = -11131.108555940606
Iteration 4400: Loss = -11131.108442817273
Iteration 4500: Loss = -11131.107987278769
Iteration 4600: Loss = -11131.107512077695
Iteration 4700: Loss = -11131.107092481689
Iteration 4800: Loss = -11131.106893652846
Iteration 4900: Loss = -11131.107599874067
1
Iteration 5000: Loss = -11131.106324040338
Iteration 5100: Loss = -11131.10613727511
Iteration 5200: Loss = -11131.106060428614
Iteration 5300: Loss = -11131.11159326497
1
Iteration 5400: Loss = -11131.105561346109
Iteration 5500: Loss = -11131.105472935324
Iteration 5600: Loss = -11131.105317118307
Iteration 5700: Loss = -11131.105086826741
Iteration 5800: Loss = -11131.105030830122
Iteration 5900: Loss = -11131.105730126425
1
Iteration 6000: Loss = -11131.105256758194
2
Iteration 6100: Loss = -11131.105034054674
Iteration 6200: Loss = -11131.105042778841
Iteration 6300: Loss = -11131.10454036127
Iteration 6400: Loss = -11131.104285310146
Iteration 6500: Loss = -11131.10418834003
Iteration 6600: Loss = -11131.113253328856
1
Iteration 6700: Loss = -11131.105949376752
2
Iteration 6800: Loss = -11131.103833571775
Iteration 6900: Loss = -11131.103809080356
Iteration 7000: Loss = -11131.103630189797
Iteration 7100: Loss = -11131.103604405584
Iteration 7200: Loss = -11131.103521647648
Iteration 7300: Loss = -11131.103531330487
Iteration 7400: Loss = -11131.104838034105
1
Iteration 7500: Loss = -11131.104213105342
2
Iteration 7600: Loss = -11131.103458681444
Iteration 7700: Loss = -11131.114198665344
1
Iteration 7800: Loss = -11131.114945436442
2
Iteration 7900: Loss = -11131.103166987607
Iteration 8000: Loss = -11131.103484112986
1
Iteration 8100: Loss = -11131.103096521549
Iteration 8200: Loss = -11131.103175428776
Iteration 8300: Loss = -11131.103222711528
Iteration 8400: Loss = -11131.102715637144
Iteration 8500: Loss = -11131.10228456055
Iteration 8600: Loss = -11131.10220858839
Iteration 8700: Loss = -11131.121363580109
1
Iteration 8800: Loss = -11131.110566051559
2
Iteration 8900: Loss = -11131.111484551142
3
Iteration 9000: Loss = -11131.102090916032
Iteration 9100: Loss = -11131.10202876283
Iteration 9200: Loss = -11131.102359581204
1
Iteration 9300: Loss = -11131.10203054015
Iteration 9400: Loss = -11131.102025184504
Iteration 9500: Loss = -11131.102162052068
1
Iteration 9600: Loss = -11131.101930335299
Iteration 9700: Loss = -11131.101864622664
Iteration 9800: Loss = -11131.118139360227
1
Iteration 9900: Loss = -11131.101832342318
Iteration 10000: Loss = -11131.101793098578
Iteration 10100: Loss = -11131.103587677313
1
Iteration 10200: Loss = -11131.10177771708
Iteration 10300: Loss = -11131.239794136889
1
Iteration 10400: Loss = -11131.101776681013
Iteration 10500: Loss = -11131.101751914492
Iteration 10600: Loss = -11131.102145565126
1
Iteration 10700: Loss = -11131.101734399155
Iteration 10800: Loss = -11131.292251354831
1
Iteration 10900: Loss = -11131.10173605749
Iteration 11000: Loss = -11131.101711467429
Iteration 11100: Loss = -11131.10208633968
1
Iteration 11200: Loss = -11131.113569673098
2
Iteration 11300: Loss = -11131.101704031815
Iteration 11400: Loss = -11131.103886694382
1
Iteration 11500: Loss = -11131.101695346875
Iteration 11600: Loss = -11131.49313053478
1
Iteration 11700: Loss = -11131.101683993998
Iteration 11800: Loss = -11131.10166532005
Iteration 11900: Loss = -11131.18102928422
1
Iteration 12000: Loss = -11131.101683502595
Iteration 12100: Loss = -11131.101666218896
Iteration 12200: Loss = -11131.132072590215
1
Iteration 12300: Loss = -11131.101670559157
Iteration 12400: Loss = -11131.11978824935
1
Iteration 12500: Loss = -11131.106531711397
2
Iteration 12600: Loss = -11131.104030168477
3
Iteration 12700: Loss = -11131.155887892772
4
Iteration 12800: Loss = -11131.10161231119
Iteration 12900: Loss = -11131.1021356772
1
Iteration 13000: Loss = -11131.101620776299
Iteration 13100: Loss = -11131.102827498778
1
Iteration 13200: Loss = -11131.101548923889
Iteration 13300: Loss = -11131.126161441265
1
Iteration 13400: Loss = -11131.101550641975
Iteration 13500: Loss = -11131.101554246454
Iteration 13600: Loss = -11131.101583499687
Iteration 13700: Loss = -11131.101516710962
Iteration 13800: Loss = -11131.102933772658
1
Iteration 13900: Loss = -11131.102327573186
2
Iteration 14000: Loss = -11131.1115773186
3
Iteration 14100: Loss = -11131.10148552781
Iteration 14200: Loss = -11131.138987062493
1
Iteration 14300: Loss = -11131.101486675638
Iteration 14400: Loss = -11131.107121302402
1
Iteration 14500: Loss = -11131.101492554173
Iteration 14600: Loss = -11131.232056658315
1
Iteration 14700: Loss = -11131.101485505797
Iteration 14800: Loss = -11131.101497669062
Iteration 14900: Loss = -11131.10155273186
Iteration 15000: Loss = -11131.11149396568
1
Iteration 15100: Loss = -11131.102238670324
2
Iteration 15200: Loss = -11131.114592653421
3
Iteration 15300: Loss = -11131.101458891291
Iteration 15400: Loss = -11131.101663221123
1
Iteration 15500: Loss = -11131.101803842357
2
Iteration 15600: Loss = -11131.101501175082
Iteration 15700: Loss = -11131.101477014523
Iteration 15800: Loss = -11131.101523908826
Iteration 15900: Loss = -11131.101439754824
Iteration 16000: Loss = -11131.104538333942
1
Iteration 16100: Loss = -11131.101463971523
Iteration 16200: Loss = -11131.369595704518
1
Iteration 16300: Loss = -11131.101465096677
Iteration 16400: Loss = -11131.10144473725
Iteration 16500: Loss = -11131.114463151971
1
Iteration 16600: Loss = -11131.101464041147
Iteration 16700: Loss = -11131.10434785722
1
Iteration 16800: Loss = -11131.158618155
2
Iteration 16900: Loss = -11131.103059300223
3
Iteration 17000: Loss = -11131.101489177334
Iteration 17100: Loss = -11131.106330344903
1
Iteration 17200: Loss = -11131.101483537586
Iteration 17300: Loss = -11131.10197961165
1
Iteration 17400: Loss = -11131.101437952031
Iteration 17500: Loss = -11131.10466095254
1
Iteration 17600: Loss = -11131.101456906901
Iteration 17700: Loss = -11131.103820530812
1
Iteration 17800: Loss = -11131.10150159985
Iteration 17900: Loss = -11131.10151573623
Iteration 18000: Loss = -11131.154133775426
1
Iteration 18100: Loss = -11131.188729662385
2
Iteration 18200: Loss = -11131.101499054534
Iteration 18300: Loss = -11131.101544469175
Iteration 18400: Loss = -11131.101496484393
Iteration 18500: Loss = -11131.10158186889
Iteration 18600: Loss = -11131.101460311806
Iteration 18700: Loss = -11131.103302415266
1
Iteration 18800: Loss = -11131.101457586856
Iteration 18900: Loss = -11131.573837909149
1
Iteration 19000: Loss = -11131.101484238423
Iteration 19100: Loss = -11131.101457855237
Iteration 19200: Loss = -11131.103751199145
1
Iteration 19300: Loss = -11131.102506490772
2
Iteration 19400: Loss = -11131.107924182348
3
Iteration 19500: Loss = -11131.101466740824
Iteration 19600: Loss = -11131.102409175977
1
Iteration 19700: Loss = -11131.101467431727
Iteration 19800: Loss = -11131.122733388234
1
Iteration 19900: Loss = -11131.10147655548
pi: tensor([[0.7696, 0.2304],
        [0.2966, 0.7034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0998, 0.9002], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3093, 0.0970],
         [0.6558, 0.1927]],

        [[0.5503, 0.0936],
         [0.5843, 0.5248]],

        [[0.6224, 0.0991],
         [0.7241, 0.7052]],

        [[0.5542, 0.1066],
         [0.6060, 0.5065]],

        [[0.5639, 0.0939],
         [0.5745, 0.5063]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.036554016474926315
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9206925302859384
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
Global Adjusted Rand Index: 0.5349084677026762
Average Adjusted Rand Index: 0.7524163333707294
11091.093687180504
[0.5349084677026762, 0.5349084677026762] [0.7524163333707294, 0.7524163333707294] [11131.102462852672, 11131.10160236439]
-------------------------------------
This iteration is 32
True Objective function: Loss = -10984.846473201182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20253.05483672897
Iteration 100: Loss = -11316.124806357502
Iteration 200: Loss = -11284.780379571448
Iteration 300: Loss = -11268.449917231912
Iteration 400: Loss = -11267.495773395605
Iteration 500: Loss = -11266.66902651267
Iteration 600: Loss = -11213.081549724577
Iteration 700: Loss = -11191.119915165695
Iteration 800: Loss = -11186.52669193134
Iteration 900: Loss = -11185.290919686036
Iteration 1000: Loss = -11185.085259247267
Iteration 1100: Loss = -11183.098550266335
Iteration 1200: Loss = -11181.341494657618
Iteration 1300: Loss = -11181.087577472028
Iteration 1400: Loss = -11181.06252345084
Iteration 1500: Loss = -11181.048269023451
Iteration 1600: Loss = -11181.038775705574
Iteration 1700: Loss = -11181.029776022755
Iteration 1800: Loss = -11180.993137394027
Iteration 1900: Loss = -11180.88112788896
Iteration 2000: Loss = -11180.873822087164
Iteration 2100: Loss = -11180.69338916235
Iteration 2200: Loss = -11180.668752124497
Iteration 2300: Loss = -11180.662193351684
Iteration 2400: Loss = -11179.130656839436
Iteration 2500: Loss = -11179.056429806962
Iteration 2600: Loss = -11179.045322721176
Iteration 2700: Loss = -11178.909854931957
Iteration 2800: Loss = -11177.321598440572
Iteration 2900: Loss = -11177.306199235876
Iteration 3000: Loss = -11177.186115423814
Iteration 3100: Loss = -11177.177385312723
Iteration 3200: Loss = -11177.172734447322
Iteration 3300: Loss = -11177.155875469185
Iteration 3400: Loss = -11172.001301050208
Iteration 3500: Loss = -11166.198723097212
Iteration 3600: Loss = -11149.456991848296
Iteration 3700: Loss = -11135.09045171912
Iteration 3800: Loss = -11127.727619596755
Iteration 3900: Loss = -11112.114257614106
Iteration 4000: Loss = -11110.358922819818
Iteration 4100: Loss = -11087.067882389492
Iteration 4200: Loss = -11085.685837757796
Iteration 4300: Loss = -11073.518549532622
Iteration 4400: Loss = -11064.856184744302
Iteration 4500: Loss = -11051.02686007355
Iteration 4600: Loss = -11039.753572293153
Iteration 4700: Loss = -11029.78881999434
Iteration 4800: Loss = -11022.781915773734
Iteration 4900: Loss = -11017.483039472283
Iteration 5000: Loss = -11017.478045722777
Iteration 5100: Loss = -11005.321349069734
Iteration 5200: Loss = -10998.314050021896
Iteration 5300: Loss = -10998.312602121347
Iteration 5400: Loss = -10998.305683861632
Iteration 5500: Loss = -10992.972595632222
Iteration 5600: Loss = -10992.987683693053
1
Iteration 5700: Loss = -10988.637360432753
Iteration 5800: Loss = -10984.103851482352
Iteration 5900: Loss = -10984.099438791985
Iteration 6000: Loss = -10984.098652301383
Iteration 6100: Loss = -10984.100090637347
1
Iteration 6200: Loss = -10984.093031579205
Iteration 6300: Loss = -10984.093636398018
1
Iteration 6400: Loss = -10977.256036685947
Iteration 6500: Loss = -10977.059365345776
Iteration 6600: Loss = -10977.058188773004
Iteration 6700: Loss = -10977.05814098924
Iteration 6800: Loss = -10977.057496175863
Iteration 6900: Loss = -10977.057512942583
Iteration 7000: Loss = -10977.05610341401
Iteration 7100: Loss = -10977.063572265119
1
Iteration 7200: Loss = -10977.054734693931
Iteration 7300: Loss = -10977.0545782035
Iteration 7400: Loss = -10977.054603942031
Iteration 7500: Loss = -10977.054307003633
Iteration 7600: Loss = -10977.059088855802
1
Iteration 7700: Loss = -10977.054311664302
Iteration 7800: Loss = -10977.053833434513
Iteration 7900: Loss = -10977.053032479653
Iteration 8000: Loss = -10977.047861281797
Iteration 8100: Loss = -10977.047818498868
Iteration 8200: Loss = -10977.04803725422
1
Iteration 8300: Loss = -10977.050939831857
2
Iteration 8400: Loss = -10977.07158870716
3
Iteration 8500: Loss = -10977.047895645866
Iteration 8600: Loss = -10977.049353852022
1
Iteration 8700: Loss = -10977.155714750592
2
Iteration 8800: Loss = -10977.048319962434
3
Iteration 8900: Loss = -10977.089678594348
4
Iteration 9000: Loss = -10977.050724382694
5
Iteration 9100: Loss = -10977.157127198228
6
Iteration 9200: Loss = -10977.007295394302
Iteration 9300: Loss = -10977.05559486114
1
Iteration 9400: Loss = -10977.011562211066
2
Iteration 9500: Loss = -10977.007105116027
Iteration 9600: Loss = -10977.0071300735
Iteration 9700: Loss = -10977.006684218366
Iteration 9800: Loss = -10977.007012630373
1
Iteration 9900: Loss = -10977.006534673897
Iteration 10000: Loss = -10977.006737960352
1
Iteration 10100: Loss = -10977.052299440455
2
Iteration 10200: Loss = -10977.006036621442
Iteration 10300: Loss = -10977.009042867225
1
Iteration 10400: Loss = -10977.004702898343
Iteration 10500: Loss = -10977.010057866919
1
Iteration 10600: Loss = -10977.013548704645
2
Iteration 10700: Loss = -10976.289791828813
Iteration 10800: Loss = -10970.69459786534
Iteration 10900: Loss = -10970.724639809174
1
Iteration 11000: Loss = -10970.6941421949
Iteration 11100: Loss = -10970.6959635526
1
Iteration 11200: Loss = -10970.691999796043
Iteration 11300: Loss = -10970.691598917054
Iteration 11400: Loss = -10970.691451790297
Iteration 11500: Loss = -10970.718014610173
1
Iteration 11600: Loss = -10970.689800969398
Iteration 11700: Loss = -10970.68934481768
Iteration 11800: Loss = -10970.688046432448
Iteration 11900: Loss = -10970.73650462771
1
Iteration 12000: Loss = -10970.685586350377
Iteration 12100: Loss = -10970.695877714767
1
Iteration 12200: Loss = -10970.68789625667
2
Iteration 12300: Loss = -10970.685821580024
3
Iteration 12400: Loss = -10970.685919906584
4
Iteration 12500: Loss = -10970.689131662966
5
Iteration 12600: Loss = -10965.720106747192
Iteration 12700: Loss = -10965.716422717702
Iteration 12800: Loss = -10965.71828793602
1
Iteration 12900: Loss = -10965.722006174747
2
Iteration 13000: Loss = -10965.715993077705
Iteration 13100: Loss = -10965.716114509045
1
Iteration 13200: Loss = -10965.716530635513
2
Iteration 13300: Loss = -10965.721801531467
3
Iteration 13400: Loss = -10965.7186269821
4
Iteration 13500: Loss = -10965.71804835758
5
Iteration 13600: Loss = -10965.734736627805
6
Iteration 13700: Loss = -10965.71626970026
7
Iteration 13800: Loss = -10965.714610218496
Iteration 13900: Loss = -10965.729802185504
1
Iteration 14000: Loss = -10965.714741147736
2
Iteration 14100: Loss = -10965.722535808452
3
Iteration 14200: Loss = -10965.693622528699
Iteration 14300: Loss = -10965.682718847213
Iteration 14400: Loss = -10965.680526109862
Iteration 14500: Loss = -10965.673119336127
Iteration 14600: Loss = -10965.682208477941
1
Iteration 14700: Loss = -10965.67285040725
Iteration 14800: Loss = -10965.673173631032
1
Iteration 14900: Loss = -10965.708260554844
2
Iteration 15000: Loss = -10965.64267285403
Iteration 15100: Loss = -10965.626767062715
Iteration 15200: Loss = -10965.619103667592
Iteration 15300: Loss = -10965.619300497545
1
Iteration 15400: Loss = -10965.621156252078
2
Iteration 15500: Loss = -10965.745142071895
3
Iteration 15600: Loss = -10965.619186248128
Iteration 15700: Loss = -10965.610377374174
Iteration 15800: Loss = -10965.610371646573
Iteration 15900: Loss = -10965.707976805023
1
Iteration 16000: Loss = -10965.606266210392
Iteration 16100: Loss = -10965.619636398525
1
Iteration 16200: Loss = -10965.596653363113
Iteration 16300: Loss = -10965.616576631508
1
Iteration 16400: Loss = -10965.596701990755
Iteration 16500: Loss = -10965.596745016806
Iteration 16600: Loss = -10965.729513440114
1
Iteration 16700: Loss = -10965.596641677
Iteration 16800: Loss = -10965.612975233382
1
Iteration 16900: Loss = -10965.596619934544
Iteration 17000: Loss = -10965.608121247813
1
Iteration 17100: Loss = -10965.608457021806
2
Iteration 17200: Loss = -10965.597414478887
3
Iteration 17300: Loss = -10965.59658485571
Iteration 17400: Loss = -10965.59907274385
1
Iteration 17500: Loss = -10965.599480435427
2
Iteration 17600: Loss = -10965.596276906754
Iteration 17700: Loss = -10965.606419156924
1
Iteration 17800: Loss = -10965.594463177675
Iteration 17900: Loss = -10965.596009326728
1
Iteration 18000: Loss = -10965.594440207495
Iteration 18100: Loss = -10965.608070118249
1
Iteration 18200: Loss = -10965.60021828955
2
Iteration 18300: Loss = -10965.733524445694
3
Iteration 18400: Loss = -10965.59508647965
4
Iteration 18500: Loss = -10965.601173860456
5
Iteration 18600: Loss = -10965.588718839517
Iteration 18700: Loss = -10965.588645191952
Iteration 18800: Loss = -10965.588775439535
1
Iteration 18900: Loss = -10965.588630385642
Iteration 19000: Loss = -10965.589085355532
1
Iteration 19100: Loss = -10965.588591535185
Iteration 19200: Loss = -10965.588743269249
1
Iteration 19300: Loss = -10965.695047676274
2
Iteration 19400: Loss = -10965.58857847617
Iteration 19500: Loss = -10965.614120927674
1
Iteration 19600: Loss = -10965.612805207753
2
Iteration 19700: Loss = -10965.590398256434
3
Iteration 19800: Loss = -10965.6306637996
4
Iteration 19900: Loss = -10965.64921514511
5
pi: tensor([[0.7658, 0.2342],
        [0.2250, 0.7750]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5905, 0.4095], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1956, 0.0901],
         [0.6161, 0.2976]],

        [[0.5710, 0.1048],
         [0.6461, 0.5074]],

        [[0.6322, 0.0945],
         [0.7025, 0.7234]],

        [[0.6944, 0.0938],
         [0.5835, 0.5762]],

        [[0.7133, 0.1009],
         [0.5992, 0.7066]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.9291542740196213
Average Adjusted Rand Index: 0.9291215876690456
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21844.66431683823
Iteration 100: Loss = -11268.940521094451
Iteration 200: Loss = -11267.74395046566
Iteration 300: Loss = -11267.155597980041
Iteration 400: Loss = -11266.5295450869
Iteration 500: Loss = -11264.86371242933
Iteration 600: Loss = -11261.086106182727
Iteration 700: Loss = -11207.964177521804
Iteration 800: Loss = -11084.24260757313
Iteration 900: Loss = -11069.934082386153
Iteration 1000: Loss = -11057.780677484612
Iteration 1100: Loss = -11057.195011366162
Iteration 1200: Loss = -11055.492748300261
Iteration 1300: Loss = -11055.333999607488
Iteration 1400: Loss = -11055.226995164016
Iteration 1500: Loss = -11055.12892874512
Iteration 1600: Loss = -11052.445792807734
Iteration 1700: Loss = -11052.398750655122
Iteration 1800: Loss = -11052.302336722676
Iteration 1900: Loss = -11052.25208922309
Iteration 2000: Loss = -11052.233866391613
Iteration 2100: Loss = -11052.218997531172
Iteration 2200: Loss = -11052.206144531869
Iteration 2300: Loss = -11052.19439526305
Iteration 2400: Loss = -11052.183138269767
Iteration 2500: Loss = -11052.170450260957
Iteration 2600: Loss = -11052.155866885842
Iteration 2700: Loss = -11052.146362371164
Iteration 2800: Loss = -11052.108705317947
Iteration 2900: Loss = -11052.081867338033
Iteration 3000: Loss = -11052.065682895798
Iteration 3100: Loss = -11052.045564532096
Iteration 3200: Loss = -11052.015369076918
Iteration 3300: Loss = -11051.970733363212
Iteration 3400: Loss = -11051.860624481536
Iteration 3500: Loss = -11024.737890814784
Iteration 3600: Loss = -10969.243305497406
Iteration 3700: Loss = -10969.06886242727
Iteration 3800: Loss = -10960.956609214536
Iteration 3900: Loss = -10960.936534085504
Iteration 4000: Loss = -10960.926427810435
Iteration 4100: Loss = -10960.918238073693
Iteration 4200: Loss = -10960.913050703975
Iteration 4300: Loss = -10960.909369559977
Iteration 4400: Loss = -10960.906392562067
Iteration 4500: Loss = -10960.904245796304
Iteration 4600: Loss = -10960.906549069214
1
Iteration 4700: Loss = -10960.900700915232
Iteration 4800: Loss = -10960.900316809391
Iteration 4900: Loss = -10960.904440108834
1
Iteration 5000: Loss = -10960.89716045545
Iteration 5100: Loss = -10960.897019282895
Iteration 5200: Loss = -10960.89951342345
1
Iteration 5300: Loss = -10960.897801885496
2
Iteration 5400: Loss = -10960.894188901555
Iteration 5500: Loss = -10960.895318532877
1
Iteration 5600: Loss = -10960.892263388996
Iteration 5700: Loss = -10960.891531458376
Iteration 5800: Loss = -10960.893958520277
1
Iteration 5900: Loss = -10960.89046528994
Iteration 6000: Loss = -10960.890186552522
Iteration 6100: Loss = -10960.90245335067
1
Iteration 6200: Loss = -10960.88906930383
Iteration 6300: Loss = -10960.888364452365
Iteration 6400: Loss = -10960.88752262813
Iteration 6500: Loss = -10960.89814974412
1
Iteration 6600: Loss = -10960.886388457884
Iteration 6700: Loss = -10960.888267028957
1
Iteration 6800: Loss = -10960.886410445313
Iteration 6900: Loss = -10960.884967297554
Iteration 7000: Loss = -10960.88485426524
Iteration 7100: Loss = -10960.884653707457
Iteration 7200: Loss = -10960.884687053895
Iteration 7300: Loss = -10960.884322738739
Iteration 7400: Loss = -10960.889343614612
1
Iteration 7500: Loss = -10960.883998749057
Iteration 7600: Loss = -10960.883830076555
Iteration 7700: Loss = -10960.883700690529
Iteration 7800: Loss = -10960.885605556601
1
Iteration 7900: Loss = -10960.883236781507
Iteration 8000: Loss = -10960.882804996058
Iteration 8100: Loss = -10960.880305609237
Iteration 8200: Loss = -10960.884141295188
1
Iteration 8300: Loss = -10960.879893745143
Iteration 8400: Loss = -10960.881587594315
1
Iteration 8500: Loss = -10960.879774961351
Iteration 8600: Loss = -10960.880101537692
1
Iteration 8700: Loss = -10960.89924822346
2
Iteration 8800: Loss = -10960.879941736684
3
Iteration 8900: Loss = -10960.87953253448
Iteration 9000: Loss = -10960.880168889464
1
Iteration 9100: Loss = -10960.881699978523
2
Iteration 9200: Loss = -10960.879814859743
3
Iteration 9300: Loss = -10960.880558864073
4
Iteration 9400: Loss = -10960.879419455589
Iteration 9500: Loss = -10960.879137784383
Iteration 9600: Loss = -10960.883001614462
1
Iteration 9700: Loss = -10960.881107411198
2
Iteration 9800: Loss = -10960.972544157179
3
Iteration 9900: Loss = -10960.878840539974
Iteration 10000: Loss = -10960.878534756866
Iteration 10100: Loss = -10960.879155377135
1
Iteration 10200: Loss = -10960.878126180884
Iteration 10300: Loss = -10960.93412160669
1
Iteration 10400: Loss = -10960.87792131222
Iteration 10500: Loss = -10960.87957975942
1
Iteration 10600: Loss = -10960.877322214239
Iteration 10700: Loss = -10960.877285976523
Iteration 10800: Loss = -10960.903697233744
1
Iteration 10900: Loss = -10960.877235920609
Iteration 11000: Loss = -10960.879545788666
1
Iteration 11100: Loss = -10960.877189982722
Iteration 11200: Loss = -10960.877479490377
1
Iteration 11300: Loss = -10960.877104299632
Iteration 11400: Loss = -10960.877184612156
Iteration 11500: Loss = -10960.876986867754
Iteration 11600: Loss = -10960.876928026779
Iteration 11700: Loss = -10960.876874663769
Iteration 11800: Loss = -10960.877035371246
1
Iteration 11900: Loss = -10960.876867189087
Iteration 12000: Loss = -10960.87725401432
1
Iteration 12100: Loss = -10960.876783614101
Iteration 12200: Loss = -10960.876998755226
1
Iteration 12300: Loss = -10960.87680917045
Iteration 12400: Loss = -10960.876972244045
1
Iteration 12500: Loss = -10960.877278055115
2
Iteration 12600: Loss = -10960.876986729096
3
Iteration 12700: Loss = -10961.185392413345
4
Iteration 12800: Loss = -10960.876785811131
Iteration 12900: Loss = -10960.896997770631
1
Iteration 13000: Loss = -10960.917277942563
2
Iteration 13100: Loss = -10960.883666251244
3
Iteration 13200: Loss = -10960.877739138861
4
Iteration 13300: Loss = -10960.878630249577
5
Iteration 13400: Loss = -10960.922326494745
6
Iteration 13500: Loss = -10960.937759851064
7
Iteration 13600: Loss = -10960.878479172054
8
Iteration 13700: Loss = -10960.877293460246
9
Iteration 13800: Loss = -10960.968578430075
10
Iteration 13900: Loss = -10960.877241816537
11
Iteration 14000: Loss = -10960.877066708581
12
Iteration 14100: Loss = -10960.886631467223
13
Iteration 14200: Loss = -10960.876763716939
Iteration 14300: Loss = -10960.876867348079
1
Iteration 14400: Loss = -10960.87706786904
2
Iteration 14500: Loss = -10960.887144138525
3
Iteration 14600: Loss = -10960.87922148024
4
Iteration 14700: Loss = -10960.881029264292
5
Iteration 14800: Loss = -10961.006277115439
6
Iteration 14900: Loss = -10960.892791665554
7
Iteration 15000: Loss = -10960.876777898839
Iteration 15100: Loss = -10960.876704604045
Iteration 15200: Loss = -10960.923866988369
1
Iteration 15300: Loss = -10960.876741147747
Iteration 15400: Loss = -10960.880250492952
1
Iteration 15500: Loss = -10960.877231212487
2
Iteration 15600: Loss = -10960.876748719127
Iteration 15700: Loss = -10960.878699195962
1
Iteration 15800: Loss = -10960.971532599278
2
Iteration 15900: Loss = -10960.876855511626
3
Iteration 16000: Loss = -10960.876963504306
4
Iteration 16100: Loss = -10960.886441195813
5
Iteration 16200: Loss = -10960.892058376226
6
Iteration 16300: Loss = -10960.87729374984
7
Iteration 16400: Loss = -10960.876771396188
Iteration 16500: Loss = -10960.955578552961
1
Iteration 16600: Loss = -10960.877001090497
2
Iteration 16700: Loss = -10960.904925832094
3
Iteration 16800: Loss = -10960.877817292983
4
Iteration 16900: Loss = -10960.877399022253
5
Iteration 17000: Loss = -10960.906128129041
6
Iteration 17100: Loss = -10960.910155856474
7
Iteration 17200: Loss = -10960.881785871221
8
Iteration 17300: Loss = -10960.87681056357
Iteration 17400: Loss = -10960.87662084796
Iteration 17500: Loss = -10960.905457516617
1
Iteration 17600: Loss = -10960.876628328115
Iteration 17700: Loss = -10960.884050966739
1
Iteration 17800: Loss = -10960.879070582847
2
Iteration 17900: Loss = -10960.938008769239
3
Iteration 18000: Loss = -10960.876586913631
Iteration 18100: Loss = -10960.876667724844
Iteration 18200: Loss = -10960.881332358986
1
Iteration 18300: Loss = -10960.876575296996
Iteration 18400: Loss = -10960.877371017215
1
Iteration 18500: Loss = -10960.888120155936
2
Iteration 18600: Loss = -10961.039133178712
3
Iteration 18700: Loss = -10960.876556310726
Iteration 18800: Loss = -10960.879851134334
1
Iteration 18900: Loss = -10960.877993399074
2
Iteration 19000: Loss = -10960.879325367312
3
Iteration 19100: Loss = -10960.882940278898
4
Iteration 19200: Loss = -10960.886379370892
5
Iteration 19300: Loss = -10960.89363225466
6
Iteration 19400: Loss = -10960.876449928874
Iteration 19500: Loss = -10960.876687411948
1
Iteration 19600: Loss = -10960.88983296686
2
Iteration 19700: Loss = -10960.876494504417
Iteration 19800: Loss = -10960.87838309225
1
Iteration 19900: Loss = -10960.8767133033
2
pi: tensor([[0.7657, 0.2343],
        [0.2377, 0.7623]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5921, 0.4079], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1954, 0.0902],
         [0.6116, 0.3030]],

        [[0.5088, 0.1050],
         [0.6228, 0.5315]],

        [[0.6969, 0.0938],
         [0.7295, 0.6292]],

        [[0.6884, 0.0946],
         [0.6965, 0.6087]],

        [[0.5214, 0.1010],
         [0.6265, 0.6792]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.944673063705021
Average Adjusted Rand Index: 0.9446387026155426
10984.846473201182
[0.9291542740196213, 0.944673063705021] [0.9291215876690456, 0.9446387026155426] [10965.588622413134, 10960.88552617444]
-------------------------------------
This iteration is 33
True Objective function: Loss = -11260.433407787514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23809.700613349603
Iteration 100: Loss = -11238.149795193222
Iteration 200: Loss = -11235.742310609123
Iteration 300: Loss = -11235.385736474778
Iteration 400: Loss = -11235.243115311225
Iteration 500: Loss = -11235.170337460273
Iteration 600: Loss = -11235.127779546161
Iteration 700: Loss = -11235.100593129697
Iteration 800: Loss = -11235.08204365122
Iteration 900: Loss = -11235.068821006222
Iteration 1000: Loss = -11235.059082635767
Iteration 1100: Loss = -11235.051625273269
Iteration 1200: Loss = -11235.045861555627
Iteration 1300: Loss = -11235.041276727574
Iteration 1400: Loss = -11235.037530578405
Iteration 1500: Loss = -11235.034496671655
Iteration 1600: Loss = -11235.031983360846
Iteration 1700: Loss = -11235.030084633905
Iteration 1800: Loss = -11235.028062072572
Iteration 1900: Loss = -11235.026543175947
Iteration 2000: Loss = -11235.02610579981
Iteration 2100: Loss = -11235.02411902817
Iteration 2200: Loss = -11235.023029992704
Iteration 2300: Loss = -11235.02221227668
Iteration 2400: Loss = -11235.021948845972
Iteration 2500: Loss = -11235.020850396559
Iteration 2600: Loss = -11235.025440111816
1
Iteration 2700: Loss = -11235.01961078873
Iteration 2800: Loss = -11235.019166041115
Iteration 2900: Loss = -11235.018694352902
Iteration 3000: Loss = -11235.018312283391
Iteration 3100: Loss = -11235.018030303998
Iteration 3200: Loss = -11235.01764649203
Iteration 3300: Loss = -11235.01747729815
Iteration 3400: Loss = -11235.017087654433
Iteration 3500: Loss = -11235.0168543515
Iteration 3600: Loss = -11235.016684747827
Iteration 3700: Loss = -11235.019642537705
1
Iteration 3800: Loss = -11235.016261218738
Iteration 3900: Loss = -11235.016229036704
Iteration 4000: Loss = -11235.015932834775
Iteration 4100: Loss = -11235.015815233552
Iteration 4200: Loss = -11235.01563688061
Iteration 4300: Loss = -11235.015651884141
Iteration 4400: Loss = -11235.015399226815
Iteration 4500: Loss = -11235.015823464872
1
Iteration 4600: Loss = -11235.015230738927
Iteration 4700: Loss = -11235.01510669004
Iteration 4800: Loss = -11235.015123731218
Iteration 4900: Loss = -11235.014959466396
Iteration 5000: Loss = -11235.014944625453
Iteration 5100: Loss = -11235.014849933794
Iteration 5200: Loss = -11235.018051608455
1
Iteration 5300: Loss = -11235.014723609998
Iteration 5400: Loss = -11235.018341374223
1
Iteration 5500: Loss = -11235.01460773475
Iteration 5600: Loss = -11235.018763583808
1
Iteration 5700: Loss = -11235.01531948738
2
Iteration 5800: Loss = -11235.014497629467
Iteration 5900: Loss = -11235.017114309841
1
Iteration 6000: Loss = -11235.014613066884
2
Iteration 6100: Loss = -11235.016309672821
3
Iteration 6200: Loss = -11235.01871270454
4
Iteration 6300: Loss = -11235.014343901148
Iteration 6400: Loss = -11235.014347454222
Iteration 6500: Loss = -11235.014245818627
Iteration 6600: Loss = -11235.014253864547
Iteration 6700: Loss = -11235.01422790607
Iteration 6800: Loss = -11235.014645087998
1
Iteration 6900: Loss = -11235.031703541055
2
Iteration 7000: Loss = -11235.019053088074
3
Iteration 7100: Loss = -11235.014146583466
Iteration 7200: Loss = -11235.014128573801
Iteration 7300: Loss = -11235.043104114377
1
Iteration 7400: Loss = -11235.014096645118
Iteration 7500: Loss = -11235.014049638035
Iteration 7600: Loss = -11235.014083839438
Iteration 7700: Loss = -11235.014050560885
Iteration 7800: Loss = -11235.0141252207
Iteration 7900: Loss = -11235.01404779465
Iteration 8000: Loss = -11235.014237963633
1
Iteration 8100: Loss = -11235.014032506528
Iteration 8200: Loss = -11235.015367894872
1
Iteration 8300: Loss = -11235.015381249497
2
Iteration 8400: Loss = -11235.014024781975
Iteration 8500: Loss = -11235.013985004529
Iteration 8600: Loss = -11235.01403532012
Iteration 8700: Loss = -11235.013992182903
Iteration 8800: Loss = -11235.015942220927
1
Iteration 8900: Loss = -11235.013984872216
Iteration 9000: Loss = -11235.014020654426
Iteration 9100: Loss = -11235.013926344423
Iteration 9200: Loss = -11235.013940045952
Iteration 9300: Loss = -11235.01394689265
Iteration 9400: Loss = -11235.033846221399
1
Iteration 9500: Loss = -11235.013914206802
Iteration 9600: Loss = -11235.013950491411
Iteration 9700: Loss = -11235.01421175038
1
Iteration 9800: Loss = -11235.01390663378
Iteration 9900: Loss = -11235.015553826644
1
Iteration 10000: Loss = -11235.01389800022
Iteration 10100: Loss = -11235.050520482033
1
Iteration 10200: Loss = -11235.013925334137
Iteration 10300: Loss = -11235.014108781608
1
Iteration 10400: Loss = -11235.121547617056
2
Iteration 10500: Loss = -11235.01397171749
Iteration 10600: Loss = -11235.087280359761
1
Iteration 10700: Loss = -11235.01621922606
2
Iteration 10800: Loss = -11235.014522352049
3
Iteration 10900: Loss = -11235.219132511274
4
Iteration 11000: Loss = -11235.013868253702
Iteration 11100: Loss = -11235.015319335618
1
Iteration 11200: Loss = -11235.014370051756
2
Iteration 11300: Loss = -11235.01389785065
Iteration 11400: Loss = -11235.343303803476
1
Iteration 11500: Loss = -11235.013883040534
Iteration 11600: Loss = -11235.036158748933
1
Iteration 11700: Loss = -11235.014178079458
2
Iteration 11800: Loss = -11235.014217127882
3
Iteration 11900: Loss = -11235.021897102246
4
Iteration 12000: Loss = -11235.128650756533
5
Iteration 12100: Loss = -11235.119740759626
6
Iteration 12200: Loss = -11235.014141550715
7
Iteration 12300: Loss = -11235.01393201243
Iteration 12400: Loss = -11235.023699730204
1
Iteration 12500: Loss = -11235.013866643321
Iteration 12600: Loss = -11235.017470437402
1
Iteration 12700: Loss = -11235.013840789887
Iteration 12800: Loss = -11235.024804612596
1
Iteration 12900: Loss = -11235.013840694271
Iteration 13000: Loss = -11235.014221447764
1
Iteration 13100: Loss = -11235.01387923092
Iteration 13200: Loss = -11235.01749401589
1
Iteration 13300: Loss = -11235.038170045638
2
Iteration 13400: Loss = -11235.01512639581
3
Iteration 13500: Loss = -11235.025354314621
4
Iteration 13600: Loss = -11235.013844628364
Iteration 13700: Loss = -11235.014102005684
1
Iteration 13800: Loss = -11235.016511727663
2
Iteration 13900: Loss = -11235.014421991187
3
Iteration 14000: Loss = -11235.013987849861
4
Iteration 14100: Loss = -11235.014978346018
5
Iteration 14200: Loss = -11235.020964856696
6
Iteration 14300: Loss = -11235.013966175282
7
Iteration 14400: Loss = -11235.013999436365
8
Iteration 14500: Loss = -11235.080306682306
9
Iteration 14600: Loss = -11235.013887857729
Iteration 14700: Loss = -11235.029829259947
1
Iteration 14800: Loss = -11235.013883848827
Iteration 14900: Loss = -11235.146407607408
1
Iteration 15000: Loss = -11235.013872640664
Iteration 15100: Loss = -11235.016684450185
1
Iteration 15200: Loss = -11235.013887174006
Iteration 15300: Loss = -11235.01531223954
1
Iteration 15400: Loss = -11235.026131177085
2
Iteration 15500: Loss = -11235.013920264066
Iteration 15600: Loss = -11235.021607287552
1
Iteration 15700: Loss = -11235.013959050822
Iteration 15800: Loss = -11235.014153493961
1
Iteration 15900: Loss = -11235.014863724227
2
Iteration 16000: Loss = -11235.013880414996
Iteration 16100: Loss = -11235.014586879202
1
Iteration 16200: Loss = -11235.013858330733
Iteration 16300: Loss = -11235.015086719446
1
Iteration 16400: Loss = -11235.013921372796
Iteration 16500: Loss = -11235.013882050667
Iteration 16600: Loss = -11235.013866363939
Iteration 16700: Loss = -11235.014014641069
1
Iteration 16800: Loss = -11235.013874870116
Iteration 16900: Loss = -11235.08872090232
1
Iteration 17000: Loss = -11235.019810388078
2
Iteration 17100: Loss = -11235.0455481448
3
Iteration 17200: Loss = -11235.0138490195
Iteration 17300: Loss = -11235.013866319176
Iteration 17400: Loss = -11235.014049091797
1
Iteration 17500: Loss = -11235.013869439017
Iteration 17600: Loss = -11235.030169874497
1
Iteration 17700: Loss = -11235.013876185085
Iteration 17800: Loss = -11235.03209350658
1
Iteration 17900: Loss = -11235.170953574778
2
Iteration 18000: Loss = -11235.016463466309
3
Iteration 18100: Loss = -11235.014630093974
4
Iteration 18200: Loss = -11235.013931961743
Iteration 18300: Loss = -11235.021082716365
1
Iteration 18400: Loss = -11235.01396989873
Iteration 18500: Loss = -11235.013995983141
Iteration 18600: Loss = -11235.230895927278
1
Iteration 18700: Loss = -11235.013858058852
Iteration 18800: Loss = -11235.014331201677
1
Iteration 18900: Loss = -11235.02267733739
2
Iteration 19000: Loss = -11235.01417587753
3
Iteration 19100: Loss = -11235.013981966344
4
Iteration 19200: Loss = -11235.016115900473
5
Iteration 19300: Loss = -11235.086399334337
6
Iteration 19400: Loss = -11235.013893718957
Iteration 19500: Loss = -11235.03485081265
1
Iteration 19600: Loss = -11235.01383196875
Iteration 19700: Loss = -11235.020446601367
1
Iteration 19800: Loss = -11235.013861226817
Iteration 19900: Loss = -11235.017223797775
1
pi: tensor([[0.8024, 0.1976],
        [0.2535, 0.7465]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5134, 0.4866], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2922, 0.0996],
         [0.5915, 0.2074]],

        [[0.5388, 0.0922],
         [0.6997, 0.6210]],

        [[0.5380, 0.0999],
         [0.6840, 0.6121]],

        [[0.5295, 0.0965],
         [0.5900, 0.6179]],

        [[0.7242, 0.1062],
         [0.5599, 0.5282]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
Global Adjusted Rand Index: 0.9214417611804818
Average Adjusted Rand Index: 0.9216028096907112
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23689.27857386968
Iteration 100: Loss = -11519.219418319273
Iteration 200: Loss = -11518.416017794802
Iteration 300: Loss = -11518.255395436205
Iteration 400: Loss = -11518.06803975494
Iteration 500: Loss = -11515.68062944106
Iteration 600: Loss = -11279.848956621763
Iteration 700: Loss = -11246.507903485037
Iteration 800: Loss = -11238.46739457518
Iteration 900: Loss = -11238.148372543019
Iteration 1000: Loss = -11238.096281641476
Iteration 1100: Loss = -11237.892432553324
Iteration 1200: Loss = -11237.863342678687
Iteration 1300: Loss = -11237.687044500793
Iteration 1400: Loss = -11237.673228382682
Iteration 1500: Loss = -11237.663088492538
Iteration 1600: Loss = -11237.655806166802
Iteration 1700: Loss = -11237.635316739123
Iteration 1800: Loss = -11235.72704122393
Iteration 1900: Loss = -11235.718990540869
Iteration 2000: Loss = -11235.698600683045
Iteration 2100: Loss = -11235.586510463854
Iteration 2200: Loss = -11235.582737070896
Iteration 2300: Loss = -11235.55988233785
Iteration 2400: Loss = -11235.39404339377
Iteration 2500: Loss = -11235.32652645974
Iteration 2600: Loss = -11235.31388034452
Iteration 2700: Loss = -11235.310967444568
Iteration 2800: Loss = -11235.309987179935
Iteration 2900: Loss = -11235.309157977244
Iteration 3000: Loss = -11235.308431828467
Iteration 3100: Loss = -11235.307748662066
Iteration 3200: Loss = -11235.307164668528
Iteration 3300: Loss = -11235.306584761114
Iteration 3400: Loss = -11235.30606564603
Iteration 3500: Loss = -11235.305519914213
Iteration 3600: Loss = -11235.305019254642
Iteration 3700: Loss = -11235.304546759375
Iteration 3800: Loss = -11235.303953089791
Iteration 3900: Loss = -11235.305008978523
1
Iteration 4000: Loss = -11235.302820565337
Iteration 4100: Loss = -11235.302516990094
Iteration 4200: Loss = -11235.30178703837
Iteration 4300: Loss = -11235.299390107271
Iteration 4400: Loss = -11235.239761553225
Iteration 4500: Loss = -11235.238791236294
Iteration 4600: Loss = -11235.238609021491
Iteration 4700: Loss = -11235.238736159028
1
Iteration 4800: Loss = -11235.238177995041
Iteration 4900: Loss = -11235.242194449089
1
Iteration 5000: Loss = -11235.23780992454
Iteration 5100: Loss = -11235.237730732493
Iteration 5200: Loss = -11235.237569508521
Iteration 5300: Loss = -11235.237451197623
Iteration 5400: Loss = -11235.237318656587
Iteration 5500: Loss = -11235.245041159737
1
Iteration 5600: Loss = -11235.236939697563
Iteration 5700: Loss = -11235.236818944062
Iteration 5800: Loss = -11235.208271606978
Iteration 5900: Loss = -11235.208571070236
1
Iteration 6000: Loss = -11235.207948919026
Iteration 6100: Loss = -11235.21670909477
1
Iteration 6200: Loss = -11235.207834070028
Iteration 6300: Loss = -11235.207774408978
Iteration 6400: Loss = -11235.231584809979
1
Iteration 6500: Loss = -11235.20766701344
Iteration 6600: Loss = -11235.22677250286
1
Iteration 6700: Loss = -11235.207568230946
Iteration 6800: Loss = -11235.20751728816
Iteration 6900: Loss = -11235.208470648655
1
Iteration 7000: Loss = -11235.165452451378
Iteration 7100: Loss = -11235.163147276658
Iteration 7200: Loss = -11235.16309813882
Iteration 7300: Loss = -11235.170800421816
1
Iteration 7400: Loss = -11235.162994810591
Iteration 7500: Loss = -11235.162807938848
Iteration 7600: Loss = -11235.128035773943
Iteration 7700: Loss = -11235.117992865851
Iteration 7800: Loss = -11235.118073816107
Iteration 7900: Loss = -11235.11794645807
Iteration 8000: Loss = -11235.11789251642
Iteration 8100: Loss = -11235.117915660303
Iteration 8200: Loss = -11235.117846729258
Iteration 8300: Loss = -11235.117959040961
1
Iteration 8400: Loss = -11235.117791523233
Iteration 8500: Loss = -11235.121496265525
1
Iteration 8600: Loss = -11235.117787829578
Iteration 8700: Loss = -11235.11777258566
Iteration 8800: Loss = -11235.12058630874
1
Iteration 8900: Loss = -11235.117732821613
Iteration 9000: Loss = -11235.11774433263
Iteration 9100: Loss = -11235.117885099524
1
Iteration 9200: Loss = -11235.114805697629
Iteration 9300: Loss = -11235.119724743834
1
Iteration 9400: Loss = -11235.114735741237
Iteration 9500: Loss = -11235.114842055109
1
Iteration 9600: Loss = -11235.124925325143
2
Iteration 9700: Loss = -11235.119108539422
3
Iteration 9800: Loss = -11235.114909735647
4
Iteration 9900: Loss = -11235.132849664913
5
Iteration 10000: Loss = -11235.086683802992
Iteration 10100: Loss = -11235.07970318921
Iteration 10200: Loss = -11235.075358798518
Iteration 10300: Loss = -11235.077005181032
1
Iteration 10400: Loss = -11235.075331849253
Iteration 10500: Loss = -11235.07524291402
Iteration 10600: Loss = -11235.077911611512
1
Iteration 10700: Loss = -11235.112619404894
2
Iteration 10800: Loss = -11235.066179522484
Iteration 10900: Loss = -11235.06893962739
1
Iteration 11000: Loss = -11235.065530476832
Iteration 11100: Loss = -11235.06557073073
Iteration 11200: Loss = -11235.122497343815
1
Iteration 11300: Loss = -11235.065722317306
2
Iteration 11400: Loss = -11235.065431842602
Iteration 11500: Loss = -11235.07278834681
1
Iteration 11600: Loss = -11235.073226029994
2
Iteration 11700: Loss = -11235.066722018377
3
Iteration 11800: Loss = -11235.065749972808
4
Iteration 11900: Loss = -11235.064942395062
Iteration 12000: Loss = -11235.06513525544
1
Iteration 12100: Loss = -11235.085641721114
2
Iteration 12200: Loss = -11235.067257137935
3
Iteration 12300: Loss = -11235.095197111785
4
Iteration 12400: Loss = -11235.291151229389
5
Iteration 12500: Loss = -11235.070318109021
6
Iteration 12600: Loss = -11235.063722809206
Iteration 12700: Loss = -11235.058960316825
Iteration 12800: Loss = -11235.057767833972
Iteration 12900: Loss = -11235.058919093044
1
Iteration 13000: Loss = -11235.063343869451
2
Iteration 13100: Loss = -11235.05187747217
Iteration 13200: Loss = -11235.046025730153
Iteration 13300: Loss = -11235.050740436956
1
Iteration 13400: Loss = -11235.044379777719
Iteration 13500: Loss = -11235.044717939556
1
Iteration 13600: Loss = -11235.04893769876
2
Iteration 13700: Loss = -11235.044321116886
Iteration 13800: Loss = -11235.04471737291
1
Iteration 13900: Loss = -11235.321426186094
2
Iteration 14000: Loss = -11235.04424814287
Iteration 14100: Loss = -11235.098605754141
1
Iteration 14200: Loss = -11235.043930415699
Iteration 14300: Loss = -11235.035399807766
Iteration 14400: Loss = -11235.033383798116
Iteration 14500: Loss = -11235.033345335361
Iteration 14600: Loss = -11235.033810273908
1
Iteration 14700: Loss = -11235.033355431648
Iteration 14800: Loss = -11235.04301154769
1
Iteration 14900: Loss = -11235.146089109177
2
Iteration 15000: Loss = -11235.033364872064
Iteration 15100: Loss = -11235.034328021755
1
Iteration 15200: Loss = -11235.245304562804
2
Iteration 15300: Loss = -11235.033363355222
Iteration 15400: Loss = -11235.046932862711
1
Iteration 15500: Loss = -11235.033329336025
Iteration 15600: Loss = -11235.188061768078
1
Iteration 15700: Loss = -11235.033178374484
Iteration 15800: Loss = -11235.14869685291
1
Iteration 15900: Loss = -11235.033174696842
Iteration 16000: Loss = -11235.112537166719
1
Iteration 16100: Loss = -11235.032481114458
Iteration 16200: Loss = -11235.039268649378
1
Iteration 16300: Loss = -11235.032448798649
Iteration 16400: Loss = -11235.051786187618
1
Iteration 16500: Loss = -11235.032267100083
Iteration 16600: Loss = -11235.039562317044
1
Iteration 16700: Loss = -11235.032193449182
Iteration 16800: Loss = -11235.036045330775
1
Iteration 16900: Loss = -11235.032409876616
2
Iteration 17000: Loss = -11235.032649576018
3
Iteration 17100: Loss = -11235.032501108371
4
Iteration 17200: Loss = -11235.032293397213
Iteration 17300: Loss = -11235.033813755208
1
Iteration 17400: Loss = -11235.036446038808
2
Iteration 17500: Loss = -11235.031676194685
Iteration 17600: Loss = -11235.0318962528
1
Iteration 17700: Loss = -11235.08057999826
2
Iteration 17800: Loss = -11235.031667433448
Iteration 17900: Loss = -11235.085550705497
1
Iteration 18000: Loss = -11235.031668750355
Iteration 18100: Loss = -11235.031857595164
1
Iteration 18200: Loss = -11235.047983323098
2
Iteration 18300: Loss = -11235.03149373212
Iteration 18400: Loss = -11235.032453237198
1
Iteration 18500: Loss = -11235.03154967698
Iteration 18600: Loss = -11235.031490115292
Iteration 18700: Loss = -11235.031681951674
1
Iteration 18800: Loss = -11235.032474535969
2
Iteration 18900: Loss = -11235.036938078945
3
Iteration 19000: Loss = -11235.031511121315
Iteration 19100: Loss = -11235.034170839837
1
Iteration 19200: Loss = -11235.031523294745
Iteration 19300: Loss = -11235.035293143188
1
Iteration 19400: Loss = -11235.031453404481
Iteration 19500: Loss = -11235.031944367503
1
Iteration 19600: Loss = -11235.056386022656
2
Iteration 19700: Loss = -11235.041037789248
3
Iteration 19800: Loss = -11235.03187908412
4
Iteration 19900: Loss = -11235.033114540875
5
pi: tensor([[0.8023, 0.1977],
        [0.2520, 0.7480]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5123, 0.4877], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2925, 0.0995],
         [0.5367, 0.2071]],

        [[0.6986, 0.0923],
         [0.6571, 0.6310]],

        [[0.7083, 0.1002],
         [0.7028, 0.6616]],

        [[0.5529, 0.0968],
         [0.6114, 0.6480]],

        [[0.6517, 0.1064],
         [0.5506, 0.6367]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
Global Adjusted Rand Index: 0.9214417611804818
Average Adjusted Rand Index: 0.9216028096907112
11260.433407787514
[0.9214417611804818, 0.9214417611804818] [0.9216028096907112, 0.9216028096907112] [11235.013847300976, 11235.039718754984]
-------------------------------------
This iteration is 34
True Objective function: Loss = -11392.761532787514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20789.473788002262
Iteration 100: Loss = -11718.681877775749
Iteration 200: Loss = -11717.230267812794
Iteration 300: Loss = -11708.8885101634
Iteration 400: Loss = -11598.962770808934
Iteration 500: Loss = -11457.426396760711
Iteration 600: Loss = -11448.27315714553
Iteration 700: Loss = -11446.985842480326
Iteration 800: Loss = -11446.081427470164
Iteration 900: Loss = -11443.737662176685
Iteration 1000: Loss = -11443.590994268458
Iteration 1100: Loss = -11443.468279750234
Iteration 1200: Loss = -11443.322416515604
Iteration 1300: Loss = -11443.073547975277
Iteration 1400: Loss = -11441.951663327933
Iteration 1500: Loss = -11381.533329064889
Iteration 1600: Loss = -11372.301503373317
Iteration 1700: Loss = -11372.15414176216
Iteration 1800: Loss = -11372.074107669425
Iteration 1900: Loss = -11372.040528181795
Iteration 2000: Loss = -11372.01610565563
Iteration 2100: Loss = -11371.997727681319
Iteration 2200: Loss = -11371.983059215594
Iteration 2300: Loss = -11371.971880337802
Iteration 2400: Loss = -11371.966420756211
Iteration 2500: Loss = -11371.957061939509
Iteration 2600: Loss = -11371.952136131047
Iteration 2700: Loss = -11371.948134026294
Iteration 2800: Loss = -11371.944881266209
Iteration 2900: Loss = -11371.942015909906
Iteration 3000: Loss = -11371.93952594441
Iteration 3100: Loss = -11371.94821917297
1
Iteration 3200: Loss = -11371.935145962982
Iteration 3300: Loss = -11371.933137112701
Iteration 3400: Loss = -11371.937703552085
1
Iteration 3500: Loss = -11371.929953094394
Iteration 3600: Loss = -11371.928727196786
Iteration 3700: Loss = -11371.92860486067
Iteration 3800: Loss = -11371.929870777933
1
Iteration 3900: Loss = -11371.925783981369
Iteration 4000: Loss = -11371.92481630388
Iteration 4100: Loss = -11371.924172076595
Iteration 4200: Loss = -11371.923472825702
Iteration 4300: Loss = -11371.922855957155
Iteration 4400: Loss = -11371.923966129694
1
Iteration 4500: Loss = -11371.928023454051
2
Iteration 4600: Loss = -11371.92145176132
Iteration 4700: Loss = -11371.920969201157
Iteration 4800: Loss = -11371.927230245648
1
Iteration 4900: Loss = -11371.924803858547
2
Iteration 5000: Loss = -11371.919931271545
Iteration 5100: Loss = -11371.925391884108
1
Iteration 5200: Loss = -11371.919052007848
Iteration 5300: Loss = -11371.9187709861
Iteration 5400: Loss = -11371.924412439152
1
Iteration 5500: Loss = -11371.918368974264
Iteration 5600: Loss = -11371.918802525142
1
Iteration 5700: Loss = -11371.917801387026
Iteration 5800: Loss = -11371.917550365515
Iteration 5900: Loss = -11371.918213600135
1
Iteration 6000: Loss = -11371.91711053334
Iteration 6100: Loss = -11371.916914254842
Iteration 6200: Loss = -11371.918393261922
1
Iteration 6300: Loss = -11371.9164534839
Iteration 6400: Loss = -11371.916644543937
1
Iteration 6500: Loss = -11371.916207071603
Iteration 6600: Loss = -11371.927056897153
1
Iteration 6700: Loss = -11371.917022124111
2
Iteration 6800: Loss = -11371.916754418955
3
Iteration 6900: Loss = -11371.915754542628
Iteration 7000: Loss = -11371.915612762816
Iteration 7100: Loss = -11371.915564697134
Iteration 7200: Loss = -11371.91540143074
Iteration 7300: Loss = -11371.915635316578
1
Iteration 7400: Loss = -11371.915457057916
Iteration 7500: Loss = -11371.917005008816
1
Iteration 7600: Loss = -11371.915136913074
Iteration 7700: Loss = -11371.915068544411
Iteration 7800: Loss = -11371.915062402242
Iteration 7900: Loss = -11371.914987807791
Iteration 8000: Loss = -11371.914957611802
Iteration 8100: Loss = -11371.915432737153
1
Iteration 8200: Loss = -11371.914864603677
Iteration 8300: Loss = -11372.122574887731
1
Iteration 8400: Loss = -11371.914786542628
Iteration 8500: Loss = -11371.914735087577
Iteration 8600: Loss = -11371.9155689638
1
Iteration 8700: Loss = -11371.914714674587
Iteration 8800: Loss = -11371.917126658876
1
Iteration 8900: Loss = -11371.914633208324
Iteration 9000: Loss = -11371.914641718517
Iteration 9100: Loss = -11371.914975584332
1
Iteration 9200: Loss = -11371.914609247558
Iteration 9300: Loss = -11371.91459898323
Iteration 9400: Loss = -11371.921184873254
1
Iteration 9500: Loss = -11371.914534758784
Iteration 9600: Loss = -11371.9173413128
1
Iteration 9700: Loss = -11371.914524857475
Iteration 9800: Loss = -11371.915256387363
1
Iteration 9900: Loss = -11371.914760294054
2
Iteration 10000: Loss = -11371.91458694549
Iteration 10100: Loss = -11371.914630291249
Iteration 10200: Loss = -11371.915510332168
1
Iteration 10300: Loss = -11371.921224660155
2
Iteration 10400: Loss = -11371.914489939163
Iteration 10500: Loss = -11372.029829742492
1
Iteration 10600: Loss = -11371.91440968707
Iteration 10700: Loss = -11372.247400062683
1
Iteration 10800: Loss = -11371.914384113888
Iteration 10900: Loss = -11371.91438887083
Iteration 11000: Loss = -11371.914715648474
1
Iteration 11100: Loss = -11371.914353506689
Iteration 11200: Loss = -11371.935942117823
1
Iteration 11300: Loss = -11371.914341676114
Iteration 11400: Loss = -11371.915281567832
1
Iteration 11500: Loss = -11371.91433861368
Iteration 11600: Loss = -11371.914745213988
1
Iteration 11700: Loss = -11371.914292444302
Iteration 11800: Loss = -11371.91432716871
Iteration 11900: Loss = -11371.947858654601
1
Iteration 12000: Loss = -11371.917407638282
2
Iteration 12100: Loss = -11371.970729010076
3
Iteration 12200: Loss = -11371.914295999823
Iteration 12300: Loss = -11371.915335267662
1
Iteration 12400: Loss = -11372.03556769494
2
Iteration 12500: Loss = -11371.914321657245
Iteration 12600: Loss = -11372.076186667162
1
Iteration 12700: Loss = -11371.914306887385
Iteration 12800: Loss = -11371.933867140184
1
Iteration 12900: Loss = -11371.914294508564
Iteration 13000: Loss = -11371.915467046456
1
Iteration 13100: Loss = -11371.914362004629
Iteration 13200: Loss = -11371.914355679028
Iteration 13300: Loss = -11371.914283425547
Iteration 13400: Loss = -11371.914280872968
Iteration 13500: Loss = -11371.914290536099
Iteration 13600: Loss = -11371.915709624316
1
Iteration 13700: Loss = -11371.914250844742
Iteration 13800: Loss = -11371.915449807695
1
Iteration 13900: Loss = -11371.914259452795
Iteration 14000: Loss = -11371.916845801807
1
Iteration 14100: Loss = -11371.921048184966
2
Iteration 14200: Loss = -11371.916883114087
3
Iteration 14300: Loss = -11371.914333518242
Iteration 14400: Loss = -11371.916259134614
1
Iteration 14500: Loss = -11372.058038544572
2
Iteration 14600: Loss = -11371.914272134718
Iteration 14700: Loss = -11371.941755927013
1
Iteration 14800: Loss = -11371.914222154948
Iteration 14900: Loss = -11371.918148068462
1
Iteration 15000: Loss = -11371.914213787182
Iteration 15100: Loss = -11371.919091351136
1
Iteration 15200: Loss = -11371.914242350556
Iteration 15300: Loss = -11371.915597639427
1
Iteration 15400: Loss = -11371.914242171499
Iteration 15500: Loss = -11371.999638920353
1
Iteration 15600: Loss = -11371.913914057386
Iteration 15700: Loss = -11371.913993650582
Iteration 15800: Loss = -11371.91621835439
1
Iteration 15900: Loss = -11371.913907797627
Iteration 16000: Loss = -11371.915975033453
1
Iteration 16100: Loss = -11372.032445974915
2
Iteration 16200: Loss = -11371.913763889585
Iteration 16300: Loss = -11371.916168193948
1
Iteration 16400: Loss = -11371.921392713517
2
Iteration 16500: Loss = -11371.927344197584
3
Iteration 16600: Loss = -11371.91378942553
Iteration 16700: Loss = -11371.914457503182
1
Iteration 16800: Loss = -11371.927047923235
2
Iteration 16900: Loss = -11371.92800953161
3
Iteration 17000: Loss = -11371.913769613619
Iteration 17100: Loss = -11371.9139016441
1
Iteration 17200: Loss = -11371.944232119684
2
Iteration 17300: Loss = -11371.943845621061
3
Iteration 17400: Loss = -11371.914670061158
4
Iteration 17500: Loss = -11371.913821488635
Iteration 17600: Loss = -11371.914315385256
1
Iteration 17700: Loss = -11371.929571271428
2
Iteration 17800: Loss = -11371.91373819218
Iteration 17900: Loss = -11371.91672493258
1
Iteration 18000: Loss = -11371.913741780398
Iteration 18100: Loss = -11372.041886861014
1
Iteration 18200: Loss = -11371.913765362742
Iteration 18300: Loss = -11371.914555779842
1
Iteration 18400: Loss = -11372.046731442953
2
Iteration 18500: Loss = -11371.913722900576
Iteration 18600: Loss = -11371.913969179059
1
Iteration 18700: Loss = -11371.915938745133
2
Iteration 18800: Loss = -11371.981999730742
3
Iteration 18900: Loss = -11371.913709020777
Iteration 19000: Loss = -11371.913786106023
Iteration 19100: Loss = -11371.913700976187
Iteration 19200: Loss = -11371.913833686365
1
Iteration 19300: Loss = -11371.932257998724
2
Iteration 19400: Loss = -11371.913700864889
Iteration 19500: Loss = -11371.919096955304
1
Iteration 19600: Loss = -11371.91368861335
Iteration 19700: Loss = -11371.914285391254
1
Iteration 19800: Loss = -11371.918085859124
2
Iteration 19900: Loss = -11371.913693811472
pi: tensor([[0.7722, 0.2278],
        [0.2522, 0.7478]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5170, 0.4830], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3060, 0.1066],
         [0.5069, 0.2083]],

        [[0.6085, 0.1010],
         [0.5215, 0.5632]],

        [[0.5782, 0.1011],
         [0.6746, 0.7186]],

        [[0.7220, 0.0983],
         [0.5269, 0.6128]],

        [[0.5531, 0.1041],
         [0.5186, 0.6839]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9446733310508894
Average Adjusted Rand Index: 0.9446421939414467
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21693.95021309475
Iteration 100: Loss = -11771.991861504586
Iteration 200: Loss = -11722.944867826685
Iteration 300: Loss = -11712.089178723305
Iteration 400: Loss = -11711.863580240622
Iteration 500: Loss = -11711.781191579183
Iteration 600: Loss = -11711.734836022764
Iteration 700: Loss = -11711.704873674082
Iteration 800: Loss = -11711.683677894847
Iteration 900: Loss = -11711.667434971712
Iteration 1000: Loss = -11711.654231469873
Iteration 1100: Loss = -11711.643290727665
Iteration 1200: Loss = -11711.63421253754
Iteration 1300: Loss = -11711.62700211666
Iteration 1400: Loss = -11711.621660724786
Iteration 1500: Loss = -11711.617807712959
Iteration 1600: Loss = -11711.615099724204
Iteration 1700: Loss = -11711.613531070077
Iteration 1800: Loss = -11711.611561480717
Iteration 1900: Loss = -11711.610289143535
Iteration 2000: Loss = -11711.609298932119
Iteration 2100: Loss = -11711.608328799945
Iteration 2200: Loss = -11711.607493710573
Iteration 2300: Loss = -11711.606875490703
Iteration 2400: Loss = -11711.606212391462
Iteration 2500: Loss = -11711.60575802924
Iteration 2600: Loss = -11711.605156639274
Iteration 2700: Loss = -11711.604672182912
Iteration 2800: Loss = -11711.62101019953
1
Iteration 2900: Loss = -11711.603903807601
Iteration 3000: Loss = -11711.603548419382
Iteration 3100: Loss = -11711.60328352656
Iteration 3200: Loss = -11711.603003490292
Iteration 3300: Loss = -11711.602709188024
Iteration 3400: Loss = -11711.602473225515
Iteration 3500: Loss = -11711.602229191772
Iteration 3600: Loss = -11711.603236133627
1
Iteration 3700: Loss = -11711.601856022813
Iteration 3800: Loss = -11711.60171481773
Iteration 3900: Loss = -11711.601520840248
Iteration 4000: Loss = -11711.601395136779
Iteration 4100: Loss = -11711.602175572463
1
Iteration 4200: Loss = -11711.601106885719
Iteration 4300: Loss = -11711.600981024969
Iteration 4400: Loss = -11711.600890044152
Iteration 4500: Loss = -11711.600802306806
Iteration 4600: Loss = -11711.601238815838
1
Iteration 4700: Loss = -11711.600572349322
Iteration 4800: Loss = -11711.600546175207
Iteration 4900: Loss = -11711.600467384667
Iteration 5000: Loss = -11711.600349544045
Iteration 5100: Loss = -11711.600474961177
1
Iteration 5200: Loss = -11711.600222849895
Iteration 5300: Loss = -11711.60012249382
Iteration 5400: Loss = -11711.600739203535
1
Iteration 5500: Loss = -11711.600058098433
Iteration 5600: Loss = -11711.599990734658
Iteration 5700: Loss = -11711.59996620231
Iteration 5800: Loss = -11711.599906010244
Iteration 5900: Loss = -11711.630734887636
1
Iteration 6000: Loss = -11711.59977881474
Iteration 6100: Loss = -11711.599766955522
Iteration 6200: Loss = -11711.602938613782
1
Iteration 6300: Loss = -11711.599727726809
Iteration 6400: Loss = -11711.599658524467
Iteration 6500: Loss = -11711.599638052921
Iteration 6600: Loss = -11711.599578919724
Iteration 6700: Loss = -11711.599745998463
1
Iteration 6800: Loss = -11711.600205778157
2
Iteration 6900: Loss = -11711.599534057768
Iteration 7000: Loss = -11711.600434118562
1
Iteration 7100: Loss = -11711.599511722425
Iteration 7200: Loss = -11711.599818611445
1
Iteration 7300: Loss = -11711.599412992271
Iteration 7400: Loss = -11711.815539770136
1
Iteration 7500: Loss = -11711.59939980722
Iteration 7600: Loss = -11711.599373970108
Iteration 7700: Loss = -11711.600716848327
1
Iteration 7800: Loss = -11711.599401243655
Iteration 7900: Loss = -11711.59936815622
Iteration 8000: Loss = -11711.599546956197
1
Iteration 8100: Loss = -11711.599333428841
Iteration 8200: Loss = -11711.599667043718
1
Iteration 8300: Loss = -11711.599331159177
Iteration 8400: Loss = -11711.599305844105
Iteration 8500: Loss = -11711.602744844105
1
Iteration 8600: Loss = -11711.59922295427
Iteration 8700: Loss = -11711.599278059512
Iteration 8800: Loss = -11711.601878353167
1
Iteration 8900: Loss = -11711.59925481912
Iteration 9000: Loss = -11711.599237924454
Iteration 9100: Loss = -11711.599419990871
1
Iteration 9200: Loss = -11711.599242022483
Iteration 9300: Loss = -11711.599260834048
Iteration 9400: Loss = -11711.599376085762
1
Iteration 9500: Loss = -11711.59917491974
Iteration 9600: Loss = -11711.721911718061
1
Iteration 9700: Loss = -11711.599216631854
Iteration 9800: Loss = -11711.599189496332
Iteration 9900: Loss = -11711.60167056561
1
Iteration 10000: Loss = -11711.599188549762
Iteration 10100: Loss = -11711.599275754119
Iteration 10200: Loss = -11711.59921205732
Iteration 10300: Loss = -11711.599151550636
Iteration 10400: Loss = -11711.815784439164
1
Iteration 10500: Loss = -11711.59916362305
Iteration 10600: Loss = -11711.599139247795
Iteration 10700: Loss = -11711.619641709414
1
Iteration 10800: Loss = -11711.599149572223
Iteration 10900: Loss = -11711.599154778825
Iteration 11000: Loss = -11711.602416738928
1
Iteration 11100: Loss = -11711.599142822677
Iteration 11200: Loss = -11711.599373099098
1
Iteration 11300: Loss = -11711.599431407525
2
Iteration 11400: Loss = -11711.861616814112
3
Iteration 11500: Loss = -11711.599151990402
Iteration 11600: Loss = -11711.599134824817
Iteration 11700: Loss = -11711.60007886045
1
Iteration 11800: Loss = -11711.623456740756
2
Iteration 11900: Loss = -11711.599156228822
Iteration 12000: Loss = -11711.600722151838
1
Iteration 12100: Loss = -11711.599620091512
2
Iteration 12200: Loss = -11711.638385782884
3
Iteration 12300: Loss = -11711.59915432185
Iteration 12400: Loss = -11711.599288795933
1
Iteration 12500: Loss = -11711.648971464889
2
Iteration 12600: Loss = -11711.605746448067
3
Iteration 12700: Loss = -11711.617193633532
4
Iteration 12800: Loss = -11711.599343439135
5
Iteration 12900: Loss = -11711.599134384724
Iteration 13000: Loss = -11711.599915780784
1
Iteration 13100: Loss = -11711.599084865142
Iteration 13200: Loss = -11711.59957681916
1
Iteration 13300: Loss = -11711.59918810033
2
Iteration 13400: Loss = -11711.599279979102
3
Iteration 13500: Loss = -11711.656324535434
4
Iteration 13600: Loss = -11711.59906542142
Iteration 13700: Loss = -11711.644666192657
1
Iteration 13800: Loss = -11711.602051798622
2
Iteration 13900: Loss = -11711.605107550267
3
Iteration 14000: Loss = -11711.599143167236
Iteration 14100: Loss = -11711.59939480654
1
Iteration 14200: Loss = -11711.651096098736
2
Iteration 14300: Loss = -11711.59916648064
Iteration 14400: Loss = -11711.752337899616
1
Iteration 14500: Loss = -11711.599133685711
Iteration 14600: Loss = -11711.616969222247
1
Iteration 14700: Loss = -11711.59922958091
Iteration 14800: Loss = -11711.599277318735
Iteration 14900: Loss = -11711.709239933652
1
Iteration 15000: Loss = -11711.644323208206
2
Iteration 15100: Loss = -11711.613421307684
3
Iteration 15200: Loss = -11711.59950989979
4
Iteration 15300: Loss = -11711.600197240514
5
Iteration 15400: Loss = -11711.599424003696
6
Iteration 15500: Loss = -11711.608643885413
7
Iteration 15600: Loss = -11711.62964558224
8
Iteration 15700: Loss = -11711.600710558125
9
Iteration 15800: Loss = -11711.599149405092
Iteration 15900: Loss = -11711.602254147625
1
Iteration 16000: Loss = -11711.71925740895
2
Iteration 16100: Loss = -11711.659792234967
3
Iteration 16200: Loss = -11711.633027560685
4
Iteration 16300: Loss = -11711.59990208987
5
Iteration 16400: Loss = -11711.599386897977
6
Iteration 16500: Loss = -11711.599380892518
7
Iteration 16600: Loss = -11711.604188054513
8
Iteration 16700: Loss = -11711.610216236078
9
Iteration 16800: Loss = -11711.613565912872
10
Iteration 16900: Loss = -11711.600366809009
11
Iteration 17000: Loss = -11711.599343528324
12
Iteration 17100: Loss = -11711.600022664989
13
Iteration 17200: Loss = -11711.599168602797
Iteration 17300: Loss = -11711.604176870367
1
Iteration 17400: Loss = -11711.599239168003
Iteration 17500: Loss = -11711.599530795145
1
Iteration 17600: Loss = -11711.599334573664
Iteration 17700: Loss = -11711.599525476437
1
Iteration 17800: Loss = -11711.599117920367
Iteration 17900: Loss = -11711.599382230148
1
Iteration 18000: Loss = -11711.59928923741
2
Iteration 18100: Loss = -11711.644914523175
3
Iteration 18200: Loss = -11711.599103824783
Iteration 18300: Loss = -11711.599889837997
1
Iteration 18400: Loss = -11711.869945010563
2
Iteration 18500: Loss = -11711.599098586345
Iteration 18600: Loss = -11711.607531296644
1
Iteration 18700: Loss = -11711.806542657438
2
Iteration 18800: Loss = -11711.599269414592
3
Iteration 18900: Loss = -11711.599146287914
Iteration 19000: Loss = -11711.599682381382
1
Iteration 19100: Loss = -11711.607573046647
2
Iteration 19200: Loss = -11711.602666698569
3
Iteration 19300: Loss = -11711.599485005623
4
Iteration 19400: Loss = -11711.627992844138
5
Iteration 19500: Loss = -11711.599422969715
6
Iteration 19600: Loss = -11711.599310111904
7
Iteration 19700: Loss = -11711.602724924238
8
Iteration 19800: Loss = -11711.599686308862
9
Iteration 19900: Loss = -11711.599505728416
10
pi: tensor([[0.9861, 0.0139],
        [0.6932, 0.3068]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.1177e-09, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1751, 0.1709],
         [0.6933, 0.1798]],

        [[0.7143, 0.1770],
         [0.5372, 0.5692]],

        [[0.6628, 0.2320],
         [0.7175, 0.5492]],

        [[0.6698, 0.2679],
         [0.5621, 0.5638]],

        [[0.5503, 0.2760],
         [0.6325, 0.6565]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.01150857271232653
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: 0.00013749821055598866
Average Adjusted Rand Index: 0.0010137735819935804
11392.761532787514
[0.9446733310508894, 0.00013749821055598866] [0.9446421939414467, 0.0010137735819935804] [11371.932008424606, 11711.59973950928]
-------------------------------------
This iteration is 35
True Objective function: Loss = -11289.108458760997
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21487.671785366492
Iteration 100: Loss = -11640.967377495339
Iteration 200: Loss = -11637.022377406187
Iteration 300: Loss = -11631.186502750868
Iteration 400: Loss = -11582.303970457986
Iteration 500: Loss = -11323.625249119546
Iteration 600: Loss = -11274.504550280883
Iteration 700: Loss = -11273.499030600928
Iteration 800: Loss = -11273.082045304689
Iteration 900: Loss = -11272.808636890533
Iteration 1000: Loss = -11272.448313522487
Iteration 1100: Loss = -11272.350789170026
Iteration 1200: Loss = -11272.289035496364
Iteration 1300: Loss = -11272.244606667035
Iteration 1400: Loss = -11272.209576091385
Iteration 1500: Loss = -11272.178574172727
Iteration 1600: Loss = -11272.145503368909
Iteration 1700: Loss = -11272.083597194538
Iteration 1800: Loss = -11272.03806993252
Iteration 1900: Loss = -11272.001622001892
Iteration 2000: Loss = -11271.969385778715
Iteration 2100: Loss = -11271.958189196403
Iteration 2200: Loss = -11271.950311490924
Iteration 2300: Loss = -11271.943745028355
Iteration 2400: Loss = -11271.938113874494
Iteration 2500: Loss = -11271.9332244576
Iteration 2600: Loss = -11271.928948553583
Iteration 2700: Loss = -11271.925150881081
Iteration 2800: Loss = -11271.921671997245
Iteration 2900: Loss = -11271.918540030432
Iteration 3000: Loss = -11271.917985521655
Iteration 3100: Loss = -11271.913151590972
Iteration 3200: Loss = -11271.9107851115
Iteration 3300: Loss = -11271.909114554803
Iteration 3400: Loss = -11271.906628629122
Iteration 3500: Loss = -11271.904577248042
Iteration 3600: Loss = -11271.902732690736
Iteration 3700: Loss = -11271.900274979298
Iteration 3800: Loss = -11271.898106765559
Iteration 3900: Loss = -11271.89666724073
Iteration 4000: Loss = -11271.89546169136
Iteration 4100: Loss = -11271.894358780368
Iteration 4200: Loss = -11271.893258638316
Iteration 4300: Loss = -11271.892094851928
Iteration 4400: Loss = -11271.890562470151
Iteration 4500: Loss = -11271.889406236009
Iteration 4600: Loss = -11271.889123894573
Iteration 4700: Loss = -11271.885745617134
Iteration 4800: Loss = -11271.884952248343
Iteration 4900: Loss = -11271.885045960611
Iteration 5000: Loss = -11271.887317386652
1
Iteration 5100: Loss = -11271.883191645236
Iteration 5200: Loss = -11271.88331584639
1
Iteration 5300: Loss = -11271.885098076144
2
Iteration 5400: Loss = -11271.880841824035
Iteration 5500: Loss = -11271.880340215383
Iteration 5600: Loss = -11271.884337363614
1
Iteration 5700: Loss = -11271.884795786027
2
Iteration 5800: Loss = -11271.879234912909
Iteration 5900: Loss = -11271.87868469199
Iteration 6000: Loss = -11271.878483239274
Iteration 6100: Loss = -11271.881281165262
1
Iteration 6200: Loss = -11271.878696389975
2
Iteration 6300: Loss = -11271.877373078705
Iteration 6400: Loss = -11271.87709463097
Iteration 6500: Loss = -11271.879549301973
1
Iteration 6600: Loss = -11271.877460548321
2
Iteration 6700: Loss = -11271.87638361207
Iteration 6800: Loss = -11271.884740241852
1
Iteration 6900: Loss = -11271.87685555642
2
Iteration 7000: Loss = -11271.876531004855
3
Iteration 7100: Loss = -11271.875191041483
Iteration 7200: Loss = -11271.895527210172
1
Iteration 7300: Loss = -11271.881725057037
2
Iteration 7400: Loss = -11271.885028623996
3
Iteration 7500: Loss = -11271.872684195449
Iteration 7600: Loss = -11271.875386216627
1
Iteration 7700: Loss = -11271.872332502018
Iteration 7800: Loss = -11271.87351568883
1
Iteration 7900: Loss = -11271.871944573773
Iteration 8000: Loss = -11271.874616701
1
Iteration 8100: Loss = -11271.864334517308
Iteration 8200: Loss = -11271.875180444142
1
Iteration 8300: Loss = -11271.863938914748
Iteration 8400: Loss = -11271.863845523563
Iteration 8500: Loss = -11271.871961849576
1
Iteration 8600: Loss = -11271.863729460003
Iteration 8700: Loss = -11271.863798733524
Iteration 8800: Loss = -11271.86981194895
1
Iteration 8900: Loss = -11271.863790983994
Iteration 9000: Loss = -11271.910529199618
1
Iteration 9100: Loss = -11271.867848072143
2
Iteration 9200: Loss = -11271.866171906488
3
Iteration 9300: Loss = -11271.865480350923
4
Iteration 9400: Loss = -11271.88306831169
5
Iteration 9500: Loss = -11271.86545206777
6
Iteration 9600: Loss = -11271.864338146106
7
Iteration 9700: Loss = -11271.88082849872
8
Iteration 9800: Loss = -11271.863134201018
Iteration 9900: Loss = -11271.86348951951
1
Iteration 10000: Loss = -11271.863085617697
Iteration 10100: Loss = -11271.863463732094
1
Iteration 10200: Loss = -11271.863427580865
2
Iteration 10300: Loss = -11271.863077473517
Iteration 10400: Loss = -11271.86328463948
1
Iteration 10500: Loss = -11271.928404117336
2
Iteration 10600: Loss = -11271.862983404699
Iteration 10700: Loss = -11271.863738375734
1
Iteration 10800: Loss = -11271.862874609149
Iteration 10900: Loss = -11271.862689282112
Iteration 11000: Loss = -11271.868721109751
1
Iteration 11100: Loss = -11271.86337963733
2
Iteration 11200: Loss = -11271.862718166984
Iteration 11300: Loss = -11271.869537037848
1
Iteration 11400: Loss = -11271.866012545985
2
Iteration 11500: Loss = -11271.86079799035
Iteration 11600: Loss = -11271.86145540377
1
Iteration 11700: Loss = -11271.861315606357
2
Iteration 11800: Loss = -11271.860499063989
Iteration 11900: Loss = -11271.863460168743
1
Iteration 12000: Loss = -11271.861068555225
2
Iteration 12100: Loss = -11271.86190758562
3
Iteration 12200: Loss = -11271.872447674485
4
Iteration 12300: Loss = -11271.953944467228
5
Iteration 12400: Loss = -11271.867604595223
6
Iteration 12500: Loss = -11271.860107831988
Iteration 12600: Loss = -11271.861192517083
1
Iteration 12700: Loss = -11271.860848841228
2
Iteration 12800: Loss = -11271.86155273123
3
Iteration 12900: Loss = -11271.860373840384
4
Iteration 13000: Loss = -11271.860070910894
Iteration 13100: Loss = -11271.860825680418
1
Iteration 13200: Loss = -11271.860035884027
Iteration 13300: Loss = -11271.864839569422
1
Iteration 13400: Loss = -11271.860334766161
2
Iteration 13500: Loss = -11271.860772756772
3
Iteration 13600: Loss = -11271.863417625393
4
Iteration 13700: Loss = -11271.860459203039
5
Iteration 13800: Loss = -11271.859962675308
Iteration 13900: Loss = -11271.860020029804
Iteration 14000: Loss = -11271.859888511108
Iteration 14100: Loss = -11271.860386612037
1
Iteration 14200: Loss = -11271.881081895139
2
Iteration 14300: Loss = -11271.90717310749
3
Iteration 14400: Loss = -11271.85985414687
Iteration 14500: Loss = -11271.860161430532
1
Iteration 14600: Loss = -11271.86163640969
2
Iteration 14700: Loss = -11271.859593407607
Iteration 14800: Loss = -11271.858728150653
Iteration 14900: Loss = -11271.858743028215
Iteration 15000: Loss = -11271.945468282802
1
Iteration 15100: Loss = -11271.858629828963
Iteration 15200: Loss = -11271.862544951173
1
Iteration 15300: Loss = -11271.858629971417
Iteration 15400: Loss = -11271.859787128402
1
Iteration 15500: Loss = -11271.859105731626
2
Iteration 15600: Loss = -11271.858299480831
Iteration 15700: Loss = -11271.91117542427
1
Iteration 15800: Loss = -11271.858272547968
Iteration 15900: Loss = -11271.861066741574
1
Iteration 16000: Loss = -11271.858619378128
2
Iteration 16100: Loss = -11271.858439316484
3
Iteration 16200: Loss = -11271.858645086164
4
Iteration 16300: Loss = -11271.868791888035
5
Iteration 16400: Loss = -11271.865684725275
6
Iteration 16500: Loss = -11271.858378506604
7
Iteration 16600: Loss = -11271.858311430136
Iteration 16700: Loss = -11271.877433962352
1
Iteration 16800: Loss = -11271.85824758633
Iteration 16900: Loss = -11271.866359927964
1
Iteration 17000: Loss = -11271.858243563802
Iteration 17100: Loss = -11271.86678941413
1
Iteration 17200: Loss = -11271.869771395855
2
Iteration 17300: Loss = -11271.858368509227
3
Iteration 17400: Loss = -11271.85865780169
4
Iteration 17500: Loss = -11271.876012886825
5
Iteration 17600: Loss = -11271.898893767673
6
Iteration 17700: Loss = -11271.865837864141
7
Iteration 17800: Loss = -11271.858240295105
Iteration 17900: Loss = -11271.86121839864
1
Iteration 18000: Loss = -11271.858252088798
Iteration 18100: Loss = -11271.858328304337
Iteration 18200: Loss = -11271.864520647521
1
Iteration 18300: Loss = -11271.858311368807
Iteration 18400: Loss = -11271.86033411588
1
Iteration 18500: Loss = -11271.860449264024
2
Iteration 18600: Loss = -11271.880506670088
3
Iteration 18700: Loss = -11271.858314379666
Iteration 18800: Loss = -11271.859053588068
1
Iteration 18900: Loss = -11271.85960613497
2
Iteration 19000: Loss = -11271.866679368566
3
Iteration 19100: Loss = -11271.8650537774
4
Iteration 19200: Loss = -11271.86139165231
5
Iteration 19300: Loss = -11271.858214148673
Iteration 19400: Loss = -11271.858750819774
1
Iteration 19500: Loss = -11271.879142573109
2
Iteration 19600: Loss = -11271.862502367445
3
Iteration 19700: Loss = -11271.859117998765
4
Iteration 19800: Loss = -11271.859704159733
5
Iteration 19900: Loss = -11271.887887886785
6
pi: tensor([[0.7988, 0.2012],
        [0.2136, 0.7864]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5546, 0.4454], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2988, 0.0974],
         [0.6287, 0.2019]],

        [[0.5608, 0.0918],
         [0.5115, 0.6264]],

        [[0.5669, 0.1062],
         [0.5828, 0.6340]],

        [[0.5976, 0.0962],
         [0.6669, 0.6725]],

        [[0.6784, 0.1054],
         [0.5596, 0.7121]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.968191341028667
Average Adjusted Rand Index: 0.9681593473659609
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23914.146855613995
Iteration 100: Loss = -11641.821242129603
Iteration 200: Loss = -11636.993764878049
Iteration 300: Loss = -11627.902661689724
Iteration 400: Loss = -11561.239478480498
Iteration 500: Loss = -11453.424400865853
Iteration 600: Loss = -11284.113533615064
Iteration 700: Loss = -11273.737917434508
Iteration 800: Loss = -11273.283199507843
Iteration 900: Loss = -11272.975841978601
Iteration 1000: Loss = -11272.594585772644
Iteration 1100: Loss = -11272.50913442929
Iteration 1200: Loss = -11272.45476618761
Iteration 1300: Loss = -11272.413527778066
Iteration 1400: Loss = -11272.37872503391
Iteration 1500: Loss = -11272.347038225907
Iteration 1600: Loss = -11272.310267592373
Iteration 1700: Loss = -11272.212810618863
Iteration 1800: Loss = -11272.196241881686
Iteration 1900: Loss = -11272.182791898002
Iteration 2000: Loss = -11272.16816438346
Iteration 2100: Loss = -11272.135942705057
Iteration 2200: Loss = -11272.120936116477
Iteration 2300: Loss = -11272.1137280307
Iteration 2400: Loss = -11272.107251805352
Iteration 2500: Loss = -11272.100679572346
Iteration 2600: Loss = -11272.093383780684
Iteration 2700: Loss = -11272.087024513965
Iteration 2800: Loss = -11272.043640652806
Iteration 2900: Loss = -11272.037417839612
Iteration 3000: Loss = -11272.034440413432
Iteration 3100: Loss = -11272.032022722005
Iteration 3200: Loss = -11272.029887241604
Iteration 3300: Loss = -11272.02794623853
Iteration 3400: Loss = -11272.026875525453
Iteration 3500: Loss = -11272.025124951284
Iteration 3600: Loss = -11272.023209902753
Iteration 3700: Loss = -11272.022120362702
Iteration 3800: Loss = -11272.020714414515
Iteration 3900: Loss = -11272.01981295401
Iteration 4000: Loss = -11272.018633942947
Iteration 4100: Loss = -11272.022331236518
1
Iteration 4200: Loss = -11272.01689808801
Iteration 4300: Loss = -11272.015953096365
Iteration 4400: Loss = -11272.015974737726
Iteration 4500: Loss = -11272.014431294294
Iteration 4600: Loss = -11272.013600270575
Iteration 4700: Loss = -11272.012695660922
Iteration 4800: Loss = -11272.012524778613
Iteration 4900: Loss = -11272.009712361492
Iteration 5000: Loss = -11272.009216247161
Iteration 5100: Loss = -11272.00883493022
Iteration 5200: Loss = -11272.008513041088
Iteration 5300: Loss = -11272.0165454653
1
Iteration 5400: Loss = -11272.007632049656
Iteration 5500: Loss = -11272.006932531966
Iteration 5600: Loss = -11272.006554419842
Iteration 5700: Loss = -11272.005785954785
Iteration 5800: Loss = -11272.004131248348
Iteration 5900: Loss = -11272.01018506168
1
Iteration 6000: Loss = -11271.999427180763
Iteration 6100: Loss = -11271.9987948909
Iteration 6200: Loss = -11271.999379501456
1
Iteration 6300: Loss = -11271.997548678048
Iteration 6400: Loss = -11271.995259322155
Iteration 6500: Loss = -11271.996182901703
1
Iteration 6600: Loss = -11271.993176877932
Iteration 6700: Loss = -11271.992961751397
Iteration 6800: Loss = -11271.992850912387
Iteration 6900: Loss = -11271.992441194929
Iteration 7000: Loss = -11271.998360318945
1
Iteration 7100: Loss = -11271.994616558573
2
Iteration 7200: Loss = -11271.991882551676
Iteration 7300: Loss = -11271.991718515228
Iteration 7400: Loss = -11271.998578572635
1
Iteration 7500: Loss = -11271.991480847766
Iteration 7600: Loss = -11271.991397148044
Iteration 7700: Loss = -11272.012586173169
1
Iteration 7800: Loss = -11271.99109577575
Iteration 7900: Loss = -11271.999914022606
1
Iteration 8000: Loss = -11272.070541345525
2
Iteration 8100: Loss = -11271.991158039746
Iteration 8200: Loss = -11271.990059151518
Iteration 8300: Loss = -11271.990238164482
1
Iteration 8400: Loss = -11271.989356260587
Iteration 8500: Loss = -11271.988999392406
Iteration 8600: Loss = -11271.992874066647
1
Iteration 8700: Loss = -11271.988701317972
Iteration 8800: Loss = -11271.988796575943
Iteration 8900: Loss = -11271.988874468097
Iteration 9000: Loss = -11271.989672600801
1
Iteration 9100: Loss = -11271.987458582256
Iteration 9200: Loss = -11271.992797655908
1
Iteration 9300: Loss = -11271.996460131317
2
Iteration 9400: Loss = -11271.98726292479
Iteration 9500: Loss = -11271.988331163173
1
Iteration 9600: Loss = -11271.986876044402
Iteration 9700: Loss = -11271.98300864229
Iteration 9800: Loss = -11271.942729321581
Iteration 9900: Loss = -11271.943272673892
1
Iteration 10000: Loss = -11271.94279373489
Iteration 10100: Loss = -11271.942785102052
Iteration 10200: Loss = -11271.9465512415
1
Iteration 10300: Loss = -11271.946482334859
2
Iteration 10400: Loss = -11271.944771424141
3
Iteration 10500: Loss = -11271.95627751735
4
Iteration 10600: Loss = -11272.16174102482
5
Iteration 10700: Loss = -11271.942377254813
Iteration 10800: Loss = -11271.946639232085
1
Iteration 10900: Loss = -11271.952729605382
2
Iteration 11000: Loss = -11271.942059625455
Iteration 11100: Loss = -11271.94180646816
Iteration 11200: Loss = -11271.954804781923
1
Iteration 11300: Loss = -11271.944120143235
2
Iteration 11400: Loss = -11271.942199537822
3
Iteration 11500: Loss = -11271.94969540318
4
Iteration 11600: Loss = -11271.946578131316
5
Iteration 11700: Loss = -11271.94159163612
Iteration 11800: Loss = -11271.935479777361
Iteration 11900: Loss = -11271.934199956837
Iteration 12000: Loss = -11271.935061436845
1
Iteration 12100: Loss = -11271.957098486617
2
Iteration 12200: Loss = -11271.934894475182
3
Iteration 12300: Loss = -11271.935173918795
4
Iteration 12400: Loss = -11271.937149142621
5
Iteration 12500: Loss = -11271.96205936196
6
Iteration 12600: Loss = -11271.932984263136
Iteration 12700: Loss = -11271.933126274225
1
Iteration 12800: Loss = -11272.127936642775
2
Iteration 12900: Loss = -11271.937517428833
3
Iteration 13000: Loss = -11271.933954353957
4
Iteration 13100: Loss = -11271.959591781131
5
Iteration 13200: Loss = -11271.937285463173
6
Iteration 13300: Loss = -11271.935671567948
7
Iteration 13400: Loss = -11271.97540973787
8
Iteration 13500: Loss = -11271.96563112427
9
Iteration 13600: Loss = -11271.932946920286
Iteration 13700: Loss = -11271.933120496988
1
Iteration 13800: Loss = -11271.933125513382
2
Iteration 13900: Loss = -11271.932827092991
Iteration 14000: Loss = -11271.933904571004
1
Iteration 14100: Loss = -11271.958046398122
2
Iteration 14200: Loss = -11271.95385449679
3
Iteration 14300: Loss = -11271.933451558314
4
Iteration 14400: Loss = -11271.931353316992
Iteration 14500: Loss = -11271.95322227044
1
Iteration 14600: Loss = -11271.943613801626
2
Iteration 14700: Loss = -11271.92559474724
Iteration 14800: Loss = -11271.95978007822
1
Iteration 14900: Loss = -11271.926450598483
2
Iteration 15000: Loss = -11271.925419736748
Iteration 15100: Loss = -11271.95828137415
1
Iteration 15200: Loss = -11271.92354756104
Iteration 15300: Loss = -11271.925154720317
1
Iteration 15400: Loss = -11271.93874969575
2
Iteration 15500: Loss = -11271.92369621861
3
Iteration 15600: Loss = -11271.923436712052
Iteration 15700: Loss = -11271.954803061768
1
Iteration 15800: Loss = -11271.927592872991
2
Iteration 15900: Loss = -11271.925496056361
3
Iteration 16000: Loss = -11271.937148025057
4
Iteration 16100: Loss = -11271.92303396306
Iteration 16200: Loss = -11271.926008063649
1
Iteration 16300: Loss = -11271.932253176346
2
Iteration 16400: Loss = -11271.92919440876
3
Iteration 16500: Loss = -11271.922770699772
Iteration 16600: Loss = -11271.922957485955
1
Iteration 16700: Loss = -11271.930665997936
2
Iteration 16800: Loss = -11271.92317746625
3
Iteration 16900: Loss = -11271.927798058687
4
Iteration 17000: Loss = -11271.932214553974
5
Iteration 17100: Loss = -11271.922498625638
Iteration 17200: Loss = -11271.922419434988
Iteration 17300: Loss = -11271.945567031928
1
Iteration 17400: Loss = -11271.92230525128
Iteration 17500: Loss = -11271.925201415212
1
Iteration 17600: Loss = -11271.943510851706
2
Iteration 17700: Loss = -11271.9220670441
Iteration 17800: Loss = -11271.921467507182
Iteration 17900: Loss = -11271.922911337158
1
Iteration 18000: Loss = -11271.924268101715
2
Iteration 18100: Loss = -11271.955380451425
3
Iteration 18200: Loss = -11271.921534087534
Iteration 18300: Loss = -11271.928665066214
1
Iteration 18400: Loss = -11272.054674880334
2
Iteration 18500: Loss = -11271.921589398795
Iteration 18600: Loss = -11271.939573010979
1
Iteration 18700: Loss = -11271.921079595042
Iteration 18800: Loss = -11271.922278706286
1
Iteration 18900: Loss = -11271.92323029908
2
Iteration 19000: Loss = -11271.892101210506
Iteration 19100: Loss = -11271.874503137646
Iteration 19200: Loss = -11271.858964728988
Iteration 19300: Loss = -11271.863554159494
1
Iteration 19400: Loss = -11271.864183678501
2
Iteration 19500: Loss = -11271.860758049666
3
Iteration 19600: Loss = -11271.858737982922
Iteration 19700: Loss = -11271.863025346269
1
Iteration 19800: Loss = -11271.90877176193
2
Iteration 19900: Loss = -11271.864187336496
3
pi: tensor([[0.7988, 0.2012],
        [0.2139, 0.7861]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5544, 0.4456], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2988, 0.0974],
         [0.7126, 0.2019]],

        [[0.6989, 0.0918],
         [0.5993, 0.6213]],

        [[0.6157, 0.1062],
         [0.5307, 0.5959]],

        [[0.6765, 0.0962],
         [0.6365, 0.5575]],

        [[0.5997, 0.1053],
         [0.7048, 0.5017]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.968191341028667
Average Adjusted Rand Index: 0.9681593473659609
11289.108458760997
[0.968191341028667, 0.968191341028667] [0.9681593473659609, 0.9681593473659609] [11271.858102641147, 11271.85875512385]
-------------------------------------
This iteration is 36
True Objective function: Loss = -11200.480323835178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24324.263231126813
Iteration 100: Loss = -11524.407711217485
Iteration 200: Loss = -11520.118573435413
Iteration 300: Loss = -11513.639415115815
Iteration 400: Loss = -11459.376265831785
Iteration 500: Loss = -11439.421691992198
Iteration 600: Loss = -11364.842028663927
Iteration 700: Loss = -11268.287983867931
Iteration 800: Loss = -11260.703698275694
Iteration 900: Loss = -11255.116090090805
Iteration 1000: Loss = -11253.635034698704
Iteration 1100: Loss = -11253.355953733397
Iteration 1200: Loss = -11250.964236885324
Iteration 1300: Loss = -11242.149151806378
Iteration 1400: Loss = -11241.972169578125
Iteration 1500: Loss = -11241.245032229373
Iteration 1600: Loss = -11240.233928297315
Iteration 1700: Loss = -11240.220045309192
Iteration 1800: Loss = -11240.12115898891
Iteration 1900: Loss = -11239.392377445914
Iteration 2000: Loss = -11239.386071134366
Iteration 2100: Loss = -11239.381184236401
Iteration 2200: Loss = -11239.374967238287
Iteration 2300: Loss = -11239.327468340925
Iteration 2400: Loss = -11239.270078475402
Iteration 2500: Loss = -11239.24974288948
Iteration 2600: Loss = -11239.244761518568
Iteration 2700: Loss = -11239.232975938003
Iteration 2800: Loss = -11239.190031834283
Iteration 2900: Loss = -11239.181907747385
Iteration 3000: Loss = -11239.179846793402
Iteration 3100: Loss = -11239.178544784549
Iteration 3200: Loss = -11239.17471390371
Iteration 3300: Loss = -11239.171228206276
Iteration 3400: Loss = -11239.168565658847
Iteration 3500: Loss = -11239.167291565429
Iteration 3600: Loss = -11239.166487719483
Iteration 3700: Loss = -11239.165258718598
Iteration 3800: Loss = -11239.163959134592
Iteration 3900: Loss = -11239.155614026296
Iteration 4000: Loss = -11239.049060438383
Iteration 4100: Loss = -11239.04834119006
Iteration 4200: Loss = -11239.047797389627
Iteration 4300: Loss = -11239.04683068203
Iteration 4400: Loss = -11239.037471090938
Iteration 4500: Loss = -11239.036699519414
Iteration 4600: Loss = -11239.037222253819
1
Iteration 4700: Loss = -11239.033089508173
Iteration 4800: Loss = -11239.032389418906
Iteration 4900: Loss = -11239.031442587322
Iteration 5000: Loss = -11239.030009969007
Iteration 5100: Loss = -11239.02854338836
Iteration 5200: Loss = -11239.032169841985
1
Iteration 5300: Loss = -11239.037327861572
2
Iteration 5400: Loss = -11239.030858082924
3
Iteration 5500: Loss = -11239.026661340744
Iteration 5600: Loss = -11239.027194113343
1
Iteration 5700: Loss = -11239.02535906651
Iteration 5800: Loss = -11239.02595682149
1
Iteration 5900: Loss = -11239.02499958742
Iteration 6000: Loss = -11239.024930799726
Iteration 6100: Loss = -11239.02564856601
1
Iteration 6200: Loss = -11239.025192492074
2
Iteration 6300: Loss = -11239.024424605544
Iteration 6400: Loss = -11239.025206930666
1
Iteration 6500: Loss = -11239.024182696272
Iteration 6600: Loss = -11239.024404712745
1
Iteration 6700: Loss = -11239.024009886696
Iteration 6800: Loss = -11239.024032859266
Iteration 6900: Loss = -11239.023837760022
Iteration 7000: Loss = -11239.023789380053
Iteration 7100: Loss = -11239.023730487137
Iteration 7200: Loss = -11239.023847602453
1
Iteration 7300: Loss = -11239.023651620766
Iteration 7400: Loss = -11239.034247376436
1
Iteration 7500: Loss = -11239.023535552964
Iteration 7600: Loss = -11239.023569808283
Iteration 7700: Loss = -11239.023461173847
Iteration 7800: Loss = -11239.02339666368
Iteration 7900: Loss = -11239.023294906572
Iteration 8000: Loss = -11239.022988953178
Iteration 8100: Loss = -11239.023174807257
1
Iteration 8200: Loss = -11239.022513163249
Iteration 8300: Loss = -11239.032086495366
1
Iteration 8400: Loss = -11239.022439812135
Iteration 8500: Loss = -11239.022419016048
Iteration 8600: Loss = -11239.026085570846
1
Iteration 8700: Loss = -11239.016768077938
Iteration 8800: Loss = -11239.016782635841
Iteration 8900: Loss = -11239.016882658198
1
Iteration 9000: Loss = -11239.016482034853
Iteration 9100: Loss = -11239.009526223586
Iteration 9200: Loss = -11239.00091219963
Iteration 9300: Loss = -11239.0002976334
Iteration 9400: Loss = -11239.000226260858
Iteration 9500: Loss = -11239.004803050642
1
Iteration 9600: Loss = -11239.000190278726
Iteration 9700: Loss = -11239.01765632131
1
Iteration 9800: Loss = -11239.008391549954
2
Iteration 9900: Loss = -11239.002946537115
3
Iteration 10000: Loss = -11238.999877500984
Iteration 10100: Loss = -11239.011640551296
1
Iteration 10200: Loss = -11238.995515935436
Iteration 10300: Loss = -11239.004934507378
1
Iteration 10400: Loss = -11239.010675068937
2
Iteration 10500: Loss = -11239.114757060897
3
Iteration 10600: Loss = -11238.996266727052
4
Iteration 10700: Loss = -11238.995484386702
Iteration 10800: Loss = -11239.028863806665
1
Iteration 10900: Loss = -11238.975944999258
Iteration 11000: Loss = -11239.056104044857
1
Iteration 11100: Loss = -11238.975888342526
Iteration 11200: Loss = -11238.990098173643
1
Iteration 11300: Loss = -11238.980829029066
2
Iteration 11400: Loss = -11238.97702889708
3
Iteration 11500: Loss = -11238.992033421007
4
Iteration 11600: Loss = -11238.979095588253
5
Iteration 11700: Loss = -11238.975720168135
Iteration 11800: Loss = -11238.977296445935
1
Iteration 11900: Loss = -11239.049652292968
2
Iteration 12000: Loss = -11238.975681670756
Iteration 12100: Loss = -11239.009533713906
1
Iteration 12200: Loss = -11238.975687291308
Iteration 12300: Loss = -11238.992317428601
1
Iteration 12400: Loss = -11238.994114141708
2
Iteration 12500: Loss = -11238.976253033756
3
Iteration 12600: Loss = -11238.989639671187
4
Iteration 12700: Loss = -11238.975842866917
5
Iteration 12800: Loss = -11238.978385317307
6
Iteration 12900: Loss = -11238.978675809167
7
Iteration 13000: Loss = -11239.265967916095
8
Iteration 13100: Loss = -11238.975194425284
Iteration 13200: Loss = -11238.978456854102
1
Iteration 13300: Loss = -11238.974779912156
Iteration 13400: Loss = -11239.007884172177
1
Iteration 13500: Loss = -11238.98380408447
2
Iteration 13600: Loss = -11238.975660998662
3
Iteration 13700: Loss = -11238.978146935031
4
Iteration 13800: Loss = -11238.97470349155
Iteration 13900: Loss = -11238.994048866887
1
Iteration 14000: Loss = -11238.98035936721
2
Iteration 14100: Loss = -11238.974691269994
Iteration 14200: Loss = -11238.977085659277
1
Iteration 14300: Loss = -11238.986624680605
2
Iteration 14400: Loss = -11238.974856419625
3
Iteration 14500: Loss = -11238.975346973668
4
Iteration 14600: Loss = -11238.974991546042
5
Iteration 14700: Loss = -11238.976245891263
6
Iteration 14800: Loss = -11238.987629988964
7
Iteration 14900: Loss = -11238.974732564348
Iteration 15000: Loss = -11238.999481379726
1
Iteration 15100: Loss = -11239.043957371408
2
Iteration 15200: Loss = -11238.975209435619
3
Iteration 15300: Loss = -11238.97472306404
Iteration 15400: Loss = -11239.002164655025
1
Iteration 15500: Loss = -11239.01032687088
2
Iteration 15600: Loss = -11238.974337291562
Iteration 15700: Loss = -11238.974239363537
Iteration 15800: Loss = -11239.015179307116
1
Iteration 15900: Loss = -11238.974149996315
Iteration 16000: Loss = -11238.990150978543
1
Iteration 16100: Loss = -11238.97422141981
Iteration 16200: Loss = -11238.97425254242
Iteration 16300: Loss = -11238.974539450131
1
Iteration 16400: Loss = -11238.974568878155
2
Iteration 16500: Loss = -11239.104152066235
3
Iteration 16600: Loss = -11238.974215816927
Iteration 16700: Loss = -11238.974150080143
Iteration 16800: Loss = -11239.026726745618
1
Iteration 16900: Loss = -11238.973775404827
Iteration 17000: Loss = -11238.988918879626
1
Iteration 17100: Loss = -11238.973783923195
Iteration 17200: Loss = -11238.9852144533
1
Iteration 17300: Loss = -11238.956108112976
Iteration 17400: Loss = -11238.908309172648
Iteration 17500: Loss = -11238.907502462123
Iteration 17600: Loss = -11239.033484913198
1
Iteration 17700: Loss = -11238.906657332514
Iteration 17800: Loss = -11238.901164259629
Iteration 17900: Loss = -11238.901548976
1
Iteration 18000: Loss = -11238.910776632863
2
Iteration 18100: Loss = -11238.910176157477
3
Iteration 18200: Loss = -11238.901539275057
4
Iteration 18300: Loss = -11238.903044394201
5
Iteration 18400: Loss = -11238.922110226431
6
Iteration 18500: Loss = -11238.90635195386
7
Iteration 18600: Loss = -11238.929296883247
8
Iteration 18700: Loss = -11238.911717205277
9
Iteration 18800: Loss = -11238.901485228002
10
Iteration 18900: Loss = -11238.901248315951
Iteration 19000: Loss = -11238.91111867084
1
Iteration 19100: Loss = -11238.90082500467
Iteration 19200: Loss = -11238.90953772537
1
Iteration 19300: Loss = -11239.10942036799
2
Iteration 19400: Loss = -11238.900856193304
Iteration 19500: Loss = -11238.901343709611
1
Iteration 19600: Loss = -11238.913156173247
2
Iteration 19700: Loss = -11238.9008234151
Iteration 19800: Loss = -11238.90114600252
1
Iteration 19900: Loss = -11238.900888517168
pi: tensor([[0.2552, 0.7448],
        [0.7909, 0.2091]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4109, 0.5891], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2468, 0.1007],
         [0.5799, 0.2609]],

        [[0.6333, 0.0909],
         [0.6367, 0.6266]],

        [[0.6352, 0.0959],
         [0.6072, 0.5082]],

        [[0.6433, 0.0960],
         [0.6116, 0.7235]],

        [[0.7021, 0.0999],
         [0.5739, 0.6860]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.737020183942648
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
Global Adjusted Rand Index: 0.041349748735303886
Average Adjusted Rand Index: 0.8632826366004538
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23373.022877945106
Iteration 100: Loss = -11525.2710167755
Iteration 200: Loss = -11523.967490393601
Iteration 300: Loss = -11521.20083614677
Iteration 400: Loss = -11509.740342577381
Iteration 500: Loss = -11456.083841077165
Iteration 600: Loss = -11285.007871212698
Iteration 700: Loss = -11226.757406415454
Iteration 800: Loss = -11206.52895220879
Iteration 900: Loss = -11197.204921152083
Iteration 1000: Loss = -11186.925064594501
Iteration 1100: Loss = -11185.528785875904
Iteration 1200: Loss = -11180.426713769732
Iteration 1300: Loss = -11180.306351419144
Iteration 1400: Loss = -11180.223642848216
Iteration 1500: Loss = -11180.157137898746
Iteration 1600: Loss = -11180.107915020424
Iteration 1700: Loss = -11180.072204194263
Iteration 1800: Loss = -11180.042821228737
Iteration 1900: Loss = -11180.016189274715
Iteration 2000: Loss = -11179.984036404774
Iteration 2100: Loss = -11179.7445385507
Iteration 2200: Loss = -11179.659808412818
Iteration 2300: Loss = -11179.643054936896
Iteration 2400: Loss = -11179.627145065428
Iteration 2500: Loss = -11179.607647751358
Iteration 2600: Loss = -11179.584542742492
Iteration 2700: Loss = -11179.558431478907
Iteration 2800: Loss = -11179.509743868537
Iteration 2900: Loss = -11179.321895447436
Iteration 3000: Loss = -11179.300607268788
Iteration 3100: Loss = -11179.29419234791
Iteration 3200: Loss = -11179.289464234804
Iteration 3300: Loss = -11179.284605344797
Iteration 3400: Loss = -11179.286383216753
1
Iteration 3500: Loss = -11179.276009487026
Iteration 3600: Loss = -11179.269716947343
Iteration 3700: Loss = -11179.195241707484
Iteration 3800: Loss = -11178.56133947748
Iteration 3900: Loss = -11178.557827111772
Iteration 4000: Loss = -11178.555687296817
Iteration 4100: Loss = -11178.559454237185
1
Iteration 4200: Loss = -11178.551699955246
Iteration 4300: Loss = -11178.54998554894
Iteration 4400: Loss = -11178.548382743242
Iteration 4500: Loss = -11178.54802014977
Iteration 4600: Loss = -11178.54207271521
Iteration 4700: Loss = -11178.53614983685
Iteration 4800: Loss = -11178.53585631182
Iteration 4900: Loss = -11178.534484832662
Iteration 5000: Loss = -11178.531067600732
Iteration 5100: Loss = -11178.52795251665
Iteration 5200: Loss = -11178.527906920779
Iteration 5300: Loss = -11178.526339402484
Iteration 5400: Loss = -11178.5260691126
Iteration 5500: Loss = -11178.523931361327
Iteration 5600: Loss = -11178.522833645118
Iteration 5700: Loss = -11178.52119190006
Iteration 5800: Loss = -11178.522044691113
1
Iteration 5900: Loss = -11178.523635256786
2
Iteration 6000: Loss = -11178.519426932453
Iteration 6100: Loss = -11178.51864426785
Iteration 6200: Loss = -11178.518604074336
Iteration 6300: Loss = -11178.516183488337
Iteration 6400: Loss = -11178.521692723165
1
Iteration 6500: Loss = -11178.508925052416
Iteration 6600: Loss = -11178.507353056439
Iteration 6700: Loss = -11178.506948757084
Iteration 6800: Loss = -11178.506304085404
Iteration 6900: Loss = -11178.507147050623
1
Iteration 7000: Loss = -11178.50556991224
Iteration 7100: Loss = -11178.505402807808
Iteration 7200: Loss = -11178.505426302132
Iteration 7300: Loss = -11178.505173162235
Iteration 7400: Loss = -11178.504895036958
Iteration 7500: Loss = -11178.506807829792
1
Iteration 7600: Loss = -11178.504968475907
Iteration 7700: Loss = -11178.504197205068
Iteration 7800: Loss = -11178.504562296272
1
Iteration 7900: Loss = -11178.505053795907
2
Iteration 8000: Loss = -11178.512604473222
3
Iteration 8100: Loss = -11178.503358781712
Iteration 8200: Loss = -11178.503232494584
Iteration 8300: Loss = -11178.503021918463
Iteration 8400: Loss = -11178.502912713378
Iteration 8500: Loss = -11178.502777443417
Iteration 8600: Loss = -11178.542363764318
1
Iteration 8700: Loss = -11178.502635522147
Iteration 8800: Loss = -11178.502503387075
Iteration 8900: Loss = -11178.522577996695
1
Iteration 9000: Loss = -11178.502335137604
Iteration 9100: Loss = -11178.582849562577
1
Iteration 9200: Loss = -11178.502222526504
Iteration 9300: Loss = -11178.502035826275
Iteration 9400: Loss = -11178.503094466812
1
Iteration 9500: Loss = -11178.501874737874
Iteration 9600: Loss = -11178.501932017796
Iteration 9700: Loss = -11178.501714236481
Iteration 9800: Loss = -11178.501667473995
Iteration 9900: Loss = -11178.508980543651
1
Iteration 10000: Loss = -11178.501500571272
Iteration 10100: Loss = -11178.501436696666
Iteration 10200: Loss = -11178.502603891446
1
Iteration 10300: Loss = -11178.496711439733
Iteration 10400: Loss = -11178.49802481623
1
Iteration 10500: Loss = -11178.49660532965
Iteration 10600: Loss = -11178.573877751993
1
Iteration 10700: Loss = -11178.496442442076
Iteration 10800: Loss = -11178.496246370738
Iteration 10900: Loss = -11178.496433243434
1
Iteration 11000: Loss = -11178.496032604166
Iteration 11100: Loss = -11178.496147491127
1
Iteration 11200: Loss = -11178.49595828627
Iteration 11300: Loss = -11178.49567782988
Iteration 11400: Loss = -11178.708496254652
1
Iteration 11500: Loss = -11178.495317223575
Iteration 11600: Loss = -11178.49822025987
1
Iteration 11700: Loss = -11178.509298683734
2
Iteration 11800: Loss = -11178.496055977359
3
Iteration 11900: Loss = -11178.49537408058
Iteration 12000: Loss = -11178.495955082768
1
Iteration 12100: Loss = -11178.495233841364
Iteration 12200: Loss = -11178.496290960056
1
Iteration 12300: Loss = -11178.495203318096
Iteration 12400: Loss = -11178.495323798303
1
Iteration 12500: Loss = -11178.504679980933
2
Iteration 12600: Loss = -11178.514802572114
3
Iteration 12700: Loss = -11178.495181195787
Iteration 12800: Loss = -11178.495042899294
Iteration 12900: Loss = -11178.509857546187
1
Iteration 13000: Loss = -11178.494905237687
Iteration 13100: Loss = -11178.498498779458
1
Iteration 13200: Loss = -11178.494894644653
Iteration 13300: Loss = -11178.648863604754
1
Iteration 13400: Loss = -11178.494885269642
Iteration 13500: Loss = -11178.494874863361
Iteration 13600: Loss = -11178.529853058859
1
Iteration 13700: Loss = -11178.49490797312
Iteration 13800: Loss = -11178.494864937244
Iteration 13900: Loss = -11178.507717380451
1
Iteration 14000: Loss = -11178.49487123052
Iteration 14100: Loss = -11178.494860480836
Iteration 14200: Loss = -11178.747531436213
1
Iteration 14300: Loss = -11178.49480068495
Iteration 14400: Loss = -11178.493199711982
Iteration 14500: Loss = -11178.523570106892
1
Iteration 14600: Loss = -11178.493171796603
Iteration 14700: Loss = -11178.493462373282
1
Iteration 14800: Loss = -11178.493963101891
2
Iteration 14900: Loss = -11178.49347398001
3
Iteration 15000: Loss = -11178.494211253923
4
Iteration 15100: Loss = -11178.496840451455
5
Iteration 15200: Loss = -11178.49324200345
Iteration 15300: Loss = -11178.606719453603
1
Iteration 15400: Loss = -11178.493132292693
Iteration 15500: Loss = -11178.515729775381
1
Iteration 15600: Loss = -11178.493147392846
Iteration 15700: Loss = -11178.493093549376
Iteration 15800: Loss = -11178.493343998578
1
Iteration 15900: Loss = -11178.493107193171
Iteration 16000: Loss = -11178.494549923651
1
Iteration 16100: Loss = -11178.49375686476
2
Iteration 16200: Loss = -11178.493067467438
Iteration 16300: Loss = -11178.490990347076
Iteration 16400: Loss = -11178.491129895843
1
Iteration 16500: Loss = -11178.491021227746
Iteration 16600: Loss = -11178.62541084287
1
Iteration 16700: Loss = -11178.491005973208
Iteration 16800: Loss = -11178.491016205131
Iteration 16900: Loss = -11178.763905629137
1
Iteration 17000: Loss = -11178.491002864817
Iteration 17100: Loss = -11178.491007255841
Iteration 17200: Loss = -11178.519461833916
1
Iteration 17300: Loss = -11178.491030414913
Iteration 17400: Loss = -11178.491015937803
Iteration 17500: Loss = -11178.617132806856
1
Iteration 17600: Loss = -11178.490993568652
Iteration 17700: Loss = -11178.49098706374
Iteration 17800: Loss = -11178.495596496108
1
Iteration 17900: Loss = -11178.490987374045
Iteration 18000: Loss = -11178.539306822306
1
Iteration 18100: Loss = -11178.490970881629
Iteration 18200: Loss = -11178.490987724916
Iteration 18300: Loss = -11178.491015199217
Iteration 18400: Loss = -11178.546338402994
1
Iteration 18500: Loss = -11178.532165792987
2
Iteration 18600: Loss = -11178.491058672427
Iteration 18700: Loss = -11178.490979042985
Iteration 18800: Loss = -11178.495090091386
1
Iteration 18900: Loss = -11178.49097671206
Iteration 19000: Loss = -11178.492848855445
1
Iteration 19100: Loss = -11178.49097283093
Iteration 19200: Loss = -11178.492040890525
1
Iteration 19300: Loss = -11178.49098445895
Iteration 19400: Loss = -11178.600098292638
1
Iteration 19500: Loss = -11178.490977724168
Iteration 19600: Loss = -11178.490982043017
Iteration 19700: Loss = -11178.491437477645
1
Iteration 19800: Loss = -11178.490971927857
Iteration 19900: Loss = -11178.490969136794
pi: tensor([[0.7491, 0.2509],
        [0.2406, 0.7594]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4450, 0.5550], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1967, 0.1031],
         [0.5778, 0.2995]],

        [[0.6530, 0.0927],
         [0.5078, 0.5368]],

        [[0.6786, 0.0973],
         [0.5687, 0.6723]],

        [[0.5194, 0.0979],
         [0.6497, 0.6645]],

        [[0.7220, 0.1029],
         [0.5310, 0.5082]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9446731604048887
Average Adjusted Rand Index: 0.9443209008582741
11200.480323835178
[0.041349748735303886, 0.9446731604048887] [0.8632826366004538, 0.9443209008582741] [11238.958824895604, 11178.495099403055]
-------------------------------------
This iteration is 37
True Objective function: Loss = -11214.76149173985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24259.04485263423
Iteration 100: Loss = -11274.406905080852
Iteration 200: Loss = -11203.193326208264
Iteration 300: Loss = -11202.60186900872
Iteration 400: Loss = -11202.401208898847
Iteration 500: Loss = -11202.314404660072
Iteration 600: Loss = -11202.26600636758
Iteration 700: Loss = -11202.233677056687
Iteration 800: Loss = -11202.209318851992
Iteration 900: Loss = -11202.194976292692
Iteration 1000: Loss = -11202.184491044538
Iteration 1100: Loss = -11202.175882293901
Iteration 1200: Loss = -11202.158592242862
Iteration 1300: Loss = -11202.145187918599
Iteration 1400: Loss = -11202.140952036832
Iteration 1500: Loss = -11202.136066603847
Iteration 1600: Loss = -11202.024139953797
Iteration 1700: Loss = -11202.021726727182
Iteration 1800: Loss = -11202.019708118707
Iteration 1900: Loss = -11202.018006421275
Iteration 2000: Loss = -11202.016438783841
Iteration 2100: Loss = -11202.014868385832
Iteration 2200: Loss = -11201.792541813262
Iteration 2300: Loss = -11201.707663732128
Iteration 2400: Loss = -11201.706754581288
Iteration 2500: Loss = -11201.706828692972
Iteration 2600: Loss = -11201.707414637965
1
Iteration 2700: Loss = -11201.708867500309
2
Iteration 2800: Loss = -11201.704329746926
Iteration 2900: Loss = -11201.704479501106
1
Iteration 3000: Loss = -11201.708566430585
2
Iteration 3100: Loss = -11201.701914437062
Iteration 3200: Loss = -11201.700718383403
Iteration 3300: Loss = -11201.700572347441
Iteration 3400: Loss = -11201.70471682946
1
Iteration 3500: Loss = -11201.694557999486
Iteration 3600: Loss = -11201.696922255765
1
Iteration 3700: Loss = -11201.696808201033
2
Iteration 3800: Loss = -11201.695150148824
3
Iteration 3900: Loss = -11201.686236630518
Iteration 4000: Loss = -11201.685852607996
Iteration 4100: Loss = -11201.685705307444
Iteration 4200: Loss = -11201.686397133868
1
Iteration 4300: Loss = -11201.686419131867
2
Iteration 4400: Loss = -11201.691474277204
3
Iteration 4500: Loss = -11201.685133249508
Iteration 4600: Loss = -11201.685042086774
Iteration 4700: Loss = -11201.685223129216
1
Iteration 4800: Loss = -11201.6856686094
2
Iteration 4900: Loss = -11201.684940984998
Iteration 5000: Loss = -11201.685275019721
1
Iteration 5100: Loss = -11201.694689692345
2
Iteration 5200: Loss = -11201.684487493107
Iteration 5300: Loss = -11201.684470923852
Iteration 5400: Loss = -11201.6855866065
1
Iteration 5500: Loss = -11201.684352850325
Iteration 5600: Loss = -11201.684251333258
Iteration 5700: Loss = -11201.684316291748
Iteration 5800: Loss = -11201.68523168774
1
Iteration 5900: Loss = -11201.68417744873
Iteration 6000: Loss = -11201.688262063584
1
Iteration 6100: Loss = -11201.690612415654
2
Iteration 6200: Loss = -11201.68754250655
3
Iteration 6300: Loss = -11201.698620287749
4
Iteration 6400: Loss = -11201.683181371282
Iteration 6500: Loss = -11201.683092234765
Iteration 6600: Loss = -11201.683443412303
1
Iteration 6700: Loss = -11201.683044106989
Iteration 6800: Loss = -11201.682979575664
Iteration 6900: Loss = -11201.682942468517
Iteration 7000: Loss = -11201.682993167831
Iteration 7100: Loss = -11201.684405421634
1
Iteration 7200: Loss = -11201.682303974369
Iteration 7300: Loss = -11201.669088959365
Iteration 7400: Loss = -11201.66835300553
Iteration 7500: Loss = -11201.66738524847
Iteration 7600: Loss = -11201.667108023012
Iteration 7700: Loss = -11201.667093285247
Iteration 7800: Loss = -11201.667069162184
Iteration 7900: Loss = -11201.667764581238
1
Iteration 8000: Loss = -11201.666838676401
Iteration 8100: Loss = -11201.666890221903
Iteration 8200: Loss = -11201.66677817899
Iteration 8300: Loss = -11201.666822556854
Iteration 8400: Loss = -11201.667728415816
1
Iteration 8500: Loss = -11201.68409330899
2
Iteration 8600: Loss = -11201.666730052351
Iteration 8700: Loss = -11201.674309631468
1
Iteration 8800: Loss = -11201.666697810477
Iteration 8900: Loss = -11201.684353045495
1
Iteration 9000: Loss = -11201.666654444309
Iteration 9100: Loss = -11201.666664967379
Iteration 9200: Loss = -11201.66669438381
Iteration 9300: Loss = -11201.66664368446
Iteration 9400: Loss = -11201.67326279413
1
Iteration 9500: Loss = -11201.666614262958
Iteration 9600: Loss = -11201.6763993921
1
Iteration 9700: Loss = -11201.66661026916
Iteration 9800: Loss = -11201.668134946409
1
Iteration 9900: Loss = -11201.666589403168
Iteration 10000: Loss = -11201.668021302256
1
Iteration 10100: Loss = -11201.6666811301
Iteration 10200: Loss = -11201.666664010321
Iteration 10300: Loss = -11201.683278927829
1
Iteration 10400: Loss = -11201.666576247848
Iteration 10500: Loss = -11201.666809691807
1
Iteration 10600: Loss = -11201.666579305987
Iteration 10700: Loss = -11201.66668708306
1
Iteration 10800: Loss = -11201.756699135416
2
Iteration 10900: Loss = -11201.666704032146
3
Iteration 11000: Loss = -11201.668364913721
4
Iteration 11100: Loss = -11201.66793931171
5
Iteration 11200: Loss = -11201.66698572654
6
Iteration 11300: Loss = -11201.66634774695
Iteration 11400: Loss = -11201.67016109953
1
Iteration 11500: Loss = -11201.711959392316
2
Iteration 11600: Loss = -11201.667604025552
3
Iteration 11700: Loss = -11201.666351320893
Iteration 11800: Loss = -11201.66741560481
1
Iteration 11900: Loss = -11201.673539019786
2
Iteration 12000: Loss = -11201.665678919442
Iteration 12100: Loss = -11201.666423377175
1
Iteration 12200: Loss = -11201.665610666982
Iteration 12300: Loss = -11201.66604754665
1
Iteration 12400: Loss = -11201.665638513607
Iteration 12500: Loss = -11201.665778368795
1
Iteration 12600: Loss = -11201.748350463517
2
Iteration 12700: Loss = -11201.677643637277
3
Iteration 12800: Loss = -11201.66562928753
Iteration 12900: Loss = -11201.666000632764
1
Iteration 13000: Loss = -11201.679112066513
2
Iteration 13100: Loss = -11201.66561269954
Iteration 13200: Loss = -11201.947820063993
1
Iteration 13300: Loss = -11201.665638093853
Iteration 13400: Loss = -11201.66563753428
Iteration 13500: Loss = -11201.665694871464
Iteration 13600: Loss = -11201.942689336112
1
Iteration 13700: Loss = -11201.6656559731
Iteration 13800: Loss = -11201.670024904743
1
Iteration 13900: Loss = -11201.665672135141
Iteration 14000: Loss = -11201.665631858777
Iteration 14100: Loss = -11201.667546956738
1
Iteration 14200: Loss = -11201.665625330657
Iteration 14300: Loss = -11201.667547306371
1
Iteration 14400: Loss = -11201.665625935022
Iteration 14500: Loss = -11201.66558658097
Iteration 14600: Loss = -11201.664953051331
Iteration 14700: Loss = -11201.664972614131
Iteration 14800: Loss = -11201.664940624325
Iteration 14900: Loss = -11201.665201900767
1
Iteration 15000: Loss = -11201.664905493235
Iteration 15100: Loss = -11201.679902247635
1
Iteration 15200: Loss = -11201.664929313918
Iteration 15300: Loss = -11201.665484363813
1
Iteration 15400: Loss = -11201.664701695598
Iteration 15500: Loss = -11201.664841866146
1
Iteration 15600: Loss = -11201.682858499124
2
Iteration 15700: Loss = -11201.664721560694
Iteration 15800: Loss = -11201.665669004482
1
Iteration 15900: Loss = -11201.664701850226
Iteration 16000: Loss = -11201.664637144517
Iteration 16100: Loss = -11201.664849259518
1
Iteration 16200: Loss = -11201.665998120938
2
Iteration 16300: Loss = -11201.664769244666
3
Iteration 16400: Loss = -11201.6647417232
4
Iteration 16500: Loss = -11201.95613026873
5
Iteration 16600: Loss = -11201.664607643093
Iteration 16700: Loss = -11201.718437108364
1
Iteration 16800: Loss = -11201.66519268021
2
Iteration 16900: Loss = -11201.672699109302
3
Iteration 17000: Loss = -11201.664678585495
Iteration 17100: Loss = -11201.814724393942
1
Iteration 17200: Loss = -11201.664721354779
Iteration 17300: Loss = -11201.664969047393
1
Iteration 17400: Loss = -11201.735660928336
2
Iteration 17500: Loss = -11201.664600022339
Iteration 17600: Loss = -11201.666488494351
1
Iteration 17700: Loss = -11201.664633782457
Iteration 17800: Loss = -11201.683738032658
1
Iteration 17900: Loss = -11201.664596222183
Iteration 18000: Loss = -11201.664728007834
1
Iteration 18100: Loss = -11201.66459333232
Iteration 18200: Loss = -11201.664604206806
Iteration 18300: Loss = -11201.664797048154
1
Iteration 18400: Loss = -11201.712259326863
2
Iteration 18500: Loss = -11201.664569536193
Iteration 18600: Loss = -11201.665353936078
1
Iteration 18700: Loss = -11201.669244231167
2
Iteration 18800: Loss = -11201.669007277778
3
Iteration 18900: Loss = -11201.664570300398
Iteration 19000: Loss = -11201.668111096902
1
Iteration 19100: Loss = -11201.664891894305
2
Iteration 19200: Loss = -11201.664967255443
3
Iteration 19300: Loss = -11201.664549620496
Iteration 19400: Loss = -11201.664566798445
Iteration 19500: Loss = -11201.666830310505
1
Iteration 19600: Loss = -11201.673533087074
2
Iteration 19700: Loss = -11201.664545788142
Iteration 19800: Loss = -11201.66641680698
1
Iteration 19900: Loss = -11201.66451250158
pi: tensor([[0.7659, 0.2341],
        [0.2614, 0.7386]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4921, 0.5079], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3003, 0.1016],
         [0.5183, 0.1991]],

        [[0.5118, 0.0946],
         [0.6472, 0.5479]],

        [[0.6494, 0.1066],
         [0.7221, 0.5419]],

        [[0.5546, 0.0967],
         [0.5532, 0.5351]],

        [[0.6000, 0.1010],
         [0.5794, 0.6801]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9681923665792226
Average Adjusted Rand Index: 0.968320385327009
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20331.466055646724
Iteration 100: Loss = -11523.43610100118
Iteration 200: Loss = -11522.550000336785
Iteration 300: Loss = -11519.318902732479
Iteration 400: Loss = -11511.39307215961
Iteration 500: Loss = -11471.566497511289
Iteration 600: Loss = -11325.305371062559
Iteration 700: Loss = -11270.092593712285
Iteration 800: Loss = -11268.72785706057
Iteration 900: Loss = -11268.39980635735
Iteration 1000: Loss = -11267.978442423879
Iteration 1100: Loss = -11265.17145000635
Iteration 1200: Loss = -11265.102660845523
Iteration 1300: Loss = -11264.995763948973
Iteration 1400: Loss = -11264.74974674043
Iteration 1500: Loss = -11264.689173201896
Iteration 1600: Loss = -11264.646520076434
Iteration 1700: Loss = -11263.99902362034
Iteration 1800: Loss = -11263.985804804624
Iteration 1900: Loss = -11263.974931609459
Iteration 2000: Loss = -11263.964659618072
Iteration 2100: Loss = -11263.955376657137
Iteration 2200: Loss = -11263.949280811996
Iteration 2300: Loss = -11263.943895227685
Iteration 2400: Loss = -11263.939159504995
Iteration 2500: Loss = -11263.93481352482
Iteration 2600: Loss = -11263.930754261024
Iteration 2700: Loss = -11263.926913714058
Iteration 2800: Loss = -11263.922811017143
Iteration 2900: Loss = -11263.917039132244
Iteration 3000: Loss = -11263.910717775892
Iteration 3100: Loss = -11263.908231702995
Iteration 3200: Loss = -11263.907172750647
Iteration 3300: Loss = -11263.905063224254
Iteration 3400: Loss = -11263.902729962014
Iteration 3500: Loss = -11263.901902198808
Iteration 3600: Loss = -11263.899709271638
Iteration 3700: Loss = -11263.898748504458
Iteration 3800: Loss = -11263.896045269365
Iteration 3900: Loss = -11263.88410672596
Iteration 4000: Loss = -11263.881162131423
Iteration 4100: Loss = -11263.855736956259
Iteration 4200: Loss = -11263.847204138045
Iteration 4300: Loss = -11263.846793804227
Iteration 4400: Loss = -11263.846337896899
Iteration 4500: Loss = -11263.851703880684
1
Iteration 4600: Loss = -11263.844259360236
Iteration 4700: Loss = -11263.841627819791
Iteration 4800: Loss = -11263.840655693835
Iteration 4900: Loss = -11263.840052221462
Iteration 5000: Loss = -11263.839481724994
Iteration 5100: Loss = -11263.842057268748
1
Iteration 5200: Loss = -11263.845522646425
2
Iteration 5300: Loss = -11263.838083338258
Iteration 5400: Loss = -11263.837680116721
Iteration 5500: Loss = -11263.839036780602
1
Iteration 5600: Loss = -11263.8368569325
Iteration 5700: Loss = -11263.836415328811
Iteration 5800: Loss = -11263.83736125506
1
Iteration 5900: Loss = -11263.835320932894
Iteration 6000: Loss = -11263.835268247523
Iteration 6100: Loss = -11263.83578521096
1
Iteration 6200: Loss = -11263.832124868602
Iteration 6300: Loss = -11263.830537283598
Iteration 6400: Loss = -11263.82495191474
Iteration 6500: Loss = -11263.813014090112
Iteration 6600: Loss = -11263.747432884315
Iteration 6700: Loss = -11263.701411233735
Iteration 6800: Loss = -11263.66166252571
Iteration 6900: Loss = -11263.65081631119
Iteration 7000: Loss = -11263.656259548436
1
Iteration 7100: Loss = -11263.640884042556
Iteration 7200: Loss = -11263.659980600072
1
Iteration 7300: Loss = -11263.631520584267
Iteration 7400: Loss = -11263.618884191035
Iteration 7500: Loss = -11263.475531026079
Iteration 7600: Loss = -11222.78561660988
Iteration 7700: Loss = -11201.78620548051
Iteration 7800: Loss = -11201.77823315603
Iteration 7900: Loss = -11201.72343071316
Iteration 8000: Loss = -11201.70886371185
Iteration 8100: Loss = -11201.699425941468
Iteration 8200: Loss = -11201.698521889331
Iteration 8300: Loss = -11201.698081316275
Iteration 8400: Loss = -11201.69782269611
Iteration 8500: Loss = -11201.698117209551
1
Iteration 8600: Loss = -11201.764925457306
2
Iteration 8700: Loss = -11201.697214933176
Iteration 8800: Loss = -11201.698088426258
1
Iteration 8900: Loss = -11201.696870202191
Iteration 9000: Loss = -11201.697319645295
1
Iteration 9100: Loss = -11201.695949705107
Iteration 9200: Loss = -11201.710476586466
1
Iteration 9300: Loss = -11201.695249986613
Iteration 9400: Loss = -11201.694273198667
Iteration 9500: Loss = -11201.693106811317
Iteration 9600: Loss = -11201.69451834766
1
Iteration 9700: Loss = -11201.693032116724
Iteration 9800: Loss = -11201.692922934119
Iteration 9900: Loss = -11201.693647252936
1
Iteration 10000: Loss = -11201.692596647585
Iteration 10100: Loss = -11201.692763297906
1
Iteration 10200: Loss = -11201.693397457282
2
Iteration 10300: Loss = -11201.692139239907
Iteration 10400: Loss = -11201.770181461226
1
Iteration 10500: Loss = -11201.686149472498
Iteration 10600: Loss = -11201.686147050958
Iteration 10700: Loss = -11201.706190931836
1
Iteration 10800: Loss = -11201.686870411992
2
Iteration 10900: Loss = -11201.686112461251
Iteration 11000: Loss = -11201.68596406472
Iteration 11100: Loss = -11201.695018849425
1
Iteration 11200: Loss = -11201.675652285347
Iteration 11300: Loss = -11201.677087116665
1
Iteration 11400: Loss = -11201.675007080934
Iteration 11500: Loss = -11201.675581633275
1
Iteration 11600: Loss = -11201.666390778164
Iteration 11700: Loss = -11201.66649708222
1
Iteration 11800: Loss = -11201.66873220993
2
Iteration 11900: Loss = -11201.666305953891
Iteration 12000: Loss = -11201.667476685081
1
Iteration 12100: Loss = -11201.66619036743
Iteration 12200: Loss = -11201.666733866929
1
Iteration 12300: Loss = -11201.666085419338
Iteration 12400: Loss = -11201.666428241959
1
Iteration 12500: Loss = -11201.666198329944
2
Iteration 12600: Loss = -11201.67449546539
3
Iteration 12700: Loss = -11201.663814072264
Iteration 12800: Loss = -11201.664436524527
1
Iteration 12900: Loss = -11201.663275791909
Iteration 13000: Loss = -11201.669525993872
1
Iteration 13100: Loss = -11201.663019831105
Iteration 13200: Loss = -11201.662995387092
Iteration 13300: Loss = -11201.678785122846
1
Iteration 13400: Loss = -11201.663191174846
2
Iteration 13500: Loss = -11201.672197812193
3
Iteration 13600: Loss = -11201.662788452068
Iteration 13700: Loss = -11201.662639827038
Iteration 13800: Loss = -11201.698662274239
1
Iteration 13900: Loss = -11201.809559674419
2
Iteration 14000: Loss = -11201.662144204758
Iteration 14100: Loss = -11201.6621238934
Iteration 14200: Loss = -11201.679630883515
1
Iteration 14300: Loss = -11201.662063653303
Iteration 14400: Loss = -11201.662039547446
Iteration 14500: Loss = -11201.679990020244
1
Iteration 14600: Loss = -11201.662016377744
Iteration 14700: Loss = -11201.664547654053
1
Iteration 14800: Loss = -11201.666166895262
2
Iteration 14900: Loss = -11201.662035569057
Iteration 15000: Loss = -11201.66212631561
Iteration 15100: Loss = -11201.662124862012
Iteration 15200: Loss = -11201.662052255324
Iteration 15300: Loss = -11201.663905562922
1
Iteration 15400: Loss = -11201.690568217313
2
Iteration 15500: Loss = -11201.66193368125
Iteration 15600: Loss = -11201.73112070736
1
Iteration 15700: Loss = -11201.661920139775
Iteration 15800: Loss = -11201.662619309864
1
Iteration 15900: Loss = -11201.661923864967
Iteration 16000: Loss = -11201.663691674208
1
Iteration 16100: Loss = -11201.664368398498
2
Iteration 16200: Loss = -11201.661925249471
Iteration 16300: Loss = -11201.668167660273
1
Iteration 16400: Loss = -11201.661947724882
Iteration 16500: Loss = -11201.663060335559
1
Iteration 16600: Loss = -11201.6627835974
2
Iteration 16700: Loss = -11201.675118922149
3
Iteration 16800: Loss = -11201.66192175185
Iteration 16900: Loss = -11201.663891418888
1
Iteration 17000: Loss = -11201.662231001466
2
Iteration 17100: Loss = -11201.662120761655
3
Iteration 17200: Loss = -11201.66404309092
4
Iteration 17300: Loss = -11201.79113834772
5
Iteration 17400: Loss = -11201.664170824897
6
Iteration 17500: Loss = -11201.661925341947
Iteration 17600: Loss = -11201.661928980839
Iteration 17700: Loss = -11201.662002443129
Iteration 17800: Loss = -11201.664384888469
1
Iteration 17900: Loss = -11201.66219301566
2
Iteration 18000: Loss = -11201.663100909269
3
Iteration 18100: Loss = -11201.66224497111
4
Iteration 18200: Loss = -11201.661293040797
Iteration 18300: Loss = -11201.966401729283
1
Iteration 18400: Loss = -11201.661205063894
Iteration 18500: Loss = -11201.766035611638
1
Iteration 18600: Loss = -11201.661205044718
Iteration 18700: Loss = -11201.662440668333
1
Iteration 18800: Loss = -11201.661654886871
2
Iteration 18900: Loss = -11201.661278553684
Iteration 19000: Loss = -11201.661358490996
Iteration 19100: Loss = -11201.69286809563
1
Iteration 19200: Loss = -11201.661358907817
Iteration 19300: Loss = -11201.661977280311
1
Iteration 19400: Loss = -11201.662006103385
2
Iteration 19500: Loss = -11201.665165266382
3
Iteration 19600: Loss = -11201.935482289837
4
Iteration 19700: Loss = -11201.661180049146
Iteration 19800: Loss = -11201.662362436351
1
Iteration 19900: Loss = -11201.67232829734
2
pi: tensor([[0.7661, 0.2339],
        [0.2617, 0.7383]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4926, 0.5074], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3001, 0.1015],
         [0.6126, 0.1992]],

        [[0.7121, 0.0946],
         [0.5808, 0.7063]],

        [[0.6814, 0.1065],
         [0.7092, 0.6986]],

        [[0.5909, 0.0966],
         [0.6475, 0.5544]],

        [[0.6161, 0.1009],
         [0.7064, 0.6914]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9681923665792226
Average Adjusted Rand Index: 0.968320385327009
11214.76149173985
[0.9681923665792226, 0.9681923665792226] [0.968320385327009, 0.968320385327009] [11201.66461445552, 11201.663268138813]
-------------------------------------
This iteration is 38
True Objective function: Loss = -11280.209631308191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23374.892864216134
Iteration 100: Loss = -11258.90129604405
Iteration 200: Loss = -11254.835610900935
Iteration 300: Loss = -11254.521092021185
Iteration 400: Loss = -11254.398957880107
Iteration 500: Loss = -11254.337363680506
Iteration 600: Loss = -11254.30183435698
Iteration 700: Loss = -11254.279293909785
Iteration 800: Loss = -11254.264110765851
Iteration 900: Loss = -11254.253340448118
Iteration 1000: Loss = -11254.245394096442
Iteration 1100: Loss = -11254.239369511968
Iteration 1200: Loss = -11254.234674229343
Iteration 1300: Loss = -11254.230953303335
Iteration 1400: Loss = -11254.22796596307
Iteration 1500: Loss = -11254.225487944846
Iteration 1600: Loss = -11254.223478191076
Iteration 1700: Loss = -11254.249820830202
1
Iteration 1800: Loss = -11254.220253182204
Iteration 1900: Loss = -11254.219070793299
Iteration 2000: Loss = -11254.218035655233
Iteration 2100: Loss = -11254.217039677305
Iteration 2200: Loss = -11254.2163777256
Iteration 2300: Loss = -11254.215528359715
Iteration 2400: Loss = -11254.214870651303
Iteration 2500: Loss = -11254.214317572787
Iteration 2600: Loss = -11254.218602447565
1
Iteration 2700: Loss = -11254.213328020081
Iteration 2800: Loss = -11254.21292195826
Iteration 2900: Loss = -11254.216860103594
1
Iteration 3000: Loss = -11254.212149223491
Iteration 3100: Loss = -11254.212773143792
1
Iteration 3200: Loss = -11254.211614983913
Iteration 3300: Loss = -11254.211416570046
Iteration 3400: Loss = -11254.211176714034
Iteration 3500: Loss = -11254.211004578705
Iteration 3600: Loss = -11254.210894383717
Iteration 3700: Loss = -11254.210634566032
Iteration 3800: Loss = -11254.210592424004
Iteration 3900: Loss = -11254.210340329479
Iteration 4000: Loss = -11254.210233167481
Iteration 4100: Loss = -11254.210142905873
Iteration 4200: Loss = -11254.209985403511
Iteration 4300: Loss = -11254.21105606666
1
Iteration 4400: Loss = -11254.209794370057
Iteration 4500: Loss = -11254.243556003137
1
Iteration 4600: Loss = -11254.209638409235
Iteration 4700: Loss = -11254.20956871843
Iteration 4800: Loss = -11254.209599514637
Iteration 4900: Loss = -11254.209435848188
Iteration 5000: Loss = -11254.211160544937
1
Iteration 5100: Loss = -11254.209339121175
Iteration 5200: Loss = -11254.209327202849
Iteration 5300: Loss = -11254.209234040216
Iteration 5400: Loss = -11254.209210551688
Iteration 5500: Loss = -11254.209735346298
1
Iteration 5600: Loss = -11254.209130959307
Iteration 5700: Loss = -11254.209092748013
Iteration 5800: Loss = -11254.209102789686
Iteration 5900: Loss = -11254.20904752188
Iteration 6000: Loss = -11254.20922577067
1
Iteration 6100: Loss = -11254.210221693786
2
Iteration 6200: Loss = -11254.209009768398
Iteration 6300: Loss = -11254.209004701841
Iteration 6400: Loss = -11254.209964875668
1
Iteration 6500: Loss = -11254.20902921618
Iteration 6600: Loss = -11254.208827136063
Iteration 6700: Loss = -11254.230272875708
1
Iteration 6800: Loss = -11254.208825471233
Iteration 6900: Loss = -11254.209117275735
1
Iteration 7000: Loss = -11254.208822721579
Iteration 7100: Loss = -11254.209514149261
1
Iteration 7200: Loss = -11254.209681442268
2
Iteration 7300: Loss = -11254.212611347199
3
Iteration 7400: Loss = -11254.210515322757
4
Iteration 7500: Loss = -11254.209000774312
5
Iteration 7600: Loss = -11254.209025620145
6
Iteration 7700: Loss = -11254.20946027949
7
Iteration 7800: Loss = -11254.208754762034
Iteration 7900: Loss = -11254.209089400489
1
Iteration 8000: Loss = -11254.215692295
2
Iteration 8100: Loss = -11254.209083889018
3
Iteration 8200: Loss = -11254.208731395784
Iteration 8300: Loss = -11254.208797486219
Iteration 8400: Loss = -11254.208706263327
Iteration 8500: Loss = -11254.208718247031
Iteration 8600: Loss = -11254.210221908097
1
Iteration 8700: Loss = -11254.208681448337
Iteration 8800: Loss = -11254.208772938448
Iteration 8900: Loss = -11254.2086907625
Iteration 9000: Loss = -11254.209500062263
1
Iteration 9100: Loss = -11254.208675772967
Iteration 9200: Loss = -11254.232361635346
1
Iteration 9300: Loss = -11254.208684046225
Iteration 9400: Loss = -11254.208661640423
Iteration 9500: Loss = -11254.21003444905
1
Iteration 9600: Loss = -11254.208676957132
Iteration 9700: Loss = -11254.211136268046
1
Iteration 9800: Loss = -11254.232548067243
2
Iteration 9900: Loss = -11254.322896816358
3
Iteration 10000: Loss = -11254.224626394422
4
Iteration 10100: Loss = -11254.20865965003
Iteration 10200: Loss = -11254.210498176926
1
Iteration 10300: Loss = -11254.208656399549
Iteration 10400: Loss = -11254.208754628613
Iteration 10500: Loss = -11254.208627222099
Iteration 10600: Loss = -11254.21302755999
1
Iteration 10700: Loss = -11254.208659964375
Iteration 10800: Loss = -11254.208664315214
Iteration 10900: Loss = -11254.208710961255
Iteration 11000: Loss = -11254.208636702206
Iteration 11100: Loss = -11254.212840430158
1
Iteration 11200: Loss = -11254.208629269999
Iteration 11300: Loss = -11254.229499701954
1
Iteration 11400: Loss = -11254.208637703252
Iteration 11500: Loss = -11254.275009552584
1
Iteration 11600: Loss = -11254.208610349398
Iteration 11700: Loss = -11254.467945768864
1
Iteration 11800: Loss = -11254.208620883568
Iteration 11900: Loss = -11254.23082708773
1
Iteration 12000: Loss = -11254.208655911832
Iteration 12100: Loss = -11254.208756769218
1
Iteration 12200: Loss = -11254.208671216018
Iteration 12300: Loss = -11254.208609629663
Iteration 12400: Loss = -11254.217620554322
1
Iteration 12500: Loss = -11254.208603023904
Iteration 12600: Loss = -11254.208616312735
Iteration 12700: Loss = -11254.209340344523
1
Iteration 12800: Loss = -11254.208601154325
Iteration 12900: Loss = -11254.208620637719
Iteration 13000: Loss = -11254.210002506978
1
Iteration 13100: Loss = -11254.208596625042
Iteration 13200: Loss = -11254.209363770273
1
Iteration 13300: Loss = -11254.208625701121
Iteration 13400: Loss = -11254.218967138606
1
Iteration 13500: Loss = -11254.208626412057
Iteration 13600: Loss = -11254.20878989636
1
Iteration 13700: Loss = -11254.210711357588
2
Iteration 13800: Loss = -11254.208711871488
Iteration 13900: Loss = -11254.22203953295
1
Iteration 14000: Loss = -11254.208624030207
Iteration 14100: Loss = -11254.245439765651
1
Iteration 14200: Loss = -11254.208633772792
Iteration 14300: Loss = -11254.20864337113
Iteration 14400: Loss = -11254.20872611218
Iteration 14500: Loss = -11254.208601343951
Iteration 14600: Loss = -11254.224054166274
1
Iteration 14700: Loss = -11254.208600821556
Iteration 14800: Loss = -11254.208626795982
Iteration 14900: Loss = -11254.208728054515
1
Iteration 15000: Loss = -11254.208623113249
Iteration 15100: Loss = -11254.212026081374
1
Iteration 15200: Loss = -11254.208608735695
Iteration 15300: Loss = -11254.287096850678
1
Iteration 15400: Loss = -11254.208610252535
Iteration 15500: Loss = -11254.216072465786
1
Iteration 15600: Loss = -11254.208606898355
Iteration 15700: Loss = -11254.210620023234
1
Iteration 15800: Loss = -11254.208598291734
Iteration 15900: Loss = -11254.208643484793
Iteration 16000: Loss = -11254.208690390464
Iteration 16100: Loss = -11254.208635011819
Iteration 16200: Loss = -11254.40859425341
1
Iteration 16300: Loss = -11254.208634813122
Iteration 16400: Loss = -11254.208617418739
Iteration 16500: Loss = -11254.221361336797
1
Iteration 16600: Loss = -11254.208619048975
Iteration 16700: Loss = -11254.208659220767
Iteration 16800: Loss = -11254.208772283417
1
Iteration 16900: Loss = -11254.222671538995
2
Iteration 17000: Loss = -11254.208610064941
Iteration 17100: Loss = -11254.210375252002
1
Iteration 17200: Loss = -11254.209434112168
2
Iteration 17300: Loss = -11254.209784757266
3
Iteration 17400: Loss = -11254.209919896439
4
Iteration 17500: Loss = -11254.217641510737
5
Iteration 17600: Loss = -11254.218044498477
6
Iteration 17700: Loss = -11254.268773462076
7
Iteration 17800: Loss = -11254.213183642198
8
Iteration 17900: Loss = -11254.209285434525
9
Iteration 18000: Loss = -11254.209433915186
10
Iteration 18100: Loss = -11254.20959062781
11
Iteration 18200: Loss = -11254.208627114986
Iteration 18300: Loss = -11254.208841763202
1
Iteration 18400: Loss = -11254.208615943056
Iteration 18500: Loss = -11254.21115276027
1
Iteration 18600: Loss = -11254.20859296382
Iteration 18700: Loss = -11254.208641375044
Iteration 18800: Loss = -11254.208692894488
Iteration 18900: Loss = -11254.208615606407
Iteration 19000: Loss = -11254.275673683993
1
Iteration 19100: Loss = -11254.20861899161
Iteration 19200: Loss = -11254.208609123005
Iteration 19300: Loss = -11254.221694104124
1
Iteration 19400: Loss = -11254.20858579556
Iteration 19500: Loss = -11254.210689533607
1
Iteration 19600: Loss = -11254.213571250815
2
Iteration 19700: Loss = -11254.210277838443
3
Iteration 19800: Loss = -11254.208603926507
Iteration 19900: Loss = -11254.210564940939
1
pi: tensor([[0.7624, 0.2376],
        [0.2469, 0.7531]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5366, 0.4634], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3004, 0.0981],
         [0.6392, 0.2031]],

        [[0.5066, 0.1009],
         [0.6112, 0.6497]],

        [[0.5943, 0.0955],
         [0.5771, 0.5684]],

        [[0.5563, 0.1113],
         [0.6885, 0.5339]],

        [[0.5899, 0.1017],
         [0.6797, 0.6674]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9214426140763939
Average Adjusted Rand Index: 0.9212867907403716
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21132.600592405834
Iteration 100: Loss = -11565.263473533458
Iteration 200: Loss = -11563.013692996106
Iteration 300: Loss = -11551.045195154484
Iteration 400: Loss = -11533.997301311669
Iteration 500: Loss = -11441.339013911473
Iteration 600: Loss = -11280.565164655183
Iteration 700: Loss = -11259.135443991065
Iteration 800: Loss = -11257.176139312063
Iteration 900: Loss = -11256.9213415324
Iteration 1000: Loss = -11256.767889248069
Iteration 1100: Loss = -11256.507727395378
Iteration 1200: Loss = -11256.33350767508
Iteration 1300: Loss = -11256.27597390475
Iteration 1400: Loss = -11256.173547678793
Iteration 1500: Loss = -11256.135696068844
Iteration 1600: Loss = -11256.090474456285
Iteration 1700: Loss = -11256.069396751003
Iteration 1800: Loss = -11256.055654844658
Iteration 1900: Loss = -11256.044593081388
Iteration 2000: Loss = -11256.035291412089
Iteration 2100: Loss = -11256.027049867851
Iteration 2200: Loss = -11256.019232291304
Iteration 2300: Loss = -11256.010680310432
Iteration 2400: Loss = -11255.996974173584
Iteration 2500: Loss = -11254.562895278494
Iteration 2600: Loss = -11254.38111486081
Iteration 2700: Loss = -11254.369880225959
Iteration 2800: Loss = -11254.364985771144
Iteration 2900: Loss = -11254.361058881006
Iteration 3000: Loss = -11254.357361081022
Iteration 3100: Loss = -11254.35358832694
Iteration 3200: Loss = -11254.348953514134
Iteration 3300: Loss = -11254.346160779935
Iteration 3400: Loss = -11254.344362363585
Iteration 3500: Loss = -11254.342862783804
Iteration 3600: Loss = -11254.34142234967
Iteration 3700: Loss = -11254.340242597937
Iteration 3800: Loss = -11254.338998294577
Iteration 3900: Loss = -11254.337819618851
Iteration 4000: Loss = -11254.338799164367
1
Iteration 4100: Loss = -11254.335520031407
Iteration 4200: Loss = -11254.33429175875
Iteration 4300: Loss = -11254.331740293705
Iteration 4400: Loss = -11254.247486783988
Iteration 4500: Loss = -11254.247023244037
Iteration 4600: Loss = -11254.246122759494
Iteration 4700: Loss = -11254.24578249067
Iteration 4800: Loss = -11254.24555705224
Iteration 4900: Loss = -11254.24487080189
Iteration 5000: Loss = -11254.244277257176
Iteration 5100: Loss = -11254.245691738295
1
Iteration 5200: Loss = -11254.243582851788
Iteration 5300: Loss = -11254.243523293693
Iteration 5400: Loss = -11254.242926992825
Iteration 5500: Loss = -11254.242796656159
Iteration 5600: Loss = -11254.243128928843
1
Iteration 5700: Loss = -11254.244383835852
2
Iteration 5800: Loss = -11254.241705399605
Iteration 5900: Loss = -11254.25359811117
1
Iteration 6000: Loss = -11254.241293532472
Iteration 6100: Loss = -11254.241088748115
Iteration 6200: Loss = -11254.240904471495
Iteration 6300: Loss = -11254.240765299422
Iteration 6400: Loss = -11254.240729388011
Iteration 6500: Loss = -11254.249754047501
1
Iteration 6600: Loss = -11254.318080254787
2
Iteration 6700: Loss = -11254.240209404192
Iteration 6800: Loss = -11254.240100594678
Iteration 6900: Loss = -11254.263514026668
1
Iteration 7000: Loss = -11254.239835197191
Iteration 7100: Loss = -11254.359193211356
1
Iteration 7200: Loss = -11254.239643254938
Iteration 7300: Loss = -11254.239520014105
Iteration 7400: Loss = -11254.239444739495
Iteration 7500: Loss = -11254.239315878187
Iteration 7600: Loss = -11254.240020037923
1
Iteration 7700: Loss = -11254.239070468206
Iteration 7800: Loss = -11254.238809045779
Iteration 7900: Loss = -11254.237853620054
Iteration 8000: Loss = -11254.23680426642
Iteration 8100: Loss = -11254.250336415753
1
Iteration 8200: Loss = -11254.23665333383
Iteration 8300: Loss = -11254.236582688603
Iteration 8400: Loss = -11254.450701413392
1
Iteration 8500: Loss = -11254.236519294815
Iteration 8600: Loss = -11254.2364469139
Iteration 8700: Loss = -11254.247373401078
1
Iteration 8800: Loss = -11254.23638109433
Iteration 8900: Loss = -11254.236349971801
Iteration 9000: Loss = -11254.24073888297
1
Iteration 9100: Loss = -11254.236271981095
Iteration 9200: Loss = -11254.239306956784
1
Iteration 9300: Loss = -11254.236486304158
2
Iteration 9400: Loss = -11254.236303002475
Iteration 9500: Loss = -11254.236012062776
Iteration 9600: Loss = -11254.236737706873
1
Iteration 9700: Loss = -11254.235740035097
Iteration 9800: Loss = -11254.252579103935
1
Iteration 9900: Loss = -11254.235706427786
Iteration 10000: Loss = -11254.239053300564
1
Iteration 10100: Loss = -11254.258365688564
2
Iteration 10200: Loss = -11254.23621806989
3
Iteration 10300: Loss = -11254.23561046784
Iteration 10400: Loss = -11254.236677674722
1
Iteration 10500: Loss = -11254.235520439324
Iteration 10600: Loss = -11254.23571787551
1
Iteration 10700: Loss = -11254.235465925709
Iteration 10800: Loss = -11254.23659927677
1
Iteration 10900: Loss = -11254.235452170065
Iteration 11000: Loss = -11254.238277422459
1
Iteration 11100: Loss = -11254.235409506691
Iteration 11200: Loss = -11254.240687399752
1
Iteration 11300: Loss = -11254.235397066184
Iteration 11400: Loss = -11254.240314624092
1
Iteration 11500: Loss = -11254.28646655257
2
Iteration 11600: Loss = -11254.246846229113
3
Iteration 11700: Loss = -11254.236085925322
4
Iteration 11800: Loss = -11254.23550485662
5
Iteration 11900: Loss = -11254.235510379849
6
Iteration 12000: Loss = -11254.241565572325
7
Iteration 12100: Loss = -11254.23515648627
Iteration 12200: Loss = -11254.238585453291
1
Iteration 12300: Loss = -11254.235170917489
Iteration 12400: Loss = -11254.314565902292
1
Iteration 12500: Loss = -11254.235145350092
Iteration 12600: Loss = -11254.235099895275
Iteration 12700: Loss = -11254.519469268747
1
Iteration 12800: Loss = -11254.234104871312
Iteration 12900: Loss = -11254.23409789531
Iteration 13000: Loss = -11254.234097843431
Iteration 13100: Loss = -11254.234626659078
1
Iteration 13200: Loss = -11254.234086585384
Iteration 13300: Loss = -11254.234201793457
1
Iteration 13400: Loss = -11254.234135063169
Iteration 13500: Loss = -11254.234544018354
1
Iteration 13600: Loss = -11254.234079452826
Iteration 13700: Loss = -11254.234403895183
1
Iteration 13800: Loss = -11254.234067255353
Iteration 13900: Loss = -11254.238062234328
1
Iteration 14000: Loss = -11254.234043701143
Iteration 14100: Loss = -11254.250449528015
1
Iteration 14200: Loss = -11254.23408706043
Iteration 14300: Loss = -11254.234059185274
Iteration 14400: Loss = -11254.252230024427
1
Iteration 14500: Loss = -11254.23405865132
Iteration 14600: Loss = -11254.234062988617
Iteration 14700: Loss = -11254.23834476066
1
Iteration 14800: Loss = -11254.233895948386
Iteration 14900: Loss = -11254.233881359918
Iteration 15000: Loss = -11254.234315151967
1
Iteration 15100: Loss = -11254.233917460451
Iteration 15200: Loss = -11254.529615839356
1
Iteration 15300: Loss = -11254.233895036845
Iteration 15400: Loss = -11254.233841663883
Iteration 15500: Loss = -11254.235059056877
1
Iteration 15600: Loss = -11254.271385185346
2
Iteration 15700: Loss = -11254.235885779835
3
Iteration 15800: Loss = -11254.240119587028
4
Iteration 15900: Loss = -11254.305167356655
5
Iteration 16000: Loss = -11254.233659518719
Iteration 16100: Loss = -11254.245856654015
1
Iteration 16200: Loss = -11254.244733709693
2
Iteration 16300: Loss = -11254.242508074201
3
Iteration 16400: Loss = -11254.24692840006
4
Iteration 16500: Loss = -11254.233358684656
Iteration 16600: Loss = -11254.233387965678
Iteration 16700: Loss = -11254.239502646042
1
Iteration 16800: Loss = -11254.233289167381
Iteration 16900: Loss = -11254.23363891974
1
Iteration 17000: Loss = -11254.2332903207
Iteration 17100: Loss = -11254.234517737608
1
Iteration 17200: Loss = -11254.233285517905
Iteration 17300: Loss = -11254.246100057655
1
Iteration 17400: Loss = -11254.233308649493
Iteration 17500: Loss = -11254.23436456926
1
Iteration 17600: Loss = -11254.233341308553
Iteration 17700: Loss = -11254.23331729897
Iteration 17800: Loss = -11254.278977100768
1
Iteration 17900: Loss = -11254.233309152245
Iteration 18000: Loss = -11254.233307666871
Iteration 18100: Loss = -11254.233720613694
1
Iteration 18200: Loss = -11254.233327914593
Iteration 18300: Loss = -11254.243898626291
1
Iteration 18400: Loss = -11254.2332928443
Iteration 18500: Loss = -11254.24219400394
1
Iteration 18600: Loss = -11254.233296554437
Iteration 18700: Loss = -11254.247502579308
1
Iteration 18800: Loss = -11254.23329519888
Iteration 18900: Loss = -11254.245951283869
1
Iteration 19000: Loss = -11254.238497786724
2
Iteration 19100: Loss = -11254.287334697256
3
Iteration 19200: Loss = -11254.23329176794
Iteration 19300: Loss = -11254.23483428172
1
Iteration 19400: Loss = -11254.233275423876
Iteration 19500: Loss = -11254.239084177318
1
Iteration 19600: Loss = -11254.233260667601
Iteration 19700: Loss = -11254.235895545322
1
Iteration 19800: Loss = -11254.233258597045
Iteration 19900: Loss = -11254.233458160865
1
pi: tensor([[0.7611, 0.2389],
        [0.2435, 0.7565]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5340, 0.4660], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3022, 0.0987],
         [0.6733, 0.2017]],

        [[0.5583, 0.1012],
         [0.6598, 0.6741]],

        [[0.5959, 0.0957],
         [0.5919, 0.7274]],

        [[0.6521, 0.1120],
         [0.6349, 0.7147]],

        [[0.5293, 0.1025],
         [0.5179, 0.6987]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.9137632467975612
Average Adjusted Rand Index: 0.9134456958283295
11280.209631308191
[0.9214426140763939, 0.9137632467975612] [0.9212867907403716, 0.9134456958283295] [11254.208624342991, 11254.325971278546]
-------------------------------------
This iteration is 39
True Objective function: Loss = -11003.702733799195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23666.787608454964
Iteration 100: Loss = -11164.72842512998
Iteration 200: Loss = -11159.170401975727
Iteration 300: Loss = -11158.862687590732
Iteration 400: Loss = -11158.734220878481
Iteration 500: Loss = -11158.663840449657
Iteration 600: Loss = -11158.619017974217
Iteration 700: Loss = -11158.587316327359
Iteration 800: Loss = -11158.563456827635
Iteration 900: Loss = -11158.545516400183
Iteration 1000: Loss = -11158.53291527442
Iteration 1100: Loss = -11158.524793681934
Iteration 1200: Loss = -11158.519706481808
Iteration 1300: Loss = -11158.51625103899
Iteration 1400: Loss = -11158.513700159987
Iteration 1500: Loss = -11158.511692815951
Iteration 1600: Loss = -11158.509946091792
Iteration 1700: Loss = -11158.50842103916
Iteration 1800: Loss = -11158.506941176374
Iteration 1900: Loss = -11158.505414578833
Iteration 2000: Loss = -11158.50380895556
Iteration 2100: Loss = -11158.501997954168
Iteration 2200: Loss = -11158.499847082889
Iteration 2300: Loss = -11158.497289363413
Iteration 2400: Loss = -11158.494155379896
Iteration 2500: Loss = -11158.490295293534
Iteration 2600: Loss = -11158.485301104174
Iteration 2700: Loss = -11158.478726515243
Iteration 2800: Loss = -11158.469574941275
Iteration 2900: Loss = -11158.456226179833
Iteration 3000: Loss = -11158.43645010066
Iteration 3100: Loss = -11158.410160368738
Iteration 3200: Loss = -11158.381204578885
Iteration 3300: Loss = -11158.350293560876
Iteration 3400: Loss = -11158.316651737301
Iteration 3500: Loss = -11158.194532857327
Iteration 3600: Loss = -11157.700955790226
Iteration 3700: Loss = -11157.677744440514
Iteration 3800: Loss = -11157.67308194279
Iteration 3900: Loss = -11157.670671234893
Iteration 4000: Loss = -11157.669025198771
Iteration 4100: Loss = -11157.667787677236
Iteration 4200: Loss = -11157.666779037787
Iteration 4300: Loss = -11157.666036152708
Iteration 4400: Loss = -11157.665329911266
Iteration 4500: Loss = -11157.664790241603
Iteration 4600: Loss = -11157.664330024443
Iteration 4700: Loss = -11157.663929854349
Iteration 4800: Loss = -11157.663526647888
Iteration 4900: Loss = -11157.663250325531
Iteration 5000: Loss = -11157.662940925722
Iteration 5100: Loss = -11157.664492583019
1
Iteration 5200: Loss = -11157.662442068688
Iteration 5300: Loss = -11157.662207086982
Iteration 5400: Loss = -11157.66215124578
Iteration 5500: Loss = -11157.66187410497
Iteration 5600: Loss = -11157.661737317789
Iteration 5700: Loss = -11157.66158364126
Iteration 5800: Loss = -11157.661442146109
Iteration 5900: Loss = -11157.661298029181
Iteration 6000: Loss = -11157.661141525901
Iteration 6100: Loss = -11157.661024550982
Iteration 6200: Loss = -11157.660965204983
Iteration 6300: Loss = -11157.660838696756
Iteration 6400: Loss = -11157.660742203918
Iteration 6500: Loss = -11157.66522064992
1
Iteration 6600: Loss = -11157.660609758539
Iteration 6700: Loss = -11157.660551866882
Iteration 6800: Loss = -11157.66045996759
Iteration 6900: Loss = -11157.66405802252
1
Iteration 7000: Loss = -11157.660657348866
2
Iteration 7100: Loss = -11157.660850818282
3
Iteration 7200: Loss = -11157.66197837194
4
Iteration 7300: Loss = -11157.661381120268
5
Iteration 7400: Loss = -11157.660793645653
6
Iteration 7500: Loss = -11157.660240358051
Iteration 7600: Loss = -11157.66018768162
Iteration 7700: Loss = -11157.660223672401
Iteration 7800: Loss = -11157.660010904989
Iteration 7900: Loss = -11157.660040155513
Iteration 8000: Loss = -11157.65998643068
Iteration 8100: Loss = -11157.659911853098
Iteration 8200: Loss = -11157.660663620567
1
Iteration 8300: Loss = -11157.66626119133
2
Iteration 8400: Loss = -11157.659910208744
Iteration 8500: Loss = -11157.660020418383
1
Iteration 8600: Loss = -11157.670388939703
2
Iteration 8700: Loss = -11157.659749501845
Iteration 8800: Loss = -11157.662380144302
1
Iteration 8900: Loss = -11157.65977342522
Iteration 9000: Loss = -11157.659772780826
Iteration 9100: Loss = -11157.659731182059
Iteration 9200: Loss = -11157.659703055244
Iteration 9300: Loss = -11157.65969512812
Iteration 9400: Loss = -11157.659672526737
Iteration 9500: Loss = -11157.659638358176
Iteration 9600: Loss = -11157.659662661146
Iteration 9700: Loss = -11157.660683987888
1
Iteration 9800: Loss = -11157.659581642563
Iteration 9900: Loss = -11157.659587118873
Iteration 10000: Loss = -11157.660710629947
1
Iteration 10100: Loss = -11157.6595613291
Iteration 10200: Loss = -11157.659590547324
Iteration 10300: Loss = -11157.661969907705
1
Iteration 10400: Loss = -11157.659530006853
Iteration 10500: Loss = -11157.659525072966
Iteration 10600: Loss = -11157.660718195122
1
Iteration 10700: Loss = -11157.659540848892
Iteration 10800: Loss = -11157.659519599281
Iteration 10900: Loss = -11157.659513593251
Iteration 11000: Loss = -11157.659499689964
Iteration 11100: Loss = -11157.659527234071
Iteration 11200: Loss = -11157.659540727578
Iteration 11300: Loss = -11157.659504624202
Iteration 11400: Loss = -11157.661841087855
1
Iteration 11500: Loss = -11157.659535046718
Iteration 11600: Loss = -11157.659486971283
Iteration 11700: Loss = -11158.057090245611
1
Iteration 11800: Loss = -11157.659460798815
Iteration 11900: Loss = -11157.659481951185
Iteration 12000: Loss = -11157.704442806616
1
Iteration 12100: Loss = -11157.659468570057
Iteration 12200: Loss = -11157.65944246115
Iteration 12300: Loss = -11158.117491662488
1
Iteration 12400: Loss = -11157.659470950226
Iteration 12500: Loss = -11157.6594694945
Iteration 12600: Loss = -11157.659483321604
Iteration 12700: Loss = -11157.659461205265
Iteration 12800: Loss = -11157.659430732074
Iteration 12900: Loss = -11157.659462923271
Iteration 13000: Loss = -11157.66623420843
1
Iteration 13100: Loss = -11157.659411347613
Iteration 13200: Loss = -11157.659411134196
Iteration 13300: Loss = -11157.7223687483
1
Iteration 13400: Loss = -11157.659412182493
Iteration 13500: Loss = -11157.659424063808
Iteration 13600: Loss = -11158.134612117532
1
Iteration 13700: Loss = -11157.6594242724
Iteration 13800: Loss = -11157.659454934921
Iteration 13900: Loss = -11157.661042490554
1
Iteration 14000: Loss = -11157.659450028228
Iteration 14100: Loss = -11157.65940592704
Iteration 14200: Loss = -11157.994162351302
1
Iteration 14300: Loss = -11157.659395918272
Iteration 14400: Loss = -11157.659391673193
Iteration 14500: Loss = -11157.659742652295
1
Iteration 14600: Loss = -11157.659420538015
Iteration 14700: Loss = -11157.66050042636
1
Iteration 14800: Loss = -11157.659907858613
2
Iteration 14900: Loss = -11157.80264841585
3
Iteration 15000: Loss = -11157.659386857233
Iteration 15100: Loss = -11157.66247689028
1
Iteration 15200: Loss = -11157.659411774854
Iteration 15300: Loss = -11157.660137253284
1
Iteration 15400: Loss = -11157.659440731768
Iteration 15500: Loss = -11157.832604654606
1
Iteration 15600: Loss = -11157.660677814321
2
Iteration 15700: Loss = -11157.660160095817
3
Iteration 15800: Loss = -11157.685101276142
4
Iteration 15900: Loss = -11157.659415564769
Iteration 16000: Loss = -11157.661058618467
1
Iteration 16100: Loss = -11157.659382843092
Iteration 16200: Loss = -11157.660160406973
1
Iteration 16300: Loss = -11157.659387466643
Iteration 16400: Loss = -11157.700295259468
1
Iteration 16500: Loss = -11157.6593935322
Iteration 16600: Loss = -11157.662478639162
1
Iteration 16700: Loss = -11157.659427226396
Iteration 16800: Loss = -11157.692617179904
1
Iteration 16900: Loss = -11157.659408842115
Iteration 17000: Loss = -11157.670458558738
1
Iteration 17100: Loss = -11157.662617730102
2
Iteration 17200: Loss = -11157.659427797505
Iteration 17300: Loss = -11157.684917739383
1
Iteration 17400: Loss = -11157.659420821154
Iteration 17500: Loss = -11157.666848124665
1
Iteration 17600: Loss = -11157.659378319098
Iteration 17700: Loss = -11157.66949287522
1
Iteration 17800: Loss = -11157.659376035514
Iteration 17900: Loss = -11157.753188189885
1
Iteration 18000: Loss = -11157.659395407018
Iteration 18100: Loss = -11157.785519966086
1
Iteration 18200: Loss = -11157.659380376846
Iteration 18300: Loss = -11157.660019953722
1
Iteration 18400: Loss = -11157.659381507263
Iteration 18500: Loss = -11157.65965828774
1
Iteration 18600: Loss = -11157.659368956383
Iteration 18700: Loss = -11157.659492285586
1
Iteration 18800: Loss = -11157.65939791357
Iteration 18900: Loss = -11157.65957757552
1
Iteration 19000: Loss = -11157.659494130117
Iteration 19100: Loss = -11157.66002341384
1
Iteration 19200: Loss = -11157.659637811692
2
Iteration 19300: Loss = -11157.659980416147
3
Iteration 19400: Loss = -11157.662736483768
4
Iteration 19500: Loss = -11157.659605836814
5
Iteration 19600: Loss = -11157.67345243121
6
Iteration 19700: Loss = -11157.659387537533
Iteration 19800: Loss = -11157.660059433663
1
Iteration 19900: Loss = -11157.65938973408
pi: tensor([[5.1130e-08, 1.0000e+00],
        [1.8785e-02, 9.8121e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3748, 0.6252], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3183, 0.1050],
         [0.7056, 0.1710]],

        [[0.6717, 0.1069],
         [0.5897, 0.5539]],

        [[0.5557, 0.0966],
         [0.5511, 0.6241]],

        [[0.5204, 0.1948],
         [0.7233, 0.5257]],

        [[0.7163, 0.1113],
         [0.5220, 0.7144]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9206289602688308
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.05505240666214738
Average Adjusted Rand Index: 0.18339851932649343
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21962.305165981237
Iteration 100: Loss = -11184.568054103967
Iteration 200: Loss = -11183.846010298019
Iteration 300: Loss = -11183.5609460191
Iteration 400: Loss = -11183.33281275452
Iteration 500: Loss = -11182.990272769379
Iteration 600: Loss = -11182.727519395474
Iteration 700: Loss = -11182.644847623687
Iteration 800: Loss = -11182.589594780538
Iteration 900: Loss = -11182.535141090415
Iteration 1000: Loss = -11182.470254166517
Iteration 1100: Loss = -11182.380961321876
Iteration 1200: Loss = -11182.253848693153
Iteration 1300: Loss = -11182.111585383018
Iteration 1400: Loss = -11181.987699915715
Iteration 1500: Loss = -11181.880963090038
Iteration 1600: Loss = -11181.786165705442
Iteration 1700: Loss = -11181.701228485159
Iteration 1800: Loss = -11181.626775375991
Iteration 1900: Loss = -11181.55935849147
Iteration 2000: Loss = -11181.491750045065
Iteration 2100: Loss = -11181.408004134777
Iteration 2200: Loss = -11181.289836368898
Iteration 2300: Loss = -11181.138568826444
Iteration 2400: Loss = -11180.991704049296
Iteration 2500: Loss = -11180.871669372897
Iteration 2600: Loss = -11180.76807743622
Iteration 2700: Loss = -11180.680540236002
Iteration 2800: Loss = -11180.602177237643
Iteration 2900: Loss = -11180.533223189665
Iteration 3000: Loss = -11180.466891165144
Iteration 3100: Loss = -11180.394723100637
Iteration 3200: Loss = -11180.293274381811
Iteration 3300: Loss = -10970.486153581427
Iteration 3400: Loss = -10967.22021363996
Iteration 3500: Loss = -10967.048591511983
Iteration 3600: Loss = -10966.972913755
Iteration 3700: Loss = -10966.831836833475
Iteration 3800: Loss = -10966.80664061802
Iteration 3900: Loss = -10966.774458449305
Iteration 4000: Loss = -10966.765424377003
Iteration 4100: Loss = -10966.756426665052
Iteration 4200: Loss = -10966.748299854602
Iteration 4300: Loss = -10966.74201809669
Iteration 4400: Loss = -10966.735720937084
Iteration 4500: Loss = -10966.733694897765
Iteration 4600: Loss = -10966.725848583674
Iteration 4700: Loss = -10966.733135417644
1
Iteration 4800: Loss = -10966.718163081363
Iteration 4900: Loss = -10966.713091050522
Iteration 5000: Loss = -10966.707352109286
Iteration 5100: Loss = -10966.660574874011
Iteration 5200: Loss = -10966.662363731912
1
Iteration 5300: Loss = -10966.671070501121
2
Iteration 5400: Loss = -10966.65662062486
Iteration 5500: Loss = -10966.638760461452
 40%|████      | 40/100 [14:02:26<21:26:27, 1286.46s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 41%|████      | 41/100 [14:21:59<20:31:31, 1252.40s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 42%|████▏     | 42/100 [14:43:40<20:24:39, 1266.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 43%|████▎     | 43/100 [15:04:15<19:54:32, 1257.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 44%|████▍     | 44/100 [15:25:40<19:41:22, 1265.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 45%|████▌     | 45/100 [15:47:03<19:24:56, 1270.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 46%|████▌     | 46/100 [16:08:20<19:05:16, 1272.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 47%|████▋     | 47/100 [16:29:55<18:50:03, 1279.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 48%|████▊     | 48/100 [16:51:11<18:27:53, 1278.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 49%|████▉     | 49/100 [17:11:28<17:51:02, 1260.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 50%|█████     | 50/100 [17:32:52<17:36:03, 1267.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 51%|█████     | 51/100 [17:51:28<16:37:41, 1221.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 52%|█████▏    | 52/100 [18:12:56<16:33:24, 1241.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 53%|█████▎    | 53/100 [18:34:22<16:23:06, 1255.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 54%|█████▍    | 54/100 [18:55:52<16:10:06, 1265.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 55%|█████▌    | 55/100 [19:17:19<15:53:54, 1271.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 56%|█████▌    | 56/100 [19:38:47<15:36:20, 1276.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 57%|█████▋    | 57/100 [20:00:18<15:17:57, 1280.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 58%|█████▊    | 58/100 [20:21:43<14:57:39, 1282.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 59%|█████▉    | 59/100 [20:43:14<14:38:02, 1284.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 60%|██████    | 60/100 [21:04:41<14:16:59, 1285.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 61%|██████    | 61/100 [21:21:32<13:02:07, 1203.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 62%|██████▏   | 62/100 [21:43:01<12:58:12, 1228.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 63%|██████▎   | 63/100 [22:04:30<12:48:59, 1247.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 64%|██████▍   | 64/100 [22:25:58<12:35:32, 1259.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 65%|██████▌   | 65/100 [22:47:28<12:19:52, 1268.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 66%|██████▌   | 66/100 [23:08:58<12:02:29, 1274.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 67%|██████▋   | 67/100 [23:30:27<11:43:31, 1279.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 68%|██████▊   | 68/100 [23:51:57<11:23:52, 1282.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 69%|██████▉   | 69/100 [24:13:23<11:03:11, 1283.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 70%|███████   | 70/100 [24:34:53<10:42:42, 1285.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 71%|███████   | 71/100 [24:56:25<10:22:12, 1287.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 72%|███████▏  | 72/100 [25:17:55<10:01:08, 1288.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 73%|███████▎  | 73/100 [25:39:18<9:39:01, 1286.71s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 74%|███████▍  | 74/100 [25:59:35<9:08:31, 1265.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 75%|███████▌  | 75/100 [26:20:33<8:46:27, 1263.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 76%|███████▌  | 76/100 [26:42:04<8:28:37, 1271.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 77%|███████▋  | 77/100 [27:03:36<8:09:47, 1277.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 78%|███████▊  | 78/100 [27:25:04<7:49:39, 1280.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 79%|███████▉  | 79/100 [27:46:30<7:28:52, 1282.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
Iteration 5600: Loss = -10966.639510309093
1
Iteration 5700: Loss = -10966.63747499758
Iteration 5800: Loss = -10966.637550840245
Iteration 5900: Loss = -10966.636574515982
Iteration 6000: Loss = -10966.636174920413
Iteration 6100: Loss = -10966.636185403384
Iteration 6200: Loss = -10966.6348938273
Iteration 6300: Loss = -10966.635110823185
1
Iteration 6400: Loss = -10966.634432294095
Iteration 6500: Loss = -10966.63449104021
Iteration 6600: Loss = -10966.633374280922
Iteration 6700: Loss = -10966.6313445424
Iteration 6800: Loss = -10966.628623769062
Iteration 6900: Loss = -10966.628366083683
Iteration 7000: Loss = -10966.62813945166
Iteration 7100: Loss = -10966.627830555095
Iteration 7200: Loss = -10966.629539615507
1
Iteration 7300: Loss = -10966.630151523377
2
Iteration 7400: Loss = -10966.654708693268
3
Iteration 7500: Loss = -10966.626883747082
Iteration 7600: Loss = -10966.628751636268
1
Iteration 7700: Loss = -10966.626476900601
Iteration 7800: Loss = -10966.631326502942
1
Iteration 7900: Loss = -10966.623494036563
Iteration 8000: Loss = -10966.62338562114
Iteration 8100: Loss = -10966.623501688708
1
Iteration 8200: Loss = -10966.622573167893
Iteration 8300: Loss = -10966.622205496602
Iteration 8400: Loss = -10966.62191020661
Iteration 8500: Loss = -10966.621698568895
Iteration 8600: Loss = -10966.622395185554
1
Iteration 8700: Loss = -10966.62157480891
Iteration 8800: Loss = -10966.700890993261
1
Iteration 8900: Loss = -10966.621152433068
Iteration 9000: Loss = -10966.621019976725
Iteration 9100: Loss = -10966.621533600013
1
Iteration 9200: Loss = -10966.620986179567
Iteration 9300: Loss = -10966.638632404405
1
Iteration 9400: Loss = -10966.620958164487
Iteration 9500: Loss = -10966.626765779985
1
Iteration 9600: Loss = -10966.620924304128
Iteration 9700: Loss = -10966.620876880475
Iteration 9800: Loss = -10966.622158848517
1
Iteration 9900: Loss = -10966.620800540306
Iteration 10000: Loss = -10966.622569143128
1
Iteration 10100: Loss = -10966.621017085408
2
Iteration 10200: Loss = -10966.623701132195
3
Iteration 10300: Loss = -10966.623733435768
4
Iteration 10400: Loss = -10966.621384969925
5
Iteration 10500: Loss = -10966.621176439643
6
Iteration 10600: Loss = -10966.636289917387
7
Iteration 10700: Loss = -10966.620676067734
Iteration 10800: Loss = -10966.620555517897
Iteration 10900: Loss = -10966.691161133685
1
Iteration 11000: Loss = -10966.620539798181
Iteration 11100: Loss = -10966.622906178369
1
Iteration 11200: Loss = -10966.624351563429
2
Iteration 11300: Loss = -10966.62053985377
Iteration 11400: Loss = -10966.621214461722
1
Iteration 11500: Loss = -10966.621239876737
2
Iteration 11600: Loss = -10966.628232116083
3
Iteration 11700: Loss = -10966.62056053687
Iteration 11800: Loss = -10966.62048527947
Iteration 11900: Loss = -10966.620658620228
1
Iteration 12000: Loss = -10966.623365956539
2
Iteration 12100: Loss = -10966.649979550119
3
Iteration 12200: Loss = -10966.620220299834
Iteration 12300: Loss = -10966.620858790382
1
Iteration 12400: Loss = -10966.643764207873
2
Iteration 12500: Loss = -10966.620395525391
3
Iteration 12600: Loss = -10966.940557556485
4
Iteration 12700: Loss = -10966.62000369647
Iteration 12800: Loss = -10966.679855576305
1
Iteration 12900: Loss = -10966.620009077622
Iteration 13000: Loss = -10966.62077793602
1
Iteration 13100: Loss = -10966.620035569476
Iteration 13200: Loss = -10966.643659131527
1
Iteration 13300: Loss = -10966.619984984767
Iteration 13400: Loss = -10966.620280258556
1
Iteration 13500: Loss = -10966.67117738635
2
Iteration 13600: Loss = -10966.619932306623
Iteration 13700: Loss = -10966.655096409915
1
Iteration 13800: Loss = -10966.619931666051
Iteration 13900: Loss = -10966.672860837125
1
Iteration 14000: Loss = -10966.619946481429
Iteration 14100: Loss = -10966.619956301585
Iteration 14200: Loss = -10966.62061904909
1
Iteration 14300: Loss = -10966.619953676107
Iteration 14400: Loss = -10966.62882267738
1
Iteration 14500: Loss = -10966.619964837595
Iteration 14600: Loss = -10966.620302032545
1
Iteration 14700: Loss = -10966.705703941974
2
Iteration 14800: Loss = -10966.619886034709
Iteration 14900: Loss = -10966.62667142973
1
Iteration 15000: Loss = -10966.61989498036
Iteration 15100: Loss = -10966.620101357677
1
Iteration 15200: Loss = -10966.627151863973
2
Iteration 15300: Loss = -10966.620015894865
3
Iteration 15400: Loss = -10966.663180362964
4
Iteration 15500: Loss = -10966.619910985326
Iteration 15600: Loss = -10966.633314371798
1
Iteration 15700: Loss = -10966.619936268973
Iteration 15800: Loss = -10966.62081322622
1
Iteration 15900: Loss = -10966.629208124976
2
Iteration 16000: Loss = -10966.61992376319
Iteration 16100: Loss = -10966.622072725017
1
Iteration 16200: Loss = -10966.619922401642
Iteration 16300: Loss = -10966.620121650118
1
Iteration 16400: Loss = -10966.631984404374
2
Iteration 16500: Loss = -10966.640828140875
3
Iteration 16600: Loss = -10966.624162401074
4
Iteration 16700: Loss = -10966.648908120329
5
Iteration 16800: Loss = -10966.61991411948
Iteration 16900: Loss = -10966.620455919703
1
Iteration 17000: Loss = -10966.619903616669
Iteration 17100: Loss = -10966.620152681884
1
Iteration 17200: Loss = -10966.619951993482
Iteration 17300: Loss = -10966.620411506794
1
Iteration 17400: Loss = -10966.620353769626
2
Iteration 17500: Loss = -10966.63749530793
3
Iteration 17600: Loss = -10966.635128751532
4
Iteration 17700: Loss = -10966.619957441024
Iteration 17800: Loss = -10966.641242303509
1
Iteration 17900: Loss = -10966.619881793951
Iteration 18000: Loss = -10966.620360749428
1
Iteration 18100: Loss = -10966.695166883599
2
Iteration 18200: Loss = -10966.620193738014
3
Iteration 18300: Loss = -10966.751353412192
4
Iteration 18400: Loss = -10966.619887531118
Iteration 18500: Loss = -10966.620419270323
1
Iteration 18600: Loss = -10966.619993123828
2
Iteration 18700: Loss = -10966.619982838829
Iteration 18800: Loss = -10966.62014493961
1
Iteration 18900: Loss = -10966.620060366176
Iteration 19000: Loss = -10966.620027855357
Iteration 19100: Loss = -10966.619898987847
Iteration 19200: Loss = -10966.621172577683
1
Iteration 19300: Loss = -10966.657566026988
2
Iteration 19400: Loss = -10966.619918643524
Iteration 19500: Loss = -10966.620074463659
1
Iteration 19600: Loss = -10966.711898504778
2
Iteration 19700: Loss = -10966.619915537567
Iteration 19800: Loss = -10966.624792023973
1
Iteration 19900: Loss = -10966.62286005302
2
pi: tensor([[0.7405, 0.2595],
        [0.2660, 0.7340]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6010, 0.3990], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2050, 0.1032],
         [0.6914, 0.2808]],

        [[0.6347, 0.0783],
         [0.5830, 0.5243]],

        [[0.6104, 0.0981],
         [0.6510, 0.6253]],

        [[0.5735, 0.1098],
         [0.6191, 0.6180]],

        [[0.5641, 0.1016],
         [0.5773, 0.6326]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9206887570795217
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9214379584608567
Average Adjusted Rand Index: 0.921591513200179
11003.702733799195
[0.05505240666214738, 0.9214379584608567] [0.18339851932649343, 0.921591513200179] [11157.680099526062, 10966.620037816794]
-------------------------------------
This iteration is 40
True Objective function: Loss = -11494.54111699616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23312.049452825675
Iteration 100: Loss = -11782.518453490546
Iteration 200: Loss = -11468.115449231527
Iteration 300: Loss = -11467.7227985029
Iteration 400: Loss = -11467.610495819397
Iteration 500: Loss = -11467.556726574021
Iteration 600: Loss = -11467.52582267551
Iteration 700: Loss = -11467.506188258492
Iteration 800: Loss = -11467.493053267493
Iteration 900: Loss = -11467.483736595092
Iteration 1000: Loss = -11467.476886742677
Iteration 1100: Loss = -11467.471694791964
Iteration 1200: Loss = -11467.467595184038
Iteration 1300: Loss = -11467.464381561225
Iteration 1400: Loss = -11467.461742452311
Iteration 1500: Loss = -11467.459559927804
Iteration 1600: Loss = -11467.457835743733
Iteration 1700: Loss = -11467.456284782385
Iteration 1800: Loss = -11467.455020418533
Iteration 1900: Loss = -11467.453907985479
Iteration 2000: Loss = -11467.452946214711
Iteration 2100: Loss = -11467.452115194968
Iteration 2200: Loss = -11467.451422124388
Iteration 2300: Loss = -11467.45080346232
Iteration 2400: Loss = -11467.450217798183
Iteration 2500: Loss = -11467.449715102544
Iteration 2600: Loss = -11467.449247105884
Iteration 2700: Loss = -11467.449207276786
Iteration 2800: Loss = -11467.448450670989
Iteration 2900: Loss = -11467.448093900854
Iteration 3000: Loss = -11467.447727880764
Iteration 3100: Loss = -11467.447276622961
Iteration 3200: Loss = -11467.446922683535
Iteration 3300: Loss = -11467.446664558807
Iteration 3400: Loss = -11467.4464563646
Iteration 3500: Loss = -11467.446280199081
Iteration 3600: Loss = -11467.446107049906
Iteration 3700: Loss = -11467.445929107811
Iteration 3800: Loss = -11467.446349540449
1
Iteration 3900: Loss = -11467.446185340566
2
Iteration 4000: Loss = -11467.4456084747
Iteration 4100: Loss = -11467.445802216042
1
Iteration 4200: Loss = -11467.445069943722
Iteration 4300: Loss = -11467.445870374688
1
Iteration 4400: Loss = -11467.444888018315
Iteration 4500: Loss = -11467.449178106444
1
Iteration 4600: Loss = -11467.44471829686
Iteration 4700: Loss = -11467.454823565977
1
Iteration 4800: Loss = -11467.444494932224
Iteration 4900: Loss = -11467.443444214652
Iteration 5000: Loss = -11467.4436462171
1
Iteration 5100: Loss = -11467.443139131357
Iteration 5200: Loss = -11467.443107922194
Iteration 5300: Loss = -11467.443032053556
Iteration 5400: Loss = -11467.442924631285
Iteration 5500: Loss = -11467.453639667396
1
Iteration 5600: Loss = -11467.442660117422
Iteration 5700: Loss = -11467.442574134777
Iteration 5800: Loss = -11467.442544059199
Iteration 5900: Loss = -11467.442430833944
Iteration 6000: Loss = -11467.442402701437
Iteration 6100: Loss = -11467.442478941408
Iteration 6200: Loss = -11467.443901403545
1
Iteration 6300: Loss = -11467.443051618004
2
Iteration 6400: Loss = -11467.442751227312
3
Iteration 6500: Loss = -11467.443191358032
4
Iteration 6600: Loss = -11467.458493810325
5
Iteration 6700: Loss = -11467.442219804916
Iteration 6800: Loss = -11467.443171486539
1
Iteration 6900: Loss = -11467.449243700206
2
Iteration 7000: Loss = -11467.4423689051
3
Iteration 7100: Loss = -11467.44226599639
Iteration 7200: Loss = -11467.44227257181
Iteration 7300: Loss = -11467.442264856698
Iteration 7400: Loss = -11467.441963875217
Iteration 7500: Loss = -11467.441055228584
Iteration 7600: Loss = -11467.446622964251
1
Iteration 7700: Loss = -11467.441091102926
Iteration 7800: Loss = -11467.44260753734
1
Iteration 7900: Loss = -11467.442078577635
2
Iteration 8000: Loss = -11467.44083368305
Iteration 8100: Loss = -11467.440350024286
Iteration 8200: Loss = -11467.44031065298
Iteration 8300: Loss = -11467.440417835907
1
Iteration 8400: Loss = -11467.445684558417
2
Iteration 8500: Loss = -11467.445920615772
3
Iteration 8600: Loss = -11467.448883943332
4
Iteration 8700: Loss = -11467.476532422485
5
Iteration 8800: Loss = -11467.440266752394
Iteration 8900: Loss = -11467.44168228423
1
Iteration 9000: Loss = -11467.440238879608
Iteration 9100: Loss = -11467.44770068861
1
Iteration 9200: Loss = -11467.440296536493
Iteration 9300: Loss = -11467.44626040769
1
Iteration 9400: Loss = -11467.44932648815
2
Iteration 9500: Loss = -11467.44378686563
3
Iteration 9600: Loss = -11467.440283370783
Iteration 9700: Loss = -11467.440389968882
1
Iteration 9800: Loss = -11467.531641107771
2
Iteration 9900: Loss = -11467.440223890237
Iteration 10000: Loss = -11467.457229733596
1
Iteration 10100: Loss = -11467.440252595368
Iteration 10200: Loss = -11467.458781516252
1
Iteration 10300: Loss = -11467.445758925529
2
Iteration 10400: Loss = -11467.440435988281
3
Iteration 10500: Loss = -11467.70253169882
4
Iteration 10600: Loss = -11467.440878031453
5
Iteration 10700: Loss = -11467.444419504722
6
Iteration 10800: Loss = -11467.446006679964
7
Iteration 10900: Loss = -11467.445585784699
8
Iteration 11000: Loss = -11467.440231245078
Iteration 11100: Loss = -11467.440970974561
1
Iteration 11200: Loss = -11467.469690533924
2
Iteration 11300: Loss = -11467.476662557872
3
Iteration 11400: Loss = -11467.44034727813
4
Iteration 11500: Loss = -11467.440296693758
Iteration 11600: Loss = -11467.45065080543
1
Iteration 11700: Loss = -11467.44441110732
2
Iteration 11800: Loss = -11467.440306375793
Iteration 11900: Loss = -11467.445815255744
1
Iteration 12000: Loss = -11467.444344374178
2
Iteration 12100: Loss = -11467.440247946457
Iteration 12200: Loss = -11467.50603614821
1
Iteration 12300: Loss = -11467.44020594656
Iteration 12400: Loss = -11467.442880686847
1
Iteration 12500: Loss = -11467.440569481754
2
Iteration 12600: Loss = -11467.4418682079
3
Iteration 12700: Loss = -11467.447608901317
4
Iteration 12800: Loss = -11467.46198966655
5
Iteration 12900: Loss = -11467.562663945562
6
Iteration 13000: Loss = -11467.443840547501
7
Iteration 13100: Loss = -11467.440200942292
Iteration 13200: Loss = -11467.440607848255
1
Iteration 13300: Loss = -11467.452580754458
2
Iteration 13400: Loss = -11467.634779405438
3
Iteration 13500: Loss = -11467.440222745718
Iteration 13600: Loss = -11467.450980116246
1
Iteration 13700: Loss = -11467.440187798802
Iteration 13800: Loss = -11467.44050795096
1
Iteration 13900: Loss = -11467.462750132285
2
Iteration 14000: Loss = -11467.440921974596
3
Iteration 14100: Loss = -11467.44989686603
4
Iteration 14200: Loss = -11467.505950902843
5
Iteration 14300: Loss = -11467.443863381
6
Iteration 14400: Loss = -11467.440278342587
Iteration 14500: Loss = -11467.448398726603
1
Iteration 14600: Loss = -11467.448388668612
2
Iteration 14700: Loss = -11467.449754424582
3
Iteration 14800: Loss = -11467.440220266479
Iteration 14900: Loss = -11467.440655809949
1
Iteration 15000: Loss = -11467.440221050794
Iteration 15100: Loss = -11467.440407692182
1
Iteration 15200: Loss = -11467.443474889791
2
Iteration 15300: Loss = -11467.467500852663
3
Iteration 15400: Loss = -11467.466411474943
4
Iteration 15500: Loss = -11467.440899820258
5
Iteration 15600: Loss = -11467.441447922212
6
Iteration 15700: Loss = -11467.4939072313
7
Iteration 15800: Loss = -11467.441518986961
8
Iteration 15900: Loss = -11467.440338812225
9
Iteration 16000: Loss = -11467.440451598573
10
Iteration 16100: Loss = -11467.450567660595
11
Iteration 16200: Loss = -11467.536393433576
12
Iteration 16300: Loss = -11467.450610616743
13
Iteration 16400: Loss = -11467.527732374949
14
Iteration 16500: Loss = -11467.454425998725
15
Stopping early at iteration 16500 due to no improvement.
pi: tensor([[0.7867, 0.2133],
        [0.1908, 0.8092]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3915, 0.6085], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2106, 0.1032],
         [0.6469, 0.3028]],

        [[0.5606, 0.0986],
         [0.5114, 0.6151]],

        [[0.5712, 0.1102],
         [0.7015, 0.5494]],

        [[0.5986, 0.0876],
         [0.6758, 0.6927]],

        [[0.6055, 0.1040],
         [0.5818, 0.5522]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9206887570795217
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446610199953188
Average Adjusted Rand Index: 0.9446173447900825
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21399.210298488364
Iteration 100: Loss = -11874.23621384062
Iteration 200: Loss = -11866.767543762175
Iteration 300: Loss = -11747.501044455577
Iteration 400: Loss = -11520.127914316468
Iteration 500: Loss = -11501.114802870728
Iteration 600: Loss = -11485.636626139336
Iteration 700: Loss = -11480.760136805953
Iteration 800: Loss = -11468.246434354965
Iteration 900: Loss = -11467.874791670076
Iteration 1000: Loss = -11467.824732875926
Iteration 1100: Loss = -11467.790145482302
Iteration 1200: Loss = -11467.76422229656
Iteration 1300: Loss = -11467.744228834625
Iteration 1400: Loss = -11467.729081264131
Iteration 1500: Loss = -11467.717977977733
Iteration 1600: Loss = -11467.708898421024
Iteration 1700: Loss = -11467.70037826236
Iteration 1800: Loss = -11467.683300463868
Iteration 1900: Loss = -11467.53744230145
Iteration 2000: Loss = -11467.532440870365
Iteration 2100: Loss = -11467.528576570076
Iteration 2200: Loss = -11467.525376256024
Iteration 2300: Loss = -11467.522486448746
Iteration 2400: Loss = -11467.519916397978
Iteration 2500: Loss = -11467.51749460555
Iteration 2600: Loss = -11467.51502119351
Iteration 2700: Loss = -11467.512267838203
Iteration 2800: Loss = -11467.50960015446
Iteration 2900: Loss = -11467.504338643168
Iteration 3000: Loss = -11467.497948605347
Iteration 3100: Loss = -11467.497694401362
Iteration 3200: Loss = -11467.495321162332
Iteration 3300: Loss = -11467.493724588843
Iteration 3400: Loss = -11467.483145390543
Iteration 3500: Loss = -11467.463947059587
Iteration 3600: Loss = -11467.463063902213
Iteration 3700: Loss = -11467.46202936088
Iteration 3800: Loss = -11467.461361857868
Iteration 3900: Loss = -11467.460701855125
Iteration 4000: Loss = -11467.460029772968
Iteration 4100: Loss = -11467.459524789701
Iteration 4200: Loss = -11467.458377890425
Iteration 4300: Loss = -11467.457117194434
Iteration 4400: Loss = -11467.457005031181
Iteration 4500: Loss = -11467.455598367937
Iteration 4600: Loss = -11467.454967823189
Iteration 4700: Loss = -11467.454974221235
Iteration 4800: Loss = -11467.453977514922
Iteration 4900: Loss = -11467.453535694174
Iteration 5000: Loss = -11467.45332571111
Iteration 5100: Loss = -11467.455561118835
1
Iteration 5200: Loss = -11467.45286589144
Iteration 5300: Loss = -11467.453664307664
1
Iteration 5400: Loss = -11467.452299569872
Iteration 5500: Loss = -11467.452310532468
Iteration 5600: Loss = -11467.452037442226
Iteration 5700: Loss = -11467.451831689048
Iteration 5800: Loss = -11467.451600499551
Iteration 5900: Loss = -11467.451427530905
Iteration 6000: Loss = -11467.45119727954
Iteration 6100: Loss = -11467.450955060478
Iteration 6200: Loss = -11467.450696539121
Iteration 6300: Loss = -11467.451805693605
1
Iteration 6400: Loss = -11467.45057370601
Iteration 6500: Loss = -11467.45014379849
Iteration 6600: Loss = -11467.44997461692
Iteration 6700: Loss = -11467.450789526905
1
Iteration 6800: Loss = -11467.449676887922
Iteration 6900: Loss = -11467.44959838731
Iteration 7000: Loss = -11467.44950121876
Iteration 7100: Loss = -11467.449443457463
Iteration 7200: Loss = -11467.450308035899
1
Iteration 7300: Loss = -11467.449980509367
2
Iteration 7400: Loss = -11467.450260341628
3
Iteration 7500: Loss = -11467.449443737933
Iteration 7600: Loss = -11467.4488997326
Iteration 7700: Loss = -11467.481927829003
1
Iteration 7800: Loss = -11467.448743889181
Iteration 7900: Loss = -11467.448767091737
Iteration 8000: Loss = -11467.449967250124
1
Iteration 8100: Loss = -11467.508045287776
2
Iteration 8200: Loss = -11467.453142662085
3
Iteration 8300: Loss = -11467.449786085743
4
Iteration 8400: Loss = -11467.44880819121
Iteration 8500: Loss = -11467.448718229845
Iteration 8600: Loss = -11467.448501477571
Iteration 8700: Loss = -11467.450586074781
1
Iteration 8800: Loss = -11467.448775467928
2
Iteration 8900: Loss = -11467.449916243651
3
Iteration 9000: Loss = -11467.448438513742
Iteration 9100: Loss = -11467.448913198234
1
Iteration 9200: Loss = -11467.457763119368
2
Iteration 9300: Loss = -11467.44842667265
Iteration 9400: Loss = -11467.446841540821
Iteration 9500: Loss = -11467.510801426877
1
Iteration 9600: Loss = -11467.45151879194
2
Iteration 9700: Loss = -11467.448236983619
3
Iteration 9800: Loss = -11467.464064032243
4
Iteration 9900: Loss = -11467.448097450575
5
Iteration 10000: Loss = -11467.448207382939
6
Iteration 10100: Loss = -11467.445880108686
Iteration 10200: Loss = -11467.44547977153
Iteration 10300: Loss = -11467.445561812563
Iteration 10400: Loss = -11467.445696879464
1
Iteration 10500: Loss = -11467.451898966632
2
Iteration 10600: Loss = -11467.445051446597
Iteration 10700: Loss = -11467.445119352175
Iteration 10800: Loss = -11467.447858357868
1
Iteration 10900: Loss = -11467.445081096717
Iteration 11000: Loss = -11467.47612470367
1
Iteration 11100: Loss = -11467.497844530168
2
Iteration 11200: Loss = -11467.44504578767
Iteration 11300: Loss = -11467.445095933606
Iteration 11400: Loss = -11467.44588026752
1
Iteration 11500: Loss = -11467.470105591738
2
Iteration 11600: Loss = -11467.445072985649
Iteration 11700: Loss = -11467.51341070629
1
Iteration 11800: Loss = -11467.446602234771
2
Iteration 11900: Loss = -11467.444792521519
Iteration 12000: Loss = -11467.444998112678
1
Iteration 12100: Loss = -11467.446699774693
2
Iteration 12200: Loss = -11467.445245600142
3
Iteration 12300: Loss = -11467.444641574695
Iteration 12400: Loss = -11467.445297349152
1
Iteration 12500: Loss = -11467.44622035616
2
Iteration 12600: Loss = -11467.491918607006
3
Iteration 12700: Loss = -11467.4481399015
4
Iteration 12800: Loss = -11467.47207119114
5
Iteration 12900: Loss = -11467.460013966624
6
Iteration 13000: Loss = -11467.453342277313
7
Iteration 13100: Loss = -11467.442828886085
Iteration 13200: Loss = -11467.49338409061
1
Iteration 13300: Loss = -11467.445737926819
2
Iteration 13400: Loss = -11467.442754674981
Iteration 13500: Loss = -11467.443392233585
1
Iteration 13600: Loss = -11467.446022382252
2
Iteration 13700: Loss = -11467.444846858089
3
Iteration 13800: Loss = -11467.442406666218
Iteration 13900: Loss = -11467.446278205563
1
Iteration 14000: Loss = -11467.444505404133
2
Iteration 14100: Loss = -11467.479056608901
3
Iteration 14200: Loss = -11467.44290696665
4
Iteration 14300: Loss = -11467.448369455069
5
Iteration 14400: Loss = -11467.442381768242
Iteration 14500: Loss = -11467.44247415267
Iteration 14600: Loss = -11467.444862982713
1
Iteration 14700: Loss = -11467.442358993354
Iteration 14800: Loss = -11467.442377853538
Iteration 14900: Loss = -11467.442754990549
1
Iteration 15000: Loss = -11467.442362788928
Iteration 15100: Loss = -11467.44289431338
1
Iteration 15200: Loss = -11467.466811580454
2
Iteration 15300: Loss = -11467.453041741595
3
Iteration 15400: Loss = -11467.44894911389
4
Iteration 15500: Loss = -11467.445671220541
5
Iteration 15600: Loss = -11467.442426965301
Iteration 15700: Loss = -11467.507255279299
1
Iteration 15800: Loss = -11467.441265140978
Iteration 15900: Loss = -11467.443144832472
1
Iteration 16000: Loss = -11467.440959473668
Iteration 16100: Loss = -11467.489222931637
1
Iteration 16200: Loss = -11467.443007917844
2
Iteration 16300: Loss = -11467.441841545344
3
Iteration 16400: Loss = -11467.442041197579
4
Iteration 16500: Loss = -11467.45001980095
5
Iteration 16600: Loss = -11467.441048856888
Iteration 16700: Loss = -11467.440933658265
Iteration 16800: Loss = -11467.451508756263
1
Iteration 16900: Loss = -11467.440901291098
Iteration 17000: Loss = -11467.550792067355
1
Iteration 17100: Loss = -11467.444429431378
2
Iteration 17200: Loss = -11467.45134722179
3
Iteration 17300: Loss = -11467.443507483778
4
Iteration 17400: Loss = -11467.440852392652
Iteration 17500: Loss = -11467.44284394289
1
Iteration 17600: Loss = -11467.459583189275
2
Iteration 17700: Loss = -11467.441103585166
3
Iteration 17800: Loss = -11467.441627414026
4
Iteration 17900: Loss = -11467.443859695219
5
Iteration 18000: Loss = -11467.441619400524
6
Iteration 18100: Loss = -11467.441583815462
7
Iteration 18200: Loss = -11467.44094526624
Iteration 18300: Loss = -11467.441332977814
1
Iteration 18400: Loss = -11467.471394494429
2
Iteration 18500: Loss = -11467.451267258843
3
Iteration 18600: Loss = -11467.44123926141
4
Iteration 18700: Loss = -11467.440883518104
Iteration 18800: Loss = -11467.441710424297
1
Iteration 18900: Loss = -11467.440941023873
Iteration 19000: Loss = -11467.440867652056
Iteration 19100: Loss = -11467.473042843672
1
Iteration 19200: Loss = -11467.440662348205
Iteration 19300: Loss = -11467.441819834867
1
Iteration 19400: Loss = -11467.44066103426
Iteration 19500: Loss = -11467.446285829014
1
Iteration 19600: Loss = -11467.440669765645
Iteration 19700: Loss = -11467.456721894388
1
Iteration 19800: Loss = -11467.44220435656
2
Iteration 19900: Loss = -11467.449802362955
3
pi: tensor([[0.7865, 0.2135],
        [0.1913, 0.8087]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3934, 0.6066], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2101, 0.1036],
         [0.7236, 0.3035]],

        [[0.7148, 0.0989],
         [0.5974, 0.6356]],

        [[0.6634, 0.1105],
         [0.5804, 0.5021]],

        [[0.6991, 0.0879],
         [0.5768, 0.6387]],

        [[0.6955, 0.1043],
         [0.7136, 0.5427]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9206887570795217
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446610199953188
Average Adjusted Rand Index: 0.9446173447900825
11494.54111699616
[0.9446610199953188, 0.9446610199953188] [0.9446173447900825, 0.9446173447900825] [11467.454425998725, 11467.440913200218]
-------------------------------------
This iteration is 41
True Objective function: Loss = -11347.452815894521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21331.842332821612
Iteration 100: Loss = -11724.373747830698
Iteration 200: Loss = -11722.485136318433
Iteration 300: Loss = -11719.240444321124
Iteration 400: Loss = -11698.922290733824
Iteration 500: Loss = -11443.439503152884
Iteration 600: Loss = -11343.451026848921
Iteration 700: Loss = -11341.919136381684
Iteration 800: Loss = -11337.100795268165
Iteration 900: Loss = -11336.655277922338
Iteration 1000: Loss = -11327.162443577216
Iteration 1100: Loss = -11326.922835558962
Iteration 1200: Loss = -11326.865583239156
Iteration 1300: Loss = -11326.818150250221
Iteration 1400: Loss = -11326.791926926913
Iteration 1500: Loss = -11326.77324191923
Iteration 1600: Loss = -11326.758597464363
Iteration 1700: Loss = -11326.746797897073
Iteration 1800: Loss = -11326.736843129896
Iteration 1900: Loss = -11326.727945169527
Iteration 2000: Loss = -11326.718717108217
Iteration 2100: Loss = -11326.710013418571
Iteration 2200: Loss = -11326.7045312118
Iteration 2300: Loss = -11326.699212920048
Iteration 2400: Loss = -11326.693732765523
Iteration 2500: Loss = -11326.68931030846
Iteration 2600: Loss = -11326.685083807091
Iteration 2700: Loss = -11326.67614388142
Iteration 2800: Loss = -11326.666710143203
Iteration 2900: Loss = -11326.658431758906
Iteration 3000: Loss = -11326.655650851006
Iteration 3100: Loss = -11326.665488169021
1
Iteration 3200: Loss = -11326.651999359196
Iteration 3300: Loss = -11326.650138502468
Iteration 3400: Loss = -11326.649264824686
Iteration 3500: Loss = -11326.648570866388
Iteration 3600: Loss = -11326.646874624601
Iteration 3700: Loss = -11326.65198966807
1
Iteration 3800: Loss = -11326.644591354434
Iteration 3900: Loss = -11326.643771929172
Iteration 4000: Loss = -11326.643140089476
Iteration 4100: Loss = -11326.642486332046
Iteration 4200: Loss = -11326.641939884252
Iteration 4300: Loss = -11326.648293212258
1
Iteration 4400: Loss = -11326.640853189045
Iteration 4500: Loss = -11326.640389038083
Iteration 4600: Loss = -11326.641730167612
1
Iteration 4700: Loss = -11326.639533231608
Iteration 4800: Loss = -11326.642624797041
1
Iteration 4900: Loss = -11326.643755549161
2
Iteration 5000: Loss = -11326.638401729122
Iteration 5100: Loss = -11326.63799955997
Iteration 5200: Loss = -11326.638755824295
1
Iteration 5300: Loss = -11326.637476488608
Iteration 5400: Loss = -11326.63658567424
Iteration 5500: Loss = -11326.636178137085
Iteration 5600: Loss = -11326.635618158574
Iteration 5700: Loss = -11326.63463081374
Iteration 5800: Loss = -11326.634012565144
Iteration 5900: Loss = -11326.640934280209
1
Iteration 6000: Loss = -11326.628016162776
Iteration 6100: Loss = -11326.630400943764
1
Iteration 6200: Loss = -11326.627395288317
Iteration 6300: Loss = -11326.629639537146
1
Iteration 6400: Loss = -11326.627277161275
Iteration 6500: Loss = -11326.626957039642
Iteration 6600: Loss = -11326.626888836867
Iteration 6700: Loss = -11326.626784330007
Iteration 6800: Loss = -11326.630911712116
1
Iteration 6900: Loss = -11326.627667358774
2
Iteration 7000: Loss = -11326.626403485192
Iteration 7100: Loss = -11326.626349897091
Iteration 7200: Loss = -11326.626333287457
Iteration 7300: Loss = -11326.62738150809
1
Iteration 7400: Loss = -11326.626694632276
2
Iteration 7500: Loss = -11326.625503200288
Iteration 7600: Loss = -11326.603777707998
Iteration 7700: Loss = -11326.606983966312
1
Iteration 7800: Loss = -11326.603612738494
Iteration 7900: Loss = -11326.603570591848
Iteration 8000: Loss = -11326.603519861888
Iteration 8100: Loss = -11326.603419875855
Iteration 8200: Loss = -11326.68100342162
1
Iteration 8300: Loss = -11326.603340156636
Iteration 8400: Loss = -11326.603312531925
Iteration 8500: Loss = -11326.71826390911
1
Iteration 8600: Loss = -11326.603162888789
Iteration 8700: Loss = -11326.603171531868
Iteration 8800: Loss = -11326.604262493078
1
Iteration 8900: Loss = -11326.593694747646
Iteration 9000: Loss = -11326.593754556521
Iteration 9100: Loss = -11326.59359952972
Iteration 9200: Loss = -11326.593636108879
Iteration 9300: Loss = -11326.593562032403
Iteration 9400: Loss = -11326.593588249501
Iteration 9500: Loss = -11326.593443858672
Iteration 9600: Loss = -11326.594882878426
1
Iteration 9700: Loss = -11326.599896330486
2
Iteration 9800: Loss = -11326.690830860034
3
Iteration 9900: Loss = -11326.593106017384
Iteration 10000: Loss = -11326.594931979727
1
Iteration 10100: Loss = -11326.672005186312
2
Iteration 10200: Loss = -11326.594074580296
3
Iteration 10300: Loss = -11326.598662011804
4
Iteration 10400: Loss = -11326.647551297545
5
Iteration 10500: Loss = -11326.60400210646
6
Iteration 10600: Loss = -11326.592657275274
Iteration 10700: Loss = -11326.594007503545
1
Iteration 10800: Loss = -11326.593589110522
2
Iteration 10900: Loss = -11326.595535216326
3
Iteration 11000: Loss = -11326.592513936112
Iteration 11100: Loss = -11326.59410877904
1
Iteration 11200: Loss = -11326.592478583007
Iteration 11300: Loss = -11326.609737202394
1
Iteration 11400: Loss = -11326.593023673204
2
Iteration 11500: Loss = -11326.592595807066
3
Iteration 11600: Loss = -11326.59242980943
Iteration 11700: Loss = -11326.597531929587
1
Iteration 11800: Loss = -11326.592601926042
2
Iteration 11900: Loss = -11326.593534005879
3
Iteration 12000: Loss = -11326.59508688171
4
Iteration 12100: Loss = -11326.593098751155
5
Iteration 12200: Loss = -11326.59276721142
6
Iteration 12300: Loss = -11326.59260327724
7
Iteration 12400: Loss = -11326.60102562006
8
Iteration 12500: Loss = -11326.681892188542
9
Iteration 12600: Loss = -11326.584774326631
Iteration 12700: Loss = -11326.584419177516
Iteration 12800: Loss = -11326.596375855692
1
Iteration 12900: Loss = -11326.58438498668
Iteration 13000: Loss = -11326.584730273868
1
Iteration 13100: Loss = -11326.586330712944
2
Iteration 13200: Loss = -11326.600011117094
3
Iteration 13300: Loss = -11326.584692633918
4
Iteration 13400: Loss = -11326.584769384343
5
Iteration 13500: Loss = -11326.585041596254
6
Iteration 13600: Loss = -11326.5852368743
7
Iteration 13700: Loss = -11326.595375152636
8
Iteration 13800: Loss = -11326.5953910864
9
Iteration 13900: Loss = -11326.584542113793
10
Iteration 14000: Loss = -11326.584456147664
Iteration 14100: Loss = -11326.692048595489
1
Iteration 14200: Loss = -11326.584330037627
Iteration 14300: Loss = -11326.587888352482
1
Iteration 14400: Loss = -11326.584321343338
Iteration 14500: Loss = -11326.58921092317
1
Iteration 14600: Loss = -11326.584238778856
Iteration 14700: Loss = -11326.584694682264
1
Iteration 14800: Loss = -11326.584215114079
Iteration 14900: Loss = -11326.584078788355
Iteration 15000: Loss = -11326.612816892504
1
Iteration 15100: Loss = -11326.584351497744
2
Iteration 15200: Loss = -11326.584165375054
Iteration 15300: Loss = -11326.596403490019
1
Iteration 15400: Loss = -11326.58392098232
Iteration 15500: Loss = -11326.585405964594
1
Iteration 15600: Loss = -11326.636314269654
2
Iteration 15700: Loss = -11326.58482481719
3
Iteration 15800: Loss = -11326.583808481491
Iteration 15900: Loss = -11326.584316714223
1
Iteration 16000: Loss = -11326.592263120869
2
Iteration 16100: Loss = -11326.58377435145
Iteration 16200: Loss = -11326.583942667834
1
Iteration 16300: Loss = -11326.601237701308
2
Iteration 16400: Loss = -11326.583772773325
Iteration 16500: Loss = -11326.583772860864
Iteration 16600: Loss = -11326.585178234362
1
Iteration 16700: Loss = -11326.58376837315
Iteration 16800: Loss = -11326.81163939738
1
Iteration 16900: Loss = -11326.583764849183
Iteration 17000: Loss = -11326.587894760765
1
Iteration 17100: Loss = -11326.591388591263
2
Iteration 17200: Loss = -11326.58377181308
Iteration 17300: Loss = -11326.586686720786
1
Iteration 17400: Loss = -11326.585284892206
2
Iteration 17500: Loss = -11326.597224111472
3
Iteration 17600: Loss = -11326.613229856865
4
Iteration 17700: Loss = -11326.583856696556
Iteration 17800: Loss = -11326.586499104003
1
Iteration 17900: Loss = -11326.617336876048
2
Iteration 18000: Loss = -11326.583696807591
Iteration 18100: Loss = -11326.610610815418
1
Iteration 18200: Loss = -11326.601527746447
2
Iteration 18300: Loss = -11326.583637053385
Iteration 18400: Loss = -11326.584259787636
1
Iteration 18500: Loss = -11326.657220627454
2
Iteration 18600: Loss = -11326.583643721759
Iteration 18700: Loss = -11326.58512445367
1
Iteration 18800: Loss = -11326.586393167729
2
Iteration 18900: Loss = -11326.584142606991
3
Iteration 19000: Loss = -11326.589963351558
4
Iteration 19100: Loss = -11326.593812616824
5
Iteration 19200: Loss = -11326.584978834497
6
Iteration 19300: Loss = -11326.583700578127
Iteration 19400: Loss = -11326.585054160778
1
Iteration 19500: Loss = -11326.668291250493
2
Iteration 19600: Loss = -11326.583633329334
Iteration 19700: Loss = -11326.589538764896
1
Iteration 19800: Loss = -11326.585864085595
2
Iteration 19900: Loss = -11326.58383370678
3
pi: tensor([[0.7678, 0.2322],
        [0.2583, 0.7417]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6383, 0.3617], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2943, 0.0873],
         [0.6017, 0.2099]],

        [[0.7203, 0.1056],
         [0.5728, 0.5964]],

        [[0.5568, 0.0940],
         [0.5710, 0.7101]],

        [[0.5953, 0.0982],
         [0.6416, 0.6235]],

        [[0.6638, 0.0921],
         [0.7086, 0.6746]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9598385576399676
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9681825115148831
Average Adjusted Rand Index: 0.9681267475433263
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23468.613326736366
Iteration 100: Loss = -11756.623792810211
Iteration 200: Loss = -11731.421521058403
Iteration 300: Loss = -11716.18997798614
Iteration 400: Loss = -11715.578517734195
Iteration 500: Loss = -11715.318170079494
Iteration 600: Loss = -11715.231274621161
Iteration 700: Loss = -11715.182282115235
Iteration 800: Loss = -11715.151900538118
Iteration 900: Loss = -11715.132113968757
Iteration 1000: Loss = -11715.118343465367
Iteration 1100: Loss = -11715.10814332041
Iteration 1200: Loss = -11715.10031277716
Iteration 1300: Loss = -11715.094030052087
Iteration 1400: Loss = -11715.088700509159
Iteration 1500: Loss = -11715.084022177469
Iteration 1600: Loss = -11715.079770652444
Iteration 1700: Loss = -11715.075833487832
Iteration 1800: Loss = -11715.071807910175
Iteration 1900: Loss = -11715.068453223115
Iteration 2000: Loss = -11715.062436994758
Iteration 2100: Loss = -11715.05592914165
Iteration 2200: Loss = -11715.045459943647
Iteration 2300: Loss = -11715.02349080434
Iteration 2400: Loss = -11714.968994785915
Iteration 2500: Loss = -11714.881196488226
Iteration 2600: Loss = -11714.798301357127
Iteration 2700: Loss = -11714.710280383108
Iteration 2800: Loss = -11714.603369049646
Iteration 2900: Loss = -11714.150334562753
Iteration 3000: Loss = -11714.111716586292
Iteration 3100: Loss = -11714.092283885095
Iteration 3200: Loss = -11714.081136645918
Iteration 3300: Loss = -11714.073252880655
Iteration 3400: Loss = -11714.06811894766
Iteration 3500: Loss = -11714.062876798043
Iteration 3600: Loss = -11714.056191191665
Iteration 3700: Loss = -11714.052301618718
Iteration 3800: Loss = -11714.049236444383
Iteration 3900: Loss = -11714.046074982458
Iteration 4000: Loss = -11714.043402559124
Iteration 4100: Loss = -11714.041873123682
Iteration 4200: Loss = -11714.041227693702
Iteration 4300: Loss = -11714.039838389795
Iteration 4400: Loss = -11714.046598054994
1
Iteration 4500: Loss = -11714.03907100425
Iteration 4600: Loss = -11714.038910588357
Iteration 4700: Loss = -11714.038950694878
Iteration 4800: Loss = -11714.038726749473
Iteration 4900: Loss = -11714.038663382344
Iteration 5000: Loss = -11714.03858575537
Iteration 5100: Loss = -11714.040518653717
1
Iteration 5200: Loss = -11714.038406723388
Iteration 5300: Loss = -11714.0383568578
Iteration 5400: Loss = -11714.038225756045
Iteration 5500: Loss = -11714.038284163285
Iteration 5600: Loss = -11714.037902090997
Iteration 5700: Loss = -11714.043641475942
1
Iteration 5800: Loss = -11714.037358668855
Iteration 5900: Loss = -11714.036962292897
Iteration 6000: Loss = -11714.036725218997
Iteration 6100: Loss = -11714.040349768984
1
Iteration 6200: Loss = -11714.03730075226
2
Iteration 6300: Loss = -11714.037450885005
3
Iteration 6400: Loss = -11714.038020192753
4
Iteration 6500: Loss = -11714.036885105164
5
Iteration 6600: Loss = -11714.0365873142
Iteration 6700: Loss = -11714.036574213054
Iteration 6800: Loss = -11714.03791287706
1
Iteration 6900: Loss = -11714.0365284517
Iteration 7000: Loss = -11714.03667207725
1
Iteration 7100: Loss = -11714.036790913522
2
Iteration 7200: Loss = -11714.03655947173
Iteration 7300: Loss = -11714.0395442432
1
Iteration 7400: Loss = -11714.036501162975
Iteration 7500: Loss = -11714.036745569556
1
Iteration 7600: Loss = -11714.036488959326
Iteration 7700: Loss = -11714.039117842809
1
Iteration 7800: Loss = -11714.036472555721
Iteration 7900: Loss = -11714.099964158124
1
Iteration 8000: Loss = -11714.036482652922
Iteration 8100: Loss = -11714.036456084912
Iteration 8200: Loss = -11714.036486533238
Iteration 8300: Loss = -11714.036455576512
Iteration 8400: Loss = -11714.036476425907
Iteration 8500: Loss = -11714.036752366274
1
Iteration 8600: Loss = -11714.036434915048
Iteration 8700: Loss = -11714.036485904518
Iteration 8800: Loss = -11714.03762380484
1
Iteration 8900: Loss = -11714.036435380707
Iteration 9000: Loss = -11714.036483465648
Iteration 9100: Loss = -11714.036503116033
Iteration 9200: Loss = -11714.036440955862
Iteration 9300: Loss = -11714.467420690451
1
Iteration 9400: Loss = -11714.036425622295
Iteration 9500: Loss = -11714.036416053694
Iteration 9600: Loss = -11714.062217047582
1
Iteration 9700: Loss = -11714.03643100303
Iteration 9800: Loss = -11714.036428430674
Iteration 9900: Loss = -11714.043098875489
1
Iteration 10000: Loss = -11714.036425249313
Iteration 10100: Loss = -11714.036441700384
Iteration 10200: Loss = -11714.036873586243
1
Iteration 10300: Loss = -11714.036432824752
Iteration 10400: Loss = -11714.03642688601
Iteration 10500: Loss = -11714.036423779738
Iteration 10600: Loss = -11714.036402238462
Iteration 10700: Loss = -11714.03649328149
Iteration 10800: Loss = -11714.036429024567
Iteration 10900: Loss = -11714.036400453248
Iteration 11000: Loss = -11714.03659165929
1
Iteration 11100: Loss = -11714.036442217537
Iteration 11200: Loss = -11714.036422486051
Iteration 11300: Loss = -11714.083878310588
1
Iteration 11400: Loss = -11714.036435951113
Iteration 11500: Loss = -11714.036408899789
Iteration 11600: Loss = -11714.036930284967
1
Iteration 11700: Loss = -11714.036451141475
Iteration 11800: Loss = -11714.036529252233
Iteration 11900: Loss = -11714.036511828168
Iteration 12000: Loss = -11714.036423614967
Iteration 12100: Loss = -11714.289311930466
1
Iteration 12200: Loss = -11714.036399623159
Iteration 12300: Loss = -11714.036413316428
Iteration 12400: Loss = -11714.049448856747
1
Iteration 12500: Loss = -11714.036452457141
Iteration 12600: Loss = -11714.036445949896
Iteration 12700: Loss = -11714.03648455167
Iteration 12800: Loss = -11714.091103836377
1
Iteration 12900: Loss = -11714.0364099765
Iteration 13000: Loss = -11714.045469914225
1
Iteration 13100: Loss = -11714.036391282581
Iteration 13200: Loss = -11714.054901082647
1
Iteration 13300: Loss = -11714.03643141478
Iteration 13400: Loss = -11714.036407736994
Iteration 13500: Loss = -11714.037087452465
1
Iteration 13600: Loss = -11714.036500367069
Iteration 13700: Loss = -11714.03639194181
Iteration 13800: Loss = -11714.037344676703
1
Iteration 13900: Loss = -11714.036433272802
Iteration 14000: Loss = -11714.078723960285
1
Iteration 14100: Loss = -11714.036408151815
Iteration 14200: Loss = -11714.03640927468
Iteration 14300: Loss = -11714.05901054732
1
Iteration 14400: Loss = -11714.036386699237
Iteration 14500: Loss = -11714.036427554074
Iteration 14600: Loss = -11714.036650808855
1
Iteration 14700: Loss = -11714.03921537656
2
Iteration 14800: Loss = -11714.03639170643
Iteration 14900: Loss = -11714.037142482786
1
Iteration 15000: Loss = -11714.036392146609
Iteration 15100: Loss = -11714.048556396716
1
Iteration 15200: Loss = -11714.036410819557
Iteration 15300: Loss = -11714.036380288348
Iteration 15400: Loss = -11714.037816569622
1
Iteration 15500: Loss = -11714.036392495385
Iteration 15600: Loss = -11714.060839165904
1
Iteration 15700: Loss = -11714.036426676234
Iteration 15800: Loss = -11714.036414899052
Iteration 15900: Loss = -11714.04453541123
1
Iteration 16000: Loss = -11714.036419830785
Iteration 16100: Loss = -11714.036420775188
Iteration 16200: Loss = -11714.036481218774
Iteration 16300: Loss = -11714.036407921769
Iteration 16400: Loss = -11714.036398015309
Iteration 16500: Loss = -11714.036404583265
Iteration 16600: Loss = -11714.036367875811
Iteration 16700: Loss = -11714.037928356762
1
Iteration 16800: Loss = -11714.036409650973
Iteration 16900: Loss = -11714.036386882037
Iteration 17000: Loss = -11714.036605668138
1
Iteration 17100: Loss = -11714.036380200303
Iteration 17200: Loss = -11714.109677812286
1
Iteration 17300: Loss = -11714.036418952617
Iteration 17400: Loss = -11714.036367623568
Iteration 17500: Loss = -11714.04734491283
1
Iteration 17600: Loss = -11714.036388736238
Iteration 17700: Loss = -11714.036349716327
Iteration 17800: Loss = -11714.059152245161
1
Iteration 17900: Loss = -11714.0364242985
Iteration 18000: Loss = -11714.036389316425
Iteration 18100: Loss = -11714.037395985104
1
Iteration 18200: Loss = -11714.036361485743
Iteration 18300: Loss = -11714.036407769381
Iteration 18400: Loss = -11714.205396489757
1
Iteration 18500: Loss = -11714.036417091274
Iteration 18600: Loss = -11714.036375612362
Iteration 18700: Loss = -11714.05937631694
1
Iteration 18800: Loss = -11714.036441377844
Iteration 18900: Loss = -11714.036380927266
Iteration 19000: Loss = -11714.037910709865
1
Iteration 19100: Loss = -11714.03641400091
Iteration 19200: Loss = -11714.526659710458
1
Iteration 19300: Loss = -11714.036453212657
Iteration 19400: Loss = -11714.036419694383
Iteration 19500: Loss = -11714.03996257459
1
Iteration 19600: Loss = -11714.036399112736
Iteration 19700: Loss = -11714.036390882575
Iteration 19800: Loss = -11714.036597036698
1
Iteration 19900: Loss = -11714.036400026453
pi: tensor([[0.0965, 0.9035],
        [0.0329, 0.9671]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9500, 0.0500], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1771, 0.2478],
         [0.6611, 0.1724]],

        [[0.5916, 0.2462],
         [0.6014, 0.5763]],

        [[0.7005, 0.2537],
         [0.7302, 0.5440]],

        [[0.6318, 0.2752],
         [0.5870, 0.5079]],

        [[0.6175, 0.2988],
         [0.6883, 0.7220]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.014778186472389411
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.014100938522947548
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -0.006982078651696003
Average Adjusted Rand Index: -0.007166201572161112
11347.452815894521
[0.9681825115148831, -0.006982078651696003] [0.9681267475433263, -0.007166201572161112] [11326.587893771466, 11714.041088128395]
-------------------------------------
This iteration is 42
True Objective function: Loss = -11088.959857070342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22027.318969127537
Iteration 100: Loss = -11359.168304253542
Iteration 200: Loss = -11357.146744373114
Iteration 300: Loss = -11355.812409882898
Iteration 400: Loss = -11354.54329334575
Iteration 500: Loss = -11352.842512297737
Iteration 600: Loss = -11350.706222881005
Iteration 700: Loss = -11182.62576531251
Iteration 800: Loss = -11098.296500065777
Iteration 900: Loss = -11072.83961309887
Iteration 1000: Loss = -11072.470761107183
Iteration 1100: Loss = -11072.339816234264
Iteration 1200: Loss = -11072.270025439733
Iteration 1300: Loss = -11072.195378330423
Iteration 1400: Loss = -11072.019489366758
Iteration 1500: Loss = -11071.986510447206
Iteration 1600: Loss = -11071.96471602007
Iteration 1700: Loss = -11071.949173514053
Iteration 1800: Loss = -11071.922522233774
Iteration 1900: Loss = -11071.21210340381
Iteration 2000: Loss = -11071.1849106061
Iteration 2100: Loss = -11071.17830172633
Iteration 2200: Loss = -11071.170715093209
Iteration 2300: Loss = -11071.165168769468
Iteration 2400: Loss = -11071.161748714856
Iteration 2500: Loss = -11071.159424265523
Iteration 2600: Loss = -11071.156454977641
Iteration 2700: Loss = -11071.154330453457
Iteration 2800: Loss = -11071.152243682167
Iteration 2900: Loss = -11071.14900298097
Iteration 3000: Loss = -11071.143338324813
Iteration 3100: Loss = -11071.142288799407
Iteration 3200: Loss = -11071.141148948329
Iteration 3300: Loss = -11071.143049584858
1
Iteration 3400: Loss = -11071.140423532956
Iteration 3500: Loss = -11071.1386084777
Iteration 3600: Loss = -11071.144113191634
1
Iteration 3700: Loss = -11071.133154564068
Iteration 3800: Loss = -11071.132567370734
Iteration 3900: Loss = -11071.143973949967
1
Iteration 4000: Loss = -11071.131583916474
Iteration 4100: Loss = -11071.133893236962
1
Iteration 4200: Loss = -11071.130018958089
Iteration 4300: Loss = -11071.124433186693
Iteration 4400: Loss = -11071.12367824149
Iteration 4500: Loss = -11071.122925943662
Iteration 4600: Loss = -11071.122712222474
Iteration 4700: Loss = -11071.122721752388
Iteration 4800: Loss = -11071.122202656694
Iteration 4900: Loss = -11071.125142580144
1
Iteration 5000: Loss = -11071.121547502657
Iteration 5100: Loss = -11071.13148720683
1
Iteration 5200: Loss = -11071.121018062868
Iteration 5300: Loss = -11071.120879045056
Iteration 5400: Loss = -11071.120703121478
Iteration 5500: Loss = -11071.121529082986
1
Iteration 5600: Loss = -11071.120468468836
Iteration 5700: Loss = -11071.120332867016
Iteration 5800: Loss = -11071.120151810206
Iteration 5900: Loss = -11071.120106800226
Iteration 6000: Loss = -11071.12059756581
1
Iteration 6100: Loss = -11071.119531337166
Iteration 6200: Loss = -11071.119476924707
Iteration 6300: Loss = -11071.119270040532
Iteration 6400: Loss = -11071.119963509696
1
Iteration 6500: Loss = -11071.119089618562
Iteration 6600: Loss = -11071.120253391546
1
Iteration 6700: Loss = -11071.123188900194
2
Iteration 6800: Loss = -11071.160800456642
3
Iteration 6900: Loss = -11071.11973443233
4
Iteration 7000: Loss = -11071.118863291249
Iteration 7100: Loss = -11071.120454806669
1
Iteration 7200: Loss = -11071.118777511372
Iteration 7300: Loss = -11071.131376733976
1
Iteration 7400: Loss = -11071.118686099462
Iteration 7500: Loss = -11071.123202311383
1
Iteration 7600: Loss = -11071.118633962387
Iteration 7700: Loss = -11071.118630042964
Iteration 7800: Loss = -11071.118622065893
Iteration 7900: Loss = -11071.129069107126
1
Iteration 8000: Loss = -11071.118683804872
Iteration 8100: Loss = -11071.118755295205
Iteration 8200: Loss = -11071.118435564516
Iteration 8300: Loss = -11071.119175906097
1
Iteration 8400: Loss = -11071.125507567864
2
Iteration 8500: Loss = -11071.118642614298
3
Iteration 8600: Loss = -11071.1193476709
4
Iteration 8700: Loss = -11071.118393826584
Iteration 8800: Loss = -11071.11812146172
Iteration 8900: Loss = -11071.11810922727
Iteration 9000: Loss = -11071.118114341372
Iteration 9100: Loss = -11071.118672662853
1
Iteration 9200: Loss = -11071.118123109349
Iteration 9300: Loss = -11071.11809770113
Iteration 9400: Loss = -11071.124521713855
1
Iteration 9500: Loss = -11071.116071759845
Iteration 9600: Loss = -11071.115516595653
Iteration 9700: Loss = -11071.115829652555
1
Iteration 9800: Loss = -11071.119504932254
2
Iteration 9900: Loss = -11071.118630258617
3
Iteration 10000: Loss = -11071.115782022387
4
Iteration 10100: Loss = -11071.13033203101
5
Iteration 10200: Loss = -11071.18364777833
6
Iteration 10300: Loss = -11071.125982985466
7
Iteration 10400: Loss = -11071.116614444727
8
Iteration 10500: Loss = -11071.115851824708
9
Iteration 10600: Loss = -11071.116021105548
10
Iteration 10700: Loss = -11071.129053444956
11
Iteration 10800: Loss = -11071.116441082797
12
Iteration 10900: Loss = -11071.115319120423
Iteration 11000: Loss = -11071.16614546871
1
Iteration 11100: Loss = -11071.114971694953
Iteration 11200: Loss = -11071.18374026751
1
Iteration 11300: Loss = -11071.114950192106
Iteration 11400: Loss = -11071.118143905509
1
Iteration 11500: Loss = -11071.115382089272
2
Iteration 11600: Loss = -11071.115250728311
3
Iteration 11700: Loss = -11071.122581065052
4
Iteration 11800: Loss = -11071.121782806138
5
Iteration 11900: Loss = -11071.122199602365
6
Iteration 12000: Loss = -11071.119024832169
7
Iteration 12100: Loss = -11071.114865066422
Iteration 12200: Loss = -11071.114588053742
Iteration 12300: Loss = -11071.114528749977
Iteration 12400: Loss = -11071.114452223466
Iteration 12500: Loss = -11071.120986793443
1
Iteration 12600: Loss = -11071.118325720467
2
Iteration 12700: Loss = -11071.114584263154
3
Iteration 12800: Loss = -11071.134259438863
4
Iteration 12900: Loss = -11071.155323339273
5
Iteration 13000: Loss = -11071.11443048406
Iteration 13100: Loss = -11071.115346833341
1
Iteration 13200: Loss = -11071.152979589675
2
Iteration 13300: Loss = -11071.122543548201
3
Iteration 13400: Loss = -11071.114363988727
Iteration 13500: Loss = -11071.115729959723
1
Iteration 13600: Loss = -11071.119256818874
2
Iteration 13700: Loss = -11071.114385171128
Iteration 13800: Loss = -11071.116842218129
1
Iteration 13900: Loss = -11071.114623784086
2
Iteration 14000: Loss = -11071.11594356566
3
Iteration 14100: Loss = -11071.220955181438
4
Iteration 14200: Loss = -11071.121062005801
5
Iteration 14300: Loss = -11071.114112046003
Iteration 14400: Loss = -11071.115293207566
1
Iteration 14500: Loss = -11071.11483544568
2
Iteration 14600: Loss = -11071.11828541154
3
Iteration 14700: Loss = -11071.114201955425
Iteration 14800: Loss = -11071.114316988627
1
Iteration 14900: Loss = -11071.114125000495
Iteration 15000: Loss = -11071.115530286472
1
Iteration 15100: Loss = -11071.117340909846
2
Iteration 15200: Loss = -11071.11462648953
3
Iteration 15300: Loss = -11071.115914407972
4
Iteration 15400: Loss = -11071.186363860956
5
Iteration 15500: Loss = -11071.131092306934
6
Iteration 15600: Loss = -11071.11487458125
7
Iteration 15700: Loss = -11071.140036174875
8
Iteration 15800: Loss = -11071.127691022071
9
Iteration 15900: Loss = -11071.126590442427
10
Iteration 16000: Loss = -11071.121033110521
11
Iteration 16100: Loss = -11071.114701581308
12
Iteration 16200: Loss = -11071.114105279608
Iteration 16300: Loss = -11071.115942238297
1
Iteration 16400: Loss = -11071.11636569224
2
Iteration 16500: Loss = -11071.114282880179
3
Iteration 16600: Loss = -11071.114528037657
4
Iteration 16700: Loss = -11071.11928005191
5
Iteration 16800: Loss = -11071.12723953491
6
Iteration 16900: Loss = -11071.11593303416
7
Iteration 17000: Loss = -11071.126079950527
8
Iteration 17100: Loss = -11071.130971615325
9
Iteration 17200: Loss = -11071.118688303713
10
Iteration 17300: Loss = -11071.114103705908
Iteration 17400: Loss = -11071.114073685527
Iteration 17500: Loss = -11071.11443125375
1
Iteration 17600: Loss = -11071.114096503068
Iteration 17700: Loss = -11071.11493541851
1
Iteration 17800: Loss = -11071.119633665514
2
Iteration 17900: Loss = -11071.118208880383
3
Iteration 18000: Loss = -11071.349620110483
4
Iteration 18100: Loss = -11071.120698025024
5
Iteration 18200: Loss = -11071.116639406491
6
Iteration 18300: Loss = -11071.120596056897
7
Iteration 18400: Loss = -11071.124622835527
8
Iteration 18500: Loss = -11071.113807098822
Iteration 18600: Loss = -11071.114246914058
1
Iteration 18700: Loss = -11071.189198401034
2
Iteration 18800: Loss = -11071.113688342279
Iteration 18900: Loss = -11071.114081520584
1
Iteration 19000: Loss = -11071.119670388529
2
Iteration 19100: Loss = -11071.117929474347
3
Iteration 19200: Loss = -11071.114141831078
4
Iteration 19300: Loss = -11071.1140409447
5
Iteration 19400: Loss = -11071.116975475918
6
Iteration 19500: Loss = -11071.114771613496
7
Iteration 19600: Loss = -11071.115292190483
8
Iteration 19700: Loss = -11071.115139160656
9
Iteration 19800: Loss = -11071.118735283819
10
Iteration 19900: Loss = -11071.120410400252
11
pi: tensor([[0.7453, 0.2547],
        [0.1993, 0.8007]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5367, 0.4633], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2035, 0.1002],
         [0.6896, 0.2849]],

        [[0.6499, 0.1005],
         [0.5460, 0.5739]],

        [[0.6404, 0.0996],
         [0.6351, 0.6596]],

        [[0.5647, 0.0914],
         [0.5973, 0.5538]],

        [[0.6537, 0.0955],
         [0.5143, 0.6267]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
Global Adjusted Rand Index: 0.9446733196745221
Average Adjusted Rand Index: 0.9444796247934606
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23973.270980471007
Iteration 100: Loss = -11359.313433746753
Iteration 200: Loss = -11357.375428439014
Iteration 300: Loss = -11356.378445744738
Iteration 400: Loss = -11354.849886477963
Iteration 500: Loss = -11352.289904049287
Iteration 600: Loss = -11346.331097980858
Iteration 700: Loss = -11105.253337441658
Iteration 800: Loss = -11077.775062276294
Iteration 900: Loss = -11075.938225490298
Iteration 1000: Loss = -11075.728065558927
Iteration 1100: Loss = -11075.324422454101
Iteration 1200: Loss = -11075.25895626913
Iteration 1300: Loss = -11075.216804464277
Iteration 1400: Loss = -11075.182933660855
Iteration 1500: Loss = -11073.42106406611
Iteration 1600: Loss = -11073.389686550841
Iteration 1700: Loss = -11073.369121594644
Iteration 1800: Loss = -11071.359391646141
Iteration 1900: Loss = -11071.324259836469
Iteration 2000: Loss = -11071.309300996218
Iteration 2100: Loss = -11071.297410515781
Iteration 2200: Loss = -11071.290070874586
Iteration 2300: Loss = -11071.278880038624
Iteration 2400: Loss = -11071.224752654989
Iteration 2500: Loss = -11071.219434173085
Iteration 2600: Loss = -11071.21487020147
Iteration 2700: Loss = -11071.211335983533
Iteration 2800: Loss = -11071.208880184595
Iteration 2900: Loss = -11071.205784021251
Iteration 3000: Loss = -11071.202433267845
Iteration 3100: Loss = -11071.195998382276
Iteration 3200: Loss = -11071.189343982589
Iteration 3300: Loss = -11071.161535088495
Iteration 3400: Loss = -11071.135948793313
Iteration 3500: Loss = -11071.146768820045
1
Iteration 3600: Loss = -11071.13261339471
Iteration 3700: Loss = -11071.131650079833
Iteration 3800: Loss = -11071.133276543771
1
Iteration 3900: Loss = -11071.129390305137
Iteration 4000: Loss = -11071.128839628576
Iteration 4100: Loss = -11071.13396157625
1
Iteration 4200: Loss = -11071.127782391915
Iteration 4300: Loss = -11071.128158044945
1
Iteration 4400: Loss = -11071.126397787386
Iteration 4500: Loss = -11071.125884256715
Iteration 4600: Loss = -11071.126795320299
1
Iteration 4700: Loss = -11071.12461075181
Iteration 4800: Loss = -11071.12385223463
Iteration 4900: Loss = -11071.123755289596
Iteration 5000: Loss = -11071.12490355451
1
Iteration 5100: Loss = -11071.122661075955
Iteration 5200: Loss = -11071.122441448804
Iteration 5300: Loss = -11071.13197172149
1
Iteration 5400: Loss = -11071.122265997907
Iteration 5500: Loss = -11071.121341354246
Iteration 5600: Loss = -11071.121124806994
Iteration 5700: Loss = -11071.121172484718
Iteration 5800: Loss = -11071.126277593165
1
Iteration 5900: Loss = -11071.120554240244
Iteration 6000: Loss = -11071.120390652239
Iteration 6100: Loss = -11071.128186996866
1
Iteration 6200: Loss = -11071.120122870041
Iteration 6300: Loss = -11071.120017951569
Iteration 6400: Loss = -11071.120081764924
Iteration 6500: Loss = -11071.119704252005
Iteration 6600: Loss = -11071.119576314833
Iteration 6700: Loss = -11071.119446253608
Iteration 6800: Loss = -11071.122830244261
1
Iteration 6900: Loss = -11071.119086151371
Iteration 7000: Loss = -11071.119179462272
Iteration 7100: Loss = -11071.118786972174
Iteration 7200: Loss = -11071.121468772566
1
Iteration 7300: Loss = -11071.11862884711
Iteration 7400: Loss = -11071.118543172033
Iteration 7500: Loss = -11071.11853407019
Iteration 7600: Loss = -11071.118494638104
Iteration 7700: Loss = -11071.119766641566
1
Iteration 7800: Loss = -11071.118367910312
Iteration 7900: Loss = -11071.118608376637
1
Iteration 8000: Loss = -11071.136762601765
2
Iteration 8100: Loss = -11071.118250779005
Iteration 8200: Loss = -11071.12524402852
1
Iteration 8300: Loss = -11071.118166773718
Iteration 8400: Loss = -11071.137350962965
1
Iteration 8500: Loss = -11071.118119435978
Iteration 8600: Loss = -11071.118063536418
Iteration 8700: Loss = -11071.125957573402
1
Iteration 8800: Loss = -11071.118021770846
Iteration 8900: Loss = -11071.117994770259
Iteration 9000: Loss = -11071.166332418521
1
Iteration 9100: Loss = -11071.117924401886
Iteration 9200: Loss = -11071.117953513518
Iteration 9300: Loss = -11071.120171947701
1
Iteration 9400: Loss = -11071.117929274522
Iteration 9500: Loss = -11071.11789592019
Iteration 9600: Loss = -11071.185056359549
1
Iteration 9700: Loss = -11071.118927184623
2
Iteration 9800: Loss = -11071.128469007923
3
Iteration 9900: Loss = -11071.124248768352
4
Iteration 10000: Loss = -11071.125010342346
5
Iteration 10100: Loss = -11071.123202731404
6
Iteration 10200: Loss = -11071.157665830635
7
Iteration 10300: Loss = -11071.119676967843
8
Iteration 10400: Loss = -11071.115968192538
Iteration 10500: Loss = -11071.118518402052
1
Iteration 10600: Loss = -11071.117481630425
2
Iteration 10700: Loss = -11071.150426412007
3
Iteration 10800: Loss = -11071.192009892264
4
Iteration 10900: Loss = -11071.117326249427
5
Iteration 11000: Loss = -11071.115604199133
Iteration 11100: Loss = -11071.127676711983
1
Iteration 11200: Loss = -11071.115600388832
Iteration 11300: Loss = -11071.115714763746
1
Iteration 11400: Loss = -11071.11828916292
2
Iteration 11500: Loss = -11071.120663991922
3
Iteration 11600: Loss = -11071.216683159924
4
Iteration 11700: Loss = -11071.115007934832
Iteration 11800: Loss = -11071.114788315217
Iteration 11900: Loss = -11071.18517483157
1
Iteration 12000: Loss = -11071.123700198315
2
Iteration 12100: Loss = -11071.114597501602
Iteration 12200: Loss = -11071.120871878877
1
Iteration 12300: Loss = -11071.114837994503
2
Iteration 12400: Loss = -11071.145013841806
3
Iteration 12500: Loss = -11071.125404075416
4
Iteration 12600: Loss = -11071.11594618442
5
Iteration 12700: Loss = -11071.1165619332
6
Iteration 12800: Loss = -11071.120476377533
7
Iteration 12900: Loss = -11071.11562137617
8
Iteration 13000: Loss = -11071.123405294977
9
Iteration 13100: Loss = -11071.11446493326
Iteration 13200: Loss = -11071.117497018235
1
Iteration 13300: Loss = -11071.156000418836
2
Iteration 13400: Loss = -11071.243640542998
3
Iteration 13500: Loss = -11071.115048616806
4
Iteration 13600: Loss = -11071.114789140669
5
Iteration 13700: Loss = -11071.126296092265
6
Iteration 13800: Loss = -11071.344170505763
7
Iteration 13900: Loss = -11071.11783506655
8
Iteration 14000: Loss = -11071.126770071505
9
Iteration 14100: Loss = -11071.122702056651
10
Iteration 14200: Loss = -11071.116983556765
11
Iteration 14300: Loss = -11071.114879255727
12
Iteration 14400: Loss = -11071.11433790993
Iteration 14500: Loss = -11071.115178999336
1
Iteration 14600: Loss = -11071.115778306188
2
Iteration 14700: Loss = -11071.114716692347
3
Iteration 14800: Loss = -11071.11452194506
4
Iteration 14900: Loss = -11071.11442118332
Iteration 15000: Loss = -11071.118260071524
1
Iteration 15100: Loss = -11071.115093493843
2
Iteration 15200: Loss = -11071.114829303759
3
Iteration 15300: Loss = -11071.114448325263
Iteration 15400: Loss = -11071.116658377492
1
Iteration 15500: Loss = -11071.123538028502
2
Iteration 15600: Loss = -11071.116936405146
3
Iteration 15700: Loss = -11071.114297822569
Iteration 15800: Loss = -11071.115982016927
1
Iteration 15900: Loss = -11071.149567608143
2
Iteration 16000: Loss = -11071.122228949223
3
Iteration 16100: Loss = -11071.11883171267
4
Iteration 16200: Loss = -11071.11432704144
Iteration 16300: Loss = -11071.114315159144
Iteration 16400: Loss = -11071.11600103403
1
Iteration 16500: Loss = -11071.11628937672
2
Iteration 16600: Loss = -11071.125386369322
3
Iteration 16700: Loss = -11071.164720350856
4
Iteration 16800: Loss = -11071.238271067532
5
Iteration 16900: Loss = -11071.116152304143
6
Iteration 17000: Loss = -11071.11390416802
Iteration 17100: Loss = -11071.114018607737
1
Iteration 17200: Loss = -11071.172947948233
2
Iteration 17300: Loss = -11071.118447869814
3
Iteration 17400: Loss = -11071.119704091214
4
Iteration 17500: Loss = -11071.130381849709
5
Iteration 17600: Loss = -11071.135939109929
6
Iteration 17700: Loss = -11071.118660286313
7
Iteration 17800: Loss = -11071.115742345035
8
Iteration 17900: Loss = -11071.133073593193
9
Iteration 18000: Loss = -11071.11455080103
10
Iteration 18100: Loss = -11071.114144368028
11
Iteration 18200: Loss = -11071.139804997196
12
Iteration 18300: Loss = -11071.116756822452
13
Iteration 18400: Loss = -11071.117803460796
14
Iteration 18500: Loss = -11071.119342927654
15
Stopping early at iteration 18500 due to no improvement.
pi: tensor([[0.8002, 0.1998],
        [0.2584, 0.7416]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4632, 0.5368], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2848, 0.1002],
         [0.6215, 0.2036]],

        [[0.6067, 0.1003],
         [0.6872, 0.5590]],

        [[0.7238, 0.0997],
         [0.7215, 0.5229]],

        [[0.5186, 0.0914],
         [0.7276, 0.5422]],

        [[0.7212, 0.0955],
         [0.7217, 0.7250]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
Global Adjusted Rand Index: 0.9446733196745221
Average Adjusted Rand Index: 0.9444796247934606
11088.959857070342
[0.9446733196745221, 0.9446733196745221] [0.9444796247934606, 0.9444796247934606] [11071.113730599394, 11071.119342927654]
-------------------------------------
This iteration is 43
True Objective function: Loss = -11098.715860112165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21240.34242461875
Iteration 100: Loss = -11393.567877019394
Iteration 200: Loss = -11378.398991595403
Iteration 300: Loss = -11263.156858709364
Iteration 400: Loss = -11176.549035904987
Iteration 500: Loss = -11139.721949884224
Iteration 600: Loss = -11132.084471149557
Iteration 700: Loss = -11131.788394949765
Iteration 800: Loss = -11130.327151929258
Iteration 900: Loss = -11129.988625779659
Iteration 1000: Loss = -11129.92938355719
Iteration 1100: Loss = -11129.890597059297
Iteration 1200: Loss = -11129.862968068643
Iteration 1300: Loss = -11129.84202893893
Iteration 1400: Loss = -11129.825619540281
Iteration 1500: Loss = -11129.812337616078
Iteration 1600: Loss = -11129.801118181047
Iteration 1700: Loss = -11129.791107041845
Iteration 1800: Loss = -11129.782505436362
Iteration 1900: Loss = -11129.775554041871
Iteration 2000: Loss = -11129.769701399227
Iteration 2100: Loss = -11129.764757216759
Iteration 2200: Loss = -11129.760461407552
Iteration 2300: Loss = -11129.756531854242
Iteration 2400: Loss = -11129.752602170105
Iteration 2500: Loss = -11129.750451088872
Iteration 2600: Loss = -11129.72659694811
Iteration 2700: Loss = -11129.71707386271
Iteration 2800: Loss = -11129.714706742632
Iteration 2900: Loss = -11129.712845781485
Iteration 3000: Loss = -11129.711100900531
Iteration 3100: Loss = -11129.709685393664
Iteration 3200: Loss = -11129.708153418105
Iteration 3300: Loss = -11129.707110211431
Iteration 3400: Loss = -11129.705261034642
Iteration 3500: Loss = -11129.703886667085
Iteration 3600: Loss = -11129.702897860776
Iteration 3700: Loss = -11129.704509213105
1
Iteration 3800: Loss = -11129.700655632148
Iteration 3900: Loss = -11129.700795271205
1
Iteration 4000: Loss = -11129.697061210281
Iteration 4100: Loss = -11129.695230919115
Iteration 4200: Loss = -11129.693431703465
Iteration 4300: Loss = -11129.690634392548
Iteration 4400: Loss = -11129.687440324817
Iteration 4500: Loss = -11129.682972397575
Iteration 4600: Loss = -11129.675689622554
Iteration 4700: Loss = -11129.661150398437
Iteration 4800: Loss = -11129.61990881167
Iteration 4900: Loss = -11129.430954622825
Iteration 5000: Loss = -11109.001916475909
Iteration 5100: Loss = -11108.373287818229
Iteration 5200: Loss = -11098.059410284048
Iteration 5300: Loss = -11098.01601714846
Iteration 5400: Loss = -11098.007920343596
Iteration 5500: Loss = -11097.983998868778
Iteration 5600: Loss = -11097.980419693618
Iteration 5700: Loss = -11094.442537577097
Iteration 5800: Loss = -11072.31327844087
Iteration 5900: Loss = -11072.278688533584
Iteration 6000: Loss = -11072.254978493864
Iteration 6100: Loss = -11072.253722820304
Iteration 6200: Loss = -11072.253053811313
Iteration 6300: Loss = -11072.252625122976
Iteration 6400: Loss = -11072.252241473489
Iteration 6500: Loss = -11072.251809206053
Iteration 6600: Loss = -11072.25049696965
Iteration 6700: Loss = -11068.81395182747
Iteration 6800: Loss = -11068.81210775039
Iteration 6900: Loss = -11068.811811492324
Iteration 7000: Loss = -11068.812136532137
1
Iteration 7100: Loss = -11068.811032674997
Iteration 7200: Loss = -11068.812349006923
1
Iteration 7300: Loss = -11068.810028609012
Iteration 7400: Loss = -11068.809724334847
Iteration 7500: Loss = -11068.810161790434
1
Iteration 7600: Loss = -11068.809698792555
Iteration 7700: Loss = -11068.810887855554
1
Iteration 7800: Loss = -11068.809746911857
Iteration 7900: Loss = -11068.819894693763
1
Iteration 8000: Loss = -11068.796216495643
Iteration 8100: Loss = -11068.787433958425
Iteration 8200: Loss = -11068.787397801838
Iteration 8300: Loss = -11068.794346877168
1
Iteration 8400: Loss = -11068.787328204562
Iteration 8500: Loss = -11068.78779038584
1
Iteration 8600: Loss = -11068.790567266658
2
Iteration 8700: Loss = -11068.78761531525
3
Iteration 8800: Loss = -11068.787444489693
4
Iteration 8900: Loss = -11068.839617512236
5
Iteration 9000: Loss = -11068.79136589697
6
Iteration 9100: Loss = -11068.78718891504
Iteration 9200: Loss = -11068.787349519278
1
Iteration 9300: Loss = -11068.78729242761
2
Iteration 9400: Loss = -11068.787104841627
Iteration 9500: Loss = -11068.789987063927
1
Iteration 9600: Loss = -11068.787265038896
2
Iteration 9700: Loss = -11068.788585959835
3
Iteration 9800: Loss = -11068.829539513525
4
Iteration 9900: Loss = -11068.78669600759
Iteration 10000: Loss = -11068.787460306448
1
Iteration 10100: Loss = -11068.786640027498
Iteration 10200: Loss = -11068.843816769306
1
Iteration 10300: Loss = -11068.786601994942
Iteration 10400: Loss = -11068.78659235305
Iteration 10500: Loss = -11068.78677921163
1
Iteration 10600: Loss = -11068.786450483225
Iteration 10700: Loss = -11068.788877688821
1
Iteration 10800: Loss = -11068.786042115726
Iteration 10900: Loss = -11068.785950855196
Iteration 11000: Loss = -11068.780050772306
Iteration 11100: Loss = -11068.770526113836
Iteration 11200: Loss = -11068.780958777528
1
Iteration 11300: Loss = -11068.805802300292
2
Iteration 11400: Loss = -11068.775673419883
3
Iteration 11500: Loss = -11068.881358348452
4
Iteration 11600: Loss = -11068.769887946908
Iteration 11700: Loss = -11068.772545349688
1
Iteration 11800: Loss = -11068.825377947422
2
Iteration 11900: Loss = -11068.769827886954
Iteration 12000: Loss = -11068.801889327062
1
Iteration 12100: Loss = -11068.769760655216
Iteration 12200: Loss = -11068.844418574294
1
Iteration 12300: Loss = -11068.80574093686
2
Iteration 12400: Loss = -11068.769886638449
3
Iteration 12500: Loss = -11068.769966426045
4
Iteration 12600: Loss = -11068.79521545246
5
Iteration 12700: Loss = -11068.771364849357
6
Iteration 12800: Loss = -11068.769845115346
Iteration 12900: Loss = -11068.774499861209
1
Iteration 13000: Loss = -11068.808760888585
2
Iteration 13100: Loss = -11068.76983472993
Iteration 13200: Loss = -11068.76980254897
Iteration 13300: Loss = -11068.847691728397
1
Iteration 13400: Loss = -11068.76973009645
Iteration 13500: Loss = -11068.783463858585
1
Iteration 13600: Loss = -11068.769740014566
Iteration 13700: Loss = -11068.769757306542
Iteration 13800: Loss = -11068.769765962092
Iteration 13900: Loss = -11068.770687723198
1
Iteration 14000: Loss = -11068.770016956953
2
Iteration 14100: Loss = -11068.818724450599
3
Iteration 14200: Loss = -11068.769765780678
Iteration 14300: Loss = -11068.769782675341
Iteration 14400: Loss = -11068.806145154489
1
Iteration 14500: Loss = -11068.769716800265
Iteration 14600: Loss = -11068.77131614197
1
Iteration 14700: Loss = -11068.769725312974
Iteration 14800: Loss = -11068.769825984135
1
Iteration 14900: Loss = -11068.781090320934
2
Iteration 15000: Loss = -11068.784078999368
3
Iteration 15100: Loss = -11068.772089634604
4
Iteration 15200: Loss = -11068.788048526982
5
Iteration 15300: Loss = -11068.769947787252
6
Iteration 15400: Loss = -11068.769781510522
Iteration 15500: Loss = -11068.87672651863
1
Iteration 15600: Loss = -11068.769720608758
Iteration 15700: Loss = -11068.800617928468
1
Iteration 15800: Loss = -11068.769746691089
Iteration 15900: Loss = -11068.769734583315
Iteration 16000: Loss = -11068.780822193487
1
Iteration 16100: Loss = -11068.76972592167
Iteration 16200: Loss = -11068.769708693279
Iteration 16300: Loss = -11068.779873702673
1
Iteration 16400: Loss = -11068.769737455372
Iteration 16500: Loss = -11068.899065656784
1
Iteration 16600: Loss = -11068.769724744265
Iteration 16700: Loss = -11068.772949820199
1
Iteration 16800: Loss = -11068.769952526374
2
Iteration 16900: Loss = -11068.770546341564
3
Iteration 17000: Loss = -11068.772936083116
4
Iteration 17100: Loss = -11068.79729876785
5
Iteration 17200: Loss = -11068.769719303491
Iteration 17300: Loss = -11068.770139414777
1
Iteration 17400: Loss = -11068.769729933972
Iteration 17500: Loss = -11068.771417865022
1
Iteration 17600: Loss = -11068.769730581243
Iteration 17700: Loss = -11068.77664405299
1
Iteration 17800: Loss = -11068.76972088689
Iteration 17900: Loss = -11068.770284775308
1
Iteration 18000: Loss = -11068.770019540463
2
Iteration 18100: Loss = -11068.769780743303
Iteration 18200: Loss = -11068.770562697571
1
Iteration 18300: Loss = -11068.781270297788
2
Iteration 18400: Loss = -11068.777917300371
3
Iteration 18500: Loss = -11068.770562720518
4
Iteration 18600: Loss = -11068.770225819879
5
Iteration 18700: Loss = -11068.785269336968
6
Iteration 18800: Loss = -11068.774524157545
7
Iteration 18900: Loss = -11068.77598655245
8
Iteration 19000: Loss = -11068.777446201972
9
Iteration 19100: Loss = -11068.771794402184
10
Iteration 19200: Loss = -11068.776297426888
11
Iteration 19300: Loss = -11068.76989196703
12
Iteration 19400: Loss = -11068.769751643693
Iteration 19500: Loss = -11068.81535732048
1
Iteration 19600: Loss = -11068.769729027488
Iteration 19700: Loss = -11068.771073399525
1
Iteration 19800: Loss = -11068.861760989932
2
Iteration 19900: Loss = -11068.771489869183
3
pi: tensor([[0.8059, 0.1941],
        [0.2311, 0.7689]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4245, 0.5755], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3044, 0.0995],
         [0.5009, 0.1919]],

        [[0.6677, 0.1009],
         [0.7026, 0.5130]],

        [[0.7178, 0.0952],
         [0.6124, 0.5178]],

        [[0.5258, 0.0934],
         [0.6291, 0.6015]],

        [[0.5124, 0.1069],
         [0.5946, 0.5232]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080702804390127
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9137632056588321
Average Adjusted Rand Index: 0.9139337518228021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23189.768314167304
Iteration 100: Loss = -11401.931299515329
Iteration 200: Loss = -11371.72168647107
Iteration 300: Loss = -11207.744450584467
Iteration 400: Loss = -11160.457295953142
Iteration 500: Loss = -11133.108472703723
Iteration 600: Loss = -11131.981692078289
Iteration 700: Loss = -11131.656749223876
Iteration 800: Loss = -11129.990689835264
Iteration 900: Loss = -11129.906196347976
Iteration 1000: Loss = -11129.862446755018
Iteration 1100: Loss = -11129.832484539133
Iteration 1200: Loss = -11129.810738673212
Iteration 1300: Loss = -11129.794577008874
Iteration 1400: Loss = -11129.782209515894
Iteration 1500: Loss = -11129.772435083893
Iteration 1600: Loss = -11129.76441777164
Iteration 1700: Loss = -11129.757546988887
Iteration 1800: Loss = -11129.750886205506
Iteration 1900: Loss = -11129.742672133809
Iteration 2000: Loss = -11129.737069419942
Iteration 2100: Loss = -11129.733396541928
Iteration 2200: Loss = -11129.730424849806
Iteration 2300: Loss = -11129.727934418925
Iteration 2400: Loss = -11129.725749005227
Iteration 2500: Loss = -11129.74619556102
1
Iteration 2600: Loss = -11129.722110785602
Iteration 2700: Loss = -11129.720625785712
Iteration 2800: Loss = -11129.71930659864
Iteration 2900: Loss = -11129.718087931358
Iteration 3000: Loss = -11129.71691560638
Iteration 3100: Loss = -11129.715943894007
Iteration 3200: Loss = -11129.714972531294
Iteration 3300: Loss = -11129.714111150877
Iteration 3400: Loss = -11129.713248212905
Iteration 3500: Loss = -11129.714849783168
1
Iteration 3600: Loss = -11129.71195750248
Iteration 3700: Loss = -11129.711440599054
Iteration 3800: Loss = -11129.710950316306
Iteration 3900: Loss = -11129.710441467894
Iteration 4000: Loss = -11129.712262967161
1
Iteration 4100: Loss = -11129.709460604412
Iteration 4200: Loss = -11129.70901621334
Iteration 4300: Loss = -11129.708771925387
Iteration 4400: Loss = -11129.708086409642
Iteration 4500: Loss = -11129.707537617795
Iteration 4600: Loss = -11129.707048325019
Iteration 4700: Loss = -11129.706473455431
Iteration 4800: Loss = -11129.70816135363
1
Iteration 4900: Loss = -11129.705079784508
Iteration 5000: Loss = -11129.704106998359
Iteration 5100: Loss = -11129.699677138757
Iteration 5200: Loss = -11129.69823170888
Iteration 5300: Loss = -11129.697687856882
Iteration 5400: Loss = -11129.697086047538
Iteration 5500: Loss = -11129.699993710565
1
Iteration 5600: Loss = -11129.69580585095
Iteration 5700: Loss = -11129.694909915948
Iteration 5800: Loss = -11129.69373270715
Iteration 5900: Loss = -11129.69090011679
Iteration 6000: Loss = -11129.69053530417
Iteration 6100: Loss = -11129.660582797038
Iteration 6200: Loss = -11128.127768709644
Iteration 6300: Loss = -11098.786452105522
Iteration 6400: Loss = -11098.714845615463
Iteration 6500: Loss = -11098.704052485875
Iteration 6600: Loss = -11098.697124343522
Iteration 6700: Loss = -11098.022461985005
Iteration 6800: Loss = -11083.386554538045
Iteration 6900: Loss = -11083.347447874592
Iteration 7000: Loss = -11072.322787330791
Iteration 7100: Loss = -11072.299558237497
Iteration 7200: Loss = -11072.288207022137
Iteration 7300: Loss = -11072.288140818588
Iteration 7400: Loss = -11072.288296792456
1
Iteration 7500: Loss = -11072.287714216383
Iteration 7600: Loss = -11072.28765159143
Iteration 7700: Loss = -11072.286799098334
Iteration 7800: Loss = -11072.282137163244
Iteration 7900: Loss = -11068.938838867629
Iteration 8000: Loss = -11068.957619294231
1
Iteration 8100: Loss = -11068.940879871192
2
Iteration 8200: Loss = -11068.934206565322
Iteration 8300: Loss = -11068.934156352496
Iteration 8400: Loss = -11068.942483537461
1
Iteration 8500: Loss = -11068.933689688947
Iteration 8600: Loss = -11068.815574332943
Iteration 8700: Loss = -11068.809250294711
Iteration 8800: Loss = -11068.832407792434
1
Iteration 8900: Loss = -11068.80937865422
2
Iteration 9000: Loss = -11068.808878723601
Iteration 9100: Loss = -11068.8092310533
1
Iteration 9200: Loss = -11068.811334108836
2
Iteration 9300: Loss = -11068.809315990911
3
Iteration 9400: Loss = -11068.808875036822
Iteration 9500: Loss = -11068.808836047716
Iteration 9600: Loss = -11068.809154419454
1
Iteration 9700: Loss = -11068.813958951378
2
Iteration 9800: Loss = -11068.80913575482
3
Iteration 9900: Loss = -11068.80880485867
Iteration 10000: Loss = -11068.81024922638
1
Iteration 10100: Loss = -11068.809349704112
2
Iteration 10200: Loss = -11068.789641741669
Iteration 10300: Loss = -11068.812472953285
1
Iteration 10400: Loss = -11068.786589242649
Iteration 10500: Loss = -11068.786780718237
1
Iteration 10600: Loss = -11068.786953667988
2
Iteration 10700: Loss = -11068.794877242142
3
Iteration 10800: Loss = -11068.87112613857
4
Iteration 10900: Loss = -11068.786583100085
Iteration 11000: Loss = -11068.789714593833
1
Iteration 11100: Loss = -11068.786536365398
Iteration 11200: Loss = -11068.787366494207
1
Iteration 11300: Loss = -11068.789540792704
2
Iteration 11400: Loss = -11068.787506832283
3
Iteration 11500: Loss = -11068.794320535091
4
Iteration 11600: Loss = -11068.786420941557
Iteration 11700: Loss = -11068.786540577856
1
Iteration 11800: Loss = -11068.842122661299
2
Iteration 11900: Loss = -11068.786271812627
Iteration 12000: Loss = -11068.790825009746
1
Iteration 12100: Loss = -11068.77010293215
Iteration 12200: Loss = -11068.7791998852
1
Iteration 12300: Loss = -11068.769771419877
Iteration 12400: Loss = -11068.795469693274
1
Iteration 12500: Loss = -11068.769759596891
Iteration 12600: Loss = -11069.07835663054
1
Iteration 12700: Loss = -11068.769760648387
Iteration 12800: Loss = -11068.770262266076
1
Iteration 12900: Loss = -11068.769789727763
Iteration 13000: Loss = -11068.772809834227
1
Iteration 13100: Loss = -11068.770455558653
2
Iteration 13200: Loss = -11068.769870512144
Iteration 13300: Loss = -11068.82630048637
1
Iteration 13400: Loss = -11068.769797678475
Iteration 13500: Loss = -11068.771705846164
1
Iteration 13600: Loss = -11068.770230334858
2
Iteration 13700: Loss = -11068.770798224734
3
Iteration 13800: Loss = -11068.769760428611
Iteration 13900: Loss = -11068.769945321354
1
Iteration 14000: Loss = -11068.783348387726
2
Iteration 14100: Loss = -11068.769785011105
Iteration 14200: Loss = -11068.772627946622
1
Iteration 14300: Loss = -11068.775653543556
2
Iteration 14400: Loss = -11068.83970828353
3
Iteration 14500: Loss = -11068.773194134516
4
Iteration 14600: Loss = -11068.828036224548
5
Iteration 14700: Loss = -11068.77020755764
6
Iteration 14800: Loss = -11068.769793081094
Iteration 14900: Loss = -11068.775877216785
1
Iteration 15000: Loss = -11068.770142905345
2
Iteration 15100: Loss = -11068.769758772294
Iteration 15200: Loss = -11068.769859518996
1
Iteration 15300: Loss = -11068.978358515165
2
Iteration 15400: Loss = -11068.769757761178
Iteration 15500: Loss = -11068.778297328501
1
Iteration 15600: Loss = -11068.769736517515
Iteration 15700: Loss = -11068.769965125539
1
Iteration 15800: Loss = -11068.771077965313
2
Iteration 15900: Loss = -11068.773302886297
3
Iteration 16000: Loss = -11068.769756772486
Iteration 16100: Loss = -11068.776578815907
1
Iteration 16200: Loss = -11068.769762057036
Iteration 16300: Loss = -11068.770164436046
1
Iteration 16400: Loss = -11068.781885006074
2
Iteration 16500: Loss = -11068.769772308358
Iteration 16600: Loss = -11068.78536417079
1
Iteration 16700: Loss = -11068.769747441258
Iteration 16800: Loss = -11068.7712918962
1
Iteration 16900: Loss = -11068.769737371507
Iteration 17000: Loss = -11068.770181668258
1
Iteration 17100: Loss = -11068.769749318913
Iteration 17200: Loss = -11068.769823217075
Iteration 17300: Loss = -11068.769828832823
Iteration 17400: Loss = -11068.769827479107
Iteration 17500: Loss = -11068.773612652987
1
Iteration 17600: Loss = -11068.995055267742
2
Iteration 17700: Loss = -11068.7793804768
3
Iteration 17800: Loss = -11068.771643035114
4
Iteration 17900: Loss = -11068.776139932268
5
Iteration 18000: Loss = -11068.92892662262
6
Iteration 18100: Loss = -11068.769749249168
Iteration 18200: Loss = -11068.770034033263
1
Iteration 18300: Loss = -11068.7714546522
2
Iteration 18400: Loss = -11068.76976768349
Iteration 18500: Loss = -11068.76986969999
1
Iteration 18600: Loss = -11068.866565614104
2
Iteration 18700: Loss = -11068.769743847708
Iteration 18800: Loss = -11069.08224689332
1
Iteration 18900: Loss = -11068.769743719344
Iteration 19000: Loss = -11068.771909679172
1
Iteration 19100: Loss = -11068.771145462144
2
Iteration 19200: Loss = -11068.770240685735
3
Iteration 19300: Loss = -11068.769725786686
Iteration 19400: Loss = -11068.76984962538
1
Iteration 19500: Loss = -11068.774947812553
2
Iteration 19600: Loss = -11068.83068007179
3
Iteration 19700: Loss = -11068.776484227867
4
Iteration 19800: Loss = -11068.783220610765
5
Iteration 19900: Loss = -11068.769721486666
pi: tensor([[0.8056, 0.1944],
        [0.2303, 0.7697]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4241, 0.5759], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3045, 0.0994],
         [0.5938, 0.1918]],

        [[0.5937, 0.1009],
         [0.5916, 0.5507]],

        [[0.5803, 0.0952],
         [0.5489, 0.6101]],

        [[0.5041, 0.0934],
         [0.7248, 0.5696]],

        [[0.5900, 0.1069],
         [0.5773, 0.5473]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080702804390127
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9137632056588321
Average Adjusted Rand Index: 0.9139337518228021
11098.715860112165
[0.9137632056588321, 0.9137632056588321] [0.9139337518228021, 0.9139337518228021] [11068.770537944514, 11068.770591747627]
-------------------------------------
This iteration is 44
True Objective function: Loss = -11118.354244653518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20881.57728953681
Iteration 100: Loss = -11361.883900217466
Iteration 200: Loss = -11360.93004114557
Iteration 300: Loss = -11352.052132057932
Iteration 400: Loss = -11318.728587137202
Iteration 500: Loss = -11162.302050927174
Iteration 600: Loss = -11145.856089295465
Iteration 700: Loss = -11145.267788805153
Iteration 800: Loss = -11145.052310167415
Iteration 900: Loss = -11144.939067974668
Iteration 1000: Loss = -11144.867921423838
Iteration 1100: Loss = -11144.818918118563
Iteration 1200: Loss = -11144.784168428205
Iteration 1300: Loss = -11144.756946717489
Iteration 1400: Loss = -11144.73284558247
Iteration 1500: Loss = -11144.705092509092
Iteration 1600: Loss = -11144.655846736647
Iteration 1700: Loss = -11144.569577360828
Iteration 1800: Loss = -11144.377952172066
Iteration 1900: Loss = -11144.194882176267
Iteration 2000: Loss = -11144.126936046503
Iteration 2100: Loss = -11143.39259839164
Iteration 2200: Loss = -11143.337032967032
Iteration 2300: Loss = -11143.268080752754
Iteration 2400: Loss = -11143.262376793917
Iteration 2500: Loss = -11143.250715899427
Iteration 2600: Loss = -11143.24707384951
Iteration 2700: Loss = -11143.244004266366
Iteration 2800: Loss = -11143.241441426217
Iteration 2900: Loss = -11143.240441870868
Iteration 3000: Loss = -11143.237315405058
Iteration 3100: Loss = -11143.235492904121
Iteration 3200: Loss = -11143.233073985608
Iteration 3300: Loss = -11143.230286231917
Iteration 3400: Loss = -11143.227564507406
Iteration 3500: Loss = -11143.225776142453
Iteration 3600: Loss = -11143.22555610586
Iteration 3700: Loss = -11143.223884006919
Iteration 3800: Loss = -11143.21875063749
Iteration 3900: Loss = -11143.209154557573
Iteration 4000: Loss = -11143.134512657472
Iteration 4100: Loss = -11143.065435737144
Iteration 4200: Loss = -11143.064403941866
Iteration 4300: Loss = -11143.065591761158
1
Iteration 4400: Loss = -11143.062972270314
Iteration 4500: Loss = -11143.066087354784
1
Iteration 4600: Loss = -11143.066132130803
2
Iteration 4700: Loss = -11143.061701935432
Iteration 4800: Loss = -11143.06124579854
Iteration 4900: Loss = -11143.06101301752
Iteration 5000: Loss = -11143.060196556102
Iteration 5100: Loss = -11143.06283620805
1
Iteration 5200: Loss = -11143.05759455546
Iteration 5300: Loss = -11143.056874171607
Iteration 5400: Loss = -11143.055454969552
Iteration 5500: Loss = -11143.054620494428
Iteration 5600: Loss = -11143.053545365006
Iteration 5700: Loss = -11143.049717845397
Iteration 5800: Loss = -11143.049160285902
Iteration 5900: Loss = -11143.048077189782
Iteration 6000: Loss = -11143.047934183958
Iteration 6100: Loss = -11143.047700372295
Iteration 6200: Loss = -11143.047572232357
Iteration 6300: Loss = -11143.047413293289
Iteration 6400: Loss = -11143.047602674193
1
Iteration 6500: Loss = -11143.052331887598
2
Iteration 6600: Loss = -11143.047040331097
Iteration 6700: Loss = -11143.046976750975
Iteration 6800: Loss = -11143.047347235928
1
Iteration 6900: Loss = -11143.046572917316
Iteration 7000: Loss = -11143.04625988794
Iteration 7100: Loss = -11143.04550859624
Iteration 7200: Loss = -11143.044759774357
Iteration 7300: Loss = -11143.04485675461
Iteration 7400: Loss = -11143.043498177578
Iteration 7500: Loss = -11143.042991188933
Iteration 7600: Loss = -11143.043024863937
Iteration 7700: Loss = -11143.059118647476
1
Iteration 7800: Loss = -11143.042779064157
Iteration 7900: Loss = -11143.07201327323
1
Iteration 8000: Loss = -11143.04266825595
Iteration 8100: Loss = -11143.04624105362
1
Iteration 8200: Loss = -11143.04254805959
Iteration 8300: Loss = -11143.045279581174
1
Iteration 8400: Loss = -11143.047932095133
2
Iteration 8500: Loss = -11143.043881608392
3
Iteration 8600: Loss = -11143.0424438779
Iteration 8700: Loss = -11143.042356765742
Iteration 8800: Loss = -11143.150242914407
1
Iteration 8900: Loss = -11143.042361068672
Iteration 9000: Loss = -11143.061338050436
1
Iteration 9100: Loss = -11143.042153914903
Iteration 9200: Loss = -11143.066006381623
1
Iteration 9300: Loss = -11143.041889668595
Iteration 9400: Loss = -11143.041657875192
Iteration 9500: Loss = -11143.053313821609
1
Iteration 9600: Loss = -11143.04123533651
Iteration 9700: Loss = -11143.041387801308
1
Iteration 9800: Loss = -11143.042523347167
2
Iteration 9900: Loss = -11143.041148143988
Iteration 10000: Loss = -11143.041091905123
Iteration 10100: Loss = -11143.041439891465
1
Iteration 10200: Loss = -11143.041022238325
Iteration 10300: Loss = -11143.054229367917
1
Iteration 10400: Loss = -11143.0409930529
Iteration 10500: Loss = -11143.041469300771
1
Iteration 10600: Loss = -11143.044288528918
2
Iteration 10700: Loss = -11143.040872575852
Iteration 10800: Loss = -11143.046082899531
1
Iteration 10900: Loss = -11143.040847236884
Iteration 11000: Loss = -11143.059301712947
1
Iteration 11100: Loss = -11143.040845757667
Iteration 11200: Loss = -11143.08093492539
1
Iteration 11300: Loss = -11143.040807566342
Iteration 11400: Loss = -11143.040936573007
1
Iteration 11500: Loss = -11143.040858291379
Iteration 11600: Loss = -11143.044011515814
1
Iteration 11700: Loss = -11143.055116506674
2
Iteration 11800: Loss = -11143.150455854926
3
Iteration 11900: Loss = -11143.042440028303
4
Iteration 12000: Loss = -11143.041686074983
5
Iteration 12100: Loss = -11143.042069521567
6
Iteration 12200: Loss = -11143.089122327781
7
Iteration 12300: Loss = -11143.040795775594
Iteration 12400: Loss = -11143.042072054845
1
Iteration 12500: Loss = -11143.115680840941
2
Iteration 12600: Loss = -11143.04076611109
Iteration 12700: Loss = -11143.04114270676
1
Iteration 12800: Loss = -11143.057185004605
2
Iteration 12900: Loss = -11143.040763039588
Iteration 13000: Loss = -11143.054310739171
1
Iteration 13100: Loss = -11143.040733596245
Iteration 13200: Loss = -11143.05743233672
1
Iteration 13300: Loss = -11143.040748310135
Iteration 13400: Loss = -11143.078075884645
1
Iteration 13500: Loss = -11143.040722138392
Iteration 13600: Loss = -11143.042952743353
1
Iteration 13700: Loss = -11143.040738424725
Iteration 13800: Loss = -11143.040809187341
Iteration 13900: Loss = -11143.040724056676
Iteration 14000: Loss = -11143.04020004361
Iteration 14100: Loss = -11143.04016696662
Iteration 14200: Loss = -11143.040779026225
1
Iteration 14300: Loss = -11143.040156571195
Iteration 14400: Loss = -11143.045601202433
1
Iteration 14500: Loss = -11143.046313521643
2
Iteration 14600: Loss = -11143.040130509271
Iteration 14700: Loss = -11143.040449270798
1
Iteration 14800: Loss = -11143.077753732516
2
Iteration 14900: Loss = -11143.040143378892
Iteration 15000: Loss = -11143.05855228349
1
Iteration 15100: Loss = -11143.040230799072
Iteration 15200: Loss = -11143.11242827682
1
Iteration 15300: Loss = -11143.040136929423
Iteration 15400: Loss = -11143.044815145684
1
Iteration 15500: Loss = -11143.046002274956
2
Iteration 15600: Loss = -11143.040127081018
Iteration 15700: Loss = -11143.040480471609
1
Iteration 15800: Loss = -11143.04074957502
2
Iteration 15900: Loss = -11143.074193347526
3
Iteration 16000: Loss = -11143.040131951759
Iteration 16100: Loss = -11143.044351459834
1
Iteration 16200: Loss = -11143.076700406862
2
Iteration 16300: Loss = -11143.040138864291
Iteration 16400: Loss = -11143.041828154855
1
Iteration 16500: Loss = -11143.040123717092
Iteration 16600: Loss = -11143.041243408587
1
Iteration 16700: Loss = -11143.040110016744
Iteration 16800: Loss = -11143.051498442977
1
Iteration 16900: Loss = -11143.253852178068
2
Iteration 17000: Loss = -11143.040128121509
Iteration 17100: Loss = -11143.044531191157
1
Iteration 17200: Loss = -11143.040121975302
Iteration 17300: Loss = -11143.044947062455
1
Iteration 17400: Loss = -11143.042595180677
2
Iteration 17500: Loss = -11143.040632882752
3
Iteration 17600: Loss = -11143.040163213109
Iteration 17700: Loss = -11143.06081616088
1
Iteration 17800: Loss = -11143.040127514623
Iteration 17900: Loss = -11143.04030300546
1
Iteration 18000: Loss = -11143.040607561303
2
Iteration 18100: Loss = -11143.040089662152
Iteration 18200: Loss = -11143.29323434393
1
Iteration 18300: Loss = -11143.040074924567
Iteration 18400: Loss = -11143.067193814673
1
Iteration 18500: Loss = -11143.040059713101
Iteration 18600: Loss = -11143.04166392422
1
Iteration 18700: Loss = -11143.044389432758
2
Iteration 18800: Loss = -11143.041170362229
3
Iteration 18900: Loss = -11143.040062875878
Iteration 19000: Loss = -11143.057097254092
1
Iteration 19100: Loss = -11143.04005883794
Iteration 19200: Loss = -11143.040992189248
1
Iteration 19300: Loss = -11143.040136027123
Iteration 19400: Loss = -11143.040401286935
1
Iteration 19500: Loss = -11143.053160654763
2
Iteration 19600: Loss = -11143.23147177344
3
Iteration 19700: Loss = -11143.040099506252
Iteration 19800: Loss = -11143.042238245267
1
Iteration 19900: Loss = -11143.040058095157
pi: tensor([[0.7849, 0.2151],
        [0.3408, 0.6592]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0530, 0.9470], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2925, 0.0878],
         [0.6393, 0.1850]],

        [[0.5707, 0.0967],
         [0.5651, 0.6723]],

        [[0.5112, 0.1017],
         [0.5799, 0.6565]],

        [[0.5417, 0.1011],
         [0.5145, 0.5443]],

        [[0.5984, 0.1001],
         [0.5194, 0.5756]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: -0.02385462411194539
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.6457213865887003
Average Adjusted Rand Index: 0.7475382070667679
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19869.77080052737
Iteration 100: Loss = -11361.919178058424
Iteration 200: Loss = -11360.537081774106
Iteration 300: Loss = -11359.378968310211
Iteration 400: Loss = -11354.045281621182
Iteration 500: Loss = -11349.452377133746
Iteration 600: Loss = -11316.65411719698
Iteration 700: Loss = -11189.306732784731
Iteration 800: Loss = -11147.545175102086
Iteration 900: Loss = -11146.58186877028
Iteration 1000: Loss = -11145.856107666346
Iteration 1100: Loss = -11145.570666513746
Iteration 1200: Loss = -11145.434746307268
Iteration 1300: Loss = -11145.172642191146
Iteration 1400: Loss = -11145.103398983041
Iteration 1500: Loss = -11145.048854532219
Iteration 1600: Loss = -11145.00494170103
Iteration 1700: Loss = -11144.937689262359
Iteration 1800: Loss = -11144.896520828948
Iteration 1900: Loss = -11143.731960802304
Iteration 2000: Loss = -11143.718391539447
Iteration 2100: Loss = -11143.709184602963
Iteration 2200: Loss = -11143.693139940533
Iteration 2300: Loss = -11143.66236373696
Iteration 2400: Loss = -11143.656832486962
Iteration 2500: Loss = -11143.6513343833
Iteration 2600: Loss = -11143.644601916936
Iteration 2700: Loss = -11143.633611741618
Iteration 2800: Loss = -11143.61789930643
Iteration 2900: Loss = -11143.594580473824
Iteration 3000: Loss = -11143.59180295596
Iteration 3100: Loss = -11143.586081490424
Iteration 3200: Loss = -11143.57295316851
Iteration 3300: Loss = -11143.555802201981
Iteration 3400: Loss = -11143.552154638153
Iteration 3500: Loss = -11143.552027784915
Iteration 3600: Loss = -11143.548837471411
Iteration 3700: Loss = -11143.539948574484
Iteration 3800: Loss = -11143.543692984758
1
Iteration 3900: Loss = -11143.53191811121
Iteration 4000: Loss = -11143.530469458843
Iteration 4100: Loss = -11143.52677825202
Iteration 4200: Loss = -11143.526628583688
Iteration 4300: Loss = -11143.525455026758
Iteration 4400: Loss = -11143.524740422306
Iteration 4500: Loss = -11143.528235623153
1
Iteration 4600: Loss = -11143.521720407149
Iteration 4700: Loss = -11143.518848330179
Iteration 4800: Loss = -11143.517111121995
Iteration 4900: Loss = -11143.513861401847
Iteration 5000: Loss = -11143.513223440885
Iteration 5100: Loss = -11143.512689771773
Iteration 5200: Loss = -11143.511302990379
Iteration 5300: Loss = -11143.510685908575
Iteration 5400: Loss = -11143.51171856206
1
Iteration 5500: Loss = -11143.502769111328
Iteration 5600: Loss = -11143.49356959513
Iteration 5700: Loss = -11143.491928369907
Iteration 5800: Loss = -11143.490323537062
Iteration 5900: Loss = -11143.490003151292
Iteration 6000: Loss = -11143.489780294276
Iteration 6100: Loss = -11143.490823138292
1
Iteration 6200: Loss = -11143.489124965157
Iteration 6300: Loss = -11143.497979085789
1
Iteration 6400: Loss = -11143.483019851526
Iteration 6500: Loss = -11143.477488769307
Iteration 6600: Loss = -11143.477205128225
Iteration 6700: Loss = -11143.477041853546
Iteration 6800: Loss = -11143.483134891561
1
Iteration 6900: Loss = -11143.476768451532
Iteration 7000: Loss = -11143.492846031391
1
Iteration 7100: Loss = -11143.47625849544
Iteration 7200: Loss = -11143.476263656681
Iteration 7300: Loss = -11143.473776369005
Iteration 7400: Loss = -11143.491458155531
1
Iteration 7500: Loss = -11143.473171963828
Iteration 7600: Loss = -11143.47671543839
1
Iteration 7700: Loss = -11143.472700268123
Iteration 7800: Loss = -11143.544588231154
1
Iteration 7900: Loss = -11143.471638701863
Iteration 8000: Loss = -11143.468004090446
Iteration 8100: Loss = -11143.467944448887
Iteration 8200: Loss = -11143.46793199539
Iteration 8300: Loss = -11143.467383114727
Iteration 8400: Loss = -11143.475877634848
1
Iteration 8500: Loss = -11143.467160389619
Iteration 8600: Loss = -11143.573673875017
1
Iteration 8700: Loss = -11143.466990384974
Iteration 8800: Loss = -11143.499424773627
1
Iteration 8900: Loss = -11143.466888585297
Iteration 9000: Loss = -11143.467495319337
1
Iteration 9100: Loss = -11143.467372519239
2
Iteration 9200: Loss = -11143.466171766062
Iteration 9300: Loss = -11143.521802383577
1
Iteration 9400: Loss = -11143.465945733362
Iteration 9500: Loss = -11143.46815360886
1
Iteration 9600: Loss = -11143.465768424412
Iteration 9700: Loss = -11143.465840365716
Iteration 9800: Loss = -11143.58203720129
1
Iteration 9900: Loss = -11143.465727923818
Iteration 10000: Loss = -11143.556755398133
1
Iteration 10100: Loss = -11143.46571496632
Iteration 10200: Loss = -11143.49138962381
1
Iteration 10300: Loss = -11143.465636604502
Iteration 10400: Loss = -11143.465632046073
Iteration 10500: Loss = -11143.46572341345
Iteration 10600: Loss = -11143.46554737621
Iteration 10700: Loss = -11143.465863154737
1
Iteration 10800: Loss = -11143.465450430953
Iteration 10900: Loss = -11143.46733755707
1
Iteration 11000: Loss = -11143.46536571177
Iteration 11100: Loss = -11143.506868508839
1
Iteration 11200: Loss = -11143.465336645626
Iteration 11300: Loss = -11143.534705958606
1
Iteration 11400: Loss = -11143.465329844137
Iteration 11500: Loss = -11143.519082370618
1
Iteration 11600: Loss = -11143.464147886023
Iteration 11700: Loss = -11143.526252486905
1
Iteration 11800: Loss = -11143.46401887388
Iteration 11900: Loss = -11143.463954753128
Iteration 12000: Loss = -11143.462200846918
Iteration 12100: Loss = -11143.461823777323
Iteration 12200: Loss = -11143.46318950742
1
Iteration 12300: Loss = -11143.461385448521
Iteration 12400: Loss = -11143.46195287071
1
Iteration 12500: Loss = -11143.461377346746
Iteration 12600: Loss = -11143.461509812005
1
Iteration 12700: Loss = -11143.46323860848
2
Iteration 12800: Loss = -11143.461308184153
Iteration 12900: Loss = -11143.478611021732
1
Iteration 13000: Loss = -11143.461277847697
Iteration 13100: Loss = -11143.47430892044
1
Iteration 13200: Loss = -11143.461244379188
Iteration 13300: Loss = -11143.461784398607
1
Iteration 13400: Loss = -11143.461261511282
Iteration 13500: Loss = -11143.461839327385
1
Iteration 13600: Loss = -11143.461027749545
Iteration 13700: Loss = -11143.46099396814
Iteration 13800: Loss = -11143.50890942988
1
Iteration 13900: Loss = -11143.460998542092
Iteration 14000: Loss = -11143.460979757416
Iteration 14100: Loss = -11143.466595879203
1
Iteration 14200: Loss = -11143.46106300481
Iteration 14300: Loss = -11143.46095167231
Iteration 14400: Loss = -11143.556804428625
1
Iteration 14500: Loss = -11143.460887695843
Iteration 14600: Loss = -11143.465194363256
1
Iteration 14700: Loss = -11143.461601990999
2
Iteration 14800: Loss = -11143.461291335527
3
Iteration 14900: Loss = -11143.466007003877
4
Iteration 15000: Loss = -11143.495470402971
5
Iteration 15100: Loss = -11143.460860330222
Iteration 15200: Loss = -11143.461057854325
1
Iteration 15300: Loss = -11143.605888341768
2
Iteration 15400: Loss = -11143.460832908671
Iteration 15500: Loss = -11143.696015269885
1
Iteration 15600: Loss = -11143.460831024955
Iteration 15700: Loss = -11143.463252529484
1
Iteration 15800: Loss = -11143.460826710318
Iteration 15900: Loss = -11143.46376392842
1
Iteration 16000: Loss = -11143.460798094728
Iteration 16100: Loss = -11143.462802136208
1
Iteration 16200: Loss = -11143.460796748746
Iteration 16300: Loss = -11143.46114862789
1
Iteration 16400: Loss = -11143.463075138357
2
Iteration 16500: Loss = -11143.456477329608
Iteration 16600: Loss = -11143.45359802226
Iteration 16700: Loss = -11143.51312686365
1
Iteration 16800: Loss = -11143.45696728268
2
Iteration 16900: Loss = -11143.454724980977
3
Iteration 17000: Loss = -11143.454154376033
4
Iteration 17100: Loss = -11143.453652239124
Iteration 17200: Loss = -11143.471589395765
1
Iteration 17300: Loss = -11143.453661159929
Iteration 17400: Loss = -11143.453646831626
Iteration 17500: Loss = -11143.61011328692
1
Iteration 17600: Loss = -11143.453001551505
Iteration 17700: Loss = -11143.455598544706
1
Iteration 17800: Loss = -11143.453018658958
Iteration 17900: Loss = -11143.453001599994
Iteration 18000: Loss = -11143.453167956213
1
Iteration 18100: Loss = -11143.45365332103
2
Iteration 18200: Loss = -11143.453064580637
Iteration 18300: Loss = -11143.484252523589
1
Iteration 18400: Loss = -11143.567044854231
2
Iteration 18500: Loss = -11143.452922588931
Iteration 18600: Loss = -11143.453073069522
1
Iteration 18700: Loss = -11143.452926673377
Iteration 18800: Loss = -11143.453409995715
1
Iteration 18900: Loss = -11143.4529166513
Iteration 19000: Loss = -11143.453969643324
1
Iteration 19100: Loss = -11143.46912656087
2
Iteration 19200: Loss = -11143.452940830739
Iteration 19300: Loss = -11143.453517661244
1
Iteration 19400: Loss = -11143.731540921823
2
Iteration 19500: Loss = -11143.452643859391
Iteration 19600: Loss = -11143.456675108668
1
Iteration 19700: Loss = -11143.452646398044
Iteration 19800: Loss = -11143.452866232814
1
Iteration 19900: Loss = -11143.458148452748
2
pi: tensor([[0.7989, 0.2011],
        [0.3310, 0.6690]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0442, 0.9558], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2926, 0.1202],
         [0.7037, 0.1826]],

        [[0.5439, 0.0969],
         [0.6625, 0.5710]],

        [[0.5984, 0.1018],
         [0.6955, 0.6355]],

        [[0.7036, 0.1014],
         [0.6773, 0.6732]],

        [[0.7307, 0.1003],
         [0.6191, 0.5440]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.6717591349889014
Average Adjusted Rand Index: 0.7523091318891569
11118.354244653518
[0.6457213865887003, 0.6717591349889014] [0.7475382070667679, 0.7523091318891569] [11143.046047443324, 11143.452665987516]
-------------------------------------
This iteration is 45
True Objective function: Loss = -11115.866066022678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22386.902481024932
Iteration 100: Loss = -11386.688944582453
Iteration 200: Loss = -11385.284961688258
Iteration 300: Loss = -11384.314486225652
Iteration 400: Loss = -11380.307531600243
Iteration 500: Loss = -11334.26615568713
Iteration 600: Loss = -11189.262086557224
Iteration 700: Loss = -11129.460625431746
Iteration 800: Loss = -11114.909634293528
Iteration 900: Loss = -11100.784269455029
Iteration 1000: Loss = -11099.179102935203
Iteration 1100: Loss = -11096.954894218543
Iteration 1200: Loss = -11092.57044713239
Iteration 1300: Loss = -11092.515173191588
Iteration 1400: Loss = -11092.469380590857
Iteration 1500: Loss = -11092.367646609213
Iteration 1600: Loss = -11092.206365003458
Iteration 1700: Loss = -11092.16428366432
Iteration 1800: Loss = -11091.852301489072
Iteration 1900: Loss = -11091.831867421119
Iteration 2000: Loss = -11091.813121512025
Iteration 2100: Loss = -11091.77543253004
Iteration 2200: Loss = -11091.72581586182
Iteration 2300: Loss = -11091.711936181078
Iteration 2400: Loss = -11091.706631790068
Iteration 2500: Loss = -11091.702595756802
Iteration 2600: Loss = -11091.699267999062
Iteration 2700: Loss = -11091.696370146085
Iteration 2800: Loss = -11091.693872709015
Iteration 2900: Loss = -11091.691541305852
Iteration 3000: Loss = -11091.689480560794
Iteration 3100: Loss = -11091.687825641728
Iteration 3200: Loss = -11091.685642799283
Iteration 3300: Loss = -11091.686156350532
1
Iteration 3400: Loss = -11091.681128037668
Iteration 3500: Loss = -11091.677954752113
Iteration 3600: Loss = -11091.667818822283
Iteration 3700: Loss = -11091.664931003936
Iteration 3800: Loss = -11091.663846666725
Iteration 3900: Loss = -11091.663711875106
Iteration 4000: Loss = -11091.661954084784
Iteration 4100: Loss = -11091.661251406855
Iteration 4200: Loss = -11091.660616817286
Iteration 4300: Loss = -11091.664121053938
1
Iteration 4400: Loss = -11091.659429213722
Iteration 4500: Loss = -11091.65895875078
Iteration 4600: Loss = -11091.659913945039
1
Iteration 4700: Loss = -11091.658077525039
Iteration 4800: Loss = -11091.657638370953
Iteration 4900: Loss = -11091.657300593572
Iteration 5000: Loss = -11091.656935179839
Iteration 5100: Loss = -11091.656630140598
Iteration 5200: Loss = -11091.65629813583
Iteration 5300: Loss = -11091.655910423227
Iteration 5400: Loss = -11091.656737899293
1
Iteration 5500: Loss = -11091.654728611597
Iteration 5600: Loss = -11091.655004432583
1
Iteration 5700: Loss = -11091.653068632948
Iteration 5800: Loss = -11091.651977169196
Iteration 5900: Loss = -11091.64853122393
Iteration 6000: Loss = -11091.648872411382
1
Iteration 6100: Loss = -11091.647495604084
Iteration 6200: Loss = -11091.605912392948
Iteration 6300: Loss = -11091.604744028287
Iteration 6400: Loss = -11091.604198797086
Iteration 6500: Loss = -11091.604238243883
Iteration 6600: Loss = -11091.603871298428
Iteration 6700: Loss = -11091.61351735659
1
Iteration 6800: Loss = -11091.612676234903
2
Iteration 6900: Loss = -11091.603543907264
Iteration 7000: Loss = -11091.603434893948
Iteration 7100: Loss = -11091.60833010702
1
Iteration 7200: Loss = -11091.603202930017
Iteration 7300: Loss = -11091.603853461895
1
Iteration 7400: Loss = -11091.60302241175
Iteration 7500: Loss = -11091.60296568706
Iteration 7600: Loss = -11091.602792427433
Iteration 7700: Loss = -11091.6025349734
Iteration 7800: Loss = -11091.600436408682
Iteration 7900: Loss = -11091.599968966939
Iteration 8000: Loss = -11091.606823833217
1
Iteration 8100: Loss = -11091.599382614259
Iteration 8200: Loss = -11091.603548118348
1
Iteration 8300: Loss = -11091.599176081536
Iteration 8400: Loss = -11091.805269434333
1
Iteration 8500: Loss = -11091.599049809465
Iteration 8600: Loss = -11091.598957643373
Iteration 8700: Loss = -11091.5993563757
1
Iteration 8800: Loss = -11091.598663518154
Iteration 8900: Loss = -11091.600541490205
1
Iteration 9000: Loss = -11091.597204982394
Iteration 9100: Loss = -11091.636897664896
1
Iteration 9200: Loss = -11091.597063380972
Iteration 9300: Loss = -11091.610320371361
1
Iteration 9400: Loss = -11091.597439301262
2
Iteration 9500: Loss = -11091.664115931386
3
Iteration 9600: Loss = -11091.596943378907
Iteration 9700: Loss = -11091.599534206944
1
Iteration 9800: Loss = -11091.596904687412
Iteration 9900: Loss = -11091.597115598177
1
Iteration 10000: Loss = -11091.599645032393
2
Iteration 10100: Loss = -11091.596865125703
Iteration 10200: Loss = -11091.597257325882
1
Iteration 10300: Loss = -11091.606633943893
2
Iteration 10400: Loss = -11091.59676929906
Iteration 10500: Loss = -11091.599215367727
1
Iteration 10600: Loss = -11091.596664617566
Iteration 10700: Loss = -11091.619637091635
1
Iteration 10800: Loss = -11091.596624703754
Iteration 10900: Loss = -11091.596914814492
1
Iteration 11000: Loss = -11091.596650974721
Iteration 11100: Loss = -11091.596580055899
Iteration 11200: Loss = -11091.604742715239
1
Iteration 11300: Loss = -11091.596530215615
Iteration 11400: Loss = -11091.596525290382
Iteration 11500: Loss = -11091.597099241804
1
Iteration 11600: Loss = -11091.596466052864
Iteration 11700: Loss = -11091.640202235496
1
Iteration 11800: Loss = -11091.596345190188
Iteration 11900: Loss = -11091.624241126347
1
Iteration 12000: Loss = -11091.82573474418
2
Iteration 12100: Loss = -11091.596306077892
Iteration 12200: Loss = -11091.682587998941
1
Iteration 12300: Loss = -11091.596201894416
Iteration 12400: Loss = -11091.596202985264
Iteration 12500: Loss = -11091.59639389872
1
Iteration 12600: Loss = -11091.59615497658
Iteration 12700: Loss = -11091.640422941671
1
Iteration 12800: Loss = -11091.595518560396
Iteration 12900: Loss = -11091.595522787522
Iteration 13000: Loss = -11091.600057164764
1
Iteration 13100: Loss = -11091.595502158521
Iteration 13200: Loss = -11091.59548893243
Iteration 13300: Loss = -11091.595762874036
1
Iteration 13400: Loss = -11091.595442658796
Iteration 13500: Loss = -11091.595768092526
1
Iteration 13600: Loss = -11091.600154370677
2
Iteration 13700: Loss = -11091.596037925958
3
Iteration 13800: Loss = -11091.595756959501
4
Iteration 13900: Loss = -11091.595714636665
5
Iteration 14000: Loss = -11091.596351092496
6
Iteration 14100: Loss = -11091.595487484965
Iteration 14200: Loss = -11091.595788585026
1
Iteration 14300: Loss = -11091.597447607526
2
Iteration 14400: Loss = -11091.597825931673
3
Iteration 14500: Loss = -11091.617973807508
4
Iteration 14600: Loss = -11091.594569610408
Iteration 14700: Loss = -11091.593551921394
Iteration 14800: Loss = -11091.624280691623
1
Iteration 14900: Loss = -11091.594238971831
2
Iteration 15000: Loss = -11091.593587705134
Iteration 15100: Loss = -11091.593703458158
1
Iteration 15200: Loss = -11091.594077450942
2
Iteration 15300: Loss = -11091.573010841823
Iteration 15400: Loss = -11091.574437146108
1
Iteration 15500: Loss = -11091.572966777812
Iteration 15600: Loss = -11091.573070841945
1
Iteration 15700: Loss = -11091.575195742384
2
Iteration 15800: Loss = -11091.572944120462
Iteration 15900: Loss = -11091.573203407761
1
Iteration 16000: Loss = -11091.572855002189
Iteration 16100: Loss = -11091.572824082126
Iteration 16200: Loss = -11091.573378455305
1
Iteration 16300: Loss = -11091.572834811626
Iteration 16400: Loss = -11091.639611794364
1
Iteration 16500: Loss = -11091.572721098508
Iteration 16600: Loss = -11091.594691162061
1
Iteration 16700: Loss = -11091.572628747781
Iteration 16800: Loss = -11091.572407514204
Iteration 16900: Loss = -11091.572641503513
1
Iteration 17000: Loss = -11091.59282117267
2
Iteration 17100: Loss = -11091.572412837526
Iteration 17200: Loss = -11091.572528103932
1
Iteration 17300: Loss = -11091.572503260442
Iteration 17400: Loss = -11091.572486688929
Iteration 17500: Loss = -11091.574897379181
1
Iteration 17600: Loss = -11091.618979959841
2
Iteration 17700: Loss = -11091.572336802275
Iteration 17800: Loss = -11091.574125827758
1
Iteration 17900: Loss = -11091.572294595175
Iteration 18000: Loss = -11091.572499469217
1
Iteration 18100: Loss = -11091.572291183551
Iteration 18200: Loss = -11091.572421340506
1
Iteration 18300: Loss = -11091.572327734806
Iteration 18400: Loss = -11091.572407956268
Iteration 18500: Loss = -11091.572313794666
Iteration 18600: Loss = -11091.572464768487
1
Iteration 18700: Loss = -11091.572309264044
Iteration 18800: Loss = -11091.576251017621
1
Iteration 18900: Loss = -11091.572298339743
Iteration 19000: Loss = -11091.572437183284
1
Iteration 19100: Loss = -11091.572342954641
Iteration 19200: Loss = -11091.572315426798
Iteration 19300: Loss = -11091.625818930926
1
Iteration 19400: Loss = -11091.572314490637
Iteration 19500: Loss = -11091.572289469701
Iteration 19600: Loss = -11091.572395491397
1
Iteration 19700: Loss = -11091.708996346748
2
Iteration 19800: Loss = -11091.57226343789
Iteration 19900: Loss = -11091.572337638378
pi: tensor([[0.7676, 0.2324],
        [0.2245, 0.7755]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4603, 0.5397], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1970, 0.0952],
         [0.5913, 0.2887]],

        [[0.5678, 0.0998],
         [0.5286, 0.6510]],

        [[0.7047, 0.0997],
         [0.5724, 0.7307]],

        [[0.6975, 0.0999],
         [0.6318, 0.5638]],

        [[0.6288, 0.1013],
         [0.5916, 0.5981]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9291542517314568
Average Adjusted Rand Index: 0.9286432231355439
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20570.34938755567
Iteration 100: Loss = -11386.630355020814
Iteration 200: Loss = -11385.604127648336
Iteration 300: Loss = -11383.980431141947
Iteration 400: Loss = -11376.857710767932
Iteration 500: Loss = -11192.719931668198
Iteration 600: Loss = -11119.863460549022
Iteration 700: Loss = -11104.079765891862
Iteration 800: Loss = -11092.872113980653
Iteration 900: Loss = -11092.686428179459
Iteration 1000: Loss = -11092.635860270138
Iteration 1100: Loss = -11092.599927936237
Iteration 1200: Loss = -11092.534655726346
Iteration 1300: Loss = -11092.456540192079
Iteration 1400: Loss = -11092.36396695695
Iteration 1500: Loss = -11092.27404132309
Iteration 1600: Loss = -11092.26512323002
Iteration 1700: Loss = -11092.253914376073
Iteration 1800: Loss = -11092.246215688072
Iteration 1900: Loss = -11092.244286805439
Iteration 2000: Loss = -11092.239115923623
Iteration 2100: Loss = -11092.240619019281
1
Iteration 2200: Loss = -11092.236160550308
Iteration 2300: Loss = -11092.23137334946
Iteration 2400: Loss = -11092.228253431207
Iteration 2500: Loss = -11092.219712598875
Iteration 2600: Loss = -11092.167328821737
Iteration 2700: Loss = -11092.164699990275
Iteration 2800: Loss = -11092.161251248432
Iteration 2900: Loss = -11092.136078441694
Iteration 3000: Loss = -11092.14417108681
1
Iteration 3100: Loss = -11092.13566146278
Iteration 3200: Loss = -11092.133611784353
Iteration 3300: Loss = -11092.146777050582
1
Iteration 3400: Loss = -11092.131972528103
Iteration 3500: Loss = -11092.131525009274
Iteration 3600: Loss = -11092.130979438118
Iteration 3700: Loss = -11092.13022432661
Iteration 3800: Loss = -11092.130065673738
Iteration 3900: Loss = -11092.129057422831
Iteration 4000: Loss = -11092.129048513976
Iteration 4100: Loss = -11092.127541208496
Iteration 4200: Loss = -11092.105086528734
Iteration 4300: Loss = -11092.098979599488
Iteration 4400: Loss = -11092.10215451428
1
Iteration 4500: Loss = -11092.09902559576
Iteration 4600: Loss = -11092.09808414594
Iteration 4700: Loss = -11092.09774630875
Iteration 4800: Loss = -11092.106164034005
1
Iteration 4900: Loss = -11092.090954223762
Iteration 5000: Loss = -11091.681846634756
Iteration 5100: Loss = -11091.68043740047
Iteration 5200: Loss = -11091.680742669825
1
Iteration 5300: Loss = -11091.67562164439
Iteration 5400: Loss = -11091.633827339312
Iteration 5500: Loss = -11091.638189390693
1
Iteration 5600: Loss = -11091.632785704165
Iteration 5700: Loss = -11091.631754571394
Iteration 5800: Loss = -11091.631698459665
Iteration 5900: Loss = -11091.631327803441
Iteration 6000: Loss = -11091.63093078074
Iteration 6100: Loss = -11091.65879456851
1
Iteration 6200: Loss = -11091.630718926797
Iteration 6300: Loss = -11091.630629778843
Iteration 6400: Loss = -11091.630513280576
Iteration 6500: Loss = -11091.63037541445
Iteration 6600: Loss = -11091.636684776353
1
Iteration 6700: Loss = -11091.631813172131
2
Iteration 6800: Loss = -11091.628867282057
Iteration 6900: Loss = -11091.629761347822
1
Iteration 7000: Loss = -11091.631434243427
2
Iteration 7100: Loss = -11091.629218518718
3
Iteration 7200: Loss = -11091.628804787311
Iteration 7300: Loss = -11091.628668934933
Iteration 7400: Loss = -11091.632902974186
1
Iteration 7500: Loss = -11091.630287019785
2
Iteration 7600: Loss = -11091.638423102433
3
Iteration 7700: Loss = -11091.636876344135
4
Iteration 7800: Loss = -11091.628474318495
Iteration 7900: Loss = -11091.628378720468
Iteration 8000: Loss = -11091.628252362923
Iteration 8100: Loss = -11091.675852049411
1
Iteration 8200: Loss = -11091.628113238812
Iteration 8300: Loss = -11091.626102225431
Iteration 8400: Loss = -11091.626643778003
1
Iteration 8500: Loss = -11091.625847146897
Iteration 8600: Loss = -11091.626152476312
1
Iteration 8700: Loss = -11091.625575572823
Iteration 8800: Loss = -11091.623254637801
Iteration 8900: Loss = -11091.623085765903
Iteration 9000: Loss = -11091.623352075174
1
Iteration 9100: Loss = -11091.623131088883
Iteration 9200: Loss = -11091.623083894614
Iteration 9300: Loss = -11091.624201638486
1
Iteration 9400: Loss = -11091.624118771982
2
Iteration 9500: Loss = -11091.622563321966
Iteration 9600: Loss = -11091.622552494768
Iteration 9700: Loss = -11091.622539747546
Iteration 9800: Loss = -11091.623241346026
1
Iteration 9900: Loss = -11091.700832665198
2
Iteration 10000: Loss = -11091.620789364519
Iteration 10100: Loss = -11091.664104025445
1
Iteration 10200: Loss = -11091.620621082635
Iteration 10300: Loss = -11091.630267300428
1
Iteration 10400: Loss = -11091.660763513688
2
Iteration 10500: Loss = -11091.620015042201
Iteration 10600: Loss = -11091.62038418142
1
Iteration 10700: Loss = -11091.64150364641
2
Iteration 10800: Loss = -11091.62121229123
3
Iteration 10900: Loss = -11091.893439065158
4
Iteration 11000: Loss = -11091.619825693926
Iteration 11100: Loss = -11091.650307781792
1
Iteration 11200: Loss = -11091.619815261634
Iteration 11300: Loss = -11091.731776585864
1
Iteration 11400: Loss = -11091.619758507257
Iteration 11500: Loss = -11091.667378981881
1
Iteration 11600: Loss = -11091.619726532086
Iteration 11700: Loss = -11091.625922753261
1
Iteration 11800: Loss = -11091.621728712555
2
Iteration 11900: Loss = -11091.620517579746
3
Iteration 12000: Loss = -11091.649857755361
4
Iteration 12100: Loss = -11091.675865766834
5
Iteration 12200: Loss = -11091.614858495057
Iteration 12300: Loss = -11091.61486363988
Iteration 12400: Loss = -11091.7612886114
1
Iteration 12500: Loss = -11091.614795938185
Iteration 12600: Loss = -11091.646460434884
1
Iteration 12700: Loss = -11091.614751169804
Iteration 12800: Loss = -11091.659635514137
1
Iteration 12900: Loss = -11091.614804725421
Iteration 13000: Loss = -11091.614924210171
1
Iteration 13100: Loss = -11091.614758783573
Iteration 13200: Loss = -11091.614735880254
Iteration 13300: Loss = -11091.615789103382
1
Iteration 13400: Loss = -11091.614703922081
Iteration 13500: Loss = -11091.616813022929
1
Iteration 13600: Loss = -11091.613885096564
Iteration 13700: Loss = -11091.612879326707
Iteration 13800: Loss = -11091.612976834185
Iteration 13900: Loss = -11091.614484817132
1
Iteration 14000: Loss = -11091.759926412651
2
Iteration 14100: Loss = -11091.612507185771
Iteration 14200: Loss = -11091.616971724594
1
Iteration 14300: Loss = -11091.612483600977
Iteration 14400: Loss = -11091.620499988592
1
Iteration 14500: Loss = -11091.612422943379
Iteration 14600: Loss = -11091.90398692253
1
Iteration 14700: Loss = -11091.612410394948
Iteration 14800: Loss = -11091.682843453073
1
Iteration 14900: Loss = -11091.647621829281
2
Iteration 15000: Loss = -11091.617626950567
3
Iteration 15100: Loss = -11091.61069875572
Iteration 15200: Loss = -11091.610495714287
Iteration 15300: Loss = -11091.67828690738
1
Iteration 15400: Loss = -11091.61030949777
Iteration 15500: Loss = -11091.592381893985
Iteration 15600: Loss = -11091.596138459598
1
Iteration 15700: Loss = -11091.665473371502
2
Iteration 15800: Loss = -11091.668631070595
3
Iteration 15900: Loss = -11091.592674207017
4
Iteration 16000: Loss = -11091.59778644983
5
Iteration 16100: Loss = -11091.591620047513
Iteration 16200: Loss = -11091.653380447637
1
Iteration 16300: Loss = -11091.589479238915
Iteration 16400: Loss = -11091.617794356787
1
Iteration 16500: Loss = -11091.59749320612
2
Iteration 16600: Loss = -11091.592771862071
3
Iteration 16700: Loss = -11091.589420237733
Iteration 16800: Loss = -11091.609315155105
1
Iteration 16900: Loss = -11091.589419253982
Iteration 17000: Loss = -11091.589392223414
Iteration 17100: Loss = -11091.590024737456
1
Iteration 17200: Loss = -11091.589438135015
Iteration 17300: Loss = -11091.649210183414
1
Iteration 17400: Loss = -11091.589311014597
Iteration 17500: Loss = -11091.589298079043
Iteration 17600: Loss = -11091.589502343939
1
Iteration 17700: Loss = -11091.589313538012
Iteration 17800: Loss = -11091.601452594026
1
Iteration 17900: Loss = -11091.592195773119
2
Iteration 18000: Loss = -11091.600386386224
3
Iteration 18100: Loss = -11091.590634107317
4
Iteration 18200: Loss = -11091.62046434716
5
Iteration 18300: Loss = -11091.589203127529
Iteration 18400: Loss = -11091.589330124896
1
Iteration 18500: Loss = -11091.67838310277
2
Iteration 18600: Loss = -11091.58919142116
Iteration 18700: Loss = -11091.591242906443
1
Iteration 18800: Loss = -11091.589068377913
Iteration 18900: Loss = -11091.59092136543
1
Iteration 19000: Loss = -11091.589037145292
Iteration 19100: Loss = -11091.591909949144
1
Iteration 19200: Loss = -11091.589060352866
Iteration 19300: Loss = -11091.590489574835
1
Iteration 19400: Loss = -11091.593533428646
2
Iteration 19500: Loss = -11091.643957683573
3
Iteration 19600: Loss = -11091.588608868422
Iteration 19700: Loss = -11091.588631315548
Iteration 19800: Loss = -11091.589341645067
1
Iteration 19900: Loss = -11091.588595131605
pi: tensor([[0.7756, 0.2244],
        [0.2322, 0.7678]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5399, 0.4601], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2887, 0.0952],
         [0.5411, 0.1971]],

        [[0.6248, 0.0998],
         [0.7277, 0.6655]],

        [[0.5308, 0.0997],
         [0.5958, 0.6224]],

        [[0.6006, 0.0998],
         [0.6205, 0.6637]],

        [[0.5116, 0.1012],
         [0.7300, 0.5929]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9291542517314568
Average Adjusted Rand Index: 0.9286432231355439
11115.866066022678
[0.9291542517314568, 0.9291542517314568] [0.9286432231355439, 0.9286432231355439] [11091.57333152059, 11091.575947931597]
-------------------------------------
This iteration is 46
True Objective function: Loss = -11186.397984727699
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22656.433105057822
Iteration 100: Loss = -11448.695928868463
Iteration 200: Loss = -11443.735375326285
Iteration 300: Loss = -11439.114330535065
Iteration 400: Loss = -11427.684211090213
Iteration 500: Loss = -11398.889045994712
Iteration 600: Loss = -11282.496944018587
Iteration 700: Loss = -11241.338384813107
Iteration 800: Loss = -11235.567354254028
Iteration 900: Loss = -11234.909589350269
Iteration 1000: Loss = -11234.601351719106
Iteration 1100: Loss = -11233.896698366025
Iteration 1200: Loss = -11233.38924946164
Iteration 1300: Loss = -11233.239589255729
Iteration 1400: Loss = -11233.14264879563
Iteration 1500: Loss = -11233.073043910505
Iteration 1600: Loss = -11233.020447429017
Iteration 1700: Loss = -11232.978705483682
Iteration 1800: Loss = -11232.945451022964
Iteration 1900: Loss = -11232.920732176402
Iteration 2000: Loss = -11232.901113045802
Iteration 2100: Loss = -11232.884608095887
Iteration 2200: Loss = -11232.865791907037
Iteration 2300: Loss = -11232.779373062622
Iteration 2400: Loss = -11232.770687359758
Iteration 2500: Loss = -11232.761190700465
Iteration 2600: Loss = -11232.75301192084
Iteration 2700: Loss = -11232.744292550595
Iteration 2800: Loss = -11232.732301200036
Iteration 2900: Loss = -11232.724906384268
Iteration 3000: Loss = -11232.720744575663
Iteration 3100: Loss = -11232.717413072241
Iteration 3200: Loss = -11232.714592663839
Iteration 3300: Loss = -11232.711446084015
Iteration 3400: Loss = -11232.709900088781
Iteration 3500: Loss = -11232.706796579381
Iteration 3600: Loss = -11232.714754949515
1
Iteration 3700: Loss = -11232.702578170067
Iteration 3800: Loss = -11232.699881959006
Iteration 3900: Loss = -11232.681937478965
Iteration 4000: Loss = -11232.673152337906
Iteration 4100: Loss = -11232.671719359596
Iteration 4200: Loss = -11232.68007298541
1
Iteration 4300: Loss = -11232.669351852013
Iteration 4400: Loss = -11232.66836101187
Iteration 4500: Loss = -11232.669748107
1
Iteration 4600: Loss = -11232.66662238981
Iteration 4700: Loss = -11232.666065679561
Iteration 4800: Loss = -11232.66511404285
Iteration 4900: Loss = -11232.666901708537
1
Iteration 5000: Loss = -11232.663829565045
Iteration 5100: Loss = -11232.663244374233
Iteration 5200: Loss = -11232.662732129887
Iteration 5300: Loss = -11232.662206779645
Iteration 5400: Loss = -11232.663597865714
1
Iteration 5500: Loss = -11232.661127743204
Iteration 5600: Loss = -11232.660699602016
Iteration 5700: Loss = -11232.659700491766
Iteration 5800: Loss = -11232.658958227814
Iteration 5900: Loss = -11232.658387484498
Iteration 6000: Loss = -11232.657949001377
Iteration 6100: Loss = -11232.657562130082
Iteration 6200: Loss = -11232.682952865267
1
Iteration 6300: Loss = -11232.656564902945
Iteration 6400: Loss = -11232.65918611987
1
Iteration 6500: Loss = -11232.653718181524
Iteration 6600: Loss = -11232.653227686564
Iteration 6700: Loss = -11232.65298580916
Iteration 6800: Loss = -11232.653062656716
Iteration 6900: Loss = -11232.65239652503
Iteration 7000: Loss = -11232.652040477693
Iteration 7100: Loss = -11232.651867331826
Iteration 7200: Loss = -11232.65165794807
Iteration 7300: Loss = -11232.651446841723
Iteration 7400: Loss = -11232.651320121748
Iteration 7500: Loss = -11232.651158587994
Iteration 7600: Loss = -11232.680862801843
1
Iteration 7700: Loss = -11232.65092323259
Iteration 7800: Loss = -11232.65076674214
Iteration 7900: Loss = -11232.711104437958
1
Iteration 8000: Loss = -11232.650574983249
Iteration 8100: Loss = -11232.650469103042
Iteration 8200: Loss = -11232.650448603741
Iteration 8300: Loss = -11232.64976402458
Iteration 8400: Loss = -11232.58548613308
Iteration 8500: Loss = -11232.583289451713
Iteration 8600: Loss = -11232.58929729842
1
Iteration 8700: Loss = -11232.582977808788
Iteration 8800: Loss = -11232.582907996559
Iteration 8900: Loss = -11232.58280720622
Iteration 9000: Loss = -11232.583419722907
1
Iteration 9100: Loss = -11232.586422220113
2
Iteration 9200: Loss = -11232.617829735926
3
Iteration 9300: Loss = -11232.759071994906
4
Iteration 9400: Loss = -11232.579719470003
Iteration 9500: Loss = -11232.578482167211
Iteration 9600: Loss = -11232.549221961182
Iteration 9700: Loss = -11232.55842078754
1
Iteration 9800: Loss = -11232.548850553365
Iteration 9900: Loss = -11232.548758650088
Iteration 10000: Loss = -11232.552283793377
1
Iteration 10100: Loss = -11232.549345546366
2
Iteration 10200: Loss = -11232.54879006144
Iteration 10300: Loss = -11232.54910487797
1
Iteration 10400: Loss = -11232.567182976742
2
Iteration 10500: Loss = -11232.555406524378
3
Iteration 10600: Loss = -11232.548206820347
Iteration 10700: Loss = -11232.548333172785
1
Iteration 10800: Loss = -11232.548725619194
2
Iteration 10900: Loss = -11232.55079045456
3
Iteration 11000: Loss = -11232.548044541545
Iteration 11100: Loss = -11232.547909128405
Iteration 11200: Loss = -11232.553686786832
1
Iteration 11300: Loss = -11232.548017902287
2
Iteration 11400: Loss = -11232.547647944171
Iteration 11500: Loss = -11232.548411048178
1
Iteration 11600: Loss = -11232.567575621022
2
Iteration 11700: Loss = -11232.547876119765
3
Iteration 11800: Loss = -11232.547631454832
Iteration 11900: Loss = -11232.57162641944
1
Iteration 12000: Loss = -11232.54749570752
Iteration 12100: Loss = -11232.56514563604
1
Iteration 12200: Loss = -11232.547632278427
2
Iteration 12300: Loss = -11232.54790296131
3
Iteration 12400: Loss = -11232.557120269294
4
Iteration 12500: Loss = -11232.548768562436
5
Iteration 12600: Loss = -11232.561465971734
6
Iteration 12700: Loss = -11232.554122428786
7
Iteration 12800: Loss = -11232.547918077356
8
Iteration 12900: Loss = -11232.548227055402
9
Iteration 13000: Loss = -11232.54673364812
Iteration 13100: Loss = -11232.55379641823
1
Iteration 13200: Loss = -11232.546677083605
Iteration 13300: Loss = -11232.546661811968
Iteration 13400: Loss = -11232.54672747699
Iteration 13500: Loss = -11232.546617535658
Iteration 13600: Loss = -11232.548182882963
1
Iteration 13700: Loss = -11232.546667492985
Iteration 13800: Loss = -11232.546613688657
Iteration 13900: Loss = -11232.547111266442
1
Iteration 14000: Loss = -11232.546626975954
Iteration 14100: Loss = -11232.548736208897
1
Iteration 14200: Loss = -11232.564375838405
2
Iteration 14300: Loss = -11232.552018294948
3
Iteration 14400: Loss = -11232.548591748626
4
Iteration 14500: Loss = -11232.54670335064
Iteration 14600: Loss = -11232.546943300258
1
Iteration 14700: Loss = -11232.547322338869
2
Iteration 14800: Loss = -11232.54805635411
3
Iteration 14900: Loss = -11232.546743518027
Iteration 15000: Loss = -11232.549329192141
1
Iteration 15100: Loss = -11232.546604966008
Iteration 15200: Loss = -11232.622626686576
1
Iteration 15300: Loss = -11232.547048889433
2
Iteration 15400: Loss = -11232.551164565399
3
Iteration 15500: Loss = -11232.605801932936
4
Iteration 15600: Loss = -11232.546601074542
Iteration 15700: Loss = -11232.548640858917
1
Iteration 15800: Loss = -11232.546555585446
Iteration 15900: Loss = -11232.549270508269
1
Iteration 16000: Loss = -11232.546614562247
Iteration 16100: Loss = -11232.546595656602
Iteration 16200: Loss = -11232.548328169973
1
Iteration 16300: Loss = -11232.634863314202
2
Iteration 16400: Loss = -11232.54793967394
3
Iteration 16500: Loss = -11232.553053318044
4
Iteration 16600: Loss = -11232.67078277062
5
Iteration 16700: Loss = -11232.55168321822
6
Iteration 16800: Loss = -11232.546602321449
Iteration 16900: Loss = -11232.546898974942
1
Iteration 17000: Loss = -11232.549483417764
2
Iteration 17100: Loss = -11232.54656241426
Iteration 17200: Loss = -11232.546830951142
1
Iteration 17300: Loss = -11232.546643870897
Iteration 17400: Loss = -11232.54662383314
Iteration 17500: Loss = -11232.546922414564
1
Iteration 17600: Loss = -11232.547094508685
2
Iteration 17700: Loss = -11232.556366799441
3
Iteration 17800: Loss = -11232.76643294511
4
Iteration 17900: Loss = -11232.546595573667
Iteration 18000: Loss = -11232.550367697973
1
Iteration 18100: Loss = -11232.54656827959
Iteration 18200: Loss = -11232.55091400394
1
Iteration 18300: Loss = -11232.54632571961
Iteration 18400: Loss = -11232.546262278873
Iteration 18500: Loss = -11232.690993274511
1
Iteration 18600: Loss = -11232.546206815643
Iteration 18700: Loss = -11232.548006246263
1
Iteration 18800: Loss = -11232.546218757838
Iteration 18900: Loss = -11232.546810993455
1
Iteration 19000: Loss = -11232.546254950212
Iteration 19100: Loss = -11232.548844284236
1
Iteration 19200: Loss = -11232.580044396593
2
Iteration 19300: Loss = -11232.552512438651
3
Iteration 19400: Loss = -11232.546166115964
Iteration 19500: Loss = -11232.547129101988
1
Iteration 19600: Loss = -11232.546188178212
Iteration 19700: Loss = -11232.54615770849
Iteration 19800: Loss = -11232.546121653586
Iteration 19900: Loss = -11232.548477905304
1
pi: tensor([[0.7288, 0.2712],
        [0.3331, 0.6669]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0104, 0.9896], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3120, 0.3236],
         [0.6030, 0.1822]],

        [[0.6029, 0.0942],
         [0.6471, 0.5288]],

        [[0.5932, 0.1028],
         [0.5260, 0.6977]],

        [[0.6028, 0.1051],
         [0.5668, 0.5885]],

        [[0.7291, 0.1117],
         [0.6921, 0.7249]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.6076306503568575
Average Adjusted Rand Index: 0.7374544367931763
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21565.361870668345
Iteration 100: Loss = -11449.492514952224
Iteration 200: Loss = -11446.006624850275
Iteration 300: Loss = -11443.020487293712
Iteration 400: Loss = -11434.280412183989
Iteration 500: Loss = -11386.914661303554
Iteration 600: Loss = -11266.645952564984
Iteration 700: Loss = -11241.916475425267
Iteration 800: Loss = -11234.999980505932
Iteration 900: Loss = -11234.386655039874
Iteration 1000: Loss = -11233.563494837006
Iteration 1100: Loss = -11233.381756159954
Iteration 1200: Loss = -11233.256556447604
Iteration 1300: Loss = -11233.157785669942
Iteration 1400: Loss = -11233.078854072859
Iteration 1500: Loss = -11233.028167920527
Iteration 1600: Loss = -11232.989863728282
Iteration 1700: Loss = -11232.926993943796
Iteration 1800: Loss = -11232.838229171264
Iteration 1900: Loss = -11232.821149291227
Iteration 2000: Loss = -11232.806987749287
Iteration 2100: Loss = -11232.793829839087
Iteration 2200: Loss = -11232.773246442392
Iteration 2300: Loss = -11232.756528162314
Iteration 2400: Loss = -11232.749542922567
Iteration 2500: Loss = -11232.743968442517
Iteration 2600: Loss = -11232.739147325781
Iteration 2700: Loss = -11232.734949243273
Iteration 2800: Loss = -11232.731400202083
Iteration 2900: Loss = -11232.728081090438
Iteration 3000: Loss = -11232.72511953762
Iteration 3100: Loss = -11232.72754578126
1
Iteration 3200: Loss = -11232.720137795328
Iteration 3300: Loss = -11232.717928440887
Iteration 3400: Loss = -11232.71636985398
Iteration 3500: Loss = -11232.713896887451
Iteration 3600: Loss = -11232.712134953768
Iteration 3700: Loss = -11232.710530547984
Iteration 3800: Loss = -11232.70887529002
Iteration 3900: Loss = -11232.708868224185
Iteration 4000: Loss = -11232.705206263467
Iteration 4100: Loss = -11232.632124610378
Iteration 4200: Loss = -11232.616538843175
Iteration 4300: Loss = -11232.604305243272
Iteration 4400: Loss = -11232.59944605109
Iteration 4500: Loss = -11232.598715610171
Iteration 4600: Loss = -11232.597731758302
Iteration 4700: Loss = -11232.59909963819
1
Iteration 4800: Loss = -11232.600139957107
2
Iteration 4900: Loss = -11232.601479145753
3
Iteration 5000: Loss = -11232.596237326017
Iteration 5100: Loss = -11232.597160368361
1
Iteration 5200: Loss = -11232.594740164424
Iteration 5300: Loss = -11232.59431427677
Iteration 5400: Loss = -11232.593813501939
Iteration 5500: Loss = -11232.594082842488
1
Iteration 5600: Loss = -11232.593118409026
Iteration 5700: Loss = -11232.593037494918
Iteration 5800: Loss = -11232.59243693648
Iteration 5900: Loss = -11232.592208243299
Iteration 6000: Loss = -11232.59187729849
Iteration 6100: Loss = -11232.605383647713
1
Iteration 6200: Loss = -11232.591375436488
Iteration 6300: Loss = -11232.591549911052
1
Iteration 6400: Loss = -11232.590939279533
Iteration 6500: Loss = -11232.590846682158
Iteration 6600: Loss = -11232.590612173739
Iteration 6700: Loss = -11232.590427406385
Iteration 6800: Loss = -11232.594043008206
1
Iteration 6900: Loss = -11232.589958502536
Iteration 7000: Loss = -11232.593000682793
1
Iteration 7100: Loss = -11232.58954083958
Iteration 7200: Loss = -11232.589480821338
Iteration 7300: Loss = -11232.589133863845
Iteration 7400: Loss = -11232.58893090585
Iteration 7500: Loss = -11232.58866621176
Iteration 7600: Loss = -11232.58867717928
Iteration 7700: Loss = -11232.588453192133
Iteration 7800: Loss = -11232.588753099048
1
Iteration 7900: Loss = -11232.588289850823
Iteration 8000: Loss = -11232.588154804962
Iteration 8100: Loss = -11232.588058665106
Iteration 8200: Loss = -11232.587669540027
Iteration 8300: Loss = -11232.592182631359
1
Iteration 8400: Loss = -11232.587322796817
Iteration 8500: Loss = -11232.587332847654
Iteration 8600: Loss = -11232.587187907478
Iteration 8700: Loss = -11232.589957414039
1
Iteration 8800: Loss = -11232.587085871855
Iteration 8900: Loss = -11232.587014862534
Iteration 9000: Loss = -11232.587139049514
1
Iteration 9100: Loss = -11232.586890298631
Iteration 9200: Loss = -11232.668613023608
1
Iteration 9300: Loss = -11232.58664937596
Iteration 9400: Loss = -11232.586412515158
Iteration 9500: Loss = -11232.587939268196
1
Iteration 9600: Loss = -11232.585244373127
Iteration 9700: Loss = -11232.583970112588
Iteration 9800: Loss = -11232.583508618469
Iteration 9900: Loss = -11232.584678891182
1
Iteration 10000: Loss = -11232.583475057028
Iteration 10100: Loss = -11232.58374415661
1
Iteration 10200: Loss = -11232.583536546541
Iteration 10300: Loss = -11232.583845557932
1
Iteration 10400: Loss = -11232.584124761648
2
Iteration 10500: Loss = -11232.581958071783
Iteration 10600: Loss = -11232.614376381882
1
Iteration 10700: Loss = -11232.581735156615
Iteration 10800: Loss = -11232.581760367584
Iteration 10900: Loss = -11232.582297093018
1
Iteration 11000: Loss = -11232.672515214048
2
Iteration 11100: Loss = -11232.581651686605
Iteration 11200: Loss = -11232.625631865501
1
Iteration 11300: Loss = -11232.581588871552
Iteration 11400: Loss = -11232.61301600087
1
Iteration 11500: Loss = -11232.581897073524
2
Iteration 11600: Loss = -11232.581971190615
3
Iteration 11700: Loss = -11232.58161097987
Iteration 11800: Loss = -11232.612751011487
1
Iteration 11900: Loss = -11232.574142713891
Iteration 12000: Loss = -11232.552232521994
Iteration 12100: Loss = -11232.553643117823
1
Iteration 12200: Loss = -11232.551810714273
Iteration 12300: Loss = -11232.552229909543
1
Iteration 12400: Loss = -11232.5553006864
2
Iteration 12500: Loss = -11232.551801697895
Iteration 12600: Loss = -11232.55204993602
1
Iteration 12700: Loss = -11232.553247418504
2
Iteration 12800: Loss = -11232.554712389352
3
Iteration 12900: Loss = -11232.601780947722
4
Iteration 13000: Loss = -11232.552869654053
5
Iteration 13100: Loss = -11232.551702892162
Iteration 13200: Loss = -11232.55181878945
1
Iteration 13300: Loss = -11232.549039011572
Iteration 13400: Loss = -11232.549993495579
1
Iteration 13500: Loss = -11232.573319361773
2
Iteration 13600: Loss = -11232.549885655444
3
Iteration 13700: Loss = -11232.550534808277
4
Iteration 13800: Loss = -11232.551734806038
5
Iteration 13900: Loss = -11232.549300579749
6
Iteration 14000: Loss = -11232.548498043438
Iteration 14100: Loss = -11232.548367770298
Iteration 14200: Loss = -11232.55326230493
1
Iteration 14300: Loss = -11232.567255394017
2
Iteration 14400: Loss = -11232.547969132875
Iteration 14500: Loss = -11232.548739020376
1
Iteration 14600: Loss = -11232.565632192403
2
Iteration 14700: Loss = -11232.551031033823
3
Iteration 14800: Loss = -11232.5477431023
Iteration 14900: Loss = -11232.547973678575
1
Iteration 15000: Loss = -11232.555295764914
2
Iteration 15100: Loss = -11232.551521302308
3
Iteration 15200: Loss = -11232.54662835892
Iteration 15300: Loss = -11232.548872726766
1
Iteration 15400: Loss = -11232.549696376052
2
Iteration 15500: Loss = -11232.547242780867
3
Iteration 15600: Loss = -11232.546319711439
Iteration 15700: Loss = -11232.552372072863
1
Iteration 15800: Loss = -11232.64101402942
2
Iteration 15900: Loss = -11232.54624896062
Iteration 16000: Loss = -11232.546372803554
1
Iteration 16100: Loss = -11232.559395970968
2
Iteration 16200: Loss = -11232.567556703976
3
Iteration 16300: Loss = -11232.546232792321
Iteration 16400: Loss = -11232.546745750033
1
Iteration 16500: Loss = -11232.570026980346
2
Iteration 16600: Loss = -11232.546254841625
Iteration 16700: Loss = -11232.547405010644
1
Iteration 16800: Loss = -11232.547971178836
2
Iteration 16900: Loss = -11232.547057491649
3
Iteration 17000: Loss = -11232.720597439094
4
Iteration 17100: Loss = -11232.547555467148
5
Iteration 17200: Loss = -11232.546568477354
6
Iteration 17300: Loss = -11232.546270446941
Iteration 17400: Loss = -11232.556180866437
1
Iteration 17500: Loss = -11232.547178374334
2
Iteration 17600: Loss = -11232.546424303959
3
Iteration 17700: Loss = -11232.546421526753
4
Iteration 17800: Loss = -11232.54650760622
5
Iteration 17900: Loss = -11232.580999802878
6
Iteration 18000: Loss = -11232.548032274117
7
Iteration 18100: Loss = -11232.548092836605
8
Iteration 18200: Loss = -11232.54620241638
Iteration 18300: Loss = -11232.546530009322
1
Iteration 18400: Loss = -11232.546377617451
2
Iteration 18500: Loss = -11232.54629668472
Iteration 18600: Loss = -11232.551075421234
1
Iteration 18700: Loss = -11232.54732650348
2
Iteration 18800: Loss = -11232.6670013485
3
Iteration 18900: Loss = -11232.5682228654
4
Iteration 19000: Loss = -11232.546215009877
Iteration 19100: Loss = -11232.546252546374
Iteration 19200: Loss = -11232.558036115483
1
Iteration 19300: Loss = -11232.54930116977
2
Iteration 19400: Loss = -11232.54620007555
Iteration 19500: Loss = -11232.547732226809
1
Iteration 19600: Loss = -11232.546052921043
Iteration 19700: Loss = -11232.546686286572
1
Iteration 19800: Loss = -11232.546073239482
Iteration 19900: Loss = -11232.546223919142
1
pi: tensor([[0.7291, 0.2709],
        [0.3330, 0.6670]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0104, 0.9896], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3120, 0.3235],
         [0.7122, 0.1821]],

        [[0.5322, 0.0942],
         [0.5765, 0.5721]],

        [[0.7069, 0.1033],
         [0.6165, 0.5328]],

        [[0.6274, 0.1050],
         [0.6805, 0.5396]],

        [[0.5508, 0.1117],
         [0.7095, 0.6220]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.6076306503568575
Average Adjusted Rand Index: 0.7374544367931763
11186.397984727699
[0.6076306503568575, 0.6076306503568575] [0.7374544367931763, 0.7374544367931763] [11232.546420581572, 11232.547427299956]
-------------------------------------
This iteration is 47
True Objective function: Loss = -11157.095394019521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23536.25239283242
Iteration 100: Loss = -11455.46325049923
Iteration 200: Loss = -11453.65652530874
Iteration 300: Loss = -11448.757429160036
Iteration 400: Loss = -11354.557993339866
Iteration 500: Loss = -11208.926741098043
Iteration 600: Loss = -11204.801959340966
Iteration 700: Loss = -11204.015688069943
Iteration 800: Loss = -11203.85668971596
Iteration 900: Loss = -11203.737933888071
Iteration 1000: Loss = -11203.596187379846
Iteration 1100: Loss = -11203.17996372727
Iteration 1200: Loss = -11196.568434762516
Iteration 1300: Loss = -11191.809000621919
Iteration 1400: Loss = -11191.548432502062
Iteration 1500: Loss = -11190.94041776074
Iteration 1600: Loss = -11189.550424618159
Iteration 1700: Loss = -11189.019909101504
Iteration 1800: Loss = -11188.978138480019
Iteration 1900: Loss = -11188.946422632667
Iteration 2000: Loss = -11188.935770934471
Iteration 2100: Loss = -11188.928753848097
Iteration 2200: Loss = -11188.923566924388
Iteration 2300: Loss = -11188.919820249559
Iteration 2400: Loss = -11188.916302357153
Iteration 2500: Loss = -11188.913568972945
Iteration 2600: Loss = -11188.915631146569
1
Iteration 2700: Loss = -11188.909350152459
Iteration 2800: Loss = -11188.907713679775
Iteration 2900: Loss = -11188.906242381992
Iteration 3000: Loss = -11188.907396283663
1
Iteration 3100: Loss = -11188.903844937542
Iteration 3200: Loss = -11188.902845354654
Iteration 3300: Loss = -11188.902082441276
Iteration 3400: Loss = -11188.901796165475
Iteration 3500: Loss = -11188.900402659381
Iteration 3600: Loss = -11188.901369604571
1
Iteration 3700: Loss = -11188.899109643377
Iteration 3800: Loss = -11188.898636588174
Iteration 3900: Loss = -11188.901635976114
1
Iteration 4000: Loss = -11188.897666621355
Iteration 4100: Loss = -11188.897191570986
Iteration 4200: Loss = -11188.89822724846
1
Iteration 4300: Loss = -11188.896346116095
Iteration 4400: Loss = -11188.89595992409
Iteration 4500: Loss = -11188.895785921151
Iteration 4600: Loss = -11188.895211227205
Iteration 4700: Loss = -11188.894775589519
Iteration 4800: Loss = -11188.894285961656
Iteration 4900: Loss = -11188.893876007933
Iteration 5000: Loss = -11188.89361070533
Iteration 5100: Loss = -11188.89338467849
Iteration 5200: Loss = -11188.893306364811
Iteration 5300: Loss = -11188.892987039007
Iteration 5400: Loss = -11188.892778394296
Iteration 5500: Loss = -11188.900908128502
1
Iteration 5600: Loss = -11188.89294644638
2
Iteration 5700: Loss = -11188.892810064212
Iteration 5800: Loss = -11188.893427389354
1
Iteration 5900: Loss = -11188.893285166083
2
Iteration 6000: Loss = -11188.891855278765
Iteration 6100: Loss = -11188.891797983408
Iteration 6200: Loss = -11188.891704479236
Iteration 6300: Loss = -11188.892119331762
1
Iteration 6400: Loss = -11188.891483538331
Iteration 6500: Loss = -11188.891335530041
Iteration 6600: Loss = -11188.891450137251
1
Iteration 6700: Loss = -11188.912856505609
2
Iteration 6800: Loss = -11188.890851901751
Iteration 6900: Loss = -11188.890744515644
Iteration 7000: Loss = -11188.889732450572
Iteration 7100: Loss = -11188.89056165674
1
Iteration 7200: Loss = -11188.88960950691
Iteration 7300: Loss = -11188.889521226603
Iteration 7400: Loss = -11188.889703481495
1
Iteration 7500: Loss = -11188.890051032606
2
Iteration 7600: Loss = -11188.88868322287
Iteration 7700: Loss = -11188.887941515079
Iteration 7800: Loss = -11188.90440575354
1
Iteration 7900: Loss = -11188.887801771576
Iteration 8000: Loss = -11188.887665175936
Iteration 8100: Loss = -11188.887905915082
1
Iteration 8200: Loss = -11188.888196629916
2
Iteration 8300: Loss = -11188.886920442108
Iteration 8400: Loss = -11188.88700141747
Iteration 8500: Loss = -11188.887013477226
Iteration 8600: Loss = -11188.886865943814
Iteration 8700: Loss = -11188.886852131618
Iteration 8800: Loss = -11188.886904475365
Iteration 8900: Loss = -11188.886799464215
Iteration 9000: Loss = -11188.88712559761
1
Iteration 9100: Loss = -11188.888524016473
2
Iteration 9200: Loss = -11188.890357952034
3
Iteration 9300: Loss = -11188.886751398597
Iteration 9400: Loss = -11188.89546847831
1
Iteration 9500: Loss = -11188.889890258397
2
Iteration 9600: Loss = -11188.887098767214
3
Iteration 9700: Loss = -11188.889487162252
4
Iteration 9800: Loss = -11188.914268204791
5
Iteration 9900: Loss = -11188.887511286328
6
Iteration 10000: Loss = -11188.895621176944
7
Iteration 10100: Loss = -11188.885581135351
Iteration 10200: Loss = -11188.8865376441
1
Iteration 10300: Loss = -11189.00197088858
2
Iteration 10400: Loss = -11188.885551815645
Iteration 10500: Loss = -11188.893185071049
1
Iteration 10600: Loss = -11188.893067468784
2
Iteration 10700: Loss = -11188.885575567741
Iteration 10800: Loss = -11188.889795487043
1
Iteration 10900: Loss = -11188.887803346788
2
Iteration 11000: Loss = -11188.886716212755
3
Iteration 11100: Loss = -11188.886589596388
4
Iteration 11200: Loss = -11188.94086267503
5
Iteration 11300: Loss = -11188.885480728344
Iteration 11400: Loss = -11188.889849781008
1
Iteration 11500: Loss = -11188.913847423708
2
Iteration 11600: Loss = -11188.885516540991
Iteration 11700: Loss = -11188.88580522012
1
Iteration 11800: Loss = -11189.072493062396
2
Iteration 11900: Loss = -11188.88546265367
Iteration 12000: Loss = -11188.893070816792
1
Iteration 12100: Loss = -11188.885436817569
Iteration 12200: Loss = -11188.888364316364
1
Iteration 12300: Loss = -11188.885433282006
Iteration 12400: Loss = -11188.917327773315
1
Iteration 12500: Loss = -11188.885064601498
Iteration 12600: Loss = -11188.885078146832
Iteration 12700: Loss = -11188.875055476034
Iteration 12800: Loss = -11188.87407176755
Iteration 12900: Loss = -11188.873146350179
Iteration 13000: Loss = -11188.878328391082
1
Iteration 13100: Loss = -11188.874199651462
2
Iteration 13200: Loss = -11188.873030954677
Iteration 13300: Loss = -11188.933612120802
1
Iteration 13400: Loss = -11188.872785044445
Iteration 13500: Loss = -11188.873654724142
1
Iteration 13600: Loss = -11188.872813599779
Iteration 13700: Loss = -11188.872908205678
Iteration 13800: Loss = -11188.872697672394
Iteration 13900: Loss = -11188.875626429672
1
Iteration 14000: Loss = -11188.87411093276
2
Iteration 14100: Loss = -11188.872702808614
Iteration 14200: Loss = -11188.873028779963
1
Iteration 14300: Loss = -11188.95118915903
2
Iteration 14400: Loss = -11188.872806166824
3
Iteration 14500: Loss = -11188.873129932146
4
Iteration 14600: Loss = -11188.872523368738
Iteration 14700: Loss = -11188.872888124577
1
Iteration 14800: Loss = -11188.872570130156
Iteration 14900: Loss = -11188.936846738288
1
Iteration 15000: Loss = -11188.902656782055
2
Iteration 15100: Loss = -11188.878170774682
3
Iteration 15200: Loss = -11188.873416948272
4
Iteration 15300: Loss = -11188.872549709029
Iteration 15400: Loss = -11188.878240167767
1
Iteration 15500: Loss = -11188.876180844843
2
Iteration 15600: Loss = -11188.872540450713
Iteration 15700: Loss = -11188.917077843089
1
Iteration 15800: Loss = -11188.87252948552
Iteration 15900: Loss = -11188.899226129117
1
Iteration 16000: Loss = -11188.872601807938
Iteration 16100: Loss = -11188.888308672087
1
Iteration 16200: Loss = -11188.873491607079
2
Iteration 16300: Loss = -11188.872934099023
3
Iteration 16400: Loss = -11188.8884799814
4
Iteration 16500: Loss = -11188.87256807938
Iteration 16600: Loss = -11188.983874063193
1
Iteration 16700: Loss = -11188.872569762128
Iteration 16800: Loss = -11188.889849237688
1
Iteration 16900: Loss = -11188.886186307367
2
Iteration 17000: Loss = -11188.938037533235
3
Iteration 17100: Loss = -11188.87418325119
4
Iteration 17200: Loss = -11188.872536596553
Iteration 17300: Loss = -11188.875219670812
1
Iteration 17400: Loss = -11188.875781774166
2
Iteration 17500: Loss = -11188.873684477492
3
Iteration 17600: Loss = -11188.87267908008
4
Iteration 17700: Loss = -11188.874038224118
5
Iteration 17800: Loss = -11188.872686422417
6
Iteration 17900: Loss = -11188.872514047955
Iteration 18000: Loss = -11188.872753764168
1
Iteration 18100: Loss = -11188.872850087317
2
Iteration 18200: Loss = -11188.900789041156
3
Iteration 18300: Loss = -11188.89097999183
4
Iteration 18400: Loss = -11188.872527253363
Iteration 18500: Loss = -11188.873171463572
1
Iteration 18600: Loss = -11188.872942360615
2
Iteration 18700: Loss = -11188.883525945766
3
Iteration 18800: Loss = -11188.874069902788
4
Iteration 18900: Loss = -11188.872730382906
5
Iteration 19000: Loss = -11188.875514774818
6
Iteration 19100: Loss = -11188.872446244093
Iteration 19200: Loss = -11188.872939700916
1
Iteration 19300: Loss = -11188.872488837875
Iteration 19400: Loss = -11188.897934155735
1
Iteration 19500: Loss = -11188.8724516509
Iteration 19600: Loss = -11188.885166123318
1
Iteration 19700: Loss = -11188.872477217328
Iteration 19800: Loss = -11188.874194058708
1
Iteration 19900: Loss = -11188.872501544432
pi: tensor([[0.6408, 0.3592],
        [0.2934, 0.7066]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8228, 0.1772], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2052, 0.0961],
         [0.5477, 0.2952]],

        [[0.7111, 0.0928],
         [0.6807, 0.5604]],

        [[0.5785, 0.0930],
         [0.5343, 0.5562]],

        [[0.5505, 0.0986],
         [0.6420, 0.6887]],

        [[0.5086, 0.1001],
         [0.7277, 0.6854]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.06185376904940662
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824276204858761
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.5174423611744878
Average Adjusted Rand Index: 0.7571779432407513
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23396.522322362416
Iteration 100: Loss = -11455.6708952772
Iteration 200: Loss = -11454.320460051966
Iteration 300: Loss = -11452.151174641931
Iteration 400: Loss = -11449.795371887833
Iteration 500: Loss = -11445.018282655887
Iteration 600: Loss = -11365.907489218655
Iteration 700: Loss = -11233.85795537112
Iteration 800: Loss = -11212.61872348853
Iteration 900: Loss = -11211.619179359748
Iteration 1000: Loss = -11211.071562443622
Iteration 1100: Loss = -11206.595086450441
Iteration 1200: Loss = -11206.470789062294
Iteration 1300: Loss = -11206.372308738206
Iteration 1400: Loss = -11204.072906658863
Iteration 1500: Loss = -11203.998664488421
Iteration 1600: Loss = -11203.960809239634
Iteration 1700: Loss = -11203.930677403378
Iteration 1800: Loss = -11203.9052805337
Iteration 1900: Loss = -11203.88199853578
Iteration 2000: Loss = -11203.86247205546
Iteration 2100: Loss = -11203.848702598536
Iteration 2200: Loss = -11203.83704685489
Iteration 2300: Loss = -11203.826519324386
Iteration 2400: Loss = -11203.816460697388
Iteration 2500: Loss = -11203.806841004987
Iteration 2600: Loss = -11203.797267866554
Iteration 2700: Loss = -11203.789504058086
Iteration 2800: Loss = -11203.779845634774
Iteration 2900: Loss = -11203.800420216221
1
Iteration 3000: Loss = -11203.754663381305
Iteration 3100: Loss = -11203.747333191686
Iteration 3200: Loss = -11203.763835288073
1
Iteration 3300: Loss = -11203.726732469622
Iteration 3400: Loss = -11203.706518533789
Iteration 3500: Loss = -11203.659093338349
Iteration 3600: Loss = -11201.707678662648
Iteration 3700: Loss = -11194.635377164434
Iteration 3800: Loss = -11193.007720519488
Iteration 3900: Loss = -11189.92394052263
Iteration 4000: Loss = -11189.336911208491
Iteration 4100: Loss = -11188.520005145589
Iteration 4200: Loss = -11188.502226818679
Iteration 4300: Loss = -11188.491574459202
Iteration 4400: Loss = -11188.480693111715
Iteration 4500: Loss = -11188.477940563942
Iteration 4600: Loss = -11188.476559212275
Iteration 4700: Loss = -11188.479053630846
1
Iteration 4800: Loss = -11188.473459579624
Iteration 4900: Loss = -11188.472511867532
Iteration 5000: Loss = -11188.4788730366
1
Iteration 5100: Loss = -11188.478661080077
2
Iteration 5200: Loss = -11188.470122968263
Iteration 5300: Loss = -11188.469760599828
Iteration 5400: Loss = -11188.469353449564
Iteration 5500: Loss = -11188.468396961662
Iteration 5600: Loss = -11188.467738826594
Iteration 5700: Loss = -11188.47373053216
1
Iteration 5800: Loss = -11188.46863598199
2
Iteration 5900: Loss = -11188.466742525641
Iteration 6000: Loss = -11188.466059324443
Iteration 6100: Loss = -11188.465820563584
Iteration 6200: Loss = -11188.466829504998
1
Iteration 6300: Loss = -11188.477505536946
2
Iteration 6400: Loss = -11188.473049442639
3
Iteration 6500: Loss = -11188.46723425791
4
Iteration 6600: Loss = -11188.46474387146
Iteration 6700: Loss = -11188.464584785397
Iteration 6800: Loss = -11188.465011366441
1
Iteration 6900: Loss = -11188.464275284801
Iteration 7000: Loss = -11188.468094870252
1
Iteration 7100: Loss = -11188.463956086549
Iteration 7200: Loss = -11188.463862946406
Iteration 7300: Loss = -11188.464602936763
1
Iteration 7400: Loss = -11188.463691038023
Iteration 7500: Loss = -11188.46692940767
1
Iteration 7600: Loss = -11188.488229336574
2
Iteration 7700: Loss = -11188.463413915548
Iteration 7800: Loss = -11188.46331307771
Iteration 7900: Loss = -11188.466971599411
1
Iteration 8000: Loss = -11188.504857850374
2
Iteration 8100: Loss = -11188.46857497212
3
Iteration 8200: Loss = -11188.466809041283
4
Iteration 8300: Loss = -11188.463591978962
5
Iteration 8400: Loss = -11188.529440981112
6
Iteration 8500: Loss = -11188.466302137094
7
Iteration 8600: Loss = -11188.463090887944
Iteration 8700: Loss = -11188.46277037126
Iteration 8800: Loss = -11188.506506088024
1
Iteration 8900: Loss = -11188.462856725526
Iteration 9000: Loss = -11188.470162057982
1
Iteration 9100: Loss = -11188.463439097974
2
Iteration 9200: Loss = -11188.462523757069
Iteration 9300: Loss = -11188.506133928544
1
Iteration 9400: Loss = -11188.462170980029
Iteration 9500: Loss = -11188.515779822941
1
Iteration 9600: Loss = -11188.469045497319
2
Iteration 9700: Loss = -11188.463185236282
3
Iteration 9800: Loss = -11188.460726703068
Iteration 9900: Loss = -11188.461331468861
1
Iteration 10000: Loss = -11188.459420805917
Iteration 10100: Loss = -11188.460011588331
1
Iteration 10200: Loss = -11188.459489244595
Iteration 10300: Loss = -11188.47744646885
1
Iteration 10400: Loss = -11188.461060057496
2
Iteration 10500: Loss = -11188.46329633753
3
Iteration 10600: Loss = -11188.46030793402
4
Iteration 10700: Loss = -11188.459390410553
Iteration 10800: Loss = -11188.635732891064
1
Iteration 10900: Loss = -11188.469966050736
2
Iteration 11000: Loss = -11188.468246066666
3
Iteration 11100: Loss = -11188.460081359044
4
Iteration 11200: Loss = -11188.493544538562
5
Iteration 11300: Loss = -11188.459129965118
Iteration 11400: Loss = -11188.459625302448
1
Iteration 11500: Loss = -11188.460851636093
2
Iteration 11600: Loss = -11188.462837187397
3
Iteration 11700: Loss = -11188.525697828833
4
Iteration 11800: Loss = -11188.45932635065
5
Iteration 11900: Loss = -11188.470364386783
6
Iteration 12000: Loss = -11188.48264687195
7
Iteration 12100: Loss = -11188.459112492368
Iteration 12200: Loss = -11188.90538463646
1
Iteration 12300: Loss = -11188.459013654741
Iteration 12400: Loss = -11188.460734695464
1
Iteration 12500: Loss = -11188.41776413844
Iteration 12600: Loss = -11188.420101306616
1
Iteration 12700: Loss = -11188.520573652639
2
Iteration 12800: Loss = -11188.417614980637
Iteration 12900: Loss = -11188.420876566943
1
Iteration 13000: Loss = -11188.418294070481
2
Iteration 13100: Loss = -11188.424418518345
3
Iteration 13200: Loss = -11188.430494154474
4
Iteration 13300: Loss = -11188.417314971111
Iteration 13400: Loss = -11188.42051559527
1
Iteration 13500: Loss = -11188.421966207074
2
Iteration 13600: Loss = -11188.424560918796
3
Iteration 13700: Loss = -11188.42049585182
4
Iteration 13800: Loss = -11188.426668750775
5
Iteration 13900: Loss = -11188.419488722815
6
Iteration 14000: Loss = -11188.41711652115
Iteration 14100: Loss = -11188.417657413816
1
Iteration 14200: Loss = -11188.413880911636
Iteration 14300: Loss = -11188.403896879074
Iteration 14400: Loss = -11188.410316036485
1
Iteration 14500: Loss = -11188.405704737037
2
Iteration 14600: Loss = -11188.403795010385
Iteration 14700: Loss = -11188.403759088136
Iteration 14800: Loss = -11188.403865094535
1
Iteration 14900: Loss = -11188.404129855524
2
Iteration 15000: Loss = -11188.404399834548
3
Iteration 15100: Loss = -11188.484707494195
4
Iteration 15200: Loss = -11188.403757322603
Iteration 15300: Loss = -11188.404535436788
1
Iteration 15400: Loss = -11188.403461273816
Iteration 15500: Loss = -11188.403422017638
Iteration 15600: Loss = -11188.447984484708
1
Iteration 15700: Loss = -11188.403482343763
Iteration 15800: Loss = -11188.403436751933
Iteration 15900: Loss = -11188.414445856715
1
Iteration 16000: Loss = -11188.403427019328
Iteration 16100: Loss = -11188.403521394015
Iteration 16200: Loss = -11188.403392026445
Iteration 16300: Loss = -11188.404018515294
1
Iteration 16400: Loss = -11188.40441607687
2
Iteration 16500: Loss = -11188.40414841875
3
Iteration 16600: Loss = -11188.407457217936
4
Iteration 16700: Loss = -11188.459942228134
5
Iteration 16800: Loss = -11188.403631647881
6
Iteration 16900: Loss = -11188.413704358112
7
Iteration 17000: Loss = -11188.403949139583
8
Iteration 17100: Loss = -11188.404303920044
9
Iteration 17200: Loss = -11188.403585165937
10
Iteration 17300: Loss = -11188.434259014666
11
Iteration 17400: Loss = -11188.408702362496
12
Iteration 17500: Loss = -11188.405102812972
13
Iteration 17600: Loss = -11188.402964109591
Iteration 17700: Loss = -11188.469918144434
1
Iteration 17800: Loss = -11188.401892722384
Iteration 17900: Loss = -11188.402104960674
1
Iteration 18000: Loss = -11188.457192822583
2
Iteration 18100: Loss = -11188.401835955097
Iteration 18200: Loss = -11188.40521889745
1
Iteration 18300: Loss = -11188.407206045118
2
Iteration 18400: Loss = -11188.402262505679
3
Iteration 18500: Loss = -11188.402927528967
4
Iteration 18600: Loss = -11188.41155927936
5
Iteration 18700: Loss = -11188.402646361556
6
Iteration 18800: Loss = -11188.402371806396
7
Iteration 18900: Loss = -11188.414665548053
8
Iteration 19000: Loss = -11188.403228207391
9
Iteration 19100: Loss = -11188.402359816708
10
Iteration 19200: Loss = -11188.40196476061
11
Iteration 19300: Loss = -11188.405250073965
12
Iteration 19400: Loss = -11188.41829386717
13
Iteration 19500: Loss = -11188.401789054145
Iteration 19600: Loss = -11188.401784040278
Iteration 19700: Loss = -11188.401907406173
1
Iteration 19800: Loss = -11188.401764694692
Iteration 19900: Loss = -11188.403631776733
1
pi: tensor([[0.6984, 0.3016],
        [0.3637, 0.6363]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2017, 0.7983], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2938, 0.0969],
         [0.6638, 0.2076]],

        [[0.6540, 0.0927],
         [0.6191, 0.6608]],

        [[0.6897, 0.0931],
         [0.5103, 0.5165]],

        [[0.5612, 0.0983],
         [0.6406, 0.5157]],

        [[0.6236, 0.0998],
         [0.5913, 0.5259]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 67
Adjusted Rand Index: 0.10971874809346592
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824276204858761
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.49461045833229156
Average Adjusted Rand Index: 0.7667506597930215
11157.095394019521
[0.5174423611744878, 0.49461045833229156] [0.7571779432407513, 0.7667506597930215] [11188.877564952012, 11188.401747814438]
-------------------------------------
This iteration is 48
True Objective function: Loss = -11099.320918385527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23254.000534605202
Iteration 100: Loss = -11396.946479351942
Iteration 200: Loss = -11396.153456371332
Iteration 300: Loss = -11395.80905744513
Iteration 400: Loss = -11395.440436478973
Iteration 500: Loss = -11394.883903003858
Iteration 600: Loss = -11394.202264524843
Iteration 700: Loss = -11393.495638962739
Iteration 800: Loss = -11392.942778964327
Iteration 900: Loss = -11392.580513012756
Iteration 1000: Loss = -11392.230437608201
Iteration 1100: Loss = -11391.699346591415
Iteration 1200: Loss = -11390.830164606456
Iteration 1300: Loss = -11390.314720250479
Iteration 1400: Loss = -11389.939857517142
Iteration 1500: Loss = -11389.209461686105
Iteration 1600: Loss = -11314.505645523128
Iteration 1700: Loss = -11213.31696805149
Iteration 1800: Loss = -11125.901015836602
Iteration 1900: Loss = -11107.500165405694
Iteration 2000: Loss = -11087.269953869984
Iteration 2100: Loss = -11086.915003390426
Iteration 2200: Loss = -11081.81373007482
Iteration 2300: Loss = -11081.689699110624
Iteration 2400: Loss = -11081.645871304077
Iteration 2500: Loss = -11081.609564816392
Iteration 2600: Loss = -11081.466886387896
Iteration 2700: Loss = -11081.428752284171
Iteration 2800: Loss = -11081.261737143894
Iteration 2900: Loss = -11081.232583367262
Iteration 3000: Loss = -11081.215639896622
Iteration 3100: Loss = -11081.21136145532
Iteration 3200: Loss = -11081.199516947396
Iteration 3300: Loss = -11081.192983914018
Iteration 3400: Loss = -11081.18698122319
Iteration 3500: Loss = -11081.180708013175
Iteration 3600: Loss = -11081.172458638615
Iteration 3700: Loss = -11081.041308118627
Iteration 3800: Loss = -11081.016990992843
Iteration 3900: Loss = -11081.00642689647
Iteration 4000: Loss = -11081.006561683593
1
Iteration 4100: Loss = -11080.996096282319
Iteration 4200: Loss = -11080.991321711586
Iteration 4300: Loss = -11080.986849692332
Iteration 4400: Loss = -11080.98223063373
Iteration 4500: Loss = -11080.979478973915
Iteration 4600: Loss = -11080.97733154362
Iteration 4700: Loss = -11080.975290734203
Iteration 4800: Loss = -11080.97356937044
Iteration 4900: Loss = -11080.97248840075
Iteration 5000: Loss = -11080.970013608712
Iteration 5100: Loss = -11080.968252285242
Iteration 5200: Loss = -11080.96750695516
Iteration 5300: Loss = -11080.965614482851
Iteration 5400: Loss = -11080.964621357547
Iteration 5500: Loss = -11080.949107411689
Iteration 5600: Loss = -11080.90949653465
Iteration 5700: Loss = -11080.898299871978
Iteration 5800: Loss = -11080.897448881407
Iteration 5900: Loss = -11080.897078140077
Iteration 6000: Loss = -11080.896081733117
Iteration 6100: Loss = -11080.895485815932
Iteration 6200: Loss = -11080.894989288856
Iteration 6300: Loss = -11080.896710266326
1
Iteration 6400: Loss = -11080.89399935427
Iteration 6500: Loss = -11080.894889202564
1
Iteration 6600: Loss = -11080.893017110448
Iteration 6700: Loss = -11080.89231554637
Iteration 6800: Loss = -11080.896337421542
1
Iteration 6900: Loss = -11080.892575409405
2
Iteration 7000: Loss = -11080.89122868624
Iteration 7100: Loss = -11080.891260114668
Iteration 7200: Loss = -11080.892315869636
1
Iteration 7300: Loss = -11080.890115302525
Iteration 7400: Loss = -11080.889882686566
Iteration 7500: Loss = -11080.889492228409
Iteration 7600: Loss = -11080.889632967097
1
Iteration 7700: Loss = -11080.888737359475
Iteration 7800: Loss = -11080.888694049992
Iteration 7900: Loss = -11080.888220884024
Iteration 8000: Loss = -11080.888201120642
Iteration 8100: Loss = -11080.893032049655
1
Iteration 8200: Loss = -11080.888527019366
2
Iteration 8300: Loss = -11080.89241839863
3
Iteration 8400: Loss = -11080.887360479346
Iteration 8500: Loss = -11080.887291822502
Iteration 8600: Loss = -11080.887836212229
1
Iteration 8700: Loss = -11080.903666557273
2
Iteration 8800: Loss = -11080.891606290374
3
Iteration 8900: Loss = -11080.889699077467
4
Iteration 9000: Loss = -11080.881882366719
Iteration 9100: Loss = -11080.881186562014
Iteration 9200: Loss = -11080.88112583412
Iteration 9300: Loss = -11080.884412931417
1
Iteration 9400: Loss = -11080.8808642132
Iteration 9500: Loss = -11080.880813103848
Iteration 9600: Loss = -11081.19529666039
1
Iteration 9700: Loss = -11080.880616579741
Iteration 9800: Loss = -11080.882634549582
1
Iteration 9900: Loss = -11080.88107263575
2
Iteration 10000: Loss = -11080.880416929951
Iteration 10100: Loss = -11080.879936388681
Iteration 10200: Loss = -11080.87996600537
Iteration 10300: Loss = -11080.880894191747
1
Iteration 10400: Loss = -11080.91098368897
2
Iteration 10500: Loss = -11080.88104306093
3
Iteration 10600: Loss = -11080.880412274479
4
Iteration 10700: Loss = -11080.87953616005
Iteration 10800: Loss = -11080.877840671188
Iteration 10900: Loss = -11080.886178360654
1
Iteration 11000: Loss = -11080.877308266721
Iteration 11100: Loss = -11080.876694610573
Iteration 11200: Loss = -11081.001950894963
1
Iteration 11300: Loss = -11080.874982264997
Iteration 11400: Loss = -11080.875533665116
1
Iteration 11500: Loss = -11080.879254412153
2
Iteration 11600: Loss = -11081.013460909602
3
Iteration 11700: Loss = -11080.880799558692
4
Iteration 11800: Loss = -11080.875597284523
5
Iteration 11900: Loss = -11080.876125698716
6
Iteration 12000: Loss = -11080.894851234176
7
Iteration 12100: Loss = -11080.89806060136
8
Iteration 12200: Loss = -11080.876611751906
9
Iteration 12300: Loss = -11080.875065540045
Iteration 12400: Loss = -11080.874893123282
Iteration 12500: Loss = -11080.876797890212
1
Iteration 12600: Loss = -11080.921552847987
2
Iteration 12700: Loss = -11080.87474890772
Iteration 12800: Loss = -11080.874769430902
Iteration 12900: Loss = -11080.872927144641
Iteration 13000: Loss = -11080.88692692872
1
Iteration 13100: Loss = -11080.872892517518
Iteration 13200: Loss = -11080.983472068976
1
Iteration 13300: Loss = -11080.865222444181
Iteration 13400: Loss = -11080.870953365076
1
Iteration 13500: Loss = -11080.870624864745
2
Iteration 13600: Loss = -11080.904242974102
3
Iteration 13700: Loss = -11080.870698194462
4
Iteration 13800: Loss = -11080.86508505291
Iteration 13900: Loss = -11080.868339501745
1
Iteration 14000: Loss = -11080.894855542918
2
Iteration 14100: Loss = -11080.864391792311
Iteration 14200: Loss = -11080.864365169136
Iteration 14300: Loss = -11080.865917295589
1
Iteration 14400: Loss = -11080.867538122115
2
Iteration 14500: Loss = -11081.025002495313
3
Iteration 14600: Loss = -11080.870564192659
4
Iteration 14700: Loss = -11080.869507167055
5
Iteration 14800: Loss = -11080.866004445317
6
Iteration 14900: Loss = -11080.865158992083
7
Iteration 15000: Loss = -11080.864430583466
Iteration 15100: Loss = -11080.865391809235
1
Iteration 15200: Loss = -11080.869430781777
2
Iteration 15300: Loss = -11080.87971554162
3
Iteration 15400: Loss = -11080.876886199245
4
Iteration 15500: Loss = -11080.864807952195
5
Iteration 15600: Loss = -11080.864183662334
Iteration 15700: Loss = -11080.864590929828
1
Iteration 15800: Loss = -11080.864292589287
2
Iteration 15900: Loss = -11080.865355737451
3
Iteration 16000: Loss = -11080.868602338694
4
Iteration 16100: Loss = -11080.867566558518
5
Iteration 16200: Loss = -11080.864175337478
Iteration 16300: Loss = -11080.865551520701
1
Iteration 16400: Loss = -11080.865824729657
2
Iteration 16500: Loss = -11080.864281883853
3
Iteration 16600: Loss = -11080.8643457485
4
Iteration 16700: Loss = -11080.968459663944
5
Iteration 16800: Loss = -11080.957107027105
6
Iteration 16900: Loss = -11080.868655224163
7
Iteration 17000: Loss = -11080.86414463274
Iteration 17100: Loss = -11080.864744559945
1
Iteration 17200: Loss = -11080.885175182102
2
Iteration 17300: Loss = -11080.869064812236
3
Iteration 17400: Loss = -11080.865850502636
4
Iteration 17500: Loss = -11080.86550292536
5
Iteration 17600: Loss = -11080.864011738324
Iteration 17700: Loss = -11080.865301855405
1
Iteration 17800: Loss = -11080.864879169289
2
Iteration 17900: Loss = -11080.864035878547
Iteration 18000: Loss = -11080.866575003223
1
Iteration 18100: Loss = -11080.86417696156
2
Iteration 18200: Loss = -11080.86646395784
3
Iteration 18300: Loss = -11080.866976560977
4
Iteration 18400: Loss = -11080.87260822648
5
Iteration 18500: Loss = -11080.864957169068
6
Iteration 18600: Loss = -11080.867624276227
7
Iteration 18700: Loss = -11080.865069340885
8
Iteration 18800: Loss = -11080.896693744806
9
Iteration 18900: Loss = -11080.863931059066
Iteration 19000: Loss = -11080.890322080222
1
Iteration 19100: Loss = -11080.86990966378
2
Iteration 19200: Loss = -11080.88325991695
3
Iteration 19300: Loss = -11080.884500325903
4
Iteration 19400: Loss = -11080.875574076492
5
Iteration 19500: Loss = -11080.866422443305
6
Iteration 19600: Loss = -11080.867483189877
7
Iteration 19700: Loss = -11080.86459913425
8
Iteration 19800: Loss = -11080.864020399687
Iteration 19900: Loss = -11080.776985226757
pi: tensor([[0.7663, 0.2337],
        [0.2797, 0.7203]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5131, 0.4869], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2064, 0.0989],
         [0.6065, 0.3022]],

        [[0.5877, 0.0895],
         [0.7299, 0.6736]],

        [[0.7302, 0.1023],
         [0.5600, 0.5698]],

        [[0.5669, 0.0936],
         [0.6424, 0.6002]],

        [[0.5974, 0.0983],
         [0.6301, 0.6838]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207352941176471
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.960320507679542
Average Adjusted Rand Index: 0.960306840746511
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22476.1507605821
Iteration 100: Loss = -11396.762123621675
Iteration 200: Loss = -11395.879418674063
Iteration 300: Loss = -11395.463151271662
Iteration 400: Loss = -11395.085813161899
Iteration 500: Loss = -11394.683740763667
Iteration 600: Loss = -11394.235981420143
Iteration 700: Loss = -11393.668209041005
Iteration 800: Loss = -11393.020612415823
Iteration 900: Loss = -11392.541701423868
Iteration 1000: Loss = -11392.11336663496
Iteration 1100: Loss = -11391.436477697149
Iteration 1200: Loss = -11390.524620749176
Iteration 1300: Loss = -11389.798648068167
Iteration 1400: Loss = -11316.749833094404
Iteration 1500: Loss = -11173.773670456976
Iteration 1600: Loss = -11117.529783061793
Iteration 1700: Loss = -11084.940403484694
Iteration 1800: Loss = -11084.740470418932
Iteration 1900: Loss = -11084.658185329714
Iteration 2000: Loss = -11083.760952141372
Iteration 2100: Loss = -11081.470344046676
Iteration 2200: Loss = -11081.424547477485
Iteration 2300: Loss = -11081.40605055964
Iteration 2400: Loss = -11081.392963756221
Iteration 2500: Loss = -11081.380861033822
Iteration 2600: Loss = -11081.36276032727
Iteration 2700: Loss = -11081.324892574377
Iteration 2800: Loss = -11081.195326263944
Iteration 2900: Loss = -11081.189864365298
Iteration 3000: Loss = -11081.185287332357
Iteration 3100: Loss = -11081.1807330375
Iteration 3200: Loss = -11081.174600647048
Iteration 3300: Loss = -11081.161748874374
Iteration 3400: Loss = -11081.155431600158
Iteration 3500: Loss = -11081.153722735202
Iteration 3600: Loss = -11081.15301466648
Iteration 3700: Loss = -11081.150490684877
Iteration 3800: Loss = -11081.148874289602
Iteration 3900: Loss = -11081.146356528701
Iteration 4000: Loss = -11081.14507252237
Iteration 4100: Loss = -11081.1438950384
Iteration 4200: Loss = -11081.142958623725
Iteration 4300: Loss = -11081.141837133207
Iteration 4400: Loss = -11081.138503108536
Iteration 4500: Loss = -11081.079151051643
Iteration 4600: Loss = -11080.95095795404
Iteration 4700: Loss = -11080.931820629034
Iteration 4800: Loss = -11080.93334761346
1
Iteration 4900: Loss = -11080.937023866469
2
Iteration 5000: Loss = -11080.926998270597
Iteration 5100: Loss = -11080.927321610157
1
Iteration 5200: Loss = -11080.926646104695
Iteration 5300: Loss = -11080.923634211103
Iteration 5400: Loss = -11080.922897167567
Iteration 5500: Loss = -11080.92102452857
Iteration 5600: Loss = -11080.920507049861
Iteration 5700: Loss = -11080.91939865545
Iteration 5800: Loss = -11080.916075077334
Iteration 5900: Loss = -11080.909612908155
Iteration 6000: Loss = -11080.909120201202
Iteration 6100: Loss = -11080.908650459525
Iteration 6200: Loss = -11080.908624316584
Iteration 6300: Loss = -11080.908184172267
Iteration 6400: Loss = -11080.911782918944
1
Iteration 6500: Loss = -11080.907881323941
Iteration 6600: Loss = -11080.907087017416
Iteration 6700: Loss = -11080.907471929373
1
Iteration 6800: Loss = -11080.906146718535
Iteration 6900: Loss = -11080.903737687098
Iteration 7000: Loss = -11080.903166552684
Iteration 7100: Loss = -11080.90365491344
1
Iteration 7200: Loss = -11080.902701868079
Iteration 7300: Loss = -11080.90294268568
1
Iteration 7400: Loss = -11080.902308401737
Iteration 7500: Loss = -11080.901956385485
Iteration 7600: Loss = -11080.901729282064
Iteration 7700: Loss = -11080.9163307234
1
Iteration 7800: Loss = -11080.900747600725
Iteration 7900: Loss = -11080.900585626197
Iteration 8000: Loss = -11080.911464984536
1
Iteration 8100: Loss = -11080.970880895013
2
Iteration 8200: Loss = -11080.899577780678
Iteration 8300: Loss = -11080.898926286904
Iteration 8400: Loss = -11080.904462174196
1
Iteration 8500: Loss = -11080.898372316915
Iteration 8600: Loss = -11080.912142710964
1
Iteration 8700: Loss = -11080.898186685099
Iteration 8800: Loss = -11080.92400904964
1
Iteration 8900: Loss = -11080.897670145529
Iteration 9000: Loss = -11080.895588839787
Iteration 9100: Loss = -11080.895822739505
1
Iteration 9200: Loss = -11080.896497547203
2
Iteration 9300: Loss = -11080.89493103604
Iteration 9400: Loss = -11080.894800505368
Iteration 9500: Loss = -11080.893077629424
Iteration 9600: Loss = -11080.90056345456
1
Iteration 9700: Loss = -11080.892943054529
Iteration 9800: Loss = -11080.893056632825
1
Iteration 9900: Loss = -11080.892552456007
Iteration 10000: Loss = -11080.883158675118
Iteration 10100: Loss = -11080.894142873021
1
Iteration 10200: Loss = -11080.882500626112
Iteration 10300: Loss = -11080.882753494505
1
Iteration 10400: Loss = -11080.924443776777
2
Iteration 10500: Loss = -11080.90216636561
3
Iteration 10600: Loss = -11080.88411734275
4
Iteration 10700: Loss = -11080.884326194287
5
Iteration 10800: Loss = -11080.934375568446
6
Iteration 10900: Loss = -11080.883887345239
7
Iteration 11000: Loss = -11080.978544356129
8
Iteration 11100: Loss = -11080.882296084283
Iteration 11200: Loss = -11080.88680064549
1
Iteration 11300: Loss = -11080.884248793558
2
Iteration 11400: Loss = -11080.882467546577
3
Iteration 11500: Loss = -11080.882295566224
Iteration 11600: Loss = -11080.882742735032
1
Iteration 11700: Loss = -11080.89310651918
2
Iteration 11800: Loss = -11080.888953920707
3
Iteration 11900: Loss = -11080.882083299128
Iteration 12000: Loss = -11080.87557849957
Iteration 12100: Loss = -11080.906557678798
1
Iteration 12200: Loss = -11080.877070706318
2
Iteration 12300: Loss = -11080.874335168552
Iteration 12400: Loss = -11080.926335586333
1
Iteration 12500: Loss = -11080.97055752808
2
Iteration 12600: Loss = -11080.880986478602
3
Iteration 12700: Loss = -11080.902572957819
4
Iteration 12800: Loss = -11080.945308437773
5
Iteration 12900: Loss = -11080.875417734786
6
Iteration 13000: Loss = -11080.899997854227
7
Iteration 13100: Loss = -11080.886701172472
8
Iteration 13200: Loss = -11080.87410574041
Iteration 13300: Loss = -11080.877841931393
1
Iteration 13400: Loss = -11080.875610467752
2
Iteration 13500: Loss = -11080.895922176596
3
Iteration 13600: Loss = -11080.875084029047
4
Iteration 13700: Loss = -11080.9517141853
5
Iteration 13800: Loss = -11080.874991557996
6
Iteration 13900: Loss = -11080.878425182447
7
Iteration 14000: Loss = -11080.874239112632
8
Iteration 14100: Loss = -11080.879320749274
9
Iteration 14200: Loss = -11080.875073911648
10
Iteration 14300: Loss = -11080.866734271745
Iteration 14400: Loss = -11080.867528271197
1
Iteration 14500: Loss = -11080.901230194766
2
Iteration 14600: Loss = -11080.869531066233
3
Iteration 14700: Loss = -11080.86927792343
4
Iteration 14800: Loss = -11080.869854108922
5
Iteration 14900: Loss = -11080.86735182682
6
Iteration 15000: Loss = -11080.87741894
7
Iteration 15100: Loss = -11080.896272321417
8
Iteration 15200: Loss = -11080.963644794505
9
Iteration 15300: Loss = -11080.869721288958
10
Iteration 15400: Loss = -11080.872384038732
11
Iteration 15500: Loss = -11080.866752645754
Iteration 15600: Loss = -11080.871843849334
1
Iteration 15700: Loss = -11080.867076270977
2
Iteration 15800: Loss = -11080.867066157296
3
Iteration 15900: Loss = -11080.865446706
Iteration 16000: Loss = -11080.865644405945
1
Iteration 16100: Loss = -11080.875319298335
2
Iteration 16200: Loss = -11080.930099539333
3
Iteration 16300: Loss = -11080.865744894983
4
Iteration 16400: Loss = -11080.864606243864
Iteration 16500: Loss = -11080.864291279839
Iteration 16600: Loss = -11080.8643056139
Iteration 16700: Loss = -11080.865598759712
1
Iteration 16800: Loss = -11080.880477348474
2
Iteration 16900: Loss = -11080.867222332963
3
Iteration 17000: Loss = -11080.865356419474
4
Iteration 17100: Loss = -11080.874229487052
5
Iteration 17200: Loss = -11080.877442881492
6
Iteration 17300: Loss = -11080.871081141175
7
Iteration 17400: Loss = -11080.86864112256
8
Iteration 17500: Loss = -11080.87006197804
9
Iteration 17600: Loss = -11080.864635622824
10
Iteration 17700: Loss = -11080.866551720084
11
Iteration 17800: Loss = -11080.869539217665
12
Iteration 17900: Loss = -11081.007413514079
13
Iteration 18000: Loss = -11080.871668166143
14
Iteration 18100: Loss = -11080.86572286437
15
Stopping early at iteration 18100 due to no improvement.
pi: tensor([[0.7661, 0.2339],
        [0.2798, 0.7202]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5118, 0.4882], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2065, 0.0987],
         [0.6202, 0.3020]],

        [[0.5356, 0.0895],
         [0.6986, 0.6544]],

        [[0.6961, 0.1022],
         [0.7195, 0.5637]],

        [[0.7132, 0.0935],
         [0.6499, 0.6603]],

        [[0.5892, 0.0982],
         [0.6658, 0.6694]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207352941176471
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.960320507679542
Average Adjusted Rand Index: 0.960306840746511
11099.320918385527
[0.960320507679542, 0.960320507679542] [0.960306840746511, 0.960306840746511] [11080.777050076604, 11080.86572286437]
-------------------------------------
This iteration is 49
True Objective function: Loss = -11147.754573707021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22812.369496027302
Iteration 100: Loss = -11412.239495235097
Iteration 200: Loss = -11411.295861353075
Iteration 300: Loss = -11409.103460910792
Iteration 400: Loss = -11382.469037663908
Iteration 500: Loss = -11362.640932444647
Iteration 600: Loss = -11328.529229953663
Iteration 700: Loss = -11150.586033026222
Iteration 800: Loss = -11131.272251706769
Iteration 900: Loss = -11128.956969272875
Iteration 1000: Loss = -11128.848866094137
Iteration 1100: Loss = -11128.789009778557
Iteration 1200: Loss = -11127.684645697698
Iteration 1300: Loss = -11127.107223969453
Iteration 1400: Loss = -11127.003926384667
Iteration 1500: Loss = -11126.991383807506
Iteration 1600: Loss = -11126.98145377603
Iteration 1700: Loss = -11126.97293951143
Iteration 1800: Loss = -11126.964560428501
Iteration 1900: Loss = -11126.953162621634
Iteration 2000: Loss = -11126.93167240019
Iteration 2100: Loss = -11126.926203488363
Iteration 2200: Loss = -11126.921987966489
Iteration 2300: Loss = -11126.918718799674
Iteration 2400: Loss = -11126.915619315772
Iteration 2500: Loss = -11126.909025404046
Iteration 2600: Loss = -11126.735476342377
Iteration 2700: Loss = -11126.73011875861
Iteration 2800: Loss = -11126.72387591082
Iteration 2900: Loss = -11126.721905097873
Iteration 3000: Loss = -11126.720267015311
Iteration 3100: Loss = -11126.717491780319
Iteration 3200: Loss = -11126.71530558407
Iteration 3300: Loss = -11126.71303799182
Iteration 3400: Loss = -11126.706303471992
Iteration 3500: Loss = -11126.704743367685
Iteration 3600: Loss = -11126.69300386013
Iteration 3700: Loss = -11126.671291056233
Iteration 3800: Loss = -11126.48640519377
Iteration 3900: Loss = -11126.485228368938
Iteration 4000: Loss = -11126.48413379402
Iteration 4100: Loss = -11126.486148345932
1
Iteration 4200: Loss = -11126.490652031827
2
Iteration 4300: Loss = -11126.483633325286
Iteration 4400: Loss = -11126.48228690602
Iteration 4500: Loss = -11126.481543739716
Iteration 4600: Loss = -11126.48120805318
Iteration 4700: Loss = -11126.481328654694
1
Iteration 4800: Loss = -11126.480482344828
Iteration 4900: Loss = -11126.480017220501
Iteration 5000: Loss = -11126.474678509443
Iteration 5100: Loss = -11126.465116315936
Iteration 5200: Loss = -11126.465667683233
1
Iteration 5300: Loss = -11126.457213182506
Iteration 5400: Loss = -11126.203790158537
Iteration 5500: Loss = -11124.855096481815
Iteration 5600: Loss = -11124.840876979413
Iteration 5700: Loss = -11124.83947520459
Iteration 5800: Loss = -11124.839369877069
Iteration 5900: Loss = -11124.838803006913
Iteration 6000: Loss = -11124.838532427846
Iteration 6100: Loss = -11124.838051969376
Iteration 6200: Loss = -11124.834072537351
Iteration 6300: Loss = -11124.834078857217
Iteration 6400: Loss = -11124.835877479964
1
Iteration 6500: Loss = -11124.833712498086
Iteration 6600: Loss = -11124.833589815933
Iteration 6700: Loss = -11124.839385562134
1
Iteration 6800: Loss = -11124.833407423757
Iteration 6900: Loss = -11124.83328865025
Iteration 7000: Loss = -11124.833434112099
1
Iteration 7100: Loss = -11124.83330868579
Iteration 7200: Loss = -11124.834611021073
1
Iteration 7300: Loss = -11124.872073694527
2
Iteration 7400: Loss = -11124.834014392343
3
Iteration 7500: Loss = -11124.832839530838
Iteration 7600: Loss = -11124.833203171982
1
Iteration 7700: Loss = -11124.832664775586
Iteration 7800: Loss = -11124.832575184035
Iteration 7900: Loss = -11124.831619156186
Iteration 8000: Loss = -11124.735377749743
Iteration 8100: Loss = -11124.762118159144
1
Iteration 8200: Loss = -11124.73631924258
2
Iteration 8300: Loss = -11124.678808045848
Iteration 8400: Loss = -11124.676555992222
Iteration 8500: Loss = -11124.695645253987
1
Iteration 8600: Loss = -11124.676454563352
Iteration 8700: Loss = -11124.676447716449
Iteration 8800: Loss = -11124.677105068065
1
Iteration 8900: Loss = -11124.67635363412
Iteration 9000: Loss = -11124.676573914538
1
Iteration 9100: Loss = -11124.676314733106
Iteration 9200: Loss = -11124.676232513526
Iteration 9300: Loss = -11124.676944239549
1
Iteration 9400: Loss = -11124.675980690301
Iteration 9500: Loss = -11124.91279912398
1
Iteration 9600: Loss = -11124.665158315349
Iteration 9700: Loss = -11124.689806394932
1
Iteration 9800: Loss = -11124.667081467782
2
Iteration 9900: Loss = -11124.66840289256
3
Iteration 10000: Loss = -11124.66512644767
Iteration 10100: Loss = -11124.665542849078
1
Iteration 10200: Loss = -11124.66956542289
2
Iteration 10300: Loss = -11124.665916602693
3
Iteration 10400: Loss = -11124.664109029769
Iteration 10500: Loss = -11124.621088502898
Iteration 10600: Loss = -11124.619532922923
Iteration 10700: Loss = -11124.618825961063
Iteration 10800: Loss = -11123.130256843475
Iteration 10900: Loss = -11123.2493409224
1
Iteration 11000: Loss = -11123.137571104024
2
Iteration 11100: Loss = -11123.127630991792
Iteration 11200: Loss = -11123.119866121095
Iteration 11300: Loss = -11123.108181396428
Iteration 11400: Loss = -11123.111545184182
1
Iteration 11500: Loss = -11123.103449395503
Iteration 11600: Loss = -11123.096887248044
Iteration 11700: Loss = -11123.10093832503
1
Iteration 11800: Loss = -11123.096652904464
Iteration 11900: Loss = -11123.099500172839
1
Iteration 12000: Loss = -11123.096606470652
Iteration 12100: Loss = -11123.099006893139
1
Iteration 12200: Loss = -11123.096117340552
Iteration 12300: Loss = -11123.093442683956
Iteration 12400: Loss = -11123.087748649208
Iteration 12500: Loss = -11123.100177831058
1
Iteration 12600: Loss = -11123.201311054858
2
Iteration 12700: Loss = -11123.087896814743
3
Iteration 12800: Loss = -11123.087744889539
Iteration 12900: Loss = -11123.096583223127
1
Iteration 13000: Loss = -11123.087689188604
Iteration 13100: Loss = -11123.08768311477
Iteration 13200: Loss = -11123.087270309323
Iteration 13300: Loss = -11123.117795371933
1
Iteration 13400: Loss = -11123.086445671184
Iteration 13500: Loss = -11123.106550889914
1
Iteration 13600: Loss = -11123.086735549214
2
Iteration 13700: Loss = -11123.086535073535
Iteration 13800: Loss = -11123.1032424023
1
Iteration 13900: Loss = -11123.086431074975
Iteration 14000: Loss = -11123.087411623905
1
Iteration 14100: Loss = -11123.090196068455
2
Iteration 14200: Loss = -11123.087433703577
3
Iteration 14300: Loss = -11123.089208263462
4
Iteration 14400: Loss = -11123.163465010352
5
Iteration 14500: Loss = -11123.085753057123
Iteration 14600: Loss = -11123.08908673838
1
Iteration 14700: Loss = -11123.089278706375
2
Iteration 14800: Loss = -11123.08577675375
Iteration 14900: Loss = -11123.085960086448
1
Iteration 15000: Loss = -11123.086099957485
2
Iteration 15100: Loss = -11123.085901575962
3
Iteration 15200: Loss = -11123.085690054215
Iteration 15300: Loss = -11123.08609835095
1
Iteration 15400: Loss = -11123.085481530743
Iteration 15500: Loss = -11123.093660075954
1
Iteration 15600: Loss = -11123.199871439681
2
Iteration 15700: Loss = -11123.091834212264
3
Iteration 15800: Loss = -11123.085665927847
4
Iteration 15900: Loss = -11123.08914101357
5
Iteration 16000: Loss = -11123.085636054095
6
Iteration 16100: Loss = -11123.085696689577
7
Iteration 16200: Loss = -11123.093292236314
8
Iteration 16300: Loss = -11123.089091180404
9
Iteration 16400: Loss = -11123.091648075588
10
Iteration 16500: Loss = -11123.09608062616
11
Iteration 16600: Loss = -11123.083999568356
Iteration 16700: Loss = -11123.086027310506
1
Iteration 16800: Loss = -11123.102244916812
2
Iteration 16900: Loss = -11123.085313501419
3
Iteration 17000: Loss = -11123.084592496203
4
Iteration 17100: Loss = -11123.087784459098
5
Iteration 17200: Loss = -11123.082803704428
Iteration 17300: Loss = -11123.08602181988
1
Iteration 17400: Loss = -11123.082677537059
Iteration 17500: Loss = -11123.084554341513
1
Iteration 17600: Loss = -11123.125234305879
2
Iteration 17700: Loss = -11123.082800005654
3
Iteration 17800: Loss = -11123.082750243999
Iteration 17900: Loss = -11123.148505435318
1
Iteration 18000: Loss = -11123.082607009603
Iteration 18100: Loss = -11123.154532585211
1
Iteration 18200: Loss = -11123.082511598219
Iteration 18300: Loss = -11123.139261575892
1
Iteration 18400: Loss = -11123.08251776645
Iteration 18500: Loss = -11123.092712281514
1
Iteration 18600: Loss = -11123.103083163063
2
Iteration 18700: Loss = -11123.082646409941
3
Iteration 18800: Loss = -11123.082769838875
4
Iteration 18900: Loss = -11123.090721716895
5
Iteration 19000: Loss = -11123.082848595788
6
Iteration 19100: Loss = -11123.092506866195
7
Iteration 19200: Loss = -11123.087930611222
8
Iteration 19300: Loss = -11123.128225744736
9
Iteration 19400: Loss = -11123.081879428655
Iteration 19500: Loss = -11123.082593275154
1
Iteration 19600: Loss = -11123.081909717783
Iteration 19700: Loss = -11123.081791095128
Iteration 19800: Loss = -11123.091392295619
1
Iteration 19900: Loss = -11123.092430141702
2
pi: tensor([[0.7250, 0.2750],
        [0.2293, 0.7707]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5836, 0.4164], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2898, 0.1034],
         [0.5342, 0.2037]],

        [[0.7221, 0.0913],
         [0.5145, 0.6846]],

        [[0.5383, 0.1021],
         [0.7279, 0.6573]],

        [[0.5445, 0.1017],
         [0.5950, 0.6502]],

        [[0.5042, 0.0944],
         [0.6529, 0.5470]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 9
Adjusted Rand Index: 0.6691246144255645
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824276204858761
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
Global Adjusted Rand Index: 0.8683602137826975
Average Adjusted Rand Index: 0.8706304222923448
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21388.155882069885
Iteration 100: Loss = -11411.688440862572
Iteration 200: Loss = -11410.758210869792
Iteration 300: Loss = -11408.732986903138
Iteration 400: Loss = -11405.071387396985
Iteration 500: Loss = -11387.891783681072
Iteration 600: Loss = -11360.329361993758
Iteration 700: Loss = -11152.406560732474
Iteration 800: Loss = -11133.773633260733
Iteration 900: Loss = -11133.057331387925
Iteration 1000: Loss = -11128.55500065457
Iteration 1100: Loss = -11127.967438979198
Iteration 1200: Loss = -11124.696692780426
Iteration 1300: Loss = -11124.565592092644
Iteration 1400: Loss = -11124.510589849462
Iteration 1500: Loss = -11124.470088668808
Iteration 1600: Loss = -11124.436106125104
Iteration 1700: Loss = -11124.41457913254
Iteration 1800: Loss = -11124.39701703234
Iteration 1900: Loss = -11124.380737581396
Iteration 2000: Loss = -11124.365745033261
Iteration 2100: Loss = -11124.353301096064
Iteration 2200: Loss = -11124.337796175412
Iteration 2300: Loss = -11124.292006247751
Iteration 2400: Loss = -11124.204880283258
Iteration 2500: Loss = -11124.191806487437
Iteration 2600: Loss = -11124.182640570813
Iteration 2700: Loss = -11124.153692291953
Iteration 2800: Loss = -11124.101380783146
Iteration 2900: Loss = -11124.122733659264
1
Iteration 3000: Loss = -11124.09272628363
Iteration 3100: Loss = -11124.089379247685
Iteration 3200: Loss = -11124.097079805979
1
Iteration 3300: Loss = -11124.078499478499
Iteration 3400: Loss = -11123.806882308247
Iteration 3500: Loss = -11122.990585305954
Iteration 3600: Loss = -11122.98759878433
Iteration 3700: Loss = -11122.98707858797
Iteration 3800: Loss = -11122.984206703793
Iteration 3900: Loss = -11122.98280948507
Iteration 4000: Loss = -11122.981416036542
Iteration 4100: Loss = -11122.982632813535
1
Iteration 4200: Loss = -11122.979014812305
Iteration 4300: Loss = -11122.980022846363
1
Iteration 4400: Loss = -11122.977061537717
Iteration 4500: Loss = -11122.973457100516
Iteration 4600: Loss = -11122.958980295678
Iteration 4700: Loss = -11122.957767723552
Iteration 4800: Loss = -11122.956682220738
Iteration 4900: Loss = -11122.955725977023
Iteration 5000: Loss = -11122.958943857811
1
Iteration 5100: Loss = -11122.947820788544
Iteration 5200: Loss = -11122.946880408612
Iteration 5300: Loss = -11122.943670962833
Iteration 5400: Loss = -11122.939660356624
Iteration 5500: Loss = -11122.934739340593
Iteration 5600: Loss = -11122.934257684847
Iteration 5700: Loss = -11122.93378084481
Iteration 5800: Loss = -11122.933495798838
Iteration 5900: Loss = -11122.933400102378
Iteration 6000: Loss = -11122.93433579516
1
Iteration 6100: Loss = -11122.937120346884
2
Iteration 6200: Loss = -11122.932142382446
Iteration 6300: Loss = -11122.931857756197
Iteration 6400: Loss = -11122.931801942605
Iteration 6500: Loss = -11122.931887507355
Iteration 6600: Loss = -11122.933146079537
1
Iteration 6700: Loss = -11122.931335377518
Iteration 6800: Loss = -11122.93112970268
Iteration 6900: Loss = -11122.932794954879
1
Iteration 7000: Loss = -11122.930719722615
Iteration 7100: Loss = -11122.931242701805
1
Iteration 7200: Loss = -11122.937908873366
2
Iteration 7300: Loss = -11122.930325434898
Iteration 7400: Loss = -11122.93065859687
1
Iteration 7500: Loss = -11122.942580515373
2
Iteration 7600: Loss = -11122.929859054073
Iteration 7700: Loss = -11122.929613751528
Iteration 7800: Loss = -11122.92641585699
Iteration 7900: Loss = -11122.926325832104
Iteration 8000: Loss = -11122.926179636464
Iteration 8100: Loss = -11122.926608172846
1
Iteration 8200: Loss = -11122.925487757511
Iteration 8300: Loss = -11122.925676004432
1
Iteration 8400: Loss = -11122.925398054225
Iteration 8500: Loss = -11122.925295466875
Iteration 8600: Loss = -11122.926130274524
1
Iteration 8700: Loss = -11122.925109915814
Iteration 8800: Loss = -11122.924969868429
Iteration 8900: Loss = -11122.958519035928
1
Iteration 9000: Loss = -11122.924377120264
Iteration 9100: Loss = -11122.923777761353
Iteration 9200: Loss = -11122.926142003313
1
Iteration 9300: Loss = -11122.930600608624
2
Iteration 9400: Loss = -11122.920554163404
Iteration 9500: Loss = -11122.920384652063
Iteration 9600: Loss = -11122.95510555725
1
Iteration 9700: Loss = -11122.920187767768
Iteration 9800: Loss = -11122.920107078538
Iteration 9900: Loss = -11122.920040731327
Iteration 10000: Loss = -11122.939254958696
1
Iteration 10100: Loss = -11122.919910326615
Iteration 10200: Loss = -11122.925522639025
1
Iteration 10300: Loss = -11122.91986820037
Iteration 10400: Loss = -11122.920055243048
1
Iteration 10500: Loss = -11122.920708245623
2
Iteration 10600: Loss = -11122.92459494519
3
Iteration 10700: Loss = -11122.919542530099
Iteration 10800: Loss = -11122.919585169991
Iteration 10900: Loss = -11122.919079927182
Iteration 11000: Loss = -11122.919527252272
1
Iteration 11100: Loss = -11122.928165377063
2
Iteration 11200: Loss = -11122.921909038385
3
Iteration 11300: Loss = -11122.919051056992
Iteration 11400: Loss = -11122.91912305191
Iteration 11500: Loss = -11122.924425714658
1
Iteration 11600: Loss = -11122.919536007963
2
Iteration 11700: Loss = -11123.001856959623
3
Iteration 11800: Loss = -11122.961986585102
4
Iteration 11900: Loss = -11122.918883512724
Iteration 12000: Loss = -11122.919131439443
1
Iteration 12100: Loss = -11122.923381887387
2
Iteration 12200: Loss = -11122.931152241821
3
Iteration 12300: Loss = -11122.919709398693
4
Iteration 12400: Loss = -11122.91928447953
5
Iteration 12500: Loss = -11122.918603805136
Iteration 12600: Loss = -11122.920169075738
1
Iteration 12700: Loss = -11122.944246874633
2
Iteration 12800: Loss = -11122.918521467138
Iteration 12900: Loss = -11122.918580593365
Iteration 13000: Loss = -11122.925038098614
1
Iteration 13100: Loss = -11122.92234967263
2
Iteration 13200: Loss = -11122.919055511953
3
Iteration 13300: Loss = -11122.919772520461
4
Iteration 13400: Loss = -11122.9192686992
5
Iteration 13500: Loss = -11122.971016457388
6
Iteration 13600: Loss = -11122.918382798998
Iteration 13700: Loss = -11122.918375142737
Iteration 13800: Loss = -11122.927333442256
1
Iteration 13900: Loss = -11122.919300139389
2
Iteration 14000: Loss = -11122.91851353911
3
Iteration 14100: Loss = -11122.918470097411
Iteration 14200: Loss = -11122.938746524509
1
Iteration 14300: Loss = -11122.922501868557
2
Iteration 14400: Loss = -11122.92013522921
3
Iteration 14500: Loss = -11122.920701011715
4
Iteration 14600: Loss = -11122.924314221933
5
Iteration 14700: Loss = -11122.921201982745
6
Iteration 14800: Loss = -11122.918073184415
Iteration 14900: Loss = -11122.91810606873
Iteration 15000: Loss = -11122.918050272816
Iteration 15100: Loss = -11122.919406655648
1
Iteration 15200: Loss = -11122.918380406698
2
Iteration 15300: Loss = -11122.9184209458
3
Iteration 15400: Loss = -11122.918017597081
Iteration 15500: Loss = -11122.918983992404
1
Iteration 15600: Loss = -11122.919815663368
2
Iteration 15700: Loss = -11122.920319916437
3
Iteration 15800: Loss = -11122.919662914015
4
Iteration 15900: Loss = -11122.921285966122
5
Iteration 16000: Loss = -11122.918214176934
6
Iteration 16100: Loss = -11122.920366476006
7
Iteration 16200: Loss = -11122.926586602825
8
Iteration 16300: Loss = -11122.918418515796
9
Iteration 16400: Loss = -11122.918391835101
10
Iteration 16500: Loss = -11122.9180096617
Iteration 16600: Loss = -11122.920379794743
1
Iteration 16700: Loss = -11122.927616456655
2
Iteration 16800: Loss = -11122.918042446452
Iteration 16900: Loss = -11122.922070598379
1
Iteration 17000: Loss = -11122.919923073123
2
Iteration 17100: Loss = -11122.996464147145
3
Iteration 17200: Loss = -11122.918438278104
4
Iteration 17300: Loss = -11122.92980079304
5
Iteration 17400: Loss = -11122.925363007476
6
Iteration 17500: Loss = -11122.919731284503
7
Iteration 17600: Loss = -11122.920772137517
8
Iteration 17700: Loss = -11122.948415719968
9
Iteration 17800: Loss = -11122.918006877922
Iteration 17900: Loss = -11122.919211978626
1
Iteration 18000: Loss = -11122.917881427076
Iteration 18100: Loss = -11122.921508131405
1
Iteration 18200: Loss = -11123.043428335995
2
Iteration 18300: Loss = -11122.918206677776
3
Iteration 18400: Loss = -11122.917664044364
Iteration 18500: Loss = -11122.924693362525
1
Iteration 18600: Loss = -11123.013537247945
2
Iteration 18700: Loss = -11122.995554110848
3
Iteration 18800: Loss = -11122.917611204492
Iteration 18900: Loss = -11122.91764826852
Iteration 19000: Loss = -11122.918444029176
1
Iteration 19100: Loss = -11122.917716965865
Iteration 19200: Loss = -11122.917504625862
Iteration 19300: Loss = -11122.917579872086
Iteration 19400: Loss = -11122.925738141746
1
Iteration 19500: Loss = -11122.953642096678
2
Iteration 19600: Loss = -11122.917440369445
Iteration 19700: Loss = -11122.924338885818
1
Iteration 19800: Loss = -11122.934111040287
2
Iteration 19900: Loss = -11122.918715774722
3
pi: tensor([[0.7239, 0.2761],
        [0.2294, 0.7706]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5791, 0.4209], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2907, 0.1041],
         [0.5604, 0.2031]],

        [[0.5278, 0.0914],
         [0.6767, 0.6879]],

        [[0.7108, 0.1019],
         [0.6632, 0.5057]],

        [[0.6445, 0.1019],
         [0.6732, 0.6235]],

        [[0.6694, 0.0944],
         [0.6177, 0.7130]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 9
Adjusted Rand Index: 0.6691246144255645
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824276204858761
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
Global Adjusted Rand Index: 0.8683602137826975
Average Adjusted Rand Index: 0.8706304222923448
11147.754573707021
[0.8683602137826975, 0.8683602137826975] [0.8706304222923448, 0.8706304222923448] [11123.092126798778, 11122.918977371264]
-------------------------------------
This iteration is 50
True Objective function: Loss = -11161.68987429604
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24106.86304083998
Iteration 100: Loss = -11414.3609562478
Iteration 200: Loss = -11413.400587091397
Iteration 300: Loss = -11412.151775696044
Iteration 400: Loss = -11410.892724714082
Iteration 500: Loss = -11408.111558272918
Iteration 600: Loss = -11406.85403748721
Iteration 700: Loss = -11398.667051934988
Iteration 800: Loss = -11395.017841842142
Iteration 900: Loss = -11394.879730845454
Iteration 1000: Loss = -11394.826543965031
Iteration 1100: Loss = -11394.802232753973
Iteration 1200: Loss = -11394.787890531657
Iteration 1300: Loss = -11394.777247549704
Iteration 1400: Loss = -11394.767933104938
Iteration 1500: Loss = -11394.757212372177
Iteration 1600: Loss = -11394.731611358939
Iteration 1700: Loss = -11393.536718473075
Iteration 1800: Loss = -11353.666249857859
Iteration 1900: Loss = -11284.370552805778
Iteration 2000: Loss = -11202.902038612323
Iteration 2100: Loss = -11143.447111960282
Iteration 2200: Loss = -11135.557138598953
Iteration 2300: Loss = -11135.512433097982
Iteration 2400: Loss = -11135.504407547045
Iteration 2500: Loss = -11135.49944031194
Iteration 2600: Loss = -11135.495787496971
Iteration 2700: Loss = -11135.492691814721
Iteration 2800: Loss = -11135.49016192527
Iteration 2900: Loss = -11135.491046374033
1
Iteration 3000: Loss = -11135.496855450803
2
Iteration 3100: Loss = -11135.483537021733
Iteration 3200: Loss = -11135.489561136093
1
Iteration 3300: Loss = -11135.475747200335
Iteration 3400: Loss = -11135.47374818608
Iteration 3500: Loss = -11135.472719789544
Iteration 3600: Loss = -11135.471254477638
Iteration 3700: Loss = -11135.468884670747
Iteration 3800: Loss = -11135.466175174128
Iteration 3900: Loss = -11135.466925942526
1
Iteration 4000: Loss = -11135.466210332434
Iteration 4100: Loss = -11135.462452405447
Iteration 4200: Loss = -11135.462356636317
Iteration 4300: Loss = -11135.459979985928
Iteration 4400: Loss = -11135.455601676598
Iteration 4500: Loss = -11135.445438195626
Iteration 4600: Loss = -11135.440839248657
Iteration 4700: Loss = -11135.44035755782
Iteration 4800: Loss = -11135.440508941332
1
Iteration 4900: Loss = -11135.439612692306
Iteration 5000: Loss = -11135.439279805549
Iteration 5100: Loss = -11135.439196826299
Iteration 5200: Loss = -11135.435751191468
Iteration 5300: Loss = -11135.435174051743
Iteration 5400: Loss = -11135.4321949242
Iteration 5500: Loss = -11135.430382216178
Iteration 5600: Loss = -11135.429748179018
Iteration 5700: Loss = -11135.424758666468
Iteration 5800: Loss = -11135.42639488136
1
Iteration 5900: Loss = -11135.416955236382
Iteration 6000: Loss = -11135.376984357514
Iteration 6100: Loss = -11135.383001026
1
Iteration 6200: Loss = -11135.377010225659
Iteration 6300: Loss = -11135.376140821954
Iteration 6400: Loss = -11135.38091526996
1
Iteration 6500: Loss = -11135.375835141549
Iteration 6600: Loss = -11135.37954608349
1
Iteration 6700: Loss = -11135.375333373018
Iteration 6800: Loss = -11135.374531413801
Iteration 6900: Loss = -11135.371580258863
Iteration 7000: Loss = -11135.365746360812
Iteration 7100: Loss = -11135.37527323927
1
Iteration 7200: Loss = -11135.370387529309
2
Iteration 7300: Loss = -11135.368892338298
3
Iteration 7400: Loss = -11135.365921630604
4
Iteration 7500: Loss = -11135.368202389145
5
Iteration 7600: Loss = -11135.366092321807
6
Iteration 7700: Loss = -11135.365805106234
Iteration 7800: Loss = -11135.366621254092
1
Iteration 7900: Loss = -11135.365572329096
Iteration 8000: Loss = -11135.365341080978
Iteration 8100: Loss = -11135.365097556056
Iteration 8200: Loss = -11135.365061967746
Iteration 8300: Loss = -11135.36496829385
Iteration 8400: Loss = -11135.386571865018
1
Iteration 8500: Loss = -11135.364802616677
Iteration 8600: Loss = -11135.36474486519
Iteration 8700: Loss = -11135.365038980834
1
Iteration 8800: Loss = -11135.364584105771
Iteration 8900: Loss = -11135.36600240001
1
Iteration 9000: Loss = -11135.414585968218
2
Iteration 9100: Loss = -11135.364447435313
Iteration 9200: Loss = -11135.366862676794
1
Iteration 9300: Loss = -11135.362506654797
Iteration 9400: Loss = -11135.363956392383
1
Iteration 9500: Loss = -11135.362419475341
Iteration 9600: Loss = -11135.368556788693
1
Iteration 9700: Loss = -11135.362405492155
Iteration 9800: Loss = -11135.387394616497
1
Iteration 9900: Loss = -11135.367018836516
2
Iteration 10000: Loss = -11135.359536926735
Iteration 10100: Loss = -11135.360610129112
1
Iteration 10200: Loss = -11135.55096014809
2
Iteration 10300: Loss = -11135.35953628541
Iteration 10400: Loss = -11135.359787872418
1
Iteration 10500: Loss = -11135.663192421398
2
Iteration 10600: Loss = -11135.359522693288
Iteration 10700: Loss = -11135.589912781603
1
Iteration 10800: Loss = -11135.37697071927
2
Iteration 10900: Loss = -11135.387222253989
3
Iteration 11000: Loss = -11135.360642345091
4
Iteration 11100: Loss = -11135.363228693817
5
Iteration 11200: Loss = -11135.359460771622
Iteration 11300: Loss = -11135.359141628755
Iteration 11400: Loss = -11135.359258677678
1
Iteration 11500: Loss = -11135.359010085864
Iteration 11600: Loss = -11135.359732545005
1
Iteration 11700: Loss = -11135.445754485387
2
Iteration 11800: Loss = -11135.413305998889
3
Iteration 11900: Loss = -11135.185133573716
Iteration 12000: Loss = -11135.182613235933
Iteration 12100: Loss = -11135.195743430562
1
Iteration 12200: Loss = -11135.181931354926
Iteration 12300: Loss = -11135.182527562149
1
Iteration 12400: Loss = -11135.18217042737
2
Iteration 12500: Loss = -11135.181699646271
Iteration 12600: Loss = -11135.220929724828
1
Iteration 12700: Loss = -11135.179161084052
Iteration 12800: Loss = -11135.180095491854
1
Iteration 12900: Loss = -11135.178989322092
Iteration 13000: Loss = -11135.179897833896
1
Iteration 13100: Loss = -11135.178906707864
Iteration 13200: Loss = -11135.185374061626
1
Iteration 13300: Loss = -11135.17893657001
Iteration 13400: Loss = -11135.20009956481
1
Iteration 13500: Loss = -11135.178870539236
Iteration 13600: Loss = -11135.178897824437
Iteration 13700: Loss = -11135.17900230508
1
Iteration 13800: Loss = -11135.178872687069
Iteration 13900: Loss = -11135.182718604528
1
Iteration 14000: Loss = -11135.180027730277
2
Iteration 14100: Loss = -11135.179587208519
3
Iteration 14200: Loss = -11135.16896360213
Iteration 14300: Loss = -11135.1684343645
Iteration 14400: Loss = -11135.17493515556
1
Iteration 14500: Loss = -11135.168316400406
Iteration 14600: Loss = -11135.169647818704
1
Iteration 14700: Loss = -11135.168321574976
Iteration 14800: Loss = -11135.169013887664
1
Iteration 14900: Loss = -11135.168736566953
2
Iteration 15000: Loss = -11135.168887112512
3
Iteration 15100: Loss = -11135.168330292618
Iteration 15200: Loss = -11135.168567620392
1
Iteration 15300: Loss = -11135.167958472131
Iteration 15400: Loss = -11135.172864924865
1
Iteration 15500: Loss = -11135.224465997913
2
Iteration 15600: Loss = -11135.167799815843
Iteration 15700: Loss = -11135.19290822134
1
Iteration 15800: Loss = -11135.167187967889
Iteration 15900: Loss = -11135.17040481579
1
Iteration 16000: Loss = -11135.167241403637
Iteration 16100: Loss = -11135.167273332572
Iteration 16200: Loss = -11135.509052179435
1
Iteration 16300: Loss = -11135.168849633963
2
Iteration 16400: Loss = -11135.167192808995
Iteration 16500: Loss = -11135.167240959963
Iteration 16600: Loss = -11135.167215645874
Iteration 16700: Loss = -11135.1675208194
1
Iteration 16800: Loss = -11135.168698048028
2
Iteration 16900: Loss = -11135.167188529578
Iteration 17000: Loss = -11135.167920089973
1
Iteration 17100: Loss = -11135.174308332302
2
Iteration 17200: Loss = -11135.167178947793
Iteration 17300: Loss = -11135.1497924342
Iteration 17400: Loss = -11135.346665012024
1
Iteration 17500: Loss = -11135.145841570462
Iteration 17600: Loss = -11135.147399879213
1
Iteration 17700: Loss = -11135.145858777176
Iteration 17800: Loss = -11135.146020365562
1
Iteration 17900: Loss = -11135.145876858152
Iteration 18000: Loss = -11135.14596819288
Iteration 18100: Loss = -11135.14585803019
Iteration 18200: Loss = -11135.149787241906
1
Iteration 18300: Loss = -11135.145843232322
Iteration 18400: Loss = -11135.145853562966
Iteration 18500: Loss = -11135.150045883087
1
Iteration 18600: Loss = -11135.145846631976
Iteration 18700: Loss = -11135.1458648738
Iteration 18800: Loss = -11135.147401984086
1
Iteration 18900: Loss = -11135.145853289492
Iteration 19000: Loss = -11135.15677681631
1
Iteration 19100: Loss = -11135.1458692817
Iteration 19200: Loss = -11135.146918663428
1
Iteration 19300: Loss = -11135.240308857068
2
Iteration 19400: Loss = -11135.344293299604
3
Iteration 19500: Loss = -11135.145860406667
Iteration 19600: Loss = -11135.147980341317
1
Iteration 19700: Loss = -11135.146395155407
2
Iteration 19800: Loss = -11135.14584354939
Iteration 19900: Loss = -11135.146357579584
1
pi: tensor([[0.6961, 0.3039],
        [0.2910, 0.7090]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4827, 0.5173], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2979, 0.1101],
         [0.5935, 0.2044]],

        [[0.5924, 0.1027],
         [0.5797, 0.5178]],

        [[0.7113, 0.0862],
         [0.7067, 0.7036]],

        [[0.5218, 0.0957],
         [0.5222, 0.7114]],

        [[0.5147, 0.1002],
         [0.6544, 0.6968]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369635135591801
Global Adjusted Rand Index: 0.9214427761527915
Average Adjusted Rand Index: 0.9235521711581123
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24432.307122098293
Iteration 100: Loss = -11415.667173535088
Iteration 200: Loss = -11414.57941593027
Iteration 300: Loss = -11414.017978195903
Iteration 400: Loss = -11413.196524822753
Iteration 500: Loss = -11412.821411522833
Iteration 600: Loss = -11412.635547733595
Iteration 700: Loss = -11412.460101959785
Iteration 800: Loss = -11412.25534220183
Iteration 900: Loss = -11411.93045668311
Iteration 1000: Loss = -11411.217129216131
Iteration 1100: Loss = -11410.540270137672
Iteration 1200: Loss = -11410.190792873573
Iteration 1300: Loss = -11409.947433147312
Iteration 1400: Loss = -11408.415527254669
Iteration 1500: Loss = -11408.064295466664
Iteration 1600: Loss = -11407.930961512251
Iteration 1700: Loss = -11407.859370138072
Iteration 1800: Loss = -11407.817982979877
Iteration 1900: Loss = -11407.788823268489
Iteration 2000: Loss = -11407.76769709648
Iteration 2100: Loss = -11407.752118396851
Iteration 2200: Loss = -11407.740522241282
Iteration 2300: Loss = -11407.73136608505
Iteration 2400: Loss = -11407.72396824593
Iteration 2500: Loss = -11407.717714340888
Iteration 2600: Loss = -11407.71241097252
Iteration 2700: Loss = -11407.707812280614
Iteration 2800: Loss = -11407.70384322834
Iteration 2900: Loss = -11407.70032937913
Iteration 3000: Loss = -11407.698191347781
Iteration 3100: Loss = -11407.694473766785
Iteration 3200: Loss = -11407.691984744524
Iteration 3300: Loss = -11407.689729701144
Iteration 3400: Loss = -11407.687649969695
Iteration 3500: Loss = -11407.685817748606
Iteration 3600: Loss = -11407.684228841528
Iteration 3700: Loss = -11407.682602122475
Iteration 3800: Loss = -11407.681343345928
Iteration 3900: Loss = -11407.679861855964
Iteration 4000: Loss = -11407.67868949844
Iteration 4100: Loss = -11407.680682878472
1
Iteration 4200: Loss = -11407.676607694797
Iteration 4300: Loss = -11407.675679811968
Iteration 4400: Loss = -11407.67483413368
Iteration 4500: Loss = -11407.673963526202
Iteration 4600: Loss = -11407.674622657572
1
Iteration 4700: Loss = -11407.67252753957
Iteration 4800: Loss = -11407.671910737728
Iteration 4900: Loss = -11407.671395113155
Iteration 5000: Loss = -11407.67071420618
Iteration 5100: Loss = -11407.67027785514
Iteration 5200: Loss = -11407.66970528385
Iteration 5300: Loss = -11407.670709028249
1
Iteration 5400: Loss = -11407.668808905048
Iteration 5500: Loss = -11407.668386545945
Iteration 5600: Loss = -11407.668007667713
Iteration 5700: Loss = -11407.667623680027
Iteration 5800: Loss = -11407.667302455679
Iteration 5900: Loss = -11407.666976098883
Iteration 6000: Loss = -11407.666702649287
Iteration 6100: Loss = -11407.666413244218
Iteration 6200: Loss = -11407.666113203579
Iteration 6300: Loss = -11407.665907528311
Iteration 6400: Loss = -11407.665709148623
Iteration 6500: Loss = -11407.665695893911
Iteration 6600: Loss = -11407.666096053725
1
Iteration 6700: Loss = -11407.665412351835
Iteration 6800: Loss = -11407.664810337777
Iteration 6900: Loss = -11407.66473940684
Iteration 7000: Loss = -11407.664472533561
Iteration 7100: Loss = -11407.664326473463
Iteration 7200: Loss = -11407.664150420318
Iteration 7300: Loss = -11407.664121296324
Iteration 7400: Loss = -11407.663899017576
Iteration 7500: Loss = -11407.663779322042
Iteration 7600: Loss = -11407.663650329026
Iteration 7700: Loss = -11407.663551644553
Iteration 7800: Loss = -11407.663422223399
Iteration 7900: Loss = -11407.663473383825
Iteration 8000: Loss = -11407.663233153833
Iteration 8100: Loss = -11407.663171963144
Iteration 8200: Loss = -11407.677506854861
1
Iteration 8300: Loss = -11407.66337754165
2
Iteration 8400: Loss = -11407.662940321221
Iteration 8500: Loss = -11407.665281508336
1
Iteration 8600: Loss = -11407.662798401534
Iteration 8700: Loss = -11407.662668221077
Iteration 8800: Loss = -11407.662583159365
Iteration 8900: Loss = -11407.662593932617
Iteration 9000: Loss = -11407.6624605326
Iteration 9100: Loss = -11407.669556234083
1
Iteration 9200: Loss = -11407.66237973231
Iteration 9300: Loss = -11407.662313342256
Iteration 9400: Loss = -11407.663973791132
1
Iteration 9500: Loss = -11407.662185227426
Iteration 9600: Loss = -11407.662179688105
Iteration 9700: Loss = -11407.66451134616
1
Iteration 9800: Loss = -11407.662075222803
Iteration 9900: Loss = -11407.662087652667
Iteration 10000: Loss = -11407.667394875933
1
Iteration 10100: Loss = -11407.661974712619
Iteration 10200: Loss = -11407.66199798961
Iteration 10300: Loss = -11407.665306308741
1
Iteration 10400: Loss = -11407.66189195331
Iteration 10500: Loss = -11407.661910879153
Iteration 10600: Loss = -11407.663621868878
1
Iteration 10700: Loss = -11407.661815818396
Iteration 10800: Loss = -11407.661812780583
Iteration 10900: Loss = -11407.662280006745
1
Iteration 11000: Loss = -11407.661755744364
Iteration 11100: Loss = -11407.722213522018
1
Iteration 11200: Loss = -11407.66287250375
2
Iteration 11300: Loss = -11407.661718174544
Iteration 11400: Loss = -11407.664593395137
1
Iteration 11500: Loss = -11407.66179625457
Iteration 11600: Loss = -11407.661794717202
Iteration 11700: Loss = -11407.70332517206
1
Iteration 11800: Loss = -11407.663916691074
2
Iteration 11900: Loss = -11407.662106167687
3
Iteration 12000: Loss = -11407.662502308041
4
Iteration 12100: Loss = -11407.672259637386
5
Iteration 12200: Loss = -11407.662638012642
6
Iteration 12300: Loss = -11407.890725520501
7
Iteration 12400: Loss = -11407.664668266701
8
Iteration 12500: Loss = -11407.664473803328
9
Iteration 12600: Loss = -11407.703616858593
10
Iteration 12700: Loss = -11407.66156341627
Iteration 12800: Loss = -11407.66597703414
1
Iteration 12900: Loss = -11407.661767525919
2
Iteration 13000: Loss = -11407.661583197909
Iteration 13100: Loss = -11407.7025994841
1
Iteration 13200: Loss = -11407.662155642836
2
Iteration 13300: Loss = -11407.661599900839
Iteration 13400: Loss = -11407.721497519044
1
Iteration 13500: Loss = -11407.662509928827
2
Iteration 13600: Loss = -11407.664401865553
3
Iteration 13700: Loss = -11407.67908115858
4
Iteration 13800: Loss = -11407.819194306994
5
Iteration 13900: Loss = -11407.756860342908
6
Iteration 14000: Loss = -11407.66193817786
7
Iteration 14100: Loss = -11407.664088549538
8
Iteration 14200: Loss = -11407.721903332847
9
Iteration 14300: Loss = -11407.669026249732
10
Iteration 14400: Loss = -11407.661777196097
11
Iteration 14500: Loss = -11407.676622329915
12
Iteration 14600: Loss = -11407.688302861523
13
Iteration 14700: Loss = -11407.67928077657
14
Iteration 14800: Loss = -11407.689730676759
15
Stopping early at iteration 14800 due to no improvement.
pi: tensor([[9.1391e-01, 8.6089e-02],
        [9.9999e-01, 7.7805e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8761, 0.1239], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1683, 0.2137],
         [0.5790, 0.3198]],

        [[0.5779, 0.2104],
         [0.5376, 0.6989]],

        [[0.5256, 0.0896],
         [0.6766, 0.6459]],

        [[0.6341, 0.2101],
         [0.6374, 0.5688]],

        [[0.6261, 0.2223],
         [0.5527, 0.6578]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.00021025550424088273
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: -0.0005252498711784502
Average Adjusted Rand Index: 0.000980358005215227
11161.68987429604
[0.9214427761527915, -0.0005252498711784502] [0.9235521711581123, 0.000980358005215227] [11135.147824177171, 11407.689730676759]
-------------------------------------
This iteration is 51
True Objective function: Loss = -11191.050636335178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21120.26466602795
Iteration 100: Loss = -11499.278482258387
Iteration 200: Loss = -11497.67961701008
Iteration 300: Loss = -11496.43062658544
Iteration 400: Loss = -11495.288043198268
Iteration 500: Loss = -11494.063394155977
Iteration 600: Loss = -11492.9449158254
Iteration 700: Loss = -11477.919728535415
Iteration 800: Loss = -11284.121128860083
Iteration 900: Loss = -11278.117727521627
Iteration 1000: Loss = -11273.604896795418
Iteration 1100: Loss = -11270.756945524208
Iteration 1200: Loss = -11270.690489764978
Iteration 1300: Loss = -11270.61342698279
Iteration 1400: Loss = -11270.534287125913
Iteration 1500: Loss = -11270.389367938104
Iteration 1600: Loss = -11269.453161606889
Iteration 1700: Loss = -11269.170324972574
Iteration 1800: Loss = -11269.121469120813
Iteration 1900: Loss = -11269.10432680547
Iteration 2000: Loss = -11269.093772083883
Iteration 2100: Loss = -11269.054980812938
Iteration 2200: Loss = -11269.04685344719
Iteration 2300: Loss = -11269.040142424608
Iteration 2400: Loss = -11265.186750463323
Iteration 2500: Loss = -11253.530694595305
Iteration 2600: Loss = -11251.980115400753
Iteration 2700: Loss = -11249.725859478398
Iteration 2800: Loss = -11249.654694580382
Iteration 2900: Loss = -11249.648430213576
Iteration 3000: Loss = -11249.639991630685
Iteration 3100: Loss = -11249.6372803912
Iteration 3200: Loss = -11249.634000202133
Iteration 3300: Loss = -11249.627772785821
Iteration 3400: Loss = -11249.62546668546
Iteration 3500: Loss = -11249.62409804608
Iteration 3600: Loss = -11249.62103340015
Iteration 3700: Loss = -11249.62136489728
1
Iteration 3800: Loss = -11249.608880940666
Iteration 3900: Loss = -11249.608401204672
Iteration 4000: Loss = -11249.605213786504
Iteration 4100: Loss = -11249.604239459932
Iteration 4200: Loss = -11249.600418283388
Iteration 4300: Loss = -11249.594426263335
Iteration 4400: Loss = -11244.278181153664
Iteration 4500: Loss = -11244.276976066729
Iteration 4600: Loss = -11244.29257679345
1
Iteration 4700: Loss = -11244.274469022344
Iteration 4800: Loss = -11244.274437395072
Iteration 4900: Loss = -11244.273853736704
Iteration 5000: Loss = -11244.273577487971
Iteration 5100: Loss = -11244.273269053241
Iteration 5200: Loss = -11244.272979409125
Iteration 5300: Loss = -11244.272545917129
Iteration 5400: Loss = -11244.272988094672
1
Iteration 5500: Loss = -11244.270208808193
Iteration 5600: Loss = -11244.269999081233
Iteration 5700: Loss = -11244.271081685647
1
Iteration 5800: Loss = -11244.269703773572
Iteration 5900: Loss = -11244.269566093864
Iteration 6000: Loss = -11244.269914996026
1
Iteration 6100: Loss = -11244.268864993446
Iteration 6200: Loss = -11244.266705764103
Iteration 6300: Loss = -11244.265685959403
Iteration 6400: Loss = -11244.26552503564
Iteration 6500: Loss = -11244.274378740327
1
Iteration 6600: Loss = -11244.264922427743
Iteration 6700: Loss = -11244.258739187555
Iteration 6800: Loss = -11244.264203621593
1
Iteration 6900: Loss = -11244.258438945288
Iteration 7000: Loss = -11244.258427119243
Iteration 7100: Loss = -11244.258325660017
Iteration 7200: Loss = -11244.258275531633
Iteration 7300: Loss = -11244.258789995087
1
Iteration 7400: Loss = -11244.257923630896
Iteration 7500: Loss = -11244.174856767893
Iteration 7600: Loss = -11244.176827509913
1
Iteration 7700: Loss = -11244.174708420294
Iteration 7800: Loss = -11244.186858497962
1
Iteration 7900: Loss = -11244.171045243105
Iteration 8000: Loss = -11244.174887806588
1
Iteration 8100: Loss = -11244.170914747976
Iteration 8200: Loss = -11244.171509382912
1
Iteration 8300: Loss = -11244.170833186261
Iteration 8400: Loss = -11244.170791476665
Iteration 8500: Loss = -11244.171669498466
1
Iteration 8600: Loss = -11244.167431792932
Iteration 8700: Loss = -11244.16588876054
Iteration 8800: Loss = -11244.16570937213
Iteration 8900: Loss = -11244.162754017949
Iteration 9000: Loss = -11244.139890935896
Iteration 9100: Loss = -11244.142822667303
1
Iteration 9200: Loss = -11244.139791812217
Iteration 9300: Loss = -11244.139692393324
Iteration 9400: Loss = -11244.139420510468
Iteration 9500: Loss = -11244.139328181134
Iteration 9600: Loss = -11244.14600171811
1
Iteration 9700: Loss = -11244.139127709701
Iteration 9800: Loss = -11244.140956474492
1
Iteration 9900: Loss = -11244.142393793605
2
Iteration 10000: Loss = -11244.15925721673
3
Iteration 10100: Loss = -11244.14127568487
4
Iteration 10200: Loss = -11244.137407440963
Iteration 10300: Loss = -11244.148997617851
1
Iteration 10400: Loss = -11244.172676772581
2
Iteration 10500: Loss = -11244.140669419765
3
Iteration 10600: Loss = -11244.134469085917
Iteration 10700: Loss = -11244.182532567005
1
Iteration 10800: Loss = -11244.134449726675
Iteration 10900: Loss = -11244.186817300233
1
Iteration 11000: Loss = -11244.134311875961
Iteration 11100: Loss = -11244.188097920316
1
Iteration 11200: Loss = -11244.13243960104
Iteration 11300: Loss = -11244.149182653673
1
Iteration 11400: Loss = -11244.132421358148
Iteration 11500: Loss = -11244.148001406598
1
Iteration 11600: Loss = -11244.140085359735
2
Iteration 11700: Loss = -11244.135339159004
3
Iteration 11800: Loss = -11244.210437641832
4
Iteration 11900: Loss = -11244.131649578794
Iteration 12000: Loss = -11244.131606834688
Iteration 12100: Loss = -11244.131492933207
Iteration 12200: Loss = -11244.131376252579
Iteration 12300: Loss = -11244.129789701397
Iteration 12400: Loss = -11244.131279942547
1
Iteration 12500: Loss = -11244.129780633955
Iteration 12600: Loss = -11244.130064941171
1
Iteration 12700: Loss = -11244.13001706815
2
Iteration 12800: Loss = -11244.129788939126
Iteration 12900: Loss = -11244.335694281588
1
Iteration 13000: Loss = -11244.129748146774
Iteration 13100: Loss = -11244.132130286733
1
Iteration 13200: Loss = -11244.13051657386
2
Iteration 13300: Loss = -11244.130012044301
3
Iteration 13400: Loss = -11244.137771949736
4
Iteration 13500: Loss = -11244.130875694607
5
Iteration 13600: Loss = -11244.12947322113
Iteration 13700: Loss = -11244.199447524059
1
Iteration 13800: Loss = -11244.11617617625
Iteration 13900: Loss = -11244.146342369184
1
Iteration 14000: Loss = -11244.12283806427
2
Iteration 14100: Loss = -11244.116169260238
Iteration 14200: Loss = -11244.119320223115
1
Iteration 14300: Loss = -11244.329120238834
2
Iteration 14400: Loss = -11244.116137400317
Iteration 14500: Loss = -11244.12511199851
1
Iteration 14600: Loss = -11244.11612656444
Iteration 14700: Loss = -11244.123339764956
1
Iteration 14800: Loss = -11244.115314573333
Iteration 14900: Loss = -11244.125740149477
1
Iteration 15000: Loss = -11244.115311276806
Iteration 15100: Loss = -11244.127909564646
1
Iteration 15200: Loss = -11244.115308716964
Iteration 15300: Loss = -11244.13685975165
1
Iteration 15400: Loss = -11244.115933338051
2
Iteration 15500: Loss = -11244.114955896473
Iteration 15600: Loss = -11244.117656359882
1
Iteration 15700: Loss = -11244.115890947713
2
Iteration 15800: Loss = -11244.115150176913
3
Iteration 15900: Loss = -11244.115461013751
4
Iteration 16000: Loss = -11244.197553110753
5
Iteration 16100: Loss = -11244.11492748295
Iteration 16200: Loss = -11244.115179333105
1
Iteration 16300: Loss = -11244.117502902467
2
Iteration 16400: Loss = -11244.275824877826
3
Iteration 16500: Loss = -11244.114910662018
Iteration 16600: Loss = -11244.144305086553
1
Iteration 16700: Loss = -11244.11491441646
Iteration 16800: Loss = -11244.126293907302
1
Iteration 16900: Loss = -11244.114898284848
Iteration 17000: Loss = -11244.115213305751
1
Iteration 17100: Loss = -11244.115093550725
2
Iteration 17200: Loss = -11244.116779475362
3
Iteration 17300: Loss = -11244.168653452747
4
Iteration 17400: Loss = -11244.114961720044
Iteration 17500: Loss = -11244.115003998471
Iteration 17600: Loss = -11244.400313542656
1
Iteration 17700: Loss = -11244.114920661106
Iteration 17800: Loss = -11244.198658036157
1
Iteration 17900: Loss = -11244.114910054092
Iteration 18000: Loss = -11244.117960319662
1
Iteration 18100: Loss = -11244.121487111202
2
Iteration 18200: Loss = -11244.115481564368
3
Iteration 18300: Loss = -11244.177477986297
4
Iteration 18400: Loss = -11244.114882957081
Iteration 18500: Loss = -11244.141341771003
1
Iteration 18600: Loss = -11244.12668271159
2
Iteration 18700: Loss = -11244.148686200942
3
Iteration 18800: Loss = -11244.119523091615
4
Iteration 18900: Loss = -11244.114905154873
Iteration 19000: Loss = -11244.13058303619
1
Iteration 19100: Loss = -11244.114825910314
Iteration 19200: Loss = -11244.205950227415
1
Iteration 19300: Loss = -11244.114846206987
Iteration 19400: Loss = -11244.11743929097
1
Iteration 19500: Loss = -11244.114942759117
Iteration 19600: Loss = -11244.114866981723
Iteration 19700: Loss = -11244.408306273928
1
Iteration 19800: Loss = -11244.114814358174
Iteration 19900: Loss = -11244.119622906173
1
pi: tensor([[0.6741, 0.3259],
        [0.3609, 0.6391]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3054, 0.6946], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2936, 0.0937],
         [0.5851, 0.2168]],

        [[0.6442, 0.0963],
         [0.7174, 0.5375]],

        [[0.6499, 0.1021],
         [0.6399, 0.6231]],

        [[0.5794, 0.1006],
         [0.5104, 0.7169]],

        [[0.7242, 0.0992],
         [0.7037, 0.6330]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 84
Adjusted Rand Index: 0.4575257538278486
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.40330698473744125
Average Adjusted Rand Index: 0.8364709646265791
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23169.623567633418
Iteration 100: Loss = -11498.2660751917
Iteration 200: Loss = -11496.941764426127
Iteration 300: Loss = -11496.252385551708
Iteration 400: Loss = -11493.592934753586
Iteration 500: Loss = -11486.1177768064
Iteration 600: Loss = -11224.269140953691
Iteration 700: Loss = -11178.10911316408
Iteration 800: Loss = -11177.490490466234
Iteration 900: Loss = -11177.22397435645
Iteration 1000: Loss = -11177.095901366138
Iteration 1100: Loss = -11177.049620991731
Iteration 1200: Loss = -11177.01858485604
Iteration 1300: Loss = -11176.995901879038
Iteration 1400: Loss = -11176.978771722625
Iteration 1500: Loss = -11176.965735242436
Iteration 1600: Loss = -11176.95564202689
Iteration 1700: Loss = -11176.947436171939
Iteration 1800: Loss = -11176.940385254687
Iteration 1900: Loss = -11176.933595972774
Iteration 2000: Loss = -11176.923386800358
Iteration 2100: Loss = -11176.917256252138
Iteration 2200: Loss = -11176.913265293942
Iteration 2300: Loss = -11176.90927439784
Iteration 2400: Loss = -11176.89844876573
Iteration 2500: Loss = -11176.892531561776
Iteration 2600: Loss = -11176.881358700533
Iteration 2700: Loss = -11176.879727516283
Iteration 2800: Loss = -11176.878318405012
Iteration 2900: Loss = -11176.877958014544
Iteration 3000: Loss = -11176.875971117315
Iteration 3100: Loss = -11176.875254004413
Iteration 3200: Loss = -11176.87629136651
1
Iteration 3300: Loss = -11176.873174247881
Iteration 3400: Loss = -11176.873140501408
Iteration 3500: Loss = -11176.872134422887
Iteration 3600: Loss = -11176.871597154013
Iteration 3700: Loss = -11176.871677691928
Iteration 3800: Loss = -11176.86980073667
Iteration 3900: Loss = -11176.869132476848
Iteration 4000: Loss = -11176.869146096842
Iteration 4100: Loss = -11176.868012933635
Iteration 4200: Loss = -11176.867379738416
Iteration 4300: Loss = -11176.87265153994
1
Iteration 4400: Loss = -11176.867432664438
Iteration 4500: Loss = -11176.865576747947
Iteration 4600: Loss = -11176.864721678852
Iteration 4700: Loss = -11176.86496074274
1
Iteration 4800: Loss = -11176.860768844372
Iteration 4900: Loss = -11176.874741072981
1
Iteration 5000: Loss = -11176.86001693197
Iteration 5100: Loss = -11176.859964494268
Iteration 5200: Loss = -11176.859386794153
Iteration 5300: Loss = -11176.859262916681
Iteration 5400: Loss = -11176.85834989531
Iteration 5500: Loss = -11176.857984727945
Iteration 5600: Loss = -11176.860547687422
1
Iteration 5700: Loss = -11176.857539898392
Iteration 5800: Loss = -11176.857442841005
Iteration 5900: Loss = -11176.85762654446
1
Iteration 6000: Loss = -11176.857170006746
Iteration 6100: Loss = -11176.859928439406
1
Iteration 6200: Loss = -11176.868510359325
2
Iteration 6300: Loss = -11176.85688417321
Iteration 6400: Loss = -11176.856845443623
Iteration 6500: Loss = -11176.856285340307
Iteration 6600: Loss = -11176.855821150662
Iteration 6700: Loss = -11176.855623227459
Iteration 6800: Loss = -11176.855523200778
Iteration 6900: Loss = -11176.855774432694
1
Iteration 7000: Loss = -11176.855420074184
Iteration 7100: Loss = -11176.863843731335
1
Iteration 7200: Loss = -11176.855287363833
Iteration 7300: Loss = -11176.85937999451
1
Iteration 7400: Loss = -11176.855217062917
Iteration 7500: Loss = -11176.855169425336
Iteration 7600: Loss = -11176.855159028311
Iteration 7700: Loss = -11176.859856709274
1
Iteration 7800: Loss = -11176.857210425664
2
Iteration 7900: Loss = -11176.855507821847
3
Iteration 8000: Loss = -11176.85497559073
Iteration 8100: Loss = -11176.856297544071
1
Iteration 8200: Loss = -11176.908064294526
2
Iteration 8300: Loss = -11176.854860734453
Iteration 8400: Loss = -11176.85522746828
1
Iteration 8500: Loss = -11176.854543579158
Iteration 8600: Loss = -11176.85185634485
Iteration 8700: Loss = -11176.851753343744
Iteration 8800: Loss = -11176.858265542678
1
Iteration 8900: Loss = -11176.851702194157
Iteration 9000: Loss = -11176.851669320891
Iteration 9100: Loss = -11176.851679968799
Iteration 9200: Loss = -11176.85163868354
Iteration 9300: Loss = -11176.852021889477
1
Iteration 9400: Loss = -11176.852077807347
2
Iteration 9500: Loss = -11176.85175152645
3
Iteration 9600: Loss = -11176.852062901275
4
Iteration 9700: Loss = -11176.856230785803
5
Iteration 9800: Loss = -11176.85213018793
6
Iteration 9900: Loss = -11176.851440879185
Iteration 10000: Loss = -11176.854127696148
1
Iteration 10100: Loss = -11176.854656798178
2
Iteration 10200: Loss = -11176.874928864376
3
Iteration 10300: Loss = -11176.905891677301
4
Iteration 10400: Loss = -11176.85126541917
Iteration 10500: Loss = -11176.850964636336
Iteration 10600: Loss = -11176.900244402077
1
Iteration 10700: Loss = -11176.85093542963
Iteration 10800: Loss = -11176.880327874118
1
Iteration 10900: Loss = -11176.850925446566
Iteration 11000: Loss = -11176.852465669928
1
Iteration 11100: Loss = -11176.850903433697
Iteration 11200: Loss = -11176.8508888191
Iteration 11300: Loss = -11176.851716552488
1
Iteration 11400: Loss = -11176.851576884894
2
Iteration 11500: Loss = -11176.887986106345
3
Iteration 11600: Loss = -11176.850882703146
Iteration 11700: Loss = -11176.85633604373
1
Iteration 11800: Loss = -11176.850815890673
Iteration 11900: Loss = -11176.853200543006
1
Iteration 12000: Loss = -11176.850844313247
Iteration 12100: Loss = -11176.851091913013
1
Iteration 12200: Loss = -11176.858543105158
2
Iteration 12300: Loss = -11176.850884447405
Iteration 12400: Loss = -11176.864580331832
1
Iteration 12500: Loss = -11176.850867548046
Iteration 12600: Loss = -11176.85395815317
1
Iteration 12700: Loss = -11176.850822368784
Iteration 12800: Loss = -11176.905080352797
1
Iteration 12900: Loss = -11176.850840085957
Iteration 13000: Loss = -11176.972261123185
1
Iteration 13100: Loss = -11176.851352521899
2
Iteration 13200: Loss = -11176.85088384952
Iteration 13300: Loss = -11176.851422079852
1
Iteration 13400: Loss = -11176.853723306336
2
Iteration 13500: Loss = -11177.070879189281
3
Iteration 13600: Loss = -11176.850814234087
Iteration 13700: Loss = -11176.875769188991
1
Iteration 13800: Loss = -11176.850768856464
Iteration 13900: Loss = -11176.850937386656
1
Iteration 14000: Loss = -11176.859925152117
2
Iteration 14100: Loss = -11177.032404391197
3
Iteration 14200: Loss = -11176.855566667109
4
Iteration 14300: Loss = -11176.850804301437
Iteration 14400: Loss = -11176.851113775141
1
Iteration 14500: Loss = -11176.860658992398
2
Iteration 14600: Loss = -11176.877387817993
3
Iteration 14700: Loss = -11176.850730573811
Iteration 14800: Loss = -11176.851635921135
1
Iteration 14900: Loss = -11176.898158910135
2
Iteration 15000: Loss = -11176.850708970756
Iteration 15100: Loss = -11176.851156701381
1
Iteration 15200: Loss = -11176.850716703873
Iteration 15300: Loss = -11176.850890723228
1
Iteration 15400: Loss = -11177.066290663246
2
Iteration 15500: Loss = -11176.850696054484
Iteration 15600: Loss = -11176.863561656668
1
Iteration 15700: Loss = -11176.858149055623
2
Iteration 15800: Loss = -11176.853310323077
3
Iteration 15900: Loss = -11176.909876407579
4
Iteration 16000: Loss = -11176.852397876466
5
Iteration 16100: Loss = -11176.850735164986
Iteration 16200: Loss = -11176.85191950389
1
Iteration 16300: Loss = -11176.851260592892
2
Iteration 16400: Loss = -11176.855396907265
3
Iteration 16500: Loss = -11176.87042329156
4
Iteration 16600: Loss = -11176.986435767707
5
Iteration 16700: Loss = -11176.856210002983
6
Iteration 16800: Loss = -11176.854671871653
7
Iteration 16900: Loss = -11176.855609311384
8
Iteration 17000: Loss = -11176.85165063565
9
Iteration 17100: Loss = -11176.854627824281
10
Iteration 17200: Loss = -11176.850852220803
11
Iteration 17300: Loss = -11176.855479787642
12
Iteration 17400: Loss = -11176.856451668984
13
Iteration 17500: Loss = -11176.851481201118
14
Iteration 17600: Loss = -11176.85075639405
Iteration 17700: Loss = -11176.851955760068
1
Iteration 17800: Loss = -11176.899325951958
2
Iteration 17900: Loss = -11176.850684372082
Iteration 18000: Loss = -11176.941167768275
1
Iteration 18100: Loss = -11176.85068105525
Iteration 18200: Loss = -11176.881973005045
1
Iteration 18300: Loss = -11176.852075607101
2
Iteration 18400: Loss = -11176.85120612497
3
Iteration 18500: Loss = -11176.978781127533
4
Iteration 18600: Loss = -11176.850701308997
Iteration 18700: Loss = -11176.85084597826
1
Iteration 18800: Loss = -11176.850699453023
Iteration 18900: Loss = -11176.850828401157
1
Iteration 19000: Loss = -11176.859863485788
2
Iteration 19100: Loss = -11176.850694181829
Iteration 19200: Loss = -11176.8514569829
1
Iteration 19300: Loss = -11176.850791861823
Iteration 19400: Loss = -11176.850764232597
Iteration 19500: Loss = -11176.871678542328
1
Iteration 19600: Loss = -11176.85069745564
Iteration 19700: Loss = -11177.042757121257
1
Iteration 19800: Loss = -11176.850704289853
Iteration 19900: Loss = -11176.890217788869
1
pi: tensor([[0.7579, 0.2421],
        [0.2360, 0.7640]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5192, 0.4808], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3013, 0.0941],
         [0.5596, 0.2064]],

        [[0.7186, 0.0965],
         [0.5924, 0.6976]],

        [[0.6813, 0.1033],
         [0.6996, 0.6864]],

        [[0.6504, 0.1017],
         [0.6542, 0.5960]],

        [[0.5901, 0.0998],
         [0.5684, 0.5888]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524807831026916
Average Adjusted Rand Index: 0.9524819058861691
11191.050636335178
[0.40330698473744125, 0.9524807831026916] [0.8364709646265791, 0.9524819058861691] [11244.114803619577, 11176.850626282165]
-------------------------------------
This iteration is 52
True Objective function: Loss = -11143.110145076182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21027.56044861817
Iteration 100: Loss = -11383.610859980428
Iteration 200: Loss = -11383.185948556398
Iteration 300: Loss = -11383.060649905197
Iteration 400: Loss = -11382.973380612077
Iteration 500: Loss = -11382.910489173106
Iteration 600: Loss = -11382.841916887795
Iteration 700: Loss = -11382.427602079899
Iteration 800: Loss = -11380.007562242134
Iteration 900: Loss = -11379.859649245913
Iteration 1000: Loss = -11379.802296411557
Iteration 1100: Loss = -11379.767963827519
Iteration 1200: Loss = -11379.742557395744
Iteration 1300: Loss = -11379.719013185717
Iteration 1400: Loss = -11379.687065332291
Iteration 1500: Loss = -11379.604355397592
Iteration 1600: Loss = -11379.3786513468
Iteration 1700: Loss = -11379.227013377093
Iteration 1800: Loss = -11379.185274885647
Iteration 1900: Loss = -11379.173401669073
Iteration 2000: Loss = -11379.168644406072
Iteration 2100: Loss = -11379.165929850695
Iteration 2200: Loss = -11379.163854797793
Iteration 2300: Loss = -11379.162311666752
Iteration 2400: Loss = -11379.161387114335
Iteration 2500: Loss = -11379.160794579579
Iteration 2600: Loss = -11379.160373934536
Iteration 2700: Loss = -11379.16001462519
Iteration 2800: Loss = -11379.15970242302
Iteration 2900: Loss = -11379.159541451485
Iteration 3000: Loss = -11379.159339839991
Iteration 3100: Loss = -11379.15922764149
Iteration 3200: Loss = -11379.159077829274
Iteration 3300: Loss = -11379.158937686614
Iteration 3400: Loss = -11379.158852049979
Iteration 3500: Loss = -11379.158776204587
Iteration 3600: Loss = -11379.158701020197
Iteration 3700: Loss = -11379.158645940519
Iteration 3800: Loss = -11379.158577731705
Iteration 3900: Loss = -11379.158542934887
Iteration 4000: Loss = -11379.158449743394
Iteration 4100: Loss = -11379.158431109241
Iteration 4200: Loss = -11379.158399208272
Iteration 4300: Loss = -11379.158373638948
Iteration 4400: Loss = -11379.158317033969
Iteration 4500: Loss = -11379.158292721799
Iteration 4600: Loss = -11379.15826107366
Iteration 4700: Loss = -11379.158250834926
Iteration 4800: Loss = -11379.1582315796
Iteration 4900: Loss = -11379.158178732589
Iteration 5000: Loss = -11379.158176365774
Iteration 5100: Loss = -11379.15818956971
Iteration 5200: Loss = -11379.15814908834
Iteration 5300: Loss = -11379.158162420248
Iteration 5400: Loss = -11379.158093281596
Iteration 5500: Loss = -11379.158118983389
Iteration 5600: Loss = -11379.158090318637
Iteration 5700: Loss = -11379.158075120204
Iteration 5800: Loss = -11379.158068995112
Iteration 5900: Loss = -11379.158669946017
1
Iteration 6000: Loss = -11379.15804940695
Iteration 6100: Loss = -11379.158031426776
Iteration 6200: Loss = -11379.158030688934
Iteration 6300: Loss = -11379.158055694985
Iteration 6400: Loss = -11379.158111012208
Iteration 6500: Loss = -11379.158009680054
Iteration 6600: Loss = -11379.159438855788
1
Iteration 6700: Loss = -11379.158034776483
Iteration 6800: Loss = -11379.15801429548
Iteration 6900: Loss = -11379.158014265911
Iteration 7000: Loss = -11379.158006095331
Iteration 7100: Loss = -11379.158326403884
1
Iteration 7200: Loss = -11379.157996638874
Iteration 7300: Loss = -11379.158002671606
Iteration 7400: Loss = -11379.158066667684
Iteration 7500: Loss = -11379.161334927876
1
Iteration 7600: Loss = -11379.160143931473
2
Iteration 7700: Loss = -11379.162798065734
3
Iteration 7800: Loss = -11379.1595110248
4
Iteration 7900: Loss = -11379.157996071606
Iteration 8000: Loss = -11379.158149884792
1
Iteration 8100: Loss = -11379.439500135395
2
Iteration 8200: Loss = -11379.157954778577
Iteration 8300: Loss = -11379.15795885872
Iteration 8400: Loss = -11379.157943403929
Iteration 8500: Loss = -11379.157980774698
Iteration 8600: Loss = -11379.17350780838
1
Iteration 8700: Loss = -11379.157933515195
Iteration 8800: Loss = -11379.163698192486
1
Iteration 8900: Loss = -11379.15796877637
Iteration 9000: Loss = -11379.158481950633
1
Iteration 9100: Loss = -11379.158735577172
2
Iteration 9200: Loss = -11379.157935246287
Iteration 9300: Loss = -11379.161108108809
1
Iteration 9400: Loss = -11379.157963666292
Iteration 9500: Loss = -11379.200194499881
1
Iteration 9600: Loss = -11379.157948165359
Iteration 9700: Loss = -11379.255190239295
1
Iteration 9800: Loss = -11379.157972063907
Iteration 9900: Loss = -11379.158103106543
1
Iteration 10000: Loss = -11379.160348935047
2
Iteration 10100: Loss = -11379.159189473357
3
Iteration 10200: Loss = -11379.157934641224
Iteration 10300: Loss = -11379.163370272337
1
Iteration 10400: Loss = -11379.159172974549
2
Iteration 10500: Loss = -11379.26628471145
3
Iteration 10600: Loss = -11379.157956593028
Iteration 10700: Loss = -11379.17066657342
1
Iteration 10800: Loss = -11379.157985452815
Iteration 10900: Loss = -11379.160811639702
1
Iteration 11000: Loss = -11379.175873601138
2
Iteration 11100: Loss = -11379.15921272301
3
Iteration 11200: Loss = -11379.221687725265
4
Iteration 11300: Loss = -11379.158815159659
5
Iteration 11400: Loss = -11379.161063702524
6
Iteration 11500: Loss = -11379.158021496029
Iteration 11600: Loss = -11379.18100679756
1
Iteration 11700: Loss = -11379.15790417065
Iteration 11800: Loss = -11379.160462516096
1
Iteration 11900: Loss = -11379.157934300714
Iteration 12000: Loss = -11379.160377773942
1
Iteration 12100: Loss = -11379.158363605842
2
Iteration 12200: Loss = -11379.263150742278
3
Iteration 12300: Loss = -11379.158296205507
4
Iteration 12400: Loss = -11379.182013558708
5
Iteration 12500: Loss = -11379.1625912953
6
Iteration 12600: Loss = -11379.161418321399
7
Iteration 12700: Loss = -11379.157959769283
Iteration 12800: Loss = -11379.159911472845
1
Iteration 12900: Loss = -11379.157946883168
Iteration 13000: Loss = -11379.158025706423
Iteration 13100: Loss = -11379.15797382249
Iteration 13200: Loss = -11379.157955715069
Iteration 13300: Loss = -11379.169284418414
1
Iteration 13400: Loss = -11379.157939370703
Iteration 13500: Loss = -11379.178085095215
1
Iteration 13600: Loss = -11379.166812756072
2
Iteration 13700: Loss = -11379.163275666222
3
Iteration 13800: Loss = -11379.168558439536
4
Iteration 13900: Loss = -11379.161849924452
5
Iteration 14000: Loss = -11379.159011284892
6
Iteration 14100: Loss = -11379.158427077304
7
Iteration 14200: Loss = -11379.158102048545
8
Iteration 14300: Loss = -11379.163168789119
9
Iteration 14400: Loss = -11379.15792300981
Iteration 14500: Loss = -11379.18140354737
1
Iteration 14600: Loss = -11379.158011132038
Iteration 14700: Loss = -11379.157979358659
Iteration 14800: Loss = -11379.15845919418
1
Iteration 14900: Loss = -11379.201086617693
2
Iteration 15000: Loss = -11379.157923381874
Iteration 15100: Loss = -11379.159070180542
1
Iteration 15200: Loss = -11379.157976329792
Iteration 15300: Loss = -11379.158691535144
1
Iteration 15400: Loss = -11379.15795750957
Iteration 15500: Loss = -11379.15854243091
1
Iteration 15600: Loss = -11379.157978270134
Iteration 15700: Loss = -11379.160914224405
1
Iteration 15800: Loss = -11379.158127955268
2
Iteration 15900: Loss = -11379.159067215152
3
Iteration 16000: Loss = -11379.180815680294
4
Iteration 16100: Loss = -11379.158025869083
Iteration 16200: Loss = -11379.158182631894
1
Iteration 16300: Loss = -11379.18920323367
2
Iteration 16400: Loss = -11379.286438082505
3
Iteration 16500: Loss = -11379.162262344187
4
Iteration 16600: Loss = -11379.158048699393
Iteration 16700: Loss = -11379.166056114434
1
Iteration 16800: Loss = -11379.157953822234
Iteration 16900: Loss = -11379.161686908155
1
Iteration 17000: Loss = -11379.157986718945
Iteration 17100: Loss = -11379.159388890926
1
Iteration 17200: Loss = -11379.189683914616
2
Iteration 17300: Loss = -11379.227818177063
3
Iteration 17400: Loss = -11379.158519523846
4
Iteration 17500: Loss = -11379.15800856401
Iteration 17600: Loss = -11379.1932976804
1
Iteration 17700: Loss = -11379.163292754882
2
Iteration 17800: Loss = -11379.170209655207
3
Iteration 17900: Loss = -11379.162377910605
4
Iteration 18000: Loss = -11379.157978953133
Iteration 18100: Loss = -11379.161890645486
1
Iteration 18200: Loss = -11379.15844065915
2
Iteration 18300: Loss = -11379.157995779797
Iteration 18400: Loss = -11379.15972772855
1
Iteration 18500: Loss = -11379.158669456965
2
Iteration 18600: Loss = -11379.158168393476
3
Iteration 18700: Loss = -11379.165256022085
4
Iteration 18800: Loss = -11379.157959442055
Iteration 18900: Loss = -11379.158711303942
1
Iteration 19000: Loss = -11379.157978405694
Iteration 19100: Loss = -11379.158119570775
1
Iteration 19200: Loss = -11379.174857493781
2
Iteration 19300: Loss = -11379.15798164147
Iteration 19400: Loss = -11379.158092234857
1
Iteration 19500: Loss = -11379.17440665103
2
Iteration 19600: Loss = -11379.157986019254
Iteration 19700: Loss = -11379.158436592712
1
Iteration 19800: Loss = -11379.183095490314
2
Iteration 19900: Loss = -11379.159420257245
3
pi: tensor([[0.9730, 0.0270],
        [0.9719, 0.0281]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0217, 0.9783], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1715, 0.2138],
         [0.6630, 0.1725]],

        [[0.6515, 0.1219],
         [0.5509, 0.5373]],

        [[0.5046, 0.2905],
         [0.6950, 0.5380]],

        [[0.5400, 0.2117],
         [0.6826, 0.5822]],

        [[0.5663, 0.1119],
         [0.5470, 0.6910]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012486693410567779
Average Adjusted Rand Index: -0.001292929292929293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23341.62511735181
Iteration 100: Loss = -11168.72455808231
Iteration 200: Loss = -11125.337120872247
Iteration 300: Loss = -11124.935870212343
Iteration 400: Loss = -11124.803559650698
Iteration 500: Loss = -11124.740616387375
Iteration 600: Loss = -11124.705560394133
Iteration 700: Loss = -11124.683496336715
Iteration 800: Loss = -11124.668600427643
Iteration 900: Loss = -11124.658312384767
Iteration 1000: Loss = -11124.65084136105
Iteration 1100: Loss = -11124.645301644716
Iteration 1200: Loss = -11124.640953884229
Iteration 1300: Loss = -11124.637607182502
Iteration 1400: Loss = -11124.634875456019
Iteration 1500: Loss = -11124.632673070515
Iteration 1600: Loss = -11124.630871813299
Iteration 1700: Loss = -11124.62932488394
Iteration 1800: Loss = -11124.628018874228
Iteration 1900: Loss = -11124.626885505608
Iteration 2000: Loss = -11124.62594358512
Iteration 2100: Loss = -11124.625127020268
Iteration 2200: Loss = -11124.624322756066
Iteration 2300: Loss = -11124.623471337116
Iteration 2400: Loss = -11124.622552221446
Iteration 2500: Loss = -11124.621766252967
Iteration 2600: Loss = -11124.621333846993
Iteration 2700: Loss = -11124.62095565146
Iteration 2800: Loss = -11124.620613764568
Iteration 2900: Loss = -11124.620326391343
Iteration 3000: Loss = -11124.620031449278
Iteration 3100: Loss = -11124.619822396939
Iteration 3200: Loss = -11124.627298130928
1
Iteration 3300: Loss = -11124.619631744665
Iteration 3400: Loss = -11124.620323048366
1
Iteration 3500: Loss = -11124.61948172853
Iteration 3600: Loss = -11124.618906238169
Iteration 3700: Loss = -11124.619575056062
1
Iteration 3800: Loss = -11124.620095977687
2
Iteration 3900: Loss = -11124.618546634354
Iteration 4000: Loss = -11124.619542069428
1
Iteration 4100: Loss = -11124.619374625876
2
Iteration 4200: Loss = -11124.618950413198
3
Iteration 4300: Loss = -11124.618181508704
Iteration 4400: Loss = -11124.618174587047
Iteration 4500: Loss = -11124.618089523552
Iteration 4600: Loss = -11124.618326595017
1
Iteration 4700: Loss = -11124.618966889882
2
Iteration 4800: Loss = -11124.618489041704
3
Iteration 4900: Loss = -11124.618480344374
4
Iteration 5000: Loss = -11124.617918331809
Iteration 5100: Loss = -11124.617726758726
Iteration 5200: Loss = -11124.617705864232
Iteration 5300: Loss = -11124.620166096893
1
Iteration 5400: Loss = -11124.617605666448
Iteration 5500: Loss = -11124.620525570612
1
Iteration 5600: Loss = -11124.617533016735
Iteration 5700: Loss = -11124.61753869425
Iteration 5800: Loss = -11124.617505697746
Iteration 5900: Loss = -11124.621601858054
1
Iteration 6000: Loss = -11124.61745301097
Iteration 6100: Loss = -11124.617385595706
Iteration 6200: Loss = -11124.619808567979
1
Iteration 6300: Loss = -11124.617342853531
Iteration 6400: Loss = -11124.629900874472
1
Iteration 6500: Loss = -11124.617314548492
Iteration 6600: Loss = -11124.617261395673
Iteration 6700: Loss = -11124.617436365801
1
Iteration 6800: Loss = -11124.617240160549
Iteration 6900: Loss = -11124.617252294654
Iteration 7000: Loss = -11124.61732135156
Iteration 7100: Loss = -11124.617310896832
Iteration 7200: Loss = -11124.626575641065
1
Iteration 7300: Loss = -11124.637896706721
2
Iteration 7400: Loss = -11124.617183846707
Iteration 7500: Loss = -11124.617164692783
Iteration 7600: Loss = -11124.623149393488
1
Iteration 7700: Loss = -11124.61715121483
Iteration 7800: Loss = -11124.639205881349
1
Iteration 7900: Loss = -11124.617106287667
Iteration 8000: Loss = -11124.638769888643
1
Iteration 8100: Loss = -11124.617146665969
Iteration 8200: Loss = -11124.617166016036
Iteration 8300: Loss = -11124.617138210582
Iteration 8400: Loss = -11124.617119695253
Iteration 8500: Loss = -11124.620770928408
1
Iteration 8600: Loss = -11124.617094500732
Iteration 8700: Loss = -11124.617912840726
1
Iteration 8800: Loss = -11124.617051836492
Iteration 8900: Loss = -11124.617736405595
1
Iteration 9000: Loss = -11124.623523304992
2
Iteration 9100: Loss = -11124.617418600596
3
Iteration 9200: Loss = -11124.617097234257
Iteration 9300: Loss = -11124.617264060393
1
Iteration 9400: Loss = -11124.61789732583
2
Iteration 9500: Loss = -11124.62211134814
3
Iteration 9600: Loss = -11124.617067806435
Iteration 9700: Loss = -11124.618151961287
1
Iteration 9800: Loss = -11124.617050019044
Iteration 9900: Loss = -11124.617310428439
1
Iteration 10000: Loss = -11124.61698029926
Iteration 10100: Loss = -11124.617715384236
1
Iteration 10200: Loss = -11124.616879244304
Iteration 10300: Loss = -11124.616921605717
Iteration 10400: Loss = -11124.616980323244
Iteration 10500: Loss = -11124.628095380764
1
Iteration 10600: Loss = -11124.624410877777
2
Iteration 10700: Loss = -11124.617622201084
3
Iteration 10800: Loss = -11124.620597729452
4
Iteration 10900: Loss = -11124.620426915635
5
Iteration 11000: Loss = -11124.682015159611
6
Iteration 11100: Loss = -11124.621204991268
7
Iteration 11200: Loss = -11124.616993396594
Iteration 11300: Loss = -11124.617471289643
1
Iteration 11400: Loss = -11124.621306036961
2
Iteration 11500: Loss = -11124.624072757992
3
Iteration 11600: Loss = -11124.634899750721
4
Iteration 11700: Loss = -11124.670479732555
5
Iteration 11800: Loss = -11124.632789596275
6
Iteration 11900: Loss = -11124.643987651865
7
Iteration 12000: Loss = -11124.625852103934
8
Iteration 12100: Loss = -11124.61977195835
9
Iteration 12200: Loss = -11124.61695153912
Iteration 12300: Loss = -11124.620247551773
1
Iteration 12400: Loss = -11124.71489901045
2
Iteration 12500: Loss = -11124.616940855507
Iteration 12600: Loss = -11124.616929384807
Iteration 12700: Loss = -11124.650591178795
1
Iteration 12800: Loss = -11124.633082010005
2
Iteration 12900: Loss = -11124.628553779537
3
Iteration 13000: Loss = -11124.622452890868
4
Iteration 13100: Loss = -11124.649421509388
5
Iteration 13200: Loss = -11124.61698092325
Iteration 13300: Loss = -11124.61694684215
Iteration 13400: Loss = -11124.619645269991
1
Iteration 13500: Loss = -11124.616986089128
Iteration 13600: Loss = -11124.617244730509
1
Iteration 13700: Loss = -11124.635962956218
2
Iteration 13800: Loss = -11124.616918644297
Iteration 13900: Loss = -11124.61862486832
1
Iteration 14000: Loss = -11124.67951443048
2
Iteration 14100: Loss = -11124.649155036648
3
Iteration 14200: Loss = -11124.72604345078
4
Iteration 14300: Loss = -11124.652793723137
5
Iteration 14400: Loss = -11124.620551466069
6
Iteration 14500: Loss = -11124.617030860076
7
Iteration 14600: Loss = -11124.616948334513
Iteration 14700: Loss = -11124.617107903385
1
Iteration 14800: Loss = -11124.617446385695
2
Iteration 14900: Loss = -11124.617479488625
3
Iteration 15000: Loss = -11124.61687680043
Iteration 15100: Loss = -11124.617045235427
1
Iteration 15200: Loss = -11124.718567700947
2
Iteration 15300: Loss = -11124.616851086404
Iteration 15400: Loss = -11124.638635580968
1
Iteration 15500: Loss = -11124.616847435444
Iteration 15600: Loss = -11124.616929439311
Iteration 15700: Loss = -11124.651661625385
1
Iteration 15800: Loss = -11124.622395385217
2
Iteration 15900: Loss = -11124.616938053154
Iteration 16000: Loss = -11124.616918196958
Iteration 16100: Loss = -11124.618478449413
1
Iteration 16200: Loss = -11124.62547207992
2
Iteration 16300: Loss = -11124.617895579324
3
Iteration 16400: Loss = -11124.617187420787
4
Iteration 16500: Loss = -11124.649684904285
5
Iteration 16600: Loss = -11124.616856107086
Iteration 16700: Loss = -11124.616913084437
Iteration 16800: Loss = -11124.620125586824
1
Iteration 16900: Loss = -11124.640455911513
2
Iteration 17000: Loss = -11124.616851837563
Iteration 17100: Loss = -11124.618910320767
1
Iteration 17200: Loss = -11124.627141475958
2
Iteration 17300: Loss = -11124.617456079357
3
Iteration 17400: Loss = -11124.621008339785
4
Iteration 17500: Loss = -11124.616831821542
Iteration 17600: Loss = -11124.623237968066
1
Iteration 17700: Loss = -11124.616837586284
Iteration 17800: Loss = -11124.617815935071
1
Iteration 17900: Loss = -11124.616830957704
Iteration 18000: Loss = -11124.63613792746
1
Iteration 18100: Loss = -11124.61708942711
2
Iteration 18200: Loss = -11124.616937372479
3
Iteration 18300: Loss = -11124.617312846047
4
Iteration 18400: Loss = -11124.617207339028
5
Iteration 18500: Loss = -11124.617287406561
6
Iteration 18600: Loss = -11124.61905025471
7
Iteration 18700: Loss = -11124.617177482634
8
Iteration 18800: Loss = -11124.619822762737
9
Iteration 18900: Loss = -11124.677570314556
10
Iteration 19000: Loss = -11124.66872990392
11
Iteration 19100: Loss = -11124.617692777721
12
Iteration 19200: Loss = -11124.616876990134
Iteration 19300: Loss = -11124.620147208481
1
Iteration 19400: Loss = -11124.640893243295
2
Iteration 19500: Loss = -11124.616817062402
Iteration 19600: Loss = -11124.637188332305
1
Iteration 19700: Loss = -11124.61683558308
Iteration 19800: Loss = -11124.626366295463
1
Iteration 19900: Loss = -11124.658252928772
2
pi: tensor([[0.7441, 0.2559],
        [0.2272, 0.7728]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4851, 0.5149], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2949, 0.1068],
         [0.6751, 0.1986]],

        [[0.6972, 0.0957],
         [0.7300, 0.5743]],

        [[0.6965, 0.1091],
         [0.5466, 0.7157]],

        [[0.5991, 0.0990],
         [0.6435, 0.7020]],

        [[0.6310, 0.0969],
         [0.5713, 0.6619]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 1
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9446732505061082
Average Adjusted Rand Index: 0.9451285247846608
11143.110145076182
[-0.0012486693410567779, 0.9446732505061082] [-0.001292929292929293, 0.9451285247846608] [11379.157943061546, 11124.618573685346]
-------------------------------------
This iteration is 53
True Objective function: Loss = -11253.581865811346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21653.112683027313
Iteration 100: Loss = -11541.593257881694
Iteration 200: Loss = -11537.893313440218
Iteration 300: Loss = -11531.412584045884
Iteration 400: Loss = -11516.011221226709
Iteration 500: Loss = -11335.158157974769
Iteration 600: Loss = -11295.80852869414
Iteration 700: Loss = -11289.4738039857
Iteration 800: Loss = -11288.895895232936
Iteration 900: Loss = -11288.639248764108
Iteration 1000: Loss = -11286.873473197631
Iteration 1100: Loss = -11286.204825360412
Iteration 1200: Loss = -11286.093769014922
Iteration 1300: Loss = -11285.626453760058
Iteration 1400: Loss = -11285.587660989617
Iteration 1500: Loss = -11285.369533686382
Iteration 1600: Loss = -11285.340813173198
Iteration 1700: Loss = -11285.324609867346
Iteration 1800: Loss = -11285.311943461731
Iteration 1900: Loss = -11285.301013338629
Iteration 2000: Loss = -11285.290252627354
Iteration 2100: Loss = -11285.266858292778
Iteration 2200: Loss = -11284.924951037943
Iteration 2300: Loss = -11284.889461807952
Iteration 2400: Loss = -11284.881944478222
Iteration 2500: Loss = -11284.876756861177
Iteration 2600: Loss = -11284.883562739333
1
Iteration 2700: Loss = -11284.867642923982
Iteration 2800: Loss = -11284.86317218202
Iteration 2900: Loss = -11284.85804211241
Iteration 3000: Loss = -11284.85063206142
Iteration 3100: Loss = -11284.845419556845
Iteration 3200: Loss = -11284.841719385067
Iteration 3300: Loss = -11284.838298462375
Iteration 3400: Loss = -11284.833663682266
Iteration 3500: Loss = -11284.828697228282
Iteration 3600: Loss = -11284.820966862262
Iteration 3700: Loss = -11284.797131708438
Iteration 3800: Loss = -11284.781154610788
Iteration 3900: Loss = -11284.76952290533
Iteration 4000: Loss = -11284.738420249596
Iteration 4100: Loss = -11284.67675231597
Iteration 4200: Loss = -11284.034415106726
Iteration 4300: Loss = -11240.715530284935
Iteration 4400: Loss = -11221.679468953736
Iteration 4500: Loss = -11221.53600350975
Iteration 4600: Loss = -11221.518191159123
Iteration 4700: Loss = -11221.514439200426
Iteration 4800: Loss = -11221.511725773738
Iteration 4900: Loss = -11221.508918451182
Iteration 5000: Loss = -11221.504039376985
Iteration 5100: Loss = -11221.505156905436
1
Iteration 5200: Loss = -11221.496494418832
Iteration 5300: Loss = -11221.497123617859
1
Iteration 5400: Loss = -11221.498216911532
2
Iteration 5500: Loss = -11221.497589265911
3
Iteration 5600: Loss = -11221.493026745902
Iteration 5700: Loss = -11221.49229105605
Iteration 5800: Loss = -11221.491723364434
Iteration 5900: Loss = -11221.496921865
1
Iteration 6000: Loss = -11221.494694970173
2
Iteration 6100: Loss = -11221.489953369697
Iteration 6200: Loss = -11221.491373075543
1
Iteration 6300: Loss = -11221.496871027492
2
Iteration 6400: Loss = -11221.489021019777
Iteration 6500: Loss = -11221.4880211592
Iteration 6600: Loss = -11221.48648008156
Iteration 6700: Loss = -11221.484009723701
Iteration 6800: Loss = -11221.483709239663
Iteration 6900: Loss = -11221.483612850727
Iteration 7000: Loss = -11221.483241275742
Iteration 7100: Loss = -11221.483520625052
1
Iteration 7200: Loss = -11221.482807683233
Iteration 7300: Loss = -11221.485988297552
1
Iteration 7400: Loss = -11221.561008998702
2
Iteration 7500: Loss = -11221.481171434403
Iteration 7600: Loss = -11221.48353009519
1
Iteration 7700: Loss = -11221.480796678523
Iteration 7800: Loss = -11221.489735525769
1
Iteration 7900: Loss = -11221.480693757716
Iteration 8000: Loss = -11221.480782144865
Iteration 8100: Loss = -11221.480647878434
Iteration 8200: Loss = -11221.480835015367
1
Iteration 8300: Loss = -11221.601385158074
2
Iteration 8400: Loss = -11221.480473228205
Iteration 8500: Loss = -11221.495159477112
1
Iteration 8600: Loss = -11221.480412890382
Iteration 8700: Loss = -11221.485561466727
1
Iteration 8800: Loss = -11221.480344921361
Iteration 8900: Loss = -11221.480320808108
Iteration 9000: Loss = -11221.499982981015
1
Iteration 9100: Loss = -11221.480248005484
Iteration 9200: Loss = -11221.48015507795
Iteration 9300: Loss = -11221.502596099415
1
Iteration 9400: Loss = -11221.471132630424
Iteration 9500: Loss = -11221.471090445799
Iteration 9600: Loss = -11221.47575004496
1
Iteration 9700: Loss = -11221.470951324656
Iteration 9800: Loss = -11221.470771776807
Iteration 9900: Loss = -11221.47065778054
Iteration 10000: Loss = -11221.47017219952
Iteration 10100: Loss = -11221.469891898363
Iteration 10200: Loss = -11221.469057492071
Iteration 10300: Loss = -11221.468752133209
Iteration 10400: Loss = -11221.472600184065
1
Iteration 10500: Loss = -11221.46864777742
Iteration 10600: Loss = -11221.468465231295
Iteration 10700: Loss = -11221.468291678102
Iteration 10800: Loss = -11221.469379947173
1
Iteration 10900: Loss = -11221.468265753352
Iteration 11000: Loss = -11221.46820153635
Iteration 11100: Loss = -11221.48299779866
1
Iteration 11200: Loss = -11221.46813014283
Iteration 11300: Loss = -11221.459618963703
Iteration 11400: Loss = -11221.459438046868
Iteration 11500: Loss = -11221.459671576855
1
Iteration 11600: Loss = -11221.459290457957
Iteration 11700: Loss = -11221.459277645095
Iteration 11800: Loss = -11221.476631363483
1
Iteration 11900: Loss = -11221.459256930857
Iteration 12000: Loss = -11221.459263905394
Iteration 12100: Loss = -11221.50025127434
1
Iteration 12200: Loss = -11221.459228205078
Iteration 12300: Loss = -11221.45922987875
Iteration 12400: Loss = -11221.517715404127
1
Iteration 12500: Loss = -11221.459225215232
Iteration 12600: Loss = -11221.459246538558
Iteration 12700: Loss = -11221.469289515942
1
Iteration 12800: Loss = -11221.459224000499
Iteration 12900: Loss = -11221.459212883648
Iteration 13000: Loss = -11221.474053147707
1
Iteration 13100: Loss = -11221.459199808372
Iteration 13200: Loss = -11221.459176112588
Iteration 13300: Loss = -11221.468040567983
1
Iteration 13400: Loss = -11221.45912417635
Iteration 13500: Loss = -11221.458988351505
Iteration 13600: Loss = -11221.5065125932
1
Iteration 13700: Loss = -11221.458776071557
Iteration 13800: Loss = -11221.458751060196
Iteration 13900: Loss = -11221.463493136362
1
Iteration 14000: Loss = -11221.458770987274
Iteration 14100: Loss = -11221.458732976365
Iteration 14200: Loss = -11221.460897021574
1
Iteration 14300: Loss = -11221.458725592509
Iteration 14400: Loss = -11221.458711239025
Iteration 14500: Loss = -11221.458727614896
Iteration 14600: Loss = -11221.459997129525
1
Iteration 14700: Loss = -11221.458636992098
Iteration 14800: Loss = -11221.45205182967
Iteration 14900: Loss = -11221.452025621695
Iteration 15000: Loss = -11221.45840769477
1
Iteration 15100: Loss = -11221.45205858993
Iteration 15200: Loss = -11221.452035570357
Iteration 15300: Loss = -11221.452272344999
1
Iteration 15400: Loss = -11221.45157589726
Iteration 15500: Loss = -11221.451547063849
Iteration 15600: Loss = -11221.451664334509
1
Iteration 15700: Loss = -11221.451564976174
Iteration 15800: Loss = -11221.461962251431
1
Iteration 15900: Loss = -11221.451559519799
Iteration 16000: Loss = -11221.451580912562
Iteration 16100: Loss = -11221.452439352186
1
Iteration 16200: Loss = -11221.451570295769
Iteration 16300: Loss = -11221.45154386589
Iteration 16400: Loss = -11221.451533144846
Iteration 16500: Loss = -11221.4617940787
1
Iteration 16600: Loss = -11221.451532129866
Iteration 16700: Loss = -11221.451533532007
Iteration 16800: Loss = -11221.453935694368
1
Iteration 16900: Loss = -11221.45154570558
Iteration 17000: Loss = -11221.451535477494
Iteration 17100: Loss = -11221.45183441894
1
Iteration 17200: Loss = -11221.451540818607
Iteration 17300: Loss = -11221.66323241777
1
Iteration 17400: Loss = -11221.451545182077
Iteration 17500: Loss = -11221.451540795219
Iteration 17600: Loss = -11221.46373405331
1
Iteration 17700: Loss = -11221.451564860092
Iteration 17800: Loss = -11221.451553012232
Iteration 17900: Loss = -11221.454094283497
1
Iteration 18000: Loss = -11221.451567430622
Iteration 18100: Loss = -11221.451550844497
Iteration 18200: Loss = -11221.561286524658
1
Iteration 18300: Loss = -11221.451572846965
Iteration 18400: Loss = -11221.451555115706
Iteration 18500: Loss = -11221.451551759033
Iteration 18600: Loss = -11221.453828191721
1
Iteration 18700: Loss = -11221.451540772861
Iteration 18800: Loss = -11221.451559263869
Iteration 18900: Loss = -11221.452028407963
1
Iteration 19000: Loss = -11221.451551670369
Iteration 19100: Loss = -11221.451556902397
Iteration 19200: Loss = -11221.45222650029
1
Iteration 19300: Loss = -11221.451559949053
Iteration 19400: Loss = -11221.460060170386
1
Iteration 19500: Loss = -11221.451542356777
Iteration 19600: Loss = -11221.451579071518
Iteration 19700: Loss = -11221.476505345414
1
Iteration 19800: Loss = -11221.451540295842
Iteration 19900: Loss = -11221.45155210187
pi: tensor([[0.7459, 0.2541],
        [0.2134, 0.7866]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5278, 0.4722], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1915, 0.1054],
         [0.6650, 0.3064]],

        [[0.5823, 0.0980],
         [0.6927, 0.5417]],

        [[0.5219, 0.1108],
         [0.5700, 0.6128]],

        [[0.6856, 0.0992],
         [0.7134, 0.5098]],

        [[0.6249, 0.1027],
         [0.5358, 0.6916]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8833667203965645
Average Adjusted Rand Index: 0.883545924623229
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22072.257972683157
Iteration 100: Loss = -11540.481443146331
Iteration 200: Loss = -11534.264950057019
Iteration 300: Loss = -11521.004305651204
Iteration 400: Loss = -11391.245127552582
Iteration 500: Loss = -11311.657946854191
Iteration 600: Loss = -11301.681237853489
Iteration 700: Loss = -11288.866467771355
Iteration 800: Loss = -11287.167135431375
Iteration 900: Loss = -11286.885473014321
Iteration 1000: Loss = -11286.384844132339
Iteration 1100: Loss = -11286.268444428306
Iteration 1200: Loss = -11286.120029414393
Iteration 1300: Loss = -11286.000821750444
Iteration 1400: Loss = -11285.505342394925
Iteration 1500: Loss = -11257.97572008559
Iteration 1600: Loss = -11244.669832545524
Iteration 1700: Loss = -11227.404829746238
Iteration 1800: Loss = -11223.77442401454
Iteration 1900: Loss = -11222.69780188243
Iteration 2000: Loss = -11222.563196376876
Iteration 2100: Loss = -11222.536663220217
Iteration 2200: Loss = -11222.50354578042
Iteration 2300: Loss = -11222.494703510998
Iteration 2400: Loss = -11222.490390793699
Iteration 2500: Loss = -11222.486840796259
Iteration 2600: Loss = -11222.483473903105
Iteration 2700: Loss = -11222.479671360492
Iteration 2800: Loss = -11222.468601451934
Iteration 2900: Loss = -11222.372590056519
Iteration 3000: Loss = -11222.35400648162
Iteration 3100: Loss = -11222.346691736579
Iteration 3200: Loss = -11222.345334980782
Iteration 3300: Loss = -11222.341719929993
Iteration 3400: Loss = -11222.33582836525
Iteration 3500: Loss = -11222.334907905866
Iteration 3600: Loss = -11222.33428727933
Iteration 3700: Loss = -11222.333760552727
Iteration 3800: Loss = -11222.333157828598
Iteration 3900: Loss = -11222.334776890682
1
Iteration 4000: Loss = -11222.324297385168
Iteration 4100: Loss = -11222.065549177909
Iteration 4200: Loss = -11222.070397564205
1
Iteration 4300: Loss = -11222.06628919857
2
Iteration 4400: Loss = -11222.070245924166
3
Iteration 4500: Loss = -11222.071048765094
4
Iteration 4600: Loss = -11222.064861297742
Iteration 4700: Loss = -11221.657002920747
Iteration 4800: Loss = -11221.632865991547
Iteration 4900: Loss = -11221.634476246509
1
Iteration 5000: Loss = -11221.63183993513
Iteration 5100: Loss = -11221.630921073564
Iteration 5200: Loss = -11221.63202074529
1
Iteration 5300: Loss = -11221.514080528766
Iteration 5400: Loss = -11221.521530692782
1
Iteration 5500: Loss = -11221.513021808383
Iteration 5600: Loss = -11221.515234040093
1
Iteration 5700: Loss = -11221.512622527056
Iteration 5800: Loss = -11221.51619004303
1
Iteration 5900: Loss = -11221.512331397256
Iteration 6000: Loss = -11221.512678417927
1
Iteration 6100: Loss = -11221.512140777093
Iteration 6200: Loss = -11221.512072657188
Iteration 6300: Loss = -11221.512355517138
1
Iteration 6400: Loss = -11221.51111779372
Iteration 6500: Loss = -11221.51541438594
1
Iteration 6600: Loss = -11221.511498944117
2
Iteration 6700: Loss = -11221.60834981747
3
Iteration 6800: Loss = -11221.510787258265
Iteration 6900: Loss = -11221.543742407865
1
Iteration 7000: Loss = -11221.510678514449
Iteration 7100: Loss = -11221.510645486922
Iteration 7200: Loss = -11221.510708313013
Iteration 7300: Loss = -11221.510552730997
Iteration 7400: Loss = -11221.518668593471
1
Iteration 7500: Loss = -11221.510490891225
Iteration 7600: Loss = -11221.540354581583
1
Iteration 7700: Loss = -11221.51044260283
Iteration 7800: Loss = -11221.510411463432
Iteration 7900: Loss = -11221.510412220465
Iteration 8000: Loss = -11221.510343889353
Iteration 8100: Loss = -11221.561669193408
1
Iteration 8200: Loss = -11221.510315904905
Iteration 8300: Loss = -11221.510264280076
Iteration 8400: Loss = -11221.51056865063
1
Iteration 8500: Loss = -11221.51018705503
Iteration 8600: Loss = -11221.510110851685
Iteration 8700: Loss = -11221.513179449972
1
Iteration 8800: Loss = -11221.51009415995
Iteration 8900: Loss = -11221.510012593359
Iteration 9000: Loss = -11221.510065521963
Iteration 9100: Loss = -11221.509722887924
Iteration 9200: Loss = -11221.509543543541
Iteration 9300: Loss = -11221.50954169232
Iteration 9400: Loss = -11221.509588308008
Iteration 9500: Loss = -11221.50947901042
Iteration 9600: Loss = -11221.597697590654
1
Iteration 9700: Loss = -11221.509494410686
Iteration 9800: Loss = -11221.509497161444
Iteration 9900: Loss = -11221.509875236388
1
Iteration 10000: Loss = -11221.509449489524
Iteration 10100: Loss = -11221.509451412914
Iteration 10200: Loss = -11221.509550860637
Iteration 10300: Loss = -11221.509430112503
Iteration 10400: Loss = -11221.509375812575
Iteration 10500: Loss = -11221.509388604518
Iteration 10600: Loss = -11221.509337605105
Iteration 10700: Loss = -11221.918093966637
1
Iteration 10800: Loss = -11221.500710281616
Iteration 10900: Loss = -11221.500523314187
Iteration 11000: Loss = -11221.748915250859
1
Iteration 11100: Loss = -11221.50043196189
Iteration 11200: Loss = -11221.500416965102
Iteration 11300: Loss = -11221.657749572612
1
Iteration 11400: Loss = -11221.500383451974
Iteration 11500: Loss = -11221.500417792351
Iteration 11600: Loss = -11221.7221266301
1
Iteration 11700: Loss = -11221.50039220205
Iteration 11800: Loss = -11221.500369769472
Iteration 11900: Loss = -11221.515844428193
1
Iteration 12000: Loss = -11221.500025940646
Iteration 12100: Loss = -11221.500012293418
Iteration 12200: Loss = -11221.537409841541
1
Iteration 12300: Loss = -11221.499981029203
Iteration 12400: Loss = -11221.499968931037
Iteration 12500: Loss = -11221.503074984079
1
Iteration 12600: Loss = -11221.499918288546
Iteration 12700: Loss = -11221.499749133036
Iteration 12800: Loss = -11221.500513257188
1
Iteration 12900: Loss = -11221.499701544213
Iteration 13000: Loss = -11221.49940753512
Iteration 13100: Loss = -11221.499527993545
1
Iteration 13200: Loss = -11221.499417242387
Iteration 13300: Loss = -11221.49939818441
Iteration 13400: Loss = -11221.501765391522
1
Iteration 13500: Loss = -11221.499391238143
Iteration 13600: Loss = -11221.499383610932
Iteration 13700: Loss = -11221.504199664578
1
Iteration 13800: Loss = -11221.49922734684
Iteration 13900: Loss = -11221.499216284392
Iteration 14000: Loss = -11221.50096311921
1
Iteration 14100: Loss = -11221.499232871205
Iteration 14200: Loss = -11221.49922411206
Iteration 14300: Loss = -11221.500172657103
1
Iteration 14400: Loss = -11221.499209988706
Iteration 14500: Loss = -11221.499185065719
Iteration 14600: Loss = -11221.514334238293
1
Iteration 14700: Loss = -11221.499184012328
Iteration 14800: Loss = -11221.499211042581
Iteration 14900: Loss = -11221.531741938405
1
Iteration 15000: Loss = -11221.499189543925
Iteration 15100: Loss = -11221.499188248114
Iteration 15200: Loss = -11221.500838985257
1
Iteration 15300: Loss = -11221.499187740434
Iteration 15400: Loss = -11221.499200897308
Iteration 15500: Loss = -11221.499600934201
1
Iteration 15600: Loss = -11221.499167342601
Iteration 15700: Loss = -11221.499705541893
1
Iteration 15800: Loss = -11221.499227777314
Iteration 15900: Loss = -11221.499181499039
Iteration 16000: Loss = -11221.5239932252
1
Iteration 16100: Loss = -11221.45167593241
Iteration 16200: Loss = -11221.584020995017
1
Iteration 16300: Loss = -11221.451674783057
Iteration 16400: Loss = -11221.451900493115
1
Iteration 16500: Loss = -11221.45166786343
Iteration 16600: Loss = -11221.451634532288
Iteration 16700: Loss = -11221.469359610735
1
Iteration 16800: Loss = -11221.451564818819
Iteration 16900: Loss = -11221.451545743486
Iteration 17000: Loss = -11221.505410891948
1
Iteration 17100: Loss = -11221.45157922561
Iteration 17200: Loss = -11221.451533012723
Iteration 17300: Loss = -11221.460395146774
1
Iteration 17400: Loss = -11221.451579651975
Iteration 17500: Loss = -11221.451559965371
Iteration 17600: Loss = -11221.452106135634
1
Iteration 17700: Loss = -11221.4515693445
Iteration 17800: Loss = -11221.837718548302
1
Iteration 17900: Loss = -11221.451553172772
Iteration 18000: Loss = -11221.451583401313
Iteration 18100: Loss = -11221.469851361522
1
Iteration 18200: Loss = -11221.451551718272
Iteration 18300: Loss = -11221.451567168015
Iteration 18400: Loss = -11221.456597728436
1
Iteration 18500: Loss = -11221.451559889987
Iteration 18600: Loss = -11221.45154950319
Iteration 18700: Loss = -11221.45537416497
1
Iteration 18800: Loss = -11221.451568364453
Iteration 18900: Loss = -11221.45155419223
Iteration 19000: Loss = -11221.451632611863
Iteration 19100: Loss = -11221.451591712232
Iteration 19200: Loss = -11221.483697258458
1
Iteration 19300: Loss = -11221.451573624285
Iteration 19400: Loss = -11221.451568191154
Iteration 19500: Loss = -11221.451649921224
Iteration 19600: Loss = -11221.451552309176
Iteration 19700: Loss = -11221.653459680827
1
Iteration 19800: Loss = -11221.45157921602
Iteration 19900: Loss = -11221.451545723563
pi: tensor([[0.7850, 0.2150],
        [0.2517, 0.7483]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4718, 0.5282], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3063, 0.1066],
         [0.5244, 0.1917]],

        [[0.7073, 0.0992],
         [0.7310, 0.7147]],

        [[0.6297, 0.1121],
         [0.6325, 0.6911]],

        [[0.6282, 0.0990],
         [0.5698, 0.6032]],

        [[0.7185, 0.1038],
         [0.6648, 0.5685]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8833667203965645
Average Adjusted Rand Index: 0.883545924623229
11253.581865811346
[0.8833667203965645, 0.8833667203965645] [0.883545924623229, 0.883545924623229] [11221.456418876665, 11221.60305392319]
-------------------------------------
This iteration is 54
True Objective function: Loss = -11344.415542328867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19456.480444573637
Iteration 100: Loss = -11712.23770243
Iteration 200: Loss = -11688.232755792502
Iteration 300: Loss = -11673.103788295111
Iteration 400: Loss = -11667.876746964715
Iteration 500: Loss = -11665.20373397217
Iteration 600: Loss = -11664.929018764053
Iteration 700: Loss = -11664.835806205509
Iteration 800: Loss = -11664.787366645916
Iteration 900: Loss = -11664.757537286898
Iteration 1000: Loss = -11664.73734176367
Iteration 1100: Loss = -11664.722799593532
Iteration 1200: Loss = -11664.71211712131
Iteration 1300: Loss = -11664.703931535903
Iteration 1400: Loss = -11664.697460716674
Iteration 1500: Loss = -11664.692225346778
Iteration 1600: Loss = -11664.687949406585
Iteration 1700: Loss = -11664.685729540737
Iteration 1800: Loss = -11664.68138089544
Iteration 1900: Loss = -11664.678812923268
Iteration 2000: Loss = -11664.676632087017
Iteration 2100: Loss = -11664.674716718895
Iteration 2200: Loss = -11664.673005708328
Iteration 2300: Loss = -11664.671534619547
Iteration 2400: Loss = -11664.670252230015
Iteration 2500: Loss = -11664.66923082451
Iteration 2600: Loss = -11664.668086758722
Iteration 2700: Loss = -11664.667162444624
Iteration 2800: Loss = -11664.679768867676
1
Iteration 2900: Loss = -11664.665569680315
Iteration 3000: Loss = -11664.664884446662
Iteration 3100: Loss = -11664.664819675265
Iteration 3200: Loss = -11664.663681144384
Iteration 3300: Loss = -11664.663225212096
Iteration 3400: Loss = -11664.662750208847
Iteration 3500: Loss = -11664.662330663763
Iteration 3600: Loss = -11664.678466152309
1
Iteration 3700: Loss = -11664.661543514949
Iteration 3800: Loss = -11664.66120671528
Iteration 3900: Loss = -11664.661968497241
1
Iteration 4000: Loss = -11664.660637189918
Iteration 4100: Loss = -11664.660357879793
Iteration 4200: Loss = -11664.660237226813
Iteration 4300: Loss = -11664.659928128536
Iteration 4400: Loss = -11664.659702474202
Iteration 4500: Loss = -11664.659706913433
Iteration 4600: Loss = -11664.65936296059
Iteration 4700: Loss = -11664.65914725263
Iteration 4800: Loss = -11664.65899364804
Iteration 4900: Loss = -11664.658857763101
Iteration 5000: Loss = -11664.661725545444
1
Iteration 5100: Loss = -11664.658556715944
Iteration 5200: Loss = -11664.658482363828
Iteration 5300: Loss = -11664.658380093377
Iteration 5400: Loss = -11664.6582622397
Iteration 5500: Loss = -11664.660265413077
1
Iteration 5600: Loss = -11664.658086941618
Iteration 5700: Loss = -11664.65798666854
Iteration 5800: Loss = -11664.657942063457
Iteration 5900: Loss = -11664.657816164963
Iteration 6000: Loss = -11664.65785432281
Iteration 6100: Loss = -11664.65767786218
Iteration 6200: Loss = -11664.657624493864
Iteration 6300: Loss = -11664.657690595373
Iteration 6400: Loss = -11664.702249278998
1
Iteration 6500: Loss = -11664.658638498136
2
Iteration 6600: Loss = -11664.657445263763
Iteration 6700: Loss = -11664.6574748084
Iteration 6800: Loss = -11664.728105736793
1
Iteration 6900: Loss = -11664.657292445861
Iteration 7000: Loss = -11664.661431895514
1
Iteration 7100: Loss = -11664.657221174704
Iteration 7200: Loss = -11664.659726747905
1
Iteration 7300: Loss = -11664.657095433264
Iteration 7400: Loss = -11664.813752687685
1
Iteration 7500: Loss = -11664.657089648197
Iteration 7600: Loss = -11664.657051171795
Iteration 7700: Loss = -11664.746883992355
1
Iteration 7800: Loss = -11664.656990042042
Iteration 7900: Loss = -11664.656987081418
Iteration 8000: Loss = -11664.658852704946
1
Iteration 8100: Loss = -11664.6568707116
Iteration 8200: Loss = -11664.6650987639
1
Iteration 8300: Loss = -11664.656911190728
Iteration 8400: Loss = -11664.656880790595
Iteration 8500: Loss = -11664.656898206504
Iteration 8600: Loss = -11664.656814669908
Iteration 8700: Loss = -11664.656930613206
1
Iteration 8800: Loss = -11664.65682581026
Iteration 8900: Loss = -11664.656769726726
Iteration 9000: Loss = -11664.668059683634
1
Iteration 9100: Loss = -11664.656713549053
Iteration 9200: Loss = -11664.65674815346
Iteration 9300: Loss = -11664.656768404937
Iteration 9400: Loss = -11664.656739251972
Iteration 9500: Loss = -11664.664500666177
1
Iteration 9600: Loss = -11664.656688403991
Iteration 9700: Loss = -11664.656723745742
Iteration 9800: Loss = -11664.68055259345
1
Iteration 9900: Loss = -11664.65665137984
Iteration 10000: Loss = -11664.662263442151
1
Iteration 10100: Loss = -11664.656673153675
Iteration 10200: Loss = -11664.656673714435
Iteration 10300: Loss = -11664.657071422058
1
Iteration 10400: Loss = -11664.65664676429
Iteration 10500: Loss = -11665.16457271249
1
Iteration 10600: Loss = -11664.656630769032
Iteration 10700: Loss = -11664.656595988385
Iteration 10800: Loss = -11664.660081854525
1
Iteration 10900: Loss = -11664.656672198409
Iteration 11000: Loss = -11664.656629066922
Iteration 11100: Loss = -11664.656672309515
Iteration 11200: Loss = -11664.656706784137
Iteration 11300: Loss = -11664.656592931395
Iteration 11400: Loss = -11664.657079977113
1
Iteration 11500: Loss = -11664.65657178819
Iteration 11600: Loss = -11664.656596274528
Iteration 11700: Loss = -11664.658702930337
1
Iteration 11800: Loss = -11664.65660537211
Iteration 11900: Loss = -11664.656568741062
Iteration 12000: Loss = -11664.661799818372
1
Iteration 12100: Loss = -11664.656614453987
Iteration 12200: Loss = -11664.65659203334
Iteration 12300: Loss = -11665.13215312404
1
Iteration 12400: Loss = -11664.656608837551
Iteration 12500: Loss = -11664.656571808226
Iteration 12600: Loss = -11664.734140114868
1
Iteration 12700: Loss = -11664.65658400883
Iteration 12800: Loss = -11664.656582949421
Iteration 12900: Loss = -11664.671461587768
1
Iteration 13000: Loss = -11664.656589738537
Iteration 13100: Loss = -11664.656558432574
Iteration 13200: Loss = -11664.656929851011
1
Iteration 13300: Loss = -11664.656586987856
Iteration 13400: Loss = -11664.65668532074
Iteration 13500: Loss = -11664.656644645582
Iteration 13600: Loss = -11664.656564521405
Iteration 13700: Loss = -11664.656579425555
Iteration 13800: Loss = -11664.656817323676
1
Iteration 13900: Loss = -11664.656566907024
Iteration 14000: Loss = -11664.693721025249
1
Iteration 14100: Loss = -11664.656533395171
Iteration 14200: Loss = -11664.656549310876
Iteration 14300: Loss = -11664.874675024985
1
Iteration 14400: Loss = -11664.656559675896
Iteration 14500: Loss = -11664.656517545483
Iteration 14600: Loss = -11664.673372738025
1
Iteration 14700: Loss = -11664.656561014173
Iteration 14800: Loss = -11664.657090540168
1
Iteration 14900: Loss = -11664.656551314793
Iteration 15000: Loss = -11664.658000647767
1
Iteration 15100: Loss = -11664.656530272929
Iteration 15200: Loss = -11664.656675354669
1
Iteration 15300: Loss = -11664.656585671843
Iteration 15400: Loss = -11664.656533148875
Iteration 15500: Loss = -11664.657235843437
1
Iteration 15600: Loss = -11664.65655359716
Iteration 15700: Loss = -11664.669549908567
1
Iteration 15800: Loss = -11664.656573944107
Iteration 15900: Loss = -11664.656560915142
Iteration 16000: Loss = -11664.662912471029
1
Iteration 16100: Loss = -11664.656536377792
Iteration 16200: Loss = -11664.66628755217
1
Iteration 16300: Loss = -11664.656547118626
Iteration 16400: Loss = -11664.730227927193
1
Iteration 16500: Loss = -11664.656525916133
Iteration 16600: Loss = -11664.65652147251
Iteration 16700: Loss = -11664.656565372874
Iteration 16800: Loss = -11664.656531420334
Iteration 16900: Loss = -11664.855362712125
1
Iteration 17000: Loss = -11664.656587090894
Iteration 17100: Loss = -11664.656839640491
1
Iteration 17200: Loss = -11664.6595065587
2
Iteration 17300: Loss = -11664.656548739977
Iteration 17400: Loss = -11664.656592748712
Iteration 17500: Loss = -11664.656729127137
1
Iteration 17600: Loss = -11664.65656036629
Iteration 17700: Loss = -11664.660348725109
1
Iteration 17800: Loss = -11664.656584075048
Iteration 17900: Loss = -11664.656590829434
Iteration 18000: Loss = -11664.657175861783
1
Iteration 18100: Loss = -11664.65657367988
Iteration 18200: Loss = -11664.656573281065
Iteration 18300: Loss = -11664.656889852035
1
Iteration 18400: Loss = -11664.656571525595
Iteration 18500: Loss = -11664.656529829399
Iteration 18600: Loss = -11664.656724777451
1
Iteration 18700: Loss = -11664.656556108679
Iteration 18800: Loss = -11664.656568417353
Iteration 18900: Loss = -11664.65688958498
1
Iteration 19000: Loss = -11664.65654225644
Iteration 19100: Loss = -11664.656528082418
Iteration 19200: Loss = -11664.65671292499
1
Iteration 19300: Loss = -11664.656506557652
Iteration 19400: Loss = -11664.66778394812
1
Iteration 19500: Loss = -11664.65652127669
Iteration 19600: Loss = -11664.656547588567
Iteration 19700: Loss = -11664.65684453843
1
Iteration 19800: Loss = -11664.65652147715
Iteration 19900: Loss = -11664.656578249673
pi: tensor([[0.2709, 0.7291],
        [0.0147, 0.9853]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 3.0147e-08], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1748, 0.1640],
         [0.7157, 0.1742]],

        [[0.7228, 0.1826],
         [0.7185, 0.5576]],

        [[0.5616, 0.2611],
         [0.5571, 0.5635]],

        [[0.7247, 0.2710],
         [0.5245, 0.6525]],

        [[0.6475, 0.3022],
         [0.7030, 0.5139]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.005131431169739117
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0037746410354473374
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
Global Adjusted Rand Index: -0.0010225991105388766
Average Adjusted Rand Index: -0.003252910041786854
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23760.65579838759
Iteration 100: Loss = -11671.87054403435
Iteration 200: Loss = -11663.917434954023
Iteration 300: Loss = -11654.848895257708
Iteration 400: Loss = -11617.002175777827
Iteration 500: Loss = -11437.473088752244
Iteration 600: Loss = -11398.678778346686
Iteration 700: Loss = -11386.20637740409
Iteration 800: Loss = -11385.78596255917
Iteration 900: Loss = -11385.656381928457
Iteration 1000: Loss = -11385.58270202757
Iteration 1100: Loss = -11385.52531381646
Iteration 1200: Loss = -11385.45469782712
Iteration 1300: Loss = -11385.046917345668
Iteration 1400: Loss = -11384.884497014948
Iteration 1500: Loss = -11384.867407994081
Iteration 1600: Loss = -11384.85369509483
Iteration 1700: Loss = -11384.842016419369
Iteration 1800: Loss = -11384.831677068525
Iteration 1900: Loss = -11384.822274170607
Iteration 2000: Loss = -11384.813754149147
Iteration 2100: Loss = -11384.806018475252
Iteration 2200: Loss = -11384.795884637808
Iteration 2300: Loss = -11384.764158050153
Iteration 2400: Loss = -11384.754388287323
Iteration 2500: Loss = -11384.734132022239
Iteration 2600: Loss = -11383.741408163783
Iteration 2700: Loss = -11383.73454130901
Iteration 2800: Loss = -11383.729846361712
Iteration 2900: Loss = -11383.727434214548
Iteration 3000: Loss = -11383.72391527816
Iteration 3100: Loss = -11383.716511844217
Iteration 3200: Loss = -11383.651038131587
Iteration 3300: Loss = -11383.6567744876
1
Iteration 3400: Loss = -11383.641062355357
Iteration 3500: Loss = -11383.640305267732
Iteration 3600: Loss = -11383.637725905819
Iteration 3700: Loss = -11383.637426512601
Iteration 3800: Loss = -11383.63534359306
Iteration 3900: Loss = -11383.634239038362
Iteration 4000: Loss = -11383.632985736544
Iteration 4100: Loss = -11383.62898211534
Iteration 4200: Loss = -11383.6239683167
Iteration 4300: Loss = -11383.622737340263
Iteration 4400: Loss = -11383.621477798431
Iteration 4500: Loss = -11383.620209271661
Iteration 4600: Loss = -11383.619176768925
Iteration 4700: Loss = -11383.617567634086
Iteration 4800: Loss = -11383.626794388367
1
Iteration 4900: Loss = -11383.608647436622
Iteration 5000: Loss = -11383.602689633768
Iteration 5100: Loss = -11383.59075390391
Iteration 5200: Loss = -11383.559140891493
Iteration 5300: Loss = -11383.508108587412
Iteration 5400: Loss = -11383.488235491548
Iteration 5500: Loss = -11383.478337038863
Iteration 5600: Loss = -11383.47079213214
Iteration 5700: Loss = -11383.465378995023
Iteration 5800: Loss = -11383.461596533836
Iteration 5900: Loss = -11383.445244257551
Iteration 6000: Loss = -11383.451005089768
1
Iteration 6100: Loss = -11383.424516899773
Iteration 6200: Loss = -11383.423502526372
Iteration 6300: Loss = -11383.414670279604
Iteration 6400: Loss = -11383.422186540061
1
Iteration 6500: Loss = -11383.409165547951
Iteration 6600: Loss = -11383.407662920372
Iteration 6700: Loss = -11383.404135987901
Iteration 6800: Loss = -11383.399722118991
Iteration 6900: Loss = -11383.396735360804
Iteration 7000: Loss = -11383.390353628154
Iteration 7100: Loss = -11383.391438726143
1
Iteration 7200: Loss = -11383.346779863215
Iteration 7300: Loss = -11383.345422163258
Iteration 7400: Loss = -11383.302249539587
Iteration 7500: Loss = -11383.30227354406
Iteration 7600: Loss = -11383.300604407837
Iteration 7700: Loss = -11383.300486436496
Iteration 7800: Loss = -11383.301639453022
1
Iteration 7900: Loss = -11383.300166193989
Iteration 8000: Loss = -11383.300790405554
1
Iteration 8100: Loss = -11383.300311418781
2
Iteration 8200: Loss = -11383.299679525437
Iteration 8300: Loss = -11383.298867725889
Iteration 8400: Loss = -11383.292837690542
Iteration 8500: Loss = -11383.292680817141
Iteration 8600: Loss = -11383.29488863435
1
Iteration 8700: Loss = -11383.292606112507
Iteration 8800: Loss = -11383.295258962418
1
Iteration 8900: Loss = -11383.299820096192
2
Iteration 9000: Loss = -11383.31197959554
3
Iteration 9100: Loss = -11383.309691499697
4
Iteration 9200: Loss = -11383.292523891992
Iteration 9300: Loss = -11383.292121863084
Iteration 9400: Loss = -11383.292622378254
1
Iteration 9500: Loss = -11383.298503885837
2
Iteration 9600: Loss = -11383.291900510434
Iteration 9700: Loss = -11383.293275166772
1
Iteration 9800: Loss = -11383.378073903594
2
Iteration 9900: Loss = -11383.291692063682
Iteration 10000: Loss = -11383.26672147491
Iteration 10100: Loss = -11383.473086490834
1
Iteration 10200: Loss = -11383.265745350745
Iteration 10300: Loss = -11383.381497102637
1
Iteration 10400: Loss = -11383.26561095683
Iteration 10500: Loss = -11383.604475950764
1
Iteration 10600: Loss = -11383.265513569524
Iteration 10700: Loss = -11383.265512896634
Iteration 10800: Loss = -11383.265513621598
Iteration 10900: Loss = -11383.265415491036
Iteration 11000: Loss = -11383.265893882464
1
Iteration 11100: Loss = -11383.265399852698
Iteration 11200: Loss = -11383.29634473963
1
Iteration 11300: Loss = -11383.265353915198
Iteration 11400: Loss = -11383.265388461612
Iteration 11500: Loss = -11383.265439102384
Iteration 11600: Loss = -11383.265365002775
Iteration 11700: Loss = -11383.270788347338
1
Iteration 11800: Loss = -11383.26592895414
2
Iteration 11900: Loss = -11383.265378812093
Iteration 12000: Loss = -11383.287384374918
1
Iteration 12100: Loss = -11383.265263868707
Iteration 12200: Loss = -11383.267688816291
1
Iteration 12300: Loss = -11383.533684001584
2
Iteration 12400: Loss = -11383.275901569588
3
Iteration 12500: Loss = -11383.249154929792
Iteration 12600: Loss = -11383.23765789814
Iteration 12700: Loss = -11383.238134288602
1
Iteration 12800: Loss = -11383.237628565868
Iteration 12900: Loss = -11383.237690645026
Iteration 13000: Loss = -11383.237378328855
Iteration 13100: Loss = -11383.238222957063
1
Iteration 13200: Loss = -11383.237350027073
Iteration 13300: Loss = -11383.262051523585
1
Iteration 13400: Loss = -11383.237355025374
Iteration 13500: Loss = -11383.260553458233
1
Iteration 13600: Loss = -11383.23734637349
Iteration 13700: Loss = -11383.237468800236
1
Iteration 13800: Loss = -11383.23744536859
Iteration 13900: Loss = -11383.328596437086
1
Iteration 14000: Loss = -11383.237325265283
Iteration 14100: Loss = -11383.264305504856
1
Iteration 14200: Loss = -11383.237315868239
Iteration 14300: Loss = -11383.29444536223
1
Iteration 14400: Loss = -11383.237331570841
Iteration 14500: Loss = -11383.250106892028
1
Iteration 14600: Loss = -11383.237337039
Iteration 14700: Loss = -11383.237671692545
1
Iteration 14800: Loss = -11383.418894411485
2
Iteration 14900: Loss = -11383.23733038255
Iteration 15000: Loss = -11383.29418154541
1
Iteration 15100: Loss = -11383.237310557373
Iteration 15200: Loss = -11383.24342706849
1
Iteration 15300: Loss = -11383.237404338932
Iteration 15400: Loss = -11383.23758975068
1
Iteration 15500: Loss = -11383.245261324222
2
Iteration 15600: Loss = -11383.236864744082
Iteration 15700: Loss = -11383.237135732248
1
Iteration 15800: Loss = -11383.236822126782
Iteration 15900: Loss = -11383.238563799934
1
Iteration 16000: Loss = -11383.242999044516
2
Iteration 16100: Loss = -11383.240262009846
3
Iteration 16200: Loss = -11383.236779679088
Iteration 16300: Loss = -11383.245154083825
1
Iteration 16400: Loss = -11383.23676847084
Iteration 16500: Loss = -11383.236883516076
1
Iteration 16600: Loss = -11383.236790359782
Iteration 16700: Loss = -11383.236864596916
Iteration 16800: Loss = -11383.236727557776
Iteration 16900: Loss = -11383.237607271829
1
Iteration 17000: Loss = -11383.238559766141
2
Iteration 17100: Loss = -11383.239192566967
3
Iteration 17200: Loss = -11383.236770314139
Iteration 17300: Loss = -11383.254979353887
1
Iteration 17400: Loss = -11383.2367465959
Iteration 17500: Loss = -11383.23689550806
1
Iteration 17600: Loss = -11383.399676095018
2
Iteration 17700: Loss = -11383.234035945985
Iteration 17800: Loss = -11383.225201779565
Iteration 17900: Loss = -11383.22161747062
Iteration 18000: Loss = -11383.220865236106
Iteration 18100: Loss = -11383.224894849476
1
Iteration 18200: Loss = -11383.22669404812
2
Iteration 18300: Loss = -11383.220858702318
Iteration 18400: Loss = -11383.239509591021
1
Iteration 18500: Loss = -11383.220833174426
Iteration 18600: Loss = -11383.221768571528
1
Iteration 18700: Loss = -11383.250923537042
2
Iteration 18800: Loss = -11383.220105670305
Iteration 18900: Loss = -11383.22011517206
Iteration 19000: Loss = -11383.39371646817
1
Iteration 19100: Loss = -11383.22010242985
Iteration 19200: Loss = -11383.283816052855
1
Iteration 19300: Loss = -11383.220051923032
Iteration 19400: Loss = -11383.22031207278
1
Iteration 19500: Loss = -11383.283745069448
2
Iteration 19600: Loss = -11383.219834822892
Iteration 19700: Loss = -11383.220376235171
1
Iteration 19800: Loss = -11383.221890332958
2
Iteration 19900: Loss = -11383.229910631333
3
pi: tensor([[0.6165, 0.3835],
        [0.2664, 0.7336]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9714, 0.0286], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1852, 0.1582],
         [0.5840, 0.3172]],

        [[0.5195, 0.0959],
         [0.5438, 0.6893]],

        [[0.7297, 0.1087],
         [0.6552, 0.6698]],

        [[0.7177, 0.0999],
         [0.5588, 0.7186]],

        [[0.6135, 0.1039],
         [0.6896, 0.6527]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.6392849046545613
Average Adjusted Rand Index: 0.7529671247329207
11344.415542328867
[-0.0010225991105388766, 0.6392849046545613] [-0.003252910041786854, 0.7529671247329207] [11664.656721339641, 11383.219794225406]
-------------------------------------
This iteration is 55
True Objective function: Loss = -11412.351068680035
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22591.70173565401
Iteration 100: Loss = -11718.593980793887
Iteration 200: Loss = -11709.334068633723
Iteration 300: Loss = -11688.336965440873
Iteration 400: Loss = -11587.370743885696
Iteration 500: Loss = -11491.77808412766
Iteration 600: Loss = -11449.288808423054
Iteration 700: Loss = -11435.441750872602
Iteration 800: Loss = -11432.182196008833
Iteration 900: Loss = -11423.089761187292
Iteration 1000: Loss = -11406.875889519213
Iteration 1100: Loss = -11399.251684586934
Iteration 1200: Loss = -11392.901252216241
Iteration 1300: Loss = -11383.527854535776
Iteration 1400: Loss = -11383.363113916948
Iteration 1500: Loss = -11383.272262421115
Iteration 1600: Loss = -11383.13798784762
Iteration 1700: Loss = -11383.071737361772
Iteration 1800: Loss = -11383.048162172949
Iteration 1900: Loss = -11383.030237655932
Iteration 2000: Loss = -11383.022464785041
Iteration 2100: Loss = -11383.00373731505
Iteration 2200: Loss = -11382.993157367235
Iteration 2300: Loss = -11383.002861819272
1
Iteration 2400: Loss = -11382.972547686428
Iteration 2500: Loss = -11382.963814732124
Iteration 2600: Loss = -11382.957949101889
Iteration 2700: Loss = -11382.953241426747
Iteration 2800: Loss = -11382.949182974016
Iteration 2900: Loss = -11382.945716251807
Iteration 3000: Loss = -11382.942659121118
Iteration 3100: Loss = -11382.939937314577
Iteration 3200: Loss = -11382.937469302638
Iteration 3300: Loss = -11382.935195291178
Iteration 3400: Loss = -11382.93342114524
Iteration 3500: Loss = -11382.93100498601
Iteration 3600: Loss = -11382.929365753971
Iteration 3700: Loss = -11382.924856027099
Iteration 3800: Loss = -11382.914182155939
Iteration 3900: Loss = -11382.913377031015
Iteration 4000: Loss = -11382.90971194701
Iteration 4100: Loss = -11382.909116203902
Iteration 4200: Loss = -11382.907408126111
Iteration 4300: Loss = -11382.910095745805
1
Iteration 4400: Loss = -11382.905444963202
Iteration 4500: Loss = -11382.904547833034
Iteration 4600: Loss = -11382.90353993247
Iteration 4700: Loss = -11382.903117941662
Iteration 4800: Loss = -11382.90500481407
1
Iteration 4900: Loss = -11382.900984788928
Iteration 5000: Loss = -11382.905441204566
1
Iteration 5100: Loss = -11382.899063160476
Iteration 5200: Loss = -11382.905881549466
1
Iteration 5300: Loss = -11382.89810147371
Iteration 5400: Loss = -11382.897673839634
Iteration 5500: Loss = -11382.913226602475
1
Iteration 5600: Loss = -11382.895946066736
Iteration 5700: Loss = -11382.89515208673
Iteration 5800: Loss = -11382.89478981519
Iteration 5900: Loss = -11382.894632745258
Iteration 6000: Loss = -11382.904612235516
1
Iteration 6100: Loss = -11382.893953838613
Iteration 6200: Loss = -11382.894416626597
1
Iteration 6300: Loss = -11382.894041410585
Iteration 6400: Loss = -11382.894451449882
1
Iteration 6500: Loss = -11382.893659449112
Iteration 6600: Loss = -11382.8928103032
Iteration 6700: Loss = -11382.892637590387
Iteration 6800: Loss = -11382.892545172628
Iteration 6900: Loss = -11382.893832223628
1
Iteration 7000: Loss = -11382.906492706472
2
Iteration 7100: Loss = -11382.93297093327
3
Iteration 7200: Loss = -11382.891968024918
Iteration 7300: Loss = -11382.906385356551
1
Iteration 7400: Loss = -11382.89179071185
Iteration 7500: Loss = -11382.907038157158
1
Iteration 7600: Loss = -11382.89162517476
Iteration 7700: Loss = -11383.07330235328
1
Iteration 7800: Loss = -11382.891412508296
Iteration 7900: Loss = -11382.89137512163
Iteration 8000: Loss = -11382.891329321112
Iteration 8100: Loss = -11382.891221805754
Iteration 8200: Loss = -11382.89369750696
1
Iteration 8300: Loss = -11382.891101332374
Iteration 8400: Loss = -11382.891064777605
Iteration 8500: Loss = -11382.891131134802
Iteration 8600: Loss = -11382.89090937489
Iteration 8700: Loss = -11382.902688693737
1
Iteration 8800: Loss = -11382.890882828126
Iteration 8900: Loss = -11382.899242035059
1
Iteration 9000: Loss = -11382.890764212785
Iteration 9100: Loss = -11382.890708134366
Iteration 9200: Loss = -11382.892307141516
1
Iteration 9300: Loss = -11382.890684108701
Iteration 9400: Loss = -11382.890620316346
Iteration 9500: Loss = -11382.894818322562
1
Iteration 9600: Loss = -11382.890579134944
Iteration 9700: Loss = -11382.890553713472
Iteration 9800: Loss = -11382.891138699275
1
Iteration 9900: Loss = -11382.89051753396
Iteration 10000: Loss = -11382.890460602819
Iteration 10100: Loss = -11382.891337121986
1
Iteration 10200: Loss = -11382.890427307606
Iteration 10300: Loss = -11382.890370320394
Iteration 10400: Loss = -11382.890507890772
1
Iteration 10500: Loss = -11382.890193344303
Iteration 10600: Loss = -11382.903700981802
1
Iteration 10700: Loss = -11382.889996724047
Iteration 10800: Loss = -11382.890015726502
Iteration 10900: Loss = -11382.889833453957
Iteration 11000: Loss = -11382.889895891112
Iteration 11100: Loss = -11382.8896357364
Iteration 11200: Loss = -11382.88890490431
Iteration 11300: Loss = -11382.88754528569
Iteration 11400: Loss = -11382.88742451012
Iteration 11500: Loss = -11382.89149257342
1
Iteration 11600: Loss = -11382.887391237993
Iteration 11700: Loss = -11382.887398088887
Iteration 11800: Loss = -11382.887441783594
Iteration 11900: Loss = -11382.88737742168
Iteration 12000: Loss = -11383.23954357607
1
Iteration 12100: Loss = -11382.887337692762
Iteration 12200: Loss = -11382.887314986454
Iteration 12300: Loss = -11382.889319793427
1
Iteration 12400: Loss = -11382.887334557307
Iteration 12500: Loss = -11382.887379200994
Iteration 12600: Loss = -11382.887398372955
Iteration 12700: Loss = -11382.988529734772
1
Iteration 12800: Loss = -11382.887305443835
Iteration 12900: Loss = -11382.904932669393
1
Iteration 13000: Loss = -11382.887320140877
Iteration 13100: Loss = -11382.887385317199
Iteration 13200: Loss = -11382.887376610719
Iteration 13300: Loss = -11382.8873044052
Iteration 13400: Loss = -11382.887382767318
Iteration 13500: Loss = -11382.897409319567
1
Iteration 13600: Loss = -11382.887259868552
Iteration 13700: Loss = -11382.955867187282
1
Iteration 13800: Loss = -11382.88724145009
Iteration 13900: Loss = -11382.90595497428
1
Iteration 14000: Loss = -11382.887199960513
Iteration 14100: Loss = -11382.888332320434
1
Iteration 14200: Loss = -11382.887201326836
Iteration 14300: Loss = -11382.887175508951
Iteration 14400: Loss = -11382.887232766006
Iteration 14500: Loss = -11382.886678037394
Iteration 14600: Loss = -11382.888391115202
1
Iteration 14700: Loss = -11382.886556336922
Iteration 14800: Loss = -11382.886599172316
Iteration 14900: Loss = -11382.886784396485
1
Iteration 15000: Loss = -11382.886549635225
Iteration 15100: Loss = -11383.000045614954
1
Iteration 15200: Loss = -11382.886559215602
Iteration 15300: Loss = -11382.88653561145
Iteration 15400: Loss = -11382.89497332461
1
Iteration 15500: Loss = -11382.885889457997
Iteration 15600: Loss = -11382.885733946177
Iteration 15700: Loss = -11382.88581303541
Iteration 15800: Loss = -11382.886053756723
1
Iteration 15900: Loss = -11382.885723546133
Iteration 16000: Loss = -11382.88573497386
Iteration 16100: Loss = -11382.886366211716
1
Iteration 16200: Loss = -11382.885725055357
Iteration 16300: Loss = -11382.96073008306
1
Iteration 16400: Loss = -11382.885687976603
Iteration 16500: Loss = -11382.966098812172
1
Iteration 16600: Loss = -11382.885720061598
Iteration 16700: Loss = -11382.887549904635
1
Iteration 16800: Loss = -11382.885707856316
Iteration 16900: Loss = -11382.885657810066
Iteration 17000: Loss = -11382.886605491924
1
Iteration 17100: Loss = -11382.885672734072
Iteration 17200: Loss = -11382.900010308047
1
Iteration 17300: Loss = -11382.885668714365
Iteration 17400: Loss = -11382.885846634172
1
Iteration 17500: Loss = -11382.896132594835
2
Iteration 17600: Loss = -11382.885666078273
Iteration 17700: Loss = -11382.885805070242
1
Iteration 17800: Loss = -11382.885671076907
Iteration 17900: Loss = -11382.88571922019
Iteration 18000: Loss = -11382.885679277064
Iteration 18100: Loss = -11382.885819037972
1
Iteration 18200: Loss = -11383.233852066714
2
Iteration 18300: Loss = -11382.885689352423
Iteration 18400: Loss = -11382.885702806672
Iteration 18500: Loss = -11382.885623197928
Iteration 18600: Loss = -11382.885565622308
Iteration 18700: Loss = -11382.886183905133
1
Iteration 18800: Loss = -11382.885570995182
Iteration 18900: Loss = -11382.88955946506
1
Iteration 19000: Loss = -11382.885556449637
Iteration 19100: Loss = -11382.906782701382
1
Iteration 19200: Loss = -11382.885585695718
Iteration 19300: Loss = -11382.885574212112
Iteration 19400: Loss = -11382.886518569989
1
Iteration 19500: Loss = -11382.885573154408
Iteration 19600: Loss = -11382.892776435643
1
Iteration 19700: Loss = -11382.885586908453
Iteration 19800: Loss = -11382.887691076226
1
Iteration 19900: Loss = -11382.885567834202
pi: tensor([[0.6833, 0.3167],
        [0.2571, 0.7429]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4963, 0.5037], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1835, 0.1153],
         [0.7181, 0.3098]],

        [[0.6291, 0.0990],
         [0.5208, 0.5757]],

        [[0.6563, 0.1090],
         [0.6666, 0.6592]],

        [[0.6376, 0.1042],
         [0.7212, 0.7181]],

        [[0.7075, 0.1008],
         [0.6369, 0.6363]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448427857772554
time is 1
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.9214398261647874
Average Adjusted Rand Index: 0.9216130522588901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20000.901176996496
Iteration 100: Loss = -11717.618650529632
Iteration 200: Loss = -11711.566074654436
Iteration 300: Loss = -11701.336873264787
Iteration 400: Loss = -11679.248034290114
Iteration 500: Loss = -11540.119749605203
Iteration 600: Loss = -11496.504124759265
Iteration 700: Loss = -11489.589770897019
Iteration 800: Loss = -11489.195042407991
Iteration 900: Loss = -11482.73635939012
Iteration 1000: Loss = -11482.560574270698
Iteration 1100: Loss = -11482.50865154989
Iteration 1200: Loss = -11482.468461454651
Iteration 1300: Loss = -11482.430433186018
Iteration 1400: Loss = -11482.284197139988
Iteration 1500: Loss = -11479.077872631533
Iteration 1600: Loss = -11478.574487059637
Iteration 1700: Loss = -11478.554276135477
Iteration 1800: Loss = -11474.26593829524
Iteration 1900: Loss = -11474.09768882432
Iteration 2000: Loss = -11471.63986447214
Iteration 2100: Loss = -11471.542857472221
Iteration 2200: Loss = -11471.502315928019
Iteration 2300: Loss = -11469.595592626234
Iteration 2400: Loss = -11469.234875257946
Iteration 2500: Loss = -11469.225802329927
Iteration 2600: Loss = -11469.2159008316
Iteration 2700: Loss = -11469.067913980565
Iteration 2800: Loss = -11469.064119087248
Iteration 2900: Loss = -11468.910977242958
Iteration 3000: Loss = -11468.347445368317
Iteration 3100: Loss = -11468.308491972035
Iteration 3200: Loss = -11468.30095219461
Iteration 3300: Loss = -11468.29657207206
Iteration 3400: Loss = -11468.29534168621
Iteration 3500: Loss = -11468.29432026175
Iteration 3600: Loss = -11468.293507571652
Iteration 3700: Loss = -11468.29260014648
Iteration 3800: Loss = -11468.292060323985
Iteration 3900: Loss = -11468.294916622444
1
Iteration 4000: Loss = -11468.29439087381
2
Iteration 4100: Loss = -11468.290206007423
Iteration 4200: Loss = -11468.30000944106
1
Iteration 4300: Loss = -11468.288130381061
Iteration 4400: Loss = -11466.43163051246
Iteration 4500: Loss = -11466.432315458731
1
Iteration 4600: Loss = -11466.344630731874
Iteration 4700: Loss = -11466.343707282376
Iteration 4800: Loss = -11466.343103367824
Iteration 4900: Loss = -11466.343065199968
Iteration 5000: Loss = -11466.342472622817
Iteration 5100: Loss = -11466.34233619156
Iteration 5200: Loss = -11466.341812648254
Iteration 5300: Loss = -11466.341452114
Iteration 5400: Loss = -11466.341700547073
1
Iteration 5500: Loss = -11466.34308072451
2
Iteration 5600: Loss = -11466.342354852874
3
Iteration 5700: Loss = -11466.33791852191
Iteration 5800: Loss = -11466.336928478459
Iteration 5900: Loss = -11466.336990983482
Iteration 6000: Loss = -11466.33649602164
Iteration 6100: Loss = -11466.33723164061
1
Iteration 6200: Loss = -11466.336118725658
Iteration 6300: Loss = -11466.335640105637
Iteration 6400: Loss = -11466.324384543796
Iteration 6500: Loss = -11466.322787568972
Iteration 6600: Loss = -11466.322626954365
Iteration 6700: Loss = -11466.322301574932
Iteration 6800: Loss = -11466.323947373614
1
Iteration 6900: Loss = -11466.321928374713
Iteration 7000: Loss = -11466.321528858347
Iteration 7100: Loss = -11466.320153021006
Iteration 7200: Loss = -11466.319200756647
Iteration 7300: Loss = -11466.318935544405
Iteration 7400: Loss = -11466.32782389858
1
Iteration 7500: Loss = -11466.312441974296
Iteration 7600: Loss = -11466.311635460372
Iteration 7700: Loss = -11466.31166613801
Iteration 7800: Loss = -11466.311054526295
Iteration 7900: Loss = -11466.570893724325
1
Iteration 8000: Loss = -11466.310741520567
Iteration 8100: Loss = -11466.310568438557
Iteration 8200: Loss = -11466.313980643517
1
Iteration 8300: Loss = -11466.310321737157
Iteration 8400: Loss = -11466.31027003059
Iteration 8500: Loss = -11466.31022959805
Iteration 8600: Loss = -11466.310156096615
Iteration 8700: Loss = -11466.313245445674
1
Iteration 8800: Loss = -11466.310086914993
Iteration 8900: Loss = -11466.543715509511
1
Iteration 9000: Loss = -11466.3100219416
Iteration 9100: Loss = -11466.309957705846
Iteration 9200: Loss = -11466.310087378535
1
Iteration 9300: Loss = -11466.309917268698
Iteration 9400: Loss = -11466.310407711198
1
Iteration 9500: Loss = -11466.30999284075
Iteration 9600: Loss = -11466.309678088995
Iteration 9700: Loss = -11466.312231201642
1
Iteration 9800: Loss = -11466.305813553
Iteration 9900: Loss = -11466.319374637766
1
Iteration 10000: Loss = -11466.304254905763
Iteration 10100: Loss = -11466.392219105868
1
Iteration 10200: Loss = -11466.30406396749
Iteration 10300: Loss = -11466.304012530158
Iteration 10400: Loss = -11466.340172504972
1
Iteration 10500: Loss = -11466.303481999374
Iteration 10600: Loss = -11466.30306518685
Iteration 10700: Loss = -11466.320552696485
1
Iteration 10800: Loss = -11466.30301951798
Iteration 10900: Loss = -11466.302994748376
Iteration 11000: Loss = -11466.305716077894
1
Iteration 11100: Loss = -11466.302973969217
Iteration 11200: Loss = -11466.302983512227
Iteration 11300: Loss = -11466.303060763985
Iteration 11400: Loss = -11466.303874187543
1
Iteration 11500: Loss = -11466.400494101583
2
Iteration 11600: Loss = -11466.303127873876
Iteration 11700: Loss = -11466.342886546909
1
Iteration 11800: Loss = -11466.304460523166
2
Iteration 11900: Loss = -11466.302651612825
Iteration 12000: Loss = -11466.30711490728
1
Iteration 12100: Loss = -11466.30258772176
Iteration 12200: Loss = -11466.320369006518
1
Iteration 12300: Loss = -11466.30259326357
Iteration 12400: Loss = -11466.330002453074
1
Iteration 12500: Loss = -11466.302561723993
Iteration 12600: Loss = -11466.342798498055
1
Iteration 12700: Loss = -11466.309096940367
2
Iteration 12800: Loss = -11466.302523612661
Iteration 12900: Loss = -11466.302507798391
Iteration 13000: Loss = -11466.302467242982
Iteration 13100: Loss = -11466.302745678728
1
Iteration 13200: Loss = -11466.303399464836
2
Iteration 13300: Loss = -11466.302467006111
Iteration 13400: Loss = -11466.302764980308
1
Iteration 13500: Loss = -11466.302442557037
Iteration 13600: Loss = -11466.302837557256
1
Iteration 13700: Loss = -11466.302428824261
Iteration 13800: Loss = -11466.303220960583
1
Iteration 13900: Loss = -11466.302418835316
Iteration 14000: Loss = -11466.311620585671
1
Iteration 14100: Loss = -11466.302273757823
Iteration 14200: Loss = -11466.40159866292
1
Iteration 14300: Loss = -11466.30228032228
Iteration 14400: Loss = -11466.624778375412
1
Iteration 14500: Loss = -11466.30222874155
Iteration 14600: Loss = -11466.302428778588
1
Iteration 14700: Loss = -11466.302775953194
2
Iteration 14800: Loss = -11466.303022290946
3
Iteration 14900: Loss = -11466.30422146167
4
Iteration 15000: Loss = -11466.352094855247
5
Iteration 15100: Loss = -11466.302240047135
Iteration 15200: Loss = -11466.372546579772
1
Iteration 15300: Loss = -11466.30224229196
Iteration 15400: Loss = -11466.307234801492
1
Iteration 15500: Loss = -11466.302305417732
Iteration 15600: Loss = -11466.508228735018
1
Iteration 15700: Loss = -11466.302513790562
2
Iteration 15800: Loss = -11466.598746798167
3
Iteration 15900: Loss = -11466.302213205769
Iteration 16000: Loss = -11466.309934272838
1
Iteration 16100: Loss = -11466.302168849266
Iteration 16200: Loss = -11466.30452391398
1
Iteration 16300: Loss = -11466.302154645482
Iteration 16400: Loss = -11466.302823792616
1
Iteration 16500: Loss = -11466.30218234298
Iteration 16600: Loss = -11466.303724894753
1
Iteration 16700: Loss = -11466.30216896744
Iteration 16800: Loss = -11466.307184487841
1
Iteration 16900: Loss = -11466.302166173644
Iteration 17000: Loss = -11466.302133249643
Iteration 17100: Loss = -11466.303023794437
1
Iteration 17200: Loss = -11466.302184624039
Iteration 17300: Loss = -11466.303440891505
1
Iteration 17400: Loss = -11466.30219003915
Iteration 17500: Loss = -11466.35421133734
1
Iteration 17600: Loss = -11466.30289119054
2
Iteration 17700: Loss = -11466.302271438935
Iteration 17800: Loss = -11466.356570187609
1
Iteration 17900: Loss = -11466.302162769596
Iteration 18000: Loss = -11466.325351703737
1
Iteration 18100: Loss = -11466.302172722375
Iteration 18200: Loss = -11466.808430761906
1
Iteration 18300: Loss = -11466.302144522198
Iteration 18400: Loss = -11466.300088308564
Iteration 18500: Loss = -11466.30334775991
1
Iteration 18600: Loss = -11466.300072227132
Iteration 18700: Loss = -11466.35268963578
1
Iteration 18800: Loss = -11466.300740928596
2
Iteration 18900: Loss = -11466.30009005647
Iteration 19000: Loss = -11466.300173378735
Iteration 19100: Loss = -11466.30008635555
Iteration 19200: Loss = -11466.297225001837
Iteration 19300: Loss = -11466.296869155682
Iteration 19400: Loss = -11466.297281227688
1
Iteration 19500: Loss = -11466.296587380612
Iteration 19600: Loss = -11466.320952167898
1
Iteration 19700: Loss = -11466.296613049453
Iteration 19800: Loss = -11466.296664081572
Iteration 19900: Loss = -11466.296668854216
pi: tensor([[0.5455, 0.4545],
        [0.4547, 0.5453]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4740, 0.5260], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2018, 0.1129],
         [0.6687, 0.3030]],

        [[0.5673, 0.0963],
         [0.6308, 0.6019]],

        [[0.6023, 0.1086],
         [0.5035, 0.6221]],

        [[0.5975, 0.1028],
         [0.5789, 0.6278]],

        [[0.5273, 0.0995],
         [0.6546, 0.5708]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080762963757459
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 20
Adjusted Rand Index: 0.35436537050623623
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
Global Adjusted Rand Index: 0.3881617494274458
Average Adjusted Rand Index: 0.7624179028048333
11412.351068680035
[0.9214398261647874, 0.3881617494274458] [0.9216130522588901, 0.7624179028048333] [11382.915217881784, 11466.29711888121]
-------------------------------------
This iteration is 56
True Objective function: Loss = -11120.939812724544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21877.39124644883
Iteration 100: Loss = -11354.402761120828
Iteration 200: Loss = -11353.784358866113
Iteration 300: Loss = -11353.563079763151
Iteration 400: Loss = -11352.835966574909
Iteration 500: Loss = -11348.73097800048
Iteration 600: Loss = -11343.904565569588
Iteration 700: Loss = -11205.996428972112
Iteration 800: Loss = -11167.107341377143
Iteration 900: Loss = -11166.450858634264
Iteration 1000: Loss = -11166.118489615475
Iteration 1100: Loss = -11165.975025531123
Iteration 1200: Loss = -11165.857372885215
Iteration 1300: Loss = -11165.782982633655
Iteration 1400: Loss = -11165.670487013682
Iteration 1500: Loss = -11163.788979982526
Iteration 1600: Loss = -11162.904802615149
Iteration 1700: Loss = -11162.815498421687
Iteration 1800: Loss = -11162.743404495317
Iteration 1900: Loss = -11162.346491692582
Iteration 2000: Loss = -11162.319686270785
Iteration 2100: Loss = -11162.30098436981
Iteration 2200: Loss = -11162.286690553436
Iteration 2300: Loss = -11162.27540412229
Iteration 2400: Loss = -11162.267270865994
Iteration 2500: Loss = -11162.258612771835
Iteration 2600: Loss = -11162.252240176913
Iteration 2700: Loss = -11162.24672291812
Iteration 2800: Loss = -11162.242010259488
Iteration 2900: Loss = -11162.237904119098
Iteration 3000: Loss = -11162.234265490708
Iteration 3100: Loss = -11162.231244389166
Iteration 3200: Loss = -11162.227628975425
Iteration 3300: Loss = -11162.22359200206
Iteration 3400: Loss = -11162.219699330395
Iteration 3500: Loss = -11162.21732906678
Iteration 3600: Loss = -11162.215404647994
Iteration 3700: Loss = -11162.214112764115
Iteration 3800: Loss = -11162.212918816638
Iteration 3900: Loss = -11162.211122301342
Iteration 4000: Loss = -11162.20993450479
Iteration 4100: Loss = -11162.2148389179
1
Iteration 4200: Loss = -11162.207487858346
Iteration 4300: Loss = -11162.206335283636
Iteration 4400: Loss = -11162.205067631914
Iteration 4500: Loss = -11162.203214730636
Iteration 4600: Loss = -11162.200428091472
Iteration 4700: Loss = -11162.198646998944
Iteration 4800: Loss = -11162.197898514569
Iteration 4900: Loss = -11162.19742603441
Iteration 5000: Loss = -11162.196617593763
Iteration 5100: Loss = -11162.196415349787
Iteration 5200: Loss = -11162.19546553485
Iteration 5300: Loss = -11162.19455568863
Iteration 5400: Loss = -11162.193782114235
Iteration 5500: Loss = -11162.193338698386
Iteration 5600: Loss = -11162.19297242338
Iteration 5700: Loss = -11162.192709668054
Iteration 5800: Loss = -11162.19246774237
Iteration 5900: Loss = -11162.192330888909
Iteration 6000: Loss = -11162.192025544548
Iteration 6100: Loss = -11162.195325649722
1
Iteration 6200: Loss = -11162.191660559174
Iteration 6300: Loss = -11162.191532509283
Iteration 6400: Loss = -11162.191390371527
Iteration 6500: Loss = -11162.191204442626
Iteration 6600: Loss = -11162.194261941957
1
Iteration 6700: Loss = -11162.19097320339
Iteration 6800: Loss = -11162.190850987025
Iteration 6900: Loss = -11162.19075096241
Iteration 7000: Loss = -11162.19065646876
Iteration 7100: Loss = -11162.190937391715
1
Iteration 7200: Loss = -11162.190424204227
Iteration 7300: Loss = -11162.19036166352
Iteration 7400: Loss = -11162.190289361006
Iteration 7500: Loss = -11162.190155278797
Iteration 7600: Loss = -11162.19024098981
Iteration 7700: Loss = -11162.1899767183
Iteration 7800: Loss = -11162.23264064074
1
Iteration 7900: Loss = -11162.189879775198
Iteration 8000: Loss = -11162.18983491444
Iteration 8100: Loss = -11162.190393459678
1
Iteration 8200: Loss = -11162.189728301886
Iteration 8300: Loss = -11162.189663966341
Iteration 8400: Loss = -11162.189637449184
Iteration 8500: Loss = -11162.189717038726
Iteration 8600: Loss = -11162.189800558386
Iteration 8700: Loss = -11162.189631358931
Iteration 8800: Loss = -11162.189512636874
Iteration 8900: Loss = -11162.190528159157
1
Iteration 9000: Loss = -11162.195550400997
2
Iteration 9100: Loss = -11162.20902662721
3
Iteration 9200: Loss = -11162.190426315678
4
Iteration 9300: Loss = -11162.190033064397
5
Iteration 9400: Loss = -11162.189459283998
Iteration 9500: Loss = -11162.189847021462
1
Iteration 9600: Loss = -11162.390143338262
2
Iteration 9700: Loss = -11162.189281881574
Iteration 9800: Loss = -11162.189304796093
Iteration 9900: Loss = -11162.189527863467
1
Iteration 10000: Loss = -11162.190171630295
2
Iteration 10100: Loss = -11162.210256908918
3
Iteration 10200: Loss = -11162.189142303014
Iteration 10300: Loss = -11162.19263765512
1
Iteration 10400: Loss = -11162.216773800808
2
Iteration 10500: Loss = -11162.19039050844
3
Iteration 10600: Loss = -11162.212401007326
4
Iteration 10700: Loss = -11162.189598184152
5
Iteration 10800: Loss = -11162.189254325873
6
Iteration 10900: Loss = -11162.190048827168
7
Iteration 11000: Loss = -11162.18910915641
Iteration 11100: Loss = -11162.265126848932
1
Iteration 11200: Loss = -11162.189088433352
Iteration 11300: Loss = -11162.189459519837
1
Iteration 11400: Loss = -11162.18907584852
Iteration 11500: Loss = -11162.189041134194
Iteration 11600: Loss = -11162.189043205974
Iteration 11700: Loss = -11162.202372529578
1
Iteration 11800: Loss = -11162.193846405105
2
Iteration 11900: Loss = -11162.189039864637
Iteration 12000: Loss = -11162.189821030384
1
Iteration 12100: Loss = -11162.189163422792
2
Iteration 12200: Loss = -11162.188984998553
Iteration 12300: Loss = -11162.26301016936
1
Iteration 12400: Loss = -11162.189064335545
Iteration 12500: Loss = -11162.189046411862
Iteration 12600: Loss = -11162.220163087126
1
Iteration 12700: Loss = -11162.38727497514
2
Iteration 12800: Loss = -11162.188961623566
Iteration 12900: Loss = -11162.189144297388
1
Iteration 13000: Loss = -11162.188932821124
Iteration 13100: Loss = -11162.190668103845
1
Iteration 13200: Loss = -11162.189243916213
2
Iteration 13300: Loss = -11162.189436842986
3
Iteration 13400: Loss = -11162.189544626033
4
Iteration 13500: Loss = -11162.198131961313
5
Iteration 13600: Loss = -11162.200889671534
6
Iteration 13700: Loss = -11162.188993375143
Iteration 13800: Loss = -11162.189106350785
1
Iteration 13900: Loss = -11162.20346287524
2
Iteration 14000: Loss = -11162.214312397771
3
Iteration 14100: Loss = -11162.18892894134
Iteration 14200: Loss = -11162.189613684253
1
Iteration 14300: Loss = -11162.18887599213
Iteration 14400: Loss = -11162.189007503264
1
Iteration 14500: Loss = -11162.188877798098
Iteration 14600: Loss = -11162.189475426832
1
Iteration 14700: Loss = -11162.188852252919
Iteration 14800: Loss = -11162.191722929796
1
Iteration 14900: Loss = -11162.18886950659
Iteration 15000: Loss = -11162.356969340495
1
Iteration 15100: Loss = -11162.188859048325
Iteration 15200: Loss = -11162.190221653138
1
Iteration 15300: Loss = -11162.188956623515
Iteration 15400: Loss = -11162.198769636
1
Iteration 15500: Loss = -11162.218122532226
2
Iteration 15600: Loss = -11162.18887294563
Iteration 15700: Loss = -11162.19123703829
1
Iteration 15800: Loss = -11162.188866661081
Iteration 15900: Loss = -11162.193248162148
1
Iteration 16000: Loss = -11162.190448623232
2
Iteration 16100: Loss = -11162.190709920316
3
Iteration 16200: Loss = -11162.19403253726
4
Iteration 16300: Loss = -11162.189393090639
5
Iteration 16400: Loss = -11162.18893323979
Iteration 16500: Loss = -11162.19739627332
1
Iteration 16600: Loss = -11162.188868636458
Iteration 16700: Loss = -11162.189040943755
1
Iteration 16800: Loss = -11162.188976485182
2
Iteration 16900: Loss = -11162.245967336667
3
Iteration 17000: Loss = -11162.188853443551
Iteration 17100: Loss = -11162.231657655213
1
Iteration 17200: Loss = -11162.1888706784
Iteration 17300: Loss = -11162.188873353669
Iteration 17400: Loss = -11162.188872261406
Iteration 17500: Loss = -11162.208927319376
1
Iteration 17600: Loss = -11162.19998469115
2
Iteration 17700: Loss = -11162.18899632403
3
Iteration 17800: Loss = -11162.188963083749
Iteration 17900: Loss = -11162.195465696926
1
Iteration 18000: Loss = -11162.189178869967
2
Iteration 18100: Loss = -11162.188919781574
Iteration 18200: Loss = -11162.189162399549
1
Iteration 18300: Loss = -11162.213844893175
2
Iteration 18400: Loss = -11162.192147428728
3
Iteration 18500: Loss = -11162.239498595807
4
Iteration 18600: Loss = -11162.188858447533
Iteration 18700: Loss = -11162.188884065303
Iteration 18800: Loss = -11162.214577672245
1
Iteration 18900: Loss = -11162.18890496076
Iteration 19000: Loss = -11162.188964100767
Iteration 19100: Loss = -11162.284583667213
1
Iteration 19200: Loss = -11162.18886098862
Iteration 19300: Loss = -11162.18977263804
1
Iteration 19400: Loss = -11162.189172006903
2
Iteration 19500: Loss = -11162.190275791472
3
Iteration 19600: Loss = -11162.188853070182
Iteration 19700: Loss = -11162.188902411684
Iteration 19800: Loss = -11162.189125244848
1
Iteration 19900: Loss = -11162.189223748688
2
pi: tensor([[0.6884, 0.3116],
        [0.3485, 0.6515]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0178, 0.9822], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3064, 0.0632],
         [0.5795, 0.1838]],

        [[0.5040, 0.1092],
         [0.6129, 0.6192]],

        [[0.6905, 0.1045],
         [0.5157, 0.7263]],

        [[0.6765, 0.1023],
         [0.5556, 0.7163]],

        [[0.6008, 0.0971],
         [0.7169, 0.6071]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824124176797128
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.6076078643855685
Average Adjusted Rand Index: 0.7362101341970987
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22425.85769044788
Iteration 100: Loss = -11353.928778425683
Iteration 200: Loss = -11352.45010288412
Iteration 300: Loss = -11349.915601866438
Iteration 400: Loss = -11343.490799680654
Iteration 500: Loss = -11227.060187633582
Iteration 600: Loss = -11170.011543066237
Iteration 700: Loss = -11166.539551042208
Iteration 800: Loss = -11166.138742858753
Iteration 900: Loss = -11165.691499723638
Iteration 1000: Loss = -11165.606585713589
Iteration 1100: Loss = -11165.552097804815
Iteration 1200: Loss = -11165.514254858203
Iteration 1300: Loss = -11165.486014804705
Iteration 1400: Loss = -11165.463478693786
Iteration 1500: Loss = -11165.440674251318
Iteration 1600: Loss = -11165.396962965442
Iteration 1700: Loss = -11165.383917401708
Iteration 1800: Loss = -11165.372885218967
Iteration 1900: Loss = -11165.360994426674
Iteration 2000: Loss = -11165.30750495191
Iteration 2100: Loss = -11165.293328820262
Iteration 2200: Loss = -11165.28103181801
Iteration 2300: Loss = -11165.257253947842
Iteration 2400: Loss = -11164.980469764336
Iteration 2500: Loss = -11162.727558676677
Iteration 2600: Loss = -11162.56665655091
Iteration 2700: Loss = -11162.483686360083
Iteration 2800: Loss = -11162.445356881137
Iteration 2900: Loss = -11162.378672035611
Iteration 3000: Loss = -11162.252407821628
Iteration 3100: Loss = -11162.22553639017
Iteration 3200: Loss = -11162.217357201505
Iteration 3300: Loss = -11162.213391956007
Iteration 3400: Loss = -11162.209254921858
Iteration 3500: Loss = -11162.206839166132
Iteration 3600: Loss = -11162.205209506441
Iteration 3700: Loss = -11162.204173525612
Iteration 3800: Loss = -11162.202297541991
Iteration 3900: Loss = -11162.201520684881
Iteration 4000: Loss = -11162.200259079078
Iteration 4100: Loss = -11162.204995752872
1
Iteration 4200: Loss = -11162.198673416837
Iteration 4300: Loss = -11162.19806797358
Iteration 4400: Loss = -11162.1974057011
Iteration 4500: Loss = -11162.196863192821
Iteration 4600: Loss = -11162.198145736682
1
Iteration 4700: Loss = -11162.195916611112
Iteration 4800: Loss = -11162.204760366076
1
Iteration 4900: Loss = -11162.195124073009
Iteration 5000: Loss = -11162.194947272483
Iteration 5100: Loss = -11162.194821579638
Iteration 5200: Loss = -11162.194162288415
Iteration 5300: Loss = -11162.193887312005
Iteration 5400: Loss = -11162.193597382216
Iteration 5500: Loss = -11162.193359864124
Iteration 5600: Loss = -11162.193141888518
Iteration 5700: Loss = -11162.193012516822
Iteration 5800: Loss = -11162.19322821134
1
Iteration 5900: Loss = -11162.192550520287
Iteration 6000: Loss = -11162.192628927884
Iteration 6100: Loss = -11162.192208032508
Iteration 6200: Loss = -11162.192061404729
Iteration 6300: Loss = -11162.209438980999
1
Iteration 6400: Loss = -11162.19182361927
Iteration 6500: Loss = -11162.191891070168
Iteration 6600: Loss = -11162.191566730287
Iteration 6700: Loss = -11162.191734769716
1
Iteration 6800: Loss = -11162.191387063553
Iteration 6900: Loss = -11162.191698433238
1
Iteration 7000: Loss = -11162.191209607805
Iteration 7100: Loss = -11162.191285822319
Iteration 7200: Loss = -11162.191045665211
Iteration 7300: Loss = -11162.191011549876
Iteration 7400: Loss = -11162.190864541715
Iteration 7500: Loss = -11162.19093222847
Iteration 7600: Loss = -11162.190743518455
Iteration 7700: Loss = -11162.19074670588
Iteration 7800: Loss = -11162.191291057383
1
Iteration 7900: Loss = -11162.201851991982
2
Iteration 8000: Loss = -11162.190449597638
Iteration 8100: Loss = -11162.190941029938
1
Iteration 8200: Loss = -11162.190417937165
Iteration 8300: Loss = -11162.228807661222
1
Iteration 8400: Loss = -11162.190334387275
Iteration 8500: Loss = -11162.209221246663
1
Iteration 8600: Loss = -11162.190258277225
Iteration 8700: Loss = -11162.190262779153
Iteration 8800: Loss = -11162.190216761279
Iteration 8900: Loss = -11162.190211194657
Iteration 9000: Loss = -11162.204014429106
1
Iteration 9100: Loss = -11162.190134852783
Iteration 9200: Loss = -11162.190403016117
1
Iteration 9300: Loss = -11162.190141349281
Iteration 9400: Loss = -11162.190072472298
Iteration 9500: Loss = -11162.190070628418
Iteration 9600: Loss = -11162.19008497835
Iteration 9700: Loss = -11162.190008445226
Iteration 9800: Loss = -11162.190541767894
1
Iteration 9900: Loss = -11162.18999880022
Iteration 10000: Loss = -11162.193688315345
1
Iteration 10100: Loss = -11162.190206986716
2
Iteration 10200: Loss = -11162.21810567591
3
Iteration 10300: Loss = -11162.189950849675
Iteration 10400: Loss = -11162.206310148786
1
Iteration 10500: Loss = -11162.18993488661
Iteration 10600: Loss = -11162.19051055129
1
Iteration 10700: Loss = -11162.190154254173
2
Iteration 10800: Loss = -11162.190468782106
3
Iteration 10900: Loss = -11162.19061184407
4
Iteration 11000: Loss = -11162.3281320952
5
Iteration 11100: Loss = -11162.189865135184
Iteration 11200: Loss = -11162.199273517226
1
Iteration 11300: Loss = -11162.195235569987
2
Iteration 11400: Loss = -11162.199303328747
3
Iteration 11500: Loss = -11162.288017261919
4
Iteration 11600: Loss = -11162.190008948506
5
Iteration 11700: Loss = -11162.190023749787
6
Iteration 11800: Loss = -11162.189886371838
Iteration 11900: Loss = -11162.189374261772
Iteration 12000: Loss = -11162.200663063226
1
Iteration 12100: Loss = -11162.194802078739
2
Iteration 12200: Loss = -11162.249863089875
3
Iteration 12300: Loss = -11162.189062873855
Iteration 12400: Loss = -11162.189952831017
1
Iteration 12500: Loss = -11162.206564270222
2
Iteration 12600: Loss = -11162.204081294583
3
Iteration 12700: Loss = -11162.191072427748
4
Iteration 12800: Loss = -11162.189061100902
Iteration 12900: Loss = -11162.18974939496
1
Iteration 13000: Loss = -11162.189715870565
2
Iteration 13100: Loss = -11162.189475473448
3
Iteration 13200: Loss = -11162.208064802424
4
Iteration 13300: Loss = -11162.326784660228
5
Iteration 13400: Loss = -11162.189068451205
Iteration 13500: Loss = -11162.222441214033
1
Iteration 13600: Loss = -11162.189047093269
Iteration 13700: Loss = -11162.189035366218
Iteration 13800: Loss = -11162.18904828226
Iteration 13900: Loss = -11162.190181724556
1
Iteration 14000: Loss = -11162.214236355763
2
Iteration 14100: Loss = -11162.188980559977
Iteration 14200: Loss = -11162.189010191805
Iteration 14300: Loss = -11162.189106539505
Iteration 14400: Loss = -11162.20819543065
1
Iteration 14500: Loss = -11162.19719813623
2
Iteration 14600: Loss = -11162.19851477886
3
Iteration 14700: Loss = -11162.189186814017
Iteration 14800: Loss = -11162.19608304351
1
Iteration 14900: Loss = -11162.194372501672
2
Iteration 15000: Loss = -11162.188967067126
Iteration 15100: Loss = -11162.201328437037
1
Iteration 15200: Loss = -11162.197432394087
2
Iteration 15300: Loss = -11162.188964123283
Iteration 15400: Loss = -11162.189149653632
1
Iteration 15500: Loss = -11162.273741102543
2
Iteration 15600: Loss = -11162.189006793014
Iteration 15700: Loss = -11162.206138823958
1
Iteration 15800: Loss = -11162.188973218756
Iteration 15900: Loss = -11162.228685436312
1
Iteration 16000: Loss = -11162.189070430572
Iteration 16100: Loss = -11162.189157878367
Iteration 16200: Loss = -11162.189077464638
Iteration 16300: Loss = -11162.190198996246
1
Iteration 16400: Loss = -11162.198504525764
2
Iteration 16500: Loss = -11162.192803390724
3
Iteration 16600: Loss = -11162.189030811509
Iteration 16700: Loss = -11162.189276624451
1
Iteration 16800: Loss = -11162.199034488256
2
Iteration 16900: Loss = -11162.18898272379
Iteration 17000: Loss = -11162.191955626973
1
Iteration 17100: Loss = -11162.189165206433
2
Iteration 17200: Loss = -11162.191464370328
3
Iteration 17300: Loss = -11162.189556593838
4
Iteration 17400: Loss = -11162.191181755665
5
Iteration 17500: Loss = -11162.194718628069
6
Iteration 17600: Loss = -11162.188897910843
Iteration 17700: Loss = -11162.189743780396
1
Iteration 17800: Loss = -11162.188884180821
Iteration 17900: Loss = -11162.190850861796
1
Iteration 18000: Loss = -11162.188874824677
Iteration 18100: Loss = -11162.238281096814
1
Iteration 18200: Loss = -11162.188867543078
Iteration 18300: Loss = -11162.189057379734
1
Iteration 18400: Loss = -11162.188873645751
Iteration 18500: Loss = -11162.192593364372
1
Iteration 18600: Loss = -11162.18885121725
Iteration 18700: Loss = -11162.189618228344
1
Iteration 18800: Loss = -11162.188869524542
Iteration 18900: Loss = -11162.2033478757
1
Iteration 19000: Loss = -11162.188870342721
Iteration 19100: Loss = -11162.190322489128
1
Iteration 19200: Loss = -11162.189205608487
2
Iteration 19300: Loss = -11162.414408563403
3
Iteration 19400: Loss = -11162.188837370015
Iteration 19500: Loss = -11162.193057066444
1
Iteration 19600: Loss = -11162.188855685477
Iteration 19700: Loss = -11162.190120721987
1
Iteration 19800: Loss = -11162.188841877132
Iteration 19900: Loss = -11162.189694906368
1
pi: tensor([[0.6515, 0.3485],
        [0.3110, 0.6890]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9822, 0.0178], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1837, 0.0632],
         [0.6955, 0.3063]],

        [[0.6940, 0.1092],
         [0.7064, 0.5264]],

        [[0.6832, 0.1045],
         [0.5753, 0.6136]],

        [[0.6771, 0.1023],
         [0.6034, 0.6909]],

        [[0.5666, 0.0971],
         [0.6052, 0.6872]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824124176797128
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.6076078643855685
Average Adjusted Rand Index: 0.7362101341970987
11120.939812724544
[0.6076078643855685, 0.6076078643855685] [0.7362101341970987, 0.7362101341970987] [11162.189248717474, 11162.188857012143]
-------------------------------------
This iteration is 57
True Objective function: Loss = -11180.65797611485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22528.63558079393
Iteration 100: Loss = -11485.095602474767
Iteration 200: Loss = -11484.208551780679
Iteration 300: Loss = -11480.43386560219
Iteration 400: Loss = -11400.740737543232
Iteration 500: Loss = -11193.711495582309
Iteration 600: Loss = -11168.850985059385
Iteration 700: Loss = -11164.454496718516
Iteration 800: Loss = -11164.045956201178
Iteration 900: Loss = -11163.942597633888
Iteration 1000: Loss = -11163.841694385337
Iteration 1100: Loss = -11163.791051216189
Iteration 1200: Loss = -11163.75077634227
Iteration 1300: Loss = -11163.68870900784
Iteration 1400: Loss = -11163.618830328014
Iteration 1500: Loss = -11163.589404490971
Iteration 1600: Loss = -11163.572865088569
Iteration 1700: Loss = -11163.556274421047
Iteration 1800: Loss = -11163.546776214647
Iteration 1900: Loss = -11163.53860102641
Iteration 2000: Loss = -11163.529936088382
Iteration 2100: Loss = -11163.514025159537
Iteration 2200: Loss = -11163.481675215371
Iteration 2300: Loss = -11163.49548535614
1
Iteration 2400: Loss = -11163.466801288494
Iteration 2500: Loss = -11163.462124962716
Iteration 2600: Loss = -11163.46255923666
1
Iteration 2700: Loss = -11163.456775551835
Iteration 2800: Loss = -11163.454737473257
Iteration 2900: Loss = -11163.452789807801
Iteration 3000: Loss = -11163.450717186026
Iteration 3100: Loss = -11163.446829726505
Iteration 3200: Loss = -11163.430874752397
Iteration 3300: Loss = -11163.429226984692
Iteration 3400: Loss = -11163.427460726372
Iteration 3500: Loss = -11163.426635593383
Iteration 3600: Loss = -11163.43093545151
1
Iteration 3700: Loss = -11163.42634942147
Iteration 3800: Loss = -11163.425215106945
Iteration 3900: Loss = -11163.424292785405
Iteration 4000: Loss = -11163.423198644621
Iteration 4100: Loss = -11163.422573578046
Iteration 4200: Loss = -11163.421744946063
Iteration 4300: Loss = -11163.421124055525
Iteration 4400: Loss = -11163.420374877884
Iteration 4500: Loss = -11163.419090198224
Iteration 4600: Loss = -11163.415229857657
Iteration 4700: Loss = -11163.41443719769
Iteration 4800: Loss = -11163.418443246983
1
Iteration 4900: Loss = -11163.413017336297
Iteration 5000: Loss = -11163.414225290568
1
Iteration 5100: Loss = -11163.418889596982
2
Iteration 5200: Loss = -11163.411688702485
Iteration 5300: Loss = -11163.411478200645
Iteration 5400: Loss = -11163.417295207593
1
Iteration 5500: Loss = -11163.411063670166
Iteration 5600: Loss = -11163.411624375987
1
Iteration 5700: Loss = -11163.410673718048
Iteration 5800: Loss = -11163.420061929104
1
Iteration 5900: Loss = -11163.410384119503
Iteration 6000: Loss = -11163.410225221482
Iteration 6100: Loss = -11163.411836339887
1
Iteration 6200: Loss = -11163.409977794263
Iteration 6300: Loss = -11163.409824360951
Iteration 6400: Loss = -11163.409665077814
Iteration 6500: Loss = -11163.415762325347
1
Iteration 6600: Loss = -11163.41161505062
2
Iteration 6700: Loss = -11163.41280877335
3
Iteration 6800: Loss = -11163.388598133883
Iteration 6900: Loss = -11163.385969621011
Iteration 7000: Loss = -11163.385926408137
Iteration 7100: Loss = -11163.38571328558
Iteration 7200: Loss = -11163.385727755205
Iteration 7300: Loss = -11163.385460914951
Iteration 7400: Loss = -11163.385706352721
1
Iteration 7500: Loss = -11163.388505855311
2
Iteration 7600: Loss = -11163.384537295986
Iteration 7700: Loss = -11163.38436139905
Iteration 7800: Loss = -11163.384358215033
Iteration 7900: Loss = -11163.386998077314
1
Iteration 8000: Loss = -11163.38418466746
Iteration 8100: Loss = -11163.385590202339
1
Iteration 8200: Loss = -11163.384817856044
2
Iteration 8300: Loss = -11163.38405622203
Iteration 8400: Loss = -11163.38392934681
Iteration 8500: Loss = -11163.384028445795
Iteration 8600: Loss = -11163.383953678796
Iteration 8700: Loss = -11163.38383543987
Iteration 8800: Loss = -11163.383845199804
Iteration 8900: Loss = -11163.383858718596
Iteration 9000: Loss = -11163.383744019038
Iteration 9100: Loss = -11163.383770303582
Iteration 9200: Loss = -11163.38436806433
1
Iteration 9300: Loss = -11163.381177293631
Iteration 9400: Loss = -11163.38106659476
Iteration 9500: Loss = -11163.425372123136
1
Iteration 9600: Loss = -11163.38093021399
Iteration 9700: Loss = -11163.382433111952
1
Iteration 9800: Loss = -11163.394137341269
2
Iteration 9900: Loss = -11163.380703614392
Iteration 10000: Loss = -11163.381004223875
1
Iteration 10100: Loss = -11163.38208267538
2
Iteration 10200: Loss = -11163.387216730744
3
Iteration 10300: Loss = -11163.388186745058
4
Iteration 10400: Loss = -11163.382926201119
5
Iteration 10500: Loss = -11163.38343682954
6
Iteration 10600: Loss = -11163.380579984221
Iteration 10700: Loss = -11163.38059458218
Iteration 10800: Loss = -11163.380516000016
Iteration 10900: Loss = -11163.38058190632
Iteration 11000: Loss = -11163.405631359636
1
Iteration 11100: Loss = -11163.381306910309
2
Iteration 11200: Loss = -11163.38050708194
Iteration 11300: Loss = -11163.38358099002
1
Iteration 11400: Loss = -11163.380465871343
Iteration 11500: Loss = -11163.38191130183
1
Iteration 11600: Loss = -11163.38044333231
Iteration 11700: Loss = -11163.381159846664
1
Iteration 11800: Loss = -11163.380446798634
Iteration 11900: Loss = -11163.380108645373
Iteration 12000: Loss = -11163.378924198432
Iteration 12100: Loss = -11163.395089430618
1
Iteration 12200: Loss = -11163.378690547852
Iteration 12300: Loss = -11163.378754350935
Iteration 12400: Loss = -11163.377334682264
Iteration 12500: Loss = -11163.384636594536
1
Iteration 12600: Loss = -11163.377304955557
Iteration 12700: Loss = -11163.3892675939
1
Iteration 12800: Loss = -11163.377339502595
Iteration 12900: Loss = -11163.377288660868
Iteration 13000: Loss = -11163.378255186544
1
Iteration 13100: Loss = -11163.376465226625
Iteration 13200: Loss = -11163.37666823217
1
Iteration 13300: Loss = -11163.376538294044
Iteration 13400: Loss = -11163.377931238572
1
Iteration 13500: Loss = -11163.376540200705
Iteration 13600: Loss = -11163.37673743047
1
Iteration 13700: Loss = -11163.386903153614
2
Iteration 13800: Loss = -11163.376413206235
Iteration 13900: Loss = -11163.38031926247
1
Iteration 14000: Loss = -11163.376417926065
Iteration 14100: Loss = -11163.432742436496
1
Iteration 14200: Loss = -11163.376421848363
Iteration 14300: Loss = -11163.376346214965
Iteration 14400: Loss = -11163.37852603708
1
Iteration 14500: Loss = -11163.376343913664
Iteration 14600: Loss = -11163.617146779594
1
Iteration 14700: Loss = -11163.376329073986
Iteration 14800: Loss = -11163.384438715639
1
Iteration 14900: Loss = -11163.387079234566
2
Iteration 15000: Loss = -11163.379853928978
3
Iteration 15100: Loss = -11163.380729603155
4
Iteration 15200: Loss = -11163.376306895403
Iteration 15300: Loss = -11163.376422208623
1
Iteration 15400: Loss = -11163.416044343181
2
Iteration 15500: Loss = -11163.37627121023
Iteration 15600: Loss = -11163.382600229455
1
Iteration 15700: Loss = -11163.376275640796
Iteration 15800: Loss = -11163.378426052643
1
Iteration 15900: Loss = -11163.376253340204
Iteration 16000: Loss = -11163.38245169924
1
Iteration 16100: Loss = -11163.376290413116
Iteration 16200: Loss = -11163.392591366392
1
Iteration 16300: Loss = -11163.364479511187
Iteration 16400: Loss = -11163.362948257076
Iteration 16500: Loss = -11163.362960858736
Iteration 16600: Loss = -11163.382086512043
1
Iteration 16700: Loss = -11163.36546645041
2
Iteration 16800: Loss = -11163.362888345398
Iteration 16900: Loss = -11163.381112201458
1
Iteration 17000: Loss = -11163.509374605783
2
Iteration 17100: Loss = -11163.36282155221
Iteration 17200: Loss = -11163.363474703248
1
Iteration 17300: Loss = -11163.376048879683
2
Iteration 17400: Loss = -11163.362828837233
Iteration 17500: Loss = -11163.398905540573
1
Iteration 17600: Loss = -11163.362673081725
Iteration 17700: Loss = -11163.362682599442
Iteration 17800: Loss = -11163.38145810615
1
Iteration 17900: Loss = -11163.362701088297
Iteration 18000: Loss = -11163.367053167929
1
Iteration 18100: Loss = -11163.363277978297
2
Iteration 18200: Loss = -11163.36273426491
Iteration 18300: Loss = -11163.364766336907
1
Iteration 18400: Loss = -11163.36271379156
Iteration 18500: Loss = -11163.3642061027
1
Iteration 18600: Loss = -11163.362532477437
Iteration 18700: Loss = -11163.362631964908
Iteration 18800: Loss = -11163.366357541909
1
Iteration 18900: Loss = -11163.365891486532
2
Iteration 19000: Loss = -11163.362540925162
Iteration 19100: Loss = -11163.363613849533
1
Iteration 19200: Loss = -11163.377868304302
2
Iteration 19300: Loss = -11163.362535450782
Iteration 19400: Loss = -11163.362908951904
1
Iteration 19500: Loss = -11163.362525508484
Iteration 19600: Loss = -11163.436107027617
1
Iteration 19700: Loss = -11163.36185345993
Iteration 19800: Loss = -11163.36184868762
Iteration 19900: Loss = -11163.37572420609
1
pi: tensor([[0.7452, 0.2548],
        [0.2468, 0.7532]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4941, 0.5059], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1974, 0.0974],
         [0.5433, 0.3052]],

        [[0.6881, 0.1124],
         [0.7081, 0.7008]],

        [[0.6940, 0.1019],
         [0.5634, 0.6503]],

        [[0.5774, 0.0954],
         [0.6384, 0.6329]],

        [[0.6246, 0.0940],
         [0.5398, 0.6775]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603204832025548
Average Adjusted Rand Index: 0.9603199464905721
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21238.648077891467
Iteration 100: Loss = -11485.449832278915
Iteration 200: Loss = -11484.570188974027
Iteration 300: Loss = -11484.033763138144
Iteration 400: Loss = -11483.304477554022
Iteration 500: Loss = -11481.566278457472
Iteration 600: Loss = -11446.981377222019
Iteration 700: Loss = -11253.775300536994
Iteration 800: Loss = -11185.0594759522
Iteration 900: Loss = -11170.076494001762
Iteration 1000: Loss = -11168.186510377687
Iteration 1100: Loss = -11167.167516445965
Iteration 1200: Loss = -11164.71635684467
Iteration 1300: Loss = -11164.606766348585
Iteration 1400: Loss = -11164.33352341061
Iteration 1500: Loss = -11164.292517320057
Iteration 1600: Loss = -11164.259016318083
Iteration 1700: Loss = -11164.202126099764
Iteration 1800: Loss = -11164.168753822147
Iteration 1900: Loss = -11164.045735657248
Iteration 2000: Loss = -11163.704830764047
Iteration 2100: Loss = -11163.689399141975
Iteration 2200: Loss = -11163.67887828666
Iteration 2300: Loss = -11163.667637944776
Iteration 2400: Loss = -11163.655467878692
Iteration 2500: Loss = -11163.644490789291
Iteration 2600: Loss = -11163.633591739948
Iteration 2700: Loss = -11163.616709900236
Iteration 2800: Loss = -11163.600165670861
Iteration 2900: Loss = -11163.593714297738
Iteration 3000: Loss = -11163.58871790885
Iteration 3100: Loss = -11163.584600444523
Iteration 3200: Loss = -11163.580747403243
Iteration 3300: Loss = -11163.576934571363
Iteration 3400: Loss = -11163.59156227644
1
Iteration 3500: Loss = -11163.563765302017
Iteration 3600: Loss = -11163.489123735171
Iteration 3700: Loss = -11163.488839101732
Iteration 3800: Loss = -11163.482693062011
Iteration 3900: Loss = -11163.488800430081
1
Iteration 4000: Loss = -11163.480004647066
Iteration 4100: Loss = -11163.480715526146
1
Iteration 4200: Loss = -11163.48093433008
2
Iteration 4300: Loss = -11163.484902625056
3
Iteration 4400: Loss = -11163.476756928347
Iteration 4500: Loss = -11163.475763450826
Iteration 4600: Loss = -11163.475913940078
1
Iteration 4700: Loss = -11163.474361797114
Iteration 4800: Loss = -11163.478609512615
1
Iteration 4900: Loss = -11163.47290919832
Iteration 5000: Loss = -11163.47334269968
1
Iteration 5100: Loss = -11163.472635397973
Iteration 5200: Loss = -11163.4735676643
1
Iteration 5300: Loss = -11163.467839955561
Iteration 5400: Loss = -11163.466856710238
Iteration 5500: Loss = -11163.472300190775
1
Iteration 5600: Loss = -11163.466160027143
Iteration 5700: Loss = -11163.464115777655
Iteration 5800: Loss = -11163.464070804788
Iteration 5900: Loss = -11163.472904214572
1
Iteration 6000: Loss = -11163.463044554503
Iteration 6100: Loss = -11163.46259220299
Iteration 6200: Loss = -11163.46338435126
1
Iteration 6300: Loss = -11163.461958355412
Iteration 6400: Loss = -11163.461999866904
Iteration 6500: Loss = -11163.462125756625
1
Iteration 6600: Loss = -11163.46122997417
Iteration 6700: Loss = -11163.461069255149
Iteration 6800: Loss = -11163.460763997742
Iteration 6900: Loss = -11163.460611272607
Iteration 7000: Loss = -11163.460430639332
Iteration 7100: Loss = -11163.460374350276
Iteration 7200: Loss = -11163.460436521267
Iteration 7300: Loss = -11163.463342899284
1
Iteration 7400: Loss = -11163.459972728759
Iteration 7500: Loss = -11163.477268877848
1
Iteration 7600: Loss = -11163.459702195225
Iteration 7700: Loss = -11163.461592913092
1
Iteration 7800: Loss = -11163.459461678534
Iteration 7900: Loss = -11163.45962240403
1
Iteration 8000: Loss = -11163.45832497899
Iteration 8100: Loss = -11163.458268470069
Iteration 8200: Loss = -11163.457759647412
Iteration 8300: Loss = -11163.457958125664
1
Iteration 8400: Loss = -11163.457622568581
Iteration 8500: Loss = -11163.457547361739
Iteration 8600: Loss = -11163.466232064937
1
Iteration 8700: Loss = -11163.457384065196
Iteration 8800: Loss = -11163.462372261258
1
Iteration 8900: Loss = -11163.457216842846
Iteration 9000: Loss = -11163.437422017394
Iteration 9100: Loss = -11163.442480130188
1
Iteration 9200: Loss = -11163.434373467304
Iteration 9300: Loss = -11163.433941787041
Iteration 9400: Loss = -11163.433568868304
Iteration 9500: Loss = -11163.397911961923
Iteration 9600: Loss = -11163.397703454437
Iteration 9700: Loss = -11163.397777206079
Iteration 9800: Loss = -11163.491391399552
1
Iteration 9900: Loss = -11163.397546270218
Iteration 10000: Loss = -11163.401937755305
1
Iteration 10100: Loss = -11163.397473270308
Iteration 10200: Loss = -11163.39764978633
1
Iteration 10300: Loss = -11163.403052130054
2
Iteration 10400: Loss = -11163.397368269938
Iteration 10500: Loss = -11163.441526833614
1
Iteration 10600: Loss = -11163.396938042797
Iteration 10700: Loss = -11163.40714356989
1
Iteration 10800: Loss = -11163.396859057302
Iteration 10900: Loss = -11163.4052176093
1
Iteration 11000: Loss = -11163.396784700282
Iteration 11100: Loss = -11163.39965669622
1
Iteration 11200: Loss = -11163.396473726609
Iteration 11300: Loss = -11163.397565308167
1
Iteration 11400: Loss = -11163.395838707016
Iteration 11500: Loss = -11163.409833087675
1
Iteration 11600: Loss = -11163.395823120078
Iteration 11700: Loss = -11163.41866942322
1
Iteration 11800: Loss = -11163.395784389812
Iteration 11900: Loss = -11163.398923794377
1
Iteration 12000: Loss = -11163.400280049014
2
Iteration 12100: Loss = -11163.404453467212
3
Iteration 12200: Loss = -11163.395721614705
Iteration 12300: Loss = -11163.395847695734
1
Iteration 12400: Loss = -11163.403230046706
2
Iteration 12500: Loss = -11163.398562756613
3
Iteration 12600: Loss = -11163.45191650898
4
Iteration 12700: Loss = -11163.394190641273
Iteration 12800: Loss = -11163.394134997816
Iteration 12900: Loss = -11163.410555083177
1
Iteration 13000: Loss = -11163.394095787718
Iteration 13100: Loss = -11163.397259255222
1
Iteration 13200: Loss = -11163.394073424966
Iteration 13300: Loss = -11163.394863085774
1
Iteration 13400: Loss = -11163.394072715193
Iteration 13500: Loss = -11163.394081311626
Iteration 13600: Loss = -11163.548278811599
1
Iteration 13700: Loss = -11163.394066702467
Iteration 13800: Loss = -11163.394130594917
Iteration 13900: Loss = -11163.590088404653
1
Iteration 14000: Loss = -11163.394015208114
Iteration 14100: Loss = -11163.457460141994
1
Iteration 14200: Loss = -11163.393986947392
Iteration 14300: Loss = -11163.394093416577
1
Iteration 14400: Loss = -11163.39393385398
Iteration 14500: Loss = -11163.395345140487
1
Iteration 14600: Loss = -11163.393923207526
Iteration 14700: Loss = -11163.3939414236
Iteration 14800: Loss = -11163.402525243404
1
Iteration 14900: Loss = -11163.412351761772
2
Iteration 15000: Loss = -11163.58434813832
3
Iteration 15100: Loss = -11163.393892176264
Iteration 15200: Loss = -11163.394444209813
1
Iteration 15300: Loss = -11163.393875640613
Iteration 15400: Loss = -11163.394363458741
1
Iteration 15500: Loss = -11163.409978846053
2
Iteration 15600: Loss = -11163.396527672163
3
Iteration 15700: Loss = -11163.394011309894
4
Iteration 15800: Loss = -11163.394272656884
5
Iteration 15900: Loss = -11163.393815225032
Iteration 16000: Loss = -11163.382240579363
Iteration 16100: Loss = -11163.399487779818
1
Iteration 16200: Loss = -11163.384779792477
2
Iteration 16300: Loss = -11163.384542692207
3
Iteration 16400: Loss = -11163.382125985563
Iteration 16500: Loss = -11163.391893624175
1
Iteration 16600: Loss = -11163.400965629735
2
Iteration 16700: Loss = -11163.477992628616
3
Iteration 16800: Loss = -11163.362759516507
Iteration 16900: Loss = -11163.362599719774
Iteration 17000: Loss = -11163.363086330124
1
Iteration 17100: Loss = -11163.37889511775
2
Iteration 17200: Loss = -11163.362444192497
Iteration 17300: Loss = -11163.40413057686
1
Iteration 17400: Loss = -11163.362448520907
Iteration 17500: Loss = -11163.362442554026
Iteration 17600: Loss = -11163.362618792966
1
Iteration 17700: Loss = -11163.362388357591
Iteration 17800: Loss = -11163.649212171253
1
Iteration 17900: Loss = -11163.361953917032
Iteration 18000: Loss = -11163.361935472733
Iteration 18100: Loss = -11163.362761819299
1
Iteration 18200: Loss = -11163.443853272302
2
Iteration 18300: Loss = -11163.363230985746
3
Iteration 18400: Loss = -11163.684298032398
4
Iteration 18500: Loss = -11163.361948826909
Iteration 18600: Loss = -11163.362922965302
1
Iteration 18700: Loss = -11163.363448912713
2
Iteration 18800: Loss = -11163.380278214156
3
Iteration 18900: Loss = -11163.361959898892
Iteration 19000: Loss = -11163.362197919394
1
Iteration 19100: Loss = -11163.361950786646
Iteration 19200: Loss = -11163.362026639883
Iteration 19300: Loss = -11163.361996178455
Iteration 19400: Loss = -11163.361983269762
Iteration 19500: Loss = -11163.361996952375
Iteration 19600: Loss = -11163.361974047119
Iteration 19700: Loss = -11163.740499632888
1
Iteration 19800: Loss = -11163.36127903204
Iteration 19900: Loss = -11163.361260349922
pi: tensor([[0.7452, 0.2548],
        [0.2467, 0.7533]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4941, 0.5059], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1975, 0.0974],
         [0.6421, 0.3051]],

        [[0.7185, 0.1124],
         [0.5282, 0.5449]],

        [[0.5744, 0.1019],
         [0.5939, 0.6103]],

        [[0.6666, 0.0954],
         [0.6719, 0.6945]],

        [[0.5719, 0.0940],
         [0.6271, 0.7135]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603204832025548
Average Adjusted Rand Index: 0.9603199464905721
11180.65797611485
[0.9603204832025548, 0.9603204832025548] [0.9603199464905721, 0.9603199464905721] [11163.361852870004, 11163.36137687745]
-------------------------------------
This iteration is 58
True Objective function: Loss = -11284.048231685876
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24046.70737658463
Iteration 100: Loss = -11544.110903059864
Iteration 200: Loss = -11543.400102886628
Iteration 300: Loss = -11543.184626884264
Iteration 400: Loss = -11543.04083066656
Iteration 500: Loss = -11542.929268792921
Iteration 600: Loss = -11542.84512480448
Iteration 700: Loss = -11542.78201922174
Iteration 800: Loss = -11542.729995677697
Iteration 900: Loss = -11542.677912241183
Iteration 1000: Loss = -11542.597785007549
Iteration 1100: Loss = -11542.11477278845
Iteration 1200: Loss = -11539.940304746138
Iteration 1300: Loss = -11490.970666574543
Iteration 1400: Loss = -11439.825663045005
Iteration 1500: Loss = -11439.44333912466
Iteration 1600: Loss = -11398.180268131386
Iteration 1700: Loss = -11285.20430399119
Iteration 1800: Loss = -11252.124566625076
Iteration 1900: Loss = -11251.878398127048
Iteration 2000: Loss = -11251.805502340088
Iteration 2100: Loss = -11251.793705164539
Iteration 2200: Loss = -11251.783171332918
Iteration 2300: Loss = -11251.749768059772
Iteration 2400: Loss = -11251.73790723236
Iteration 2500: Loss = -11251.72505732704
Iteration 2600: Loss = -11251.71557896421
Iteration 2700: Loss = -11251.700193228831
Iteration 2800: Loss = -11250.505751042865
Iteration 2900: Loss = -11250.51730014179
1
Iteration 3000: Loss = -11250.496153125134
Iteration 3100: Loss = -11250.495454992033
Iteration 3200: Loss = -11250.499118880367
1
Iteration 3300: Loss = -11250.494311262573
Iteration 3400: Loss = -11250.493839139504
Iteration 3500: Loss = -11250.513346287455
1
Iteration 3600: Loss = -11250.491311672255
Iteration 3700: Loss = -11250.450786261248
Iteration 3800: Loss = -11250.450389577658
Iteration 3900: Loss = -11250.450567349915
1
Iteration 4000: Loss = -11250.451222014648
2
Iteration 4100: Loss = -11250.449147935828
Iteration 4200: Loss = -11250.44872474577
Iteration 4300: Loss = -11250.448402155213
Iteration 4400: Loss = -11250.448043597085
Iteration 4500: Loss = -11250.457414188168
1
Iteration 4600: Loss = -11250.447359229474
Iteration 4700: Loss = -11250.446839560875
Iteration 4800: Loss = -11250.445967827412
Iteration 4900: Loss = -11250.44429361666
Iteration 5000: Loss = -11250.444601711144
1
Iteration 5100: Loss = -11250.443522062598
Iteration 5200: Loss = -11250.440629946615
Iteration 5300: Loss = -11250.440803360347
1
Iteration 5400: Loss = -11250.432637826842
Iteration 5500: Loss = -11250.432118829503
Iteration 5600: Loss = -11250.435228318589
1
Iteration 5700: Loss = -11250.434450262606
2
Iteration 5800: Loss = -11250.431979021952
Iteration 5900: Loss = -11250.43264039256
1
Iteration 6000: Loss = -11250.435088292634
2
Iteration 6100: Loss = -11250.431414294962
Iteration 6200: Loss = -11250.43122669943
Iteration 6300: Loss = -11250.436273415553
1
Iteration 6400: Loss = -11250.430240222291
Iteration 6500: Loss = -11250.436487776953
1
Iteration 6600: Loss = -11250.429846347319
Iteration 6700: Loss = -11250.429818136097
Iteration 6800: Loss = -11250.42970773243
Iteration 6900: Loss = -11250.429535179419
Iteration 7000: Loss = -11250.450605059092
1
Iteration 7100: Loss = -11250.429018968463
Iteration 7200: Loss = -11250.44473183224
1
Iteration 7300: Loss = -11250.427965716663
Iteration 7400: Loss = -11250.428610650017
1
Iteration 7500: Loss = -11250.428200812647
2
Iteration 7600: Loss = -11250.428596420477
3
Iteration 7700: Loss = -11250.441839366156
4
Iteration 7800: Loss = -11250.427915294562
Iteration 7900: Loss = -11250.456181728861
1
Iteration 8000: Loss = -11250.427810730287
Iteration 8100: Loss = -11250.42787982881
Iteration 8200: Loss = -11250.438540312934
1
Iteration 8300: Loss = -11250.427702557237
Iteration 8400: Loss = -11250.4277097068
Iteration 8500: Loss = -11250.42932288305
1
Iteration 8600: Loss = -11250.436541352572
2
Iteration 8700: Loss = -11250.43295308395
3
Iteration 8800: Loss = -11250.43161579348
4
Iteration 8900: Loss = -11250.440422378428
5
Iteration 9000: Loss = -11250.427948173437
6
Iteration 9100: Loss = -11250.428303359497
7
Iteration 9200: Loss = -11250.43851423971
8
Iteration 9300: Loss = -11250.443307814934
9
Iteration 9400: Loss = -11250.427308512468
Iteration 9500: Loss = -11250.432162844589
1
Iteration 9600: Loss = -11250.428200974575
2
Iteration 9700: Loss = -11250.429599735418
3
Iteration 9800: Loss = -11250.436875203275
4
Iteration 9900: Loss = -11250.445707799234
5
Iteration 10000: Loss = -11250.427288693827
Iteration 10100: Loss = -11250.427436163773
1
Iteration 10200: Loss = -11250.431331734317
2
Iteration 10300: Loss = -11250.42722015384
Iteration 10400: Loss = -11250.427375548143
1
Iteration 10500: Loss = -11250.427219397321
Iteration 10600: Loss = -11250.427909997843
1
Iteration 10700: Loss = -11250.428656181435
2
Iteration 10800: Loss = -11250.427263566338
Iteration 10900: Loss = -11250.433841121274
1
Iteration 11000: Loss = -11250.427187151321
Iteration 11100: Loss = -11250.488634032394
1
Iteration 11200: Loss = -11250.427004455629
Iteration 11300: Loss = -11250.4269742559
Iteration 11400: Loss = -11250.42763531977
1
Iteration 11500: Loss = -11250.426959714603
Iteration 11600: Loss = -11250.545957381015
1
Iteration 11700: Loss = -11250.427042289733
Iteration 11800: Loss = -11250.427122883359
Iteration 11900: Loss = -11250.432563250202
1
Iteration 12000: Loss = -11250.590310946083
2
Iteration 12100: Loss = -11250.426955678895
Iteration 12200: Loss = -11250.428073499359
1
Iteration 12300: Loss = -11250.426934038838
Iteration 12400: Loss = -11250.426995343212
Iteration 12500: Loss = -11250.426918499606
Iteration 12600: Loss = -11250.438023294317
1
Iteration 12700: Loss = -11250.426799778143
Iteration 12800: Loss = -11250.42678516572
Iteration 12900: Loss = -11250.427149670319
1
Iteration 13000: Loss = -11250.728410356547
2
Iteration 13100: Loss = -11250.433918590397
3
Iteration 13200: Loss = -11250.471191741235
4
Iteration 13300: Loss = -11250.434912003413
5
Iteration 13400: Loss = -11250.43733768753
6
Iteration 13500: Loss = -11250.426719443185
Iteration 13600: Loss = -11250.453901826322
1
Iteration 13700: Loss = -11250.426703935906
Iteration 13800: Loss = -11250.426767785497
Iteration 13900: Loss = -11250.426758632193
Iteration 14000: Loss = -11250.426661649939
Iteration 14100: Loss = -11250.427038712518
1
Iteration 14200: Loss = -11250.426669118806
Iteration 14300: Loss = -11250.42747669498
1
Iteration 14400: Loss = -11250.440298099315
2
Iteration 14500: Loss = -11250.429481400031
3
Iteration 14600: Loss = -11250.426947416998
4
Iteration 14700: Loss = -11250.426711923395
Iteration 14800: Loss = -11250.569167019301
1
Iteration 14900: Loss = -11250.4266854782
Iteration 15000: Loss = -11250.468081932306
1
Iteration 15100: Loss = -11250.426688067908
Iteration 15200: Loss = -11250.426678292222
Iteration 15300: Loss = -11250.426834110674
1
Iteration 15400: Loss = -11250.426667080885
Iteration 15500: Loss = -11250.428193308273
1
Iteration 15600: Loss = -11250.426810867897
2
Iteration 15700: Loss = -11250.432088897982
3
Iteration 15800: Loss = -11250.426660579997
Iteration 15900: Loss = -11250.428153878096
1
Iteration 16000: Loss = -11250.431094145464
2
Iteration 16100: Loss = -11250.449468672467
3
Iteration 16200: Loss = -11250.427230679152
4
Iteration 16300: Loss = -11250.42696800092
5
Iteration 16400: Loss = -11250.61415954681
6
Iteration 16500: Loss = -11250.438450244912
7
Iteration 16600: Loss = -11250.4279637934
8
Iteration 16700: Loss = -11250.432089608346
9
Iteration 16800: Loss = -11250.615945784693
10
Iteration 16900: Loss = -11250.426700711105
Iteration 17000: Loss = -11250.446306859345
1
Iteration 17100: Loss = -11250.426681960389
Iteration 17200: Loss = -11250.439031743539
1
Iteration 17300: Loss = -11250.45060236594
2
Iteration 17400: Loss = -11250.426940237643
3
Iteration 17500: Loss = -11250.429233495619
4
Iteration 17600: Loss = -11250.428519966841
5
Iteration 17700: Loss = -11250.426715870932
Iteration 17800: Loss = -11250.426787373453
Iteration 17900: Loss = -11250.573354072063
1
Iteration 18000: Loss = -11250.426616435607
Iteration 18100: Loss = -11250.43387055687
1
Iteration 18200: Loss = -11250.427654163506
2
Iteration 18300: Loss = -11250.42687153966
3
Iteration 18400: Loss = -11250.430192922966
4
Iteration 18500: Loss = -11250.546042843682
5
Iteration 18600: Loss = -11250.426602947227
Iteration 18700: Loss = -11250.427831923475
1
Iteration 18800: Loss = -11250.432246143198
2
Iteration 18900: Loss = -11250.427844316344
3
Iteration 19000: Loss = -11250.432159335387
4
Iteration 19100: Loss = -11250.426609849525
Iteration 19200: Loss = -11250.427408415779
1
Iteration 19300: Loss = -11250.42661028169
Iteration 19400: Loss = -11250.446506953816
1
Iteration 19500: Loss = -11250.426599155546
Iteration 19600: Loss = -11250.426612183837
Iteration 19700: Loss = -11250.42666555007
Iteration 19800: Loss = -11250.426605939416
Iteration 19900: Loss = -11250.427247201971
1
pi: tensor([[0.7105, 0.2895],
        [0.2819, 0.7181]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5627, 0.4373], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2041, 0.0934],
         [0.6866, 0.3065]],

        [[0.7109, 0.1061],
         [0.7131, 0.5610]],

        [[0.5452, 0.1025],
         [0.6722, 0.6378]],

        [[0.6032, 0.1068],
         [0.6537, 0.6825]],

        [[0.5346, 0.0983],
         [0.6454, 0.5017]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
Global Adjusted Rand Index: 0.9137634849403808
Average Adjusted Rand Index: 0.9139357207512713
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24076.004043549856
Iteration 100: Loss = -11542.46835902706
Iteration 200: Loss = -11533.332278186408
Iteration 300: Loss = -11501.618379852871
Iteration 400: Loss = -11373.878320647042
Iteration 500: Loss = -11320.57836651434
Iteration 600: Loss = -11312.817564772857
Iteration 700: Loss = -11311.93951170777
Iteration 800: Loss = -11311.640400801112
Iteration 900: Loss = -11311.5399952463
Iteration 1000: Loss = -11311.478417920327
Iteration 1100: Loss = -11311.437222071787
Iteration 1200: Loss = -11311.405915445563
Iteration 1300: Loss = -11311.379859053872
Iteration 1400: Loss = -11311.355772831397
Iteration 1500: Loss = -11311.329727534947
Iteration 1600: Loss = -11311.297147331981
Iteration 1700: Loss = -11311.235860500756
Iteration 1800: Loss = -11311.066361234287
Iteration 1900: Loss = -11310.400802063145
Iteration 2000: Loss = -11307.205584180176
Iteration 2100: Loss = -11306.829262132935
Iteration 2200: Loss = -11306.50899955083
Iteration 2300: Loss = -11306.474126016334
Iteration 2400: Loss = -11306.46360466412
Iteration 2500: Loss = -11306.454538792008
Iteration 2600: Loss = -11306.449983446226
Iteration 2700: Loss = -11306.446702068644
Iteration 2800: Loss = -11306.444029598111
Iteration 2900: Loss = -11306.442053505869
Iteration 3000: Loss = -11306.44044136386
Iteration 3100: Loss = -11306.439148398236
Iteration 3200: Loss = -11306.437985011702
Iteration 3300: Loss = -11306.469570079551
1
Iteration 3400: Loss = -11306.43607301022
Iteration 3500: Loss = -11306.435279568892
Iteration 3600: Loss = -11306.435541265422
1
Iteration 3700: Loss = -11306.434070019368
Iteration 3800: Loss = -11306.433604520831
Iteration 3900: Loss = -11306.432787743437
Iteration 4000: Loss = -11306.432085903065
Iteration 4100: Loss = -11306.431014094986
Iteration 4200: Loss = -11306.41744818605
Iteration 4300: Loss = -11306.416926013397
Iteration 4400: Loss = -11306.41658435425
Iteration 4500: Loss = -11306.416368302302
Iteration 4600: Loss = -11306.416048575731
Iteration 4700: Loss = -11306.420734064517
1
Iteration 4800: Loss = -11306.415605837441
Iteration 4900: Loss = -11306.415417548698
Iteration 5000: Loss = -11306.425360091594
1
Iteration 5100: Loss = -11306.416793022678
2
Iteration 5200: Loss = -11306.305130108582
Iteration 5300: Loss = -11306.30490719822
Iteration 5400: Loss = -11306.304666720831
Iteration 5500: Loss = -11306.304575342834
Iteration 5600: Loss = -11306.304273798234
Iteration 5700: Loss = -11306.304862189363
1
Iteration 5800: Loss = -11306.30341601313
Iteration 5900: Loss = -11306.303265438788
Iteration 6000: Loss = -11306.302839537815
Iteration 6100: Loss = -11306.302804073508
Iteration 6200: Loss = -11306.302619472413
Iteration 6300: Loss = -11306.312369044066
1
Iteration 6400: Loss = -11306.302449415454
Iteration 6500: Loss = -11306.304442838007
1
Iteration 6600: Loss = -11306.30232026228
Iteration 6700: Loss = -11306.302222153492
Iteration 6800: Loss = -11306.302189664033
Iteration 6900: Loss = -11306.30209950577
Iteration 7000: Loss = -11306.302386857262
1
Iteration 7100: Loss = -11306.302022931954
Iteration 7200: Loss = -11306.34847787933
1
Iteration 7300: Loss = -11306.30191089273
Iteration 7400: Loss = -11306.301894447563
Iteration 7500: Loss = -11306.303073542149
1
Iteration 7600: Loss = -11306.301818354668
Iteration 7700: Loss = -11306.315345698069
1
Iteration 7800: Loss = -11306.301791416125
Iteration 7900: Loss = -11306.301761479053
Iteration 8000: Loss = -11306.301796805674
Iteration 8100: Loss = -11306.301713601519
Iteration 8200: Loss = -11306.31447436235
1
Iteration 8300: Loss = -11306.3016717203
Iteration 8400: Loss = -11306.310636173917
1
Iteration 8500: Loss = -11306.30172493191
Iteration 8600: Loss = -11306.301618902646
Iteration 8700: Loss = -11306.30184376339
1
Iteration 8800: Loss = -11306.301600842471
Iteration 8900: Loss = -11306.301547675099
Iteration 9000: Loss = -11306.301239668479
Iteration 9100: Loss = -11306.276520889858
Iteration 9200: Loss = -11306.281452895963
1
Iteration 9300: Loss = -11306.27646562313
Iteration 9400: Loss = -11306.278061295634
1
Iteration 9500: Loss = -11306.276512764578
Iteration 9600: Loss = -11306.27658013062
Iteration 9700: Loss = -11306.278679248124
1
Iteration 9800: Loss = -11306.278172862054
2
Iteration 9900: Loss = -11306.276431263306
Iteration 10000: Loss = -11306.414263897941
1
Iteration 10100: Loss = -11306.276352287965
Iteration 10200: Loss = -11306.40609286575
1
Iteration 10300: Loss = -11306.276359039859
Iteration 10400: Loss = -11306.276363581044
Iteration 10500: Loss = -11306.276343016532
Iteration 10600: Loss = -11306.276517556285
1
Iteration 10700: Loss = -11306.277763734915
2
Iteration 10800: Loss = -11306.27687849104
3
Iteration 10900: Loss = -11306.310497214063
4
Iteration 11000: Loss = -11306.276244375953
Iteration 11100: Loss = -11306.286367723327
1
Iteration 11200: Loss = -11306.276242494016
Iteration 11300: Loss = -11306.327280112135
1
Iteration 11400: Loss = -11306.276246456158
Iteration 11500: Loss = -11306.278471932264
1
Iteration 11600: Loss = -11306.27632513649
Iteration 11700: Loss = -11306.276261877672
Iteration 11800: Loss = -11306.276388301658
1
Iteration 11900: Loss = -11306.308161114132
2
Iteration 12000: Loss = -11306.275634164247
Iteration 12100: Loss = -11306.276700477112
1
Iteration 12200: Loss = -11306.275215937307
Iteration 12300: Loss = -11306.292158699656
1
Iteration 12400: Loss = -11306.275183293517
Iteration 12500: Loss = -11306.336189449159
1
Iteration 12600: Loss = -11306.275188325812
Iteration 12700: Loss = -11306.275166281217
Iteration 12800: Loss = -11306.27522237386
Iteration 12900: Loss = -11306.287865288392
1
Iteration 13000: Loss = -11306.27527679458
Iteration 13100: Loss = -11306.27689293834
1
Iteration 13200: Loss = -11306.275134105432
Iteration 13300: Loss = -11306.27549462369
1
Iteration 13400: Loss = -11306.275142242841
Iteration 13500: Loss = -11306.300076571628
1
Iteration 13600: Loss = -11306.275037794641
Iteration 13700: Loss = -11306.275077603052
Iteration 13800: Loss = -11306.275278293606
1
Iteration 13900: Loss = -11306.282201259013
2
Iteration 14000: Loss = -11306.361612753413
3
Iteration 14100: Loss = -11306.275026545243
Iteration 14200: Loss = -11306.276069097621
1
Iteration 14300: Loss = -11306.275023143033
Iteration 14400: Loss = -11306.27509701612
Iteration 14500: Loss = -11306.31687315482
1
Iteration 14600: Loss = -11306.275006910973
Iteration 14700: Loss = -11306.288067059346
1
Iteration 14800: Loss = -11306.275014639283
Iteration 14900: Loss = -11306.347106534988
1
Iteration 15000: Loss = -11306.27501372867
Iteration 15100: Loss = -11306.275038596852
Iteration 15200: Loss = -11306.275211508157
1
Iteration 15300: Loss = -11306.275023748396
Iteration 15400: Loss = -11306.29631481871
1
Iteration 15500: Loss = -11306.275006776646
Iteration 15600: Loss = -11306.275013894416
Iteration 15700: Loss = -11306.276647075294
1
Iteration 15800: Loss = -11306.275011148247
Iteration 15900: Loss = -11306.306708275586
1
Iteration 16000: Loss = -11306.275017234477
Iteration 16100: Loss = -11306.275223698969
1
Iteration 16200: Loss = -11306.27502402654
Iteration 16300: Loss = -11306.275296836975
1
Iteration 16400: Loss = -11306.275018303068
Iteration 16500: Loss = -11306.27832445662
1
Iteration 16600: Loss = -11306.275018638531
Iteration 16700: Loss = -11306.286352448182
1
Iteration 16800: Loss = -11306.273370054887
Iteration 16900: Loss = -11306.27336086501
Iteration 17000: Loss = -11306.543694326936
1
Iteration 17100: Loss = -11306.273309127722
Iteration 17200: Loss = -11306.346795436706
1
Iteration 17300: Loss = -11306.273274475654
Iteration 17400: Loss = -11306.291214922872
1
Iteration 17500: Loss = -11306.273343136932
Iteration 17600: Loss = -11306.27341897843
Iteration 17700: Loss = -11306.273947696316
1
Iteration 17800: Loss = -11306.325408288769
2
Iteration 17900: Loss = -11306.280870934059
3
Iteration 18000: Loss = -11306.275203169225
4
Iteration 18100: Loss = -11306.277334009266
5
Iteration 18200: Loss = -11306.273183404359
Iteration 18300: Loss = -11306.27343963879
1
Iteration 18400: Loss = -11306.273174040436
Iteration 18500: Loss = -11306.273258026822
Iteration 18600: Loss = -11306.273907577543
1
Iteration 18700: Loss = -11306.273288820656
Iteration 18800: Loss = -11306.273598030257
1
Iteration 18900: Loss = -11306.296114658862
2
Iteration 19000: Loss = -11306.273155118603
Iteration 19100: Loss = -11306.278172530798
1
Iteration 19200: Loss = -11306.27318065158
Iteration 19300: Loss = -11306.273153919923
Iteration 19400: Loss = -11306.273453215577
1
Iteration 19500: Loss = -11306.273180605587
Iteration 19600: Loss = -11306.288357963876
1
Iteration 19700: Loss = -11306.273205574793
Iteration 19800: Loss = -11306.274000955013
1
Iteration 19900: Loss = -11306.273210552841
pi: tensor([[0.6563, 0.3437],
        [0.2885, 0.7115]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9004, 0.0996], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1910, 0.1027],
         [0.5875, 0.3141]],

        [[0.6819, 0.1085],
         [0.5618, 0.5020]],

        [[0.7249, 0.1030],
         [0.5645, 0.5187]],

        [[0.6819, 0.1073],
         [0.5909, 0.6997]],

        [[0.7068, 0.0987],
         [0.7103, 0.6420]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.016623577451856865
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
Global Adjusted Rand Index: 0.5646526218668886
Average Adjusted Rand Index: 0.7112572608059611
11284.048231685876
[0.9137634849403808, 0.5646526218668886] [0.9139357207512713, 0.7112572608059611] [11250.43586772707, 11306.276531267922]
-------------------------------------
This iteration is 59
True Objective function: Loss = -11230.067217373846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20659.99070473565
Iteration 100: Loss = -11465.186894688686
Iteration 200: Loss = -11463.862663929758
Iteration 300: Loss = -11463.625968380262
Iteration 400: Loss = -11462.41949424464
Iteration 500: Loss = -11458.568101612771
Iteration 600: Loss = -11457.896362427846
Iteration 700: Loss = -11457.405327152615
Iteration 800: Loss = -11457.041375800345
Iteration 900: Loss = -11456.848151525286
Iteration 1000: Loss = -11456.729035113396
Iteration 1100: Loss = -11456.651950133733
Iteration 1200: Loss = -11456.58880226448
Iteration 1300: Loss = -11338.945455297908
Iteration 1400: Loss = -11290.179390220268
Iteration 1500: Loss = -11283.392963245306
Iteration 1600: Loss = -11283.321340620043
Iteration 1700: Loss = -11276.813128999986
Iteration 1800: Loss = -11276.144534317838
Iteration 1900: Loss = -11276.130537874087
Iteration 2000: Loss = -11276.096748584761
Iteration 2100: Loss = -11276.02279638465
Iteration 2200: Loss = -11276.015212480295
Iteration 2300: Loss = -11271.087277204722
Iteration 2400: Loss = -11271.015263539743
Iteration 2500: Loss = -11267.526230149932
Iteration 2600: Loss = -11266.465302801636
Iteration 2700: Loss = -11266.44740256029
Iteration 2800: Loss = -11263.881529716899
Iteration 2900: Loss = -11263.890855464855
1
Iteration 3000: Loss = -11263.877582816229
Iteration 3100: Loss = -11263.873995000233
Iteration 3200: Loss = -11263.872740703988
Iteration 3300: Loss = -11263.871135012932
Iteration 3400: Loss = -11263.872886817411
1
Iteration 3500: Loss = -11263.864722192018
Iteration 3600: Loss = -11263.800019883605
Iteration 3700: Loss = -11263.797647532001
Iteration 3800: Loss = -11263.744866076986
Iteration 3900: Loss = -11263.741512264383
Iteration 4000: Loss = -11263.73041838084
Iteration 4100: Loss = -11259.057720142397
Iteration 4200: Loss = -11259.058292401327
1
Iteration 4300: Loss = -11259.053519654764
Iteration 4400: Loss = -11259.03540934689
Iteration 4500: Loss = -11259.024179093461
Iteration 4600: Loss = -11259.021143963884
Iteration 4700: Loss = -11259.009050292934
Iteration 4800: Loss = -11258.98812281387
Iteration 4900: Loss = -11258.987780327432
Iteration 5000: Loss = -11258.987670673583
Iteration 5100: Loss = -11258.984946845163
Iteration 5200: Loss = -11258.981238223434
Iteration 5300: Loss = -11258.978717904702
Iteration 5400: Loss = -11258.983949978045
1
Iteration 5500: Loss = -11258.982078827163
2
Iteration 5600: Loss = -11258.935475924533
Iteration 5700: Loss = -11258.950675159236
1
Iteration 5800: Loss = -11258.933309662068
Iteration 5900: Loss = -11258.933599118156
1
Iteration 6000: Loss = -11258.93369987888
2
Iteration 6100: Loss = -11258.93310485757
Iteration 6200: Loss = -11258.933107878245
Iteration 6300: Loss = -11258.933488576147
1
Iteration 6400: Loss = -11258.932912496479
Iteration 6500: Loss = -11258.932890791175
Iteration 6600: Loss = -11258.9326985414
Iteration 6700: Loss = -11258.93256073511
Iteration 6800: Loss = -11258.932349726989
Iteration 6900: Loss = -11258.931297327912
Iteration 7000: Loss = -11258.938650345992
1
Iteration 7100: Loss = -11258.92970177723
Iteration 7200: Loss = -11258.93805848815
1
Iteration 7300: Loss = -11258.929400559073
Iteration 7400: Loss = -11258.93165692571
1
Iteration 7500: Loss = -11258.929131842871
Iteration 7600: Loss = -11258.954608876415
1
Iteration 7700: Loss = -11258.928845818644
Iteration 7800: Loss = -11258.928564919468
Iteration 7900: Loss = -11258.92885153284
1
Iteration 8000: Loss = -11258.923268001754
Iteration 8100: Loss = -11258.880113895379
Iteration 8200: Loss = -11258.85176450202
Iteration 8300: Loss = -11258.85176270493
Iteration 8400: Loss = -11258.852768037485
1
Iteration 8500: Loss = -11258.075802255033
Iteration 8600: Loss = -11257.812849364731
Iteration 8700: Loss = -11257.812068782638
Iteration 8800: Loss = -11257.813922184161
1
Iteration 8900: Loss = -11257.812022752552
Iteration 9000: Loss = -11257.812041155405
Iteration 9100: Loss = -11257.81192634704
Iteration 9200: Loss = -11257.811497517361
Iteration 9300: Loss = -11257.638511310914
Iteration 9400: Loss = -11257.638888020834
1
Iteration 9500: Loss = -11257.691653166956
2
Iteration 9600: Loss = -11257.664647275511
3
Iteration 9700: Loss = -11257.638010661627
Iteration 9800: Loss = -11257.638360713894
1
Iteration 9900: Loss = -11257.76016461769
2
Iteration 10000: Loss = -11257.642790273161
3
Iteration 10100: Loss = -11257.637820273118
Iteration 10200: Loss = -11257.637344396342
Iteration 10300: Loss = -11257.689067222944
1
Iteration 10400: Loss = -11257.639615722728
2
Iteration 10500: Loss = -11257.636539077308
Iteration 10600: Loss = -11257.650019698582
1
Iteration 10700: Loss = -11257.662864430933
2
Iteration 10800: Loss = -11257.636495299454
Iteration 10900: Loss = -11257.63745338164
1
Iteration 11000: Loss = -11257.635715477909
Iteration 11100: Loss = -11257.635731039103
Iteration 11200: Loss = -11257.636109693729
1
Iteration 11300: Loss = -11257.871146011996
2
Iteration 11400: Loss = -11257.63559761869
Iteration 11500: Loss = -11257.636225540611
1
Iteration 11600: Loss = -11257.636470682486
2
Iteration 11700: Loss = -11257.660244389512
3
Iteration 11800: Loss = -11257.634852776282
Iteration 11900: Loss = -11257.63508780005
1
Iteration 12000: Loss = -11257.634814304029
Iteration 12100: Loss = -11257.635270581464
1
Iteration 12200: Loss = -11257.634809323547
Iteration 12300: Loss = -11257.634910467828
1
Iteration 12400: Loss = -11257.634795249784
Iteration 12500: Loss = -11257.635369423917
1
Iteration 12600: Loss = -11257.634791077475
Iteration 12700: Loss = -11257.660601839803
1
Iteration 12800: Loss = -11257.634707776979
Iteration 12900: Loss = -11257.63436599743
Iteration 13000: Loss = -11257.633535097331
Iteration 13100: Loss = -11257.651923031624
1
Iteration 13200: Loss = -11257.654457211944
2
Iteration 13300: Loss = -11257.714526483265
3
Iteration 13400: Loss = -11257.637313648229
4
Iteration 13500: Loss = -11257.64040639979
5
Iteration 13600: Loss = -11257.633636593262
6
Iteration 13700: Loss = -11257.634186627221
7
Iteration 13800: Loss = -11257.633475426563
Iteration 13900: Loss = -11257.633679628318
1
Iteration 14000: Loss = -11257.633253896016
Iteration 14100: Loss = -11257.71687017706
1
Iteration 14200: Loss = -11257.632579613632
Iteration 14300: Loss = -11257.633841905632
1
Iteration 14400: Loss = -11257.646662578209
2
Iteration 14500: Loss = -11257.63347596706
3
Iteration 14600: Loss = -11257.635905814912
4
Iteration 14700: Loss = -11257.724449171239
5
Iteration 14800: Loss = -11257.632434832918
Iteration 14900: Loss = -11257.634313850149
1
Iteration 15000: Loss = -11257.632409957818
Iteration 15100: Loss = -11257.632583440478
1
Iteration 15200: Loss = -11257.638152398755
2
Iteration 15300: Loss = -11257.6324671542
Iteration 15400: Loss = -11257.698668916093
1
Iteration 15500: Loss = -11257.632591420772
2
Iteration 15600: Loss = -11257.764137437083
3
Iteration 15700: Loss = -11257.65153874924
4
Iteration 15800: Loss = -11257.632475551796
Iteration 15900: Loss = -11257.632481724137
Iteration 16000: Loss = -11257.637252672303
1
Iteration 16100: Loss = -11257.722175356192
2
Iteration 16200: Loss = -11257.632357851902
Iteration 16300: Loss = -11257.642201005525
1
Iteration 16400: Loss = -11257.632342102963
Iteration 16500: Loss = -11257.659045926192
1
Iteration 16600: Loss = -11257.626847042258
Iteration 16700: Loss = -11257.609223570173
Iteration 16800: Loss = -11257.609146460714
Iteration 16900: Loss = -11257.902607631859
1
Iteration 17000: Loss = -11257.608460046496
Iteration 17100: Loss = -11257.700344999057
1
Iteration 17200: Loss = -11257.608551665859
Iteration 17300: Loss = -11257.608476610132
Iteration 17400: Loss = -11257.608555450786
Iteration 17500: Loss = -11257.608665291029
1
Iteration 17600: Loss = -11257.608789527645
2
Iteration 17700: Loss = -11257.614416211085
3
Iteration 17800: Loss = -11257.640382457703
4
Iteration 17900: Loss = -11257.607146828761
Iteration 18000: Loss = -11255.139784032253
Iteration 18100: Loss = -11255.135559141865
Iteration 18200: Loss = -11255.134986141971
Iteration 18300: Loss = -11255.33063089463
1
Iteration 18400: Loss = -11255.134916812749
Iteration 18500: Loss = -11255.310197978619
1
Iteration 18600: Loss = -11255.134930495791
Iteration 18700: Loss = -11255.133672256396
Iteration 18800: Loss = -11255.13603086399
1
Iteration 18900: Loss = -11255.135019167737
2
Iteration 19000: Loss = -11255.133298470204
Iteration 19100: Loss = -11255.133140681004
Iteration 19200: Loss = -11255.13544747899
1
Iteration 19300: Loss = -11255.120714478939
Iteration 19400: Loss = -11255.118587308152
Iteration 19500: Loss = -11255.50928896575
1
Iteration 19600: Loss = -11255.118576227083
Iteration 19700: Loss = -11255.118949673502
1
Iteration 19800: Loss = -11255.118576707167
Iteration 19900: Loss = -11255.170379378691
1
pi: tensor([[0.2111, 0.7889],
        [0.7288, 0.2712]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5913, 0.4087], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2731, 0.0973],
         [0.7134, 0.2323]],

        [[0.6961, 0.0944],
         [0.5425, 0.5467]],

        [[0.6976, 0.0992],
         [0.6423, 0.6326]],

        [[0.6106, 0.0944],
         [0.6175, 0.5254]],

        [[0.6688, 0.1055],
         [0.5525, 0.7223]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721426378272603
time is 2
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.05008901330577659
Average Adjusted Rand Index: 0.8549483589552731
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20964.581563325373
Iteration 100: Loss = -11533.3675488522
Iteration 200: Loss = -11532.610021494036
Iteration 300: Loss = -11532.04532249499
Iteration 400: Loss = -11529.339533259938
Iteration 500: Loss = -11517.26202222569
Iteration 600: Loss = -11287.75945604962
Iteration 700: Loss = -11210.110769029072
Iteration 800: Loss = -11204.337536116676
Iteration 900: Loss = -11204.036430083417
Iteration 1000: Loss = -11203.85327629056
Iteration 1100: Loss = -11203.582480682826
Iteration 1200: Loss = -11203.127301419916
Iteration 1300: Loss = -11203.055045186695
Iteration 1400: Loss = -11202.780362572576
Iteration 1500: Loss = -11202.751602643946
Iteration 1600: Loss = -11202.735171425098
Iteration 1700: Loss = -11202.723034233366
Iteration 1800: Loss = -11202.713162964716
Iteration 1900: Loss = -11202.704705665146
Iteration 2000: Loss = -11202.696662092158
Iteration 2100: Loss = -11202.677844594342
Iteration 2200: Loss = -11202.627960091238
Iteration 2300: Loss = -11202.623051022076
Iteration 2400: Loss = -11202.619044508185
Iteration 2500: Loss = -11202.615319144124
Iteration 2600: Loss = -11202.61303535275
Iteration 2700: Loss = -11202.607699740951
Iteration 2800: Loss = -11202.602586435123
Iteration 2900: Loss = -11202.600188083328
Iteration 3000: Loss = -11202.593726937286
Iteration 3100: Loss = -11202.591684372042
Iteration 3200: Loss = -11202.590160261123
Iteration 3300: Loss = -11202.588544514241
Iteration 3400: Loss = -11202.587271671648
Iteration 3500: Loss = -11202.585708175511
Iteration 3600: Loss = -11202.584114096535
Iteration 3700: Loss = -11202.582942795616
Iteration 3800: Loss = -11202.579812680551
Iteration 3900: Loss = -11202.550399508935
Iteration 4000: Loss = -11202.548782723092
Iteration 4100: Loss = -11202.54680426944
Iteration 4200: Loss = -11202.54858743247
1
Iteration 4300: Loss = -11202.545270247168
Iteration 4400: Loss = -11202.545750218304
1
Iteration 4500: Loss = -11202.544293701889
Iteration 4600: Loss = -11202.542943771878
Iteration 4700: Loss = -11202.539264114792
Iteration 4800: Loss = -11202.5516801444
1
Iteration 4900: Loss = -11202.535758860537
Iteration 5000: Loss = -11202.537131770692
1
Iteration 5100: Loss = -11202.535033561857
Iteration 5200: Loss = -11202.53659958455
1
Iteration 5300: Loss = -11202.534446761392
Iteration 5400: Loss = -11202.539633474746
1
Iteration 5500: Loss = -11202.533898527967
Iteration 5600: Loss = -11202.533720874224
Iteration 5700: Loss = -11202.53338370469
Iteration 5800: Loss = -11202.533358769677
Iteration 5900: Loss = -11202.53293500945
Iteration 6000: Loss = -11202.534625170056
1
Iteration 6100: Loss = -11202.535612964872
2
Iteration 6200: Loss = -11202.532351780112
Iteration 6300: Loss = -11202.532221183457
Iteration 6400: Loss = -11202.53193845787
Iteration 6500: Loss = -11202.530241842409
Iteration 6600: Loss = -11202.526371366328
Iteration 6700: Loss = -11202.549231130311
1
Iteration 6800: Loss = -11202.53100890479
2
Iteration 6900: Loss = -11202.52130430039
Iteration 7000: Loss = -11202.539149220385
1
Iteration 7100: Loss = -11202.521087004696
Iteration 7200: Loss = -11202.536276064231
1
Iteration 7300: Loss = -11202.520861511106
Iteration 7400: Loss = -11202.520725082515
Iteration 7500: Loss = -11202.521014463899
1
Iteration 7600: Loss = -11202.520537958562
Iteration 7700: Loss = -11202.554228779227
1
Iteration 7800: Loss = -11202.520403817249
Iteration 7900: Loss = -11202.520325452204
Iteration 8000: Loss = -11202.531371863904
1
Iteration 8100: Loss = -11202.519780395429
Iteration 8200: Loss = -11202.516252542784
Iteration 8300: Loss = -11202.526857507404
1
Iteration 8400: Loss = -11202.516041591516
Iteration 8500: Loss = -11202.526246043628
1
Iteration 8600: Loss = -11202.515772941812
Iteration 8700: Loss = -11202.640217191785
1
Iteration 8800: Loss = -11202.515559045592
Iteration 8900: Loss = -11202.515499847557
Iteration 9000: Loss = -11202.516019679446
1
Iteration 9100: Loss = -11202.51544775925
Iteration 9200: Loss = -11202.515408794183
Iteration 9300: Loss = -11202.515350635515
Iteration 9400: Loss = -11202.515229869967
Iteration 9500: Loss = -11202.515802231854
1
Iteration 9600: Loss = -11202.515168403781
Iteration 9700: Loss = -11202.515115202401
Iteration 9800: Loss = -11202.58807272287
1
Iteration 9900: Loss = -11202.514669805047
Iteration 10000: Loss = -11202.530732428953
1
Iteration 10100: Loss = -11202.51458869429
Iteration 10200: Loss = -11202.514588017264
Iteration 10300: Loss = -11202.51731852764
1
Iteration 10400: Loss = -11202.514465431303
Iteration 10500: Loss = -11202.514450479724
Iteration 10600: Loss = -11202.515219983503
1
Iteration 10700: Loss = -11202.514380152596
Iteration 10800: Loss = -11202.55335185252
1
Iteration 10900: Loss = -11202.514238089641
Iteration 11000: Loss = -11202.513475872676
Iteration 11100: Loss = -11202.71207456867
1
Iteration 11200: Loss = -11202.513268762681
Iteration 11300: Loss = -11202.513241604613
Iteration 11400: Loss = -11202.951354584213
1
Iteration 11500: Loss = -11202.513224113574
Iteration 11600: Loss = -11202.513172887047
Iteration 11700: Loss = -11202.722357461575
1
Iteration 11800: Loss = -11202.513179095735
Iteration 11900: Loss = -11202.51689006141
1
Iteration 12000: Loss = -11202.51310892333
Iteration 12100: Loss = -11202.514221703554
1
Iteration 12200: Loss = -11202.513135915404
Iteration 12300: Loss = -11202.513784982933
1
Iteration 12400: Loss = -11202.519867019912
2
Iteration 12500: Loss = -11202.513086427678
Iteration 12600: Loss = -11202.513227225558
1
Iteration 12700: Loss = -11202.513032366907
Iteration 12800: Loss = -11202.513266674447
1
Iteration 12900: Loss = -11202.512985265268
Iteration 13000: Loss = -11202.513693992214
1
Iteration 13100: Loss = -11202.512971900114
Iteration 13200: Loss = -11202.552020199542
1
Iteration 13300: Loss = -11202.512743853307
Iteration 13400: Loss = -11202.512455645525
Iteration 13500: Loss = -11202.512507933885
Iteration 13600: Loss = -11202.512394619178
Iteration 13700: Loss = -11202.517058182033
1
Iteration 13800: Loss = -11202.511974219515
Iteration 13900: Loss = -11202.512578145106
1
Iteration 14000: Loss = -11202.511734831138
Iteration 14100: Loss = -11202.511633458653
Iteration 14200: Loss = -11202.601904160245
1
Iteration 14300: Loss = -11202.51164116654
Iteration 14400: Loss = -11202.511634436765
Iteration 14500: Loss = -11202.512321383838
1
Iteration 14600: Loss = -11202.511597451567
Iteration 14700: Loss = -11202.936606356081
1
Iteration 14800: Loss = -11202.511618997201
Iteration 14900: Loss = -11202.511613009667
Iteration 15000: Loss = -11202.546595238824
1
Iteration 15100: Loss = -11202.511310260401
Iteration 15200: Loss = -11202.511274606759
Iteration 15300: Loss = -11202.518607009582
1
Iteration 15400: Loss = -11202.511319189767
Iteration 15500: Loss = -11202.515070894611
1
Iteration 15600: Loss = -11202.511331099391
Iteration 15700: Loss = -11202.511431580988
1
Iteration 15800: Loss = -11202.737342524868
2
Iteration 15900: Loss = -11202.51131375813
Iteration 16000: Loss = -11202.512653529047
1
Iteration 16100: Loss = -11202.511319765044
Iteration 16200: Loss = -11202.511329918818
Iteration 16300: Loss = -11202.511377554098
Iteration 16400: Loss = -11202.521832801016
1
Iteration 16500: Loss = -11202.511244574156
Iteration 16600: Loss = -11202.532030813763
1
Iteration 16700: Loss = -11202.511243764431
Iteration 16800: Loss = -11202.538176950464
1
Iteration 16900: Loss = -11202.511223932013
Iteration 17000: Loss = -11202.538452370463
1
Iteration 17100: Loss = -11202.511237030008
Iteration 17200: Loss = -11202.571352026514
1
Iteration 17300: Loss = -11202.511220368824
Iteration 17400: Loss = -11202.511751997477
1
Iteration 17500: Loss = -11202.51122417207
Iteration 17600: Loss = -11202.511153835487
Iteration 17700: Loss = -11202.515261299093
1
Iteration 17800: Loss = -11202.511161135006
Iteration 17900: Loss = -11202.511375253784
1
Iteration 18000: Loss = -11202.511213762145
Iteration 18100: Loss = -11202.511161250268
Iteration 18200: Loss = -11202.586871584423
1
Iteration 18300: Loss = -11202.511174662155
Iteration 18400: Loss = -11202.511164607975
Iteration 18500: Loss = -11202.52238024969
1
Iteration 18600: Loss = -11202.511170443537
Iteration 18700: Loss = -11202.511144966458
Iteration 18800: Loss = -11202.528561015148
1
Iteration 18900: Loss = -11202.511154118636
Iteration 19000: Loss = -11202.534423368745
1
Iteration 19100: Loss = -11202.511756863143
2
Iteration 19200: Loss = -11202.51135929376
3
Iteration 19300: Loss = -11202.511180652376
Iteration 19400: Loss = -11202.51421200386
1
Iteration 19500: Loss = -11202.512039745232
2
Iteration 19600: Loss = -11202.511094625068
Iteration 19700: Loss = -11202.517227599808
1
Iteration 19800: Loss = -11202.511048774006
Iteration 19900: Loss = -11202.51097260122
pi: tensor([[0.7630, 0.2370],
        [0.2328, 0.7672]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5723, 0.4277], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2992, 0.0983],
         [0.7154, 0.1946]],

        [[0.6235, 0.0975],
         [0.6653, 0.5914]],

        [[0.6887, 0.1006],
         [0.6508, 0.7079]],

        [[0.6092, 0.0997],
         [0.6955, 0.7125]],

        [[0.5394, 0.1071],
         [0.5301, 0.5740]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9291543832751267
Average Adjusted Rand Index: 0.9292906561800287
11230.067217373846
[0.05008901330577659, 0.9291543832751267] [0.8549483589552731, 0.9292906561800287] [11255.117533188402, 11202.51121405753]
-------------------------------------
This iteration is 60
True Objective function: Loss = -11178.484209561346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22476.749331329906
Iteration 100: Loss = -11460.776929203023
Iteration 200: Loss = -11459.850602159087
Iteration 300: Loss = -11459.32644770264
Iteration 400: Loss = -11454.373174522687
Iteration 500: Loss = -11449.45358348622
Iteration 600: Loss = -11440.548354432072
Iteration 700: Loss = -11305.337752545658
Iteration 800: Loss = -11240.600065783268
Iteration 900: Loss = -11235.412302475379
Iteration 1000: Loss = -11231.724627268897
Iteration 1100: Loss = -11231.429887769174
Iteration 1200: Loss = -11231.344710015943
Iteration 1300: Loss = -11231.234758367678
Iteration 1400: Loss = -11230.70791552247
Iteration 1500: Loss = -11173.123138687934
Iteration 1600: Loss = -11169.038771447073
Iteration 1700: Loss = -11168.97448585694
Iteration 1800: Loss = -11168.932127755219
Iteration 1900: Loss = -11168.90621638388
Iteration 2000: Loss = -11168.896057563281
Iteration 2100: Loss = -11168.888295893026
Iteration 2200: Loss = -11168.877304944728
Iteration 2300: Loss = -11162.797557945387
Iteration 2400: Loss = -11162.786665197906
Iteration 2500: Loss = -11162.780841962009
Iteration 2600: Loss = -11162.67749064278
Iteration 2700: Loss = -11162.650912162171
Iteration 2800: Loss = -11162.650605236999
Iteration 2900: Loss = -11162.644926037337
Iteration 3000: Loss = -11162.640245233506
Iteration 3100: Loss = -11162.61624369501
Iteration 3200: Loss = -11162.613802093783
Iteration 3300: Loss = -11162.611520976423
Iteration 3400: Loss = -11162.610384512318
Iteration 3500: Loss = -11162.610383364581
Iteration 3600: Loss = -11162.610377160527
Iteration 3700: Loss = -11162.605316312507
Iteration 3800: Loss = -11162.595670381888
Iteration 3900: Loss = -11162.59307355759
Iteration 4000: Loss = -11162.592374117614
Iteration 4100: Loss = -11162.591862612466
Iteration 4200: Loss = -11162.591409336934
Iteration 4300: Loss = -11162.591071162708
Iteration 4400: Loss = -11162.592582496482
1
Iteration 4500: Loss = -11162.590343405598
Iteration 4600: Loss = -11162.591143060537
1
Iteration 4700: Loss = -11162.59772049215
2
Iteration 4800: Loss = -11162.589488198515
Iteration 4900: Loss = -11162.589244181665
Iteration 5000: Loss = -11162.58902231633
Iteration 5100: Loss = -11162.588748776921
Iteration 5200: Loss = -11162.588930866305
1
Iteration 5300: Loss = -11162.601552051197
2
Iteration 5400: Loss = -11162.590457826976
3
Iteration 5500: Loss = -11162.588663418484
Iteration 5600: Loss = -11162.587679044864
Iteration 5700: Loss = -11162.587346346898
Iteration 5800: Loss = -11162.587192756644
Iteration 5900: Loss = -11162.58819534811
1
Iteration 6000: Loss = -11162.587130903306
Iteration 6100: Loss = -11162.586775485881
Iteration 6200: Loss = -11162.586581278327
Iteration 6300: Loss = -11162.586441483581
Iteration 6400: Loss = -11162.603335907479
1
Iteration 6500: Loss = -11162.58619574727
Iteration 6600: Loss = -11162.586158282757
Iteration 6700: Loss = -11162.586044865198
Iteration 6800: Loss = -11162.60997867797
1
Iteration 6900: Loss = -11162.585659053822
Iteration 7000: Loss = -11162.585474948186
Iteration 7100: Loss = -11162.598438470062
1
Iteration 7200: Loss = -11162.58534523123
Iteration 7300: Loss = -11162.588629858914
1
Iteration 7400: Loss = -11162.585213096616
Iteration 7500: Loss = -11162.585194770973
Iteration 7600: Loss = -11162.59282069389
1
Iteration 7700: Loss = -11162.585050928134
Iteration 7800: Loss = -11162.584976010665
Iteration 7900: Loss = -11162.584957787714
Iteration 8000: Loss = -11162.587406407536
1
Iteration 8100: Loss = -11162.58634285943
2
Iteration 8200: Loss = -11162.584563816861
Iteration 8300: Loss = -11162.580982065423
Iteration 8400: Loss = -11162.569169914384
Iteration 8500: Loss = -11162.56918453128
Iteration 8600: Loss = -11162.570601685273
1
Iteration 8700: Loss = -11162.567798937549
Iteration 8800: Loss = -11162.58541569831
1
Iteration 8900: Loss = -11162.567721957106
Iteration 9000: Loss = -11162.57117660416
1
Iteration 9100: Loss = -11162.567716637117
Iteration 9200: Loss = -11162.567796161662
Iteration 9300: Loss = -11162.568349803121
1
Iteration 9400: Loss = -11162.568248907592
2
Iteration 9500: Loss = -11162.56820347755
3
Iteration 9600: Loss = -11162.5807706112
4
Iteration 9700: Loss = -11162.56889323403
5
Iteration 9800: Loss = -11162.568115605389
6
Iteration 9900: Loss = -11162.57961335743
7
Iteration 10000: Loss = -11162.569387104455
8
Iteration 10100: Loss = -11162.56958745432
9
Iteration 10200: Loss = -11162.574665742215
10
Iteration 10300: Loss = -11162.567354860006
Iteration 10400: Loss = -11162.56876353841
1
Iteration 10500: Loss = -11162.582491356778
2
Iteration 10600: Loss = -11162.569859877123
3
Iteration 10700: Loss = -11162.567230104714
Iteration 10800: Loss = -11162.567143697746
Iteration 10900: Loss = -11162.56752973432
1
Iteration 11000: Loss = -11162.568671216723
2
Iteration 11100: Loss = -11162.567920748726
3
Iteration 11200: Loss = -11162.567327634515
4
Iteration 11300: Loss = -11162.56723757873
Iteration 11400: Loss = -11162.569875843088
1
Iteration 11500: Loss = -11162.570970761124
2
Iteration 11600: Loss = -11162.577847567727
3
Iteration 11700: Loss = -11162.652240916981
4
Iteration 11800: Loss = -11162.567112231278
Iteration 11900: Loss = -11162.637467211996
1
Iteration 12000: Loss = -11162.568472286377
2
Iteration 12100: Loss = -11162.572513378955
3
Iteration 12200: Loss = -11162.572498652697
4
Iteration 12300: Loss = -11162.567086440833
Iteration 12400: Loss = -11162.567404494514
1
Iteration 12500: Loss = -11162.581418122032
2
Iteration 12600: Loss = -11162.567078331225
Iteration 12700: Loss = -11162.58875240568
1
Iteration 12800: Loss = -11162.580649426596
2
Iteration 12900: Loss = -11162.590522187102
3
Iteration 13000: Loss = -11162.567334587198
4
Iteration 13100: Loss = -11162.568442317894
5
Iteration 13200: Loss = -11162.788684575136
6
Iteration 13300: Loss = -11162.579631062428
7
Iteration 13400: Loss = -11162.568159534629
8
Iteration 13500: Loss = -11162.57078453407
9
Iteration 13600: Loss = -11162.593941059395
10
Iteration 13700: Loss = -11162.567776577154
11
Iteration 13800: Loss = -11162.570156712141
12
Iteration 13900: Loss = -11162.56697097032
Iteration 14000: Loss = -11162.571278621943
1
Iteration 14100: Loss = -11162.56754875183
2
Iteration 14200: Loss = -11162.569390722427
3
Iteration 14300: Loss = -11162.57700777593
4
Iteration 14400: Loss = -11162.577092052725
5
Iteration 14500: Loss = -11162.56700554889
Iteration 14600: Loss = -11162.573467321597
1
Iteration 14700: Loss = -11162.56701978247
Iteration 14800: Loss = -11162.573064686456
1
Iteration 14900: Loss = -11162.60387062824
2
Iteration 15000: Loss = -11162.592495241435
3
Iteration 15100: Loss = -11162.594136688813
4
Iteration 15200: Loss = -11162.57912911217
5
Iteration 15300: Loss = -11162.680534514271
6
Iteration 15400: Loss = -11162.56935946588
7
Iteration 15500: Loss = -11162.568063048422
8
Iteration 15600: Loss = -11162.568018899456
9
Iteration 15700: Loss = -11162.566937926335
Iteration 15800: Loss = -11162.572288404095
1
Iteration 15900: Loss = -11162.570288510571
2
Iteration 16000: Loss = -11162.567956760242
3
Iteration 16100: Loss = -11162.571842528829
4
Iteration 16200: Loss = -11162.572291901584
5
Iteration 16300: Loss = -11162.575609478334
6
Iteration 16400: Loss = -11162.568426124608
7
Iteration 16500: Loss = -11162.56724240079
8
Iteration 16600: Loss = -11162.586261172946
9
Iteration 16700: Loss = -11162.573238263065
10
Iteration 16800: Loss = -11162.614567000479
11
Iteration 16900: Loss = -11162.609550777564
12
Iteration 17000: Loss = -11162.569066362803
13
Iteration 17100: Loss = -11162.567683242743
14
Iteration 17200: Loss = -11162.569645024305
15
Stopping early at iteration 17200 due to no improvement.
pi: tensor([[0.7799, 0.2201],
        [0.2419, 0.7581]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4837, 0.5163], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2914, 0.1038],
         [0.6768, 0.2046]],

        [[0.7152, 0.1009],
         [0.7120, 0.5787]],

        [[0.5497, 0.0943],
         [0.5925, 0.5768]],

        [[0.6323, 0.0959],
         [0.5799, 0.6429]],

        [[0.6342, 0.1030],
         [0.5268, 0.5666]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.960320569116647
Average Adjusted Rand Index: 0.9603209670298476
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21012.221715920885
Iteration 100: Loss = -11460.046734495168
Iteration 200: Loss = -11456.603642479653
Iteration 300: Loss = -11455.85214382233
Iteration 400: Loss = -11455.62770225716
Iteration 500: Loss = -11455.158238450313
Iteration 600: Loss = -11454.910233001625
Iteration 700: Loss = -11454.840652260016
Iteration 800: Loss = -11454.778989796358
Iteration 900: Loss = -11454.698556943593
Iteration 1000: Loss = -11454.529652977599
Iteration 1100: Loss = -11453.465300302525
Iteration 1200: Loss = -11451.769054353304
Iteration 1300: Loss = -11451.646949393993
Iteration 1400: Loss = -11451.612198929975
Iteration 1500: Loss = -11451.597216627626
Iteration 1600: Loss = -11451.589069543817
Iteration 1700: Loss = -11451.583765183337
Iteration 1800: Loss = -11451.579759717672
Iteration 1900: Loss = -11451.576571613476
Iteration 2000: Loss = -11451.573954305724
Iteration 2100: Loss = -11451.571727799213
Iteration 2200: Loss = -11451.56973745336
Iteration 2300: Loss = -11451.56802107491
Iteration 2400: Loss = -11451.566493468883
Iteration 2500: Loss = -11451.565055297102
Iteration 2600: Loss = -11451.56378594709
Iteration 2700: Loss = -11451.562575461614
Iteration 2800: Loss = -11451.561478031903
Iteration 2900: Loss = -11451.560410118636
Iteration 3000: Loss = -11451.559423392213
Iteration 3100: Loss = -11451.558365662568
Iteration 3200: Loss = -11451.557387612142
Iteration 3300: Loss = -11451.55636412118
Iteration 3400: Loss = -11451.555212743731
Iteration 3500: Loss = -11451.554014922185
Iteration 3600: Loss = -11451.552566023023
Iteration 3700: Loss = -11451.550783107064
Iteration 3800: Loss = -11451.548342313205
Iteration 3900: Loss = -11451.544608228105
Iteration 4000: Loss = -11451.540048738576
Iteration 4100: Loss = -11451.51873123664
Iteration 4200: Loss = -11451.361015660525
Iteration 4300: Loss = -11449.968957705965
Iteration 4400: Loss = -11449.754811098152
Iteration 4500: Loss = -11449.690522240975
Iteration 4600: Loss = -11449.684127418052
Iteration 4700: Loss = -11449.683105026947
Iteration 4800: Loss = -11449.685562936467
1
Iteration 4900: Loss = -11449.682707127477
Iteration 5000: Loss = -11449.686920154561
1
Iteration 5100: Loss = -11449.682513609245
Iteration 5200: Loss = -11449.682433298103
Iteration 5300: Loss = -11449.682404866095
Iteration 5400: Loss = -11449.682366658057
Iteration 5500: Loss = -11449.682755932177
1
Iteration 5600: Loss = -11449.682263225215
Iteration 5700: Loss = -11449.68229251487
Iteration 5800: Loss = -11449.682198656654
Iteration 5900: Loss = -11449.682174950929
Iteration 6000: Loss = -11449.682150417186
Iteration 6100: Loss = -11449.682105576418
Iteration 6200: Loss = -11449.682472747445
1
Iteration 6300: Loss = -11449.682059582034
Iteration 6400: Loss = -11449.682068113249
Iteration 6500: Loss = -11449.682790033907
1
Iteration 6600: Loss = -11449.682012273897
Iteration 6700: Loss = -11449.68196745325
Iteration 6800: Loss = -11449.681988108487
Iteration 6900: Loss = -11449.68193717822
Iteration 7000: Loss = -11449.682126649072
1
Iteration 7100: Loss = -11449.681920037652
Iteration 7200: Loss = -11449.681930801065
Iteration 7300: Loss = -11449.681955066939
Iteration 7400: Loss = -11449.717458924482
1
Iteration 7500: Loss = -11449.681855789056
Iteration 7600: Loss = -11449.681907226892
Iteration 7700: Loss = -11449.737433429284
1
Iteration 7800: Loss = -11449.681837015578
Iteration 7900: Loss = -11449.729064326493
1
Iteration 8000: Loss = -11449.681857639802
Iteration 8100: Loss = -11449.701328941484
1
Iteration 8200: Loss = -11449.68180531689
Iteration 8300: Loss = -11449.683389487178
1
Iteration 8400: Loss = -11449.681788123253
Iteration 8500: Loss = -11449.681765494883
Iteration 8600: Loss = -11450.073582075056
1
Iteration 8700: Loss = -11449.681758414152
Iteration 8800: Loss = -11449.681789839946
Iteration 8900: Loss = -11449.720104088603
1
Iteration 9000: Loss = -11449.681764287125
Iteration 9100: Loss = -11449.681740604789
Iteration 9200: Loss = -11449.921366035662
1
Iteration 9300: Loss = -11449.681729063823
Iteration 9400: Loss = -11449.681728933883
Iteration 9500: Loss = -11450.345577461472
1
Iteration 9600: Loss = -11449.68172748124
Iteration 9700: Loss = -11449.681706561181
Iteration 9800: Loss = -11449.681710609184
Iteration 9900: Loss = -11449.682280068733
1
Iteration 10000: Loss = -11449.681706041038
Iteration 10100: Loss = -11449.681689761168
Iteration 10200: Loss = -11449.686543620563
1
Iteration 10300: Loss = -11449.681698399247
Iteration 10400: Loss = -11449.681693939414
Iteration 10500: Loss = -11449.785577066306
1
Iteration 10600: Loss = -11449.681681276228
Iteration 10700: Loss = -11449.681689851324
Iteration 10800: Loss = -11449.922007503981
1
Iteration 10900: Loss = -11449.68393261367
2
Iteration 11000: Loss = -11449.681677293192
Iteration 11100: Loss = -11449.68290226272
1
Iteration 11200: Loss = -11449.682131960486
2
Iteration 11300: Loss = -11449.68174749052
Iteration 11400: Loss = -11449.967019414822
1
Iteration 11500: Loss = -11449.68168731299
Iteration 11600: Loss = -11449.694285456915
1
Iteration 11700: Loss = -11449.68947105473
2
Iteration 11800: Loss = -11449.681738391442
Iteration 11900: Loss = -11449.687001111528
1
Iteration 12000: Loss = -11449.684154842185
2
Iteration 12100: Loss = -11449.681681494942
Iteration 12200: Loss = -11449.695157948925
1
Iteration 12300: Loss = -11449.681681523243
Iteration 12400: Loss = -11449.6848415211
1
Iteration 12500: Loss = -11449.737404354839
2
Iteration 12600: Loss = -11449.689266872901
3
Iteration 12700: Loss = -11449.681984826695
4
Iteration 12800: Loss = -11449.68176492206
Iteration 12900: Loss = -11449.682603708627
1
Iteration 13000: Loss = -11449.697620198614
2
Iteration 13100: Loss = -11449.776525162542
3
Iteration 13200: Loss = -11449.71206638071
4
Iteration 13300: Loss = -11449.686720998036
5
Iteration 13400: Loss = -11449.682313570094
6
Iteration 13500: Loss = -11449.704392245121
7
Iteration 13600: Loss = -11449.68266492132
8
Iteration 13700: Loss = -11449.682192410073
9
Iteration 13800: Loss = -11449.722953361908
10
Iteration 13900: Loss = -11449.684254202353
11
Iteration 14000: Loss = -11449.682813206424
12
Iteration 14100: Loss = -11449.684429219627
13
Iteration 14200: Loss = -11449.729614998048
14
Iteration 14300: Loss = -11449.695719860863
15
Stopping early at iteration 14300 due to no improvement.
pi: tensor([[1.8518e-06, 1.0000e+00],
        [6.9972e-02, 9.3003e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1012, 0.8988], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1088, 0.1286],
         [0.5758, 0.1814]],

        [[0.7043, 0.1077],
         [0.6660, 0.7220]],

        [[0.6086, 0.0878],
         [0.5806, 0.6615]],

        [[0.6033, 0.1147],
         [0.6099, 0.6071]],

        [[0.5925, 0.2684],
         [0.5636, 0.6170]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.011530202595462608
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.012148896880163282
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.030303030303030304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.008125898971292667
Global Adjusted Rand Index: 0.0022753246940548423
Average Adjusted Rand Index: 0.012932375277287031
11178.484209561346
[0.960320569116647, 0.0022753246940548423] [0.9603209670298476, 0.012932375277287031] [11162.569645024305, 11449.695719860863]
-------------------------------------
This iteration is 61
True Objective function: Loss = -11155.919735912514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21803.002995063198
Iteration 100: Loss = -11362.349316376045
Iteration 200: Loss = -11361.660082579227
Iteration 300: Loss = -11361.429805021553
Iteration 400: Loss = -11361.325233660238
Iteration 500: Loss = -11361.264703912771
Iteration 600: Loss = -11361.217236752978
Iteration 700: Loss = -11361.171000964037
Iteration 800: Loss = -11361.119852635775
Iteration 900: Loss = -11361.059498187835
Iteration 1000: Loss = -11360.985038899462
Iteration 1100: Loss = -11360.889598740032
Iteration 1200: Loss = -11360.763730595061
Iteration 1300: Loss = -11360.60287154578
Iteration 1400: Loss = -11360.42915731711
Iteration 1500: Loss = -11360.266335665812
Iteration 1600: Loss = -11360.095139212173
Iteration 1700: Loss = -11359.87751439202
Iteration 1800: Loss = -11359.52255442633
Iteration 1900: Loss = -11358.805469550563
Iteration 2000: Loss = -11187.264275214595
Iteration 2100: Loss = -11186.163638880062
Iteration 2200: Loss = -11185.790195191628
Iteration 2300: Loss = -11185.716934388316
Iteration 2400: Loss = -11185.650881091504
Iteration 2500: Loss = -11185.618933765276
Iteration 2600: Loss = -11185.59524711067
Iteration 2700: Loss = -11185.478862193198
Iteration 2800: Loss = -11182.379305766166
Iteration 2900: Loss = -11182.327298015103
Iteration 3000: Loss = -11182.308567545993
Iteration 3100: Loss = -11182.290772131453
Iteration 3200: Loss = -11182.279629373017
Iteration 3300: Loss = -11182.27213371303
Iteration 3400: Loss = -11182.266201975019
Iteration 3500: Loss = -11182.263348101551
Iteration 3600: Loss = -11182.260297120754
Iteration 3700: Loss = -11182.258167580932
Iteration 3800: Loss = -11182.255964476466
Iteration 3900: Loss = -11182.25471849929
Iteration 4000: Loss = -11182.25174476403
Iteration 4100: Loss = -11182.250113159414
Iteration 4200: Loss = -11181.616844427786
Iteration 4300: Loss = -11181.530446630104
Iteration 4400: Loss = -11181.528077963598
Iteration 4500: Loss = -11181.526514065094
Iteration 4600: Loss = -11181.526702297533
1
Iteration 4700: Loss = -11181.524828725116
Iteration 4800: Loss = -11181.523546452963
Iteration 4900: Loss = -11181.525856997258
1
Iteration 5000: Loss = -11181.521658649379
Iteration 5100: Loss = -11181.521006186738
Iteration 5200: Loss = -11181.519949929065
Iteration 5300: Loss = -11181.519366808221
Iteration 5400: Loss = -11181.518704919092
Iteration 5500: Loss = -11181.517433991738
Iteration 5600: Loss = -11181.513970215985
Iteration 5700: Loss = -11181.509334269103
Iteration 5800: Loss = -11181.50780254547
Iteration 5900: Loss = -11181.507518691793
Iteration 6000: Loss = -11181.507294122699
Iteration 6100: Loss = -11181.507069320898
Iteration 6200: Loss = -11181.523365252713
1
Iteration 6300: Loss = -11181.506675359611
Iteration 6400: Loss = -11181.506522465526
Iteration 6500: Loss = -11181.506410937644
Iteration 6600: Loss = -11181.506087362475
Iteration 6700: Loss = -11181.505905502629
Iteration 6800: Loss = -11181.506722956434
1
Iteration 6900: Loss = -11181.505258750401
Iteration 7000: Loss = -11181.505208954055
Iteration 7100: Loss = -11181.504330405656
Iteration 7200: Loss = -11181.504211244423
Iteration 7300: Loss = -11181.503848720215
Iteration 7400: Loss = -11181.504082713886
1
Iteration 7500: Loss = -11181.503564870867
Iteration 7600: Loss = -11181.50473560946
1
Iteration 7700: Loss = -11181.503303565833
Iteration 7800: Loss = -11181.505872960099
1
Iteration 7900: Loss = -11181.504829094769
2
Iteration 8000: Loss = -11181.503712710448
3
Iteration 8100: Loss = -11181.502414561774
Iteration 8200: Loss = -11181.512465653406
1
Iteration 8300: Loss = -11181.501939777085
Iteration 8400: Loss = -11181.501423409194
Iteration 8500: Loss = -11181.505504119637
1
Iteration 8600: Loss = -11181.500906430398
Iteration 8700: Loss = -11181.500859693459
Iteration 8800: Loss = -11181.521338086573
1
Iteration 8900: Loss = -11181.500736153443
Iteration 9000: Loss = -11181.500708042393
Iteration 9100: Loss = -11181.500698338108
Iteration 9200: Loss = -11181.50063015513
Iteration 9300: Loss = -11181.516450623432
1
Iteration 9400: Loss = -11181.500545743203
Iteration 9500: Loss = -11181.50053069595
Iteration 9600: Loss = -11181.50078485821
1
Iteration 9700: Loss = -11181.500433844487
Iteration 9800: Loss = -11181.500496128732
Iteration 9900: Loss = -11181.500306976739
Iteration 10000: Loss = -11181.500288836292
Iteration 10100: Loss = -11182.02445544293
1
Iteration 10200: Loss = -11181.500290384362
Iteration 10300: Loss = -11181.500262997366
Iteration 10400: Loss = -11181.5035868664
1
Iteration 10500: Loss = -11181.500300987795
Iteration 10600: Loss = -11181.500276122135
Iteration 10700: Loss = -11181.500230669479
Iteration 10800: Loss = -11181.50024335107
Iteration 10900: Loss = -11181.50017976344
Iteration 11000: Loss = -11181.5001998019
Iteration 11100: Loss = -11181.500135510923
Iteration 11200: Loss = -11181.520061286128
1
Iteration 11300: Loss = -11181.500059819555
Iteration 11400: Loss = -11181.502531650269
1
Iteration 11500: Loss = -11181.500441440585
2
Iteration 11600: Loss = -11181.50007323685
Iteration 11700: Loss = -11181.500182416592
1
Iteration 11800: Loss = -11181.499958723522
Iteration 11900: Loss = -11181.500101143283
1
Iteration 12000: Loss = -11181.499948674413
Iteration 12100: Loss = -11181.507233725742
1
Iteration 12200: Loss = -11181.499947685223
Iteration 12300: Loss = -11181.499919290116
Iteration 12400: Loss = -11181.501004085494
1
Iteration 12500: Loss = -11181.499941712518
Iteration 12600: Loss = -11181.499949121953
Iteration 12700: Loss = -11181.5002073758
1
Iteration 12800: Loss = -11181.499897711361
Iteration 12900: Loss = -11181.499904697115
Iteration 13000: Loss = -11181.500063485344
1
Iteration 13100: Loss = -11181.49993659761
Iteration 13200: Loss = -11181.504549150814
1
Iteration 13300: Loss = -11181.499916197166
Iteration 13400: Loss = -11181.499910386838
Iteration 13500: Loss = -11181.52736428248
1
Iteration 13600: Loss = -11181.499887612852
Iteration 13700: Loss = -11181.499874928253
Iteration 13800: Loss = -11181.500010440972
1
Iteration 13900: Loss = -11181.50353738282
2
Iteration 14000: Loss = -11181.499947400822
Iteration 14100: Loss = -11181.50238601785
1
Iteration 14200: Loss = -11181.49988194038
Iteration 14300: Loss = -11181.571463548125
1
Iteration 14400: Loss = -11181.50341494807
2
Iteration 14500: Loss = -11181.505359879053
3
Iteration 14600: Loss = -11181.524970631619
4
Iteration 14700: Loss = -11181.50010905662
5
Iteration 14800: Loss = -11181.499956706053
Iteration 14900: Loss = -11181.50959013835
1
Iteration 15000: Loss = -11181.499892187216
Iteration 15100: Loss = -11181.513840766784
1
Iteration 15200: Loss = -11181.499894244034
Iteration 15300: Loss = -11181.522630963607
1
Iteration 15400: Loss = -11181.499904876171
Iteration 15500: Loss = -11181.501529686973
1
Iteration 15600: Loss = -11181.499876416881
Iteration 15700: Loss = -11181.51954572723
1
Iteration 15800: Loss = -11181.499894808812
Iteration 15900: Loss = -11181.563844202445
1
Iteration 16000: Loss = -11181.4998775036
Iteration 16100: Loss = -11181.686512510096
1
Iteration 16200: Loss = -11181.499892257842
Iteration 16300: Loss = -11181.499899093309
Iteration 16400: Loss = -11181.49991874216
Iteration 16500: Loss = -11181.499889700423
Iteration 16600: Loss = -11181.511458986679
1
Iteration 16700: Loss = -11181.499878134617
Iteration 16800: Loss = -11181.499848317362
Iteration 16900: Loss = -11181.49995508317
1
Iteration 17000: Loss = -11181.499974860157
2
Iteration 17100: Loss = -11181.499929293674
Iteration 17200: Loss = -11181.500224173842
1
Iteration 17300: Loss = -11181.499952680646
Iteration 17400: Loss = -11181.509644858359
1
Iteration 17500: Loss = -11181.499905274846
Iteration 17600: Loss = -11181.499919454132
Iteration 17700: Loss = -11181.500451706801
1
Iteration 17800: Loss = -11181.49993181574
Iteration 17900: Loss = -11181.499954565774
Iteration 18000: Loss = -11181.499880259451
Iteration 18100: Loss = -11181.500977962234
1
Iteration 18200: Loss = -11181.499867308363
Iteration 18300: Loss = -11181.583790350956
1
Iteration 18400: Loss = -11181.499892575639
Iteration 18500: Loss = -11181.499908037806
Iteration 18600: Loss = -11181.568382442078
1
Iteration 18700: Loss = -11181.499861782098
Iteration 18800: Loss = -11181.499930242897
Iteration 18900: Loss = -11181.501118273125
1
Iteration 19000: Loss = -11181.499920409262
Iteration 19100: Loss = -11181.532293204113
1
Iteration 19200: Loss = -11181.499881122503
Iteration 19300: Loss = -11181.499881029065
Iteration 19400: Loss = -11181.499998180096
1
Iteration 19500: Loss = -11181.499897900483
Iteration 19600: Loss = -11181.509016806267
1
Iteration 19700: Loss = -11181.499887258122
Iteration 19800: Loss = -11181.713342086385
1
Iteration 19900: Loss = -11181.49986307294
pi: tensor([[0.7615, 0.2385],
        [0.2771, 0.7229]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0285, 0.9715], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3027, 0.0828],
         [0.6440, 0.1895]],

        [[0.6225, 0.0970],
         [0.6153, 0.6277]],

        [[0.5109, 0.1035],
         [0.5817, 0.5420]],

        [[0.6711, 0.1036],
         [0.6810, 0.6885]],

        [[0.7159, 0.1109],
         [0.6460, 0.5633]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: -0.004063414211148451
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9204282407407407
time is 2
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080740404436667
time is 3
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844845420066659
Global Adjusted Rand Index: 0.6324210072298968
Average Adjusted Rand Index: 0.7138568574079837
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19452.73550978864
Iteration 100: Loss = -11361.464826867836
Iteration 200: Loss = -11361.172667630892
Iteration 300: Loss = -11360.98942420798
Iteration 400: Loss = -11360.778249278557
Iteration 500: Loss = -11360.37699371106
Iteration 600: Loss = -11359.827251569977
Iteration 700: Loss = -11359.298749059064
Iteration 800: Loss = -11337.846450455636
Iteration 900: Loss = -11185.740555837681
Iteration 1000: Loss = -11185.064914539334
Iteration 1100: Loss = -11181.975046935771
Iteration 1200: Loss = -11181.708563612077
Iteration 1300: Loss = -11181.656227046617
Iteration 1400: Loss = -11181.585100990376
Iteration 1500: Loss = -11181.561410603294
Iteration 1600: Loss = -11181.550454725024
Iteration 1700: Loss = -11181.545370374448
Iteration 1800: Loss = -11181.54146366842
Iteration 1900: Loss = -11181.53817511535
Iteration 2000: Loss = -11181.534263566826
Iteration 2100: Loss = -11181.522731928844
Iteration 2200: Loss = -11181.515793019573
Iteration 2300: Loss = -11181.510793671798
Iteration 2400: Loss = -11181.509820875139
Iteration 2500: Loss = -11181.509046681595
Iteration 2600: Loss = -11181.508364803953
Iteration 2700: Loss = -11181.507792589415
Iteration 2800: Loss = -11181.507243134994
Iteration 2900: Loss = -11181.506756877017
Iteration 3000: Loss = -11181.506491681033
Iteration 3100: Loss = -11181.506036195135
Iteration 3200: Loss = -11181.505708131084
Iteration 3300: Loss = -11181.505439238139
Iteration 3400: Loss = -11181.507746908628
1
Iteration 3500: Loss = -11181.504917010361
Iteration 3600: Loss = -11181.504715817031
Iteration 3700: Loss = -11181.50432445532
Iteration 3800: Loss = -11181.504025532651
Iteration 3900: Loss = -11181.50318514228
Iteration 4000: Loss = -11181.501811290751
Iteration 4100: Loss = -11181.501481634643
Iteration 4200: Loss = -11181.501438099558
Iteration 4300: Loss = -11181.501314207915
Iteration 4400: Loss = -11181.501246724547
Iteration 4500: Loss = -11181.501241208796
Iteration 4600: Loss = -11181.50107964109
Iteration 4700: Loss = -11181.501003302119
Iteration 4800: Loss = -11181.500890242412
Iteration 4900: Loss = -11181.50083054687
Iteration 5000: Loss = -11181.501584974902
1
Iteration 5100: Loss = -11181.500695960409
Iteration 5200: Loss = -11181.50061299198
Iteration 5300: Loss = -11181.506347621747
1
Iteration 5400: Loss = -11181.500494492368
Iteration 5500: Loss = -11181.500436691453
Iteration 5600: Loss = -11181.500481059076
Iteration 5700: Loss = -11181.500370967684
Iteration 5800: Loss = -11181.501035453492
1
Iteration 5900: Loss = -11181.500298566154
Iteration 6000: Loss = -11181.500349610247
Iteration 6100: Loss = -11181.500237608321
Iteration 6200: Loss = -11181.500457738284
1
Iteration 6300: Loss = -11181.500223840296
Iteration 6400: Loss = -11181.507002820392
1
Iteration 6500: Loss = -11181.500168929688
Iteration 6600: Loss = -11181.500181369562
Iteration 6700: Loss = -11181.500592333901
1
Iteration 6800: Loss = -11181.50012817936
Iteration 6900: Loss = -11181.5001279177
Iteration 7000: Loss = -11181.500288459809
1
Iteration 7100: Loss = -11181.500083786827
Iteration 7200: Loss = -11181.500079312727
Iteration 7300: Loss = -11181.500089374777
Iteration 7400: Loss = -11181.50029195176
1
Iteration 7500: Loss = -11181.50658572016
2
Iteration 7600: Loss = -11181.500672477858
3
Iteration 7700: Loss = -11181.500840317163
4
Iteration 7800: Loss = -11181.500388931507
5
Iteration 7900: Loss = -11181.500054769223
Iteration 8000: Loss = -11181.500067519914
Iteration 8100: Loss = -11181.500019722696
Iteration 8200: Loss = -11181.502561365704
1
Iteration 8300: Loss = -11181.500008216111
Iteration 8400: Loss = -11181.500125237642
1
Iteration 8500: Loss = -11181.499961278674
Iteration 8600: Loss = -11181.500264799739
1
Iteration 8700: Loss = -11181.500063150672
2
Iteration 8800: Loss = -11181.499955279736
Iteration 8900: Loss = -11181.519547443726
1
Iteration 9000: Loss = -11181.499957590859
Iteration 9100: Loss = -11181.500369841788
1
Iteration 9200: Loss = -11181.499977726608
Iteration 9300: Loss = -11181.49991754122
Iteration 9400: Loss = -11181.501720862369
1
Iteration 9500: Loss = -11181.499946578544
Iteration 9600: Loss = -11181.500244176019
1
Iteration 9700: Loss = -11181.499905724917
Iteration 9800: Loss = -11181.499893654964
Iteration 9900: Loss = -11181.505703088347
1
Iteration 10000: Loss = -11181.499926965813
Iteration 10100: Loss = -11181.49991717066
Iteration 10200: Loss = -11181.499890347526
Iteration 10300: Loss = -11181.50008944342
1
Iteration 10400: Loss = -11181.499906848137
Iteration 10500: Loss = -11181.499899999506
Iteration 10600: Loss = -11181.49993372822
Iteration 10700: Loss = -11181.499889889821
Iteration 10800: Loss = -11181.537942878582
1
Iteration 10900: Loss = -11181.49989907351
Iteration 11000: Loss = -11181.499895521509
Iteration 11100: Loss = -11181.502460893353
1
Iteration 11200: Loss = -11181.499902010679
Iteration 11300: Loss = -11181.49988449095
Iteration 11400: Loss = -11181.50285735478
1
Iteration 11500: Loss = -11181.499880517838
Iteration 11600: Loss = -11181.499884732846
Iteration 11700: Loss = -11181.500069226051
1
Iteration 11800: Loss = -11181.499903979682
Iteration 11900: Loss = -11181.533050948927
1
Iteration 12000: Loss = -11181.499899616201
Iteration 12100: Loss = -11181.501831395619
1
Iteration 12200: Loss = -11181.499917049901
Iteration 12300: Loss = -11181.515089143339
1
Iteration 12400: Loss = -11181.501421124118
2
Iteration 12500: Loss = -11181.499874281133
Iteration 12600: Loss = -11181.50226939633
1
Iteration 12700: Loss = -11181.49988316765
Iteration 12800: Loss = -11181.508230737672
1
Iteration 12900: Loss = -11181.49986857678
Iteration 13000: Loss = -11181.49989603104
Iteration 13100: Loss = -11181.499958496057
Iteration 13200: Loss = -11181.49983973728
Iteration 13300: Loss = -11181.709369945294
1
Iteration 13400: Loss = -11181.499863117364
Iteration 13500: Loss = -11181.499879868747
Iteration 13600: Loss = -11181.502723008056
1
Iteration 13700: Loss = -11181.499882548544
Iteration 13800: Loss = -11181.508579477726
1
Iteration 13900: Loss = -11181.499933174686
Iteration 14000: Loss = -11181.499991835422
Iteration 14100: Loss = -11181.50042691484
1
Iteration 14200: Loss = -11181.499880748253
Iteration 14300: Loss = -11181.509882835613
1
Iteration 14400: Loss = -11181.499851549364
Iteration 14500: Loss = -11181.50043119392
1
Iteration 14600: Loss = -11181.499895819034
Iteration 14700: Loss = -11181.500226052598
1
Iteration 14800: Loss = -11181.499901301451
Iteration 14900: Loss = -11181.500332913709
1
Iteration 15000: Loss = -11181.499882691887
Iteration 15100: Loss = -11181.501159363619
1
Iteration 15200: Loss = -11181.49985683529
Iteration 15300: Loss = -11181.503183570381
1
Iteration 15400: Loss = -11181.49988211058
Iteration 15500: Loss = -11181.526166235133
1
Iteration 15600: Loss = -11181.499874037345
Iteration 15700: Loss = -11181.563885044117
1
Iteration 15800: Loss = -11181.499864507923
Iteration 15900: Loss = -11181.510204241033
1
Iteration 16000: Loss = -11181.504069749897
2
Iteration 16100: Loss = -11181.500424955357
3
Iteration 16200: Loss = -11181.500151504097
4
Iteration 16300: Loss = -11181.501088590067
5
Iteration 16400: Loss = -11181.500413766846
6
Iteration 16500: Loss = -11181.499886949776
Iteration 16600: Loss = -11181.741283995496
1
Iteration 16700: Loss = -11181.4998641585
Iteration 16800: Loss = -11181.564469466755
1
Iteration 16900: Loss = -11181.499882908383
Iteration 17000: Loss = -11181.49988306689
Iteration 17100: Loss = -11181.499906657627
Iteration 17200: Loss = -11181.49985850938
Iteration 17300: Loss = -11181.606410736145
1
Iteration 17400: Loss = -11181.499862144332
Iteration 17500: Loss = -11181.4998800362
Iteration 17600: Loss = -11181.519796261726
1
Iteration 17700: Loss = -11181.4998520122
Iteration 17800: Loss = -11181.499874611238
Iteration 17900: Loss = -11181.80409375909
1
Iteration 18000: Loss = -11181.499904406752
Iteration 18100: Loss = -11181.499873964938
Iteration 18200: Loss = -11181.50451110196
1
Iteration 18300: Loss = -11181.499879181958
Iteration 18400: Loss = -11181.51361692025
1
Iteration 18500: Loss = -11181.499893395336
Iteration 18600: Loss = -11181.50731411608
1
Iteration 18700: Loss = -11181.584991919908
2
Iteration 18800: Loss = -11181.499880048108
Iteration 18900: Loss = -11181.500167691534
1
Iteration 19000: Loss = -11181.499898843542
Iteration 19100: Loss = -11181.520444809545
1
Iteration 19200: Loss = -11181.499889833201
Iteration 19300: Loss = -11181.499983603602
Iteration 19400: Loss = -11181.499884882991
Iteration 19500: Loss = -11181.49988400787
Iteration 19600: Loss = -11181.508294667348
1
Iteration 19700: Loss = -11181.499874389261
Iteration 19800: Loss = -11181.500034453527
1
Iteration 19900: Loss = -11181.499888419645
pi: tensor([[0.7229, 0.2771],
        [0.2385, 0.7615]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9715, 0.0285], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1895, 0.0828],
         [0.7078, 0.3027]],

        [[0.5445, 0.0970],
         [0.6271, 0.5382]],

        [[0.5488, 0.1035],
         [0.5955, 0.6819]],

        [[0.6675, 0.1036],
         [0.6653, 0.6801]],

        [[0.7073, 0.1109],
         [0.6256, 0.5481]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: -0.004063414211148451
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9204282407407407
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.844845420066659
Global Adjusted Rand Index: 0.6324210072298968
Average Adjusted Rand Index: 0.7138568574079837
11155.919735912514
[0.6324210072298968, 0.6324210072298968] [0.7138568574079837, 0.7138568574079837] [11181.499886704712, 11181.499877869846]
-------------------------------------
This iteration is 62
True Objective function: Loss = -11366.229511463333
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22446.558701724724
Iteration 100: Loss = -11717.932360778408
Iteration 200: Loss = -11717.01886845068
Iteration 300: Loss = -11715.950155554716
Iteration 400: Loss = -11707.33100667248
Iteration 500: Loss = -11667.292735732068
Iteration 600: Loss = -11460.900225264226
Iteration 700: Loss = -11362.07548020445
Iteration 800: Loss = -11351.349588800598
Iteration 900: Loss = -11350.864073079158
Iteration 1000: Loss = -11350.238062895925
Iteration 1100: Loss = -11349.642446013047
Iteration 1200: Loss = -11349.555734591988
Iteration 1300: Loss = -11349.494877040479
Iteration 1400: Loss = -11349.435205334916
Iteration 1500: Loss = -11349.389167987427
Iteration 1600: Loss = -11349.361654423603
Iteration 1700: Loss = -11349.339689035765
Iteration 1800: Loss = -11349.321260416787
Iteration 1900: Loss = -11349.30477446156
Iteration 2000: Loss = -11349.289163579791
Iteration 2100: Loss = -11349.271202996842
Iteration 2200: Loss = -11349.250903109178
Iteration 2300: Loss = -11349.238403828065
Iteration 2400: Loss = -11349.22556895553
Iteration 2500: Loss = -11349.15689497869
Iteration 2600: Loss = -11347.8549371757
Iteration 2700: Loss = -11346.344987722288
Iteration 2800: Loss = -11345.99911748848
Iteration 2900: Loss = -11345.772856919297
Iteration 3000: Loss = -11345.765939619103
Iteration 3100: Loss = -11345.759104268698
Iteration 3200: Loss = -11345.71675522344
Iteration 3300: Loss = -11345.699827003444
Iteration 3400: Loss = -11345.69713019858
Iteration 3500: Loss = -11345.703865271873
1
Iteration 3600: Loss = -11345.692304786127
Iteration 3700: Loss = -11345.689244830593
Iteration 3800: Loss = -11345.694854845464
1
Iteration 3900: Loss = -11345.682514070253
Iteration 4000: Loss = -11345.658160291981
Iteration 4100: Loss = -11345.628129466282
Iteration 4200: Loss = -11345.585140828656
Iteration 4300: Loss = -11345.582773775686
Iteration 4400: Loss = -11345.581353604774
Iteration 4500: Loss = -11345.5797453589
Iteration 4600: Loss = -11345.577111054281
Iteration 4700: Loss = -11345.575225634166
Iteration 4800: Loss = -11345.574483593777
Iteration 4900: Loss = -11345.574311057082
Iteration 5000: Loss = -11345.573085681342
Iteration 5100: Loss = -11345.572399143659
Iteration 5200: Loss = -11345.574209918315
1
Iteration 5300: Loss = -11345.577576259964
2
Iteration 5400: Loss = -11345.571718270596
Iteration 5500: Loss = -11345.570275185626
Iteration 5600: Loss = -11345.569790957394
Iteration 5700: Loss = -11345.568063786697
Iteration 5800: Loss = -11345.557421847625
Iteration 5900: Loss = -11345.555268774468
Iteration 6000: Loss = -11345.555002765177
Iteration 6100: Loss = -11345.55880362496
1
Iteration 6200: Loss = -11345.554883707564
Iteration 6300: Loss = -11345.553674394929
Iteration 6400: Loss = -11345.55223445945
Iteration 6500: Loss = -11345.551827658097
Iteration 6600: Loss = -11345.5509599906
Iteration 6700: Loss = -11345.550621130225
Iteration 6800: Loss = -11345.550358303255
Iteration 6900: Loss = -11345.558058451705
1
Iteration 7000: Loss = -11345.549878331221
Iteration 7100: Loss = -11345.561824145734
1
Iteration 7200: Loss = -11345.548522795896
Iteration 7300: Loss = -11345.539530367447
Iteration 7400: Loss = -11345.537942316425
Iteration 7500: Loss = -11345.536820889616
Iteration 7600: Loss = -11345.536485071276
Iteration 7700: Loss = -11345.536491460978
Iteration 7800: Loss = -11345.536432381103
Iteration 7900: Loss = -11345.5361556103
Iteration 8000: Loss = -11345.54193847869
1
Iteration 8100: Loss = -11345.535922363644
Iteration 8200: Loss = -11345.53603773049
1
Iteration 8300: Loss = -11345.551226757358
2
Iteration 8400: Loss = -11345.53568572773
Iteration 8500: Loss = -11345.577213209157
1
Iteration 8600: Loss = -11345.535521360296
Iteration 8700: Loss = -11345.536027318834
1
Iteration 8800: Loss = -11345.589900008397
2
Iteration 8900: Loss = -11345.535348562138
Iteration 9000: Loss = -11345.53555639452
1
Iteration 9100: Loss = -11345.535806296517
2
Iteration 9200: Loss = -11345.535338488271
Iteration 9300: Loss = -11345.53842201744
1
Iteration 9400: Loss = -11345.533238080097
Iteration 9500: Loss = -11345.536061416658
1
Iteration 9600: Loss = -11345.531388164203
Iteration 9700: Loss = -11345.537047394942
1
Iteration 9800: Loss = -11345.530328866835
Iteration 9900: Loss = -11345.530516093311
1
Iteration 10000: Loss = -11345.536147799796
2
Iteration 10100: Loss = -11345.545528053475
3
Iteration 10200: Loss = -11345.554264015842
4
Iteration 10300: Loss = -11345.530166842334
Iteration 10400: Loss = -11345.530680213387
1
Iteration 10500: Loss = -11345.529848689006
Iteration 10600: Loss = -11345.531247443187
1
Iteration 10700: Loss = -11345.540607706638
2
Iteration 10800: Loss = -11345.540463733252
3
Iteration 10900: Loss = -11345.529663693182
Iteration 11000: Loss = -11345.531362651242
1
Iteration 11100: Loss = -11345.529456524524
Iteration 11200: Loss = -11345.530006143188
1
Iteration 11300: Loss = -11345.531137821308
2
Iteration 11400: Loss = -11345.530022739387
3
Iteration 11500: Loss = -11345.539841818534
4
Iteration 11600: Loss = -11345.532046500839
5
Iteration 11700: Loss = -11345.529424044586
Iteration 11800: Loss = -11345.529794552125
1
Iteration 11900: Loss = -11345.544724114277
2
Iteration 12000: Loss = -11345.546106499718
3
Iteration 12100: Loss = -11345.563996227454
4
Iteration 12200: Loss = -11345.530003315249
5
Iteration 12300: Loss = -11345.529293889842
Iteration 12400: Loss = -11345.535867044791
1
Iteration 12500: Loss = -11345.52913684761
Iteration 12600: Loss = -11345.52962829626
1
Iteration 12700: Loss = -11345.644028806586
2
Iteration 12800: Loss = -11345.528955956903
Iteration 12900: Loss = -11345.551171340923
1
Iteration 13000: Loss = -11345.529638203421
2
Iteration 13100: Loss = -11345.533842786293
3
Iteration 13200: Loss = -11345.531889450496
4
Iteration 13300: Loss = -11345.531112439734
5
Iteration 13400: Loss = -11345.532701010472
6
Iteration 13500: Loss = -11345.530293150272
7
Iteration 13600: Loss = -11345.529720524019
8
Iteration 13700: Loss = -11345.536215731352
9
Iteration 13800: Loss = -11345.529158611325
10
Iteration 13900: Loss = -11345.533323824046
11
Iteration 14000: Loss = -11345.576100234011
12
Iteration 14100: Loss = -11345.528771742944
Iteration 14200: Loss = -11345.528549766452
Iteration 14300: Loss = -11345.58498355743
1
Iteration 14400: Loss = -11345.528409710572
Iteration 14500: Loss = -11345.548346532205
1
Iteration 14600: Loss = -11345.528677742648
2
Iteration 14700: Loss = -11345.532341074759
3
Iteration 14800: Loss = -11345.54349407856
4
Iteration 14900: Loss = -11345.529365213484
5
Iteration 15000: Loss = -11345.530325142183
6
Iteration 15100: Loss = -11345.529029231177
7
Iteration 15200: Loss = -11345.528209709457
Iteration 15300: Loss = -11345.52867252917
1
Iteration 15400: Loss = -11345.657893371716
2
Iteration 15500: Loss = -11345.529067257905
3
Iteration 15600: Loss = -11345.529711610438
4
Iteration 15700: Loss = -11345.529392642875
5
Iteration 15800: Loss = -11345.532051693532
6
Iteration 15900: Loss = -11345.539210294717
7
Iteration 16000: Loss = -11345.530094645535
8
Iteration 16100: Loss = -11345.52782984523
Iteration 16200: Loss = -11345.54494850931
1
Iteration 16300: Loss = -11345.52604014248
Iteration 16400: Loss = -11345.528432748872
1
Iteration 16500: Loss = -11345.529932541302
2
Iteration 16600: Loss = -11345.526137923964
Iteration 16700: Loss = -11345.52655111972
1
Iteration 16800: Loss = -11345.715660776394
2
Iteration 16900: Loss = -11345.52595034359
Iteration 17000: Loss = -11345.525065383747
Iteration 17100: Loss = -11345.523783989893
Iteration 17200: Loss = -11345.528245416459
1
Iteration 17300: Loss = -11345.531198265213
2
Iteration 17400: Loss = -11345.522626623713
Iteration 17500: Loss = -11345.523194978832
1
Iteration 17600: Loss = -11345.528865885755
2
Iteration 17700: Loss = -11345.525306542586
3
Iteration 17800: Loss = -11345.525279107005
4
Iteration 17900: Loss = -11345.523149828174
5
Iteration 18000: Loss = -11345.525515990264
6
Iteration 18100: Loss = -11345.528728979043
7
Iteration 18200: Loss = -11345.522679501642
Iteration 18300: Loss = -11345.52242260908
Iteration 18400: Loss = -11345.673737108818
1
Iteration 18500: Loss = -11345.523529240576
2
Iteration 18600: Loss = -11345.525131315631
3
Iteration 18700: Loss = -11345.719817464818
4
Iteration 18800: Loss = -11345.522381626135
Iteration 18900: Loss = -11345.524851295257
1
Iteration 19000: Loss = -11345.52253316434
2
Iteration 19100: Loss = -11345.52245668248
Iteration 19200: Loss = -11345.660661710055
1
Iteration 19300: Loss = -11345.522305388551
Iteration 19400: Loss = -11345.522458569083
1
Iteration 19500: Loss = -11345.549280574065
2
Iteration 19600: Loss = -11345.52641162594
3
Iteration 19700: Loss = -11345.522403871497
Iteration 19800: Loss = -11345.532544451515
1
Iteration 19900: Loss = -11345.524387792808
2
pi: tensor([[0.7690, 0.2310],
        [0.2036, 0.7964]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4193, 0.5807], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2073, 0.1079],
         [0.6704, 0.2987]],

        [[0.5011, 0.0951],
         [0.6454, 0.5930]],

        [[0.7064, 0.1011],
         [0.6419, 0.5823]],

        [[0.5610, 0.1011],
         [0.7131, 0.5329]],

        [[0.6510, 0.0942],
         [0.6341, 0.5663]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681903407835813
Average Adjusted Rand Index: 0.9684808159668682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20573.83649568846
Iteration 100: Loss = -11716.590684671142
Iteration 200: Loss = -11712.665729463428
Iteration 300: Loss = -11710.185560904922
Iteration 400: Loss = -11679.054848190159
Iteration 500: Loss = -11428.419101940477
Iteration 600: Loss = -11362.560728918626
Iteration 700: Loss = -11350.272629592928
Iteration 800: Loss = -11350.031110898239
Iteration 900: Loss = -11349.87880076122
Iteration 1000: Loss = -11349.645906213684
Iteration 1100: Loss = -11349.590530743988
Iteration 1200: Loss = -11349.554269328484
Iteration 1300: Loss = -11349.52714897875
Iteration 1400: Loss = -11349.504359896107
Iteration 1500: Loss = -11349.47825875476
Iteration 1600: Loss = -11349.369561176605
Iteration 1700: Loss = -11349.23213076534
Iteration 1800: Loss = -11349.217264997213
Iteration 1900: Loss = -11349.204742846052
Iteration 2000: Loss = -11349.166257777744
Iteration 2100: Loss = -11347.475450319718
Iteration 2200: Loss = -11346.035719168352
Iteration 2300: Loss = -11346.030142413842
Iteration 2400: Loss = -11346.025929085587
Iteration 2500: Loss = -11346.022145295405
Iteration 2600: Loss = -11346.018352830839
Iteration 2700: Loss = -11346.013606352852
Iteration 2800: Loss = -11346.008744618342
Iteration 2900: Loss = -11346.0062270053
Iteration 3000: Loss = -11346.00446766454
Iteration 3100: Loss = -11346.002689238829
Iteration 3200: Loss = -11346.0011105221
Iteration 3300: Loss = -11346.000322008427
Iteration 3400: Loss = -11346.0011551427
1
Iteration 3500: Loss = -11345.990826409183
Iteration 3600: Loss = -11345.945966905665
Iteration 3700: Loss = -11345.893325338113
Iteration 3800: Loss = -11345.881071241625
Iteration 3900: Loss = -11345.877443528256
Iteration 4000: Loss = -11345.877670883801
1
Iteration 4100: Loss = -11345.875558502084
Iteration 4200: Loss = -11345.87460495425
Iteration 4300: Loss = -11345.886556354943
1
Iteration 4400: Loss = -11345.871627125565
Iteration 4500: Loss = -11345.870057717748
Iteration 4600: Loss = -11345.869556713169
Iteration 4700: Loss = -11345.86907336634
Iteration 4800: Loss = -11345.865085833157
Iteration 4900: Loss = -11345.863477347564
Iteration 5000: Loss = -11345.86305464293
Iteration 5100: Loss = -11345.884486302979
1
Iteration 5200: Loss = -11345.862280770809
Iteration 5300: Loss = -11345.86183157881
Iteration 5400: Loss = -11345.860903400555
Iteration 5500: Loss = -11345.859560182485
Iteration 5600: Loss = -11345.860195963576
1
Iteration 5700: Loss = -11345.85890343006
Iteration 5800: Loss = -11345.863773819983
1
Iteration 5900: Loss = -11345.860452056411
2
Iteration 6000: Loss = -11345.871048116804
3
Iteration 6100: Loss = -11345.866314655212
4
Iteration 6200: Loss = -11345.857899721497
Iteration 6300: Loss = -11345.857786446624
Iteration 6400: Loss = -11345.858660801508
1
Iteration 6500: Loss = -11345.85760508708
Iteration 6600: Loss = -11345.857097023427
Iteration 6700: Loss = -11345.856691684166
Iteration 6800: Loss = -11345.66455473028
Iteration 6900: Loss = -11345.669503175854
1
Iteration 7000: Loss = -11345.662209633978
Iteration 7100: Loss = -11345.664361223378
1
Iteration 7200: Loss = -11345.662032737886
Iteration 7300: Loss = -11345.662543765133
1
Iteration 7400: Loss = -11345.618808447538
Iteration 7500: Loss = -11345.630840672948
1
Iteration 7600: Loss = -11345.612866013718
Iteration 7700: Loss = -11345.612133771321
Iteration 7800: Loss = -11345.642548390528
1
Iteration 7900: Loss = -11345.617636778308
2
Iteration 8000: Loss = -11345.635255448764
3
Iteration 8100: Loss = -11345.608127886018
Iteration 8200: Loss = -11345.61091286861
1
Iteration 8300: Loss = -11345.61414429593
2
Iteration 8400: Loss = -11345.614660978046
3
Iteration 8500: Loss = -11345.605704487956
Iteration 8600: Loss = -11345.60514256155
Iteration 8700: Loss = -11345.604439298328
Iteration 8800: Loss = -11345.60603534036
1
Iteration 8900: Loss = -11345.604501222806
Iteration 9000: Loss = -11345.60448139669
Iteration 9100: Loss = -11345.605285225225
1
Iteration 9200: Loss = -11345.6046996116
2
Iteration 9300: Loss = -11345.604039275597
Iteration 9400: Loss = -11345.604004636261
Iteration 9500: Loss = -11345.6553558738
1
Iteration 9600: Loss = -11345.603787846381
Iteration 9700: Loss = -11345.605653863722
1
Iteration 9800: Loss = -11345.603667076235
Iteration 9900: Loss = -11345.593506611249
Iteration 10000: Loss = -11345.60316382556
1
Iteration 10100: Loss = -11345.622045869417
2
Iteration 10200: Loss = -11345.57616719808
Iteration 10300: Loss = -11345.572927229832
Iteration 10400: Loss = -11345.570980352073
Iteration 10500: Loss = -11345.570940133075
Iteration 10600: Loss = -11345.58451147527
1
Iteration 10700: Loss = -11345.570862161047
Iteration 10800: Loss = -11345.581340674133
1
Iteration 10900: Loss = -11345.579232435191
2
Iteration 11000: Loss = -11345.598134117516
3
Iteration 11100: Loss = -11345.703362895587
4
Iteration 11200: Loss = -11345.570840293849
Iteration 11300: Loss = -11345.5710892468
1
Iteration 11400: Loss = -11345.57134306788
2
Iteration 11500: Loss = -11345.64595084361
3
Iteration 11600: Loss = -11345.570744098108
Iteration 11700: Loss = -11345.571096005893
1
Iteration 11800: Loss = -11345.573819458106
2
Iteration 11900: Loss = -11345.5715541476
3
Iteration 12000: Loss = -11345.574686541719
4
Iteration 12100: Loss = -11345.57984056804
5
Iteration 12200: Loss = -11345.580411820145
6
Iteration 12300: Loss = -11345.570140031588
Iteration 12400: Loss = -11345.572572334951
1
Iteration 12500: Loss = -11345.573212077552
2
Iteration 12600: Loss = -11345.60123013286
3
Iteration 12700: Loss = -11345.56993598084
Iteration 12800: Loss = -11345.570292108281
1
Iteration 12900: Loss = -11345.591474249288
2
Iteration 13000: Loss = -11345.575795426545
3
Iteration 13100: Loss = -11345.569774745663
Iteration 13200: Loss = -11345.569847064195
Iteration 13300: Loss = -11345.57674771184
1
Iteration 13400: Loss = -11345.622030459162
2
Iteration 13500: Loss = -11345.569740247909
Iteration 13600: Loss = -11345.569813332464
Iteration 13700: Loss = -11345.575774528996
1
Iteration 13800: Loss = -11345.779328494948
2
Iteration 13900: Loss = -11345.569824895163
Iteration 14000: Loss = -11345.570139673118
1
Iteration 14100: Loss = -11345.57051621795
2
Iteration 14200: Loss = -11345.570274109652
3
Iteration 14300: Loss = -11345.569702515832
Iteration 14400: Loss = -11345.569796608637
Iteration 14500: Loss = -11345.578904992686
1
Iteration 14600: Loss = -11345.569384446224
Iteration 14700: Loss = -11345.57315885589
1
Iteration 14800: Loss = -11345.709697651571
2
Iteration 14900: Loss = -11345.52962201776
Iteration 15000: Loss = -11345.52560648208
Iteration 15100: Loss = -11345.523864465757
Iteration 15200: Loss = -11345.527303352936
1
Iteration 15300: Loss = -11345.52277144133
Iteration 15400: Loss = -11345.523166042069
1
Iteration 15500: Loss = -11345.550478558336
2
Iteration 15600: Loss = -11345.52209054055
Iteration 15700: Loss = -11345.524493788307
1
Iteration 15800: Loss = -11345.52859827879
2
Iteration 15900: Loss = -11345.521853439326
Iteration 16000: Loss = -11345.52242246185
1
Iteration 16100: Loss = -11345.531760615688
2
Iteration 16200: Loss = -11345.521887140183
Iteration 16300: Loss = -11345.526352291412
1
Iteration 16400: Loss = -11345.52504743556
2
Iteration 16500: Loss = -11345.52182759874
Iteration 16600: Loss = -11345.522738034237
1
Iteration 16700: Loss = -11345.538998912374
2
Iteration 16800: Loss = -11345.521784240244
Iteration 16900: Loss = -11345.527203473994
1
Iteration 17000: Loss = -11345.5215234798
Iteration 17100: Loss = -11345.537364311858
1
Iteration 17200: Loss = -11345.52150390124
Iteration 17300: Loss = -11345.55438921082
1
Iteration 17400: Loss = -11345.522263407158
2
Iteration 17500: Loss = -11345.526309571718
3
Iteration 17600: Loss = -11345.521337650594
Iteration 17700: Loss = -11345.522583332073
1
Iteration 17800: Loss = -11345.526320277813
2
Iteration 17900: Loss = -11345.526521701777
3
Iteration 18000: Loss = -11345.606194057475
4
Iteration 18100: Loss = -11345.546067022944
5
Iteration 18200: Loss = -11345.522297247337
6
Iteration 18300: Loss = -11345.546203312675
7
Iteration 18400: Loss = -11345.521613055302
8
Iteration 18500: Loss = -11345.540516107254
9
Iteration 18600: Loss = -11345.527644837699
10
Iteration 18700: Loss = -11345.521168753863
Iteration 18800: Loss = -11345.522032181038
1
Iteration 18900: Loss = -11345.521410609183
2
Iteration 19000: Loss = -11345.521230646991
Iteration 19100: Loss = -11345.521418584469
1
Iteration 19200: Loss = -11345.5211183442
Iteration 19300: Loss = -11345.712910265169
1
Iteration 19400: Loss = -11345.521091437398
Iteration 19500: Loss = -11345.524405876558
1
Iteration 19600: Loss = -11345.539155708286
2
Iteration 19700: Loss = -11345.52112091778
Iteration 19800: Loss = -11345.52628287653
1
Iteration 19900: Loss = -11345.791089231874
2
pi: tensor([[0.7694, 0.2306],
        [0.2017, 0.7983]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4194, 0.5806], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2073, 0.1080],
         [0.6799, 0.2987]],

        [[0.5035, 0.0951],
         [0.5418, 0.6905]],

        [[0.5711, 0.1012],
         [0.6892, 0.5639]],

        [[0.5558, 0.1014],
         [0.7223, 0.5017]],

        [[0.7185, 0.0942],
         [0.6957, 0.5150]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681903407835813
Average Adjusted Rand Index: 0.9684808159668682
11366.229511463333
[0.9681903407835813, 0.9681903407835813] [0.9684808159668682, 0.9684808159668682] [11345.521909134104, 11345.528825677498]
-------------------------------------
This iteration is 63
True Objective function: Loss = -11228.01128650153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23699.420767466258
Iteration 100: Loss = -11486.189477806223
Iteration 200: Loss = -11485.139893000289
Iteration 300: Loss = -11481.996066556001
Iteration 400: Loss = -11449.468896010216
Iteration 500: Loss = -11325.494426498444
Iteration 600: Loss = -11220.47999997174
Iteration 700: Loss = -11212.242707792895
Iteration 800: Loss = -11212.108410612276
Iteration 900: Loss = -11210.420701487561
Iteration 1000: Loss = -11210.139470285692
Iteration 1100: Loss = -11210.111889618134
Iteration 1200: Loss = -11206.564130722376
Iteration 1300: Loss = -11199.354970257311
Iteration 1400: Loss = -11199.34437994885
Iteration 1500: Loss = -11199.337895952509
Iteration 1600: Loss = -11199.332990635172
Iteration 1700: Loss = -11199.32903637287
Iteration 1800: Loss = -11199.325711367812
Iteration 1900: Loss = -11199.320750900997
Iteration 2000: Loss = -11199.233957424893
Iteration 2100: Loss = -11199.231365562813
Iteration 2200: Loss = -11199.229558691046
Iteration 2300: Loss = -11199.228171753975
Iteration 2400: Loss = -11199.227081163355
Iteration 2500: Loss = -11199.226043511726
Iteration 2600: Loss = -11199.225060716939
Iteration 2700: Loss = -11199.223950553382
Iteration 2800: Loss = -11199.219936633417
Iteration 2900: Loss = -11199.020596195765
Iteration 3000: Loss = -11199.01818928403
Iteration 3100: Loss = -11199.010459948317
Iteration 3200: Loss = -11199.008003307647
Iteration 3300: Loss = -11199.007534706836
Iteration 3400: Loss = -11199.00721290191
Iteration 3500: Loss = -11199.006666326963
Iteration 3600: Loss = -11199.006146328
Iteration 3700: Loss = -11199.003874345952
Iteration 3800: Loss = -11199.001532997072
Iteration 3900: Loss = -11199.002983793114
1
Iteration 4000: Loss = -11199.001123804881
Iteration 4100: Loss = -11199.000949808418
Iteration 4200: Loss = -11199.0009665442
Iteration 4300: Loss = -11199.000564589052
Iteration 4400: Loss = -11199.00053209435
Iteration 4500: Loss = -11199.001404335311
1
Iteration 4600: Loss = -11199.009424644039
2
Iteration 4700: Loss = -11199.00010407338
Iteration 4800: Loss = -11198.99997374366
Iteration 4900: Loss = -11198.999826063173
Iteration 5000: Loss = -11198.999702525804
Iteration 5100: Loss = -11199.000394144188
1
Iteration 5200: Loss = -11198.999493872127
Iteration 5300: Loss = -11198.999656300497
1
Iteration 5400: Loss = -11198.999328788226
Iteration 5500: Loss = -11198.999325056204
Iteration 5600: Loss = -11198.999252133584
Iteration 5700: Loss = -11198.999178706643
Iteration 5800: Loss = -11198.999501060149
1
Iteration 5900: Loss = -11198.999002391089
Iteration 6000: Loss = -11199.011233865447
1
Iteration 6100: Loss = -11198.998864369996
Iteration 6200: Loss = -11198.998787310024
Iteration 6300: Loss = -11198.998732814774
Iteration 6400: Loss = -11198.99868015232
Iteration 6500: Loss = -11198.998876016914
1
Iteration 6600: Loss = -11198.998579901101
Iteration 6700: Loss = -11199.002023918365
1
Iteration 6800: Loss = -11198.998452845051
Iteration 6900: Loss = -11198.998408266863
Iteration 7000: Loss = -11198.998268322586
Iteration 7100: Loss = -11198.999206172144
1
Iteration 7200: Loss = -11198.998305604893
Iteration 7300: Loss = -11198.998611533225
1
Iteration 7400: Loss = -11198.997069042478
Iteration 7500: Loss = -11199.021790575183
1
Iteration 7600: Loss = -11198.9970192745
Iteration 7700: Loss = -11198.996957235146
Iteration 7800: Loss = -11198.997652584689
1
Iteration 7900: Loss = -11198.996956493615
Iteration 8000: Loss = -11199.001181762653
1
Iteration 8100: Loss = -11198.997156227619
2
Iteration 8200: Loss = -11198.99693002057
Iteration 8300: Loss = -11198.998477166164
1
Iteration 8400: Loss = -11198.996855352649
Iteration 8500: Loss = -11198.9969068762
Iteration 8600: Loss = -11198.997293533816
1
Iteration 8700: Loss = -11198.996903891135
Iteration 8800: Loss = -11198.996956383773
Iteration 8900: Loss = -11198.996977340708
Iteration 9000: Loss = -11199.009627063764
1
Iteration 9100: Loss = -11198.996603012467
Iteration 9200: Loss = -11198.99600545111
Iteration 9300: Loss = -11199.017680205063
1
Iteration 9400: Loss = -11198.995683838408
Iteration 9500: Loss = -11198.995809047427
1
Iteration 9600: Loss = -11198.997285067835
2
Iteration 9700: Loss = -11198.993650915743
Iteration 9800: Loss = -11198.995023449921
1
Iteration 9900: Loss = -11198.995701821883
2
Iteration 10000: Loss = -11198.993616114223
Iteration 10100: Loss = -11198.999178242948
1
Iteration 10200: Loss = -11198.994252108321
2
Iteration 10300: Loss = -11199.026393654674
3
Iteration 10400: Loss = -11198.993565220093
Iteration 10500: Loss = -11198.991983610209
Iteration 10600: Loss = -11198.993234828678
1
Iteration 10700: Loss = -11198.994601918954
2
Iteration 10800: Loss = -11198.993428701137
3
Iteration 10900: Loss = -11198.994752983152
4
Iteration 11000: Loss = -11198.99848693385
5
Iteration 11100: Loss = -11198.992052931106
Iteration 11200: Loss = -11198.991939894446
Iteration 11300: Loss = -11199.007151147709
1
Iteration 11400: Loss = -11198.997981707651
2
Iteration 11500: Loss = -11198.992222649093
3
Iteration 11600: Loss = -11198.991976118687
Iteration 11700: Loss = -11198.993298300788
1
Iteration 11800: Loss = -11198.99246480607
2
Iteration 11900: Loss = -11198.992857590914
3
Iteration 12000: Loss = -11198.992271605157
4
Iteration 12100: Loss = -11198.99275918073
5
Iteration 12200: Loss = -11199.01067045754
6
Iteration 12300: Loss = -11199.016612266732
7
Iteration 12400: Loss = -11198.99195046554
Iteration 12500: Loss = -11198.994463412935
1
Iteration 12600: Loss = -11198.994490301975
2
Iteration 12700: Loss = -11198.991939167203
Iteration 12800: Loss = -11199.08416412247
1
Iteration 12900: Loss = -11198.992534009063
2
Iteration 13000: Loss = -11198.996363116361
3
Iteration 13100: Loss = -11199.01514570636
4
Iteration 13200: Loss = -11198.992688935861
5
Iteration 13300: Loss = -11198.991981864576
Iteration 13400: Loss = -11198.996895039358
1
Iteration 13500: Loss = -11199.004086151948
2
Iteration 13600: Loss = -11198.99466146866
3
Iteration 13700: Loss = -11199.01400287193
4
Iteration 13800: Loss = -11199.029069007875
5
Iteration 13900: Loss = -11198.991774750135
Iteration 14000: Loss = -11198.992539885845
1
Iteration 14100: Loss = -11199.005715276484
2
Iteration 14200: Loss = -11199.111658604892
3
Iteration 14300: Loss = -11198.987572146625
Iteration 14400: Loss = -11199.004006939087
1
Iteration 14500: Loss = -11198.9875754327
Iteration 14600: Loss = -11198.988583406452
1
Iteration 14700: Loss = -11199.033435388093
2
Iteration 14800: Loss = -11198.989363731476
3
Iteration 14900: Loss = -11198.98848999793
4
Iteration 15000: Loss = -11198.988182188508
5
Iteration 15100: Loss = -11198.988484020547
6
Iteration 15200: Loss = -11199.004064267383
7
Iteration 15300: Loss = -11198.987717690727
8
Iteration 15400: Loss = -11198.988406599683
9
Iteration 15500: Loss = -11198.99666952279
10
Iteration 15600: Loss = -11198.999740194786
11
Iteration 15700: Loss = -11198.988018845723
12
Iteration 15800: Loss = -11198.987476163617
Iteration 15900: Loss = -11198.989415644497
1
Iteration 16000: Loss = -11199.033320763903
2
Iteration 16100: Loss = -11198.990948127774
3
Iteration 16200: Loss = -11198.987580193805
4
Iteration 16300: Loss = -11198.987772611346
5
Iteration 16400: Loss = -11199.009147298775
6
Iteration 16500: Loss = -11198.98773132626
7
Iteration 16600: Loss = -11198.987740336775
8
Iteration 16700: Loss = -11198.98849414154
9
Iteration 16800: Loss = -11198.987424713823
Iteration 16900: Loss = -11198.987897320829
1
Iteration 17000: Loss = -11199.194424385478
2
Iteration 17100: Loss = -11198.987393752628
Iteration 17200: Loss = -11198.992775627012
1
Iteration 17300: Loss = -11198.987366826153
Iteration 17400: Loss = -11198.988986223007
1
Iteration 17500: Loss = -11198.988367952952
2
Iteration 17600: Loss = -11198.987655513307
3
Iteration 17700: Loss = -11198.990121859382
4
Iteration 17800: Loss = -11199.246617469766
5
Iteration 17900: Loss = -11198.987496545918
6
Iteration 18000: Loss = -11198.995694676005
7
Iteration 18100: Loss = -11199.017858363182
8
Iteration 18200: Loss = -11199.071674499584
9
Iteration 18300: Loss = -11198.988399914082
10
Iteration 18400: Loss = -11198.98737157295
Iteration 18500: Loss = -11198.999852950843
1
Iteration 18600: Loss = -11198.987013942467
Iteration 18700: Loss = -11198.98720848603
1
Iteration 18800: Loss = -11198.987044025187
Iteration 18900: Loss = -11198.987078434327
Iteration 19000: Loss = -11198.987016913457
Iteration 19100: Loss = -11198.987001708743
Iteration 19200: Loss = -11198.987564011864
1
Iteration 19300: Loss = -11198.986634229928
Iteration 19400: Loss = -11198.990751724576
1
Iteration 19500: Loss = -11198.987944636785
2
Iteration 19600: Loss = -11199.034420545968
3
Iteration 19700: Loss = -11199.022177629295
4
Iteration 19800: Loss = -11198.994658989892
5
Iteration 19900: Loss = -11199.002966241662
6
pi: tensor([[0.7443, 0.2557],
        [0.2700, 0.7300]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4952, 0.5048], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2905, 0.1018],
         [0.6010, 0.2085]],

        [[0.6180, 0.0977],
         [0.5754, 0.6070]],

        [[0.6825, 0.0964],
         [0.5789, 0.5560]],

        [[0.6690, 0.1062],
         [0.5514, 0.7258]],

        [[0.5841, 0.0934],
         [0.6832, 0.7117]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448509923071951
Global Adjusted Rand Index: 0.9137632499893574
Average Adjusted Rand Index: 0.9140986387303538
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19870.1727185478
Iteration 100: Loss = -11486.604662753534
Iteration 200: Loss = -11486.142927070305
Iteration 300: Loss = -11485.833173393286
Iteration 400: Loss = -11484.293514166095
Iteration 500: Loss = -11480.272939925055
Iteration 600: Loss = -11465.641312026632
Iteration 700: Loss = -11315.874579795143
Iteration 800: Loss = -11273.0065076125
Iteration 900: Loss = -11271.275126889414
Iteration 1000: Loss = -11269.21179965998
Iteration 1100: Loss = -11269.07648843014
Iteration 1200: Loss = -11268.989864146388
Iteration 1300: Loss = -11268.92575850229
Iteration 1400: Loss = -11268.887351297524
Iteration 1500: Loss = -11268.860201774793
Iteration 1600: Loss = -11268.839298771547
Iteration 1700: Loss = -11268.822549850325
Iteration 1800: Loss = -11268.80876539695
Iteration 1900: Loss = -11268.79691774739
Iteration 2000: Loss = -11268.786239442321
Iteration 2100: Loss = -11268.775728205148
Iteration 2200: Loss = -11268.760678110597
Iteration 2300: Loss = -11268.698870085303
Iteration 2400: Loss = -11268.690502603815
Iteration 2500: Loss = -11268.683080662855
Iteration 2600: Loss = -11268.6732265052
Iteration 2700: Loss = -11268.656890310542
Iteration 2800: Loss = -11268.582910754702
Iteration 2900: Loss = -11267.587327309995
Iteration 3000: Loss = -11267.503749956988
Iteration 3100: Loss = -11267.47802560044
Iteration 3200: Loss = -11267.470358930099
Iteration 3300: Loss = -11267.464418638096
Iteration 3400: Loss = -11267.460148427488
Iteration 3500: Loss = -11267.45897338807
Iteration 3600: Loss = -11267.450721146633
Iteration 3700: Loss = -11267.334822468802
Iteration 3800: Loss = -11267.289012867222
Iteration 3900: Loss = -11267.269631867022
Iteration 4000: Loss = -11267.01209952844
Iteration 4100: Loss = -11267.008237907186
Iteration 4200: Loss = -11266.997208413512
Iteration 4300: Loss = -11266.989519379278
Iteration 4400: Loss = -11266.963195809327
Iteration 4500: Loss = -11266.899680417311
Iteration 4600: Loss = -11266.892503129377
Iteration 4700: Loss = -11264.793141701013
Iteration 4800: Loss = -11264.752620611542
Iteration 4900: Loss = -11264.729858874201
Iteration 5000: Loss = -11264.681551745416
Iteration 5100: Loss = -11264.680435787152
Iteration 5200: Loss = -11264.67981608118
Iteration 5300: Loss = -11264.679170334291
Iteration 5400: Loss = -11264.679035641526
Iteration 5500: Loss = -11264.678102312522
Iteration 5600: Loss = -11264.677537779571
Iteration 5700: Loss = -11264.677911021321
1
Iteration 5800: Loss = -11264.674036378638
Iteration 5900: Loss = -11264.6528214909
Iteration 6000: Loss = -11264.61124205351
Iteration 6100: Loss = -11264.569638137618
Iteration 6200: Loss = -11264.55192548798
Iteration 6300: Loss = -11264.551407162367
Iteration 6400: Loss = -11264.551309813076
Iteration 6500: Loss = -11264.552442831766
1
Iteration 6600: Loss = -11264.552284206147
2
Iteration 6700: Loss = -11264.550536742228
Iteration 6800: Loss = -11264.550381897714
Iteration 6900: Loss = -11264.550273888035
Iteration 7000: Loss = -11264.551074181341
1
Iteration 7100: Loss = -11264.54998534738
Iteration 7200: Loss = -11264.550422825541
1
Iteration 7300: Loss = -11264.549934871753
Iteration 7400: Loss = -11264.54961906964
Iteration 7500: Loss = -11264.55707478264
1
Iteration 7600: Loss = -11264.547542578573
Iteration 7700: Loss = -11264.547539474228
Iteration 7800: Loss = -11264.543322824287
Iteration 7900: Loss = -11264.559192826327
1
Iteration 8000: Loss = -11264.54315179217
Iteration 8100: Loss = -11264.54312957347
Iteration 8200: Loss = -11264.543086846246
Iteration 8300: Loss = -11264.54305385617
Iteration 8400: Loss = -11264.590498147614
1
Iteration 8500: Loss = -11264.544104589415
2
Iteration 8600: Loss = -11264.542913717036
Iteration 8700: Loss = -11264.544567272478
1
Iteration 8800: Loss = -11264.543812221964
2
Iteration 8900: Loss = -11264.542207468683
Iteration 9000: Loss = -11264.541496625488
Iteration 9100: Loss = -11264.711983805197
1
Iteration 9200: Loss = -11264.54144845376
Iteration 9300: Loss = -11264.549719384631
1
Iteration 9400: Loss = -11264.54524553122
2
Iteration 9500: Loss = -11264.544970045928
3
Iteration 9600: Loss = -11264.541315750052
Iteration 9700: Loss = -11264.554104355675
1
Iteration 9800: Loss = -11264.541589496297
2
Iteration 9900: Loss = -11264.541268655406
Iteration 10000: Loss = -11264.542565071757
1
Iteration 10100: Loss = -11264.592798428135
2
Iteration 10200: Loss = -11264.541127063609
Iteration 10300: Loss = -11264.558973221847
1
Iteration 10400: Loss = -11264.541047383991
Iteration 10500: Loss = -11264.5472543932
1
Iteration 10600: Loss = -11264.54108840393
Iteration 10700: Loss = -11264.635163050187
1
Iteration 10800: Loss = -11264.644506998462
2
Iteration 10900: Loss = -11264.54100512725
Iteration 11000: Loss = -11264.541260383778
1
Iteration 11100: Loss = -11264.542136335685
2
Iteration 11200: Loss = -11264.544213594332
3
Iteration 11300: Loss = -11264.540941705614
Iteration 11400: Loss = -11264.540833905803
Iteration 11500: Loss = -11264.541228002718
1
Iteration 11600: Loss = -11264.546674594514
2
Iteration 11700: Loss = -11264.65046036954
3
Iteration 11800: Loss = -11264.540633462791
Iteration 11900: Loss = -11264.56250385514
1
Iteration 12000: Loss = -11264.540584914572
Iteration 12100: Loss = -11264.540730762928
1
Iteration 12200: Loss = -11264.556643302281
2
Iteration 12300: Loss = -11264.540515940056
Iteration 12400: Loss = -11264.551031122408
1
Iteration 12500: Loss = -11264.540490613686
Iteration 12600: Loss = -11264.540954824095
1
Iteration 12700: Loss = -11264.606706258026
2
Iteration 12800: Loss = -11264.540464479063
Iteration 12900: Loss = -11264.540593025658
1
Iteration 13000: Loss = -11264.540503213166
Iteration 13100: Loss = -11264.540458408057
Iteration 13200: Loss = -11264.540518673179
Iteration 13300: Loss = -11264.551343446952
1
Iteration 13400: Loss = -11264.542051934106
2
Iteration 13500: Loss = -11264.540504957193
Iteration 13600: Loss = -11264.544926096418
1
Iteration 13700: Loss = -11264.54838500902
2
Iteration 13800: Loss = -11264.550633537325
3
Iteration 13900: Loss = -11264.541519169721
4
Iteration 14000: Loss = -11264.677582743076
5
Iteration 14100: Loss = -11264.543937320828
6
Iteration 14200: Loss = -11264.540494274406
Iteration 14300: Loss = -11264.557272416316
1
Iteration 14400: Loss = -11264.540711053478
2
Iteration 14500: Loss = -11264.540925641719
3
Iteration 14600: Loss = -11264.540842363725
4
Iteration 14700: Loss = -11264.540392279827
Iteration 14800: Loss = -11264.541097358227
1
Iteration 14900: Loss = -11264.540547869392
2
Iteration 15000: Loss = -11264.787377809786
3
Iteration 15100: Loss = -11264.549754356543
4
Iteration 15200: Loss = -11264.599133605892
5
Iteration 15300: Loss = -11264.540397991248
Iteration 15400: Loss = -11264.551962088799
1
Iteration 15500: Loss = -11264.54474779739
2
Iteration 15600: Loss = -11264.540513076956
3
Iteration 15700: Loss = -11264.540480191761
Iteration 15800: Loss = -11264.556973015191
1
Iteration 15900: Loss = -11264.552733509345
2
Iteration 16000: Loss = -11264.561347839393
3
Iteration 16100: Loss = -11264.561507703813
4
Iteration 16200: Loss = -11264.557448999005
5
Iteration 16300: Loss = -11264.652677799415
6
Iteration 16400: Loss = -11264.540691743818
7
Iteration 16500: Loss = -11264.540459413856
Iteration 16600: Loss = -11264.541624985457
1
Iteration 16700: Loss = -11264.542205792544
2
Iteration 16800: Loss = -11264.540427099388
Iteration 16900: Loss = -11264.540620248134
1
Iteration 17000: Loss = -11264.545744417133
2
Iteration 17100: Loss = -11264.540886350205
3
Iteration 17200: Loss = -11264.541610477305
4
Iteration 17300: Loss = -11264.558333859573
5
Iteration 17400: Loss = -11264.540763025987
6
Iteration 17500: Loss = -11264.541980068601
7
Iteration 17600: Loss = -11264.540409905881
Iteration 17700: Loss = -11264.540471911088
Iteration 17800: Loss = -11264.55331916708
1
Iteration 17900: Loss = -11264.540512054293
Iteration 18000: Loss = -11264.574870922592
1
Iteration 18100: Loss = -11264.544125100507
2
Iteration 18200: Loss = -11264.59597433008
3
Iteration 18300: Loss = -11264.635695582947
4
Iteration 18400: Loss = -11264.546518753272
5
Iteration 18500: Loss = -11264.540403311155
Iteration 18600: Loss = -11264.590760915613
1
Iteration 18700: Loss = -11264.542922530352
2
Iteration 18800: Loss = -11264.543611085
3
Iteration 18900: Loss = -11264.540471438037
Iteration 19000: Loss = -11264.554017418106
1
Iteration 19100: Loss = -11264.630130136318
2
Iteration 19200: Loss = -11264.54037519463
Iteration 19300: Loss = -11264.54169097499
1
Iteration 19400: Loss = -11264.542717017175
2
Iteration 19500: Loss = -11264.76527445784
3
Iteration 19600: Loss = -11264.541894977821
4
Iteration 19700: Loss = -11264.540592631487
5
Iteration 19800: Loss = -11264.542627852312
6
Iteration 19900: Loss = -11264.625709326661
7
pi: tensor([[0.6365, 0.3635],
        [0.2916, 0.7084]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8601, 0.1399], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2009, 0.1079],
         [0.6150, 0.2931]],

        [[0.6145, 0.0997],
         [0.5398, 0.6044]],

        [[0.7090, 0.0973],
         [0.5882, 0.5263]],

        [[0.6729, 0.1067],
         [0.7107, 0.6282]],

        [[0.6761, 0.0938],
         [0.5409, 0.5905]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.033695242120345935
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 2
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448509923071951
Global Adjusted Rand Index: 0.5232372110739644
Average Adjusted Rand Index: 0.7363536872125336
11228.01128650153
[0.9137632499893574, 0.5232372110739644] [0.9140986387303538, 0.7363536872125336] [11199.08515132088, 11264.540381106632]
-------------------------------------
This iteration is 64
True Objective function: Loss = -11297.64409900153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23184.696285114092
Iteration 100: Loss = -11505.041802886362
Iteration 200: Loss = -11503.242912184698
Iteration 300: Loss = -11498.522694236215
Iteration 400: Loss = -11496.672023365705
Iteration 500: Loss = -11488.339343575775
Iteration 600: Loss = -11459.22699306634
Iteration 700: Loss = -11457.939887102086
Iteration 800: Loss = -11457.7476675432
Iteration 900: Loss = -11457.652067565074
Iteration 1000: Loss = -11457.587348952457
Iteration 1100: Loss = -11457.530840753532
Iteration 1200: Loss = -11457.467959612079
Iteration 1300: Loss = -11457.37772353908
Iteration 1400: Loss = -11457.203027786409
Iteration 1500: Loss = -11456.864087578155
Iteration 1600: Loss = -11456.744107202327
Iteration 1700: Loss = -11456.69077128001
Iteration 1800: Loss = -11456.651790996233
Iteration 1900: Loss = -11456.615629501513
Iteration 2000: Loss = -11456.578541071452
Iteration 2100: Loss = -11456.539508658858
Iteration 2200: Loss = -11456.499710691061
Iteration 2300: Loss = -11456.461832224257
Iteration 2400: Loss = -11456.429709755186
Iteration 2500: Loss = -11456.405822641367
Iteration 2600: Loss = -11456.39016476725
Iteration 2700: Loss = -11456.38120847125
Iteration 2800: Loss = -11456.376441836926
Iteration 2900: Loss = -11456.373846375402
Iteration 3000: Loss = -11456.372192351697
Iteration 3100: Loss = -11456.37107427624
Iteration 3200: Loss = -11456.37023345643
Iteration 3300: Loss = -11456.369594220165
Iteration 3400: Loss = -11456.369080027738
Iteration 3500: Loss = -11456.36865052609
Iteration 3600: Loss = -11456.368258328397
Iteration 3700: Loss = -11456.367969999117
Iteration 3800: Loss = -11456.367675490555
Iteration 3900: Loss = -11456.36746722014
Iteration 4000: Loss = -11456.367221217599
Iteration 4100: Loss = -11456.367008763858
Iteration 4200: Loss = -11456.366835017421
Iteration 4300: Loss = -11456.366665843325
Iteration 4400: Loss = -11456.366521065165
Iteration 4500: Loss = -11456.36640580735
Iteration 4600: Loss = -11456.366271218963
Iteration 4700: Loss = -11456.366166989266
Iteration 4800: Loss = -11456.366029049086
Iteration 4900: Loss = -11456.365957907788
Iteration 5000: Loss = -11456.365884991432
Iteration 5100: Loss = -11456.36577086648
Iteration 5200: Loss = -11456.365721705612
Iteration 5300: Loss = -11456.36566202462
Iteration 5400: Loss = -11456.3656243812
Iteration 5500: Loss = -11456.36557033604
Iteration 5600: Loss = -11456.365446923823
Iteration 5700: Loss = -11456.365424469628
Iteration 5800: Loss = -11456.365378245348
Iteration 5900: Loss = -11456.365362506447
Iteration 6000: Loss = -11456.365328679223
Iteration 6100: Loss = -11456.365261260447
Iteration 6200: Loss = -11456.365244803057
Iteration 6300: Loss = -11456.366264282518
1
Iteration 6400: Loss = -11456.365182738444
Iteration 6500: Loss = -11456.36656875668
1
Iteration 6600: Loss = -11456.365063896197
Iteration 6700: Loss = -11456.36507223609
Iteration 6800: Loss = -11456.365219137235
1
Iteration 6900: Loss = -11456.364988811278
Iteration 7000: Loss = -11456.370738818094
1
Iteration 7100: Loss = -11456.36500217276
Iteration 7200: Loss = -11456.365107312893
1
Iteration 7300: Loss = -11456.365002306648
Iteration 7400: Loss = -11456.36513262612
1
Iteration 7500: Loss = -11456.364896376876
Iteration 7600: Loss = -11456.383315974328
1
Iteration 7700: Loss = -11456.364877993366
Iteration 7800: Loss = -11456.419176953905
1
Iteration 7900: Loss = -11456.364833715372
Iteration 8000: Loss = -11456.364871762878
Iteration 8100: Loss = -11456.364854013766
Iteration 8200: Loss = -11456.364798106588
Iteration 8300: Loss = -11456.908755829854
1
Iteration 8400: Loss = -11456.364751384282
Iteration 8500: Loss = -11456.364660414452
Iteration 8600: Loss = -11456.364006115587
Iteration 8700: Loss = -11456.36539459183
1
Iteration 8800: Loss = -11456.363843322075
Iteration 8900: Loss = -11456.36382264083
Iteration 9000: Loss = -11456.364893165204
1
Iteration 9100: Loss = -11456.363806292096
Iteration 9200: Loss = -11456.363793284554
Iteration 9300: Loss = -11456.36382713861
Iteration 9400: Loss = -11456.363799060104
Iteration 9500: Loss = -11456.363775359163
Iteration 9600: Loss = -11456.365217524852
1
Iteration 9700: Loss = -11456.363769364194
Iteration 9800: Loss = -11456.363777080993
Iteration 9900: Loss = -11456.383338858934
1
Iteration 10000: Loss = -11456.363778876315
Iteration 10100: Loss = -11456.363775871177
Iteration 10200: Loss = -11456.380345588217
1
Iteration 10300: Loss = -11456.363734811657
Iteration 10400: Loss = -11456.80907423747
1
Iteration 10500: Loss = -11456.363767220471
Iteration 10600: Loss = -11456.36375886585
Iteration 10700: Loss = -11456.363833511145
Iteration 10800: Loss = -11456.363754543676
Iteration 10900: Loss = -11456.47697540107
1
Iteration 11000: Loss = -11456.363717642545
Iteration 11100: Loss = -11456.712468532207
1
Iteration 11200: Loss = -11456.363382043488
Iteration 11300: Loss = -11456.363314193968
Iteration 11400: Loss = -11456.363579529354
1
Iteration 11500: Loss = -11456.363299007733
Iteration 11600: Loss = -11456.581565513401
1
Iteration 11700: Loss = -11456.363351322841
Iteration 11800: Loss = -11456.363316522287
Iteration 11900: Loss = -11456.363923736162
1
Iteration 12000: Loss = -11456.363330216522
Iteration 12100: Loss = -11456.363298711316
Iteration 12200: Loss = -11456.38830556481
1
Iteration 12300: Loss = -11456.363348928768
Iteration 12400: Loss = -11456.363305558105
Iteration 12500: Loss = -11456.363657406784
1
Iteration 12600: Loss = -11456.36336484137
Iteration 12700: Loss = -11456.363313207154
Iteration 12800: Loss = -11456.3632928724
Iteration 12900: Loss = -11456.363367498063
Iteration 13000: Loss = -11456.363298380813
Iteration 13100: Loss = -11456.371109795335
1
Iteration 13200: Loss = -11456.363324231148
Iteration 13300: Loss = -11456.372844046558
1
Iteration 13400: Loss = -11456.414908800622
2
Iteration 13500: Loss = -11456.363302899952
Iteration 13600: Loss = -11456.363550371387
1
Iteration 13700: Loss = -11456.363640728083
2
Iteration 13800: Loss = -11456.363318060045
Iteration 13900: Loss = -11456.446243329921
1
Iteration 14000: Loss = -11456.36437122212
2
Iteration 14100: Loss = -11456.363326573373
Iteration 14200: Loss = -11456.368848410077
1
Iteration 14300: Loss = -11456.363286887059
Iteration 14400: Loss = -11456.363439420324
1
Iteration 14500: Loss = -11456.363940648778
2
Iteration 14600: Loss = -11456.363275958152
Iteration 14700: Loss = -11456.363446861285
1
Iteration 14800: Loss = -11456.36332813836
Iteration 14900: Loss = -11456.392624277292
1
Iteration 15000: Loss = -11456.363283255025
Iteration 15100: Loss = -11456.382201225326
1
Iteration 15200: Loss = -11456.363280041303
Iteration 15300: Loss = -11456.399673023181
1
Iteration 15400: Loss = -11456.363328052095
Iteration 15500: Loss = -11456.370787234191
1
Iteration 15600: Loss = -11456.3633133457
Iteration 15700: Loss = -11456.363860411799
1
Iteration 15800: Loss = -11456.368524118914
2
Iteration 15900: Loss = -11456.36331274498
Iteration 16000: Loss = -11456.363963668773
1
Iteration 16100: Loss = -11456.363382825066
Iteration 16200: Loss = -11456.363299820221
Iteration 16300: Loss = -11456.36337412321
Iteration 16400: Loss = -11456.373939894926
1
Iteration 16500: Loss = -11456.363345528674
Iteration 16600: Loss = -11456.363729546074
1
Iteration 16700: Loss = -11456.363311064684
Iteration 16800: Loss = -11456.363573352583
1
Iteration 16900: Loss = -11456.363317034842
Iteration 17000: Loss = -11456.363615461196
1
Iteration 17100: Loss = -11456.374380705358
2
Iteration 17200: Loss = -11456.363326607196
Iteration 17300: Loss = -11456.364629411602
1
Iteration 17400: Loss = -11456.36486202324
2
Iteration 17500: Loss = -11456.3633283503
Iteration 17600: Loss = -11456.385150782215
1
Iteration 17700: Loss = -11456.363528222955
2
Iteration 17800: Loss = -11456.36333053856
Iteration 17900: Loss = -11456.366064332487
1
Iteration 18000: Loss = -11456.36459563945
2
Iteration 18100: Loss = -11456.365020868574
3
Iteration 18200: Loss = -11456.368185612633
4
Iteration 18300: Loss = -11456.363393104104
Iteration 18400: Loss = -11456.548993541415
1
Iteration 18500: Loss = -11456.363280486603
Iteration 18600: Loss = -11456.363497546481
1
Iteration 18700: Loss = -11456.394554154918
2
Iteration 18800: Loss = -11456.363269380423
Iteration 18900: Loss = -11456.363502255352
1
Iteration 19000: Loss = -11456.396302878891
2
Iteration 19100: Loss = -11456.363288572187
Iteration 19200: Loss = -11456.367091353592
1
Iteration 19300: Loss = -11456.36329306888
Iteration 19400: Loss = -11456.451398307761
1
Iteration 19500: Loss = -11456.36331110741
Iteration 19600: Loss = -11456.389763387017
1
Iteration 19700: Loss = -11456.535822908014
2
Iteration 19800: Loss = -11456.364514601892
3
Iteration 19900: Loss = -11456.363272247156
pi: tensor([[0.9731, 0.0269],
        [0.9421, 0.0579]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4996, 0.5004], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1759, 0.1141],
         [0.5110, 0.3161]],

        [[0.5759, 0.0944],
         [0.6009, 0.6450]],

        [[0.7301, 0.1870],
         [0.5242, 0.6119]],

        [[0.7253, 0.1921],
         [0.5598, 0.6472]],

        [[0.6662, 0.2239],
         [0.6453, 0.5021]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.736960421744899
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.04708277547009064
Average Adjusted Rand Index: 0.14725873987121002
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24410.600268471015
Iteration 100: Loss = -11503.600621235903
Iteration 200: Loss = -11500.534344934003
Iteration 300: Loss = -11498.139577855389
Iteration 400: Loss = -11495.607879550089
Iteration 500: Loss = -11464.266633112551
Iteration 600: Loss = -11457.899416878352
Iteration 700: Loss = -11457.75015062961
Iteration 800: Loss = -11457.687377394937
Iteration 900: Loss = -11457.651431220811
Iteration 1000: Loss = -11457.625997078669
Iteration 1100: Loss = -11457.605278654406
Iteration 1200: Loss = -11457.58574787713
Iteration 1300: Loss = -11457.563900548168
Iteration 1400: Loss = -11457.533977215964
Iteration 1500: Loss = -11457.481729924244
Iteration 1600: Loss = -11457.34702622672
Iteration 1700: Loss = -11456.887750214652
Iteration 1800: Loss = -11456.817809020056
Iteration 1900: Loss = -11456.803566986659
Iteration 2000: Loss = -11456.794369308174
Iteration 2100: Loss = -11456.78397520637
Iteration 2200: Loss = -11456.771730052518
Iteration 2300: Loss = -11456.758927792229
Iteration 2400: Loss = -11456.73678159459
Iteration 2500: Loss = -11456.679673809
Iteration 2600: Loss = -11456.440338508213
Iteration 2700: Loss = -11456.375730581285
Iteration 2800: Loss = -11456.371237966641
Iteration 2900: Loss = -11456.36867189008
Iteration 3000: Loss = -11456.367784727565
Iteration 3100: Loss = -11456.367399418734
Iteration 3200: Loss = -11456.367049512317
Iteration 3300: Loss = -11456.366737103655
Iteration 3400: Loss = -11456.366480199655
Iteration 3500: Loss = -11456.3662923368
Iteration 3600: Loss = -11456.366118604226
Iteration 3700: Loss = -11456.366045173148
Iteration 3800: Loss = -11456.365865321588
Iteration 3900: Loss = -11456.36572841147
Iteration 4000: Loss = -11456.365617083457
Iteration 4100: Loss = -11456.365591133426
Iteration 4200: Loss = -11456.365500607348
Iteration 4300: Loss = -11456.365463161297
Iteration 4400: Loss = -11456.365385329964
Iteration 4500: Loss = -11456.36537818099
Iteration 4600: Loss = -11456.365344765647
Iteration 4700: Loss = -11456.365277372166
Iteration 4800: Loss = -11456.365249164926
Iteration 4900: Loss = -11456.365240773064
Iteration 5000: Loss = -11456.365192096657
Iteration 5100: Loss = -11456.365163814271
Iteration 5200: Loss = -11456.365119371912
Iteration 5300: Loss = -11456.365057066956
Iteration 5400: Loss = -11456.365095883402
Iteration 5500: Loss = -11456.365056351968
Iteration 5600: Loss = -11456.36501378847
Iteration 5700: Loss = -11456.364981649169
Iteration 5800: Loss = -11456.364949385526
Iteration 5900: Loss = -11456.364936761916
Iteration 6000: Loss = -11456.364961332509
Iteration 6100: Loss = -11456.364897360725
Iteration 6200: Loss = -11456.370099252128
1
Iteration 6300: Loss = -11456.364908926787
Iteration 6400: Loss = -11456.364972239484
Iteration 6500: Loss = -11456.367915516139
1
Iteration 6600: Loss = -11456.364876450456
Iteration 6700: Loss = -11456.365612740216
1
Iteration 6800: Loss = -11456.36586692541
2
Iteration 6900: Loss = -11456.364956002499
Iteration 7000: Loss = -11456.36487049483
Iteration 7100: Loss = -11456.36532799065
1
Iteration 7200: Loss = -11456.36918332266
2
Iteration 7300: Loss = -11456.364973715725
3
Iteration 7400: Loss = -11456.367831196749
4
Iteration 7500: Loss = -11456.368623876
5
Iteration 7600: Loss = -11456.365133709902
6
Iteration 7700: Loss = -11456.364870234003
Iteration 7800: Loss = -11456.364983932788
1
Iteration 7900: Loss = -11456.364963131025
Iteration 8000: Loss = -11456.367670294667
1
Iteration 8100: Loss = -11456.366765952243
2
Iteration 8200: Loss = -11456.380095969573
3
Iteration 8300: Loss = -11456.36481109215
Iteration 8400: Loss = -11456.364829124454
Iteration 8500: Loss = -11456.376314699351
1
Iteration 8600: Loss = -11456.36476916531
Iteration 8700: Loss = -11456.365081344708
1
Iteration 8800: Loss = -11456.364782873803
Iteration 8900: Loss = -11456.439007946676
1
Iteration 9000: Loss = -11456.36474865715
Iteration 9100: Loss = -11456.364748072554
Iteration 9200: Loss = -11456.415930196501
1
Iteration 9300: Loss = -11456.364707099772
Iteration 9400: Loss = -11456.364724738489
Iteration 9500: Loss = -11456.460798541468
1
Iteration 9600: Loss = -11456.36472968781
Iteration 9700: Loss = -11456.364746096895
Iteration 9800: Loss = -11456.373131490673
1
Iteration 9900: Loss = -11456.364732164426
Iteration 10000: Loss = -11456.364702197687
Iteration 10100: Loss = -11456.370880020528
1
Iteration 10200: Loss = -11456.363742958114
Iteration 10300: Loss = -11456.363782300197
Iteration 10400: Loss = -11456.36383366978
Iteration 10500: Loss = -11456.363745538083
Iteration 10600: Loss = -11456.43204404385
1
Iteration 10700: Loss = -11456.363726415391
Iteration 10800: Loss = -11456.363739436027
Iteration 10900: Loss = -11456.365809919358
1
Iteration 11000: Loss = -11456.363747504745
Iteration 11100: Loss = -11456.363734058214
Iteration 11200: Loss = -11456.363743158958
Iteration 11300: Loss = -11456.363883425835
1
Iteration 11400: Loss = -11456.363720842617
Iteration 11500: Loss = -11456.393583658852
1
Iteration 11600: Loss = -11456.364040966475
2
Iteration 11700: Loss = -11456.363785899122
Iteration 11800: Loss = -11456.373570570478
1
Iteration 11900: Loss = -11456.363725611049
Iteration 12000: Loss = -11456.367187294285
1
Iteration 12100: Loss = -11456.371064825993
2
Iteration 12200: Loss = -11456.367822804783
3
Iteration 12300: Loss = -11456.363742734407
Iteration 12400: Loss = -11456.447317140703
1
Iteration 12500: Loss = -11456.36332024201
Iteration 12600: Loss = -11456.486021041144
1
Iteration 12700: Loss = -11456.363322620637
Iteration 12800: Loss = -11456.383955325597
1
Iteration 12900: Loss = -11456.392347612806
2
Iteration 13000: Loss = -11456.367121038922
3
Iteration 13100: Loss = -11456.363363975566
Iteration 13200: Loss = -11456.363367593047
Iteration 13300: Loss = -11456.364960943627
1
Iteration 13400: Loss = -11456.363312863368
Iteration 13500: Loss = -11456.36367675957
1
Iteration 13600: Loss = -11456.363295885149
Iteration 13700: Loss = -11456.36372317798
1
Iteration 13800: Loss = -11456.363280172163
Iteration 13900: Loss = -11456.364010988604
1
Iteration 14000: Loss = -11456.363285244368
Iteration 14100: Loss = -11456.363921256545
1
Iteration 14200: Loss = -11456.363302678468
Iteration 14300: Loss = -11456.363755417968
1
Iteration 14400: Loss = -11456.37955328054
2
Iteration 14500: Loss = -11456.363287092723
Iteration 14600: Loss = -11456.363499415773
1
Iteration 14700: Loss = -11456.363347378132
Iteration 14800: Loss = -11456.376538238595
1
Iteration 14900: Loss = -11456.363293394294
Iteration 15000: Loss = -11456.392021927437
1
Iteration 15100: Loss = -11456.363296860321
Iteration 15200: Loss = -11456.363471082544
1
Iteration 15300: Loss = -11456.36332596695
Iteration 15400: Loss = -11456.370464160782
1
Iteration 15500: Loss = -11456.363275100515
Iteration 15600: Loss = -11456.363353017226
Iteration 15700: Loss = -11456.448018571222
1
Iteration 15800: Loss = -11456.36328222434
Iteration 15900: Loss = -11456.363332677754
Iteration 16000: Loss = -11456.36330281956
Iteration 16100: Loss = -11456.36822924592
1
Iteration 16200: Loss = -11456.363692723531
2
Iteration 16300: Loss = -11456.36518209466
3
Iteration 16400: Loss = -11456.363307689457
Iteration 16500: Loss = -11456.36349243017
1
Iteration 16600: Loss = -11456.36329206391
Iteration 16700: Loss = -11456.363730737368
1
Iteration 16800: Loss = -11456.363334438705
Iteration 16900: Loss = -11456.416500553796
1
Iteration 17000: Loss = -11456.363281598526
Iteration 17100: Loss = -11456.363858384084
1
Iteration 17200: Loss = -11456.363295413983
Iteration 17300: Loss = -11456.36384234896
1
Iteration 17400: Loss = -11456.365211245777
2
Iteration 17500: Loss = -11456.363326636296
Iteration 17600: Loss = -11456.363849781876
1
Iteration 17700: Loss = -11456.363310818582
Iteration 17800: Loss = -11456.36361716961
1
Iteration 17900: Loss = -11456.61451763711
2
Iteration 18000: Loss = -11456.363308961547
Iteration 18100: Loss = -11456.376107821234
1
Iteration 18200: Loss = -11456.363299805525
Iteration 18300: Loss = -11456.397473004135
1
Iteration 18400: Loss = -11456.363298221737
Iteration 18500: Loss = -11456.363998898514
1
Iteration 18600: Loss = -11456.363317615394
Iteration 18700: Loss = -11456.365130139635
1
Iteration 18800: Loss = -11456.36332459545
Iteration 18900: Loss = -11456.365251954185
1
Iteration 19000: Loss = -11456.363889173652
2
Iteration 19100: Loss = -11456.369921386437
3
Iteration 19200: Loss = -11456.367604320247
4
Iteration 19300: Loss = -11456.375084999781
5
Iteration 19400: Loss = -11456.363330717737
Iteration 19500: Loss = -11456.379932397
1
Iteration 19600: Loss = -11456.363276143702
Iteration 19700: Loss = -11456.36346076729
1
Iteration 19800: Loss = -11456.363265768567
Iteration 19900: Loss = -11456.363789631927
1
pi: tensor([[0.9728, 0.0272],
        [0.9420, 0.0580]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4947, 0.5053], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1758, 0.1131],
         [0.6670, 0.3138]],

        [[0.5537, 0.0949],
         [0.7082, 0.7025]],

        [[0.5855, 0.1872],
         [0.6867, 0.6541]],

        [[0.5019, 0.1924],
         [0.5784, 0.7294]],

        [[0.5154, 0.2231],
         [0.6953, 0.6253]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.736960421744899
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.04708277547009064
Average Adjusted Rand Index: 0.14725873987121002
11297.64409900153
[0.04708277547009064, 0.04708277547009064] [0.14725873987121002, 0.14725873987121002] [11456.378100491118, 11456.396545568852]
-------------------------------------
This iteration is 65
True Objective function: Loss = -11090.972388192185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23576.537795356
Iteration 100: Loss = -11344.535107194142
Iteration 200: Loss = -11341.85558810009
Iteration 300: Loss = -11340.804868363006
Iteration 400: Loss = -11339.297009473708
Iteration 500: Loss = -11337.886797311834
Iteration 600: Loss = -11336.329300921421
Iteration 700: Loss = -11308.557537748564
Iteration 800: Loss = -11219.997301987398
Iteration 900: Loss = -11188.233933524085
Iteration 1000: Loss = -11175.182613765715
Iteration 1100: Loss = -11150.604488227125
Iteration 1200: Loss = -11130.94942436771
Iteration 1300: Loss = -11127.41798243344
Iteration 1400: Loss = -11121.648603685415
Iteration 1500: Loss = -11116.885394570587
Iteration 1600: Loss = -11116.784742041216
Iteration 1700: Loss = -11116.62367591647
Iteration 1800: Loss = -11116.577338203027
Iteration 1900: Loss = -11113.383928875766
Iteration 2000: Loss = -11111.497432970835
Iteration 2100: Loss = -11110.595216330115
Iteration 2200: Loss = -11110.487411224147
Iteration 2300: Loss = -11110.383045414075
Iteration 2400: Loss = -11110.368579452173
Iteration 2500: Loss = -11110.360363959466
Iteration 2600: Loss = -11110.343905074991
Iteration 2700: Loss = -11110.335311623634
Iteration 2800: Loss = -11110.327901833913
Iteration 2900: Loss = -11110.318361677935
Iteration 3000: Loss = -11109.953862247485
Iteration 3100: Loss = -11109.94665189389
Iteration 3200: Loss = -11109.942096357845
Iteration 3300: Loss = -11109.91933373218
Iteration 3400: Loss = -11109.89682804678
Iteration 3500: Loss = -11109.893959539177
Iteration 3600: Loss = -11109.908842043007
1
Iteration 3700: Loss = -11109.888255526286
Iteration 3800: Loss = -11109.886210801975
Iteration 3900: Loss = -11109.883573631629
Iteration 4000: Loss = -11109.879783909772
Iteration 4100: Loss = -11109.891644512369
1
Iteration 4200: Loss = -11109.877102016893
Iteration 4300: Loss = -11109.874575910937
Iteration 4400: Loss = -11109.873316359932
Iteration 4500: Loss = -11109.87173683176
Iteration 4600: Loss = -11109.870511602745
Iteration 4700: Loss = -11109.869270488865
Iteration 4800: Loss = -11109.875336777332
1
Iteration 4900: Loss = -11109.866244990666
Iteration 5000: Loss = -11109.864627810162
Iteration 5100: Loss = -11109.868046461204
1
Iteration 5200: Loss = -11109.860739804186
Iteration 5300: Loss = -11109.857698674105
Iteration 5400: Loss = -11109.8654916236
1
Iteration 5500: Loss = -11109.843513936008
Iteration 5600: Loss = -11109.62008759518
Iteration 5700: Loss = -11103.95290849734
Iteration 5800: Loss = -11099.518140154974
Iteration 5900: Loss = -11096.546990859164
Iteration 6000: Loss = -11096.360851445841
Iteration 6100: Loss = -11095.476721764166
Iteration 6200: Loss = -11095.075436952266
Iteration 6300: Loss = -11095.067563694929
Iteration 6400: Loss = -11095.062301592578
Iteration 6500: Loss = -11095.052777959376
Iteration 6600: Loss = -11094.266956495037
Iteration 6700: Loss = -11094.191845314948
Iteration 6800: Loss = -11094.190311714752
Iteration 6900: Loss = -11094.188201271307
Iteration 7000: Loss = -11094.188031859769
Iteration 7100: Loss = -11094.196815645406
1
Iteration 7200: Loss = -11094.194897912343
2
Iteration 7300: Loss = -11094.187170509394
Iteration 7400: Loss = -11094.187134766014
Iteration 7500: Loss = -11094.186740228617
Iteration 7600: Loss = -11094.185542244108
Iteration 7700: Loss = -11094.185296338994
Iteration 7800: Loss = -11094.186642859831
1
Iteration 7900: Loss = -11094.184813544236
Iteration 8000: Loss = -11094.184403974332
Iteration 8100: Loss = -11094.185705980706
1
Iteration 8200: Loss = -11094.183940861698
Iteration 8300: Loss = -11094.183822020275
Iteration 8400: Loss = -11094.192832451381
1
Iteration 8500: Loss = -11094.183648136886
Iteration 8600: Loss = -11094.234228349795
1
Iteration 8700: Loss = -11094.18348283087
Iteration 8800: Loss = -11094.183434198036
Iteration 8900: Loss = -11094.18344015963
Iteration 9000: Loss = -11094.18337582786
Iteration 9100: Loss = -11094.184477922481
1
Iteration 9200: Loss = -11094.183280792437
Iteration 9300: Loss = -11094.188025648835
1
Iteration 9400: Loss = -11094.183862858696
2
Iteration 9500: Loss = -11094.18394784918
3
Iteration 9600: Loss = -11094.183407355093
4
Iteration 9700: Loss = -11094.183121173595
Iteration 9800: Loss = -11094.182990353669
Iteration 9900: Loss = -11094.18312470724
1
Iteration 10000: Loss = -11094.182893812778
Iteration 10100: Loss = -11094.184796093372
1
Iteration 10200: Loss = -11094.18246121693
Iteration 10300: Loss = -11094.184679112755
1
Iteration 10400: Loss = -11094.181715381585
Iteration 10500: Loss = -11094.16882434074
Iteration 10600: Loss = -11094.171929039623
1
Iteration 10700: Loss = -11094.16946245775
2
Iteration 10800: Loss = -11094.16848326295
Iteration 10900: Loss = -11094.169690903209
1
Iteration 11000: Loss = -11094.169853118676
2
Iteration 11100: Loss = -11094.169913866528
3
Iteration 11200: Loss = -11094.199640934925
4
Iteration 11300: Loss = -11094.165879337026
Iteration 11400: Loss = -11094.155509271464
Iteration 11500: Loss = -11094.149016494352
Iteration 11600: Loss = -11094.144418647498
Iteration 11700: Loss = -11094.144573238264
1
Iteration 11800: Loss = -11094.153080787619
2
Iteration 11900: Loss = -11094.144290118937
Iteration 12000: Loss = -11094.144507056148
1
Iteration 12100: Loss = -11094.144487410611
2
Iteration 12200: Loss = -11094.14450377162
3
Iteration 12300: Loss = -11094.145314808731
4
Iteration 12400: Loss = -11094.15846147526
5
Iteration 12500: Loss = -11094.1466799552
6
Iteration 12600: Loss = -11094.144570350727
7
Iteration 12700: Loss = -11094.169192921981
8
Iteration 12800: Loss = -11094.144206195451
Iteration 12900: Loss = -11094.14524040636
1
Iteration 13000: Loss = -11094.144610988442
2
Iteration 13100: Loss = -11094.144228842351
Iteration 13200: Loss = -11094.209155585797
1
Iteration 13300: Loss = -11094.14344300036
Iteration 13400: Loss = -11094.143617686672
1
Iteration 13500: Loss = -11094.138906486167
Iteration 13600: Loss = -11094.136755933041
Iteration 13700: Loss = -11094.138505648847
1
Iteration 13800: Loss = -11094.17158194556
2
Iteration 13900: Loss = -11094.13673690838
Iteration 14000: Loss = -11094.136058695522
Iteration 14100: Loss = -11094.136466676713
1
Iteration 14200: Loss = -11094.14360013056
2
Iteration 14300: Loss = -11094.136103883404
Iteration 14400: Loss = -11094.136056968573
Iteration 14500: Loss = -11094.206613909502
1
Iteration 14600: Loss = -11094.138399953095
2
Iteration 14700: Loss = -11094.137301170038
3
Iteration 14800: Loss = -11094.157581694484
4
Iteration 14900: Loss = -11094.206309661944
5
Iteration 15000: Loss = -11094.136168330175
6
Iteration 15100: Loss = -11094.137782243717
7
Iteration 15200: Loss = -11094.144379590367
8
Iteration 15300: Loss = -11094.135974263218
Iteration 15400: Loss = -11094.137218697011
1
Iteration 15500: Loss = -11094.184755116785
2
Iteration 15600: Loss = -11094.135914830678
Iteration 15700: Loss = -11094.130791888236
Iteration 15800: Loss = -11094.156819472259
1
Iteration 15900: Loss = -11094.12991386269
Iteration 16000: Loss = -11094.130031988161
1
Iteration 16100: Loss = -11094.130094036771
2
Iteration 16200: Loss = -11094.130004783929
Iteration 16300: Loss = -11094.158088088447
1
Iteration 16400: Loss = -11094.12996411958
Iteration 16500: Loss = -11094.18865100659
1
Iteration 16600: Loss = -11094.175726036805
2
Iteration 16700: Loss = -11094.12994416011
Iteration 16800: Loss = -11094.133272367506
1
Iteration 16900: Loss = -11094.130987916165
2
Iteration 17000: Loss = -11094.135861527462
3
Iteration 17100: Loss = -11094.137550576303
4
Iteration 17200: Loss = -11094.131059654332
5
Iteration 17300: Loss = -11094.129968054887
Iteration 17400: Loss = -11094.139491368778
1
Iteration 17500: Loss = -11094.160826677296
2
Iteration 17600: Loss = -11094.131737612419
3
Iteration 17700: Loss = -11094.130577043263
4
Iteration 17800: Loss = -11094.131909339867
5
Iteration 17900: Loss = -11094.129938431277
Iteration 18000: Loss = -11094.130024835058
Iteration 18100: Loss = -11094.154860885094
1
Iteration 18200: Loss = -11094.129974224534
Iteration 18300: Loss = -11094.130200518524
1
Iteration 18400: Loss = -11094.141253934053
2
Iteration 18500: Loss = -11094.132236806498
3
Iteration 18600: Loss = -11094.17102112574
4
Iteration 18700: Loss = -11094.144147652964
5
Iteration 18800: Loss = -11094.130085485009
6
Iteration 18900: Loss = -11094.13125447653
7
Iteration 19000: Loss = -11094.197893319499
8
Iteration 19100: Loss = -11094.13451558748
9
Iteration 19200: Loss = -11094.177452785594
10
Iteration 19300: Loss = -11094.153160016329
11
Iteration 19400: Loss = -11094.129856355337
Iteration 19500: Loss = -11094.130494187413
1
Iteration 19600: Loss = -11094.129859785333
Iteration 19700: Loss = -11094.130141362908
1
Iteration 19800: Loss = -11094.129838925674
Iteration 19900: Loss = -11094.130238743624
1
pi: tensor([[0.6738, 0.3262],
        [0.3274, 0.6726]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2030, 0.7970], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3037, 0.0906],
         [0.6118, 0.2048]],

        [[0.6877, 0.0915],
         [0.6794, 0.6509]],

        [[0.6819, 0.0913],
         [0.5611, 0.6395]],

        [[0.5857, 0.1001],
         [0.5536, 0.6274]],

        [[0.5811, 0.1010],
         [0.6661, 0.7069]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 71
Adjusted Rand Index: 0.17074174476317683
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.48338387242127756
Average Adjusted Rand Index: 0.7944711424394307
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22660.410600394523
Iteration 100: Loss = -11344.424983154278
Iteration 200: Loss = -11341.557825979748
Iteration 300: Loss = -11340.45343635632
Iteration 400: Loss = -11339.504154920454
Iteration 500: Loss = -11338.499851226787
Iteration 600: Loss = -11337.3508735477
Iteration 700: Loss = -11335.716944798305
Iteration 800: Loss = -11290.752190088471
Iteration 900: Loss = -11201.42355886414
Iteration 1000: Loss = -11179.192704610676
Iteration 1100: Loss = -11177.118696785668
Iteration 1200: Loss = -11167.234280156674
Iteration 1300: Loss = -11162.039508475187
Iteration 1400: Loss = -11149.880175306867
Iteration 1500: Loss = -11147.869129877074
Iteration 1600: Loss = -11139.20809998748
Iteration 1700: Loss = -11139.048347650169
Iteration 1800: Loss = -11133.538425605197
Iteration 1900: Loss = -11127.926144636302
Iteration 2000: Loss = -11127.862097238378
Iteration 2100: Loss = -11127.82688875358
Iteration 2200: Loss = -11119.918933928915
Iteration 2300: Loss = -11119.803493075848
Iteration 2400: Loss = -11119.622578867238
Iteration 2500: Loss = -11116.975831291238
Iteration 2600: Loss = -11110.609208779888
Iteration 2700: Loss = -11108.62146300017
Iteration 2800: Loss = -11107.4275871824
Iteration 2900: Loss = -11107.287951271703
Iteration 3000: Loss = -11106.827400197255
Iteration 3100: Loss = -11106.80732340703
Iteration 3200: Loss = -11106.797311607632
Iteration 3300: Loss = -11106.782293522396
Iteration 3400: Loss = -11101.452731132234
Iteration 3500: Loss = -11101.385066829755
Iteration 3600: Loss = -11101.290881602547
Iteration 3700: Loss = -11101.30202721096
1
Iteration 3800: Loss = -11101.27969902498
Iteration 3900: Loss = -11101.276810733125
Iteration 4000: Loss = -11101.274103768097
Iteration 4100: Loss = -11101.266880686635
Iteration 4200: Loss = -11094.993138930873
Iteration 4300: Loss = -11094.973053245392
Iteration 4400: Loss = -11094.969579057817
Iteration 4500: Loss = -11094.966985657396
Iteration 4600: Loss = -11094.965190127577
Iteration 4700: Loss = -11094.963557762409
Iteration 4800: Loss = -11094.96823230625
1
Iteration 4900: Loss = -11094.96168929809
Iteration 5000: Loss = -11094.971206295597
1
Iteration 5100: Loss = -11094.959977128
Iteration 5200: Loss = -11094.960431904217
1
Iteration 5300: Loss = -11094.958584404896
Iteration 5400: Loss = -11094.957785760747
Iteration 5500: Loss = -11094.95561448746
Iteration 5600: Loss = -11094.953380175293
Iteration 5700: Loss = -11094.955618991811
1
Iteration 5800: Loss = -11094.951797421101
Iteration 5900: Loss = -11094.954758333413
1
Iteration 6000: Loss = -11094.951581279875
Iteration 6100: Loss = -11094.949630258398
Iteration 6200: Loss = -11094.94689112275
Iteration 6300: Loss = -11094.942098063817
Iteration 6400: Loss = -11094.941353535227
Iteration 6500: Loss = -11094.94080500738
Iteration 6600: Loss = -11094.940612589902
Iteration 6700: Loss = -11094.93984857746
Iteration 6800: Loss = -11094.93943243499
Iteration 6900: Loss = -11094.942333524596
1
Iteration 7000: Loss = -11094.938050669502
Iteration 7100: Loss = -11094.937882892433
Iteration 7200: Loss = -11094.93709902902
Iteration 7300: Loss = -11094.937096847307
Iteration 7400: Loss = -11094.936484030895
Iteration 7500: Loss = -11094.93622119607
Iteration 7600: Loss = -11094.943387851961
1
Iteration 7700: Loss = -11094.935951453741
Iteration 7800: Loss = -11094.948375683209
1
Iteration 7900: Loss = -11094.935729282957
Iteration 8000: Loss = -11094.947070575467
1
Iteration 8100: Loss = -11094.935556340683
Iteration 8200: Loss = -11094.941995791169
1
Iteration 8300: Loss = -11094.935445674906
Iteration 8400: Loss = -11094.935447643204
Iteration 8500: Loss = -11094.935160718607
Iteration 8600: Loss = -11094.935117523566
Iteration 8700: Loss = -11094.936149900725
1
Iteration 8800: Loss = -11094.934982816576
Iteration 8900: Loss = -11094.933834682712
Iteration 9000: Loss = -11094.934983952513
1
Iteration 9100: Loss = -11094.933590326953
Iteration 9200: Loss = -11094.935693349847
1
Iteration 9300: Loss = -11094.93286583481
Iteration 9400: Loss = -11094.932662883506
Iteration 9500: Loss = -11094.932237973546
Iteration 9600: Loss = -11094.951380012397
1
Iteration 9700: Loss = -11094.931941029206
Iteration 9800: Loss = -11094.93526994766
1
Iteration 9900: Loss = -11094.930173751623
Iteration 10000: Loss = -11095.083909975476
1
Iteration 10100: Loss = -11094.929827017562
Iteration 10200: Loss = -11094.93044593313
1
Iteration 10300: Loss = -11094.929875871178
Iteration 10400: Loss = -11094.935514665245
1
Iteration 10500: Loss = -11094.929938188843
Iteration 10600: Loss = -11094.92948162327
Iteration 10700: Loss = -11094.939458931958
1
Iteration 10800: Loss = -11094.924809957702
Iteration 10900: Loss = -11094.925110895709
1
Iteration 11000: Loss = -11094.924254626956
Iteration 11100: Loss = -11094.924592694557
1
Iteration 11200: Loss = -11094.939005167407
2
Iteration 11300: Loss = -11094.923540222706
Iteration 11400: Loss = -11094.923428155285
Iteration 11500: Loss = -11094.966896634412
1
Iteration 11600: Loss = -11094.925085110051
2
Iteration 11700: Loss = -11094.923789268185
3
Iteration 11800: Loss = -11094.923357389567
Iteration 11900: Loss = -11094.92500015303
1
Iteration 12000: Loss = -11094.92311547608
Iteration 12100: Loss = -11094.92457625055
1
Iteration 12200: Loss = -11094.939548720236
2
Iteration 12300: Loss = -11094.922867708887
Iteration 12400: Loss = -11094.922891109185
Iteration 12500: Loss = -11094.921815064374
Iteration 12600: Loss = -11094.92227188763
1
Iteration 12700: Loss = -11095.16910508666
2
Iteration 12800: Loss = -11094.9212421055
Iteration 12900: Loss = -11094.842323287798
Iteration 13000: Loss = -11094.992598671151
1
Iteration 13100: Loss = -11094.838662221979
Iteration 13200: Loss = -11094.792124908534
Iteration 13300: Loss = -11094.849325763105
1
Iteration 13400: Loss = -11094.798715474919
2
Iteration 13500: Loss = -11094.79168032141
Iteration 13600: Loss = -11094.79191390631
1
Iteration 13700: Loss = -11094.794374944271
2
Iteration 13800: Loss = -11094.83606705521
3
Iteration 13900: Loss = -11094.792992189521
4
Iteration 14000: Loss = -11094.791965105687
5
Iteration 14100: Loss = -11094.795432145143
6
Iteration 14200: Loss = -11094.85759300871
7
Iteration 14300: Loss = -11094.791254725966
Iteration 14400: Loss = -11094.792021490968
1
Iteration 14500: Loss = -11095.015265911648
2
Iteration 14600: Loss = -11094.785934507478
Iteration 14700: Loss = -11094.795374342846
1
Iteration 14800: Loss = -11094.779235937756
Iteration 14900: Loss = -11094.810726010319
1
Iteration 15000: Loss = -11094.778635793919
Iteration 15100: Loss = -11094.796772823724
1
Iteration 15200: Loss = -11094.782493670205
2
Iteration 15300: Loss = -11094.778416569508
Iteration 15400: Loss = -11094.779488281521
1
Iteration 15500: Loss = -11094.77887177704
2
Iteration 15600: Loss = -11094.782020913746
3
Iteration 15700: Loss = -11094.814042306674
4
Iteration 15800: Loss = -11094.778357455572
Iteration 15900: Loss = -11094.832253435236
1
Iteration 16000: Loss = -11094.78166993916
2
Iteration 16100: Loss = -11094.779186032021
3
Iteration 16200: Loss = -11094.797642761234
4
Iteration 16300: Loss = -11094.778310058322
Iteration 16400: Loss = -11094.777941021746
Iteration 16500: Loss = -11094.780804299831
1
Iteration 16600: Loss = -11094.780200882251
2
Iteration 16700: Loss = -11094.776661181972
Iteration 16800: Loss = -11094.788676119511
1
Iteration 16900: Loss = -11094.777651786739
2
Iteration 17000: Loss = -11094.782559103694
3
Iteration 17100: Loss = -11094.81039679073
4
Iteration 17200: Loss = -11094.777811525742
5
Iteration 17300: Loss = -11094.776913884454
6
Iteration 17400: Loss = -11094.890532433987
7
Iteration 17500: Loss = -11094.80004680194
8
Iteration 17600: Loss = -11094.776657267386
Iteration 17700: Loss = -11094.776962257596
1
Iteration 17800: Loss = -11094.858728143334
2
Iteration 17900: Loss = -11094.776660291614
Iteration 18000: Loss = -11094.777151073982
1
Iteration 18100: Loss = -11094.778690061621
2
Iteration 18200: Loss = -11094.79493229992
3
Iteration 18300: Loss = -11094.885266798728
4
Iteration 18400: Loss = -11094.79145554769
5
Iteration 18500: Loss = -11094.769974329665
Iteration 18600: Loss = -11094.771456254151
1
Iteration 18700: Loss = -11094.773956387045
2
Iteration 18800: Loss = -11094.768121941259
Iteration 18900: Loss = -11094.772400774275
1
Iteration 19000: Loss = -11094.767976516672
Iteration 19100: Loss = -11094.768314529365
1
Iteration 19200: Loss = -11094.767982555559
Iteration 19300: Loss = -11094.770477723485
1
Iteration 19400: Loss = -11094.76799209714
Iteration 19500: Loss = -11094.779485033361
1
Iteration 19600: Loss = -11094.768329876086
2
Iteration 19700: Loss = -11094.771183923995
3
Iteration 19800: Loss = -11094.82028722228
4
Iteration 19900: Loss = -11094.786135380202
5
pi: tensor([[0.6767, 0.3233],
        [0.3089, 0.6911]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8279, 0.1721], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2035, 0.0906],
         [0.5629, 0.3030]],

        [[0.6054, 0.0916],
         [0.5655, 0.5511]],

        [[0.6332, 0.0910],
         [0.5488, 0.5700]],

        [[0.6408, 0.1008],
         [0.6339, 0.5364]],

        [[0.6828, 0.1016],
         [0.5197, 0.6994]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 30
Adjusted Rand Index: 0.1544111187250716
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.48897910072538714
Average Adjusted Rand Index: 0.7912050172318097
11090.972388192185
[0.48338387242127756, 0.48897910072538714] [0.7944711424394307, 0.7912050172318097] [11094.159643437504, 11094.770661576935]
-------------------------------------
This iteration is 66
True Objective function: Loss = -11171.571997567185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20710.928503305116
Iteration 100: Loss = -11433.821698538353
Iteration 200: Loss = -11433.081161861757
Iteration 300: Loss = -11432.733229199443
Iteration 400: Loss = -11431.39563262614
Iteration 500: Loss = -11427.978466559734
Iteration 600: Loss = -11354.373010849406
Iteration 700: Loss = -11163.494348268749
Iteration 800: Loss = -11153.38474497148
Iteration 900: Loss = -11150.810985439408
Iteration 1000: Loss = -11143.857812924281
Iteration 1100: Loss = -11143.807502740636
Iteration 1200: Loss = -11143.762312824996
Iteration 1300: Loss = -11143.559404011865
Iteration 1400: Loss = -11143.537797907511
Iteration 1500: Loss = -11143.523332750467
Iteration 1600: Loss = -11143.511804298167
Iteration 1700: Loss = -11143.503498812199
Iteration 1800: Loss = -11143.496591672112
Iteration 1900: Loss = -11143.49049117107
Iteration 2000: Loss = -11143.484686882486
Iteration 2100: Loss = -11143.476986088133
Iteration 2200: Loss = -11143.46397650435
Iteration 2300: Loss = -11143.456545858093
Iteration 2400: Loss = -11143.449687890448
Iteration 2500: Loss = -11143.432308008323
Iteration 2600: Loss = -11142.923594528356
Iteration 2700: Loss = -11140.947011457582
Iteration 2800: Loss = -11140.92634081631
Iteration 2900: Loss = -11140.854257548584
Iteration 3000: Loss = -11140.859270567082
1
Iteration 3100: Loss = -11140.846181550109
Iteration 3200: Loss = -11140.84469260427
Iteration 3300: Loss = -11140.845068613882
1
Iteration 3400: Loss = -11140.829395291923
Iteration 3500: Loss = -11140.83818668259
1
Iteration 3600: Loss = -11140.826584202317
Iteration 3700: Loss = -11140.825784025646
Iteration 3800: Loss = -11140.824858623007
Iteration 3900: Loss = -11140.822811565562
Iteration 4000: Loss = -11140.820296701428
Iteration 4100: Loss = -11140.819470238901
Iteration 4200: Loss = -11140.819214043704
Iteration 4300: Loss = -11140.818835651788
Iteration 4400: Loss = -11140.818553118293
Iteration 4500: Loss = -11140.81901996539
1
Iteration 4600: Loss = -11140.818190359423
Iteration 4700: Loss = -11140.818230944524
Iteration 4800: Loss = -11140.817965349259
Iteration 4900: Loss = -11140.8184018489
1
Iteration 5000: Loss = -11140.817378078376
Iteration 5100: Loss = -11140.817670641916
1
Iteration 5200: Loss = -11140.820272302051
2
Iteration 5300: Loss = -11140.817187404924
Iteration 5400: Loss = -11140.816875167457
Iteration 5500: Loss = -11140.817953584588
1
Iteration 5600: Loss = -11140.722249025072
Iteration 5700: Loss = -11140.72179453841
Iteration 5800: Loss = -11140.722144922336
1
Iteration 5900: Loss = -11140.72449665479
2
Iteration 6000: Loss = -11140.721639510642
Iteration 6100: Loss = -11140.721129275184
Iteration 6200: Loss = -11140.7194113803
Iteration 6300: Loss = -11140.718934683957
Iteration 6400: Loss = -11140.718694418018
Iteration 6500: Loss = -11140.718512852778
Iteration 6600: Loss = -11140.718292591224
Iteration 6700: Loss = -11140.049236139788
Iteration 6800: Loss = -11139.715583619536
Iteration 6900: Loss = -11139.715317001308
Iteration 7000: Loss = -11139.718704567002
1
Iteration 7100: Loss = -11139.714957945313
Iteration 7200: Loss = -11139.714979103064
Iteration 7300: Loss = -11139.714752764357
Iteration 7400: Loss = -11139.714923344032
1
Iteration 7500: Loss = -11139.720467270372
2
Iteration 7600: Loss = -11139.714401079898
Iteration 7700: Loss = -11139.72700268658
1
Iteration 7800: Loss = -11139.71338224026
Iteration 7900: Loss = -11139.713202965911
Iteration 8000: Loss = -11139.704490153079
Iteration 8100: Loss = -11139.703246127678
Iteration 8200: Loss = -11139.703052141509
Iteration 8300: Loss = -11139.70278122168
Iteration 8400: Loss = -11139.702379515653
Iteration 8500: Loss = -11139.72013668213
1
Iteration 8600: Loss = -11139.702306028928
Iteration 8700: Loss = -11139.702279840556
Iteration 8800: Loss = -11139.70517296556
1
Iteration 8900: Loss = -11139.70226304123
Iteration 9000: Loss = -11139.702230053335
Iteration 9100: Loss = -11139.704361185748
1
Iteration 9200: Loss = -11139.702202785295
Iteration 9300: Loss = -11139.704941587483
1
Iteration 9400: Loss = -11139.72192208001
2
Iteration 9500: Loss = -11139.70216830868
Iteration 9600: Loss = -11139.707760112096
1
Iteration 9700: Loss = -11139.707718890522
2
Iteration 9800: Loss = -11139.881363221475
3
Iteration 9900: Loss = -11139.702145601923
Iteration 10000: Loss = -11139.702667677317
1
Iteration 10100: Loss = -11139.703167769625
2
Iteration 10200: Loss = -11139.702076185024
Iteration 10300: Loss = -11139.698785235021
Iteration 10400: Loss = -11139.704008214794
1
Iteration 10500: Loss = -11139.698505887249
Iteration 10600: Loss = -11139.910246942485
1
Iteration 10700: Loss = -11139.69872722156
2
Iteration 10800: Loss = -11139.697777797577
Iteration 10900: Loss = -11139.70672234305
1
Iteration 11000: Loss = -11139.69658854837
Iteration 11100: Loss = -11139.69728815775
1
Iteration 11200: Loss = -11139.712561047525
2
Iteration 11300: Loss = -11139.696616777055
Iteration 11400: Loss = -11139.697831543519
1
Iteration 11500: Loss = -11139.696676598893
Iteration 11600: Loss = -11139.69660119882
Iteration 11700: Loss = -11139.743078234647
1
Iteration 11800: Loss = -11139.698829098796
2
Iteration 11900: Loss = -11139.696967949063
3
Iteration 12000: Loss = -11139.698078059986
4
Iteration 12100: Loss = -11139.704443287239
5
Iteration 12200: Loss = -11139.69663470218
Iteration 12300: Loss = -11139.696604499622
Iteration 12400: Loss = -11139.767562387677
1
Iteration 12500: Loss = -11139.696525532052
Iteration 12600: Loss = -11139.700383081721
1
Iteration 12700: Loss = -11139.696508804178
Iteration 12800: Loss = -11139.696689093915
1
Iteration 12900: Loss = -11139.697146442877
2
Iteration 13000: Loss = -11139.696663558234
3
Iteration 13100: Loss = -11139.767802459986
4
Iteration 13200: Loss = -11139.696505995116
Iteration 13300: Loss = -11139.701485027943
1
Iteration 13400: Loss = -11139.696475817163
Iteration 13500: Loss = -11139.697534727755
1
Iteration 13600: Loss = -11139.697718195972
2
Iteration 13700: Loss = -11139.697876762944
3
Iteration 13800: Loss = -11139.704711880104
4
Iteration 13900: Loss = -11139.80019398436
5
Iteration 14000: Loss = -11139.696570733255
Iteration 14100: Loss = -11139.698103414985
1
Iteration 14200: Loss = -11139.705648799416
2
Iteration 14300: Loss = -11139.696772604268
3
Iteration 14400: Loss = -11139.697222969558
4
Iteration 14500: Loss = -11139.69748147579
5
Iteration 14600: Loss = -11139.71662110783
6
Iteration 14700: Loss = -11139.696391494293
Iteration 14800: Loss = -11139.715074560621
1
Iteration 14900: Loss = -11139.696160990865
Iteration 15000: Loss = -11139.704125795753
1
Iteration 15100: Loss = -11139.700365287887
2
Iteration 15200: Loss = -11139.696560186774
3
Iteration 15300: Loss = -11139.69754789609
4
Iteration 15400: Loss = -11139.739476628089
5
Iteration 15500: Loss = -11139.69567229875
Iteration 15600: Loss = -11139.747038875554
1
Iteration 15700: Loss = -11139.696031624371
2
Iteration 15800: Loss = -11139.695669294748
Iteration 15900: Loss = -11139.697420085913
1
Iteration 16000: Loss = -11139.695599829683
Iteration 16100: Loss = -11139.699072525233
1
Iteration 16200: Loss = -11139.695775912558
2
Iteration 16300: Loss = -11139.697048151063
3
Iteration 16400: Loss = -11139.780047863116
4
Iteration 16500: Loss = -11139.695591399997
Iteration 16600: Loss = -11139.697280128983
1
Iteration 16700: Loss = -11139.696054621352
2
Iteration 16800: Loss = -11139.693170440602
Iteration 16900: Loss = -11139.695921677536
1
Iteration 17000: Loss = -11139.69151371764
Iteration 17100: Loss = -11139.691673583951
1
Iteration 17200: Loss = -11139.692688155814
2
Iteration 17300: Loss = -11139.691478493723
Iteration 17400: Loss = -11139.692230235254
1
Iteration 17500: Loss = -11139.691537342558
Iteration 17600: Loss = -11139.691591112289
Iteration 17700: Loss = -11139.692643928582
1
Iteration 17800: Loss = -11139.70314904346
2
Iteration 17900: Loss = -11139.696805624235
3
Iteration 18000: Loss = -11139.692841240103
4
Iteration 18100: Loss = -11139.69257708562
5
Iteration 18200: Loss = -11139.697617860485
6
Iteration 18300: Loss = -11139.774331825003
7
Iteration 18400: Loss = -11139.693078504417
8
Iteration 18500: Loss = -11139.691848052491
9
Iteration 18600: Loss = -11139.691745203052
10
Iteration 18700: Loss = -11139.71681204969
11
Iteration 18800: Loss = -11139.691620928365
Iteration 18900: Loss = -11139.713038669073
1
Iteration 19000: Loss = -11139.718227309224
2
Iteration 19100: Loss = -11139.693390778011
3
Iteration 19200: Loss = -11139.695131131062
4
Iteration 19300: Loss = -11139.69092335521
Iteration 19400: Loss = -11139.693166883431
1
Iteration 19500: Loss = -11139.69190810588
2
Iteration 19600: Loss = -11139.693561682156
3
Iteration 19700: Loss = -11139.6908957695
Iteration 19800: Loss = -11139.69245964823
1
Iteration 19900: Loss = -11139.69533564241
2
pi: tensor([[0.7048, 0.2952],
        [0.2301, 0.7699]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4596, 0.5404], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3134, 0.0983],
         [0.5590, 0.2021]],

        [[0.6425, 0.1039],
         [0.5588, 0.5577]],

        [[0.7039, 0.1010],
         [0.5258, 0.7281]],

        [[0.7061, 0.1015],
         [0.5952, 0.7068]],

        [[0.5127, 0.0968],
         [0.5811, 0.5096]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207675179163246
Global Adjusted Rand Index: 0.9291502021908267
Average Adjusted Rand Index: 0.9291174909370383
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23404.613167173728
Iteration 100: Loss = -11434.040627674549
Iteration 200: Loss = -11433.494910507481
Iteration 300: Loss = -11433.279684343803
Iteration 400: Loss = -11433.045623721637
Iteration 500: Loss = -11432.469445591976
Iteration 600: Loss = -11429.679037673832
Iteration 700: Loss = -11426.807256890252
Iteration 800: Loss = -11400.09079543934
Iteration 900: Loss = -11297.844413867257
Iteration 1000: Loss = -11167.666394666567
Iteration 1100: Loss = -11147.287418122272
Iteration 1200: Loss = -11144.61217009207
Iteration 1300: Loss = -11142.037032971844
Iteration 1400: Loss = -11141.822021640286
Iteration 1500: Loss = -11141.735116175114
Iteration 1600: Loss = -11141.633199399446
Iteration 1700: Loss = -11140.939059950353
Iteration 1800: Loss = -11140.905767791768
Iteration 1900: Loss = -11140.8812329702
Iteration 2000: Loss = -11140.861493705766
Iteration 2100: Loss = -11140.844675717672
Iteration 2200: Loss = -11140.829036157356
Iteration 2300: Loss = -11140.81275361372
Iteration 2400: Loss = -11140.799130736506
Iteration 2500: Loss = -11140.78884124675
Iteration 2600: Loss = -11140.779225526901
Iteration 2700: Loss = -11140.767100007528
Iteration 2800: Loss = -11140.7601060715
Iteration 2900: Loss = -11140.754512042939
Iteration 3000: Loss = -11140.749639769923
Iteration 3100: Loss = -11140.745738334914
Iteration 3200: Loss = -11140.741648739304
Iteration 3300: Loss = -11140.738294602905
Iteration 3400: Loss = -11140.73357766458
Iteration 3500: Loss = -11140.729062038983
Iteration 3600: Loss = -11140.727507089125
Iteration 3700: Loss = -11140.72942620161
1
Iteration 3800: Loss = -11140.722287653098
Iteration 3900: Loss = -11140.721476279678
Iteration 4000: Loss = -11140.718277596221
Iteration 4100: Loss = -11140.716805369904
Iteration 4200: Loss = -11140.715352702473
Iteration 4300: Loss = -11140.713974874521
Iteration 4400: Loss = -11140.297506744242
Iteration 4500: Loss = -11139.709915006422
Iteration 4600: Loss = -11139.710303731306
1
Iteration 4700: Loss = -11139.707655043821
Iteration 4800: Loss = -11139.711269371612
1
Iteration 4900: Loss = -11139.70595010213
Iteration 5000: Loss = -11139.705607216149
Iteration 5100: Loss = -11139.707606470263
1
Iteration 5200: Loss = -11139.703914157919
Iteration 5300: Loss = -11139.704669555147
1
Iteration 5400: Loss = -11139.702906758534
Iteration 5500: Loss = -11139.702333103956
Iteration 5600: Loss = -11139.70340867459
1
Iteration 5700: Loss = -11139.704579765512
2
Iteration 5800: Loss = -11139.701396942939
Iteration 5900: Loss = -11139.701387686326
Iteration 6000: Loss = -11139.701563221222
1
Iteration 6100: Loss = -11139.705831548807
2
Iteration 6200: Loss = -11139.699601413304
Iteration 6300: Loss = -11139.711589878112
1
Iteration 6400: Loss = -11139.69917738947
Iteration 6500: Loss = -11139.698372201678
Iteration 6600: Loss = -11139.69798752955
Iteration 6700: Loss = -11139.697559292501
Iteration 6800: Loss = -11139.697826877107
1
Iteration 6900: Loss = -11139.697497272076
Iteration 7000: Loss = -11139.69683435395
Iteration 7100: Loss = -11139.696974470591
1
Iteration 7200: Loss = -11139.705182497284
2
Iteration 7300: Loss = -11139.697346948353
3
Iteration 7400: Loss = -11139.696340910015
Iteration 7500: Loss = -11139.699405070302
1
Iteration 7600: Loss = -11139.696237078768
Iteration 7700: Loss = -11139.695776396862
Iteration 7800: Loss = -11139.695721637412
Iteration 7900: Loss = -11139.69555841014
Iteration 8000: Loss = -11139.69617764805
1
Iteration 8100: Loss = -11139.695701433346
2
Iteration 8200: Loss = -11139.698015120683
3
Iteration 8300: Loss = -11139.706422517243
4
Iteration 8400: Loss = -11139.695033047667
Iteration 8500: Loss = -11139.757337813298
1
Iteration 8600: Loss = -11139.694891587662
Iteration 8700: Loss = -11139.694796926928
Iteration 8800: Loss = -11139.701704974825
1
Iteration 8900: Loss = -11139.694644982304
Iteration 9000: Loss = -11139.694497537874
Iteration 9100: Loss = -11139.922105737187
1
Iteration 9200: Loss = -11139.694059962576
Iteration 9300: Loss = -11139.69395955543
Iteration 9400: Loss = -11139.696256814994
1
Iteration 9500: Loss = -11139.693843677773
Iteration 9600: Loss = -11139.701134573488
1
Iteration 9700: Loss = -11139.693813504413
Iteration 9800: Loss = -11139.69432170535
1
Iteration 9900: Loss = -11139.69363908838
Iteration 10000: Loss = -11139.692703589495
Iteration 10100: Loss = -11139.691935780864
Iteration 10200: Loss = -11139.700659050315
1
Iteration 10300: Loss = -11139.691816379625
Iteration 10400: Loss = -11139.693667289745
1
Iteration 10500: Loss = -11139.69218819877
2
Iteration 10600: Loss = -11139.691749659816
Iteration 10700: Loss = -11139.701924505398
1
Iteration 10800: Loss = -11139.691735941276
Iteration 10900: Loss = -11139.6917674123
Iteration 11000: Loss = -11139.691758690482
Iteration 11100: Loss = -11139.693488862811
1
Iteration 11200: Loss = -11139.696478580834
2
Iteration 11300: Loss = -11139.69765799416
3
Iteration 11400: Loss = -11139.796844735962
4
Iteration 11500: Loss = -11139.692886420205
5
Iteration 11600: Loss = -11139.691900896902
6
Iteration 11700: Loss = -11139.701718824612
7
Iteration 11800: Loss = -11139.77323667234
8
Iteration 11900: Loss = -11139.79127205042
9
Iteration 12000: Loss = -11139.69315940264
10
Iteration 12100: Loss = -11139.691596143997
Iteration 12200: Loss = -11139.724317659437
1
Iteration 12300: Loss = -11139.691626380218
Iteration 12400: Loss = -11139.691503060893
Iteration 12500: Loss = -11139.691981618478
1
Iteration 12600: Loss = -11139.691468736177
Iteration 12700: Loss = -11139.700094968948
1
Iteration 12800: Loss = -11139.691412440168
Iteration 12900: Loss = -11139.69913511178
1
Iteration 13000: Loss = -11139.718994698209
2
Iteration 13100: Loss = -11139.691652737798
3
Iteration 13200: Loss = -11139.691859606643
4
Iteration 13300: Loss = -11139.69896329276
5
Iteration 13400: Loss = -11139.694516179643
6
Iteration 13500: Loss = -11139.691494477718
Iteration 13600: Loss = -11139.792430394406
1
Iteration 13700: Loss = -11139.691702535914
2
Iteration 13800: Loss = -11139.69165195974
3
Iteration 13900: Loss = -11139.691642692344
4
Iteration 14000: Loss = -11139.95641326072
5
Iteration 14100: Loss = -11139.691801372497
6
Iteration 14200: Loss = -11139.715512638943
7
Iteration 14300: Loss = -11139.691386742172
Iteration 14400: Loss = -11139.691455896347
Iteration 14500: Loss = -11139.696517920527
1
Iteration 14600: Loss = -11139.691890114307
2
Iteration 14700: Loss = -11139.691556365977
3
Iteration 14800: Loss = -11139.797747840033
4
Iteration 14900: Loss = -11139.692023410287
5
Iteration 15000: Loss = -11139.780741300125
6
Iteration 15100: Loss = -11139.691952535377
7
Iteration 15200: Loss = -11139.933455213124
8
Iteration 15300: Loss = -11139.701117432922
9
Iteration 15400: Loss = -11139.705211000057
10
Iteration 15500: Loss = -11139.691321528106
Iteration 15600: Loss = -11139.706700164375
1
Iteration 15700: Loss = -11139.693469763446
2
Iteration 15800: Loss = -11139.731925743923
3
Iteration 15900: Loss = -11139.691311501914
Iteration 16000: Loss = -11139.747347621484
1
Iteration 16100: Loss = -11139.691336588368
Iteration 16200: Loss = -11139.69148758843
1
Iteration 16300: Loss = -11139.698148954538
2
Iteration 16400: Loss = -11139.691476548354
3
Iteration 16500: Loss = -11139.693244422719
4
Iteration 16600: Loss = -11139.691387575747
Iteration 16700: Loss = -11139.691366204986
Iteration 16800: Loss = -11139.691315873755
Iteration 16900: Loss = -11139.691366019024
Iteration 17000: Loss = -11139.701763419782
1
Iteration 17100: Loss = -11139.691339343453
Iteration 17200: Loss = -11139.729297003025
1
Iteration 17300: Loss = -11139.71345284467
2
Iteration 17400: Loss = -11139.691419907887
Iteration 17500: Loss = -11139.692307515166
1
Iteration 17600: Loss = -11139.697547430866
2
Iteration 17700: Loss = -11139.690831300208
Iteration 17800: Loss = -11139.690912483646
Iteration 17900: Loss = -11139.695143480036
1
Iteration 18000: Loss = -11139.690789657883
Iteration 18100: Loss = -11139.690764278686
Iteration 18200: Loss = -11139.739948615035
1
Iteration 18300: Loss = -11139.696083166615
2
Iteration 18400: Loss = -11139.695739409945
3
Iteration 18500: Loss = -11139.692060839634
4
Iteration 18600: Loss = -11139.691901379812
5
Iteration 18700: Loss = -11139.691505065193
6
Iteration 18800: Loss = -11139.690885957978
7
Iteration 18900: Loss = -11139.69058563951
Iteration 19000: Loss = -11139.696160606354
1
Iteration 19100: Loss = -11139.690510813725
Iteration 19200: Loss = -11139.694674299608
1
Iteration 19300: Loss = -11139.6941840736
2
Iteration 19400: Loss = -11139.694095798064
3
Iteration 19500: Loss = -11139.69067422607
4
Iteration 19600: Loss = -11139.820944299232
5
Iteration 19700: Loss = -11139.693298333978
6
Iteration 19800: Loss = -11139.691521113626
7
Iteration 19900: Loss = -11139.690467051327
pi: tensor([[0.7706, 0.2294],
        [0.2934, 0.7066]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5403, 0.4597], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.0981],
         [0.5330, 0.3136]],

        [[0.6982, 0.1034],
         [0.7060, 0.5489]],

        [[0.6971, 0.1010],
         [0.5897, 0.5364]],

        [[0.6986, 0.1017],
         [0.5882, 0.6555]],

        [[0.6866, 0.0965],
         [0.7035, 0.5650]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
Global Adjusted Rand Index: 0.9291502021908267
Average Adjusted Rand Index: 0.9291174909370383
11171.571997567185
[0.9291502021908267, 0.9291502021908267] [0.9291174909370383, 0.9291174909370383] [11139.69089473779, 11139.697212727939]
-------------------------------------
This iteration is 67
True Objective function: Loss = -11380.606784388212
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21335.50724291884
Iteration 100: Loss = -11685.78306635046
Iteration 200: Loss = -11683.762077360456
Iteration 300: Loss = -11677.218667195788
Iteration 400: Loss = -11673.399998676015
Iteration 500: Loss = -11664.302689611388
Iteration 600: Loss = -11614.090267392377
Iteration 700: Loss = -11524.668804724111
Iteration 800: Loss = -11505.018638021176
Iteration 900: Loss = -11504.018994027856
Iteration 1000: Loss = -11503.570860228494
Iteration 1100: Loss = -11502.613998625162
Iteration 1200: Loss = -11502.254342879054
Iteration 1300: Loss = -11500.00294290123
Iteration 1400: Loss = -11497.310686193134
Iteration 1500: Loss = -11496.578688011525
Iteration 1600: Loss = -11496.505626498345
Iteration 1700: Loss = -11496.45003554811
Iteration 1800: Loss = -11496.391235697996
Iteration 1900: Loss = -11492.779206898787
Iteration 2000: Loss = -11492.41144010814
Iteration 2100: Loss = -11492.313105329882
Iteration 2200: Loss = -11492.200320093438
Iteration 2300: Loss = -11492.167905702292
Iteration 2400: Loss = -11492.14560537335
Iteration 2500: Loss = -11492.124935895441
Iteration 2600: Loss = -11491.476992474954
Iteration 2700: Loss = -11491.402113568449
Iteration 2800: Loss = -11491.365946520837
Iteration 2900: Loss = -11491.286895979472
Iteration 3000: Loss = -11491.213361369963
Iteration 3100: Loss = -11491.19744717855
Iteration 3200: Loss = -11491.185275414002
Iteration 3300: Loss = -11491.107737469481
Iteration 3400: Loss = -11491.081915516585
Iteration 3500: Loss = -11490.98544117297
Iteration 3600: Loss = -11490.920568618976
Iteration 3700: Loss = -11461.895927406058
Iteration 3800: Loss = -11438.272815540955
Iteration 3900: Loss = -11437.34154378515
Iteration 4000: Loss = -11437.296146096125
Iteration 4100: Loss = -11437.239184777482
Iteration 4200: Loss = -11427.89592190704
Iteration 4300: Loss = -11427.846065891308
Iteration 4400: Loss = -11425.898068237502
Iteration 4500: Loss = -11425.796001268522
Iteration 4600: Loss = -11425.791571905149
Iteration 4700: Loss = -11425.788552258857
Iteration 4800: Loss = -11425.786204919226
Iteration 4900: Loss = -11425.784067868604
Iteration 5000: Loss = -11425.781649494767
Iteration 5100: Loss = -11425.774505563866
Iteration 5200: Loss = -11423.048166757842
Iteration 5300: Loss = -11423.060909402582
1
Iteration 5400: Loss = -11423.040420384814
Iteration 5500: Loss = -11423.039037792572
Iteration 5600: Loss = -11423.037217125015
Iteration 5700: Loss = -11422.941569374547
Iteration 5800: Loss = -11422.871240425364
Iteration 5900: Loss = -11422.869533502479
Iteration 6000: Loss = -11422.884915232475
1
Iteration 6100: Loss = -11422.83194416424
Iteration 6200: Loss = -11422.82685015725
Iteration 6300: Loss = -11422.821080385549
Iteration 6400: Loss = -11422.820628954298
Iteration 6500: Loss = -11422.820543464384
Iteration 6600: Loss = -11422.819451729963
Iteration 6700: Loss = -11422.816710232084
Iteration 6800: Loss = -11422.807902764347
Iteration 6900: Loss = -11422.803679302142
Iteration 7000: Loss = -11422.803174314633
Iteration 7100: Loss = -11422.803404388642
1
Iteration 7200: Loss = -11422.8018144433
Iteration 7300: Loss = -11422.803066090175
1
Iteration 7400: Loss = -11422.81616552674
2
Iteration 7500: Loss = -11422.804908634518
3
Iteration 7600: Loss = -11422.800419441326
Iteration 7700: Loss = -11422.807027859692
1
Iteration 7800: Loss = -11422.80021519938
Iteration 7900: Loss = -11422.80524743291
1
Iteration 8000: Loss = -11422.79997113106
Iteration 8100: Loss = -11422.806399289992
1
Iteration 8200: Loss = -11422.799776207641
Iteration 8300: Loss = -11422.800271089058
1
Iteration 8400: Loss = -11422.799366895979
Iteration 8500: Loss = -11422.801860576448
1
Iteration 8600: Loss = -11422.79857450292
Iteration 8700: Loss = -11423.063228428706
1
Iteration 8800: Loss = -11422.798355525165
Iteration 8900: Loss = -11422.79816658188
Iteration 9000: Loss = -11422.80397243099
1
Iteration 9100: Loss = -11422.797344403394
Iteration 9200: Loss = -11422.797798460837
1
Iteration 9300: Loss = -11422.797216221432
Iteration 9400: Loss = -11422.80384312975
1
Iteration 9500: Loss = -11422.796955853039
Iteration 9600: Loss = -11422.799631975737
1
Iteration 9700: Loss = -11422.796565256487
Iteration 9800: Loss = -11422.811447951808
1
Iteration 9900: Loss = -11422.799144781779
2
Iteration 10000: Loss = -11422.796416047728
Iteration 10100: Loss = -11422.796489914512
Iteration 10200: Loss = -11422.801969274486
1
Iteration 10300: Loss = -11422.796973006994
2
Iteration 10400: Loss = -11422.796895988007
3
Iteration 10500: Loss = -11422.799189658552
4
Iteration 10600: Loss = -11422.797533478652
5
Iteration 10700: Loss = -11422.796305717915
Iteration 10800: Loss = -11422.800810318191
1
Iteration 10900: Loss = -11422.799152813777
2
Iteration 11000: Loss = -11422.796180679994
Iteration 11100: Loss = -11422.829568788196
1
Iteration 11200: Loss = -11422.796127046384
Iteration 11300: Loss = -11422.796104273457
Iteration 11400: Loss = -11422.797582439092
1
Iteration 11500: Loss = -11422.796028632823
Iteration 11600: Loss = -11422.796714830733
1
Iteration 11700: Loss = -11422.796113166674
Iteration 11800: Loss = -11422.796041642938
Iteration 11900: Loss = -11422.798512495165
1
Iteration 12000: Loss = -11422.889118210676
2
Iteration 12100: Loss = -11422.796056939227
Iteration 12200: Loss = -11422.797197685282
1
Iteration 12300: Loss = -11422.81565182074
2
Iteration 12400: Loss = -11422.795937696332
Iteration 12500: Loss = -11422.804342012556
1
Iteration 12600: Loss = -11422.795818795208
Iteration 12700: Loss = -11422.798260135682
1
Iteration 12800: Loss = -11422.800976203556
2
Iteration 12900: Loss = -11422.795828339553
Iteration 13000: Loss = -11422.823702621508
1
Iteration 13100: Loss = -11422.835535596667
2
Iteration 13200: Loss = -11422.804048082124
3
Iteration 13300: Loss = -11422.886170320156
4
Iteration 13400: Loss = -11422.80362617708
5
Iteration 13500: Loss = -11422.796277226851
6
Iteration 13600: Loss = -11422.80292285343
7
Iteration 13700: Loss = -11422.800255131973
8
Iteration 13800: Loss = -11422.795871496708
Iteration 13900: Loss = -11422.795876412294
Iteration 14000: Loss = -11422.835263014225
1
Iteration 14100: Loss = -11422.795749997716
Iteration 14200: Loss = -11422.799297629419
1
Iteration 14300: Loss = -11422.79583919194
Iteration 14400: Loss = -11422.79674625364
1
Iteration 14500: Loss = -11422.795774057748
Iteration 14600: Loss = -11422.797208083157
1
Iteration 14700: Loss = -11422.795752418731
Iteration 14800: Loss = -11422.79632233737
1
Iteration 14900: Loss = -11422.795808173722
Iteration 15000: Loss = -11422.796321218435
1
Iteration 15100: Loss = -11422.79590759389
Iteration 15200: Loss = -11422.795832451167
Iteration 15300: Loss = -11422.941349028493
1
Iteration 15400: Loss = -11422.796098274752
2
Iteration 15500: Loss = -11422.795860435077
Iteration 15600: Loss = -11422.800593606626
1
Iteration 15700: Loss = -11422.799269473377
2
Iteration 15800: Loss = -11422.795898813596
Iteration 15900: Loss = -11422.79745418096
1
Iteration 16000: Loss = -11422.830314181118
2
Iteration 16100: Loss = -11422.883580254285
3
Iteration 16200: Loss = -11422.79576353046
Iteration 16300: Loss = -11422.80005786526
1
Iteration 16400: Loss = -11422.79606130692
2
Iteration 16500: Loss = -11422.79788589295
3
Iteration 16600: Loss = -11422.79922015782
4
Iteration 16700: Loss = -11422.796515918168
5
Iteration 16800: Loss = -11422.963416358545
6
Iteration 16900: Loss = -11422.795705385679
Iteration 17000: Loss = -11422.802590088364
1
Iteration 17100: Loss = -11422.799483571334
2
Iteration 17200: Loss = -11422.795741884014
Iteration 17300: Loss = -11422.796037567608
1
Iteration 17400: Loss = -11422.94997001921
2
Iteration 17500: Loss = -11422.795772903512
Iteration 17600: Loss = -11422.795826711348
Iteration 17700: Loss = -11422.79655749079
1
Iteration 17800: Loss = -11422.796345562738
2
Iteration 17900: Loss = -11422.859780865178
3
Iteration 18000: Loss = -11422.79583670854
Iteration 18100: Loss = -11422.833815798345
1
Iteration 18200: Loss = -11422.801462262618
2
Iteration 18300: Loss = -11423.048915663343
3
Iteration 18400: Loss = -11422.795917826572
Iteration 18500: Loss = -11422.808309471717
1
Iteration 18600: Loss = -11422.795725319658
Iteration 18700: Loss = -11422.8973351661
1
Iteration 18800: Loss = -11422.797716643454
2
Iteration 18900: Loss = -11422.796078355763
3
Iteration 19000: Loss = -11422.795790688348
Iteration 19100: Loss = -11422.796088385761
1
Iteration 19200: Loss = -11422.801581371103
2
Iteration 19300: Loss = -11422.800474458561
3
Iteration 19400: Loss = -11422.824980348516
4
Iteration 19500: Loss = -11422.808499610623
5
Iteration 19600: Loss = -11422.844838234967
6
Iteration 19700: Loss = -11422.795655191998
Iteration 19800: Loss = -11422.7807373492
Iteration 19900: Loss = -11422.783125628128
1
pi: tensor([[0.2975, 0.7025],
        [0.5236, 0.4764]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5725, 0.4275], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2519, 0.0992],
         [0.5131, 0.2635]],

        [[0.7089, 0.0935],
         [0.7078, 0.5398]],

        [[0.5809, 0.0911],
         [0.5427, 0.6290]],

        [[0.7302, 0.0973],
         [0.6937, 0.6803]],

        [[0.7145, 0.1087],
         [0.5684, 0.6917]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 91
Adjusted Rand Index: 0.6691544249601056
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.844331361923687
Global Adjusted Rand Index: 0.023584929507224142
Average Adjusted Rand Index: 0.8478220654497426
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22151.578288268207
Iteration 100: Loss = -11684.218130781812
Iteration 200: Loss = -11677.348535624435
Iteration 300: Loss = -11659.340517649034
Iteration 400: Loss = -11552.19524548192
Iteration 500: Loss = -11436.007778687543
Iteration 600: Loss = -11435.056248099701
Iteration 700: Loss = -11434.92121029116
Iteration 800: Loss = -11434.86254837986
Iteration 900: Loss = -11434.825165166365
Iteration 1000: Loss = -11434.800227994703
Iteration 1100: Loss = -11434.782833078245
Iteration 1200: Loss = -11434.770130364233
Iteration 1300: Loss = -11434.760632825355
Iteration 1400: Loss = -11434.753051314874
Iteration 1500: Loss = -11434.74547365624
Iteration 1600: Loss = -11434.641604029608
Iteration 1700: Loss = -11434.63589952002
Iteration 1800: Loss = -11434.631613839405
Iteration 1900: Loss = -11434.625845708053
Iteration 2000: Loss = -11434.522834524658
Iteration 2100: Loss = -11434.519182059581
Iteration 2200: Loss = -11434.524115354805
1
Iteration 2300: Loss = -11434.512089884814
Iteration 2400: Loss = -11434.506884860677
Iteration 2500: Loss = -11434.497460636481
Iteration 2600: Loss = -11434.468419671635
Iteration 2700: Loss = -11433.916743885893
Iteration 2800: Loss = -11433.606649437286
Iteration 2900: Loss = -11433.594581044317
Iteration 3000: Loss = -11433.545650577244
Iteration 3100: Loss = -11433.537110453855
Iteration 3200: Loss = -11433.53287180054
Iteration 3300: Loss = -11433.530186661308
Iteration 3400: Loss = -11433.530246956461
Iteration 3500: Loss = -11433.525786738928
Iteration 3600: Loss = -11433.524046000128
Iteration 3700: Loss = -11433.524181786912
1
Iteration 3800: Loss = -11433.516725512996
Iteration 3900: Loss = -11433.499202258496
Iteration 4000: Loss = -11433.502447867782
1
Iteration 4100: Loss = -11433.497634130323
Iteration 4200: Loss = -11433.498136930666
1
Iteration 4300: Loss = -11433.47318848001
Iteration 4400: Loss = -11433.473488957101
1
Iteration 4500: Loss = -11433.470399093752
Iteration 4600: Loss = -11433.486564022587
1
Iteration 4700: Loss = -11433.468749361647
Iteration 4800: Loss = -11433.467714699913
Iteration 4900: Loss = -11433.464551871473
Iteration 5000: Loss = -11433.460861033911
Iteration 5100: Loss = -11433.463118704052
1
Iteration 5200: Loss = -11433.460114896921
Iteration 5300: Loss = -11433.461202625062
1
Iteration 5400: Loss = -11433.459955761306
Iteration 5500: Loss = -11433.459668300478
Iteration 5600: Loss = -11433.45982225722
1
Iteration 5700: Loss = -11433.461700379106
2
Iteration 5800: Loss = -11433.456854092281
Iteration 5900: Loss = -11433.456555407085
Iteration 6000: Loss = -11433.456336547715
Iteration 6100: Loss = -11433.45644266054
1
Iteration 6200: Loss = -11433.47504450092
2
Iteration 6300: Loss = -11433.456384708463
Iteration 6400: Loss = -11433.456660756894
1
Iteration 6500: Loss = -11433.463203482264
2
Iteration 6600: Loss = -11433.45608862844
Iteration 6700: Loss = -11433.456081454213
Iteration 6800: Loss = -11433.456024144924
Iteration 6900: Loss = -11433.494738475056
1
Iteration 7000: Loss = -11433.455954137587
Iteration 7100: Loss = -11433.455949475654
Iteration 7200: Loss = -11433.46092902298
1
Iteration 7300: Loss = -11433.455868767936
Iteration 7400: Loss = -11433.4563340776
1
Iteration 7500: Loss = -11433.455870217214
Iteration 7600: Loss = -11433.455922368405
Iteration 7700: Loss = -11432.988745637673
Iteration 7800: Loss = -11432.98328311085
Iteration 7900: Loss = -11432.981547913072
Iteration 8000: Loss = -11432.9846233556
1
Iteration 8100: Loss = -11433.00772400001
2
Iteration 8200: Loss = -11432.96845482629
Iteration 8300: Loss = -11432.968329457704
Iteration 8400: Loss = -11432.969170879522
1
Iteration 8500: Loss = -11432.974390259227
2
Iteration 8600: Loss = -11432.968103801084
Iteration 8700: Loss = -11432.967110171177
Iteration 8800: Loss = -11432.48505314644
Iteration 8900: Loss = -11432.487164966933
1
Iteration 9000: Loss = -11432.456299882657
Iteration 9100: Loss = -11431.70192470087
Iteration 9200: Loss = -11431.902037109907
1
Iteration 9300: Loss = -11431.699314589294
Iteration 9400: Loss = -11431.768031802958
1
Iteration 9500: Loss = -11431.699301961782
Iteration 9600: Loss = -11431.699284113889
Iteration 9700: Loss = -11431.699344218498
Iteration 9800: Loss = -11431.69924607512
Iteration 9900: Loss = -11431.70245172349
1
Iteration 10000: Loss = -11431.699250306649
Iteration 10100: Loss = -11431.699241576067
Iteration 10200: Loss = -11431.699304222535
Iteration 10300: Loss = -11431.699486372085
1
Iteration 10400: Loss = -11431.699294856067
Iteration 10500: Loss = -11431.699245426988
Iteration 10600: Loss = -11431.699296414332
Iteration 10700: Loss = -11431.699214773776
Iteration 10800: Loss = -11431.711230249977
1
Iteration 10900: Loss = -11431.699225271894
Iteration 11000: Loss = -11431.726758175784
1
Iteration 11100: Loss = -11431.699197132431
Iteration 11200: Loss = -11431.699209935421
Iteration 11300: Loss = -11431.699310583826
1
Iteration 11400: Loss = -11431.699219866137
Iteration 11500: Loss = -11431.70259282066
1
Iteration 11600: Loss = -11431.699234501752
Iteration 11700: Loss = -11431.772911778382
1
Iteration 11800: Loss = -11431.693379543818
Iteration 11900: Loss = -11427.807915255797
Iteration 12000: Loss = -11427.769647034702
Iteration 12100: Loss = -11427.769483731128
Iteration 12200: Loss = -11427.778387933064
1
Iteration 12300: Loss = -11427.760803566474
Iteration 12400: Loss = -11427.768997873525
1
Iteration 12500: Loss = -11427.76080485668
Iteration 12600: Loss = -11427.765964075683
1
Iteration 12700: Loss = -11427.759126914321
Iteration 12800: Loss = -11427.80342653452
1
Iteration 12900: Loss = -11427.759107292837
Iteration 13000: Loss = -11427.974999365862
1
Iteration 13100: Loss = -11427.759102246808
Iteration 13200: Loss = -11427.759066811806
Iteration 13300: Loss = -11427.758876546763
Iteration 13400: Loss = -11427.758525821984
Iteration 13500: Loss = -11427.776917017021
1
Iteration 13600: Loss = -11427.758527878099
Iteration 13700: Loss = -11428.002593568523
1
Iteration 13800: Loss = -11427.755241118623
Iteration 13900: Loss = -11427.758390157596
1
Iteration 14000: Loss = -11427.755049317584
Iteration 14100: Loss = -11427.753744055311
Iteration 14200: Loss = -11427.750360502854
Iteration 14300: Loss = -11427.748927287312
Iteration 14400: Loss = -11427.74838430189
Iteration 14500: Loss = -11427.750089244277
1
Iteration 14600: Loss = -11427.74838156011
Iteration 14700: Loss = -11427.748627594177
1
Iteration 14800: Loss = -11427.822656579172
2
Iteration 14900: Loss = -11427.748394383612
Iteration 15000: Loss = -11427.749521822869
1
Iteration 15100: Loss = -11427.748352695207
Iteration 15200: Loss = -11427.748351150793
Iteration 15300: Loss = -11427.747673918124
Iteration 15400: Loss = -11427.75050105494
1
Iteration 15500: Loss = -11427.747679106027
Iteration 15600: Loss = -11427.800675746004
1
Iteration 15700: Loss = -11427.747675937288
Iteration 15800: Loss = -11427.807689270576
1
Iteration 15900: Loss = -11427.747660390623
Iteration 16000: Loss = -11427.758697752437
1
Iteration 16100: Loss = -11427.747664116587
Iteration 16200: Loss = -11427.747707009212
Iteration 16300: Loss = -11427.7529416061
1
Iteration 16400: Loss = -11427.747384959812
Iteration 16500: Loss = -11427.835403489187
1
Iteration 16600: Loss = -11426.47749057466
Iteration 16700: Loss = -11426.442369377774
Iteration 16800: Loss = -11426.439420117027
Iteration 16900: Loss = -11426.439706517616
1
Iteration 17000: Loss = -11426.659038135896
2
Iteration 17100: Loss = -11426.439150001717
Iteration 17200: Loss = -11426.444218753419
1
Iteration 17300: Loss = -11426.424720652858
Iteration 17400: Loss = -11426.424241976547
Iteration 17500: Loss = -11426.427242964124
1
Iteration 17600: Loss = -11426.444743203805
2
Iteration 17700: Loss = -11426.426307518503
3
Iteration 17800: Loss = -11426.424211195586
Iteration 17900: Loss = -11426.430308892923
1
Iteration 18000: Loss = -11426.422673486586
Iteration 18100: Loss = -11426.4239378458
1
Iteration 18200: Loss = -11426.407301672118
Iteration 18300: Loss = -11426.40742598102
1
Iteration 18400: Loss = -11426.407703288
2
Iteration 18500: Loss = -11426.406962247353
Iteration 18600: Loss = -11426.588527533155
1
Iteration 18700: Loss = -11426.404166311391
Iteration 18800: Loss = -11426.435866896392
1
Iteration 18900: Loss = -11426.404135297904
Iteration 19000: Loss = -11426.419848965714
1
Iteration 19100: Loss = -11426.404073373731
Iteration 19200: Loss = -11426.427980388427
1
Iteration 19300: Loss = -11426.404079636055
Iteration 19400: Loss = -11426.404334912351
1
Iteration 19500: Loss = -11426.612986219627
2
Iteration 19600: Loss = -11426.404089304133
Iteration 19700: Loss = -11426.700364214077
1
Iteration 19800: Loss = -11426.404086419678
Iteration 19900: Loss = -11426.482347091047
1
pi: tensor([[0.7492, 0.2508],
        [0.4088, 0.5912]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0276, 0.9724], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2998, 0.1337],
         [0.6805, 0.1910]],

        [[0.5025, 0.1008],
         [0.6696, 0.5555]],

        [[0.5370, 0.0918],
         [0.6799, 0.6144]],

        [[0.6969, 0.0990],
         [0.6170, 0.6571]],

        [[0.5414, 0.1111],
         [0.6482, 0.6287]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599252625159036
Global Adjusted Rand Index: 0.5767093953190865
Average Adjusted Rand Index: 0.7446252394865208
11380.606784388212
[0.023584929507224142, 0.5767093953190865] [0.8478220654497426, 0.7446252394865208] [11422.77972963699, 11426.404070517983]
-------------------------------------
This iteration is 68
True Objective function: Loss = -11302.30055423985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20608.46508951418
Iteration 100: Loss = -11616.007401959632
Iteration 200: Loss = -11611.626600209262
Iteration 300: Loss = -11593.264604448263
Iteration 400: Loss = -11466.258943588786
Iteration 500: Loss = -11378.142193412597
Iteration 600: Loss = -11366.915392468749
Iteration 700: Loss = -11338.744140716646
Iteration 800: Loss = -11294.272971516817
Iteration 900: Loss = -11286.80000096247
Iteration 1000: Loss = -11286.336334648962
Iteration 1100: Loss = -11286.075750557804
Iteration 1200: Loss = -11286.002200602796
Iteration 1300: Loss = -11285.952796241207
Iteration 1400: Loss = -11285.695911944751
Iteration 1500: Loss = -11282.705010068694
Iteration 1600: Loss = -11282.664819827874
Iteration 1700: Loss = -11282.639615391096
Iteration 1800: Loss = -11282.626310301952
Iteration 1900: Loss = -11282.614088307724
Iteration 2000: Loss = -11282.591298869347
Iteration 2100: Loss = -11282.568270731821
Iteration 2200: Loss = -11282.579758713788
1
Iteration 2300: Loss = -11282.5592429642
Iteration 2400: Loss = -11282.555647950912
Iteration 2500: Loss = -11282.552549402126
Iteration 2600: Loss = -11282.552391915604
Iteration 2700: Loss = -11282.54750923025
Iteration 2800: Loss = -11282.54546567496
Iteration 2900: Loss = -11282.543632509007
Iteration 3000: Loss = -11282.542075529129
Iteration 3100: Loss = -11282.54060813549
Iteration 3200: Loss = -11282.539313208732
Iteration 3300: Loss = -11282.538158116944
Iteration 3400: Loss = -11282.537120530364
Iteration 3500: Loss = -11282.536189326464
Iteration 3600: Loss = -11282.536169507228
Iteration 3700: Loss = -11282.534559854952
Iteration 3800: Loss = -11282.533976061884
Iteration 3900: Loss = -11282.536372026463
1
Iteration 4000: Loss = -11282.532574331712
Iteration 4100: Loss = -11282.53245585483
Iteration 4200: Loss = -11282.53211147032
Iteration 4300: Loss = -11282.53083868798
Iteration 4400: Loss = -11282.530255797392
Iteration 4500: Loss = -11282.52959348447
Iteration 4600: Loss = -11282.529313797215
Iteration 4700: Loss = -11282.528426600138
Iteration 4800: Loss = -11282.5280574993
Iteration 4900: Loss = -11282.527958125855
Iteration 5000: Loss = -11282.532062008966
1
Iteration 5100: Loss = -11282.530901521572
2
Iteration 5200: Loss = -11282.526381733589
Iteration 5300: Loss = -11282.525720126807
Iteration 5400: Loss = -11282.52634837297
1
Iteration 5500: Loss = -11282.525967448268
2
Iteration 5600: Loss = -11282.524944492006
Iteration 5700: Loss = -11282.524830450111
Iteration 5800: Loss = -11282.524698470226
Iteration 5900: Loss = -11282.5253375753
1
Iteration 6000: Loss = -11282.524339622087
Iteration 6100: Loss = -11282.526344520036
1
Iteration 6200: Loss = -11282.52626696964
2
Iteration 6300: Loss = -11282.523882969392
Iteration 6400: Loss = -11282.523810816545
Iteration 6500: Loss = -11282.524325955961
1
Iteration 6600: Loss = -11282.523570251553
Iteration 6700: Loss = -11282.52348859922
Iteration 6800: Loss = -11282.523423310346
Iteration 6900: Loss = -11282.523285431329
Iteration 7000: Loss = -11282.52326774218
Iteration 7100: Loss = -11282.523095957302
Iteration 7200: Loss = -11282.687852233188
1
Iteration 7300: Loss = -11282.522698629873
Iteration 7400: Loss = -11282.522430014793
Iteration 7500: Loss = -11282.522599008154
1
Iteration 7600: Loss = -11282.54386496144
2
Iteration 7700: Loss = -11282.521928895221
Iteration 7800: Loss = -11282.522114633015
1
Iteration 7900: Loss = -11282.558844078498
2
Iteration 8000: Loss = -11282.521768282837
Iteration 8100: Loss = -11282.52171971125
Iteration 8200: Loss = -11282.56470107752
1
Iteration 8300: Loss = -11282.521573482642
Iteration 8400: Loss = -11282.540605737828
1
Iteration 8500: Loss = -11282.521586312016
Iteration 8600: Loss = -11282.543460576218
1
Iteration 8700: Loss = -11282.52151107682
Iteration 8800: Loss = -11282.521459899302
Iteration 8900: Loss = -11282.525160376728
1
Iteration 9000: Loss = -11282.52144734348
Iteration 9100: Loss = -11282.521614412739
1
Iteration 9200: Loss = -11282.521454832027
Iteration 9300: Loss = -11282.529123940762
1
Iteration 9400: Loss = -11282.521419462497
Iteration 9500: Loss = -11282.52456207296
1
Iteration 9600: Loss = -11282.5218026175
2
Iteration 9700: Loss = -11282.521359805503
Iteration 9800: Loss = -11282.521954330925
1
Iteration 9900: Loss = -11282.55385102742
2
Iteration 10000: Loss = -11282.521316802697
Iteration 10100: Loss = -11282.521423804932
1
Iteration 10200: Loss = -11282.530221795208
2
Iteration 10300: Loss = -11282.521252628598
Iteration 10400: Loss = -11282.548366584173
1
Iteration 10500: Loss = -11282.521204949475
Iteration 10600: Loss = -11282.522758392715
1
Iteration 10700: Loss = -11282.521162445062
Iteration 10800: Loss = -11282.521153032096
Iteration 10900: Loss = -11282.521526225544
1
Iteration 11000: Loss = -11282.521094133888
Iteration 11100: Loss = -11282.521488376691
1
Iteration 11200: Loss = -11282.52108130429
Iteration 11300: Loss = -11282.531854523704
1
Iteration 11400: Loss = -11282.521072020285
Iteration 11500: Loss = -11282.52103877063
Iteration 11600: Loss = -11282.521398856761
1
Iteration 11700: Loss = -11282.52099193816
Iteration 11800: Loss = -11282.558621005845
1
Iteration 11900: Loss = -11282.51040835448
Iteration 12000: Loss = -11282.51063678543
1
Iteration 12100: Loss = -11282.521481090778
2
Iteration 12200: Loss = -11282.525686372663
3
Iteration 12300: Loss = -11282.517248274946
4
Iteration 12400: Loss = -11282.510448185594
Iteration 12500: Loss = -11282.51034115553
Iteration 12600: Loss = -11282.51053365477
1
Iteration 12700: Loss = -11282.522249807385
2
Iteration 12800: Loss = -11282.510225910324
Iteration 12900: Loss = -11282.51034589021
1
Iteration 13000: Loss = -11282.510772933625
2
Iteration 13100: Loss = -11282.510178468236
Iteration 13200: Loss = -11282.51086971248
1
Iteration 13300: Loss = -11282.510158615361
Iteration 13400: Loss = -11282.510318180817
1
Iteration 13500: Loss = -11282.510173375025
Iteration 13600: Loss = -11282.510172911656
Iteration 13700: Loss = -11282.511289927501
1
Iteration 13800: Loss = -11282.51014955557
Iteration 13900: Loss = -11282.864998201925
1
Iteration 14000: Loss = -11282.510102794282
Iteration 14100: Loss = -11282.51001387697
Iteration 14200: Loss = -11282.51076988739
1
Iteration 14300: Loss = -11282.510002037618
Iteration 14400: Loss = -11282.59355828634
1
Iteration 14500: Loss = -11282.509980790144
Iteration 14600: Loss = -11282.510001649429
Iteration 14700: Loss = -11282.510034317285
Iteration 14800: Loss = -11282.573649207661
1
Iteration 14900: Loss = -11282.509977818283
Iteration 15000: Loss = -11282.518138494965
1
Iteration 15100: Loss = -11282.519559884333
2
Iteration 15200: Loss = -11282.55039580391
3
Iteration 15300: Loss = -11282.510002242727
Iteration 15400: Loss = -11282.510227028402
1
Iteration 15500: Loss = -11282.51027891194
2
Iteration 15600: Loss = -11282.5105943816
3
Iteration 15700: Loss = -11282.52548382098
4
Iteration 15800: Loss = -11282.509977804915
Iteration 15900: Loss = -11282.521931522293
1
Iteration 16000: Loss = -11282.50997750671
Iteration 16100: Loss = -11282.510040257053
Iteration 16200: Loss = -11282.510076432334
Iteration 16300: Loss = -11282.50999908077
Iteration 16400: Loss = -11282.510969806606
1
Iteration 16500: Loss = -11282.509982702168
Iteration 16600: Loss = -11282.514075165265
1
Iteration 16700: Loss = -11282.510165324891
2
Iteration 16800: Loss = -11282.51041193703
3
Iteration 16900: Loss = -11282.51041192155
4
Iteration 17000: Loss = -11282.504458898477
Iteration 17100: Loss = -11282.504676809023
1
Iteration 17200: Loss = -11282.50447859037
Iteration 17300: Loss = -11282.505106498866
1
Iteration 17400: Loss = -11282.50443514924
Iteration 17500: Loss = -11282.569697473356
1
Iteration 17600: Loss = -11282.504466476505
Iteration 17700: Loss = -11282.504511093715
Iteration 17800: Loss = -11282.504437984948
Iteration 17900: Loss = -11282.505422374621
1
Iteration 18000: Loss = -11282.504551469401
2
Iteration 18100: Loss = -11282.505541350787
3
Iteration 18200: Loss = -11282.505093644124
4
Iteration 18300: Loss = -11282.505139780029
5
Iteration 18400: Loss = -11282.50446278859
Iteration 18500: Loss = -11282.537659228752
1
Iteration 18600: Loss = -11282.50444121573
Iteration 18700: Loss = -11282.511153069814
1
Iteration 18800: Loss = -11282.504438861706
Iteration 18900: Loss = -11282.516480365526
1
Iteration 19000: Loss = -11282.504468214476
Iteration 19100: Loss = -11282.507092576727
1
Iteration 19200: Loss = -11282.5044511564
Iteration 19300: Loss = -11282.50457342781
1
Iteration 19400: Loss = -11282.50527805938
2
Iteration 19500: Loss = -11282.530255747659
3
Iteration 19600: Loss = -11282.504462135848
Iteration 19700: Loss = -11282.504582040392
1
Iteration 19800: Loss = -11282.571326398142
2
Iteration 19900: Loss = -11282.504474646648
pi: tensor([[0.7617, 0.2383],
        [0.2497, 0.7503]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5897, 0.4103], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3014, 0.0995],
         [0.5413, 0.1930]],

        [[0.6071, 0.1056],
         [0.7067, 0.5017]],

        [[0.6675, 0.1022],
         [0.6613, 0.7039]],

        [[0.6620, 0.0962],
         [0.5887, 0.5971]],

        [[0.7265, 0.1057],
         [0.5261, 0.7060]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446722034026118
Average Adjusted Rand Index: 0.945124984031677
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22603.94922596221
Iteration 100: Loss = -11617.062437788107
Iteration 200: Loss = -11613.4410122294
Iteration 300: Loss = -11598.075440583209
Iteration 400: Loss = -11532.880514534794
Iteration 500: Loss = -11403.233365328482
Iteration 600: Loss = -11307.824956637327
Iteration 700: Loss = -11292.651552795516
Iteration 800: Loss = -11287.974997817004
Iteration 900: Loss = -11287.64014044876
Iteration 1000: Loss = -11286.673470338676
Iteration 1100: Loss = -11286.576760086808
Iteration 1200: Loss = -11286.019976580395
Iteration 1300: Loss = -11283.34597390878
Iteration 1400: Loss = -11283.152073641859
Iteration 1500: Loss = -11283.092275180647
Iteration 1600: Loss = -11283.062782834577
Iteration 1700: Loss = -11283.029231524622
Iteration 1800: Loss = -11283.00263392876
Iteration 1900: Loss = -11282.975588030666
Iteration 2000: Loss = -11282.914991755266
Iteration 2100: Loss = -11282.838860975962
Iteration 2200: Loss = -11282.649831834704
Iteration 2300: Loss = -11282.642375339872
Iteration 2400: Loss = -11282.63655595857
Iteration 2500: Loss = -11282.63168150415
Iteration 2600: Loss = -11282.627356970901
Iteration 2700: Loss = -11282.623685554321
Iteration 2800: Loss = -11282.62056398599
Iteration 2900: Loss = -11282.61744046185
Iteration 3000: Loss = -11282.614788223007
Iteration 3100: Loss = -11282.621475972108
1
Iteration 3200: Loss = -11282.610230127983
Iteration 3300: Loss = -11282.608229699248
Iteration 3400: Loss = -11282.606328314601
Iteration 3500: Loss = -11282.604514650244
Iteration 3600: Loss = -11282.602621785985
Iteration 3700: Loss = -11282.600451213937
Iteration 3800: Loss = -11282.595020457211
Iteration 3900: Loss = -11282.585931983276
Iteration 4000: Loss = -11282.584521032846
Iteration 4100: Loss = -11282.583133848522
Iteration 4200: Loss = -11282.58219872505
Iteration 4300: Loss = -11282.583928219055
1
Iteration 4400: Loss = -11282.580280710263
Iteration 4500: Loss = -11282.579461704327
Iteration 4600: Loss = -11282.578778579058
Iteration 4700: Loss = -11282.577873446333
Iteration 4800: Loss = -11282.584235592916
1
Iteration 4900: Loss = -11282.577093253485
Iteration 5000: Loss = -11282.576785524678
Iteration 5100: Loss = -11282.57568286654
Iteration 5200: Loss = -11282.57811044031
1
Iteration 5300: Loss = -11282.582654258083
2
Iteration 5400: Loss = -11282.574283073744
Iteration 5500: Loss = -11282.574033167035
Iteration 5600: Loss = -11282.57576991847
1
Iteration 5700: Loss = -11282.57986196422
2
Iteration 5800: Loss = -11282.57287143507
Iteration 5900: Loss = -11282.572719034637
Iteration 6000: Loss = -11282.572345423123
Iteration 6100: Loss = -11282.572092726417
Iteration 6200: Loss = -11282.57300854788
1
Iteration 6300: Loss = -11282.57300584887
2
Iteration 6400: Loss = -11282.571678571607
Iteration 6500: Loss = -11282.571565611834
Iteration 6600: Loss = -11282.574779078997
1
Iteration 6700: Loss = -11282.570960168257
Iteration 6800: Loss = -11282.570736229309
Iteration 6900: Loss = -11282.571809636547
1
Iteration 7000: Loss = -11282.57050117308
Iteration 7100: Loss = -11282.583786397001
1
Iteration 7200: Loss = -11282.57020485009
Iteration 7300: Loss = -11282.570148965833
Iteration 7400: Loss = -11282.715042164127
1
Iteration 7500: Loss = -11282.569947503485
Iteration 7600: Loss = -11282.569827540023
Iteration 7700: Loss = -11282.570291237125
1
Iteration 7800: Loss = -11282.569668124534
Iteration 7900: Loss = -11282.570192803372
1
Iteration 8000: Loss = -11282.570705259368
2
Iteration 8100: Loss = -11282.569472872408
Iteration 8200: Loss = -11282.574772007045
1
Iteration 8300: Loss = -11282.574017577623
2
Iteration 8400: Loss = -11282.611945952067
3
Iteration 8500: Loss = -11282.577074528404
4
Iteration 8600: Loss = -11282.57443412009
5
Iteration 8700: Loss = -11282.656133025153
6
Iteration 8800: Loss = -11282.570569477986
7
Iteration 8900: Loss = -11282.569493533772
Iteration 9000: Loss = -11282.568997339
Iteration 9100: Loss = -11282.569440633755
1
Iteration 9200: Loss = -11282.569249532766
2
Iteration 9300: Loss = -11282.568913075302
Iteration 9400: Loss = -11282.57058654665
1
Iteration 9500: Loss = -11282.637741275008
2
Iteration 9600: Loss = -11282.661777390666
3
Iteration 9700: Loss = -11282.568876668192
Iteration 9800: Loss = -11282.568508292226
Iteration 9900: Loss = -11282.568506453432
Iteration 10000: Loss = -11282.708051026928
1
Iteration 10100: Loss = -11282.570936967937
2
Iteration 10200: Loss = -11282.571618093585
3
Iteration 10300: Loss = -11282.558844598656
Iteration 10400: Loss = -11282.542798126555
Iteration 10500: Loss = -11282.536169342873
Iteration 10600: Loss = -11282.535986157425
Iteration 10700: Loss = -11282.53616375136
1
Iteration 10800: Loss = -11282.538415838582
2
Iteration 10900: Loss = -11282.535735092168
Iteration 11000: Loss = -11282.542886612064
1
Iteration 11100: Loss = -11282.535475326427
Iteration 11200: Loss = -11282.591073258103
1
Iteration 11300: Loss = -11282.535417497318
Iteration 11400: Loss = -11282.535366896505
Iteration 11500: Loss = -11282.552079988964
1
Iteration 11600: Loss = -11282.5353269556
Iteration 11700: Loss = -11282.535287951552
Iteration 11800: Loss = -11282.545925153503
1
Iteration 11900: Loss = -11282.535227628352
Iteration 12000: Loss = -11282.535176739077
Iteration 12100: Loss = -11282.525493260324
Iteration 12200: Loss = -11282.524597312502
Iteration 12300: Loss = -11282.529619847699
1
Iteration 12400: Loss = -11282.552746165016
2
Iteration 12500: Loss = -11282.524865826497
3
Iteration 12600: Loss = -11282.524610413957
Iteration 12700: Loss = -11282.536613051258
1
Iteration 12800: Loss = -11282.524495003841
Iteration 12900: Loss = -11282.524919799193
1
Iteration 13000: Loss = -11282.770998738675
2
Iteration 13100: Loss = -11282.524430631835
Iteration 13200: Loss = -11282.530306361876
1
Iteration 13300: Loss = -11282.524041675282
Iteration 13400: Loss = -11282.848508820072
1
Iteration 13500: Loss = -11282.524007985374
Iteration 13600: Loss = -11282.523988972007
Iteration 13700: Loss = -11282.524079575149
Iteration 13800: Loss = -11282.524026231731
Iteration 13900: Loss = -11282.528357479256
1
Iteration 14000: Loss = -11282.523969855501
Iteration 14100: Loss = -11282.526624867183
1
Iteration 14200: Loss = -11282.524003626828
Iteration 14300: Loss = -11282.524205158239
1
Iteration 14400: Loss = -11282.523969201151
Iteration 14500: Loss = -11282.52440289314
1
Iteration 14600: Loss = -11282.522582545336
Iteration 14700: Loss = -11282.53037549027
1
Iteration 14800: Loss = -11282.522229791004
Iteration 14900: Loss = -11282.524482534842
1
Iteration 15000: Loss = -11282.52222110004
Iteration 15100: Loss = -11282.52218551595
Iteration 15200: Loss = -11282.524654406947
1
Iteration 15300: Loss = -11282.522177106892
Iteration 15400: Loss = -11282.53301935263
1
Iteration 15500: Loss = -11282.523419918278
2
Iteration 15600: Loss = -11282.523825971137
3
Iteration 15700: Loss = -11282.526706424822
4
Iteration 15800: Loss = -11282.523252265466
5
Iteration 15900: Loss = -11282.52217974181
Iteration 16000: Loss = -11282.517791158332
Iteration 16100: Loss = -11282.505069630366
Iteration 16200: Loss = -11282.505072739383
Iteration 16300: Loss = -11282.505431014612
1
Iteration 16400: Loss = -11282.50503827924
Iteration 16500: Loss = -11282.505580452713
1
Iteration 16600: Loss = -11282.780298760754
2
Iteration 16700: Loss = -11282.505030986174
Iteration 16800: Loss = -11282.51125506009
1
Iteration 16900: Loss = -11282.546630944442
2
Iteration 17000: Loss = -11282.509789672464
3
Iteration 17100: Loss = -11282.50990096741
4
Iteration 17200: Loss = -11282.505308461143
5
Iteration 17300: Loss = -11282.50514531368
6
Iteration 17400: Loss = -11282.507701097344
7
Iteration 17500: Loss = -11282.50505502867
Iteration 17600: Loss = -11282.505092042698
Iteration 17700: Loss = -11282.505029170532
Iteration 17800: Loss = -11282.506945432451
1
Iteration 17900: Loss = -11282.504943395508
Iteration 18000: Loss = -11282.504843694547
Iteration 18100: Loss = -11282.507841733888
1
Iteration 18200: Loss = -11282.504838556157
Iteration 18300: Loss = -11282.50488177498
Iteration 18400: Loss = -11282.505277126269
1
Iteration 18500: Loss = -11282.504898829793
Iteration 18600: Loss = -11282.505031873192
1
Iteration 18700: Loss = -11282.52344386991
2
Iteration 18800: Loss = -11282.504860531099
Iteration 18900: Loss = -11282.505031107503
1
Iteration 19000: Loss = -11282.50624359816
2
Iteration 19100: Loss = -11282.504945471768
Iteration 19200: Loss = -11282.50550772464
1
Iteration 19300: Loss = -11282.506351024836
2
Iteration 19400: Loss = -11282.505973589627
3
Iteration 19500: Loss = -11282.504909889602
Iteration 19600: Loss = -11282.5059880499
1
Iteration 19700: Loss = -11282.520449367712
2
Iteration 19800: Loss = -11282.519267817252
3
Iteration 19900: Loss = -11282.535916791336
4
pi: tensor([[0.7581, 0.2419],
        [0.2483, 0.7517]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5875, 0.4125], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3020, 0.1000],
         [0.5436, 0.1924]],

        [[0.7117, 0.1051],
         [0.5040, 0.5217]],

        [[0.5095, 0.1025],
         [0.6611, 0.6025]],

        [[0.5840, 0.0961],
         [0.5733, 0.6909]],

        [[0.6829, 0.1057],
         [0.5303, 0.6848]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446722034026118
Average Adjusted Rand Index: 0.945124984031677
11302.30055423985
[0.9446722034026118, 0.9446722034026118] [0.945124984031677, 0.945124984031677] [11282.51757785714, 11282.507383170847]
-------------------------------------
This iteration is 69
True Objective function: Loss = -11126.938023790199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23712.46372291929
Iteration 100: Loss = -11379.138412312996
Iteration 200: Loss = -11378.336500237736
Iteration 300: Loss = -11377.803496770712
Iteration 400: Loss = -11376.752103627896
Iteration 500: Loss = -11375.7054513259
Iteration 600: Loss = -11375.363544593178
Iteration 700: Loss = -11375.138361923982
Iteration 800: Loss = -11374.972024899815
Iteration 900: Loss = -11374.819652878217
Iteration 1000: Loss = -11374.670587435296
Iteration 1100: Loss = -11374.526577662826
Iteration 1200: Loss = -11374.392277656174
Iteration 1300: Loss = -11374.265854538276
Iteration 1400: Loss = -11374.147227454045
Iteration 1500: Loss = -11374.0155723741
Iteration 1600: Loss = -11373.634468062346
Iteration 1700: Loss = -11372.252239610505
Iteration 1800: Loss = -11370.114198346975
Iteration 1900: Loss = -11366.00442905766
Iteration 2000: Loss = -11363.412308836903
Iteration 2100: Loss = -11290.16989301949
Iteration 2200: Loss = -11283.048378183259
Iteration 2300: Loss = -11280.401298568218
Iteration 2400: Loss = -11280.32740683662
Iteration 2500: Loss = -11276.550512553926
Iteration 2600: Loss = -11276.33160432335
Iteration 2700: Loss = -11276.30604392402
Iteration 2800: Loss = -11276.304123301174
Iteration 2900: Loss = -11275.790810574625
Iteration 3000: Loss = -11275.313601736667
Iteration 3100: Loss = -11270.854626788167
Iteration 3200: Loss = -11270.609870760956
Iteration 3300: Loss = -11270.541740310153
Iteration 3400: Loss = -11270.538563409813
Iteration 3500: Loss = -11270.536692688562
Iteration 3600: Loss = -11270.523062156628
Iteration 3700: Loss = -11270.50997653928
Iteration 3800: Loss = -11270.508617607671
Iteration 3900: Loss = -11270.504161053046
Iteration 4000: Loss = -11270.50095267217
Iteration 4100: Loss = -11270.500279725136
Iteration 4200: Loss = -11270.512726480236
1
Iteration 4300: Loss = -11270.49908003565
Iteration 4400: Loss = -11270.498229904584
Iteration 4500: Loss = -11269.6730949674
Iteration 4600: Loss = -11269.614358262275
Iteration 4700: Loss = -11269.613867240048
Iteration 4800: Loss = -11269.612888808071
Iteration 4900: Loss = -11269.611951575076
Iteration 5000: Loss = -11269.619052018534
1
Iteration 5100: Loss = -11269.522249927542
Iteration 5200: Loss = -11269.517051828609
Iteration 5300: Loss = -11269.225964672267
Iteration 5400: Loss = -11250.5863508554
Iteration 5500: Loss = -11235.43630582975
Iteration 5600: Loss = -11224.198886292723
Iteration 5700: Loss = -11195.409983349666
Iteration 5800: Loss = -11190.172330632948
Iteration 5900: Loss = -11163.175797506221
Iteration 6000: Loss = -11157.37523450578
Iteration 6100: Loss = -11157.369652192512
Iteration 6200: Loss = -11154.99823291704
Iteration 6300: Loss = -11140.613471601304
Iteration 6400: Loss = -11140.403688332959
Iteration 6500: Loss = -11136.694561678732
Iteration 6600: Loss = -11136.632428643537
Iteration 6700: Loss = -11136.631270853422
Iteration 6800: Loss = -11136.627021357977
Iteration 6900: Loss = -11122.417281654029
Iteration 7000: Loss = -11111.323080037837
Iteration 7100: Loss = -11111.172533067867
Iteration 7200: Loss = -11111.142935056443
Iteration 7300: Loss = -11111.142136859715
Iteration 7400: Loss = -11109.934787451471
Iteration 7500: Loss = -11109.89204408363
Iteration 7600: Loss = -11109.88601320585
Iteration 7700: Loss = -11109.810282143353
Iteration 7800: Loss = -11109.818721914611
1
Iteration 7900: Loss = -11109.809401713028
Iteration 8000: Loss = -11109.80916190316
Iteration 8100: Loss = -11109.777150512446
Iteration 8200: Loss = -11109.78007839535
1
Iteration 8300: Loss = -11109.780770808582
2
Iteration 8400: Loss = -11109.776885826659
Iteration 8500: Loss = -11109.778165152484
1
Iteration 8600: Loss = -11109.775825775758
Iteration 8700: Loss = -11109.776136709019
1
Iteration 8800: Loss = -11109.777585446767
2
Iteration 8900: Loss = -11109.77654701438
3
Iteration 9000: Loss = -11109.77838567824
4
Iteration 9100: Loss = -11109.77799804362
5
Iteration 9200: Loss = -11109.79098262627
6
Iteration 9300: Loss = -11109.772892689705
Iteration 9400: Loss = -11109.771381497514
Iteration 9500: Loss = -11109.770672301653
Iteration 9600: Loss = -11109.773337743543
1
Iteration 9700: Loss = -11109.770709284247
Iteration 9800: Loss = -11109.769269172264
Iteration 9900: Loss = -11109.794312558022
1
Iteration 10000: Loss = -11109.7557569283
Iteration 10100: Loss = -11109.755838246603
Iteration 10200: Loss = -11109.76931034688
1
Iteration 10300: Loss = -11109.767625172091
2
Iteration 10400: Loss = -11109.754442662996
Iteration 10500: Loss = -11103.481822201153
Iteration 10600: Loss = -11103.436677428112
Iteration 10700: Loss = -11103.462297566071
1
Iteration 10800: Loss = -11103.436461797271
Iteration 10900: Loss = -11103.42649817833
Iteration 11000: Loss = -11103.4250304014
Iteration 11100: Loss = -11103.430078092131
1
Iteration 11200: Loss = -11103.434617878003
2
Iteration 11300: Loss = -11103.428203468693
3
Iteration 11400: Loss = -11103.422934015744
Iteration 11500: Loss = -11103.423363988719
1
Iteration 11600: Loss = -11103.420756334725
Iteration 11700: Loss = -11103.41985040126
Iteration 11800: Loss = -11103.711459965427
1
Iteration 11900: Loss = -11103.419554747641
Iteration 12000: Loss = -11103.480804241613
1
Iteration 12100: Loss = -11103.4212132139
2
Iteration 12200: Loss = -11103.421018656607
3
Iteration 12300: Loss = -11103.419610610059
Iteration 12400: Loss = -11103.418591252826
Iteration 12500: Loss = -11103.42229397493
1
Iteration 12600: Loss = -11103.423598193582
2
Iteration 12700: Loss = -11103.412839219014
Iteration 12800: Loss = -11103.412496807188
Iteration 12900: Loss = -11103.431115754467
1
Iteration 13000: Loss = -11103.41179819439
Iteration 13100: Loss = -11103.415699428275
1
Iteration 13200: Loss = -11103.411674493072
Iteration 13300: Loss = -11103.411239477186
Iteration 13400: Loss = -11103.410993861527
Iteration 13500: Loss = -11103.410657233204
Iteration 13600: Loss = -11103.415812716124
1
Iteration 13700: Loss = -11103.410386271718
Iteration 13800: Loss = -11103.461702005656
1
Iteration 13900: Loss = -11103.409258407995
Iteration 14000: Loss = -11103.40732717939
Iteration 14100: Loss = -11103.404835178588
Iteration 14200: Loss = -11103.405812944766
1
Iteration 14300: Loss = -11103.404618240003
Iteration 14400: Loss = -11103.405097310078
1
Iteration 14500: Loss = -11103.403887390374
Iteration 14600: Loss = -11103.403449764335
Iteration 14700: Loss = -11103.416919954718
1
Iteration 14800: Loss = -11103.40574140637
2
Iteration 14900: Loss = -11103.422966934731
3
Iteration 15000: Loss = -11103.403018650475
Iteration 15100: Loss = -11103.410978741265
1
Iteration 15200: Loss = -11103.40282496274
Iteration 15300: Loss = -11103.40219215068
Iteration 15400: Loss = -11103.43661415731
1
Iteration 15500: Loss = -11103.402056253582
Iteration 15600: Loss = -11103.417951492494
1
Iteration 15700: Loss = -11103.396032295377
Iteration 15800: Loss = -11103.399866496908
1
Iteration 15900: Loss = -11103.41116527994
2
Iteration 16000: Loss = -11103.399489100499
3
Iteration 16100: Loss = -11103.607377876819
4
Iteration 16200: Loss = -11103.395657198045
Iteration 16300: Loss = -11103.398083795644
1
Iteration 16400: Loss = -11103.401378567594
2
Iteration 16500: Loss = -11103.395839989847
3
Iteration 16600: Loss = -11103.395764115141
4
Iteration 16700: Loss = -11103.39642169754
5
Iteration 16800: Loss = -11103.515767477124
6
Iteration 16900: Loss = -11103.399615107692
7
Iteration 17000: Loss = -11103.398850648173
8
Iteration 17100: Loss = -11103.40481469377
9
Iteration 17200: Loss = -11103.395612215312
Iteration 17300: Loss = -11103.396782849277
1
Iteration 17400: Loss = -11103.397329669544
2
Iteration 17500: Loss = -11103.413733125017
3
Iteration 17600: Loss = -11103.395662808245
Iteration 17700: Loss = -11103.395519423326
Iteration 17800: Loss = -11103.39803548283
1
Iteration 17900: Loss = -11103.394422374182
Iteration 18000: Loss = -11103.39465247566
1
Iteration 18100: Loss = -11103.394401729687
Iteration 18200: Loss = -11103.394703949009
1
Iteration 18300: Loss = -11103.394823973707
2
Iteration 18400: Loss = -11103.394426928391
Iteration 18500: Loss = -11103.395906021377
1
Iteration 18600: Loss = -11103.39521369598
2
Iteration 18700: Loss = -11103.40771495746
3
Iteration 18800: Loss = -11103.406432075511
4
Iteration 18900: Loss = -11103.394424558397
Iteration 19000: Loss = -11103.394518926521
Iteration 19100: Loss = -11103.445365368694
1
Iteration 19200: Loss = -11103.419371802654
2
Iteration 19300: Loss = -11103.40870456953
3
Iteration 19400: Loss = -11103.400638745432
4
Iteration 19500: Loss = -11103.468260723377
5
Iteration 19600: Loss = -11103.429215139895
6
Iteration 19700: Loss = -11103.397758282996
7
Iteration 19800: Loss = -11103.396200950776
8
Iteration 19900: Loss = -11103.399959295004
9
pi: tensor([[0.7081, 0.2919],
        [0.2755, 0.7245]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4541, 0.5459], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3060, 0.1053],
         [0.7279, 0.1964]],

        [[0.6818, 0.1056],
         [0.7059, 0.6016]],

        [[0.5033, 0.0980],
         [0.7243, 0.7028]],

        [[0.5894, 0.0970],
         [0.6095, 0.6543]],

        [[0.5499, 0.0959],
         [0.6823, 0.5323]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9446733541449006
Average Adjusted Rand Index: 0.9446433623199184
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22711.680097268323
Iteration 100: Loss = -11378.930851115358
Iteration 200: Loss = -11378.186585224248
Iteration 300: Loss = -11376.841186507774
Iteration 400: Loss = -11368.347575186364
Iteration 500: Loss = -11174.421385642216
Iteration 600: Loss = -11165.202154635443
Iteration 700: Loss = -11163.761171495797
Iteration 800: Loss = -11163.52350694482
Iteration 900: Loss = -11162.451535454913
Iteration 1000: Loss = -11162.353557331979
Iteration 1100: Loss = -11162.294375882324
Iteration 1200: Loss = -11162.2547331705
Iteration 1300: Loss = -11162.223837536736
Iteration 1400: Loss = -11162.187673643211
Iteration 1500: Loss = -11162.138215995212
Iteration 1600: Loss = -11162.121582663733
Iteration 1700: Loss = -11162.11077245553
Iteration 1800: Loss = -11162.102381514824
Iteration 1900: Loss = -11162.09556571336
Iteration 2000: Loss = -11162.095105455002
Iteration 2100: Loss = -11162.085087831621
Iteration 2200: Loss = -11162.081009786743
Iteration 2300: Loss = -11162.077475007254
Iteration 2400: Loss = -11162.074419002229
Iteration 2500: Loss = -11162.071743723971
Iteration 2600: Loss = -11162.069411086304
Iteration 2700: Loss = -11162.067576818088
Iteration 2800: Loss = -11162.06555297009
Iteration 2900: Loss = -11162.06392476738
Iteration 3000: Loss = -11162.062665113273
Iteration 3100: Loss = -11162.061627277026
Iteration 3200: Loss = -11162.060179004078
Iteration 3300: Loss = -11162.059109986438
Iteration 3400: Loss = -11162.06084256959
1
Iteration 3500: Loss = -11162.057395429849
Iteration 3600: Loss = -11162.088959433648
1
Iteration 3700: Loss = -11162.056027896493
Iteration 3800: Loss = -11162.055385824051
Iteration 3900: Loss = -11162.054856938366
Iteration 4000: Loss = -11162.05447266939
Iteration 4100: Loss = -11162.064908187911
1
Iteration 4200: Loss = -11162.05348350342
Iteration 4300: Loss = -11162.054845297676
1
Iteration 4400: Loss = -11162.052750099676
Iteration 4500: Loss = -11162.053757649419
1
Iteration 4600: Loss = -11162.053421578417
2
Iteration 4700: Loss = -11162.051839325493
Iteration 4800: Loss = -11162.051643907726
Iteration 4900: Loss = -11162.051412019708
Iteration 5000: Loss = -11162.05118090712
Iteration 5100: Loss = -11162.050982544853
Iteration 5200: Loss = -11162.056380568614
1
Iteration 5300: Loss = -11162.050602001675
Iteration 5400: Loss = -11162.057427351896
1
Iteration 5500: Loss = -11162.05028957487
Iteration 5600: Loss = -11162.050172880388
Iteration 5700: Loss = -11162.050045945181
Iteration 5800: Loss = -11162.050218093613
1
Iteration 5900: Loss = -11162.049768809682
Iteration 6000: Loss = -11162.049620745433
Iteration 6100: Loss = -11162.04954157433
Iteration 6200: Loss = -11162.049452875437
Iteration 6300: Loss = -11162.049470362339
Iteration 6400: Loss = -11162.049160172377
Iteration 6500: Loss = -11162.055973893535
1
Iteration 6600: Loss = -11162.048787399764
Iteration 6700: Loss = -11162.048687580185
Iteration 6800: Loss = -11162.048743334291
Iteration 6900: Loss = -11162.048603988693
Iteration 7000: Loss = -11162.048541535825
Iteration 7100: Loss = -11162.048495810723
Iteration 7200: Loss = -11162.048417507236
Iteration 7300: Loss = -11162.048424303544
Iteration 7400: Loss = -11162.048329604007
Iteration 7500: Loss = -11162.04826082722
Iteration 7600: Loss = -11162.048233211142
Iteration 7700: Loss = -11162.048156732828
Iteration 7800: Loss = -11162.048260433035
1
Iteration 7900: Loss = -11162.047952679488
Iteration 8000: Loss = -11162.047869738304
Iteration 8100: Loss = -11162.047782788602
Iteration 8200: Loss = -11162.047764385094
Iteration 8300: Loss = -11162.05055780147
1
Iteration 8400: Loss = -11162.047687221244
Iteration 8500: Loss = -11162.047862203592
1
Iteration 8600: Loss = -11162.047633881286
Iteration 8700: Loss = -11162.047785133538
1
Iteration 8800: Loss = -11162.04751757106
Iteration 8900: Loss = -11162.04753515916
Iteration 9000: Loss = -11162.05141263153
1
Iteration 9100: Loss = -11162.079274299427
2
Iteration 9200: Loss = -11162.047451056986
Iteration 9300: Loss = -11162.04950971158
1
Iteration 9400: Loss = -11162.047763047209
2
Iteration 9500: Loss = -11162.047370529213
Iteration 9600: Loss = -11162.080127406382
1
Iteration 9700: Loss = -11162.047313970768
Iteration 9800: Loss = -11162.048106809445
1
Iteration 9900: Loss = -11162.056459765352
2
Iteration 10000: Loss = -11162.047438590043
3
Iteration 10100: Loss = -11162.048095897535
4
Iteration 10200: Loss = -11162.047677834955
5
Iteration 10300: Loss = -11162.046861379911
Iteration 10400: Loss = -11162.048483134475
1
Iteration 10500: Loss = -11162.330817586831
2
Iteration 10600: Loss = -11162.046514274567
Iteration 10700: Loss = -11162.268804669931
1
Iteration 10800: Loss = -11162.04646847557
Iteration 10900: Loss = -11162.048198004197
1
Iteration 11000: Loss = -11162.063864063886
2
Iteration 11100: Loss = -11162.046637541813
3
Iteration 11200: Loss = -11162.04688027698
4
Iteration 11300: Loss = -11162.048664401635
5
Iteration 11400: Loss = -11162.047893311448
6
Iteration 11500: Loss = -11162.107849102269
7
Iteration 11600: Loss = -11162.046538045877
Iteration 11700: Loss = -11162.047135586317
1
Iteration 11800: Loss = -11162.052544787042
2
Iteration 11900: Loss = -11162.046421076266
Iteration 12000: Loss = -11162.048507352114
1
Iteration 12100: Loss = -11162.30182737976
2
Iteration 12200: Loss = -11162.046339194621
Iteration 12300: Loss = -11162.049355882533
1
Iteration 12400: Loss = -11162.048453762316
2
Iteration 12500: Loss = -11162.061041359475
3
Iteration 12600: Loss = -11162.059726577254
4
Iteration 12700: Loss = -11162.046607017608
5
Iteration 12800: Loss = -11162.046277910307
Iteration 12900: Loss = -11162.046482907474
1
Iteration 13000: Loss = -11162.062596544865
2
Iteration 13100: Loss = -11162.046158982244
Iteration 13200: Loss = -11162.04626847147
1
Iteration 13300: Loss = -11162.167987761059
2
Iteration 13400: Loss = -11162.046088334508
Iteration 13500: Loss = -11162.065420508465
1
Iteration 13600: Loss = -11162.046110297008
Iteration 13700: Loss = -11162.046410971654
1
Iteration 13800: Loss = -11162.107521861153
2
Iteration 13900: Loss = -11162.046074649541
Iteration 14000: Loss = -11162.046487908481
1
Iteration 14100: Loss = -11162.109147442034
2
Iteration 14200: Loss = -11162.046061858618
Iteration 14300: Loss = -11162.067821498034
1
Iteration 14400: Loss = -11162.046408063254
2
Iteration 14500: Loss = -11162.048884192873
3
Iteration 14600: Loss = -11162.053302588078
4
Iteration 14700: Loss = -11162.050060205775
5
Iteration 14800: Loss = -11162.048271095367
6
Iteration 14900: Loss = -11162.053281420947
7
Iteration 15000: Loss = -11162.046138031803
Iteration 15100: Loss = -11162.046179881087
Iteration 15200: Loss = -11162.272251955497
1
Iteration 15300: Loss = -11162.046500082206
2
Iteration 15400: Loss = -11162.074760434465
3
Iteration 15500: Loss = -11162.046072855537
Iteration 15600: Loss = -11162.049837743485
1
Iteration 15700: Loss = -11162.04958504511
2
Iteration 15800: Loss = -11162.04702151485
3
Iteration 15900: Loss = -11162.060977228217
4
Iteration 16000: Loss = -11162.046371648652
5
Iteration 16100: Loss = -11162.060759315786
6
Iteration 16200: Loss = -11162.047420738396
7
Iteration 16300: Loss = -11162.046308861452
8
Iteration 16400: Loss = -11162.047157958054
9
Iteration 16500: Loss = -11162.053860383568
10
Iteration 16600: Loss = -11162.046155174243
Iteration 16700: Loss = -11162.047675499998
1
Iteration 16800: Loss = -11162.050783043045
2
Iteration 16900: Loss = -11162.048673065698
3
Iteration 17000: Loss = -11162.046976565904
4
Iteration 17100: Loss = -11162.049650088895
5
Iteration 17200: Loss = -11162.141239106728
6
Iteration 17300: Loss = -11162.049116374184
7
Iteration 17400: Loss = -11162.04806084421
8
Iteration 17500: Loss = -11162.07050755525
9
Iteration 17600: Loss = -11162.04612761628
Iteration 17700: Loss = -11162.049159090471
1
Iteration 17800: Loss = -11162.046309771855
2
Iteration 17900: Loss = -11162.049585156949
3
Iteration 18000: Loss = -11162.121503075527
4
Iteration 18100: Loss = -11162.046032457107
Iteration 18200: Loss = -11162.050133493556
1
Iteration 18300: Loss = -11162.04677190054
2
Iteration 18400: Loss = -11162.046184177378
3
Iteration 18500: Loss = -11162.046104634872
Iteration 18600: Loss = -11162.048664431071
1
Iteration 18700: Loss = -11162.047873841699
2
Iteration 18800: Loss = -11162.04608485861
Iteration 18900: Loss = -11162.047404351091
1
Iteration 19000: Loss = -11162.050203501552
2
Iteration 19100: Loss = -11162.046029935847
Iteration 19200: Loss = -11162.046552451946
1
Iteration 19300: Loss = -11162.067836397988
2
Iteration 19400: Loss = -11162.045896099868
Iteration 19500: Loss = -11162.07031068432
1
Iteration 19600: Loss = -11162.045938430721
Iteration 19700: Loss = -11162.046452298538
1
Iteration 19800: Loss = -11162.190342405625
2
Iteration 19900: Loss = -11162.045872235509
pi: tensor([[0.6550, 0.3450],
        [0.2913, 0.7087]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9817, 0.0183], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1835, 0.2708],
         [0.5340, 0.3064]],

        [[0.6409, 0.1073],
         [0.6588, 0.5043]],

        [[0.6003, 0.0982],
         [0.6054, 0.6941]],

        [[0.6647, 0.0971],
         [0.5526, 0.6410]],

        [[0.6861, 0.0961],
         [0.5111, 0.7162]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.6328830751170331
Average Adjusted Rand Index: 0.7455322512088074
11126.938023790199
[0.9446733541449006, 0.6328830751170331] [0.9446433623199184, 0.7455322512088074] [11103.39663633437, 11162.046141384257]
-------------------------------------
This iteration is 70
True Objective function: Loss = -11433.091775103168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24643.655964807334
Iteration 100: Loss = -11748.91360445515
Iteration 200: Loss = -11738.79828252544
Iteration 300: Loss = -11664.589914988346
Iteration 400: Loss = -11533.282355878302
Iteration 500: Loss = -11512.343042963676
Iteration 600: Loss = -11505.917255002998
Iteration 700: Loss = -11505.626302271738
Iteration 800: Loss = -11504.6686624571
Iteration 900: Loss = -11504.619290945955
Iteration 1000: Loss = -11504.579705636006
Iteration 1100: Loss = -11502.287823446453
Iteration 1200: Loss = -11502.23206982015
Iteration 1300: Loss = -11502.163097135024
Iteration 1400: Loss = -11502.147892351768
Iteration 1500: Loss = -11502.138390878847
Iteration 1600: Loss = -11502.129634256415
Iteration 1700: Loss = -11502.11290299445
Iteration 1800: Loss = -11502.107192472396
Iteration 1900: Loss = -11502.102690161806
Iteration 2000: Loss = -11502.098598054088
Iteration 2100: Loss = -11502.093867591935
Iteration 2200: Loss = -11502.081689353805
Iteration 2300: Loss = -11502.076983783727
Iteration 2400: Loss = -11502.072020626354
Iteration 2500: Loss = -11502.065292047218
Iteration 2600: Loss = -11502.056663329411
Iteration 2700: Loss = -11502.046005306782
Iteration 2800: Loss = -11502.029029336729
Iteration 2900: Loss = -11501.99021723254
Iteration 3000: Loss = -11501.876702507549
Iteration 3100: Loss = -11499.629092256153
Iteration 3200: Loss = -11436.506633057028
Iteration 3300: Loss = -11423.584080557099
Iteration 3400: Loss = -11423.534874183448
Iteration 3500: Loss = -11423.504250236942
Iteration 3600: Loss = -11413.129880792954
Iteration 3700: Loss = -11413.123555409484
Iteration 3800: Loss = -11410.180774887423
Iteration 3900: Loss = -11404.953337724715
Iteration 4000: Loss = -11404.944709398003
Iteration 4100: Loss = -11404.883240603676
Iteration 4200: Loss = -11404.88156345249
Iteration 4300: Loss = -11404.88138764664
Iteration 4400: Loss = -11404.880366525158
Iteration 4500: Loss = -11404.885543347234
1
Iteration 4600: Loss = -11404.879363330521
Iteration 4700: Loss = -11404.878988103892
Iteration 4800: Loss = -11404.878796366716
Iteration 4900: Loss = -11404.88051640955
1
Iteration 5000: Loss = -11404.879071758845
2
Iteration 5100: Loss = -11404.878509407194
Iteration 5200: Loss = -11404.87824372319
Iteration 5300: Loss = -11404.877917622573
Iteration 5400: Loss = -11404.879064578181
1
Iteration 5500: Loss = -11404.878326967995
2
Iteration 5600: Loss = -11404.877532685709
Iteration 5700: Loss = -11404.877464865847
Iteration 5800: Loss = -11404.879359046121
1
Iteration 5900: Loss = -11404.877794297901
2
Iteration 6000: Loss = -11404.878025425103
3
Iteration 6100: Loss = -11404.877388320569
Iteration 6200: Loss = -11404.877153555322
Iteration 6300: Loss = -11404.877202108819
Iteration 6400: Loss = -11404.886274547986
1
Iteration 6500: Loss = -11404.88140023542
2
Iteration 6600: Loss = -11404.87679448531
Iteration 6700: Loss = -11404.876963489314
1
Iteration 6800: Loss = -11404.87731239428
2
Iteration 6900: Loss = -11404.8766822529
Iteration 7000: Loss = -11404.908581823394
1
Iteration 7100: Loss = -11404.876615704654
Iteration 7200: Loss = -11404.9195091408
1
Iteration 7300: Loss = -11404.876491916795
Iteration 7400: Loss = -11404.874675627865
Iteration 7500: Loss = -11404.873969881946
Iteration 7600: Loss = -11404.873895148961
Iteration 7700: Loss = -11405.067095990098
1
Iteration 7800: Loss = -11404.873856373675
Iteration 7900: Loss = -11404.873840330358
Iteration 8000: Loss = -11404.89582039089
1
Iteration 8100: Loss = -11404.873744486698
Iteration 8200: Loss = -11404.873680217444
Iteration 8300: Loss = -11404.903625490533
1
Iteration 8400: Loss = -11404.873681279263
Iteration 8500: Loss = -11404.873681672412
Iteration 8600: Loss = -11404.904105114081
1
Iteration 8700: Loss = -11404.873637516974
Iteration 8800: Loss = -11404.873581103646
Iteration 8900: Loss = -11404.892979170923
1
Iteration 9000: Loss = -11404.873584860803
Iteration 9100: Loss = -11404.873499009194
Iteration 9200: Loss = -11404.87412564101
1
Iteration 9300: Loss = -11404.873408235826
Iteration 9400: Loss = -11404.873305659208
Iteration 9500: Loss = -11404.870490058072
Iteration 9600: Loss = -11404.870695002017
1
Iteration 9700: Loss = -11404.870403875628
Iteration 9800: Loss = -11404.870864486325
1
Iteration 9900: Loss = -11404.870436334977
Iteration 10000: Loss = -11404.871105486025
1
Iteration 10100: Loss = -11404.875772356114
2
Iteration 10200: Loss = -11404.870876742156
3
Iteration 10300: Loss = -11404.870396507698
Iteration 10400: Loss = -11404.87376593088
1
Iteration 10500: Loss = -11404.873627142557
2
Iteration 10600: Loss = -11404.870861075786
3
Iteration 10700: Loss = -11404.965385821535
4
Iteration 10800: Loss = -11404.869479407342
Iteration 10900: Loss = -11404.869643369613
1
Iteration 11000: Loss = -11404.867114186929
Iteration 11100: Loss = -11404.87461569827
1
Iteration 11200: Loss = -11404.867085395043
Iteration 11300: Loss = -11404.868080564906
1
Iteration 11400: Loss = -11404.989946757012
2
Iteration 11500: Loss = -11404.866988021458
Iteration 11600: Loss = -11404.870520399429
1
Iteration 11700: Loss = -11404.866993880145
Iteration 11800: Loss = -11404.872143997289
1
Iteration 11900: Loss = -11404.867006078432
Iteration 12000: Loss = -11404.867122585612
1
Iteration 12100: Loss = -11404.866968308594
Iteration 12200: Loss = -11404.868205682165
1
Iteration 12300: Loss = -11404.866933343295
Iteration 12400: Loss = -11404.86771312865
1
Iteration 12500: Loss = -11404.866836988434
Iteration 12600: Loss = -11404.8669853996
1
Iteration 12700: Loss = -11404.866821789079
Iteration 12800: Loss = -11404.864582947948
Iteration 12900: Loss = -11404.864084542362
Iteration 13000: Loss = -11405.017445311445
1
Iteration 13100: Loss = -11404.864098039387
Iteration 13200: Loss = -11404.864148228151
Iteration 13300: Loss = -11404.894367739935
1
Iteration 13400: Loss = -11404.97520895963
2
Iteration 13500: Loss = -11404.864083598597
Iteration 13600: Loss = -11404.867742667138
1
Iteration 13700: Loss = -11404.864092198468
Iteration 13800: Loss = -11404.864757925425
1
Iteration 13900: Loss = -11404.863911902889
Iteration 14000: Loss = -11404.87934032291
1
Iteration 14100: Loss = -11404.863919823147
Iteration 14200: Loss = -11404.863951470481
Iteration 14300: Loss = -11404.864022675527
Iteration 14400: Loss = -11404.971517946306
1
Iteration 14500: Loss = -11404.863622047766
Iteration 14600: Loss = -11404.867143115194
1
Iteration 14700: Loss = -11404.863599408116
Iteration 14800: Loss = -11404.868732651168
1
Iteration 14900: Loss = -11404.863605554303
Iteration 15000: Loss = -11404.86359192076
Iteration 15100: Loss = -11404.86362845365
Iteration 15200: Loss = -11404.863281544356
Iteration 15300: Loss = -11404.996180310296
1
Iteration 15400: Loss = -11404.863304122488
Iteration 15500: Loss = -11404.873833077589
1
Iteration 15600: Loss = -11404.863319004113
Iteration 15700: Loss = -11404.865936153943
1
Iteration 15800: Loss = -11404.863309743736
Iteration 15900: Loss = -11404.863485441541
1
Iteration 16000: Loss = -11404.918151027749
2
Iteration 16100: Loss = -11404.863458964848
3
Iteration 16200: Loss = -11404.86330686136
Iteration 16300: Loss = -11404.87300169229
1
Iteration 16400: Loss = -11404.864164699939
2
Iteration 16500: Loss = -11404.863476393484
3
Iteration 16600: Loss = -11404.86600672104
4
Iteration 16700: Loss = -11404.86327920886
Iteration 16800: Loss = -11404.864264626918
1
Iteration 16900: Loss = -11404.863298349068
Iteration 17000: Loss = -11404.863275213202
Iteration 17100: Loss = -11404.863788105637
1
Iteration 17200: Loss = -11404.8632792026
Iteration 17300: Loss = -11404.881196156512
1
Iteration 17400: Loss = -11404.86326594976
Iteration 17500: Loss = -11404.863443569742
1
Iteration 17600: Loss = -11404.86332514203
Iteration 17700: Loss = -11404.863253921218
Iteration 17800: Loss = -11404.870265732301
1
Iteration 17900: Loss = -11404.86305269228
Iteration 18000: Loss = -11404.873494123018
1
Iteration 18100: Loss = -11404.863045345099
Iteration 18200: Loss = -11404.871591202816
1
Iteration 18300: Loss = -11404.909201925639
2
Iteration 18400: Loss = -11404.864419863834
3
Iteration 18500: Loss = -11404.863335425813
4
Iteration 18600: Loss = -11404.863096912974
Iteration 18700: Loss = -11404.87330462937
1
Iteration 18800: Loss = -11404.863062082579
Iteration 18900: Loss = -11405.306289522297
1
Iteration 19000: Loss = -11404.86305422848
Iteration 19100: Loss = -11404.863061990354
Iteration 19200: Loss = -11404.863140046973
Iteration 19300: Loss = -11404.874314748671
1
Iteration 19400: Loss = -11404.8637146957
2
Iteration 19500: Loss = -11404.86312666752
Iteration 19600: Loss = -11404.863059760437
Iteration 19700: Loss = -11404.863286594347
1
Iteration 19800: Loss = -11404.863069226822
Iteration 19900: Loss = -11404.863082071442
pi: tensor([[0.7703, 0.2297],
        [0.2184, 0.7816]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4758, 0.5242], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2038, 0.1019],
         [0.5521, 0.3093]],

        [[0.6962, 0.1006],
         [0.6940, 0.5183]],

        [[0.5292, 0.1039],
         [0.5331, 0.5399]],

        [[0.5304, 0.1129],
         [0.6732, 0.5493]],

        [[0.5262, 0.1031],
         [0.5584, 0.6402]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9137635455841352
Average Adjusted Rand Index: 0.9136146698549611
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22778.552183743803
Iteration 100: Loss = -11750.941196468224
Iteration 200: Loss = -11750.114753951511
Iteration 300: Loss = -11748.57174201273
Iteration 400: Loss = -11745.076905982844
Iteration 500: Loss = -11743.58997673667
Iteration 600: Loss = -11727.777847176816
Iteration 700: Loss = -11536.670223262683
Iteration 800: Loss = -11478.88763054113
Iteration 900: Loss = -11451.108672097638
Iteration 1000: Loss = -11414.732585881482
Iteration 1100: Loss = -11407.575221457897
Iteration 1200: Loss = -11406.415975029757
Iteration 1300: Loss = -11406.092425808538
Iteration 1400: Loss = -11406.03132800898
Iteration 1500: Loss = -11405.98870932467
Iteration 1600: Loss = -11405.95772745796
Iteration 1700: Loss = -11405.933718993896
Iteration 1800: Loss = -11405.913217171506
Iteration 1900: Loss = -11405.891564994603
Iteration 2000: Loss = -11405.818508473794
Iteration 2100: Loss = -11405.452887929487
Iteration 2200: Loss = -11405.439918299844
Iteration 2300: Loss = -11405.429390199524
Iteration 2400: Loss = -11405.417071215306
Iteration 2500: Loss = -11405.16150103764
Iteration 2600: Loss = -11405.052726548252
Iteration 2700: Loss = -11405.029857429938
Iteration 2800: Loss = -11405.015880093153
Iteration 2900: Loss = -11404.952066573476
Iteration 3000: Loss = -11404.944735967134
Iteration 3100: Loss = -11404.9405861844
Iteration 3200: Loss = -11404.936699278864
Iteration 3300: Loss = -11404.93560335613
Iteration 3400: Loss = -11404.932135964069
Iteration 3500: Loss = -11404.93098245851
Iteration 3600: Loss = -11404.930062929641
Iteration 3700: Loss = -11404.921646658127
Iteration 3800: Loss = -11404.920168739181
Iteration 3900: Loss = -11404.918797500552
Iteration 4000: Loss = -11404.917622628114
Iteration 4100: Loss = -11404.917435560386
Iteration 4200: Loss = -11404.91564983411
Iteration 4300: Loss = -11404.916153888307
1
Iteration 4400: Loss = -11404.913962309538
Iteration 4500: Loss = -11404.913278049025
Iteration 4600: Loss = -11404.912475319255
Iteration 4700: Loss = -11404.911660853859
Iteration 4800: Loss = -11404.911908359885
1
Iteration 4900: Loss = -11404.90982658284
Iteration 5000: Loss = -11404.90866687916
Iteration 5100: Loss = -11404.907594971173
Iteration 5200: Loss = -11404.904514772777
Iteration 5300: Loss = -11404.903459363486
Iteration 5400: Loss = -11404.902942466086
Iteration 5500: Loss = -11404.902494501308
Iteration 5600: Loss = -11404.902090624119
Iteration 5700: Loss = -11404.90175272245
Iteration 5800: Loss = -11404.90137832711
Iteration 5900: Loss = -11404.901096023588
Iteration 6000: Loss = -11404.900969381744
Iteration 6100: Loss = -11404.900543017806
Iteration 6200: Loss = -11404.900294931804
Iteration 6300: Loss = -11404.905241360926
1
Iteration 6400: Loss = -11404.899759438273
Iteration 6500: Loss = -11404.899398331716
Iteration 6600: Loss = -11404.899008881881
Iteration 6700: Loss = -11404.90059420013
1
Iteration 6800: Loss = -11404.896338310416
Iteration 6900: Loss = -11404.879269920688
Iteration 7000: Loss = -11404.872401438684
Iteration 7100: Loss = -11404.872128447087
Iteration 7200: Loss = -11404.873008887032
1
Iteration 7300: Loss = -11404.871807886551
Iteration 7400: Loss = -11404.871671017165
Iteration 7500: Loss = -11404.871482917937
Iteration 7600: Loss = -11404.871331766542
Iteration 7700: Loss = -11404.871139159417
Iteration 7800: Loss = -11404.886198936707
1
Iteration 7900: Loss = -11404.870732987765
Iteration 8000: Loss = -11404.880586000889
1
Iteration 8100: Loss = -11404.870096099823
Iteration 8200: Loss = -11404.869870854669
Iteration 8300: Loss = -11404.869843859911
Iteration 8400: Loss = -11404.869661484752
Iteration 8500: Loss = -11404.881704194855
1
Iteration 8600: Loss = -11404.869517435794
Iteration 8700: Loss = -11404.869427469432
Iteration 8800: Loss = -11404.870443197573
1
Iteration 8900: Loss = -11404.8673589351
Iteration 9000: Loss = -11404.867259314955
Iteration 9100: Loss = -11404.867835184958
1
Iteration 9200: Loss = -11404.867165332827
Iteration 9300: Loss = -11404.871826002085
1
Iteration 9400: Loss = -11404.86705822859
Iteration 9500: Loss = -11404.866958135335
Iteration 9600: Loss = -11404.867485963838
1
Iteration 9700: Loss = -11404.866776825394
Iteration 9800: Loss = -11404.868271042676
1
Iteration 9900: Loss = -11404.923960730484
2
Iteration 10000: Loss = -11404.866286315142
Iteration 10100: Loss = -11404.86587074971
Iteration 10200: Loss = -11404.864697562745
Iteration 10300: Loss = -11404.864880117493
1
Iteration 10400: Loss = -11404.86465263493
Iteration 10500: Loss = -11404.864660469075
Iteration 10600: Loss = -11404.864611742662
Iteration 10700: Loss = -11404.867562236443
1
Iteration 10800: Loss = -11404.864562306739
Iteration 10900: Loss = -11404.871200858513
1
Iteration 11000: Loss = -11404.864569180036
Iteration 11100: Loss = -11404.864531193101
Iteration 11200: Loss = -11404.864646585618
1
Iteration 11300: Loss = -11404.864529850078
Iteration 11400: Loss = -11404.864522517964
Iteration 11500: Loss = -11404.864449591845
Iteration 11600: Loss = -11404.865378813807
1
Iteration 11700: Loss = -11404.864153882632
Iteration 11800: Loss = -11404.864144219322
Iteration 11900: Loss = -11404.865928730365
1
Iteration 12000: Loss = -11404.864143347833
Iteration 12100: Loss = -11404.864180835282
Iteration 12200: Loss = -11404.864188713836
Iteration 12300: Loss = -11404.864118047166
Iteration 12400: Loss = -11404.951750853663
1
Iteration 12500: Loss = -11404.864078520972
Iteration 12600: Loss = -11404.864106269162
Iteration 12700: Loss = -11404.871592439507
1
Iteration 12800: Loss = -11404.864051167575
Iteration 12900: Loss = -11404.864037122097
Iteration 13000: Loss = -11404.866011025326
1
Iteration 13100: Loss = -11404.864044616355
Iteration 13200: Loss = -11404.979647362969
1
Iteration 13300: Loss = -11404.8639653398
Iteration 13400: Loss = -11404.863956270217
Iteration 13500: Loss = -11404.882225658072
1
Iteration 13600: Loss = -11404.872091022462
2
Iteration 13700: Loss = -11404.867193757545
3
Iteration 13800: Loss = -11404.871411631759
4
Iteration 13900: Loss = -11404.863776021373
Iteration 14000: Loss = -11404.870132950013
1
Iteration 14100: Loss = -11404.863768752768
Iteration 14200: Loss = -11405.284831659941
1
Iteration 14300: Loss = -11404.86520425418
2
Iteration 14400: Loss = -11404.863754440576
Iteration 14500: Loss = -11404.864122049297
1
Iteration 14600: Loss = -11404.863740513953
Iteration 14700: Loss = -11404.863945615964
1
Iteration 14800: Loss = -11404.926458388734
2
Iteration 14900: Loss = -11404.863738476368
Iteration 15000: Loss = -11404.911544454435
1
Iteration 15100: Loss = -11404.863728277143
Iteration 15200: Loss = -11404.8924217603
1
Iteration 15300: Loss = -11404.864124634976
2
Iteration 15400: Loss = -11404.868231016388
3
Iteration 15500: Loss = -11404.88481182944
4
Iteration 15600: Loss = -11404.8658108773
5
Iteration 15700: Loss = -11404.86375711622
Iteration 15800: Loss = -11404.881293414328
1
Iteration 15900: Loss = -11404.863750369846
Iteration 16000: Loss = -11404.883821475158
1
Iteration 16100: Loss = -11404.863822087409
Iteration 16200: Loss = -11404.863767264504
Iteration 16300: Loss = -11404.970954695737
1
Iteration 16400: Loss = -11404.86372956562
Iteration 16500: Loss = -11405.246293736647
1
Iteration 16600: Loss = -11404.863718004439
Iteration 16700: Loss = -11404.86369869589
Iteration 16800: Loss = -11404.864089182327
1
Iteration 16900: Loss = -11404.863715547357
Iteration 17000: Loss = -11404.863703965952
Iteration 17100: Loss = -11404.86381062759
1
Iteration 17200: Loss = -11404.863730491308
Iteration 17300: Loss = -11404.999060044376
1
Iteration 17400: Loss = -11404.863635304084
Iteration 17500: Loss = -11404.86370424681
Iteration 17600: Loss = -11404.863639843279
Iteration 17700: Loss = -11404.87774250997
1
Iteration 17800: Loss = -11404.86418772204
2
Iteration 17900: Loss = -11404.865111485915
3
Iteration 18000: Loss = -11404.864716494127
4
Iteration 18100: Loss = -11404.883348348303
5
Iteration 18200: Loss = -11404.870273938213
6
Iteration 18300: Loss = -11404.863547149173
Iteration 18400: Loss = -11404.864127684958
1
Iteration 18500: Loss = -11404.864109895256
2
Iteration 18600: Loss = -11404.870702309117
3
Iteration 18700: Loss = -11404.872341884355
4
Iteration 18800: Loss = -11404.86457321668
5
Iteration 18900: Loss = -11404.864718057437
6
Iteration 19000: Loss = -11404.863532118665
Iteration 19100: Loss = -11404.863946364287
1
Iteration 19200: Loss = -11404.863575033512
Iteration 19300: Loss = -11404.86406055036
1
Iteration 19400: Loss = -11404.869142635318
2
Iteration 19500: Loss = -11404.86366861731
Iteration 19600: Loss = -11404.887469991827
1
Iteration 19700: Loss = -11404.86308548854
Iteration 19800: Loss = -11404.865690121693
1
Iteration 19900: Loss = -11404.863087915734
pi: tensor([[0.7816, 0.2184],
        [0.2300, 0.7700]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5243, 0.4757], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3093, 0.1019],
         [0.5325, 0.2037]],

        [[0.6912, 0.1008],
         [0.5358, 0.5839]],

        [[0.7066, 0.1040],
         [0.5014, 0.6499]],

        [[0.5414, 0.1129],
         [0.5197, 0.6071]],

        [[0.6955, 0.1032],
         [0.5061, 0.5251]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9137635455841352
Average Adjusted Rand Index: 0.9136146698549611
11433.091775103168
[0.9137635455841352, 0.9137635455841352] [0.9136146698549611, 0.9136146698549611] [11404.863144972096, 11404.86461628651]
-------------------------------------
This iteration is 71
True Objective function: Loss = -11031.76442142735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23551.200740564705
Iteration 100: Loss = -11236.241030739868
Iteration 200: Loss = -11235.142183866967
Iteration 300: Loss = -11234.419086314983
Iteration 400: Loss = -11229.445485894576
Iteration 500: Loss = -11223.762410738189
Iteration 600: Loss = -11198.858757057837
Iteration 700: Loss = -11153.424131413709
Iteration 800: Loss = -11129.993584597316
Iteration 900: Loss = -11018.133286030332
Iteration 1000: Loss = -11014.051052885823
Iteration 1100: Loss = -11013.44968087716
Iteration 1200: Loss = -11011.676429501993
Iteration 1300: Loss = -11011.48612094611
Iteration 1400: Loss = -11011.347925574395
Iteration 1500: Loss = -11011.032441248888
Iteration 1600: Loss = -11010.9731579369
Iteration 1700: Loss = -11010.943165195704
Iteration 1800: Loss = -11010.917672173326
Iteration 1900: Loss = -11010.895382864122
Iteration 2000: Loss = -11010.878280949353
Iteration 2100: Loss = -11010.865669913435
Iteration 2200: Loss = -11010.853433452192
Iteration 2300: Loss = -11010.797969964073
Iteration 2400: Loss = -11010.750455641
Iteration 2500: Loss = -11010.74459885515
Iteration 2600: Loss = -11010.73884434254
Iteration 2700: Loss = -11010.733595055664
Iteration 2800: Loss = -11010.728051410922
Iteration 2900: Loss = -11010.72135321207
Iteration 3000: Loss = -11010.711837568197
Iteration 3100: Loss = -11010.702894003705
Iteration 3200: Loss = -11010.697943356783
Iteration 3300: Loss = -11010.69196384161
Iteration 3400: Loss = -11010.673703077877
Iteration 3500: Loss = -11010.678297334951
1
Iteration 3600: Loss = -11010.668136823599
Iteration 3700: Loss = -11010.667191693074
Iteration 3800: Loss = -11010.662171003118
Iteration 3900: Loss = -11010.661721054723
Iteration 4000: Loss = -11010.658545723425
Iteration 4100: Loss = -11010.656718522801
Iteration 4200: Loss = -11010.655641311598
Iteration 4300: Loss = -11009.00872992604
Iteration 4400: Loss = -11008.99871493273
Iteration 4500: Loss = -11008.997656893269
Iteration 4600: Loss = -11008.996287625258
Iteration 4700: Loss = -11008.997678116228
1
Iteration 4800: Loss = -11008.994107354878
Iteration 4900: Loss = -11008.992825272295
Iteration 5000: Loss = -11008.990535901334
Iteration 5100: Loss = -11009.021645109438
1
Iteration 5200: Loss = -11008.988780837164
Iteration 5300: Loss = -11008.987964177499
Iteration 5400: Loss = -11008.987164249553
Iteration 5500: Loss = -11008.98579749972
Iteration 5600: Loss = -11008.986278026505
1
Iteration 5700: Loss = -11008.984668911138
Iteration 5800: Loss = -11008.984209356404
Iteration 5900: Loss = -11008.984198598066
Iteration 6000: Loss = -11008.983024437479
Iteration 6100: Loss = -11008.978349388699
Iteration 6200: Loss = -11008.965087063862
Iteration 6300: Loss = -11008.963588920817
Iteration 6400: Loss = -11008.963726072205
1
Iteration 6500: Loss = -11008.963866544327
2
Iteration 6600: Loss = -11008.96260736806
Iteration 6700: Loss = -11008.964731052
1
Iteration 6800: Loss = -11008.961823687856
Iteration 6900: Loss = -11008.96225109432
1
Iteration 7000: Loss = -11008.960851195176
Iteration 7100: Loss = -11007.58455669527
Iteration 7200: Loss = -11007.502330386083
Iteration 7300: Loss = -11007.505487607
1
Iteration 7400: Loss = -11007.513586609592
2
Iteration 7500: Loss = -11007.501623416174
Iteration 7600: Loss = -11007.501853681826
1
Iteration 7700: Loss = -11007.502791348215
2
Iteration 7800: Loss = -11007.500551780498
Iteration 7900: Loss = -11007.500248862585
Iteration 8000: Loss = -11007.505965662674
1
Iteration 8100: Loss = -11007.491636800516
Iteration 8200: Loss = -11007.490919194206
Iteration 8300: Loss = -11007.459542727958
Iteration 8400: Loss = -11007.511886648477
1
Iteration 8500: Loss = -11007.458088936131
Iteration 8600: Loss = -11007.458629564637
1
Iteration 8700: Loss = -11007.457656240602
Iteration 8800: Loss = -11007.54801083533
1
Iteration 8900: Loss = -11007.455906033885
Iteration 9000: Loss = -11007.456181876501
1
Iteration 9100: Loss = -11007.459379768487
2
Iteration 9200: Loss = -11007.455806022353
Iteration 9300: Loss = -11007.455668509083
Iteration 9400: Loss = -11007.456387614016
1
Iteration 9500: Loss = -11007.455639096715
Iteration 9600: Loss = -11007.455814166886
1
Iteration 9700: Loss = -11007.45902521086
2
Iteration 9800: Loss = -11007.459366334291
3
Iteration 9900: Loss = -11007.456180392746
4
Iteration 10000: Loss = -11007.50238922783
5
Iteration 10100: Loss = -11007.455818667573
6
Iteration 10200: Loss = -11007.454300329691
Iteration 10300: Loss = -11007.45715406926
1
Iteration 10400: Loss = -11007.454225522626
Iteration 10500: Loss = -11007.454347093526
1
Iteration 10600: Loss = -11007.454162497748
Iteration 10700: Loss = -11007.455251704114
1
Iteration 10800: Loss = -11007.454075231617
Iteration 10900: Loss = -11007.477519427892
1
Iteration 11000: Loss = -11007.45404559765
Iteration 11100: Loss = -11007.453982912899
Iteration 11200: Loss = -11007.447095799267
Iteration 11300: Loss = -11007.44661384835
Iteration 11400: Loss = -11007.457482375492
1
Iteration 11500: Loss = -11007.485884943519
2
Iteration 11600: Loss = -11007.451938396327
3
Iteration 11700: Loss = -11007.44910445281
4
Iteration 11800: Loss = -11007.440465686417
Iteration 11900: Loss = -11007.437900685984
Iteration 12000: Loss = -11007.438365507345
1
Iteration 12100: Loss = -11007.743892325432
2
Iteration 12200: Loss = -11007.437281331346
Iteration 12300: Loss = -11007.668607508178
1
Iteration 12400: Loss = -11007.437237454384
Iteration 12500: Loss = -11007.442921818476
1
Iteration 12600: Loss = -11007.436651742164
Iteration 12700: Loss = -11007.436916975448
1
Iteration 12800: Loss = -11007.439389938178
2
Iteration 12900: Loss = -11007.445194078546
3
Iteration 13000: Loss = -11007.437941720154
4
Iteration 13100: Loss = -11007.43781875015
5
Iteration 13200: Loss = -11007.559857206375
6
Iteration 13300: Loss = -11007.436483626227
Iteration 13400: Loss = -11007.437530164247
1
Iteration 13500: Loss = -11007.443787179105
2
Iteration 13600: Loss = -11007.43536842435
Iteration 13700: Loss = -11007.43243958816
Iteration 13800: Loss = -11007.442716706792
1
Iteration 13900: Loss = -11007.431758530325
Iteration 14000: Loss = -11007.43781753173
1
Iteration 14100: Loss = -11007.431663587726
Iteration 14200: Loss = -11007.494957553663
1
Iteration 14300: Loss = -11007.42643568508
Iteration 14400: Loss = -11007.426320903927
Iteration 14500: Loss = -11007.428207588722
1
Iteration 14600: Loss = -11007.427636914414
2
Iteration 14700: Loss = -11007.43902980928
3
Iteration 14800: Loss = -11007.425846823739
Iteration 14900: Loss = -11007.425935420226
Iteration 15000: Loss = -11007.427762851363
1
Iteration 15100: Loss = -11007.43353512383
2
Iteration 15200: Loss = -11007.416188826748
Iteration 15300: Loss = -11007.41990009126
1
Iteration 15400: Loss = -11007.41729469715
2
Iteration 15500: Loss = -11007.414235885919
Iteration 15600: Loss = -11007.414123897734
Iteration 15700: Loss = -11007.4145049734
1
Iteration 15800: Loss = -11007.426883409807
2
Iteration 15900: Loss = -11007.61800831377
3
Iteration 16000: Loss = -11007.423901201493
4
Iteration 16100: Loss = -11007.413878650868
Iteration 16200: Loss = -11007.41874768869
1
Iteration 16300: Loss = -11007.413553541317
Iteration 16400: Loss = -11007.413358431475
Iteration 16500: Loss = -11007.41999619322
1
Iteration 16600: Loss = -11007.412773908845
Iteration 16700: Loss = -11007.457323944574
1
Iteration 16800: Loss = -11007.414570614987
2
Iteration 16900: Loss = -11007.411929378077
Iteration 17000: Loss = -11007.411869185553
Iteration 17100: Loss = -11007.424582587344
1
Iteration 17200: Loss = -11007.412125776425
2
Iteration 17300: Loss = -11007.42349052862
3
Iteration 17400: Loss = -11007.411594053257
Iteration 17500: Loss = -11007.419433637944
1
Iteration 17600: Loss = -11007.412593924919
2
Iteration 17700: Loss = -11007.411648355494
Iteration 17800: Loss = -11007.41548824275
1
Iteration 17900: Loss = -11007.42961105561
2
Iteration 18000: Loss = -11007.411589367755
Iteration 18100: Loss = -11007.41177790143
1
Iteration 18200: Loss = -11007.438328232058
2
Iteration 18300: Loss = -11007.413938141688
3
Iteration 18400: Loss = -11007.410741204825
Iteration 18500: Loss = -11007.41089276222
1
Iteration 18600: Loss = -11007.41511566701
2
Iteration 18700: Loss = -11007.414988345874
3
Iteration 18800: Loss = -11007.414419966615
4
Iteration 18900: Loss = -11007.450438325326
5
Iteration 19000: Loss = -11007.41089662782
6
Iteration 19100: Loss = -11007.41071677442
Iteration 19200: Loss = -11007.45082343468
1
Iteration 19300: Loss = -11007.410253493164
Iteration 19400: Loss = -11007.535792981676
1
Iteration 19500: Loss = -11007.410266902092
Iteration 19600: Loss = -11007.412566273359
1
Iteration 19700: Loss = -11007.410500015345
2
Iteration 19800: Loss = -11007.411128723328
3
Iteration 19900: Loss = -11007.410264585662
pi: tensor([[0.7111, 0.2889],
        [0.1802, 0.8198]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5377, 0.4623], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2919, 0.0961],
         [0.5663, 0.1954]],

        [[0.7289, 0.0968],
         [0.6068, 0.5874]],

        [[0.6669, 0.1033],
         [0.6686, 0.6815]],

        [[0.6162, 0.1016],
         [0.5789, 0.5847]],

        [[0.6595, 0.1049],
         [0.6955, 0.6014]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9205441326485486
Global Adjusted Rand Index: 0.9446707940051707
Average Adjusted Rand Index: 0.944582246684106
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21263.8449457688
Iteration 100: Loss = -11235.33151737082
Iteration 200: Loss = -11231.470779817299
Iteration 300: Loss = -11229.339618730037
Iteration 400: Loss = -11216.434041832532
Iteration 500: Loss = -11159.73970191448
Iteration 600: Loss = -11124.272101798604
Iteration 700: Loss = -11014.265424386245
Iteration 800: Loss = -11012.104312256966
Iteration 900: Loss = -11011.524321322566
Iteration 1000: Loss = -11011.38491508023
Iteration 1100: Loss = -11011.297466680693
Iteration 1200: Loss = -11011.221128232068
Iteration 1300: Loss = -11011.168414603708
Iteration 1400: Loss = -11011.137430094666
Iteration 1500: Loss = -11011.034363879982
Iteration 1600: Loss = -11011.008278024437
Iteration 1700: Loss = -11010.989098622551
Iteration 1800: Loss = -11010.823188705228
Iteration 1900: Loss = -11010.661870136679
Iteration 2000: Loss = -11010.65071145304
Iteration 2100: Loss = -11010.643225571368
Iteration 2200: Loss = -11010.634639000433
Iteration 2300: Loss = -11009.210363539232
Iteration 2400: Loss = -11008.955609033266
Iteration 2500: Loss = -11008.951162241376
Iteration 2600: Loss = -11008.94767838219
Iteration 2700: Loss = -11008.94491058963
Iteration 2800: Loss = -11008.945770546065
1
Iteration 2900: Loss = -11008.94076961117
Iteration 3000: Loss = -11008.938947202354
Iteration 3100: Loss = -11008.938801935872
Iteration 3200: Loss = -11008.935333896088
Iteration 3300: Loss = -11008.937704782864
1
Iteration 3400: Loss = -11008.931237844094
Iteration 3500: Loss = -11008.929100033287
Iteration 3600: Loss = -11008.927546204417
Iteration 3700: Loss = -11008.926406586856
Iteration 3800: Loss = -11008.934682140936
1
Iteration 3900: Loss = -11008.924546081536
Iteration 4000: Loss = -11008.923562301516
Iteration 4100: Loss = -11008.92267328384
Iteration 4200: Loss = -11008.921428128768
Iteration 4300: Loss = -11008.919719809093
Iteration 4400: Loss = -11008.912814376781
Iteration 4500: Loss = -11008.914218465243
1
Iteration 4600: Loss = -11008.910193203552
Iteration 4700: Loss = -11008.911829460885
1
Iteration 4800: Loss = -11008.90951807337
Iteration 4900: Loss = -11008.913180258402
1
Iteration 5000: Loss = -11008.90821277991
Iteration 5100: Loss = -11008.90821966418
Iteration 5200: Loss = -11008.90568767319
Iteration 5300: Loss = -11008.902157274679
Iteration 5400: Loss = -11008.902864440828
1
Iteration 5500: Loss = -11008.901307104554
Iteration 5600: Loss = -11008.902020358135
1
Iteration 5700: Loss = -11008.900301878131
Iteration 5800: Loss = -11008.910479597049
1
Iteration 5900: Loss = -11008.88953440589
Iteration 6000: Loss = -11008.889761163266
1
Iteration 6100: Loss = -11008.889132524613
Iteration 6200: Loss = -11008.888969436706
Iteration 6300: Loss = -11008.888895410122
Iteration 6400: Loss = -11008.888766307919
Iteration 6500: Loss = -11008.889697235438
1
Iteration 6600: Loss = -11008.888525199249
Iteration 6700: Loss = -11008.89377991275
1
Iteration 6800: Loss = -11008.888350337442
Iteration 6900: Loss = -11008.888201212449
Iteration 7000: Loss = -11008.888120260888
Iteration 7100: Loss = -11008.88782057917
Iteration 7200: Loss = -11008.887620439964
Iteration 7300: Loss = -11008.88752424776
Iteration 7400: Loss = -11008.887416678106
Iteration 7500: Loss = -11008.890087553904
1
Iteration 7600: Loss = -11008.88872331805
2
Iteration 7700: Loss = -11008.887174579026
Iteration 7800: Loss = -11008.886687751998
Iteration 7900: Loss = -11008.858509380647
Iteration 8000: Loss = -11007.486134465818
Iteration 8100: Loss = -11007.517022565562
1
Iteration 8200: Loss = -11007.426786691776
Iteration 8300: Loss = -11007.422088463802
Iteration 8400: Loss = -11007.42154952738
Iteration 8500: Loss = -11007.421525119364
Iteration 8600: Loss = -11007.424767733586
1
Iteration 8700: Loss = -11007.423063070966
2
Iteration 8800: Loss = -11007.422054594146
3
Iteration 8900: Loss = -11007.421348840677
Iteration 9000: Loss = -11007.421341453395
Iteration 9100: Loss = -11007.421286698454
Iteration 9200: Loss = -11007.421292599096
Iteration 9300: Loss = -11007.421205707293
Iteration 9400: Loss = -11007.421666005275
1
Iteration 9500: Loss = -11007.42112319732
Iteration 9600: Loss = -11007.42760486613
1
Iteration 9700: Loss = -11007.421065623237
Iteration 9800: Loss = -11007.69011919762
1
Iteration 9900: Loss = -11007.411841433655
Iteration 10000: Loss = -11007.419486198987
1
Iteration 10100: Loss = -11007.413389930474
2
Iteration 10200: Loss = -11007.415847901262
3
Iteration 10300: Loss = -11007.42299198115
4
Iteration 10400: Loss = -11007.431988111746
5
Iteration 10500: Loss = -11007.444246346708
6
Iteration 10600: Loss = -11007.413999635051
7
Iteration 10700: Loss = -11007.41212119907
8
Iteration 10800: Loss = -11007.407969654236
Iteration 10900: Loss = -11007.421922964291
1
Iteration 11000: Loss = -11007.404378128263
Iteration 11100: Loss = -11007.406568178794
1
Iteration 11200: Loss = -11007.404452102486
Iteration 11300: Loss = -11007.404784613429
1
Iteration 11400: Loss = -11007.404737915562
2
Iteration 11500: Loss = -11007.413353688444
3
Iteration 11600: Loss = -11007.462438947201
4
Iteration 11700: Loss = -11007.406675583079
5
Iteration 11800: Loss = -11007.404355614508
Iteration 11900: Loss = -11007.404364616476
Iteration 12000: Loss = -11007.404923515911
1
Iteration 12100: Loss = -11007.40411649272
Iteration 12200: Loss = -11007.540447561098
1
Iteration 12300: Loss = -11007.400520368652
Iteration 12400: Loss = -11007.400463455237
Iteration 12500: Loss = -11007.404410934827
1
Iteration 12600: Loss = -11007.4088076055
2
Iteration 12700: Loss = -11007.41356098404
3
Iteration 12800: Loss = -11007.402286941302
4
Iteration 12900: Loss = -11007.39994257246
Iteration 13000: Loss = -11007.399934464285
Iteration 13100: Loss = -11007.399715794696
Iteration 13200: Loss = -11007.399685944316
Iteration 13300: Loss = -11007.409036509292
1
Iteration 13400: Loss = -11007.57249847142
2
Iteration 13500: Loss = -11007.402176382386
3
Iteration 13600: Loss = -11007.400869501833
4
Iteration 13700: Loss = -11007.415079803048
5
Iteration 13800: Loss = -11007.412197205145
6
Iteration 13900: Loss = -11007.40057421578
7
Iteration 14000: Loss = -11007.398940405003
Iteration 14100: Loss = -11007.409585894682
1
Iteration 14200: Loss = -11007.399693402942
2
Iteration 14300: Loss = -11007.400899742825
3
Iteration 14400: Loss = -11007.398870314724
Iteration 14500: Loss = -11007.399160008295
1
Iteration 14600: Loss = -11007.39965152567
2
Iteration 14700: Loss = -11007.398891159151
Iteration 14800: Loss = -11007.533272569945
1
Iteration 14900: Loss = -11007.39383529494
Iteration 15000: Loss = -11007.390938813602
Iteration 15100: Loss = -11007.396368222557
1
Iteration 15200: Loss = -11007.386124782724
Iteration 15300: Loss = -11007.386374839542
1
Iteration 15400: Loss = -11007.397335551772
2
Iteration 15500: Loss = -11007.3921421016
3
Iteration 15600: Loss = -11007.38618347789
Iteration 15700: Loss = -11007.385996586952
Iteration 15800: Loss = -11007.386663597246
1
Iteration 15900: Loss = -11007.38599429908
Iteration 16000: Loss = -11007.386066825595
Iteration 16100: Loss = -11007.386024257266
Iteration 16200: Loss = -11007.386044311143
Iteration 16300: Loss = -11007.385950186597
Iteration 16400: Loss = -11007.38604186548
Iteration 16500: Loss = -11007.41266298373
1
Iteration 16600: Loss = -11007.38074453555
Iteration 16700: Loss = -11007.384441106871
1
Iteration 16800: Loss = -11007.380785641226
Iteration 16900: Loss = -11007.381063817023
1
Iteration 17000: Loss = -11007.381666463854
2
Iteration 17100: Loss = -11007.380406775947
Iteration 17200: Loss = -11007.382002505059
1
Iteration 17300: Loss = -11007.389705508456
2
Iteration 17400: Loss = -11007.47148021911
3
Iteration 17500: Loss = -11007.383341558132
4
Iteration 17600: Loss = -11007.38156199528
5
Iteration 17700: Loss = -11007.379910345231
Iteration 17800: Loss = -11007.38214283373
1
Iteration 17900: Loss = -11007.380621261782
2
Iteration 18000: Loss = -11007.379489765828
Iteration 18100: Loss = -11007.379728876047
1
Iteration 18200: Loss = -11007.39962467091
2
Iteration 18300: Loss = -11007.387302415773
3
Iteration 18400: Loss = -11007.379606865763
4
Iteration 18500: Loss = -11007.379775187512
5
Iteration 18600: Loss = -11007.40711435529
6
Iteration 18700: Loss = -11007.472459426575
7
Iteration 18800: Loss = -11007.381464106478
8
Iteration 18900: Loss = -11007.380390574715
9
Iteration 19000: Loss = -11007.380039974323
10
Iteration 19100: Loss = -11007.38038130314
11
Iteration 19200: Loss = -11007.386812566077
12
Iteration 19300: Loss = -11007.378892009487
Iteration 19400: Loss = -11007.457020303647
1
Iteration 19500: Loss = -11007.379263730938
2
Iteration 19600: Loss = -11007.378695491641
Iteration 19700: Loss = -11007.379022513234
1
Iteration 19800: Loss = -11007.38049154525
2
Iteration 19900: Loss = -11007.387247718143
3
pi: tensor([[0.7132, 0.2868],
        [0.1808, 0.8192]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5400, 0.4600], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2911, 0.0959],
         [0.6616, 0.1959]],

        [[0.6979, 0.0965],
         [0.5493, 0.6325]],

        [[0.5646, 0.1035],
         [0.6141, 0.6357]],

        [[0.6042, 0.1014],
         [0.5477, 0.5587]],

        [[0.6951, 0.1046],
         [0.7151, 0.6087]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9205441326485486
Global Adjusted Rand Index: 0.9446707940051707
Average Adjusted Rand Index: 0.944582246684106
11031.76442142735
[0.9446707940051707, 0.9446707940051707] [0.944582246684106, 0.944582246684106] [11007.40065931038, 11007.380462889858]
-------------------------------------
This iteration is 72
True Objective function: Loss = -11042.567947650363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20668.004364900942
Iteration 100: Loss = -11266.64125477578
Iteration 200: Loss = -11265.201391467803
Iteration 300: Loss = -11260.309097724768
Iteration 400: Loss = -11252.025169417639
Iteration 500: Loss = -11193.76748641135
Iteration 600: Loss = -11032.641141281083
Iteration 700: Loss = -11027.735416065767
Iteration 800: Loss = -11023.766203957415
Iteration 900: Loss = -11016.944919599046
Iteration 1000: Loss = -11016.908053386025
Iteration 1100: Loss = -11016.869691581287
Iteration 1200: Loss = -11016.808650686931
Iteration 1300: Loss = -11016.761254826171
Iteration 1400: Loss = -11016.73421705876
Iteration 1500: Loss = -11016.726558840723
Iteration 1600: Loss = -11016.717491278343
Iteration 1700: Loss = -11016.388295937855
Iteration 1800: Loss = -11016.300660395555
Iteration 1900: Loss = -11016.296354027952
Iteration 2000: Loss = -11016.292914418374
Iteration 2100: Loss = -11016.285890053834
Iteration 2200: Loss = -11016.276119939881
Iteration 2300: Loss = -11016.271769999297
Iteration 2400: Loss = -11016.270090171369
Iteration 2500: Loss = -11016.268771321307
Iteration 2600: Loss = -11016.267878597148
Iteration 2700: Loss = -11016.266697280862
Iteration 2800: Loss = -11016.26545878645
Iteration 2900: Loss = -11016.264214208535
Iteration 3000: Loss = -11016.26304774284
Iteration 3100: Loss = -11016.262944417575
Iteration 3200: Loss = -11016.259748596902
Iteration 3300: Loss = -11016.259614642966
Iteration 3400: Loss = -11016.258932928451
Iteration 3500: Loss = -11016.260310462902
1
Iteration 3600: Loss = -11016.262594780615
2
Iteration 3700: Loss = -11016.26707044707
3
Iteration 3800: Loss = -11016.257454596014
Iteration 3900: Loss = -11016.255763930956
Iteration 4000: Loss = -11016.249485177314
Iteration 4100: Loss = -11016.248813971579
Iteration 4200: Loss = -11016.248171678866
Iteration 4300: Loss = -11016.248574569468
1
Iteration 4400: Loss = -11016.254836403532
2
Iteration 4500: Loss = -11016.247626453634
Iteration 4600: Loss = -11016.247502318256
Iteration 4700: Loss = -11016.248351966442
1
Iteration 4800: Loss = -11016.25655966548
2
Iteration 4900: Loss = -11016.246922892271
Iteration 5000: Loss = -11016.24672957882
Iteration 5100: Loss = -11016.246581246767
Iteration 5200: Loss = -11016.249053594616
1
Iteration 5300: Loss = -11016.24784305308
2
Iteration 5400: Loss = -11016.246927762053
3
Iteration 5500: Loss = -11016.24817939047
4
Iteration 5600: Loss = -11016.24620673879
Iteration 5700: Loss = -11016.246092823601
Iteration 5800: Loss = -11016.245573144713
Iteration 5900: Loss = -11016.240872331646
Iteration 6000: Loss = -11016.240896269957
Iteration 6100: Loss = -11016.240959532506
Iteration 6200: Loss = -11016.246425689329
1
Iteration 6300: Loss = -11016.241567314217
2
Iteration 6400: Loss = -11016.241060783319
3
Iteration 6500: Loss = -11016.252753226014
4
Iteration 6600: Loss = -11016.243006742407
5
Iteration 6700: Loss = -11016.206545026136
Iteration 6800: Loss = -11016.21786634914
1
Iteration 6900: Loss = -11016.20672202403
2
Iteration 7000: Loss = -11016.206103452758
Iteration 7100: Loss = -11016.206037209806
Iteration 7200: Loss = -11016.206554645629
1
Iteration 7300: Loss = -11016.205922865352
Iteration 7400: Loss = -11016.206818989262
1
Iteration 7500: Loss = -11016.202828697256
Iteration 7600: Loss = -11016.204528360708
1
Iteration 7700: Loss = -11016.202758730022
Iteration 7800: Loss = -11016.290471587512
1
Iteration 7900: Loss = -11016.202742657393
Iteration 8000: Loss = -11016.202725465919
Iteration 8100: Loss = -11016.202851935419
1
Iteration 8200: Loss = -11016.202689705577
Iteration 8300: Loss = -11016.202679368478
Iteration 8400: Loss = -11016.202951024876
1
Iteration 8500: Loss = -11016.202528265603
Iteration 8600: Loss = -11016.202311064006
Iteration 8700: Loss = -11016.202229297929
Iteration 8800: Loss = -11016.202088020906
Iteration 8900: Loss = -11016.201413091356
Iteration 9000: Loss = -11016.200046758775
Iteration 9100: Loss = -11016.200035834692
Iteration 9200: Loss = -11016.200047644672
Iteration 9300: Loss = -11016.199903892402
Iteration 9400: Loss = -11016.16386705844
Iteration 9500: Loss = -11016.163848081602
Iteration 9600: Loss = -11016.163883169651
Iteration 9700: Loss = -11016.1778073349
1
Iteration 9800: Loss = -11016.1651831566
2
Iteration 9900: Loss = -11016.165463835265
3
Iteration 10000: Loss = -11016.166072546057
4
Iteration 10100: Loss = -11016.391114251353
5
Iteration 10200: Loss = -11016.163685906266
Iteration 10300: Loss = -11016.160695577277
Iteration 10400: Loss = -11016.157501779073
Iteration 10500: Loss = -11016.159138227002
1
Iteration 10600: Loss = -11016.157322265184
Iteration 10700: Loss = -11016.157501561245
1
Iteration 10800: Loss = -11016.15737124793
Iteration 10900: Loss = -11016.158713108769
1
Iteration 11000: Loss = -11016.193156453937
2
Iteration 11100: Loss = -11016.15730559422
Iteration 11200: Loss = -11016.269565270073
1
Iteration 11300: Loss = -11016.156550984813
Iteration 11400: Loss = -11016.181712685884
1
Iteration 11500: Loss = -11016.15654748256
Iteration 11600: Loss = -11016.159491647184
1
Iteration 11700: Loss = -11016.161398981738
2
Iteration 11800: Loss = -11016.152319804978
Iteration 11900: Loss = -11016.17975405759
1
Iteration 12000: Loss = -11016.150183121052
Iteration 12100: Loss = -11016.215667200653
1
Iteration 12200: Loss = -11016.148102826783
Iteration 12300: Loss = -11016.148080003542
Iteration 12400: Loss = -11016.148148931543
Iteration 12500: Loss = -11016.148029512933
Iteration 12600: Loss = -11016.238402688236
1
Iteration 12700: Loss = -11016.14802815497
Iteration 12800: Loss = -11016.148032957797
Iteration 12900: Loss = -11016.15551299734
1
Iteration 13000: Loss = -11016.144026110724
Iteration 13100: Loss = -11016.15280086436
1
Iteration 13200: Loss = -11016.146605152433
2
Iteration 13300: Loss = -11016.141789741263
Iteration 13400: Loss = -11016.14179734483
Iteration 13500: Loss = -11016.465903474607
1
Iteration 13600: Loss = -11016.141650519416
Iteration 13700: Loss = -11016.225292456527
1
Iteration 13800: Loss = -11016.141624548349
Iteration 13900: Loss = -11016.141597136933
Iteration 14000: Loss = -11016.141313628237
Iteration 14100: Loss = -11016.140724093544
Iteration 14200: Loss = -11016.186812137166
1
Iteration 14300: Loss = -11016.139654935852
Iteration 14400: Loss = -11016.139627319611
Iteration 14500: Loss = -11016.143893412107
1
Iteration 14600: Loss = -11016.139646868403
Iteration 14700: Loss = -11016.139654359904
Iteration 14800: Loss = -11016.13978487713
1
Iteration 14900: Loss = -11016.139642557659
Iteration 15000: Loss = -11016.143022316168
1
Iteration 15100: Loss = -11016.139656466003
Iteration 15200: Loss = -11016.179909105467
1
Iteration 15300: Loss = -11016.139657797656
Iteration 15400: Loss = -11016.176442177579
1
Iteration 15500: Loss = -11016.139653667442
Iteration 15600: Loss = -11016.140065322086
1
Iteration 15700: Loss = -11016.197478310472
2
Iteration 15800: Loss = -11016.139641603892
Iteration 15900: Loss = -11016.139777511491
1
Iteration 16000: Loss = -11016.203425725353
2
Iteration 16100: Loss = -11016.139514787426
Iteration 16200: Loss = -11016.140916325134
1
Iteration 16300: Loss = -11016.139518580481
Iteration 16400: Loss = -11016.139751771745
1
Iteration 16500: Loss = -11016.440169314357
2
Iteration 16600: Loss = -11016.139492575181
Iteration 16700: Loss = -11016.203823327378
1
Iteration 16800: Loss = -11016.139508090695
Iteration 16900: Loss = -11016.170057392028
1
Iteration 17000: Loss = -11016.139489450285
Iteration 17100: Loss = -11016.15297656016
1
Iteration 17200: Loss = -11016.13927328141
Iteration 17300: Loss = -11016.166824634089
1
Iteration 17400: Loss = -11016.139260685024
Iteration 17500: Loss = -11016.139253425394
Iteration 17600: Loss = -11016.139362138687
1
Iteration 17700: Loss = -11016.13925154307
Iteration 17800: Loss = -11016.16243645017
1
Iteration 17900: Loss = -11016.139278384391
Iteration 18000: Loss = -11016.306110662093
1
Iteration 18100: Loss = -11016.139189223915
Iteration 18200: Loss = -11016.15755347672
1
Iteration 18300: Loss = -11016.139181661238
Iteration 18400: Loss = -11016.165200923064
1
Iteration 18500: Loss = -11016.138316610892
Iteration 18600: Loss = -11016.2323911812
1
Iteration 18700: Loss = -11016.139193418912
2
Iteration 18800: Loss = -11016.184844957856
3
Iteration 18900: Loss = -11016.1331688714
Iteration 19000: Loss = -11016.133382265558
1
Iteration 19100: Loss = -11016.154042995922
2
Iteration 19200: Loss = -11016.13313817248
Iteration 19300: Loss = -11016.132790223854
Iteration 19400: Loss = -11016.131624089012
Iteration 19500: Loss = -11016.132541179684
1
Iteration 19600: Loss = -11016.131594086246
Iteration 19700: Loss = -11016.135088267043
1
Iteration 19800: Loss = -11016.131578494573
Iteration 19900: Loss = -11016.146471317195
1
pi: tensor([[0.6931, 0.3069],
        [0.2598, 0.7402]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5034, 0.4966], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2943, 0.1032],
         [0.5076, 0.1957]],

        [[0.5411, 0.1008],
         [0.5250, 0.5470]],

        [[0.5539, 0.1000],
         [0.6246, 0.5560]],

        [[0.5252, 0.0952],
         [0.7263, 0.6843]],

        [[0.6490, 0.0959],
         [0.5349, 0.5764]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
Global Adjusted Rand Index: 0.8985010094129969
Average Adjusted Rand Index: 0.8979298330697919
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24440.891876697897
Iteration 100: Loss = -11054.819041492654
Iteration 200: Loss = -11016.333414800674
Iteration 300: Loss = -11016.006826347993
Iteration 400: Loss = -11015.897510536193
Iteration 500: Loss = -11015.843229465276
Iteration 600: Loss = -11015.813618600208
Iteration 700: Loss = -11015.795121849422
Iteration 800: Loss = -11015.782543410898
Iteration 900: Loss = -11015.773291678051
Iteration 1000: Loss = -11015.765813926471
Iteration 1100: Loss = -11015.758976410225
Iteration 1200: Loss = -11015.753501103256
Iteration 1300: Loss = -11015.749888524417
Iteration 1400: Loss = -11015.74700436153
Iteration 1500: Loss = -11015.744687027192
Iteration 1600: Loss = -11015.742691008478
Iteration 1700: Loss = -11015.741003435296
Iteration 1800: Loss = -11015.739622742118
Iteration 1900: Loss = -11015.738508696288
Iteration 2000: Loss = -11015.737499745883
Iteration 2100: Loss = -11015.73672174447
Iteration 2200: Loss = -11015.73601187231
Iteration 2300: Loss = -11015.735376524091
Iteration 2400: Loss = -11015.734664215846
Iteration 2500: Loss = -11015.733353265217
Iteration 2600: Loss = -11015.730902872438
Iteration 2700: Loss = -11015.730367340831
Iteration 2800: Loss = -11015.730004646728
Iteration 2900: Loss = -11015.73131321847
1
Iteration 3000: Loss = -11015.72943613129
Iteration 3100: Loss = -11015.729185766968
Iteration 3200: Loss = -11015.728998568335
Iteration 3300: Loss = -11015.728786591884
Iteration 3400: Loss = -11015.728672158517
Iteration 3500: Loss = -11015.734439071144
1
Iteration 3600: Loss = -11015.729155107116
2
Iteration 3700: Loss = -11015.73162337998
3
Iteration 3800: Loss = -11015.727964166888
Iteration 3900: Loss = -11015.727898676036
Iteration 4000: Loss = -11015.727686842883
Iteration 4100: Loss = -11015.727567511873
Iteration 4200: Loss = -11015.729878594917
1
Iteration 4300: Loss = -11015.727307536927
Iteration 4400: Loss = -11015.730771171897
1
Iteration 4500: Loss = -11015.72700878622
Iteration 4600: Loss = -11015.727669272996
1
Iteration 4700: Loss = -11015.726856128817
Iteration 4800: Loss = -11015.727656186336
1
Iteration 4900: Loss = -11015.726755016978
Iteration 5000: Loss = -11015.72793636627
1
Iteration 5100: Loss = -11015.727227225383
2
Iteration 5200: Loss = -11015.726597017667
Iteration 5300: Loss = -11015.726561095127
Iteration 5400: Loss = -11015.736657500507
1
Iteration 5500: Loss = -11015.72641915514
Iteration 5600: Loss = -11015.728065370733
1
Iteration 5700: Loss = -11015.726306933882
Iteration 5800: Loss = -11015.727680728849
1
Iteration 5900: Loss = -11015.726246438537
Iteration 6000: Loss = -11015.726865200322
1
Iteration 6100: Loss = -11015.726198785069
Iteration 6200: Loss = -11015.72621306782
Iteration 6300: Loss = -11015.726599482063
1
Iteration 6400: Loss = -11015.726148224932
Iteration 6500: Loss = -11015.726165138718
Iteration 6600: Loss = -11015.727115971904
1
Iteration 6700: Loss = -11015.726126437656
Iteration 6800: Loss = -11015.726495022274
1
Iteration 6900: Loss = -11015.727255771395
2
Iteration 7000: Loss = -11015.726041100444
Iteration 7100: Loss = -11015.735348910743
1
Iteration 7200: Loss = -11015.729825191485
2
Iteration 7300: Loss = -11015.726848426628
3
Iteration 7400: Loss = -11015.726016918501
Iteration 7500: Loss = -11015.727323574209
1
Iteration 7600: Loss = -11015.736329530419
2
Iteration 7700: Loss = -11015.72684633086
3
Iteration 7800: Loss = -11015.725944471025
Iteration 7900: Loss = -11015.725973561124
Iteration 8000: Loss = -11015.726322051134
1
Iteration 8100: Loss = -11015.725940789413
Iteration 8200: Loss = -11015.72604192754
1
Iteration 8300: Loss = -11015.727425209769
2
Iteration 8400: Loss = -11015.725880022985
Iteration 8500: Loss = -11015.72811390317
1
Iteration 8600: Loss = -11015.725792482359
Iteration 8700: Loss = -11015.725783101281
Iteration 8800: Loss = -11015.727134283825
1
Iteration 8900: Loss = -11015.732332117239
2
Iteration 9000: Loss = -11015.725986388034
3
Iteration 9100: Loss = -11015.726266288437
4
Iteration 9200: Loss = -11015.745517980076
5
Iteration 9300: Loss = -11015.726760726458
6
Iteration 9400: Loss = -11015.725750670052
Iteration 9500: Loss = -11015.72601574018
1
Iteration 9600: Loss = -11015.796870479195
2
Iteration 9700: Loss = -11015.725731426148
Iteration 9800: Loss = -11015.72913113066
1
Iteration 9900: Loss = -11015.72572902355
Iteration 10000: Loss = -11015.728897175752
1
Iteration 10100: Loss = -11015.725693625101
Iteration 10200: Loss = -11015.742299300005
1
Iteration 10300: Loss = -11015.725734250494
Iteration 10400: Loss = -11015.725702269985
Iteration 10500: Loss = -11015.726989376797
1
Iteration 10600: Loss = -11015.725684721145
Iteration 10700: Loss = -11015.725693866647
Iteration 10800: Loss = -11015.725909635714
1
Iteration 10900: Loss = -11015.725704416229
Iteration 11000: Loss = -11015.725710963801
Iteration 11100: Loss = -11015.728649363471
1
Iteration 11200: Loss = -11015.72565889023
Iteration 11300: Loss = -11015.725662722483
Iteration 11400: Loss = -11015.739069489182
1
Iteration 11500: Loss = -11015.72569171415
Iteration 11600: Loss = -11015.725684935227
Iteration 11700: Loss = -11015.77539217594
1
Iteration 11800: Loss = -11015.725626174419
Iteration 11900: Loss = -11015.72564527293
Iteration 12000: Loss = -11015.754721980871
1
Iteration 12100: Loss = -11015.725652377761
Iteration 12200: Loss = -11015.7259011798
1
Iteration 12300: Loss = -11015.72570328015
Iteration 12400: Loss = -11015.732169140081
1
Iteration 12500: Loss = -11015.725646566989
Iteration 12600: Loss = -11015.734583381245
1
Iteration 12700: Loss = -11015.72565726804
Iteration 12800: Loss = -11015.859605577414
1
Iteration 12900: Loss = -11015.725666816652
Iteration 13000: Loss = -11015.725656065137
Iteration 13100: Loss = -11015.72568064991
Iteration 13200: Loss = -11015.725659072586
Iteration 13300: Loss = -11015.73220932033
1
Iteration 13400: Loss = -11015.725645043109
Iteration 13500: Loss = -11015.725638711465
Iteration 13600: Loss = -11015.725800807899
1
Iteration 13700: Loss = -11015.725638576108
Iteration 13800: Loss = -11015.730351640896
1
Iteration 13900: Loss = -11015.725644768108
Iteration 14000: Loss = -11015.72561188153
Iteration 14100: Loss = -11015.727030793321
1
Iteration 14200: Loss = -11015.72561031747
Iteration 14300: Loss = -11015.725739187492
1
Iteration 14400: Loss = -11015.725629552282
Iteration 14500: Loss = -11015.745037125645
1
Iteration 14600: Loss = -11015.72563308705
Iteration 14700: Loss = -11015.725643818885
Iteration 14800: Loss = -11015.72570738384
Iteration 14900: Loss = -11016.097700457085
1
Iteration 15000: Loss = -11015.725659369284
Iteration 15100: Loss = -11015.780539393245
1
Iteration 15200: Loss = -11015.758747698317
2
Iteration 15300: Loss = -11015.725653818688
Iteration 15400: Loss = -11015.872690383721
1
Iteration 15500: Loss = -11015.725697356593
Iteration 15600: Loss = -11015.725723429896
Iteration 15700: Loss = -11015.750172418839
1
Iteration 15800: Loss = -11015.725617618358
Iteration 15900: Loss = -11015.763913184566
1
Iteration 16000: Loss = -11015.725631124107
Iteration 16100: Loss = -11015.725634310555
Iteration 16200: Loss = -11015.72614322042
1
Iteration 16300: Loss = -11015.725622499202
Iteration 16400: Loss = -11015.832546528276
1
Iteration 16500: Loss = -11015.725639195398
Iteration 16600: Loss = -11015.725622126201
Iteration 16700: Loss = -11015.726692465665
1
Iteration 16800: Loss = -11015.725600676366
Iteration 16900: Loss = -11015.728129985111
1
Iteration 17000: Loss = -11015.725626176012
Iteration 17100: Loss = -11015.753270438947
1
Iteration 17200: Loss = -11015.7256306245
Iteration 17300: Loss = -11015.725959556177
1
Iteration 17400: Loss = -11015.725701170468
Iteration 17500: Loss = -11015.725623227749
Iteration 17600: Loss = -11015.725624038098
Iteration 17700: Loss = -11015.725844932387
1
Iteration 17800: Loss = -11015.725621063502
Iteration 17900: Loss = -11015.727383718902
1
Iteration 18000: Loss = -11015.725624133876
Iteration 18100: Loss = -11015.727601889976
1
Iteration 18200: Loss = -11015.725691141479
Iteration 18300: Loss = -11015.729091003835
1
Iteration 18400: Loss = -11015.725623966204
Iteration 18500: Loss = -11015.72584696124
1
Iteration 18600: Loss = -11015.725620664514
Iteration 18700: Loss = -11015.727029654145
1
Iteration 18800: Loss = -11015.7256111087
Iteration 18900: Loss = -11015.74397207006
1
Iteration 19000: Loss = -11015.725625736448
Iteration 19100: Loss = -11015.728102334944
1
Iteration 19200: Loss = -11015.72563656509
Iteration 19300: Loss = -11015.727254305384
1
Iteration 19400: Loss = -11015.725651059682
Iteration 19500: Loss = -11015.7257496408
Iteration 19600: Loss = -11015.832911565742
1
Iteration 19700: Loss = -11015.72563658165
Iteration 19800: Loss = -11015.821625850544
1
Iteration 19900: Loss = -11015.725651225091
pi: tensor([[0.6896, 0.3104],
        [0.2604, 0.7396]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4993, 0.5007], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2948, 0.1035],
         [0.6186, 0.1954]],

        [[0.5773, 0.1003],
         [0.5041, 0.7166]],

        [[0.6152, 0.0999],
         [0.5465, 0.6534]],

        [[0.7048, 0.0955],
         [0.6189, 0.5999]],

        [[0.5135, 0.0960],
         [0.5663, 0.5877]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
Global Adjusted Rand Index: 0.8985010094129969
Average Adjusted Rand Index: 0.8979298330697919
11042.567947650363
[0.8985010094129969, 0.8985010094129969] [0.8979298330697919, 0.8979298330697919] [11016.131547091083, 11015.738212048263]
-------------------------------------
This iteration is 73
True Objective function: Loss = -11303.076109055504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25767.01904574227
Iteration 100: Loss = -11584.515851377555
Iteration 200: Loss = -11582.65017556643
Iteration 300: Loss = -11580.63508441044
Iteration 400: Loss = -11578.405464348574
Iteration 500: Loss = -11572.620305060524
Iteration 600: Loss = -11571.595562297254
Iteration 700: Loss = -11569.516428826657
Iteration 800: Loss = -11481.49998969712
Iteration 900: Loss = -11429.891906826719
Iteration 1000: Loss = -11424.6203713418
Iteration 1100: Loss = -11404.375077968129
Iteration 1200: Loss = -11368.015675028284
Iteration 1300: Loss = -11328.188668339224
Iteration 1400: Loss = -11321.073120950403
Iteration 1500: Loss = -11310.253049352039
Iteration 1600: Loss = -11299.714519475947
Iteration 1700: Loss = -11286.960018457576
Iteration 1800: Loss = -11286.86086341499
Iteration 1900: Loss = -11286.855167400665
Iteration 2000: Loss = -11286.850865462658
Iteration 2100: Loss = -11286.845570502543
Iteration 2200: Loss = -11277.906468251156
Iteration 2300: Loss = -11277.85807037187
Iteration 2400: Loss = -11277.852702877433
Iteration 2500: Loss = -11274.428464878174
Iteration 2600: Loss = -11274.390908369613
Iteration 2700: Loss = -11274.389555909642
Iteration 2800: Loss = -11274.40373800623
1
Iteration 2900: Loss = -11274.38797302903
Iteration 3000: Loss = -11274.387403197761
Iteration 3100: Loss = -11274.388146982299
1
Iteration 3200: Loss = -11274.386370856017
Iteration 3300: Loss = -11274.385895164649
Iteration 3400: Loss = -11274.385430661168
Iteration 3500: Loss = -11274.384965832536
Iteration 3600: Loss = -11274.384408609052
Iteration 3700: Loss = -11274.396337355069
1
Iteration 3800: Loss = -11274.382614728213
Iteration 3900: Loss = -11274.380050225436
Iteration 4000: Loss = -11274.376071793544
Iteration 4100: Loss = -11274.375280803753
Iteration 4200: Loss = -11274.37416038829
Iteration 4300: Loss = -11274.37230508702
Iteration 4400: Loss = -11274.363573922692
Iteration 4500: Loss = -11274.352186659386
Iteration 4600: Loss = -11274.352427874277
1
Iteration 4700: Loss = -11274.351797248308
Iteration 4800: Loss = -11274.351587782
Iteration 4900: Loss = -11274.351838813172
1
Iteration 5000: Loss = -11274.35108224515
Iteration 5100: Loss = -11274.350473144217
Iteration 5200: Loss = -11274.349168316952
Iteration 5300: Loss = -11274.348830373949
Iteration 5400: Loss = -11274.35920213199
1
Iteration 5500: Loss = -11274.347844950804
Iteration 5600: Loss = -11274.351247217273
1
Iteration 5700: Loss = -11274.361383273983
2
Iteration 5800: Loss = -11274.34566458454
Iteration 5900: Loss = -11274.345601511899
Iteration 6000: Loss = -11274.345839292373
1
Iteration 6100: Loss = -11274.348250740366
2
Iteration 6200: Loss = -11274.34646102613
3
Iteration 6300: Loss = -11274.345778738809
4
Iteration 6400: Loss = -11274.346406723933
5
Iteration 6500: Loss = -11274.345249416832
Iteration 6600: Loss = -11274.344819954329
Iteration 6700: Loss = -11274.344625247028
Iteration 6800: Loss = -11274.346449649964
1
Iteration 6900: Loss = -11274.34527559243
2
Iteration 7000: Loss = -11274.344124759193
Iteration 7100: Loss = -11274.34426268371
1
Iteration 7200: Loss = -11274.357836286803
2
Iteration 7300: Loss = -11274.344147077003
Iteration 7400: Loss = -11274.343985964902
Iteration 7500: Loss = -11274.343968986841
Iteration 7600: Loss = -11274.344746158244
1
Iteration 7700: Loss = -11274.344297288964
2
Iteration 7800: Loss = -11274.344182327084
3
Iteration 7900: Loss = -11274.343845811292
Iteration 8000: Loss = -11274.343771665568
Iteration 8100: Loss = -11274.340551681737
Iteration 8200: Loss = -11274.338972663527
Iteration 8300: Loss = -11274.344825117661
1
Iteration 8400: Loss = -11274.346499742884
2
Iteration 8500: Loss = -11274.338353986812
Iteration 8600: Loss = -11274.337758359556
Iteration 8700: Loss = -11274.337831954272
Iteration 8800: Loss = -11274.337859093688
Iteration 8900: Loss = -11274.337777214401
Iteration 9000: Loss = -11274.34020856917
1
Iteration 9100: Loss = -11274.338854936737
2
Iteration 9200: Loss = -11274.33877485709
3
Iteration 9300: Loss = -11274.358708312247
4
Iteration 9400: Loss = -11274.353206134438
5
Iteration 9500: Loss = -11274.342729218944
6
Iteration 9600: Loss = -11274.347883826733
7
Iteration 9700: Loss = -11274.341288554451
8
Iteration 9800: Loss = -11274.337758570104
Iteration 9900: Loss = -11274.33770482385
Iteration 10000: Loss = -11274.338438705581
1
Iteration 10100: Loss = -11274.343132695085
2
Iteration 10200: Loss = -11274.337528509115
Iteration 10300: Loss = -11274.34762738125
1
Iteration 10400: Loss = -11274.337616894902
Iteration 10500: Loss = -11274.337494410995
Iteration 10600: Loss = -11274.343908450739
1
Iteration 10700: Loss = -11274.337470671519
Iteration 10800: Loss = -11274.33752786602
Iteration 10900: Loss = -11274.3382009545
1
Iteration 11000: Loss = -11274.337416531545
Iteration 11100: Loss = -11274.337411945813
Iteration 11200: Loss = -11274.336328990565
Iteration 11300: Loss = -11274.336029923863
Iteration 11400: Loss = -11274.335854046833
Iteration 11500: Loss = -11274.335775290889
Iteration 11600: Loss = -11274.342337172935
1
Iteration 11700: Loss = -11274.34446863488
2
Iteration 11800: Loss = -11274.335786226073
Iteration 11900: Loss = -11274.33719408305
1
Iteration 12000: Loss = -11274.335987235718
2
Iteration 12100: Loss = -11274.336791443711
3
Iteration 12200: Loss = -11274.33603300209
4
Iteration 12300: Loss = -11274.33944071485
5
Iteration 12400: Loss = -11274.335313832902
Iteration 12500: Loss = -11274.33584005962
1
Iteration 12600: Loss = -11274.335307442338
Iteration 12700: Loss = -11274.335488643008
1
Iteration 12800: Loss = -11274.35278519446
2
Iteration 12900: Loss = -11274.3353286288
Iteration 13000: Loss = -11274.33575483183
1
Iteration 13100: Loss = -11274.354022947846
2
Iteration 13200: Loss = -11274.335331140892
Iteration 13300: Loss = -11274.335382935793
Iteration 13400: Loss = -11274.335571202819
1
Iteration 13500: Loss = -11274.335299107748
Iteration 13600: Loss = -11274.335385644661
Iteration 13700: Loss = -11274.33630803987
1
Iteration 13800: Loss = -11274.339176307303
2
Iteration 13900: Loss = -11274.336295848314
3
Iteration 14000: Loss = -11274.335345650068
Iteration 14100: Loss = -11274.340622879761
1
Iteration 14200: Loss = -11274.335308748057
Iteration 14300: Loss = -11274.335659513863
1
Iteration 14400: Loss = -11274.393652059249
2
Iteration 14500: Loss = -11274.335320935905
Iteration 14600: Loss = -11274.335456916044
1
Iteration 14700: Loss = -11274.346648082317
2
Iteration 14800: Loss = -11274.335306509342
Iteration 14900: Loss = -11274.335468071293
1
Iteration 15000: Loss = -11274.335438904862
2
Iteration 15100: Loss = -11274.335349553452
Iteration 15200: Loss = -11274.338106442134
1
Iteration 15300: Loss = -11274.339025073125
2
Iteration 15400: Loss = -11274.335336346563
Iteration 15500: Loss = -11274.335387034746
Iteration 15600: Loss = -11274.336873629278
1
Iteration 15700: Loss = -11274.335743871095
2
Iteration 15800: Loss = -11274.347892149099
3
Iteration 15900: Loss = -11274.336828441943
4
Iteration 16000: Loss = -11274.51580004302
5
Iteration 16100: Loss = -11274.335324643576
Iteration 16200: Loss = -11274.33587954001
1
Iteration 16300: Loss = -11274.385220981992
2
Iteration 16400: Loss = -11274.368328506509
3
Iteration 16500: Loss = -11274.345181340395
4
Iteration 16600: Loss = -11274.336589977538
5
Iteration 16700: Loss = -11274.335511815105
6
Iteration 16800: Loss = -11274.34075620121
7
Iteration 16900: Loss = -11274.355675910541
8
Iteration 17000: Loss = -11274.381056838905
9
Iteration 17100: Loss = -11274.338418225307
10
Iteration 17200: Loss = -11274.335825788366
11
Iteration 17300: Loss = -11274.33769015806
12
Iteration 17400: Loss = -11274.337012352216
13
Iteration 17500: Loss = -11274.355314346187
14
Iteration 17600: Loss = -11274.340387738172
15
Stopping early at iteration 17600 due to no improvement.
pi: tensor([[0.7610, 0.2390],
        [0.2321, 0.7679]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4735, 0.5265], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.1170],
         [0.6043, 0.3029]],

        [[0.6448, 0.0996],
         [0.5188, 0.6473]],

        [[0.6479, 0.1015],
         [0.7047, 0.5987]],

        [[0.7302, 0.0983],
         [0.6813, 0.5005]],

        [[0.5045, 0.1012],
         [0.7151, 0.6134]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 1
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
Global Adjusted Rand Index: 0.8909176834578761
Average Adjusted Rand Index: 0.8918751532525888
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22969.457244543166
Iteration 100: Loss = -11280.32117409345
Iteration 200: Loss = -11274.752181791167
Iteration 300: Loss = -11274.526455245252
Iteration 400: Loss = -11274.448976835765
Iteration 500: Loss = -11274.413130364886
Iteration 600: Loss = -11274.392877373557
Iteration 700: Loss = -11274.380181480961
Iteration 800: Loss = -11274.37171520498
Iteration 900: Loss = -11274.365636799876
Iteration 1000: Loss = -11274.361115161964
Iteration 1100: Loss = -11274.357048904352
Iteration 1200: Loss = -11274.349767334365
Iteration 1300: Loss = -11274.347601388828
Iteration 1400: Loss = -11274.345887176349
Iteration 1500: Loss = -11274.344469409376
Iteration 1600: Loss = -11274.343331257918
Iteration 1700: Loss = -11274.34230394343
Iteration 1800: Loss = -11274.34140850898
Iteration 1900: Loss = -11274.340597918226
Iteration 2000: Loss = -11274.339970262416
Iteration 2100: Loss = -11274.33947679621
Iteration 2200: Loss = -11274.33906338588
Iteration 2300: Loss = -11274.33872947588
Iteration 2400: Loss = -11274.338361566219
Iteration 2500: Loss = -11274.338078402836
Iteration 2600: Loss = -11274.337845783108
Iteration 2700: Loss = -11274.337602057067
Iteration 2800: Loss = -11274.33803710284
1
Iteration 2900: Loss = -11274.337228280005
Iteration 3000: Loss = -11274.337111658717
Iteration 3100: Loss = -11274.343289138636
1
Iteration 3200: Loss = -11274.336783296112
Iteration 3300: Loss = -11274.336939586176
1
Iteration 3400: Loss = -11274.336585876219
Iteration 3500: Loss = -11274.336461778352
Iteration 3600: Loss = -11274.336551415694
Iteration 3700: Loss = -11274.336295853494
Iteration 3800: Loss = -11274.336175847251
Iteration 3900: Loss = -11274.337166432388
1
Iteration 4000: Loss = -11274.335990186471
Iteration 4100: Loss = -11274.342262290627
1
Iteration 4200: Loss = -11274.335901067076
Iteration 4300: Loss = -11274.3359847603
Iteration 4400: Loss = -11274.357132039933
1
Iteration 4500: Loss = -11274.335686563889
Iteration 4600: Loss = -11274.335964587397
1
Iteration 4700: Loss = -11274.3356280195
Iteration 4800: Loss = -11274.336003098839
1
Iteration 4900: Loss = -11274.338359183783
2
Iteration 5000: Loss = -11274.335749344713
3
Iteration 5100: Loss = -11274.335689306377
Iteration 5200: Loss = -11274.335466122915
Iteration 5300: Loss = -11274.341304195626
1
Iteration 5400: Loss = -11274.335423698492
Iteration 5500: Loss = -11274.335388403517
Iteration 5600: Loss = -11274.337006612062
1
Iteration 5700: Loss = -11274.335315424432
Iteration 5800: Loss = -11274.337638429137
1
Iteration 5900: Loss = -11274.33532820498
Iteration 6000: Loss = -11274.335229413147
Iteration 6100: Loss = -11274.33521404777
Iteration 6200: Loss = -11274.335393539804
1
Iteration 6300: Loss = -11274.339298837778
2
Iteration 6400: Loss = -11274.335550790307
3
Iteration 6500: Loss = -11274.335130536128
Iteration 6600: Loss = -11274.33558116652
1
Iteration 6700: Loss = -11274.335434978735
2
Iteration 6800: Loss = -11274.33654298098
3
Iteration 6900: Loss = -11274.335121524566
Iteration 7000: Loss = -11274.344983135714
1
Iteration 7100: Loss = -11274.334922815417
Iteration 7200: Loss = -11274.33468750789
Iteration 7300: Loss = -11274.337597328786
1
Iteration 7400: Loss = -11274.334686524993
Iteration 7500: Loss = -11274.335328557141
1
Iteration 7600: Loss = -11274.334679438116
Iteration 7700: Loss = -11274.334968914718
1
Iteration 7800: Loss = -11274.334722968828
Iteration 7900: Loss = -11274.342123766839
1
Iteration 8000: Loss = -11274.335518690254
2
Iteration 8100: Loss = -11274.334980756279
3
Iteration 8200: Loss = -11274.334729751099
Iteration 8300: Loss = -11274.335362847913
1
Iteration 8400: Loss = -11274.356351091299
2
Iteration 8500: Loss = -11274.335857392525
3
Iteration 8600: Loss = -11274.339186160101
4
Iteration 8700: Loss = -11274.389545216092
5
Iteration 8800: Loss = -11274.334632549733
Iteration 8900: Loss = -11274.334646159308
Iteration 9000: Loss = -11274.335293416181
1
Iteration 9100: Loss = -11274.334612293496
Iteration 9200: Loss = -11274.358446806824
1
Iteration 9300: Loss = -11274.33461213995
Iteration 9400: Loss = -11274.35569163088
1
Iteration 9500: Loss = -11274.334611873972
Iteration 9600: Loss = -11274.335471332404
1
Iteration 9700: Loss = -11274.34140086098
2
Iteration 9800: Loss = -11274.335938458486
3
Iteration 9900: Loss = -11274.334803238205
4
Iteration 10000: Loss = -11274.3347291261
5
Iteration 10100: Loss = -11274.340165613403
6
Iteration 10200: Loss = -11274.343668025605
7
Iteration 10300: Loss = -11274.334727838765
8
Iteration 10400: Loss = -11274.334799731343
9
Iteration 10500: Loss = -11274.33589927514
10
Iteration 10600: Loss = -11274.50038089546
11
Iteration 10700: Loss = -11274.337344530057
12
Iteration 10800: Loss = -11274.337283955723
13
Iteration 10900: Loss = -11274.371842861403
14
Iteration 11000: Loss = -11274.334604573007
Iteration 11100: Loss = -11274.33471174604
1
Iteration 11200: Loss = -11274.496460857645
2
Iteration 11300: Loss = -11274.334611257453
Iteration 11400: Loss = -11274.334995884818
1
Iteration 11500: Loss = -11274.33501582357
2
Iteration 11600: Loss = -11274.53766179974
3
Iteration 11700: Loss = -11274.334617377594
Iteration 11800: Loss = -11274.346597790656
1
Iteration 11900: Loss = -11274.334581910289
Iteration 12000: Loss = -11274.344843153427
1
Iteration 12100: Loss = -11274.341656645625
2
Iteration 12200: Loss = -11274.334985018753
3
Iteration 12300: Loss = -11274.334636838894
Iteration 12400: Loss = -11274.336402302222
1
Iteration 12500: Loss = -11274.336194002592
2
Iteration 12600: Loss = -11274.33593387449
3
Iteration 12700: Loss = -11274.37543969753
4
Iteration 12800: Loss = -11274.33461201115
Iteration 12900: Loss = -11274.334711046477
Iteration 13000: Loss = -11274.33605053211
1
Iteration 13100: Loss = -11274.362592963973
2
Iteration 13200: Loss = -11274.334699368419
Iteration 13300: Loss = -11274.346742442536
1
Iteration 13400: Loss = -11274.336209983692
2
Iteration 13500: Loss = -11274.33520664181
3
Iteration 13600: Loss = -11274.334899156034
4
Iteration 13700: Loss = -11274.335071214966
5
Iteration 13800: Loss = -11274.380471527227
6
Iteration 13900: Loss = -11274.337711823788
7
Iteration 14000: Loss = -11274.338310515486
8
Iteration 14100: Loss = -11274.347180635647
9
Iteration 14200: Loss = -11274.392040450062
10
Iteration 14300: Loss = -11274.334708736804
Iteration 14400: Loss = -11274.335796309553
1
Iteration 14500: Loss = -11274.356228559636
2
Iteration 14600: Loss = -11274.36961840775
3
Iteration 14700: Loss = -11274.334615926255
Iteration 14800: Loss = -11274.345256943228
1
Iteration 14900: Loss = -11274.336039311842
2
Iteration 15000: Loss = -11274.334719820412
3
Iteration 15100: Loss = -11274.334656411596
Iteration 15200: Loss = -11274.423660809107
1
Iteration 15300: Loss = -11274.436770515927
2
Iteration 15400: Loss = -11274.342625080668
3
Iteration 15500: Loss = -11274.334642841568
Iteration 15600: Loss = -11274.335481347214
1
Iteration 15700: Loss = -11274.337803896547
2
Iteration 15800: Loss = -11274.334625023906
Iteration 15900: Loss = -11274.337596970816
1
Iteration 16000: Loss = -11274.369555475132
2
Iteration 16100: Loss = -11274.341048979604
3
Iteration 16200: Loss = -11274.342940963503
4
Iteration 16300: Loss = -11274.335021730221
5
Iteration 16400: Loss = -11274.338979539245
6
Iteration 16500: Loss = -11274.513890005604
7
Iteration 16600: Loss = -11274.334595452718
Iteration 16700: Loss = -11274.336563641666
1
Iteration 16800: Loss = -11274.337026100638
2
Iteration 16900: Loss = -11274.334648034954
Iteration 17000: Loss = -11274.334668274007
Iteration 17100: Loss = -11274.336411522028
1
Iteration 17200: Loss = -11274.397051381113
2
Iteration 17300: Loss = -11274.33462644496
Iteration 17400: Loss = -11274.334688470464
Iteration 17500: Loss = -11274.335816795905
1
Iteration 17600: Loss = -11274.344840295902
2
Iteration 17700: Loss = -11274.334657592055
Iteration 17800: Loss = -11274.337245678034
1
Iteration 17900: Loss = -11274.338997048562
2
Iteration 18000: Loss = -11274.334704357714
Iteration 18100: Loss = -11274.337284710586
1
Iteration 18200: Loss = -11274.382591880143
2
Iteration 18300: Loss = -11274.338761744317
3
Iteration 18400: Loss = -11274.337675737335
4
Iteration 18500: Loss = -11274.334609319309
Iteration 18600: Loss = -11274.341470049592
1
Iteration 18700: Loss = -11274.33661758941
2
Iteration 18800: Loss = -11274.336125680242
3
Iteration 18900: Loss = -11274.334612753642
Iteration 19000: Loss = -11274.334636521391
Iteration 19100: Loss = -11274.343504562317
1
Iteration 19200: Loss = -11274.341810148342
2
Iteration 19300: Loss = -11274.347928255298
3
Iteration 19400: Loss = -11274.336090545203
4
Iteration 19500: Loss = -11274.334812712219
5
Iteration 19600: Loss = -11274.416817190035
6
Iteration 19700: Loss = -11274.334585689774
Iteration 19800: Loss = -11274.335988855115
1
Iteration 19900: Loss = -11274.335358430086
2
pi: tensor([[0.7624, 0.2376],
        [0.2305, 0.7695]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4732, 0.5268], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1967, 0.1169],
         [0.6324, 0.3027]],

        [[0.5385, 0.0999],
         [0.6974, 0.6456]],

        [[0.7001, 0.1018],
         [0.5631, 0.5171]],

        [[0.6323, 0.0983],
         [0.6826, 0.6762]],

        [[0.5394, 0.1011],
         [0.5741, 0.5024]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 1
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
Global Adjusted Rand Index: 0.8909176834578761
Average Adjusted Rand Index: 0.8918751532525888
11303.076109055504
[0.8909176834578761, 0.8909176834578761] [0.8918751532525888, 0.8918751532525888] [11274.340387738172, 11274.335093969441]
-------------------------------------
This iteration is 74
True Objective function: Loss = -11262.661172612165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21573.24728247491
Iteration 100: Loss = -11556.850261233118
Iteration 200: Loss = -11552.398416549537
Iteration 300: Loss = -11549.790610329046
Iteration 400: Loss = -11546.758732891118
Iteration 500: Loss = -11490.76498589219
Iteration 600: Loss = -11262.189389806823
Iteration 700: Loss = -11223.7394068274
Iteration 800: Loss = -11221.626930806586
Iteration 900: Loss = -11221.402034029597
Iteration 1000: Loss = -11221.269424245762
Iteration 1100: Loss = -11221.186443723194
Iteration 1200: Loss = -11221.122323432484
Iteration 1300: Loss = -11220.957981678028
Iteration 1400: Loss = -11220.843553127354
Iteration 1500: Loss = -11220.814662693949
Iteration 1600: Loss = -11220.792429798486
Iteration 1700: Loss = -11220.777516916274
Iteration 1800: Loss = -11220.765971387318
Iteration 1900: Loss = -11220.756302622707
Iteration 2000: Loss = -11220.748094361878
Iteration 2100: Loss = -11220.741006901593
Iteration 2200: Loss = -11220.734868697407
Iteration 2300: Loss = -11220.729458267924
Iteration 2400: Loss = -11220.724136957995
Iteration 2500: Loss = -11220.71804089212
Iteration 2600: Loss = -11220.700779285135
Iteration 2700: Loss = -11220.650349498148
Iteration 2800: Loss = -11220.637088692523
Iteration 2900: Loss = -11219.692925109297
Iteration 3000: Loss = -11219.689647084124
Iteration 3100: Loss = -11219.687010076752
Iteration 3200: Loss = -11219.684749562011
Iteration 3300: Loss = -11219.682361787325
Iteration 3400: Loss = -11219.680112524746
Iteration 3500: Loss = -11219.676413006768
Iteration 3600: Loss = -11219.676099631744
Iteration 3700: Loss = -11219.671888353249
Iteration 3800: Loss = -11219.670936475755
Iteration 3900: Loss = -11219.669205224165
Iteration 4000: Loss = -11219.667954546401
Iteration 4100: Loss = -11219.6660951557
Iteration 4200: Loss = -11219.664943482849
Iteration 4300: Loss = -11219.663925872126
Iteration 4400: Loss = -11219.663301349785
Iteration 4500: Loss = -11219.662230308248
Iteration 4600: Loss = -11219.66106321711
Iteration 4700: Loss = -11219.63808921309
Iteration 4800: Loss = -11219.63959466614
1
Iteration 4900: Loss = -11219.634939106782
Iteration 5000: Loss = -11219.634049334938
Iteration 5100: Loss = -11219.651626002063
1
Iteration 5200: Loss = -11219.630452608586
Iteration 5300: Loss = -11219.629383745245
Iteration 5400: Loss = -11219.628422474965
Iteration 5500: Loss = -11219.628048154587
Iteration 5600: Loss = -11219.627688919669
Iteration 5700: Loss = -11219.6274498438
Iteration 5800: Loss = -11219.627135733062
Iteration 5900: Loss = -11219.63349283055
1
Iteration 6000: Loss = -11219.62655264968
Iteration 6100: Loss = -11219.631196601758
1
Iteration 6200: Loss = -11219.626066660188
Iteration 6300: Loss = -11219.62577056104
Iteration 6400: Loss = -11219.625500901371
Iteration 6500: Loss = -11219.625506908174
Iteration 6600: Loss = -11219.627196718531
1
Iteration 6700: Loss = -11219.63938684698
2
Iteration 6800: Loss = -11219.73573248121
3
Iteration 6900: Loss = -11219.624398797312
Iteration 7000: Loss = -11219.789011805264
1
Iteration 7100: Loss = -11219.62326456
Iteration 7200: Loss = -11219.622491241804
Iteration 7300: Loss = -11219.62005649538
Iteration 7400: Loss = -11219.619858881833
Iteration 7500: Loss = -11219.626815294994
1
Iteration 7600: Loss = -11219.619620416923
Iteration 7700: Loss = -11219.6195062125
Iteration 7800: Loss = -11219.62013739905
1
Iteration 7900: Loss = -11219.619200877196
Iteration 8000: Loss = -11219.618858340913
Iteration 8100: Loss = -11219.622080807281
1
Iteration 8200: Loss = -11219.618090874545
Iteration 8300: Loss = -11219.61529772927
Iteration 8400: Loss = -11219.748425777763
1
Iteration 8500: Loss = -11219.614737585447
Iteration 8600: Loss = -11219.639789722782
1
Iteration 8700: Loss = -11219.614580550653
Iteration 8800: Loss = -11219.61447527511
Iteration 8900: Loss = -11219.674319251817
1
Iteration 9000: Loss = -11219.614343670375
Iteration 9100: Loss = -11219.62165997619
1
Iteration 9200: Loss = -11219.614296258314
Iteration 9300: Loss = -11219.614133627148
Iteration 9400: Loss = -11219.615349781292
1
Iteration 9500: Loss = -11219.62451567279
2
Iteration 9600: Loss = -11219.61358454305
Iteration 9700: Loss = -11219.615588221419
1
Iteration 9800: Loss = -11219.641349808404
2
Iteration 9900: Loss = -11219.620377548223
3
Iteration 10000: Loss = -11219.61447710258
4
Iteration 10100: Loss = -11219.620778792685
5
Iteration 10200: Loss = -11219.613069550845
Iteration 10300: Loss = -11219.614393506827
1
Iteration 10400: Loss = -11219.66134773043
2
Iteration 10500: Loss = -11219.613005650306
Iteration 10600: Loss = -11219.614687778652
1
Iteration 10700: Loss = -11219.723565595263
2
Iteration 10800: Loss = -11219.610390763173
Iteration 10900: Loss = -11219.618019236766
1
Iteration 11000: Loss = -11219.614024750517
2
Iteration 11100: Loss = -11219.613111784693
3
Iteration 11200: Loss = -11219.616269069302
4
Iteration 11300: Loss = -11219.61165715168
5
Iteration 11400: Loss = -11219.626368878147
6
Iteration 11500: Loss = -11219.610823782243
7
Iteration 11600: Loss = -11219.611827890243
8
Iteration 11700: Loss = -11219.707112913937
9
Iteration 11800: Loss = -11219.610367366451
Iteration 11900: Loss = -11219.707555950157
1
Iteration 12000: Loss = -11219.61050488245
2
Iteration 12100: Loss = -11219.612088596688
3
Iteration 12200: Loss = -11219.614255638699
4
Iteration 12300: Loss = -11219.610117830987
Iteration 12400: Loss = -11219.652470475388
1
Iteration 12500: Loss = -11219.614922916593
2
Iteration 12600: Loss = -11219.612851296233
3
Iteration 12700: Loss = -11219.609394882638
Iteration 12800: Loss = -11219.614745639592
1
Iteration 12900: Loss = -11219.617429582782
2
Iteration 13000: Loss = -11219.62331422945
3
Iteration 13100: Loss = -11219.609255201873
Iteration 13200: Loss = -11219.61380109026
1
Iteration 13300: Loss = -11219.609682268605
2
Iteration 13400: Loss = -11219.613623618901
3
Iteration 13500: Loss = -11219.62391309551
4
Iteration 13600: Loss = -11219.613014491662
5
Iteration 13700: Loss = -11219.609419098479
6
Iteration 13800: Loss = -11219.633404232192
7
Iteration 13900: Loss = -11219.6102451732
8
Iteration 14000: Loss = -11219.600915893041
Iteration 14100: Loss = -11219.668238959539
1
Iteration 14200: Loss = -11219.605000188589
2
Iteration 14300: Loss = -11219.610824074876
3
Iteration 14400: Loss = -11219.601372193052
4
Iteration 14500: Loss = -11219.600876480397
Iteration 14600: Loss = -11219.75691629015
1
Iteration 14700: Loss = -11219.60882444814
2
Iteration 14800: Loss = -11219.601615806332
3
Iteration 14900: Loss = -11219.60020898347
Iteration 15000: Loss = -11219.601880878628
1
Iteration 15100: Loss = -11219.600642914216
2
Iteration 15200: Loss = -11219.60088541754
3
Iteration 15300: Loss = -11219.6042949696
4
Iteration 15400: Loss = -11219.605133598961
5
Iteration 15500: Loss = -11219.600070310638
Iteration 15600: Loss = -11219.608260458095
1
Iteration 15700: Loss = -11219.604962541616
2
Iteration 15800: Loss = -11219.603322451076
3
Iteration 15900: Loss = -11219.600366963587
4
Iteration 16000: Loss = -11219.61998742637
5
Iteration 16100: Loss = -11219.599547720189
Iteration 16200: Loss = -11219.61403574767
1
Iteration 16300: Loss = -11219.638806970057
2
Iteration 16400: Loss = -11219.650845362206
3
Iteration 16500: Loss = -11219.60139737007
4
Iteration 16600: Loss = -11219.603357693259
5
Iteration 16700: Loss = -11219.601358341582
6
Iteration 16800: Loss = -11219.59931833603
Iteration 16900: Loss = -11219.599568272235
1
Iteration 17000: Loss = -11219.600380256834
2
Iteration 17100: Loss = -11219.606475427203
3
Iteration 17200: Loss = -11219.739454609877
4
Iteration 17300: Loss = -11219.6080891205
5
Iteration 17400: Loss = -11219.599703308702
6
Iteration 17500: Loss = -11219.694828987727
7
Iteration 17600: Loss = -11219.599060346132
Iteration 17700: Loss = -11219.61125366647
1
Iteration 17800: Loss = -11219.599040459923
Iteration 17900: Loss = -11219.605987449164
1
Iteration 18000: Loss = -11219.61383102056
2
Iteration 18100: Loss = -11219.600202118441
3
Iteration 18200: Loss = -11219.601545554999
4
Iteration 18300: Loss = -11219.601014285068
5
Iteration 18400: Loss = -11219.599209473576
6
Iteration 18500: Loss = -11219.602044521522
7
Iteration 18600: Loss = -11219.607991500834
8
Iteration 18700: Loss = -11219.600346534198
9
Iteration 18800: Loss = -11219.611606854996
10
Iteration 18900: Loss = -11219.600057400865
11
Iteration 19000: Loss = -11219.601253399502
12
Iteration 19100: Loss = -11219.599995947941
13
Iteration 19200: Loss = -11219.600848350032
14
Iteration 19300: Loss = -11219.59916660444
15
Stopping early at iteration 19300 due to no improvement.
pi: tensor([[0.8069, 0.1931],
        [0.2533, 0.7467]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3752, 0.6248], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2045, 0.0857],
         [0.6325, 0.2989]],

        [[0.6894, 0.1004],
         [0.6348, 0.7184]],

        [[0.6168, 0.0972],
         [0.6720, 0.7287]],

        [[0.7259, 0.1053],
         [0.5583, 0.5410]],

        [[0.5392, 0.1084],
         [0.7033, 0.5942]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9206245835695015
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824276204858761
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9291542212855101
Average Adjusted Rand Index: 0.9290948504594871
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21886.571431713925
Iteration 100: Loss = -11557.384566015067
Iteration 200: Loss = -11554.95576376956
Iteration 300: Loss = -11549.945748327611
Iteration 400: Loss = -11547.7289219154
Iteration 500: Loss = -11536.172856113131
Iteration 600: Loss = -11404.936578541558
Iteration 700: Loss = -11237.687922194837
Iteration 800: Loss = -11222.774886321673
Iteration 900: Loss = -11222.004273720537
Iteration 1000: Loss = -11221.545375384307
Iteration 1100: Loss = -11221.321946769418
Iteration 1200: Loss = -11221.214338999307
Iteration 1300: Loss = -11221.089122447822
Iteration 1400: Loss = -11220.890645944806
Iteration 1500: Loss = -11220.846374337716
Iteration 1600: Loss = -11220.815674924195
Iteration 1700: Loss = -11220.792095698083
Iteration 1800: Loss = -11220.7728937498
Iteration 1900: Loss = -11220.756891093564
Iteration 2000: Loss = -11220.743185585974
Iteration 2100: Loss = -11220.741825578645
Iteration 2200: Loss = -11220.719765702876
Iteration 2300: Loss = -11220.706741065522
Iteration 2400: Loss = -11220.695064494763
Iteration 2500: Loss = -11220.685241281177
Iteration 2600: Loss = -11220.663472834465
Iteration 2700: Loss = -11220.651464666984
Iteration 2800: Loss = -11220.647138667307
Iteration 2900: Loss = -11220.643170810075
Iteration 3000: Loss = -11220.638681197343
Iteration 3100: Loss = -11220.635093554867
Iteration 3200: Loss = -11220.629997267557
Iteration 3300: Loss = -11219.681437288626
Iteration 3400: Loss = -11219.67659943829
Iteration 3500: Loss = -11219.67404300281
Iteration 3600: Loss = -11219.671871223258
Iteration 3700: Loss = -11219.670421289004
Iteration 3800: Loss = -11219.671661376095
1
Iteration 3900: Loss = -11219.666289959925
Iteration 4000: Loss = -11219.668082515018
1
Iteration 4100: Loss = -11219.663083547644
Iteration 4200: Loss = -11219.664983633616
1
Iteration 4300: Loss = -11219.65921290018
Iteration 4400: Loss = -11219.657520331026
Iteration 4500: Loss = -11219.670506361728
1
Iteration 4600: Loss = -11219.655809021586
Iteration 4700: Loss = -11219.65522326522
Iteration 4800: Loss = -11219.653353472891
Iteration 4900: Loss = -11219.652838258118
Iteration 5000: Loss = -11219.652363100968
Iteration 5100: Loss = -11219.651367692322
Iteration 5200: Loss = -11219.6507122635
Iteration 5300: Loss = -11219.650071507878
Iteration 5400: Loss = -11219.64989149121
Iteration 5500: Loss = -11219.654715062605
1
Iteration 5600: Loss = -11219.64846677199
Iteration 5700: Loss = -11219.647918400316
Iteration 5800: Loss = -11219.647295330558
Iteration 5900: Loss = -11219.647325230213
Iteration 6000: Loss = -11219.645598484962
Iteration 6100: Loss = -11219.64518192992
Iteration 6200: Loss = -11219.64249818621
Iteration 6300: Loss = -11219.640957582214
Iteration 6400: Loss = -11219.646063359
1
Iteration 6500: Loss = -11219.642794373109
2
Iteration 6600: Loss = -11219.641203908937
3
Iteration 6700: Loss = -11219.641323376929
4
Iteration 6800: Loss = -11219.63938041056
Iteration 6900: Loss = -11219.639207019436
Iteration 7000: Loss = -11219.645417320135
1
Iteration 7100: Loss = -11219.638752806351
Iteration 7200: Loss = -11219.638821547183
Iteration 7300: Loss = -11219.669705702338
1
Iteration 7400: Loss = -11219.638261997217
Iteration 7500: Loss = -11219.63828176985
Iteration 7600: Loss = -11219.639655244904
1
Iteration 7700: Loss = -11219.63774603559
Iteration 7800: Loss = -11219.637629433764
Iteration 7900: Loss = -11219.637481277421
Iteration 8000: Loss = -11219.63735647268
Iteration 8100: Loss = -11219.637908016392
1
Iteration 8200: Loss = -11219.637155967479
Iteration 8300: Loss = -11219.640226372241
1
Iteration 8400: Loss = -11219.636933790038
Iteration 8500: Loss = -11219.637195265328
1
Iteration 8600: Loss = -11219.636876890481
Iteration 8700: Loss = -11219.63674368318
Iteration 8800: Loss = -11219.692030859738
1
Iteration 8900: Loss = -11219.636605441865
Iteration 9000: Loss = -11219.636612094891
Iteration 9100: Loss = -11219.63662838935
Iteration 9200: Loss = -11219.636974779389
1
Iteration 9300: Loss = -11219.638264489013
2
Iteration 9400: Loss = -11219.636382939641
Iteration 9500: Loss = -11219.636045669788
Iteration 9600: Loss = -11219.636880396883
1
Iteration 9700: Loss = -11219.636861766154
2
Iteration 9800: Loss = -11219.631102698822
Iteration 9900: Loss = -11219.632085639647
1
Iteration 10000: Loss = -11219.632738017432
2
Iteration 10100: Loss = -11219.64414271287
3
Iteration 10200: Loss = -11219.632686494084
4
Iteration 10300: Loss = -11219.633409526268
5
Iteration 10400: Loss = -11219.683357678454
6
Iteration 10500: Loss = -11219.62935829139
Iteration 10600: Loss = -11219.630993582985
1
Iteration 10700: Loss = -11219.629897587094
2
Iteration 10800: Loss = -11219.628276723863
Iteration 10900: Loss = -11219.707199689628
1
Iteration 11000: Loss = -11219.634739510775
2
Iteration 11100: Loss = -11219.627745767442
Iteration 11200: Loss = -11219.63627864704
1
Iteration 11300: Loss = -11219.629255742482
2
Iteration 11400: Loss = -11219.623233606519
Iteration 11500: Loss = -11219.622488421197
Iteration 11600: Loss = -11219.622413238467
Iteration 11700: Loss = -11219.625362147734
1
Iteration 11800: Loss = -11219.62291877706
2
Iteration 11900: Loss = -11219.635450234926
3
Iteration 12000: Loss = -11219.621767175064
Iteration 12100: Loss = -11219.622366597692
1
Iteration 12200: Loss = -11219.629405903976
2
Iteration 12300: Loss = -11219.63065902982
3
Iteration 12400: Loss = -11219.621177583293
Iteration 12500: Loss = -11219.621354008732
1
Iteration 12600: Loss = -11219.62126027637
Iteration 12700: Loss = -11219.626107497388
1
Iteration 12800: Loss = -11219.60708076003
Iteration 12900: Loss = -11219.601555815849
Iteration 13000: Loss = -11219.607234790703
1
Iteration 13100: Loss = -11219.651776827668
2
Iteration 13200: Loss = -11219.601621648948
Iteration 13300: Loss = -11219.60181885181
1
Iteration 13400: Loss = -11219.605931871005
2
Iteration 13500: Loss = -11219.606934859707
3
Iteration 13600: Loss = -11219.601617438087
Iteration 13700: Loss = -11219.602964135667
1
Iteration 13800: Loss = -11219.619331669901
2
Iteration 13900: Loss = -11219.604179183072
3
Iteration 14000: Loss = -11219.610266173739
4
Iteration 14100: Loss = -11219.60134237925
Iteration 14200: Loss = -11219.60739408687
1
Iteration 14300: Loss = -11219.603699863555
2
Iteration 14400: Loss = -11219.603623934066
3
Iteration 14500: Loss = -11219.601649821649
4
Iteration 14600: Loss = -11219.601451411318
5
Iteration 14700: Loss = -11219.605238468228
6
Iteration 14800: Loss = -11219.721804825514
7
Iteration 14900: Loss = -11219.601595769762
8
Iteration 15000: Loss = -11219.610098543915
9
Iteration 15100: Loss = -11219.608840842302
10
Iteration 15200: Loss = -11219.602981119982
11
Iteration 15300: Loss = -11219.61546294828
12
Iteration 15400: Loss = -11219.60177579734
13
Iteration 15500: Loss = -11219.638494593148
14
Iteration 15600: Loss = -11219.601119903024
Iteration 15700: Loss = -11219.604919575393
1
Iteration 15800: Loss = -11219.61533449376
2
Iteration 15900: Loss = -11219.614530217077
3
Iteration 16000: Loss = -11219.656894885458
4
Iteration 16100: Loss = -11219.601185641697
Iteration 16200: Loss = -11219.611052688317
1
Iteration 16300: Loss = -11219.600887985034
Iteration 16400: Loss = -11219.620419185096
1
Iteration 16500: Loss = -11219.605757609994
2
Iteration 16600: Loss = -11219.60287679234
3
Iteration 16700: Loss = -11219.607737093986
4
Iteration 16800: Loss = -11219.602519455177
5
Iteration 16900: Loss = -11219.614423450557
6
Iteration 17000: Loss = -11219.600882584438
Iteration 17100: Loss = -11219.600135144437
Iteration 17200: Loss = -11219.602900226722
1
Iteration 17300: Loss = -11219.600149830107
Iteration 17400: Loss = -11219.600344348266
1
Iteration 17500: Loss = -11219.602926866522
2
Iteration 17600: Loss = -11219.606761508589
3
Iteration 17700: Loss = -11219.601959856533
4
Iteration 17800: Loss = -11219.600381487493
5
Iteration 17900: Loss = -11219.60015338509
Iteration 18000: Loss = -11219.603542268935
1
Iteration 18100: Loss = -11219.624924277017
2
Iteration 18200: Loss = -11219.600065237673
Iteration 18300: Loss = -11219.637053206357
1
Iteration 18400: Loss = -11219.600800263184
2
Iteration 18500: Loss = -11219.600453880683
3
Iteration 18600: Loss = -11219.600736148228
4
Iteration 18700: Loss = -11219.608822923463
5
Iteration 18800: Loss = -11219.606107177087
6
Iteration 18900: Loss = -11219.600174046886
7
Iteration 19000: Loss = -11219.600872088298
8
Iteration 19100: Loss = -11219.601697269742
9
Iteration 19200: Loss = -11219.607437098068
10
Iteration 19300: Loss = -11219.662970176036
11
Iteration 19400: Loss = -11219.60234126866
12
Iteration 19500: Loss = -11219.632697890938
13
Iteration 19600: Loss = -11219.600614466292
14
Iteration 19700: Loss = -11219.604938561026
15
Stopping early at iteration 19700 due to no improvement.
pi: tensor([[0.8062, 0.1938],
        [0.2536, 0.7464]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3753, 0.6247], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2044, 0.0857],
         [0.5743, 0.2990]],

        [[0.6994, 0.1004],
         [0.7310, 0.6676]],

        [[0.5302, 0.0966],
         [0.6308, 0.5761]],

        [[0.5574, 0.1054],
         [0.6715, 0.6825]],

        [[0.6911, 0.1084],
         [0.7127, 0.6946]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9206245835695015
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824276204858761
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9291542212855101
Average Adjusted Rand Index: 0.9290948504594871
11262.661172612165
[0.9291542212855101, 0.9291542212855101] [0.9290948504594871, 0.9290948504594871] [11219.59916660444, 11219.604938561026]
-------------------------------------
This iteration is 75
True Objective function: Loss = -11039.099361841018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23416.503631155578
Iteration 100: Loss = -11253.472873746252
Iteration 200: Loss = -11251.999922998937
Iteration 300: Loss = -11250.668435140755
Iteration 400: Loss = -11249.850138208501
Iteration 500: Loss = -11249.532462299629
Iteration 600: Loss = -11249.084156988058
Iteration 700: Loss = -11248.82530314924
Iteration 800: Loss = -11248.678013510502
Iteration 900: Loss = -11248.562823212922
Iteration 1000: Loss = -11248.477499283734
Iteration 1100: Loss = -11248.417325019467
Iteration 1200: Loss = -11248.374822237734
Iteration 1300: Loss = -11248.344115041828
Iteration 1400: Loss = -11248.321273466905
Iteration 1500: Loss = -11248.303902801059
Iteration 1600: Loss = -11248.29032599807
Iteration 1700: Loss = -11248.279465446578
Iteration 1800: Loss = -11248.270576118044
Iteration 1900: Loss = -11248.263127259988
Iteration 2000: Loss = -11248.25666538608
Iteration 2100: Loss = -11248.25050439144
Iteration 2200: Loss = -11248.224806167551
Iteration 2300: Loss = -11206.791953232381
Iteration 2400: Loss = -11206.661700904902
Iteration 2500: Loss = -11206.583522526182
Iteration 2600: Loss = -11206.572126594383
Iteration 2700: Loss = -11206.564448922883
Iteration 2800: Loss = -11206.558463837344
Iteration 2900: Loss = -11206.552821686893
Iteration 3000: Loss = -11206.543060971964
Iteration 3100: Loss = -11206.531031554981
Iteration 3200: Loss = -11206.528802815239
Iteration 3300: Loss = -11206.526923560212
Iteration 3400: Loss = -11206.525341819339
Iteration 3500: Loss = -11206.523946521747
Iteration 3600: Loss = -11206.522710354977
Iteration 3700: Loss = -11206.521639960363
Iteration 3800: Loss = -11206.520682178285
Iteration 3900: Loss = -11206.519857013449
Iteration 4000: Loss = -11206.519514481754
Iteration 4100: Loss = -11206.518466961368
Iteration 4200: Loss = -11206.520870849687
1
Iteration 4300: Loss = -11206.519164927897
2
Iteration 4400: Loss = -11206.516783746043
Iteration 4500: Loss = -11206.51629957406
Iteration 4600: Loss = -11206.515887009331
Iteration 4700: Loss = -11206.51589545916
Iteration 4800: Loss = -11206.515248624899
Iteration 4900: Loss = -11206.514786637503
Iteration 5000: Loss = -11206.515119540125
1
Iteration 5100: Loss = -11206.514187179157
Iteration 5200: Loss = -11206.513975343816
Iteration 5300: Loss = -11206.514519903354
1
Iteration 5400: Loss = -11206.51421458782
2
Iteration 5500: Loss = -11206.513248817244
Iteration 5600: Loss = -11206.513101774975
Iteration 5700: Loss = -11206.513756972578
1
Iteration 5800: Loss = -11206.512707793063
Iteration 5900: Loss = -11206.514105254228
1
Iteration 6000: Loss = -11206.51239255927
Iteration 6100: Loss = -11206.512998730374
1
Iteration 6200: Loss = -11206.512091807841
Iteration 6300: Loss = -11206.51226458417
1
Iteration 6400: Loss = -11206.511862893814
Iteration 6500: Loss = -11206.511803601214
Iteration 6600: Loss = -11206.516903181064
1
Iteration 6700: Loss = -11206.511577405827
Iteration 6800: Loss = -11206.51149170798
Iteration 6900: Loss = -11206.511458421988
Iteration 7000: Loss = -11206.511320065785
Iteration 7100: Loss = -11206.517508103454
1
Iteration 7200: Loss = -11206.511154937021
Iteration 7300: Loss = -11206.512936551375
1
Iteration 7400: Loss = -11206.511025758919
Iteration 7500: Loss = -11206.513102704
1
Iteration 7600: Loss = -11206.510883500972
Iteration 7700: Loss = -11206.521249427022
1
Iteration 7800: Loss = -11206.510795035963
Iteration 7900: Loss = -11206.510795271159
Iteration 8000: Loss = -11206.510737557463
Iteration 8100: Loss = -11206.51066665448
Iteration 8200: Loss = -11206.510690066927
Iteration 8300: Loss = -11206.510646790195
Iteration 8400: Loss = -11206.510534907142
Iteration 8500: Loss = -11206.510802014325
1
Iteration 8600: Loss = -11206.523052862845
2
Iteration 8700: Loss = -11206.51043620392
Iteration 8800: Loss = -11206.529968033923
1
Iteration 8900: Loss = -11206.510367245382
Iteration 9000: Loss = -11206.519813763513
1
Iteration 9100: Loss = -11206.510329846626
Iteration 9200: Loss = -11206.510318825518
Iteration 9300: Loss = -11206.510699151515
1
Iteration 9400: Loss = -11206.510264814087
Iteration 9500: Loss = -11206.510256456102
Iteration 9600: Loss = -11206.51097755284
1
Iteration 9700: Loss = -11206.510219358717
Iteration 9800: Loss = -11206.510189807186
Iteration 9900: Loss = -11206.511538945326
1
Iteration 10000: Loss = -11206.510159524192
Iteration 10100: Loss = -11206.510165954061
Iteration 10200: Loss = -11206.514172173906
1
Iteration 10300: Loss = -11206.510105036248
Iteration 10400: Loss = -11206.510097891827
Iteration 10500: Loss = -11206.510111477772
Iteration 10600: Loss = -11206.51008477273
Iteration 10700: Loss = -11206.510358639469
1
Iteration 10800: Loss = -11206.510174516534
Iteration 10900: Loss = -11206.510038266139
Iteration 11000: Loss = -11206.511128713975
1
Iteration 11100: Loss = -11206.510052106863
Iteration 11200: Loss = -11206.51040350959
1
Iteration 11300: Loss = -11206.602757564011
2
Iteration 11400: Loss = -11206.509987168045
Iteration 11500: Loss = -11206.512854241877
1
Iteration 11600: Loss = -11206.52193587054
2
Iteration 11700: Loss = -11206.511121010339
3
Iteration 11800: Loss = -11206.518508081324
4
Iteration 11900: Loss = -11206.631345436412
5
Iteration 12000: Loss = -11206.510023396759
Iteration 12100: Loss = -11206.510497164114
1
Iteration 12200: Loss = -11206.509971216572
Iteration 12300: Loss = -11206.510057682393
Iteration 12400: Loss = -11206.63907670365
1
Iteration 12500: Loss = -11206.509909481356
Iteration 12600: Loss = -11206.510350348703
1
Iteration 12700: Loss = -11206.509962973487
Iteration 12800: Loss = -11206.516033730348
1
Iteration 12900: Loss = -11206.509917931598
Iteration 13000: Loss = -11206.510534579727
1
Iteration 13100: Loss = -11206.509938176836
Iteration 13200: Loss = -11206.509972074844
Iteration 13300: Loss = -11206.50990124949
Iteration 13400: Loss = -11206.510269698996
1
Iteration 13500: Loss = -11206.512250166508
2
Iteration 13600: Loss = -11206.510051242498
3
Iteration 13700: Loss = -11206.509944843747
Iteration 13800: Loss = -11206.51734376511
1
Iteration 13900: Loss = -11206.527238367054
2
Iteration 14000: Loss = -11206.510224428186
3
Iteration 14100: Loss = -11206.509893534207
Iteration 14200: Loss = -11206.564156249193
1
Iteration 14300: Loss = -11206.50989268083
Iteration 14400: Loss = -11206.528511573082
1
Iteration 14500: Loss = -11206.509891201309
Iteration 14600: Loss = -11206.544349411104
1
Iteration 14700: Loss = -11206.50986997336
Iteration 14800: Loss = -11206.539858356766
1
Iteration 14900: Loss = -11206.535945731015
2
Iteration 15000: Loss = -11206.509874060122
Iteration 15100: Loss = -11206.510739363363
1
Iteration 15200: Loss = -11206.52322753042
2
Iteration 15300: Loss = -11206.512125187359
3
Iteration 15400: Loss = -11206.510005304277
4
Iteration 15500: Loss = -11206.51004204763
5
Iteration 15600: Loss = -11206.511400006246
6
Iteration 15700: Loss = -11206.547977953342
7
Iteration 15800: Loss = -11206.50987798385
Iteration 15900: Loss = -11206.510547450214
1
Iteration 16000: Loss = -11206.561367860922
2
Iteration 16100: Loss = -11206.509900225608
Iteration 16200: Loss = -11206.510072958632
1
Iteration 16300: Loss = -11206.52986515616
2
Iteration 16400: Loss = -11206.509891619391
Iteration 16500: Loss = -11206.510447600225
1
Iteration 16600: Loss = -11206.509958381714
Iteration 16700: Loss = -11206.58325435649
1
Iteration 16800: Loss = -11206.515104667666
2
Iteration 16900: Loss = -11206.510078256699
3
Iteration 17000: Loss = -11206.53942759464
4
Iteration 17100: Loss = -11206.509922373365
Iteration 17200: Loss = -11206.569925564278
1
Iteration 17300: Loss = -11206.511580206818
2
Iteration 17400: Loss = -11206.519478970906
3
Iteration 17500: Loss = -11206.510475874551
4
Iteration 17600: Loss = -11206.510613902166
5
Iteration 17700: Loss = -11206.511376119206
6
Iteration 17800: Loss = -11206.518199782353
7
Iteration 17900: Loss = -11206.509942417
Iteration 18000: Loss = -11206.510074407377
1
Iteration 18100: Loss = -11206.510410951247
2
Iteration 18200: Loss = -11206.51079657391
3
Iteration 18300: Loss = -11206.513554188386
4
Iteration 18400: Loss = -11206.531582121348
5
Iteration 18500: Loss = -11206.50987769134
Iteration 18600: Loss = -11206.510064225531
1
Iteration 18700: Loss = -11206.509908081805
Iteration 18800: Loss = -11206.509944831909
Iteration 18900: Loss = -11206.513639871504
1
Iteration 19000: Loss = -11206.509871204344
Iteration 19100: Loss = -11206.610580606435
1
Iteration 19200: Loss = -11206.509829707666
Iteration 19300: Loss = -11206.511009394177
1
Iteration 19400: Loss = -11206.680443346278
2
Iteration 19500: Loss = -11206.50986589106
Iteration 19600: Loss = -11206.509874021714
Iteration 19700: Loss = -11206.510785874323
1
Iteration 19800: Loss = -11206.509869053303
Iteration 19900: Loss = -11206.519891744018
1
pi: tensor([[9.3038e-01, 6.9620e-02],
        [1.0000e+00, 2.3411e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5852, 0.4148], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1659, 0.1015],
         [0.6399, 0.3214]],

        [[0.6509, 0.0996],
         [0.5857, 0.5746]],

        [[0.5179, 0.2169],
         [0.6265, 0.5291]],

        [[0.5533, 0.2509],
         [0.5613, 0.5832]],

        [[0.5699, 0.2066],
         [0.6410, 0.6653]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721545392564556
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: -0.011374456256342739
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.010547661869155423
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.06341521870177559
Average Adjusted Rand Index: 0.15442729322384557
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21469.052465685792
Iteration 100: Loss = -11254.333569607541
Iteration 200: Loss = -11252.68365633098
Iteration 300: Loss = -11251.891803517843
Iteration 400: Loss = -11251.150933993415
Iteration 500: Loss = -11250.336344510217
Iteration 600: Loss = -11248.465182470029
Iteration 700: Loss = -11246.174537137695
Iteration 800: Loss = -11208.78154359848
Iteration 900: Loss = -11083.295937493907
Iteration 1000: Loss = -11030.470762942954
Iteration 1100: Loss = -11012.663055582103
Iteration 1200: Loss = -11009.780624943585
Iteration 1300: Loss = -11009.64077093326
Iteration 1400: Loss = -11009.577258500305
Iteration 1500: Loss = -11009.549777025173
Iteration 1600: Loss = -11009.526900649538
Iteration 1700: Loss = -11009.475982045258
Iteration 1800: Loss = -11009.437675391862
Iteration 1900: Loss = -11009.424316662218
Iteration 2000: Loss = -11009.403800288013
Iteration 2100: Loss = -11009.375952640125
Iteration 2200: Loss = -11009.15870282386
Iteration 2300: Loss = -11009.150807928047
Iteration 2400: Loss = -11009.141835574976
Iteration 2500: Loss = -11008.684393891363
Iteration 2600: Loss = -11008.67812492316
Iteration 2700: Loss = -11008.675085949873
Iteration 2800: Loss = -11008.671877770785
Iteration 2900: Loss = -11008.668706679478
Iteration 3000: Loss = -11008.66594603605
Iteration 3100: Loss = -11008.659518405599
Iteration 3200: Loss = -11008.589613096321
Iteration 3300: Loss = -11008.547421404619
Iteration 3400: Loss = -11008.53996603076
Iteration 3500: Loss = -11008.540406461205
1
Iteration 3600: Loss = -11008.53807456459
Iteration 3700: Loss = -11008.534713459465
Iteration 3800: Loss = -11008.533590440575
Iteration 3900: Loss = -11008.531577850754
Iteration 4000: Loss = -11008.530761883527
Iteration 4100: Loss = -11008.542210467285
1
Iteration 4200: Loss = -11008.52939384035
Iteration 4300: Loss = -11008.527461511454
Iteration 4400: Loss = -11008.52179349938
Iteration 4500: Loss = -11008.521469777808
Iteration 4600: Loss = -11008.5216762429
1
Iteration 4700: Loss = -11008.52062454971
Iteration 4800: Loss = -11008.520345654328
Iteration 4900: Loss = -11008.523025559634
1
Iteration 5000: Loss = -11008.519655847187
Iteration 5100: Loss = -11008.51945777906
Iteration 5200: Loss = -11008.535385731437
1
Iteration 5300: Loss = -11008.517801683032
Iteration 5400: Loss = -11008.517221309685
Iteration 5500: Loss = -11008.516953954188
Iteration 5600: Loss = -11008.528583887572
1
Iteration 5700: Loss = -11008.516450245257
Iteration 5800: Loss = -11008.516284229401
Iteration 5900: Loss = -11008.516100242645
Iteration 6000: Loss = -11008.515907517349
Iteration 6100: Loss = -11008.544436652306
1
Iteration 6200: Loss = -11008.515524096687
Iteration 6300: Loss = -11008.524643649313
1
Iteration 6400: Loss = -11008.518729409134
2
Iteration 6500: Loss = -11008.514727821961
Iteration 6600: Loss = -11008.51422103964
Iteration 6700: Loss = -11008.512137415653
Iteration 6800: Loss = -11008.51136723479
Iteration 6900: Loss = -11008.511249956677
Iteration 7000: Loss = -11008.51109333803
Iteration 7100: Loss = -11008.51103053767
Iteration 7200: Loss = -11008.51704107079
1
Iteration 7300: Loss = -11008.509661518934
Iteration 7400: Loss = -11008.509306420612
Iteration 7500: Loss = -11008.513756143853
1
Iteration 7600: Loss = -11008.530149826225
2
Iteration 7700: Loss = -11008.469959857559
Iteration 7800: Loss = -11008.46987418883
Iteration 7900: Loss = -11008.469080615661
Iteration 8000: Loss = -11008.456291217564
Iteration 8100: Loss = -11008.45578417134
Iteration 8200: Loss = -11008.447502302155
Iteration 8300: Loss = -11008.447022526365
Iteration 8400: Loss = -11008.448363841286
1
Iteration 8500: Loss = -11008.44560956414
Iteration 8600: Loss = -11008.503771846958
1
Iteration 8700: Loss = -11008.444998375615
Iteration 8800: Loss = -11008.445327794796
1
Iteration 8900: Loss = -11008.44478408229
Iteration 9000: Loss = -11008.442806670608
Iteration 9100: Loss = -11008.442732649542
Iteration 9200: Loss = -11008.442354135803
Iteration 9300: Loss = -11008.442304441196
Iteration 9400: Loss = -11008.442334401669
Iteration 9500: Loss = -11008.442263013418
Iteration 9600: Loss = -11008.446508835961
1
Iteration 9700: Loss = -11008.442230014613
Iteration 9800: Loss = -11008.442318798076
Iteration 9900: Loss = -11008.442207058159
Iteration 10000: Loss = -11008.442152446945
Iteration 10100: Loss = -11008.442133155382
Iteration 10200: Loss = -11008.442604529917
1
Iteration 10300: Loss = -11008.442046723523
Iteration 10400: Loss = -11008.354010382145
Iteration 10500: Loss = -11008.334069156568
Iteration 10600: Loss = -11008.499730914904
1
Iteration 10700: Loss = -11008.334020656659
Iteration 10800: Loss = -11008.33396361217
Iteration 10900: Loss = -11008.330890430716
Iteration 11000: Loss = -11008.33696696169
1
Iteration 11100: Loss = -11008.33069103783
Iteration 11200: Loss = -11008.33097388001
1
Iteration 11300: Loss = -11008.330647461395
Iteration 11400: Loss = -11008.33094262622
1
Iteration 11500: Loss = -11008.417926792994
2
Iteration 11600: Loss = -11008.33055685731
Iteration 11700: Loss = -11008.312721410259
Iteration 11800: Loss = -11008.308658074751
Iteration 11900: Loss = -11008.309576639602
1
Iteration 12000: Loss = -11008.310291591266
2
Iteration 12100: Loss = -11008.308447197363
Iteration 12200: Loss = -11008.308055437836
Iteration 12300: Loss = -11008.307906464823
Iteration 12400: Loss = -11007.79032768248
Iteration 12500: Loss = -11007.790334385083
Iteration 12600: Loss = -11007.790227809592
Iteration 12700: Loss = -11007.790181955788
Iteration 12800: Loss = -11007.791213512457
1
Iteration 12900: Loss = -11007.78062849449
Iteration 13000: Loss = -11007.91409093754
1
Iteration 13100: Loss = -11007.780557582973
Iteration 13200: Loss = -11007.780546409831
Iteration 13300: Loss = -11007.781345499341
1
Iteration 13400: Loss = -11007.780536726648
Iteration 13500: Loss = -11007.891734715467
1
Iteration 13600: Loss = -11007.777887940372
Iteration 13700: Loss = -11007.781711350419
1
Iteration 13800: Loss = -11007.777850946042
Iteration 13900: Loss = -11007.778829262284
1
Iteration 14000: Loss = -11007.77788461227
Iteration 14100: Loss = -11007.778056905874
1
Iteration 14200: Loss = -11007.777875582258
Iteration 14300: Loss = -11007.777999646469
1
Iteration 14400: Loss = -11007.777836120826
Iteration 14500: Loss = -11007.778257862714
1
Iteration 14600: Loss = -11007.777837814077
Iteration 14700: Loss = -11007.786378866636
1
Iteration 14800: Loss = -11007.777863470488
Iteration 14900: Loss = -11007.77785387599
Iteration 15000: Loss = -11007.777915762787
Iteration 15100: Loss = -11007.777826113399
Iteration 15200: Loss = -11007.781097326295
1
Iteration 15300: Loss = -11007.777810075868
Iteration 15400: Loss = -11007.812309759001
1
Iteration 15500: Loss = -11007.777829413058
Iteration 15600: Loss = -11008.048257745317
1
Iteration 15700: Loss = -11007.777842602469
Iteration 15800: Loss = -11007.779575683771
1
Iteration 15900: Loss = -11007.776947188318
Iteration 16000: Loss = -11007.77682998031
Iteration 16100: Loss = -11007.777232004119
1
Iteration 16200: Loss = -11007.77681533657
Iteration 16300: Loss = -11007.783002509308
1
Iteration 16400: Loss = -11007.776820016581
Iteration 16500: Loss = -11007.839760172914
1
Iteration 16600: Loss = -11007.776858307005
Iteration 16700: Loss = -11007.776074140462
Iteration 16800: Loss = -11007.7758978159
Iteration 16900: Loss = -11007.775541110725
Iteration 17000: Loss = -11007.788544800347
1
Iteration 17100: Loss = -11007.775528038113
Iteration 17200: Loss = -11007.775527923306
Iteration 17300: Loss = -11007.772365566521
Iteration 17400: Loss = -11007.771242864937
Iteration 17500: Loss = -11007.817644570365
1
Iteration 17600: Loss = -11007.771218757372
Iteration 17700: Loss = -11007.771222554036
Iteration 17800: Loss = -11007.772215896537
1
Iteration 17900: Loss = -11007.772543412993
2
Iteration 18000: Loss = -11007.771481644873
3
Iteration 18100: Loss = -11007.771408805016
4
Iteration 18200: Loss = -11007.781145866928
5
Iteration 18300: Loss = -11007.77122743633
Iteration 18400: Loss = -11007.93346539904
1
Iteration 18500: Loss = -11007.771223541453
Iteration 18600: Loss = -11007.77119577661
Iteration 18700: Loss = -11007.771446138267
1
Iteration 18800: Loss = -11007.770881610757
Iteration 18900: Loss = -11008.134948186555
1
Iteration 19000: Loss = -11007.77085612769
Iteration 19100: Loss = -11007.77083930678
Iteration 19200: Loss = -11007.77102194291
1
Iteration 19300: Loss = -11007.881943011425
2
Iteration 19400: Loss = -11007.7708446272
Iteration 19500: Loss = -11007.854344525575
1
Iteration 19600: Loss = -11007.770839170684
Iteration 19700: Loss = -11007.77083943853
Iteration 19800: Loss = -11007.772048091476
1
Iteration 19900: Loss = -11007.770824815423
pi: tensor([[0.7619, 0.2381],
        [0.2770, 0.7230]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5662, 0.4338], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1885, 0.0995],
         [0.6727, 0.3066]],

        [[0.6339, 0.1054],
         [0.5565, 0.6858]],

        [[0.5018, 0.0972],
         [0.6881, 0.7182]],

        [[0.6099, 0.0994],
         [0.5897, 0.6975]],

        [[0.5861, 0.1092],
         [0.5295, 0.5573]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448582102964589
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080123577576726
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.8984938950332846
Average Adjusted Rand Index: 0.8987310722462825
11039.099361841018
[0.06341521870177559, 0.8984938950332846] [0.15442729322384557, 0.8987310722462825] [11206.678909194809, 11007.770861960928]
-------------------------------------
This iteration is 76
True Objective function: Loss = -11094.308448835178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23543.20509140672
Iteration 100: Loss = -11349.358561737372
Iteration 200: Loss = -11346.02565174661
Iteration 300: Loss = -11333.787762195629
Iteration 400: Loss = -11152.650952351716
Iteration 500: Loss = -11108.287491883984
Iteration 600: Loss = -11082.024332407434
Iteration 700: Loss = -11076.797881175942
Iteration 800: Loss = -11073.858909117724
Iteration 900: Loss = -11070.813081508899
Iteration 1000: Loss = -11070.754245575852
Iteration 1100: Loss = -11070.698847609116
Iteration 1200: Loss = -11070.672292433852
Iteration 1300: Loss = -11070.660128577048
Iteration 1400: Loss = -11070.652442069522
Iteration 1500: Loss = -11070.646868333599
Iteration 1600: Loss = -11070.642523959412
Iteration 1700: Loss = -11070.639046827724
Iteration 1800: Loss = -11070.636170029054
Iteration 1900: Loss = -11070.63373211554
Iteration 2000: Loss = -11070.631677274667
Iteration 2100: Loss = -11070.62991027221
Iteration 2200: Loss = -11070.628320581953
Iteration 2300: Loss = -11070.626966064629
Iteration 2400: Loss = -11070.625762376942
Iteration 2500: Loss = -11070.624674882154
Iteration 2600: Loss = -11070.623631437864
Iteration 2700: Loss = -11070.622720925043
Iteration 2800: Loss = -11070.621879437813
Iteration 2900: Loss = -11070.621004011831
Iteration 3000: Loss = -11070.63443813309
1
Iteration 3100: Loss = -11070.62140340831
2
Iteration 3200: Loss = -11070.621094519585
Iteration 3300: Loss = -11070.61797607106
Iteration 3400: Loss = -11070.617993820088
Iteration 3500: Loss = -11070.620402242572
1
Iteration 3600: Loss = -11070.615885789604
Iteration 3700: Loss = -11070.615561046565
Iteration 3800: Loss = -11070.61525434889
Iteration 3900: Loss = -11070.616879885942
1
Iteration 4000: Loss = -11070.614730415466
Iteration 4100: Loss = -11070.614951697995
1
Iteration 4200: Loss = -11070.614181866746
Iteration 4300: Loss = -11070.613999170011
Iteration 4400: Loss = -11070.613705930105
Iteration 4500: Loss = -11070.613492060329
Iteration 4600: Loss = -11070.615059247682
1
Iteration 4700: Loss = -11070.61284171506
Iteration 4800: Loss = -11070.612597057148
Iteration 4900: Loss = -11070.61236599556
Iteration 5000: Loss = -11070.612353995615
Iteration 5100: Loss = -11070.61192846912
Iteration 5200: Loss = -11070.611699365661
Iteration 5300: Loss = -11070.611477095099
Iteration 5400: Loss = -11070.611278735621
Iteration 5500: Loss = -11070.611169688931
Iteration 5600: Loss = -11070.611115795326
Iteration 5700: Loss = -11070.611023965967
Iteration 5800: Loss = -11070.611052214223
Iteration 5900: Loss = -11070.617906795982
1
Iteration 6000: Loss = -11070.610849849485
Iteration 6100: Loss = -11070.611145589117
1
Iteration 6200: Loss = -11070.610730207745
Iteration 6300: Loss = -11070.610694917112
Iteration 6400: Loss = -11070.610674011634
Iteration 6500: Loss = -11070.610606672637
Iteration 6600: Loss = -11070.610530366352
Iteration 6700: Loss = -11070.611673177555
1
Iteration 6800: Loss = -11070.610405915482
Iteration 6900: Loss = -11070.610413509567
Iteration 7000: Loss = -11070.610339790537
Iteration 7100: Loss = -11070.61031908697
Iteration 7200: Loss = -11070.610391063758
Iteration 7300: Loss = -11070.611258192585
1
Iteration 7400: Loss = -11070.610239426065
Iteration 7500: Loss = -11070.610228562831
Iteration 7600: Loss = -11070.614685193987
1
Iteration 7700: Loss = -11070.616775888142
2
Iteration 7800: Loss = -11070.612106677403
3
Iteration 7900: Loss = -11070.640112005845
4
Iteration 8000: Loss = -11070.615149413605
5
Iteration 8100: Loss = -11070.61018488958
Iteration 8200: Loss = -11070.612078663082
1
Iteration 8300: Loss = -11070.61013612246
Iteration 8400: Loss = -11070.610611481714
1
Iteration 8500: Loss = -11070.610037630957
Iteration 8600: Loss = -11070.609949572861
Iteration 8700: Loss = -11070.632150220225
1
Iteration 8800: Loss = -11070.609871872417
Iteration 8900: Loss = -11070.616350174487
1
Iteration 9000: Loss = -11070.643423854997
2
Iteration 9100: Loss = -11070.609987519489
3
Iteration 9200: Loss = -11070.612285866595
4
Iteration 9300: Loss = -11070.610906790875
5
Iteration 9400: Loss = -11070.609856147985
Iteration 9500: Loss = -11070.609955386017
Iteration 9600: Loss = -11070.616890965744
1
Iteration 9700: Loss = -11070.610693014109
2
Iteration 9800: Loss = -11070.61970339153
3
Iteration 9900: Loss = -11070.617328591516
4
Iteration 10000: Loss = -11070.647862458909
5
Iteration 10100: Loss = -11070.610097158653
6
Iteration 10200: Loss = -11070.610195401881
7
Iteration 10300: Loss = -11070.612808770338
8
Iteration 10400: Loss = -11070.61065065061
9
Iteration 10500: Loss = -11070.646242409997
10
Iteration 10600: Loss = -11070.609672528433
Iteration 10700: Loss = -11070.610082848303
1
Iteration 10800: Loss = -11070.60887218434
Iteration 10900: Loss = -11070.609196133615
1
Iteration 11000: Loss = -11070.608942676581
Iteration 11100: Loss = -11070.60892517659
Iteration 11200: Loss = -11070.608846623489
Iteration 11300: Loss = -11070.609047219928
1
Iteration 11400: Loss = -11070.611196443266
2
Iteration 11500: Loss = -11070.610761641583
3
Iteration 11600: Loss = -11070.610597871408
4
Iteration 11700: Loss = -11070.608857755413
Iteration 11800: Loss = -11070.608976676982
1
Iteration 11900: Loss = -11070.610881555485
2
Iteration 12000: Loss = -11070.584522168367
Iteration 12100: Loss = -11070.584700258762
1
Iteration 12200: Loss = -11070.585554361187
2
Iteration 12300: Loss = -11070.584685501257
3
Iteration 12400: Loss = -11070.586810621755
4
Iteration 12500: Loss = -11070.585289571045
5
Iteration 12600: Loss = -11070.591678222696
6
Iteration 12700: Loss = -11070.584709959256
7
Iteration 12800: Loss = -11070.58434755913
Iteration 12900: Loss = -11070.584269405519
Iteration 13000: Loss = -11070.585056212618
1
Iteration 13100: Loss = -11070.584731139994
2
Iteration 13200: Loss = -11070.58639813802
3
Iteration 13300: Loss = -11070.608736532726
4
Iteration 13400: Loss = -11070.589329158525
5
Iteration 13500: Loss = -11070.63079287037
6
Iteration 13600: Loss = -11070.584865393806
7
Iteration 13700: Loss = -11070.584421392761
8
Iteration 13800: Loss = -11070.58752982406
9
Iteration 13900: Loss = -11070.585068121052
10
Iteration 14000: Loss = -11070.583648310696
Iteration 14100: Loss = -11070.583962229486
1
Iteration 14200: Loss = -11070.591161605355
2
Iteration 14300: Loss = -11070.583656075867
Iteration 14400: Loss = -11070.584501318719
1
Iteration 14500: Loss = -11070.584457984804
2
Iteration 14600: Loss = -11070.583778167911
3
Iteration 14700: Loss = -11070.583600757358
Iteration 14800: Loss = -11070.584059657258
1
Iteration 14900: Loss = -11070.587239930488
2
Iteration 15000: Loss = -11070.614589372613
3
Iteration 15100: Loss = -11070.583590091093
Iteration 15200: Loss = -11070.58382542991
1
Iteration 15300: Loss = -11070.583648749132
Iteration 15400: Loss = -11070.584563392575
1
Iteration 15500: Loss = -11070.584721056939
2
Iteration 15600: Loss = -11070.585233083895
3
Iteration 15700: Loss = -11070.583645623718
Iteration 15800: Loss = -11070.58410000366
1
Iteration 15900: Loss = -11070.58475522144
2
Iteration 16000: Loss = -11070.585167456293
3
Iteration 16100: Loss = -11070.584633315353
4
Iteration 16200: Loss = -11070.583951436805
5
Iteration 16300: Loss = -11070.595528215326
6
Iteration 16400: Loss = -11070.583654342305
Iteration 16500: Loss = -11070.5836180615
Iteration 16600: Loss = -11070.583826290243
1
Iteration 16700: Loss = -11070.597815988973
2
Iteration 16800: Loss = -11070.697363835507
3
Iteration 16900: Loss = -11070.585408433992
4
Iteration 17000: Loss = -11070.584335746156
5
Iteration 17100: Loss = -11070.58496733307
6
Iteration 17200: Loss = -11070.61101755415
7
Iteration 17300: Loss = -11070.587375683817
8
Iteration 17400: Loss = -11070.583560987418
Iteration 17500: Loss = -11070.586458091891
1
Iteration 17600: Loss = -11070.584398074414
2
Iteration 17700: Loss = -11070.584535426366
3
Iteration 17800: Loss = -11070.588180777795
4
Iteration 17900: Loss = -11070.650746701067
5
Iteration 18000: Loss = -11070.719931173835
6
Iteration 18100: Loss = -11070.611476973609
7
Iteration 18200: Loss = -11070.586360963334
8
Iteration 18300: Loss = -11070.583576920892
Iteration 18400: Loss = -11070.584300252154
1
Iteration 18500: Loss = -11070.586903848765
2
Iteration 18600: Loss = -11070.587446264824
3
Iteration 18700: Loss = -11070.584079644761
4
Iteration 18800: Loss = -11070.59404832243
5
Iteration 18900: Loss = -11070.588143269433
6
Iteration 19000: Loss = -11070.584189696086
7
Iteration 19100: Loss = -11070.584677915072
8
Iteration 19200: Loss = -11070.631745711073
9
Iteration 19300: Loss = -11070.583750731104
10
Iteration 19400: Loss = -11070.585717965247
11
Iteration 19500: Loss = -11070.588321498828
12
Iteration 19600: Loss = -11070.583556756983
Iteration 19700: Loss = -11070.583781409252
1
Iteration 19800: Loss = -11070.59917985571
2
Iteration 19900: Loss = -11070.583532173705
pi: tensor([[0.7522, 0.2478],
        [0.2423, 0.7577]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4756, 0.5244], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3011, 0.1029],
         [0.5804, 0.1933]],

        [[0.5942, 0.1139],
         [0.5980, 0.6055]],

        [[0.5410, 0.0936],
         [0.7003, 0.5287]],

        [[0.5633, 0.0965],
         [0.6103, 0.6945]],

        [[0.7156, 0.0983],
         [0.6186, 0.5161]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.906116223753757
Average Adjusted Rand Index: 0.9067437231673011
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22764.688281233404
Iteration 100: Loss = -11349.508257159616
Iteration 200: Loss = -11344.222556626564
Iteration 300: Loss = -11341.325307401026
Iteration 400: Loss = -11338.371038071917
Iteration 500: Loss = -11261.655938970867
Iteration 600: Loss = -11095.594525770832
Iteration 700: Loss = -11085.407903565108
Iteration 800: Loss = -11083.210602926963
Iteration 900: Loss = -11078.48086783169
Iteration 1000: Loss = -11072.955733157103
Iteration 1100: Loss = -11072.91892710203
Iteration 1200: Loss = -11072.896075127857
Iteration 1300: Loss = -11072.87346822509
Iteration 1400: Loss = -11072.825689314253
Iteration 1500: Loss = -11072.814772876602
Iteration 1600: Loss = -11072.774626670014
Iteration 1700: Loss = -11072.256267658446
Iteration 1800: Loss = -11072.217231470499
Iteration 1900: Loss = -11072.210632553846
Iteration 2000: Loss = -11072.203071782045
Iteration 2100: Loss = -11072.195628892277
Iteration 2200: Loss = -11072.180410039173
Iteration 2300: Loss = -11072.166522800468
Iteration 2400: Loss = -11072.162544170833
Iteration 2500: Loss = -11072.159219154382
Iteration 2600: Loss = -11072.094020847655
Iteration 2700: Loss = -11072.09144537996
Iteration 2800: Loss = -11072.063001136232
Iteration 2900: Loss = -11071.928391017624
Iteration 3000: Loss = -11071.84701173046
Iteration 3100: Loss = -11071.811326471638
Iteration 3200: Loss = -11071.808490286929
Iteration 3300: Loss = -11071.812502763889
1
Iteration 3400: Loss = -11071.806914581795
Iteration 3500: Loss = -11071.805804667003
Iteration 3600: Loss = -11071.805066927334
Iteration 3700: Loss = -11071.803872382781
Iteration 3800: Loss = -11071.771893970457
Iteration 3900: Loss = -11071.636827632045
Iteration 4000: Loss = -11071.635531717408
Iteration 4100: Loss = -11070.660970355293
Iteration 4200: Loss = -11070.634059676502
Iteration 4300: Loss = -11070.633098169312
Iteration 4400: Loss = -11070.632570909012
Iteration 4500: Loss = -11070.632075038237
Iteration 4600: Loss = -11070.631048091553
Iteration 4700: Loss = -11070.630237696398
Iteration 4800: Loss = -11070.630527895473
1
Iteration 4900: Loss = -11070.63094567605
2
Iteration 5000: Loss = -11070.629346695816
Iteration 5100: Loss = -11070.629388832629
Iteration 5200: Loss = -11070.628909854702
Iteration 5300: Loss = -11070.630757258717
1
Iteration 5400: Loss = -11070.628425687772
Iteration 5500: Loss = -11070.628699053004
1
Iteration 5600: Loss = -11070.627958777299
Iteration 5700: Loss = -11070.62783790133
Iteration 5800: Loss = -11070.627482068663
Iteration 5900: Loss = -11070.627168409173
Iteration 6000: Loss = -11070.626834923243
Iteration 6100: Loss = -11070.629753008387
1
Iteration 6200: Loss = -11070.626073214915
Iteration 6300: Loss = -11070.625875775395
Iteration 6400: Loss = -11070.629487070228
1
Iteration 6500: Loss = -11070.638672075565
2
Iteration 6600: Loss = -11070.625688044305
Iteration 6700: Loss = -11070.625841740042
1
Iteration 6800: Loss = -11070.625633505317
Iteration 6900: Loss = -11070.62548265261
Iteration 7000: Loss = -11070.626097427865
1
Iteration 7100: Loss = -11070.64321612751
2
Iteration 7200: Loss = -11070.627961432736
3
Iteration 7300: Loss = -11070.619894485224
Iteration 7400: Loss = -11070.618779981549
Iteration 7500: Loss = -11070.61968447311
1
Iteration 7600: Loss = -11070.614668732369
Iteration 7700: Loss = -11070.616396082323
1
Iteration 7800: Loss = -11070.614034336611
Iteration 7900: Loss = -11070.621611506413
1
Iteration 8000: Loss = -11070.642087084138
2
Iteration 8100: Loss = -11070.616843312004
3
Iteration 8200: Loss = -11070.622279043313
4
Iteration 8300: Loss = -11070.613381822372
Iteration 8400: Loss = -11070.613044599397
Iteration 8500: Loss = -11070.73191335259
1
Iteration 8600: Loss = -11070.612858397653
Iteration 8700: Loss = -11070.682701239164
1
Iteration 8800: Loss = -11070.612799525677
Iteration 8900: Loss = -11070.612897566532
Iteration 9000: Loss = -11070.619423883776
1
Iteration 9100: Loss = -11070.614306334872
2
Iteration 9200: Loss = -11070.612841131962
Iteration 9300: Loss = -11070.614869995083
1
Iteration 9400: Loss = -11070.618551263895
2
Iteration 9500: Loss = -11070.667025869316
3
Iteration 9600: Loss = -11070.613778129165
4
Iteration 9700: Loss = -11070.613034174321
5
Iteration 9800: Loss = -11070.617457604012
6
Iteration 9900: Loss = -11070.644716151002
7
Iteration 10000: Loss = -11070.624684134189
8
Iteration 10100: Loss = -11070.615231496926
9
Iteration 10200: Loss = -11070.617429341835
10
Iteration 10300: Loss = -11070.617340633195
11
Iteration 10400: Loss = -11070.779712534846
12
Iteration 10500: Loss = -11070.612563223598
Iteration 10600: Loss = -11070.613326236427
1
Iteration 10700: Loss = -11070.613778263862
2
Iteration 10800: Loss = -11070.612685019767
3
Iteration 10900: Loss = -11070.614117430208
4
Iteration 11000: Loss = -11070.616335590688
5
Iteration 11100: Loss = -11070.628242474622
6
Iteration 11200: Loss = -11070.615802275726
7
Iteration 11300: Loss = -11070.612909326892
8
Iteration 11400: Loss = -11070.613329642498
9
Iteration 11500: Loss = -11070.625035622561
10
Iteration 11600: Loss = -11070.616283447398
11
Iteration 11700: Loss = -11070.612132491058
Iteration 11800: Loss = -11070.631152942511
1
Iteration 11900: Loss = -11070.612458490888
2
Iteration 12000: Loss = -11070.612409728867
3
Iteration 12100: Loss = -11070.612353520288
4
Iteration 12200: Loss = -11070.615644222587
5
Iteration 12300: Loss = -11070.626303563879
6
Iteration 12400: Loss = -11070.612343671062
7
Iteration 12500: Loss = -11070.620805211747
8
Iteration 12600: Loss = -11070.629374614238
9
Iteration 12700: Loss = -11070.614071455717
10
Iteration 12800: Loss = -11070.61095417403
Iteration 12900: Loss = -11070.610855948591
Iteration 13000: Loss = -11070.619528979141
1
Iteration 13100: Loss = -11070.773770172127
2
Iteration 13200: Loss = -11070.611502642601
3
Iteration 13300: Loss = -11070.614387384589
4
Iteration 13400: Loss = -11070.627440794102
5
Iteration 13500: Loss = -11070.616877846203
6
Iteration 13600: Loss = -11070.611052426748
7
Iteration 13700: Loss = -11070.61085420816
Iteration 13800: Loss = -11070.611344836483
1
Iteration 13900: Loss = -11070.610720181707
Iteration 14000: Loss = -11070.610768660457
Iteration 14100: Loss = -11070.61064351281
Iteration 14200: Loss = -11070.612764908872
1
Iteration 14300: Loss = -11070.650083320796
2
Iteration 14400: Loss = -11070.675335118543
3
Iteration 14500: Loss = -11070.612949930126
4
Iteration 14600: Loss = -11070.610812914707
5
Iteration 14700: Loss = -11070.62540203134
6
Iteration 14800: Loss = -11070.61561966917
7
Iteration 14900: Loss = -11070.612733506106
8
Iteration 15000: Loss = -11070.60995698247
Iteration 15100: Loss = -11070.731677506963
1
Iteration 15200: Loss = -11070.60865753741
Iteration 15300: Loss = -11070.60894811439
1
Iteration 15400: Loss = -11070.623884245753
2
Iteration 15500: Loss = -11070.8305639064
3
Iteration 15600: Loss = -11070.608663000583
Iteration 15700: Loss = -11070.614501006405
1
Iteration 15800: Loss = -11070.608646388793
Iteration 15900: Loss = -11070.608757500497
1
Iteration 16000: Loss = -11070.609121349016
2
Iteration 16100: Loss = -11070.609913603987
3
Iteration 16200: Loss = -11070.61422320996
4
Iteration 16300: Loss = -11070.639274324754
5
Iteration 16400: Loss = -11070.608664291549
Iteration 16500: Loss = -11070.595450162204
Iteration 16600: Loss = -11070.608103176914
1
Iteration 16700: Loss = -11070.584098094492
Iteration 16800: Loss = -11070.584316720951
1
Iteration 16900: Loss = -11070.588713242025
2
Iteration 17000: Loss = -11070.585967619772
3
Iteration 17100: Loss = -11070.613274873107
4
Iteration 17200: Loss = -11070.586457450836
5
Iteration 17300: Loss = -11070.587030624667
6
Iteration 17400: Loss = -11070.602395191037
7
Iteration 17500: Loss = -11070.632061827086
8
Iteration 17600: Loss = -11070.596288428615
9
Iteration 17700: Loss = -11070.584148201517
Iteration 17800: Loss = -11070.585075228864
1
Iteration 17900: Loss = -11070.58433580719
2
Iteration 18000: Loss = -11070.596046658588
3
Iteration 18100: Loss = -11070.586220932912
4
Iteration 18200: Loss = -11070.584174559088
Iteration 18300: Loss = -11070.584077686362
Iteration 18400: Loss = -11070.588147438126
1
Iteration 18500: Loss = -11070.598731626527
2
Iteration 18600: Loss = -11070.714040452343
3
Iteration 18700: Loss = -11070.58510024647
4
Iteration 18800: Loss = -11070.584106742237
Iteration 18900: Loss = -11070.59020385361
1
Iteration 19000: Loss = -11070.676459224173
2
Iteration 19100: Loss = -11070.646276787033
3
Iteration 19200: Loss = -11070.614520714998
4
Iteration 19300: Loss = -11070.584741593717
5
Iteration 19400: Loss = -11070.589132752199
6
Iteration 19500: Loss = -11070.584603126594
7
Iteration 19600: Loss = -11070.583860953491
Iteration 19700: Loss = -11070.58410465889
1
Iteration 19800: Loss = -11070.58640485994
2
Iteration 19900: Loss = -11070.590284072121
3
pi: tensor([[0.7523, 0.2477],
        [0.2434, 0.7566]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4759, 0.5241], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3008, 0.1022],
         [0.5597, 0.1936]],

        [[0.5794, 0.1134],
         [0.7027, 0.6976]],

        [[0.6684, 0.0931],
         [0.5620, 0.6468]],

        [[0.7261, 0.0965],
         [0.6676, 0.6840]],

        [[0.6752, 0.0983],
         [0.6533, 0.7079]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.906116223753757
Average Adjusted Rand Index: 0.9067437231673011
11094.308448835178
[0.906116223753757, 0.906116223753757] [0.9067437231673011, 0.9067437231673011] [11070.584070280473, 11070.614336874345]
-------------------------------------
This iteration is 77
True Objective function: Loss = -11233.008623623846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19862.970924657064
Iteration 100: Loss = -11558.290222664256
Iteration 200: Loss = -11556.404357436384
Iteration 300: Loss = -11549.483908053691
Iteration 400: Loss = -11411.343700302743
Iteration 500: Loss = -11274.922857977794
Iteration 600: Loss = -11235.75374297037
Iteration 700: Loss = -11214.945656150356
Iteration 800: Loss = -11211.260283839334
Iteration 900: Loss = -11211.151421999417
Iteration 1000: Loss = -11210.954353411395
Iteration 1100: Loss = -11210.921115005749
Iteration 1200: Loss = -11210.904376973875
Iteration 1300: Loss = -11210.892486915187
Iteration 1400: Loss = -11210.88334860519
Iteration 1500: Loss = -11210.876021115018
Iteration 1600: Loss = -11210.870042322216
Iteration 1700: Loss = -11210.86464177591
Iteration 1800: Loss = -11210.858198776297
Iteration 1900: Loss = -11210.837089186887
Iteration 2000: Loss = -11210.823201033741
Iteration 2100: Loss = -11210.818297206588
Iteration 2200: Loss = -11210.78050806062
Iteration 2300: Loss = -11210.764495919966
Iteration 2400: Loss = -11210.761681435946
Iteration 2500: Loss = -11210.744117216434
Iteration 2600: Loss = -11210.74281431683
Iteration 2700: Loss = -11210.740581033155
Iteration 2800: Loss = -11210.728104581412
Iteration 2900: Loss = -11210.664203470444
Iteration 3000: Loss = -11210.646291704541
Iteration 3100: Loss = -11210.63989441755
Iteration 3200: Loss = -11210.638481396241
Iteration 3300: Loss = -11210.637890945194
Iteration 3400: Loss = -11210.637243115705
Iteration 3500: Loss = -11210.634671700305
Iteration 3600: Loss = -11210.62980496153
Iteration 3700: Loss = -11210.640836130575
1
Iteration 3800: Loss = -11210.628914504625
Iteration 3900: Loss = -11210.631501185448
1
Iteration 4000: Loss = -11210.627825877653
Iteration 4100: Loss = -11210.627801556802
Iteration 4200: Loss = -11210.622802348787
Iteration 4300: Loss = -11210.618534277686
Iteration 4400: Loss = -11210.618354622238
Iteration 4500: Loss = -11210.62444546194
1
Iteration 4600: Loss = -11210.617594225447
Iteration 4700: Loss = -11210.617423617412
Iteration 4800: Loss = -11210.618641569528
1
Iteration 4900: Loss = -11210.635868362371
2
Iteration 5000: Loss = -11210.61719582856
Iteration 5100: Loss = -11210.636777417903
1
Iteration 5200: Loss = -11210.616320131223
Iteration 5300: Loss = -11210.616081140817
Iteration 5400: Loss = -11210.616184522458
1
Iteration 5500: Loss = -11210.61569511094
Iteration 5600: Loss = -11210.616055397615
1
Iteration 5700: Loss = -11210.617153747691
2
Iteration 5800: Loss = -11210.615116293737
Iteration 5900: Loss = -11210.615081329726
Iteration 6000: Loss = -11210.616172027647
1
Iteration 6100: Loss = -11210.620052945997
2
Iteration 6200: Loss = -11210.619225403454
3
Iteration 6300: Loss = -11210.625189915248
4
Iteration 6400: Loss = -11210.669252102172
5
Iteration 6500: Loss = -11210.616514839925
6
Iteration 6600: Loss = -11210.614618471642
Iteration 6700: Loss = -11210.614627436202
Iteration 6800: Loss = -11210.614569210506
Iteration 6900: Loss = -11210.614703079911
1
Iteration 7000: Loss = -11210.614470855075
Iteration 7100: Loss = -11210.614967170446
1
Iteration 7200: Loss = -11210.614435761318
Iteration 7300: Loss = -11210.618382558068
1
Iteration 7400: Loss = -11210.614386420328
Iteration 7500: Loss = -11210.885200980985
1
Iteration 7600: Loss = -11210.614289325064
Iteration 7700: Loss = -11210.614238366945
Iteration 7800: Loss = -11210.62733285954
1
Iteration 7900: Loss = -11210.613747778383
Iteration 8000: Loss = -11210.61376157397
Iteration 8100: Loss = -11210.638238674288
1
Iteration 8200: Loss = -11210.613623823689
Iteration 8300: Loss = -11210.613645944768
Iteration 8400: Loss = -11210.665537940156
1
Iteration 8500: Loss = -11210.613566004482
Iteration 8600: Loss = -11210.613561540762
Iteration 8700: Loss = -11210.653267046311
1
Iteration 8800: Loss = -11210.61352571607
Iteration 8900: Loss = -11210.613507482147
Iteration 9000: Loss = -11210.616907945405
1
Iteration 9100: Loss = -11210.613476714274
Iteration 9200: Loss = -11210.613500052254
Iteration 9300: Loss = -11210.61355345478
Iteration 9400: Loss = -11210.613454517748
Iteration 9500: Loss = -11210.626394565505
1
Iteration 9600: Loss = -11210.613464905391
Iteration 9700: Loss = -11210.613501817928
Iteration 9800: Loss = -11210.62322933768
1
Iteration 9900: Loss = -11210.613446067675
Iteration 10000: Loss = -11210.613499333009
Iteration 10100: Loss = -11210.618115541118
1
Iteration 10200: Loss = -11210.626791470788
2
Iteration 10300: Loss = -11210.67515203057
3
Iteration 10400: Loss = -11210.638101442295
4
Iteration 10500: Loss = -11210.613450081812
Iteration 10600: Loss = -11210.613536983396
Iteration 10700: Loss = -11210.613409058136
Iteration 10800: Loss = -11210.613772788725
1
Iteration 10900: Loss = -11210.613416024149
Iteration 11000: Loss = -11210.618059222546
1
Iteration 11100: Loss = -11210.61341171009
Iteration 11200: Loss = -11210.613412522487
Iteration 11300: Loss = -11210.614494255864
1
Iteration 11400: Loss = -11210.613398521991
Iteration 11500: Loss = -11210.613381966992
Iteration 11600: Loss = -11210.673422284723
1
Iteration 11700: Loss = -11210.613385828587
Iteration 11800: Loss = -11210.613386569683
Iteration 11900: Loss = -11210.614360005395
1
Iteration 12000: Loss = -11210.76151442958
2
Iteration 12100: Loss = -11210.614168166832
3
Iteration 12200: Loss = -11210.61404819667
4
Iteration 12300: Loss = -11210.613063775989
Iteration 12400: Loss = -11210.613640675181
1
Iteration 12500: Loss = -11210.628816428492
2
Iteration 12600: Loss = -11210.613035152905
Iteration 12700: Loss = -11210.663010536926
1
Iteration 12800: Loss = -11210.613089424602
Iteration 12900: Loss = -11210.613041384291
Iteration 13000: Loss = -11210.635798271654
1
Iteration 13100: Loss = -11210.613035553895
Iteration 13200: Loss = -11210.613066353304
Iteration 13300: Loss = -11210.615271791881
1
Iteration 13400: Loss = -11210.613044273656
Iteration 13500: Loss = -11210.676971951418
1
Iteration 13600: Loss = -11210.613330768845
2
Iteration 13700: Loss = -11210.614061313498
3
Iteration 13800: Loss = -11210.613116385888
Iteration 13900: Loss = -11210.622311348166
1
Iteration 14000: Loss = -11210.642959830802
2
Iteration 14100: Loss = -11210.613100944682
Iteration 14200: Loss = -11210.613260211665
1
Iteration 14300: Loss = -11210.613565382466
2
Iteration 14400: Loss = -11210.612618533616
Iteration 14500: Loss = -11210.619635172186
1
Iteration 14600: Loss = -11210.643336319215
2
Iteration 14700: Loss = -11210.612568342975
Iteration 14800: Loss = -11210.620563747105
1
Iteration 14900: Loss = -11210.61257018114
Iteration 15000: Loss = -11210.617135127872
1
Iteration 15100: Loss = -11210.612574870127
Iteration 15200: Loss = -11210.616807952465
1
Iteration 15300: Loss = -11210.612579160419
Iteration 15400: Loss = -11210.843170926462
1
Iteration 15500: Loss = -11210.612596499002
Iteration 15600: Loss = -11210.612566572201
Iteration 15700: Loss = -11210.612680889977
1
Iteration 15800: Loss = -11210.612566353999
Iteration 15900: Loss = -11210.620712680256
1
Iteration 16000: Loss = -11210.612552163258
Iteration 16100: Loss = -11210.621308349038
1
Iteration 16200: Loss = -11210.612535834707
Iteration 16300: Loss = -11210.99827965243
1
Iteration 16400: Loss = -11210.612535862765
Iteration 16500: Loss = -11210.612528361284
Iteration 16600: Loss = -11210.612621071688
Iteration 16700: Loss = -11210.612505174564
Iteration 16800: Loss = -11210.613156655196
1
Iteration 16900: Loss = -11210.614556758528
2
Iteration 17000: Loss = -11210.619366670993
3
Iteration 17100: Loss = -11210.612742785645
4
Iteration 17200: Loss = -11210.612547370638
Iteration 17300: Loss = -11210.616510481566
1
Iteration 17400: Loss = -11210.612522931899
Iteration 17500: Loss = -11210.612755490316
1
Iteration 17600: Loss = -11210.612513092497
Iteration 17700: Loss = -11210.613228055225
1
Iteration 17800: Loss = -11210.612501227797
Iteration 17900: Loss = -11210.618637550864
1
Iteration 18000: Loss = -11210.612503703378
Iteration 18100: Loss = -11210.636203072818
1
Iteration 18200: Loss = -11210.612524350807
Iteration 18300: Loss = -11210.612557670942
Iteration 18400: Loss = -11210.612567919567
Iteration 18500: Loss = -11210.612739553324
1
Iteration 18600: Loss = -11210.61253547432
Iteration 18700: Loss = -11210.821127289502
1
Iteration 18800: Loss = -11210.61253818713
Iteration 18900: Loss = -11210.61355338897
1
Iteration 19000: Loss = -11210.612514725877
Iteration 19100: Loss = -11210.61280009883
1
Iteration 19200: Loss = -11210.612529609947
Iteration 19300: Loss = -11210.612628119534
Iteration 19400: Loss = -11210.612508164859
Iteration 19500: Loss = -11210.613101247029
1
Iteration 19600: Loss = -11210.6125034217
Iteration 19700: Loss = -11210.6685289397
1
Iteration 19800: Loss = -11210.612510542982
Iteration 19900: Loss = -11210.615753234371
1
pi: tensor([[0.7414, 0.2586],
        [0.2043, 0.7957]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4530, 0.5470], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1923, 0.1012],
         [0.7152, 0.2960]],

        [[0.6246, 0.0954],
         [0.6659, 0.6432]],

        [[0.6464, 0.0977],
         [0.5320, 0.6443]],

        [[0.5471, 0.0991],
         [0.6266, 0.6903]],

        [[0.7301, 0.1030],
         [0.6519, 0.5333]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
Global Adjusted Rand Index: 0.9214379584608567
Average Adjusted Rand Index: 0.9209661208204318
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25053.34275804568
Iteration 100: Loss = -11554.156272394715
Iteration 200: Loss = -11545.939296672519
Iteration 300: Loss = -11508.306389789752
Iteration 400: Loss = -11324.88749691778
Iteration 500: Loss = -11318.86886402746
Iteration 600: Loss = -11317.320934689731
Iteration 700: Loss = -11317.236732197665
Iteration 800: Loss = -11317.190963372672
Iteration 900: Loss = -11317.167873495817
Iteration 1000: Loss = -11317.127303547344
Iteration 1100: Loss = -11313.994869693666
Iteration 1200: Loss = -11313.96638056569
Iteration 1300: Loss = -11313.955544175371
Iteration 1400: Loss = -11313.949867616639
Iteration 1500: Loss = -11313.94510571283
Iteration 1600: Loss = -11313.941647003792
Iteration 1700: Loss = -11313.939094641091
Iteration 1800: Loss = -11313.936873824414
Iteration 1900: Loss = -11313.934422974145
Iteration 2000: Loss = -11313.927389989283
Iteration 2100: Loss = -11313.857325254552
Iteration 2200: Loss = -11313.8557535289
Iteration 2300: Loss = -11313.854519459544
Iteration 2400: Loss = -11313.853220981438
Iteration 2500: Loss = -11313.849979631555
Iteration 2600: Loss = -11312.214729319598
Iteration 2700: Loss = -11312.206452508133
Iteration 2800: Loss = -11312.20270731638
Iteration 2900: Loss = -11312.201971431099
Iteration 3000: Loss = -11312.201639228755
Iteration 3100: Loss = -11312.201077108217
Iteration 3200: Loss = -11312.20063481018
Iteration 3300: Loss = -11312.200208127506
Iteration 3400: Loss = -11312.19976159098
Iteration 3500: Loss = -11312.187154153407
Iteration 3600: Loss = -11312.186514813973
Iteration 3700: Loss = -11312.186218746987
Iteration 3800: Loss = -11312.185955272078
Iteration 3900: Loss = -11312.185718190627
Iteration 4000: Loss = -11312.184481014223
Iteration 4100: Loss = -11312.17805290182
Iteration 4200: Loss = -11312.160410355022
Iteration 4300: Loss = -11312.177045620796
1
Iteration 4400: Loss = -11312.160770210976
2
Iteration 4500: Loss = -11312.159691060131
Iteration 4600: Loss = -11312.159727277585
Iteration 4700: Loss = -11312.15944918495
Iteration 4800: Loss = -11312.15922197184
Iteration 4900: Loss = -11312.163091525324
1
Iteration 5000: Loss = -11312.159013098411
Iteration 5100: Loss = -11312.15875693838
Iteration 5200: Loss = -11312.158718907076
Iteration 5300: Loss = -11312.158499257104
Iteration 5400: Loss = -11312.162640088023
1
Iteration 5500: Loss = -11312.160322489926
2
Iteration 5600: Loss = -11312.158226130789
Iteration 5700: Loss = -11312.158833207715
1
Iteration 5800: Loss = -11312.158735630182
2
Iteration 5900: Loss = -11312.158128516974
Iteration 6000: Loss = -11312.158436887294
1
Iteration 6100: Loss = -11312.158823440112
2
Iteration 6200: Loss = -11312.158479069883
3
Iteration 6300: Loss = -11312.159301836306
4
Iteration 6400: Loss = -11312.158170146615
Iteration 6500: Loss = -11312.158225018928
Iteration 6600: Loss = -11312.158202014221
Iteration 6700: Loss = -11312.158607166702
1
Iteration 6800: Loss = -11312.153799606916
Iteration 6900: Loss = -11312.152712585419
Iteration 7000: Loss = -11312.15276694313
Iteration 7100: Loss = -11312.15250547233
Iteration 7200: Loss = -11312.153045482017
1
Iteration 7300: Loss = -11312.15244277736
Iteration 7400: Loss = -11312.15268377693
1
Iteration 7500: Loss = -11312.155504020693
2
Iteration 7600: Loss = -11312.161403947193
3
Iteration 7700: Loss = -11312.15740554301
4
Iteration 7800: Loss = -11312.157336406292
5
Iteration 7900: Loss = -11312.152245687988
Iteration 8000: Loss = -11312.15218374718
Iteration 8100: Loss = -11312.15711395052
1
Iteration 8200: Loss = -11312.151999641794
Iteration 8300: Loss = -11312.152698316891
1
Iteration 8400: Loss = -11312.151937037566
Iteration 8500: Loss = -11312.151931968801
Iteration 8600: Loss = -11312.152206763525
1
Iteration 8700: Loss = -11312.151959759965
Iteration 8800: Loss = -11312.151906203248
Iteration 8900: Loss = -11312.150972414447
Iteration 9000: Loss = -11312.150413397214
Iteration 9100: Loss = -11312.150227350492
Iteration 9200: Loss = -11312.149925247257
Iteration 9300: Loss = -11312.149938943376
Iteration 9400: Loss = -11312.149891542087
Iteration 9500: Loss = -11312.14985213581
Iteration 9600: Loss = -11312.371794657765
1
Iteration 9700: Loss = -11312.149802021118
Iteration 9800: Loss = -11312.147975715576
Iteration 9900: Loss = -11312.272727658488
1
Iteration 10000: Loss = -11312.147970913604
Iteration 10100: Loss = -11312.147946288918
Iteration 10200: Loss = -11312.147949465296
Iteration 10300: Loss = -11312.1480168341
Iteration 10400: Loss = -11312.14794034924
Iteration 10500: Loss = -11312.22153338397
1
Iteration 10600: Loss = -11312.168795244286
2
Iteration 10700: Loss = -11312.147108036035
Iteration 10800: Loss = -11312.147202302038
Iteration 10900: Loss = -11312.473406904859
1
Iteration 11000: Loss = -11312.14703043527
Iteration 11100: Loss = -11312.1551586089
1
Iteration 11200: Loss = -11312.147089841466
Iteration 11300: Loss = -11312.147150379009
Iteration 11400: Loss = -11312.317041004568
1
Iteration 11500: Loss = -11312.147209423132
Iteration 11600: Loss = -11312.147069629476
Iteration 11700: Loss = -11312.148967160367
1
Iteration 11800: Loss = -11312.146942455951
Iteration 11900: Loss = -11312.147094580214
1
Iteration 12000: Loss = -11312.146936859295
Iteration 12100: Loss = -11312.147602851082
1
Iteration 12200: Loss = -11312.15329428209
2
Iteration 12300: Loss = -11312.17355825132
3
Iteration 12400: Loss = -11312.157693042709
4
Iteration 12500: Loss = -11312.151700778166
5
Iteration 12600: Loss = -11312.201870169341
6
Iteration 12700: Loss = -11312.175183562562
7
Iteration 12800: Loss = -11312.147058767114
8
Iteration 12900: Loss = -11312.14700916573
Iteration 13000: Loss = -11312.167636876098
1
Iteration 13100: Loss = -11312.14695718415
Iteration 13200: Loss = -11312.147833349121
1
Iteration 13300: Loss = -11312.146924340923
Iteration 13400: Loss = -11312.160349963919
1
Iteration 13500: Loss = -11312.146909978825
Iteration 13600: Loss = -11312.161333563718
1
Iteration 13700: Loss = -11312.156683709436
2
Iteration 13800: Loss = -11312.147534182566
3
Iteration 13900: Loss = -11312.14689663555
Iteration 14000: Loss = -11312.268747577145
1
Iteration 14100: Loss = -11312.148535067954
2
Iteration 14200: Loss = -11312.146807169223
Iteration 14300: Loss = -11312.147007532194
1
Iteration 14400: Loss = -11312.146755522703
Iteration 14500: Loss = -11312.147162927593
1
Iteration 14600: Loss = -11312.148364736713
2
Iteration 14700: Loss = -11312.14686504929
3
Iteration 14800: Loss = -11312.147252500397
4
Iteration 14900: Loss = -11312.149772083996
5
Iteration 15000: Loss = -11312.148876801877
6
Iteration 15100: Loss = -11312.148890582412
7
Iteration 15200: Loss = -11312.15614956602
8
Iteration 15300: Loss = -11312.221725537009
9
Iteration 15400: Loss = -11312.159964444943
10
Iteration 15500: Loss = -11312.146772651735
Iteration 15600: Loss = -11312.147039625303
1
Iteration 15700: Loss = -11312.146797141424
Iteration 15800: Loss = -11312.149006611708
1
Iteration 15900: Loss = -11312.147387033478
2
Iteration 16000: Loss = -11312.147215742805
3
Iteration 16100: Loss = -11312.151905500274
4
Iteration 16200: Loss = -11312.18944726852
5
Iteration 16300: Loss = -11312.147012838814
6
Iteration 16400: Loss = -11312.148570822446
7
Iteration 16500: Loss = -11312.151677835944
8
Iteration 16600: Loss = -11312.338592973407
9
Iteration 16700: Loss = -11312.146811977389
Iteration 16800: Loss = -11312.148920099728
1
Iteration 16900: Loss = -11312.146531907561
Iteration 17000: Loss = -11312.146173545158
Iteration 17100: Loss = -11312.146123369444
Iteration 17200: Loss = -11312.14670825925
1
Iteration 17300: Loss = -11312.146114585927
Iteration 17400: Loss = -11312.146392974157
1
Iteration 17500: Loss = -11312.147705649713
2
Iteration 17600: Loss = -11312.146191488322
Iteration 17700: Loss = -11312.19097227936
1
Iteration 17800: Loss = -11312.146151477587
Iteration 17900: Loss = -11312.146258845496
1
Iteration 18000: Loss = -11312.146167068473
Iteration 18100: Loss = -11312.14619309971
Iteration 18200: Loss = -11312.152694820497
1
Iteration 18300: Loss = -11312.146199317953
Iteration 18400: Loss = -11312.146522815425
1
Iteration 18500: Loss = -11312.175098911875
2
Iteration 18600: Loss = -11312.146158895886
Iteration 18700: Loss = -11312.146169631833
Iteration 18800: Loss = -11312.16730711589
1
Iteration 18900: Loss = -11312.14606281927
Iteration 19000: Loss = -11312.148302887439
1
Iteration 19100: Loss = -11312.146104002091
Iteration 19200: Loss = -11312.146882145651
1
Iteration 19300: Loss = -11312.146075390709
Iteration 19400: Loss = -11312.147560364732
1
Iteration 19500: Loss = -11312.146085311768
Iteration 19600: Loss = -11312.14626852785
1
Iteration 19700: Loss = -11312.146096660408
Iteration 19800: Loss = -11312.14898842255
1
Iteration 19900: Loss = -11312.146067891645
pi: tensor([[0.2940, 0.7060],
        [0.5824, 0.4176]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3122, 0.6878], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2644, 0.0920],
         [0.5932, 0.2431]],

        [[0.5949, 0.0958],
         [0.6146, 0.5621]],

        [[0.6459, 0.0922],
         [0.5052, 0.6174]],

        [[0.5786, 0.0951],
         [0.5840, 0.6496]],

        [[0.7174, 0.1003],
         [0.6241, 0.6031]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 85
Adjusted Rand Index: 0.48510267107807936
time is 1
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448427857772554
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7367283283406936
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 91
Adjusted Rand Index: 0.6690909090909091
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824124176797128
Global Adjusted Rand Index: 0.008734440920105023
Average Adjusted Rand Index: 0.7236354223933301
11233.008623623846
[0.9214379584608567, 0.008734440920105023] [0.9209661208204318, 0.7236354223933301] [11210.61254717279, 11312.15815442444]
-------------------------------------
This iteration is 78
True Objective function: Loss = -11137.123529617535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22819.417761524095
Iteration 100: Loss = -11104.081832445698
Iteration 200: Loss = -11101.753721301218
Iteration 300: Loss = -11101.41956206802
Iteration 400: Loss = -11101.289061732303
Iteration 500: Loss = -11101.223396429197
Iteration 600: Loss = -11101.185223413851
Iteration 700: Loss = -11101.160910120032
Iteration 800: Loss = -11101.144380747191
Iteration 900: Loss = -11101.132641696902
Iteration 1000: Loss = -11101.123937611132
Iteration 1100: Loss = -11101.117347674397
Iteration 1200: Loss = -11101.112143283764
Iteration 1300: Loss = -11101.108039313596
Iteration 1400: Loss = -11101.104703685924
Iteration 1500: Loss = -11101.102015128763
Iteration 1600: Loss = -11101.099728499903
Iteration 1700: Loss = -11101.097839136008
Iteration 1800: Loss = -11101.096238640193
Iteration 1900: Loss = -11101.094869617911
Iteration 2000: Loss = -11101.093714976663
Iteration 2100: Loss = -11101.092668469608
Iteration 2200: Loss = -11101.091790851287
Iteration 2300: Loss = -11101.090997803329
Iteration 2400: Loss = -11101.0903212444
Iteration 2500: Loss = -11101.089713016829
Iteration 2600: Loss = -11101.089162890914
Iteration 2700: Loss = -11101.088672193087
Iteration 2800: Loss = -11101.088262391386
Iteration 2900: Loss = -11101.09402115124
1
Iteration 3000: Loss = -11101.087518829348
Iteration 3100: Loss = -11101.087229968427
Iteration 3200: Loss = -11101.086954366026
Iteration 3300: Loss = -11101.086883606486
Iteration 3400: Loss = -11101.086471456212
Iteration 3500: Loss = -11101.086332929692
Iteration 3600: Loss = -11101.08613841177
Iteration 3700: Loss = -11101.086707877614
1
Iteration 3800: Loss = -11101.088471272855
2
Iteration 3900: Loss = -11101.085897581914
Iteration 4000: Loss = -11101.087298746403
1
Iteration 4100: Loss = -11101.085679078871
Iteration 4200: Loss = -11101.089956730511
1
Iteration 4300: Loss = -11101.085058794266
Iteration 4400: Loss = -11101.084982214907
Iteration 4500: Loss = -11101.091813541409
1
Iteration 4600: Loss = -11101.084823134614
Iteration 4700: Loss = -11101.0919650217
1
Iteration 4800: Loss = -11101.087112303758
2
Iteration 4900: Loss = -11101.092029909209
3
Iteration 5000: Loss = -11101.08449840927
Iteration 5100: Loss = -11101.084433332864
Iteration 5200: Loss = -11101.08436671022
Iteration 5300: Loss = -11101.084316827355
Iteration 5400: Loss = -11101.084278079416
Iteration 5500: Loss = -11101.084384072674
1
Iteration 5600: Loss = -11101.084170767437
Iteration 5700: Loss = -11101.084218380032
Iteration 5800: Loss = -11101.084291324145
Iteration 5900: Loss = -11101.097396892126
1
Iteration 6000: Loss = -11101.084033797193
Iteration 6100: Loss = -11101.085896022465
1
Iteration 6200: Loss = -11101.083958393601
Iteration 6300: Loss = -11101.084689205783
1
Iteration 6400: Loss = -11101.084003024143
Iteration 6500: Loss = -11101.092063544771
1
Iteration 6600: Loss = -11101.084364391463
2
Iteration 6700: Loss = -11101.084145537083
3
Iteration 6800: Loss = -11101.091260166313
4
Iteration 6900: Loss = -11101.08389164026
Iteration 7000: Loss = -11101.083860616758
Iteration 7100: Loss = -11101.083825022402
Iteration 7200: Loss = -11101.08386890175
Iteration 7300: Loss = -11101.084663014395
1
Iteration 7400: Loss = -11101.083808051559
Iteration 7500: Loss = -11101.083870337327
Iteration 7600: Loss = -11101.083900696649
Iteration 7700: Loss = -11101.085335358619
1
Iteration 7800: Loss = -11101.089926839697
2
Iteration 7900: Loss = -11101.102876643852
3
Iteration 8000: Loss = -11101.090854836344
4
Iteration 8100: Loss = -11101.085458719046
5
Iteration 8200: Loss = -11101.08584335193
6
Iteration 8300: Loss = -11101.083768319435
Iteration 8400: Loss = -11101.084092403646
1
Iteration 8500: Loss = -11101.084402117578
2
Iteration 8600: Loss = -11101.115405072504
3
Iteration 8700: Loss = -11101.08461444389
4
Iteration 8800: Loss = -11101.083650609324
Iteration 8900: Loss = -11101.083643678416
Iteration 9000: Loss = -11101.086346647657
1
Iteration 9100: Loss = -11101.08363060656
Iteration 9200: Loss = -11101.085524629998
1
Iteration 9300: Loss = -11101.083616997003
Iteration 9400: Loss = -11101.125038701814
1
Iteration 9500: Loss = -11101.08357705537
Iteration 9600: Loss = -11101.103326400707
1
Iteration 9700: Loss = -11101.083583393991
Iteration 9800: Loss = -11101.083592315674
Iteration 9900: Loss = -11101.086237751588
1
Iteration 10000: Loss = -11101.083546312384
Iteration 10100: Loss = -11101.08358109569
Iteration 10200: Loss = -11101.085999803732
1
Iteration 10300: Loss = -11101.083557543494
Iteration 10400: Loss = -11101.083526508033
Iteration 10500: Loss = -11101.08380381356
1
Iteration 10600: Loss = -11101.083547559627
Iteration 10700: Loss = -11101.11661252292
1
Iteration 10800: Loss = -11101.083538286048
Iteration 10900: Loss = -11101.104542915433
1
Iteration 11000: Loss = -11101.083570102672
Iteration 11100: Loss = -11101.084829284497
1
Iteration 11200: Loss = -11101.083787867556
2
Iteration 11300: Loss = -11101.083605151412
Iteration 11400: Loss = -11101.101003515845
1
Iteration 11500: Loss = -11101.083531924818
Iteration 11600: Loss = -11101.276677603091
1
Iteration 11700: Loss = -11101.083519010617
Iteration 11800: Loss = -11101.085363829792
1
Iteration 11900: Loss = -11101.08352552126
Iteration 12000: Loss = -11101.08410123998
1
Iteration 12100: Loss = -11101.083518268597
Iteration 12200: Loss = -11101.084526616381
1
Iteration 12300: Loss = -11101.084627907881
2
Iteration 12400: Loss = -11101.090431915853
3
Iteration 12500: Loss = -11101.08356142195
Iteration 12600: Loss = -11101.143683478864
1
Iteration 12700: Loss = -11101.083525943252
Iteration 12800: Loss = -11101.083915913625
1
Iteration 12900: Loss = -11101.083573645183
Iteration 13000: Loss = -11101.083520560818
Iteration 13100: Loss = -11101.084889696547
1
Iteration 13200: Loss = -11101.083558516548
Iteration 13300: Loss = -11101.184683759631
1
Iteration 13400: Loss = -11101.083534898746
Iteration 13500: Loss = -11101.083587865354
Iteration 13600: Loss = -11101.091479037474
1
Iteration 13700: Loss = -11101.083531914828
Iteration 13800: Loss = -11101.149013126622
1
Iteration 13900: Loss = -11101.083510525408
Iteration 14000: Loss = -11101.083512421506
Iteration 14100: Loss = -11101.085680193204
1
Iteration 14200: Loss = -11101.083821600001
2
Iteration 14300: Loss = -11101.089704577718
3
Iteration 14400: Loss = -11101.083525397034
Iteration 14500: Loss = -11101.084007183033
1
Iteration 14600: Loss = -11101.08399914799
2
Iteration 14700: Loss = -11101.083622652803
Iteration 14800: Loss = -11101.185608694253
1
Iteration 14900: Loss = -11101.083541415854
Iteration 15000: Loss = -11101.12213595606
1
Iteration 15100: Loss = -11101.083516167455
Iteration 15200: Loss = -11101.461893663638
1
Iteration 15300: Loss = -11101.083553027642
Iteration 15400: Loss = -11101.083544891571
Iteration 15500: Loss = -11101.083698568722
1
Iteration 15600: Loss = -11101.083539977515
Iteration 15700: Loss = -11101.085814278329
1
Iteration 15800: Loss = -11101.083533098776
Iteration 15900: Loss = -11101.088151736578
1
Iteration 16000: Loss = -11101.085789288512
2
Iteration 16100: Loss = -11101.132754124306
3
Iteration 16200: Loss = -11101.083596136197
Iteration 16300: Loss = -11101.089657031283
1
Iteration 16400: Loss = -11101.083534702266
Iteration 16500: Loss = -11101.083758652314
1
Iteration 16600: Loss = -11101.08591037764
2
Iteration 16700: Loss = -11101.09052699396
3
Iteration 16800: Loss = -11101.08356239801
Iteration 16900: Loss = -11101.09499236936
1
Iteration 17000: Loss = -11101.083525969485
Iteration 17100: Loss = -11101.086088234515
1
Iteration 17200: Loss = -11101.091296178336
2
Iteration 17300: Loss = -11101.083530494112
Iteration 17400: Loss = -11101.086680763554
1
Iteration 17500: Loss = -11101.098138995894
2
Iteration 17600: Loss = -11101.083513072908
Iteration 17700: Loss = -11101.08399963063
1
Iteration 17800: Loss = -11101.093608264906
2
Iteration 17900: Loss = -11101.083581119481
Iteration 18000: Loss = -11101.084257254763
1
Iteration 18100: Loss = -11101.114458297814
2
Iteration 18200: Loss = -11101.083561515916
Iteration 18300: Loss = -11101.08584805698
1
Iteration 18400: Loss = -11101.083652877956
Iteration 18500: Loss = -11101.083854295382
1
Iteration 18600: Loss = -11101.117253723736
2
Iteration 18700: Loss = -11101.08440705884
3
Iteration 18800: Loss = -11101.09060599177
4
Iteration 18900: Loss = -11101.200906553411
5
Iteration 19000: Loss = -11101.083574213504
Iteration 19100: Loss = -11101.08400895587
1
Iteration 19200: Loss = -11101.130181853336
2
Iteration 19300: Loss = -11101.083563331456
Iteration 19400: Loss = -11101.084870636252
1
Iteration 19500: Loss = -11101.083517477024
Iteration 19600: Loss = -11101.083603032224
Iteration 19700: Loss = -11101.08484710351
1
Iteration 19800: Loss = -11101.083563318054
Iteration 19900: Loss = -11101.09553407627
1
pi: tensor([[0.7513, 0.2487],
        [0.3226, 0.6774]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5272, 0.4728], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2128, 0.1015],
         [0.6758, 0.2867]],

        [[0.7074, 0.1087],
         [0.7243, 0.7226]],

        [[0.6028, 0.0904],
         [0.5705, 0.7110]],

        [[0.6055, 0.0983],
         [0.6506, 0.6932]],

        [[0.6256, 0.0935],
         [0.6797, 0.6682]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9598385576399676
Global Adjusted Rand Index: 0.9137594882934749
Average Adjusted Rand Index: 0.9140652796277747
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20068.898639130635
Iteration 100: Loss = -11339.000812740727
Iteration 200: Loss = -11338.005482222306
Iteration 300: Loss = -11337.685168831096
Iteration 400: Loss = -11337.515669460765
Iteration 500: Loss = -11337.37735241798
Iteration 600: Loss = -11337.219360803032
Iteration 700: Loss = -11336.994569968718
Iteration 800: Loss = -11336.662297548473
Iteration 900: Loss = -11336.27350210506
Iteration 1000: Loss = -11335.955536825955
Iteration 1100: Loss = -11335.661303409046
Iteration 1200: Loss = -11335.236877853906
Iteration 1300: Loss = -11334.428945591824
Iteration 1400: Loss = -11333.88435504403
Iteration 1500: Loss = -11333.637894243337
Iteration 1600: Loss = -11333.452601341065
Iteration 1700: Loss = -11333.14650954932
Iteration 1800: Loss = -11332.74053159995
Iteration 1900: Loss = -11332.485753125395
Iteration 2000: Loss = -11332.322105209569
Iteration 2100: Loss = -11332.207956401058
Iteration 2200: Loss = -11332.124551593632
Iteration 2300: Loss = -11332.060470902148
Iteration 2400: Loss = -11332.009681807003
Iteration 2500: Loss = -11331.968916842447
Iteration 2600: Loss = -11331.93578952469
Iteration 2700: Loss = -11331.908876627142
Iteration 2800: Loss = -11331.886900013638
Iteration 2900: Loss = -11331.868813311032
Iteration 3000: Loss = -11331.853782194587
Iteration 3100: Loss = -11331.84124495324
Iteration 3200: Loss = -11331.830631340667
Iteration 3300: Loss = -11331.821779115322
Iteration 3400: Loss = -11331.81430741675
Iteration 3500: Loss = -11331.80799186793
Iteration 3600: Loss = -11331.80262729586
Iteration 3700: Loss = -11331.798002016341
Iteration 3800: Loss = -11331.794094097011
Iteration 3900: Loss = -11331.79073993347
Iteration 4000: Loss = -11331.787746468166
Iteration 4100: Loss = -11331.785208359515
Iteration 4200: Loss = -11331.782928587547
Iteration 4300: Loss = -11331.78093117972
Iteration 4400: Loss = -11331.779223111984
Iteration 4500: Loss = -11331.77772726415
Iteration 4600: Loss = -11331.776403730566
Iteration 4700: Loss = -11331.775216333774
Iteration 4800: Loss = -11331.77420985251
Iteration 4900: Loss = -11331.773243929396
Iteration 5000: Loss = -11331.77242318212
Iteration 5100: Loss = -11331.771656044757
Iteration 5200: Loss = -11331.770914660658
Iteration 5300: Loss = -11331.770292662202
Iteration 5400: Loss = -11331.769658339963
Iteration 5500: Loss = -11331.769119868799
Iteration 5600: Loss = -11331.771249980544
1
Iteration 5700: Loss = -11331.768117485566
Iteration 5800: Loss = -11331.767659692001
Iteration 5900: Loss = -11331.76725317137
Iteration 6000: Loss = -11331.766890949419
Iteration 6100: Loss = -11331.766539701925
Iteration 6200: Loss = -11331.766302832877
Iteration 6300: Loss = -11331.765914812358
Iteration 6400: Loss = -11331.765609060898
Iteration 6500: Loss = -11331.765316791745
Iteration 6600: Loss = -11331.765071988531
Iteration 6700: Loss = -11331.7674630974
1
Iteration 6800: Loss = -11331.76461104778
Iteration 6900: Loss = -11331.76442250373
Iteration 7000: Loss = -11331.764207978173
Iteration 7100: Loss = -11331.764222438245
Iteration 7200: Loss = -11331.763891134375
Iteration 7300: Loss = -11331.76368707266
Iteration 7400: Loss = -11331.77080100825
1
Iteration 7500: Loss = -11331.768765911147
2
Iteration 7600: Loss = -11331.763274097559
Iteration 7700: Loss = -11331.781229012542
1
Iteration 7800: Loss = -11331.763055632877
Iteration 7900: Loss = -11331.773826488168
1
Iteration 8000: Loss = -11331.762809326
Iteration 8100: Loss = -11331.802586535441
1
Iteration 8200: Loss = -11331.762595317574
Iteration 8300: Loss = -11331.762811863682
1
Iteration 8400: Loss = -11331.762488953427
Iteration 8500: Loss = -11331.762305452708
Iteration 8600: Loss = -11331.763065290716
1
Iteration 8700: Loss = -11331.762194702353
Iteration 8800: Loss = -11331.793235211626
1
Iteration 8900: Loss = -11331.762090659135
Iteration 9000: Loss = -11331.7620107683
Iteration 9100: Loss = -11331.919408761707
1
Iteration 9200: Loss = -11331.761867169263
Iteration 9300: Loss = -11331.761864962276
Iteration 9400: Loss = -11331.761942416251
Iteration 9500: Loss = -11331.761735650754
Iteration 9600: Loss = -11331.768031400148
1
Iteration 9700: Loss = -11331.76170159562
Iteration 9800: Loss = -11331.761654957341
Iteration 9900: Loss = -11331.761681350246
Iteration 10000: Loss = -11331.761534019919
Iteration 10100: Loss = -11331.761478607423
Iteration 10200: Loss = -11331.762103284043
1
Iteration 10300: Loss = -11331.761570523331
Iteration 10400: Loss = -11331.761427413392
Iteration 10500: Loss = -11331.776305414884
1
Iteration 10600: Loss = -11331.761342549578
Iteration 10700: Loss = -11331.76137450937
Iteration 10800: Loss = -11331.828330594466
1
Iteration 10900: Loss = -11331.761332558723
Iteration 11000: Loss = -11331.761299211956
Iteration 11100: Loss = -11331.761452958901
1
Iteration 11200: Loss = -11331.76123301723
Iteration 11300: Loss = -11331.767573615769
1
Iteration 11400: Loss = -11331.761172473922
Iteration 11500: Loss = -11331.779752165785
1
Iteration 11600: Loss = -11331.761893139495
2
Iteration 11700: Loss = -11331.761118253859
Iteration 11800: Loss = -11331.773902520144
1
Iteration 11900: Loss = -11331.761132864249
Iteration 12000: Loss = -11331.763207613983
1
Iteration 12100: Loss = -11331.761214157752
Iteration 12200: Loss = -11331.81497482675
1
Iteration 12300: Loss = -11331.762087453093
2
Iteration 12400: Loss = -11331.761954094303
3
Iteration 12500: Loss = -11331.761678434861
4
Iteration 12600: Loss = -11331.762875911076
5
Iteration 12700: Loss = -11331.761283135862
Iteration 12800: Loss = -11331.791383651203
1
Iteration 12900: Loss = -11331.763379093289
2
Iteration 13000: Loss = -11331.761351222225
Iteration 13100: Loss = -11331.761049157942
Iteration 13200: Loss = -11331.783798777464
1
Iteration 13300: Loss = -11331.76099996733
Iteration 13400: Loss = -11331.770946287552
1
Iteration 13500: Loss = -11331.761073038806
Iteration 13600: Loss = -11331.761692184473
1
Iteration 13700: Loss = -11331.761010202936
Iteration 13800: Loss = -11331.932128039083
1
Iteration 13900: Loss = -11331.760961644326
Iteration 14000: Loss = -11331.760976242665
Iteration 14100: Loss = -11331.763777525164
1
Iteration 14200: Loss = -11331.765220810166
2
Iteration 14300: Loss = -11331.79977342414
3
Iteration 14400: Loss = -11331.76155122533
4
Iteration 14500: Loss = -11331.76097761454
Iteration 14600: Loss = -11331.761048635515
Iteration 14700: Loss = -11331.766476202281
1
Iteration 14800: Loss = -11331.761148858233
2
Iteration 14900: Loss = -11331.762877281508
3
Iteration 15000: Loss = -11331.766798959856
4
Iteration 15100: Loss = -11331.760992069112
Iteration 15200: Loss = -11331.76124054487
1
Iteration 15300: Loss = -11331.761143071692
2
Iteration 15400: Loss = -11331.794786960674
3
Iteration 15500: Loss = -11331.761281560934
4
Iteration 15600: Loss = -11331.788717414513
5
Iteration 15700: Loss = -11331.760978819946
Iteration 15800: Loss = -11331.762589754619
1
Iteration 15900: Loss = -11331.776137214947
2
Iteration 16000: Loss = -11331.761565342487
3
Iteration 16100: Loss = -11331.762238285735
4
Iteration 16200: Loss = -11331.761269723322
5
Iteration 16300: Loss = -11331.760972652772
Iteration 16400: Loss = -11331.791306717854
1
Iteration 16500: Loss = -11331.76175580677
2
Iteration 16600: Loss = -11331.762355633127
3
Iteration 16700: Loss = -11331.760947048157
Iteration 16800: Loss = -11331.761898508668
1
Iteration 16900: Loss = -11331.760962980867
Iteration 17000: Loss = -11331.761902089122
1
Iteration 17100: Loss = -11331.761002211522
Iteration 17200: Loss = -11331.76130277563
1
Iteration 17300: Loss = -11331.763353304632
2
Iteration 17400: Loss = -11331.76102939029
Iteration 17500: Loss = -11331.763738502934
1
Iteration 17600: Loss = -11331.76260346514
2
Iteration 17700: Loss = -11331.760930540308
Iteration 17800: Loss = -11331.761001160196
Iteration 17900: Loss = -11331.761022739729
Iteration 18000: Loss = -11331.76191881629
1
Iteration 18100: Loss = -11331.761279075768
2
Iteration 18200: Loss = -11331.760889060019
Iteration 18300: Loss = -11331.76118670001
1
Iteration 18400: Loss = -11331.764412781597
2
Iteration 18500: Loss = -11331.761139434328
3
Iteration 18600: Loss = -11331.761459790312
4
Iteration 18700: Loss = -11331.763869602164
5
Iteration 18800: Loss = -11331.763136753385
6
Iteration 18900: Loss = -11331.760864167672
Iteration 19000: Loss = -11331.762843866518
1
Iteration 19100: Loss = -11331.762286226884
2
Iteration 19200: Loss = -11331.761616211583
3
Iteration 19300: Loss = -11331.760889620227
Iteration 19400: Loss = -11331.76103691031
1
Iteration 19500: Loss = -11331.760959100708
Iteration 19600: Loss = -11331.760893901628
Iteration 19700: Loss = -11331.761673883675
1
Iteration 19800: Loss = -11331.763025256456
2
Iteration 19900: Loss = -11331.762472407177
3
pi: tensor([[1.0000e+00, 1.3975e-08],
        [1.6637e-01, 8.3363e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9451, 0.0549], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1704, 0.1468],
         [0.6700, 0.0198]],

        [[0.6713, 0.2667],
         [0.6869, 0.5853]],

        [[0.6531, 0.2123],
         [0.6354, 0.6382]],

        [[0.5082, 0.1763],
         [0.6472, 0.6004]],

        [[0.6188, 0.0959],
         [0.7293, 0.6089]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.002821183194503557
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.001684040076915292
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0029298663248489287
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 80%|████████  | 80/100 [28:08:05<7:08:42, 1286.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 81%|████████  | 81/100 [28:29:39<6:48:03, 1288.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 82%|████████▏ | 82/100 [28:51:06<6:26:24, 1288.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 83%|████████▎ | 83/100 [29:12:35<6:05:01, 1288.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 84%|████████▍ | 84/100 [29:34:06<5:43:45, 1289.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 85%|████████▌ | 85/100 [29:55:45<5:23:01, 1292.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 86%|████████▌ | 86/100 [30:17:20<5:01:39, 1292.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 87%|████████▋ | 87/100 [30:36:54<4:32:25, 1257.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 88%|████████▊ | 88/100 [30:55:23<4:02:33, 1212.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 89%|████████▉ | 89/100 [31:16:59<3:46:56, 1237.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 90%|█████████ | 90/100 [31:38:30<3:28:56, 1253.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 91%|█████████ | 91/100 [31:59:52<3:09:21, 1262.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 92%|█████████▏| 92/100 [32:21:23<2:49:26, 1270.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 93%|█████████▎| 93/100 [32:40:23<2:23:41, 1231.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 94%|█████████▍| 94/100 [33:00:56<2:03:11, 1231.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 95%|█████████▌| 95/100 [33:22:24<1:44:04, 1248.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 96%|█████████▌| 96/100 [33:43:48<1:23:57, 1259.28s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 97%|█████████▋| 97/100 [34:05:23<1:03:30, 1270.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 98%|█████████▊| 98/100 [34:25:27<41:40, 1250.33s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 99%|█████████▉| 99/100 [34:46:54<21:01, 1261.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
100%|██████████| 100/100 [35:08:17<00:00, 1267.91s/it]100%|██████████| 100/100 [35:08:17<00:00, 1264.98s/it]
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 65
Adjusted Rand Index: 0.021128754914891985
Global Adjusted Rand Index: 0.005447564731894951
Average Adjusted Rand Index: 0.0052240059973166515
11137.123529617535
[0.9137594882934749, 0.005447564731894951] [0.9140652796277747, 0.0052240059973166515] [11101.084113632192, 11331.760994323657]
-------------------------------------
This iteration is 79
True Objective function: Loss = -11353.179542585178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24174.543896962256
Iteration 100: Loss = -11621.198332456244
Iteration 200: Loss = -11618.784528859129
Iteration 300: Loss = -11614.970016514817
Iteration 400: Loss = -11603.63099573608
Iteration 500: Loss = -11567.076389942134
Iteration 600: Loss = -11494.217931004527
Iteration 700: Loss = -11418.512296387004
Iteration 800: Loss = -11388.692018339852
Iteration 900: Loss = -11385.429972050062
Iteration 1000: Loss = -11379.542260813678
Iteration 1100: Loss = -11379.34632319335
Iteration 1200: Loss = -11377.7489230023
Iteration 1300: Loss = -11377.363424230029
Iteration 1400: Loss = -11377.299910453128
Iteration 1500: Loss = -11377.245657622165
Iteration 1600: Loss = -11377.216712423875
Iteration 1700: Loss = -11375.50213035808
Iteration 1800: Loss = -11375.365358001163
Iteration 1900: Loss = -11375.310344546737
Iteration 2000: Loss = -11375.297600337695
Iteration 2100: Loss = -11372.543650135634
Iteration 2200: Loss = -11372.510572556628
Iteration 2300: Loss = -11372.498642647388
Iteration 2400: Loss = -11370.900862081035
Iteration 2500: Loss = -11370.879257065028
Iteration 2600: Loss = -11370.872453995213
Iteration 2700: Loss = -11368.844929981113
Iteration 2800: Loss = -11368.805166446908
Iteration 2900: Loss = -11368.786000983979
Iteration 3000: Loss = -11368.67742329045
Iteration 3100: Loss = -11368.612123399374
Iteration 3200: Loss = -11368.532976341834
Iteration 3300: Loss = -11368.525023821154
Iteration 3400: Loss = -11368.522907127905
Iteration 3500: Loss = -11368.521714946017
Iteration 3600: Loss = -11368.520124463208
Iteration 3700: Loss = -11368.52289273286
1
Iteration 3800: Loss = -11368.520327574377
2
Iteration 3900: Loss = -11368.515005229014
Iteration 4000: Loss = -11368.513579316323
Iteration 4100: Loss = -11368.497267775976
Iteration 4200: Loss = -11368.461633362296
Iteration 4300: Loss = -11368.461167670652
Iteration 4400: Loss = -11368.461085973877
Iteration 4500: Loss = -11368.460577776736
Iteration 4600: Loss = -11368.460140474072
Iteration 4700: Loss = -11368.459103780217
Iteration 4800: Loss = -11368.459475515874
1
Iteration 4900: Loss = -11368.459002354597
Iteration 5000: Loss = -11368.458648197646
Iteration 5100: Loss = -11368.457527006814
Iteration 5200: Loss = -11368.458104766883
1
Iteration 5300: Loss = -11368.45644298919
Iteration 5400: Loss = -11367.427285071786
Iteration 5500: Loss = -11367.224558176482
Iteration 5600: Loss = -11367.223663616656
Iteration 5700: Loss = -11367.222996613302
Iteration 5800: Loss = -11367.222429002115
Iteration 5900: Loss = -11367.220554723903
Iteration 6000: Loss = -11367.218478613837
Iteration 6100: Loss = -11367.223469337176
1
Iteration 6200: Loss = -11367.215379084546
Iteration 6300: Loss = -11367.212804263067
Iteration 6400: Loss = -11367.213427612247
1
Iteration 6500: Loss = -11367.212717749735
Iteration 6600: Loss = -11367.213635537364
1
Iteration 6700: Loss = -11367.211277358005
Iteration 6800: Loss = -11367.212001258678
1
Iteration 6900: Loss = -11367.210779170588
Iteration 7000: Loss = -11367.21062351702
Iteration 7100: Loss = -11367.217415635761
1
Iteration 7200: Loss = -11367.210320276969
Iteration 7300: Loss = -11367.210220942974
Iteration 7400: Loss = -11367.20889440424
Iteration 7500: Loss = -11367.208126521695
Iteration 7600: Loss = -11367.207894583104
Iteration 7700: Loss = -11367.218567827862
1
Iteration 7800: Loss = -11367.206984961425
Iteration 7900: Loss = -11367.206688812788
Iteration 8000: Loss = -11367.21335066741
1
Iteration 8100: Loss = -11367.206523926487
Iteration 8200: Loss = -11367.20637522029
Iteration 8300: Loss = -11367.183732480313
Iteration 8400: Loss = -11367.183584702627
Iteration 8500: Loss = -11367.183558517761
Iteration 8600: Loss = -11367.185504373738
1
Iteration 8700: Loss = -11367.18345046587
Iteration 8800: Loss = -11367.183370102444
Iteration 8900: Loss = -11367.486349864024
1
Iteration 9000: Loss = -11367.183131452697
Iteration 9100: Loss = -11367.18308634127
Iteration 9200: Loss = -11367.183060109423
Iteration 9300: Loss = -11367.183017332196
Iteration 9400: Loss = -11367.18294861063
Iteration 9500: Loss = -11367.182789312279
Iteration 9600: Loss = -11367.18278186421
Iteration 9700: Loss = -11367.182352231237
Iteration 9800: Loss = -11367.21874567925
1
Iteration 9900: Loss = -11367.185289364537
2
Iteration 10000: Loss = -11367.182922346015
3
Iteration 10100: Loss = -11367.182769022309
4
Iteration 10200: Loss = -11367.181464465515
Iteration 10300: Loss = -11367.182741759756
1
Iteration 10400: Loss = -11367.18139518562
Iteration 10500: Loss = -11367.182031579778
1
Iteration 10600: Loss = -11367.181325214808
Iteration 10700: Loss = -11367.646537164654
1
Iteration 10800: Loss = -11367.181233725114
Iteration 10900: Loss = -11367.181225165808
Iteration 11000: Loss = -11367.181647420386
1
Iteration 11100: Loss = -11367.186444623652
2
Iteration 11200: Loss = -11367.196146211205
3
Iteration 11300: Loss = -11367.180351263825
Iteration 11400: Loss = -11367.17694282806
Iteration 11500: Loss = -11367.184854532354
1
Iteration 11600: Loss = -11367.176936370328
Iteration 11700: Loss = -11367.297819368918
1
Iteration 11800: Loss = -11367.176933418526
Iteration 11900: Loss = -11367.176903761883
Iteration 12000: Loss = -11367.183156920968
1
Iteration 12100: Loss = -11367.17690376797
Iteration 12200: Loss = -11367.17684962778
Iteration 12300: Loss = -11367.30099562678
1
Iteration 12400: Loss = -11367.176817399084
Iteration 12500: Loss = -11367.176862724222
Iteration 12600: Loss = -11367.176655876121
Iteration 12700: Loss = -11367.176896445904
1
Iteration 12800: Loss = -11367.174075530878
Iteration 12900: Loss = -11367.251693557939
1
Iteration 13000: Loss = -11367.172133708546
Iteration 13100: Loss = -11367.172120231924
Iteration 13200: Loss = -11367.174455937935
1
Iteration 13300: Loss = -11367.172101507023
Iteration 13400: Loss = -11367.175178708332
1
Iteration 13500: Loss = -11367.181214583446
2
Iteration 13600: Loss = -11367.195800897212
3
Iteration 13700: Loss = -11367.182929870747
4
Iteration 13800: Loss = -11367.172409378047
5
Iteration 13900: Loss = -11367.192963180278
6
Iteration 14000: Loss = -11367.175415946323
7
Iteration 14100: Loss = -11367.172123030183
Iteration 14200: Loss = -11367.204581192043
1
Iteration 14300: Loss = -11367.192674878439
2
Iteration 14400: Loss = -11367.172747362723
3
Iteration 14500: Loss = -11367.172134272107
Iteration 14600: Loss = -11367.203832119982
1
Iteration 14700: Loss = -11367.172027031995
Iteration 14800: Loss = -11367.189962419376
1
Iteration 14900: Loss = -11367.396064946073
2
Iteration 15000: Loss = -11367.174550640782
3
Iteration 15100: Loss = -11367.177367601722
4
Iteration 15200: Loss = -11367.272351690268
5
Iteration 15300: Loss = -11367.194420424928
6
Iteration 15400: Loss = -11367.20804939753
7
Iteration 15500: Loss = -11367.170639236027
Iteration 15600: Loss = -11367.170883699266
1
Iteration 15700: Loss = -11367.170517314535
Iteration 15800: Loss = -11367.171037845206
1
Iteration 15900: Loss = -11367.170511718847
Iteration 16000: Loss = -11367.170850199258
1
Iteration 16100: Loss = -11367.17051363276
Iteration 16200: Loss = -11367.183264014362
1
Iteration 16300: Loss = -11367.170534312509
Iteration 16400: Loss = -11367.170562029753
Iteration 16500: Loss = -11367.170573626354
Iteration 16600: Loss = -11367.170498946462
Iteration 16700: Loss = -11367.822815345313
1
Iteration 16800: Loss = -11367.170436428354
Iteration 16900: Loss = -11367.170531475304
Iteration 17000: Loss = -11367.192720596342
1
Iteration 17100: Loss = -11367.170461707345
Iteration 17200: Loss = -11367.264865236899
1
Iteration 17300: Loss = -11367.200618696
2
Iteration 17400: Loss = -11367.372701418884
3
Iteration 17500: Loss = -11367.170380764932
Iteration 17600: Loss = -11367.169446191763
Iteration 17700: Loss = -11367.168780713228
Iteration 17800: Loss = -11367.169007417058
1
Iteration 17900: Loss = -11367.168818943324
Iteration 18000: Loss = -11367.17302466958
1
Iteration 18100: Loss = -11367.168816823772
Iteration 18200: Loss = -11367.168786935134
Iteration 18300: Loss = -11367.168969030392
1
Iteration 18400: Loss = -11367.168784491421
Iteration 18500: Loss = -11367.168799388353
Iteration 18600: Loss = -11367.170227160414
1
Iteration 18700: Loss = -11367.168789744548
Iteration 18800: Loss = -11367.168843606401
Iteration 18900: Loss = -11367.16894430649
1
Iteration 19000: Loss = -11367.188715244096
2
Iteration 19100: Loss = -11367.176692914576
3
Iteration 19200: Loss = -11367.169083283577
4
Iteration 19300: Loss = -11367.16854723934
Iteration 19400: Loss = -11367.164886163095
Iteration 19500: Loss = -11367.250958404793
1
Iteration 19600: Loss = -11367.165500376084
2
Iteration 19700: Loss = -11367.165806461702
3
Iteration 19800: Loss = -11367.234520300783
4
Iteration 19900: Loss = -11367.16431788116
pi: tensor([[0.2418, 0.7582],
        [0.7630, 0.2370]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5949, 0.4051], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2656, 0.1021],
         [0.6612, 0.2420]],

        [[0.6041, 0.0905],
         [0.6086, 0.6568]],

        [[0.5735, 0.0987],
         [0.5852, 0.5670]],

        [[0.7274, 0.1041],
         [0.7055, 0.5506]],

        [[0.5172, 0.1131],
         [0.5924, 0.7242]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824283882000855
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448509923071951
Global Adjusted Rand Index: 0.03341228743467
Average Adjusted Rand Index: 0.8825812257053997
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22962.372161458086
Iteration 100: Loss = -11621.769832382595
Iteration 200: Loss = -11620.932208843746
Iteration 300: Loss = -11620.24554497168
Iteration 400: Loss = -11618.049306996509
Iteration 500: Loss = -11610.038423802864
Iteration 600: Loss = -11605.714431955848
Iteration 700: Loss = -11604.97997491453
Iteration 800: Loss = -11604.475152678238
Iteration 900: Loss = -11604.159425505006
Iteration 1000: Loss = -11603.941511975483
Iteration 1100: Loss = -11603.764969668366
Iteration 1200: Loss = -11603.635108583798
Iteration 1300: Loss = -11603.542100787152
Iteration 1400: Loss = -11603.443342518574
Iteration 1500: Loss = -11603.28220382428
Iteration 1600: Loss = -11602.304006982587
Iteration 1700: Loss = -11593.651900165405
Iteration 1800: Loss = -11579.14690783839
Iteration 1900: Loss = -11518.490484360233
Iteration 2000: Loss = -11498.667113612963
Iteration 2100: Loss = -11423.602877273004
Iteration 2200: Loss = -11396.526027493828
Iteration 2300: Loss = -11377.819868687597
Iteration 2400: Loss = -11366.063411189818
Iteration 2500: Loss = -11358.207043636416
Iteration 2600: Loss = -11357.330542457565
Iteration 2700: Loss = -11356.044695698294
Iteration 2800: Loss = -11355.97491733179
Iteration 2900: Loss = -11355.9558328388
Iteration 3000: Loss = -11354.718902818424
Iteration 3100: Loss = -11349.099018785828
Iteration 3200: Loss = -11349.095338850912
Iteration 3300: Loss = -11348.966594699812
Iteration 3400: Loss = -11348.884593319997
Iteration 3500: Loss = -11344.214279068168
Iteration 3600: Loss = -11337.958736559194
Iteration 3700: Loss = -11337.82742400486
Iteration 3800: Loss = -11337.82489730539
Iteration 3900: Loss = -11337.830079363637
1
Iteration 4000: Loss = -11337.824523796744
Iteration 4100: Loss = -11337.818734784623
Iteration 4200: Loss = -11333.946699105218
Iteration 4300: Loss = -11331.108337441401
Iteration 4400: Loss = -11331.095218500723
Iteration 4500: Loss = -11331.094167844523
Iteration 4600: Loss = -11325.672759703848
Iteration 4700: Loss = -11325.661309496541
Iteration 4800: Loss = -11323.089830953404
Iteration 4900: Loss = -11323.077350358282
Iteration 5000: Loss = -11322.207750660542
Iteration 5100: Loss = -11321.674345677835
Iteration 5200: Loss = -11321.671965452875
Iteration 5300: Loss = -11321.671325889727
Iteration 5400: Loss = -11321.671824164849
1
Iteration 5500: Loss = -11321.670265980969
Iteration 5600: Loss = -11321.670127276257
Iteration 5700: Loss = -11321.677083858403
1
Iteration 5800: Loss = -11321.668441566519
Iteration 5900: Loss = -11321.670840855377
1
Iteration 6000: Loss = -11321.623775637947
Iteration 6100: Loss = -11321.621567383248
Iteration 6200: Loss = -11321.620648378797
Iteration 6300: Loss = -11321.60063544626
Iteration 6400: Loss = -11321.59361139048
Iteration 6500: Loss = -11321.59250993317
Iteration 6600: Loss = -11321.59173025222
Iteration 6700: Loss = -11321.590631202418
Iteration 6800: Loss = -11321.586475182594
Iteration 6900: Loss = -11321.58615800488
Iteration 7000: Loss = -11321.586054844858
Iteration 7100: Loss = -11321.594290296767
1
Iteration 7200: Loss = -11321.58605160279
Iteration 7300: Loss = -11321.586435393678
1
Iteration 7400: Loss = -11321.585883835689
Iteration 7500: Loss = -11321.590826113012
1
Iteration 7600: Loss = -11321.590264241662
2
Iteration 7700: Loss = -11321.585260394544
Iteration 7800: Loss = -11321.585085286402
Iteration 7900: Loss = -11321.58381813922
Iteration 8000: Loss = -11321.57569317771
Iteration 8100: Loss = -11321.574023813115
Iteration 8200: Loss = -11321.574291074843
1
Iteration 8300: Loss = -11321.573856155645
Iteration 8400: Loss = -11321.573914837449
Iteration 8500: Loss = -11321.60110819585
1
Iteration 8600: Loss = -11321.573714264809
Iteration 8700: Loss = -11321.57327226875
Iteration 8800: Loss = -11321.56146145271
Iteration 8900: Loss = -11321.561243582677
Iteration 9000: Loss = -11321.560973360592
Iteration 9100: Loss = -11321.557185627371
Iteration 9200: Loss = -11321.558113997822
1
Iteration 9300: Loss = -11321.573024641972
2
Iteration 9400: Loss = -11321.557588350237
3
Iteration 9500: Loss = -11321.54668106755
Iteration 9600: Loss = -11321.531066524218
Iteration 9700: Loss = -11321.530946850171
Iteration 9800: Loss = -11321.537861659976
1
Iteration 9900: Loss = -11321.530881172786
Iteration 10000: Loss = -11321.531215478937
1
Iteration 10100: Loss = -11321.530842156755
Iteration 10200: Loss = -11321.533282051507
1
Iteration 10300: Loss = -11321.530720777586
Iteration 10400: Loss = -11321.53068293478
Iteration 10500: Loss = -11321.530746606246
Iteration 10600: Loss = -11321.530580319359
Iteration 10700: Loss = -11322.015462912866
1
Iteration 10800: Loss = -11321.530471768192
Iteration 10900: Loss = -11321.530395282709
Iteration 11000: Loss = -11321.777356943396
1
Iteration 11100: Loss = -11321.527280660985
Iteration 11200: Loss = -11321.52716521083
Iteration 11300: Loss = -11321.527177576274
Iteration 11400: Loss = -11321.52902552585
1
Iteration 11500: Loss = -11321.527142276114
Iteration 11600: Loss = -11321.52709508157
Iteration 11700: Loss = -11321.527220554086
1
Iteration 11800: Loss = -11321.527081136119
Iteration 11900: Loss = -11321.526485114133
Iteration 12000: Loss = -11321.52717381194
1
Iteration 12100: Loss = -11321.526434366875
Iteration 12200: Loss = -11321.526419189364
Iteration 12300: Loss = -11321.584522447183
1
Iteration 12400: Loss = -11321.52639254657
Iteration 12500: Loss = -11321.525072889926
Iteration 12600: Loss = -11321.5164642172
Iteration 12700: Loss = -11321.516121899022
Iteration 12800: Loss = -11321.516114798938
Iteration 12900: Loss = -11321.517961538208
1
Iteration 13000: Loss = -11321.514768779643
Iteration 13100: Loss = -11321.524936713733
1
Iteration 13200: Loss = -11321.522377700441
2
Iteration 13300: Loss = -11321.525495020247
3
Iteration 13400: Loss = -11321.512743195677
Iteration 13500: Loss = -11321.509761199522
Iteration 13600: Loss = -11321.50976300677
Iteration 13700: Loss = -11321.510626887155
1
Iteration 13800: Loss = -11321.509754960984
Iteration 13900: Loss = -11321.888007520132
1
Iteration 14000: Loss = -11321.50973465326
Iteration 14100: Loss = -11321.509738412711
Iteration 14200: Loss = -11321.590619275265
1
Iteration 14300: Loss = -11320.461662364425
Iteration 14400: Loss = -11320.461296200652
Iteration 14500: Loss = -11320.466825514532
1
Iteration 14600: Loss = -11320.461335537255
Iteration 14700: Loss = -11320.461289114086
Iteration 14800: Loss = -11320.728363438324
1
Iteration 14900: Loss = -11320.461282902806
Iteration 15000: Loss = -11320.496747060633
1
Iteration 15100: Loss = -11320.461313093836
Iteration 15200: Loss = -11320.549296504778
1
Iteration 15300: Loss = -11320.461325280641
Iteration 15400: Loss = -11320.467050907075
1
Iteration 15500: Loss = -11320.461302968475
Iteration 15600: Loss = -11320.462816304958
1
Iteration 15700: Loss = -11320.45978052457
Iteration 15800: Loss = -11320.460921606082
1
Iteration 15900: Loss = -11320.45978352866
Iteration 16000: Loss = -11320.473888037423
1
Iteration 16100: Loss = -11320.459564585208
Iteration 16200: Loss = -11320.45953552096
Iteration 16300: Loss = -11320.608032797678
1
Iteration 16400: Loss = -11320.459548422976
Iteration 16500: Loss = -11320.459499394716
Iteration 16600: Loss = -11320.345049661733
Iteration 16700: Loss = -11320.317190739714
Iteration 16800: Loss = -11320.317266382086
Iteration 16900: Loss = -11320.31721420122
Iteration 17000: Loss = -11320.317874740946
1
Iteration 17100: Loss = -11320.333638749955
2
Iteration 17200: Loss = -11320.441135551508
3
Iteration 17300: Loss = -11320.316422484624
Iteration 17400: Loss = -11320.316049295818
Iteration 17500: Loss = -11320.316796880621
1
Iteration 17600: Loss = -11320.334001419169
2
Iteration 17700: Loss = -11320.316853460701
3
Iteration 17800: Loss = -11320.31605367589
Iteration 17900: Loss = -11320.344765089234
1
Iteration 18000: Loss = -11320.316033248893
Iteration 18100: Loss = -11320.461282734444
1
Iteration 18200: Loss = -11320.316028772964
Iteration 18300: Loss = -11320.316026674183
Iteration 18400: Loss = -11320.316038505174
Iteration 18500: Loss = -11320.31602773825
Iteration 18600: Loss = -11320.316044695976
Iteration 18700: Loss = -11320.316296304647
1
Iteration 18800: Loss = -11320.316010376595
Iteration 18900: Loss = -11320.316021615392
Iteration 19000: Loss = -11320.32864833158
1
Iteration 19100: Loss = -11320.316035424365
Iteration 19200: Loss = -11320.31595579902
Iteration 19300: Loss = -11320.347174031444
1
Iteration 19400: Loss = -11320.315961542774
Iteration 19500: Loss = -11320.316259977373
1
Iteration 19600: Loss = -11320.34773648763
2
Iteration 19700: Loss = -11320.31596903638
Iteration 19800: Loss = -11320.31983192173
1
Iteration 19900: Loss = -11320.323013218947
2
pi: tensor([[0.7706, 0.2294],
        [0.2471, 0.7529]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5453, 0.4547], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2964, 0.1062],
         [0.7114, 0.2016]],

        [[0.6809, 0.0922],
         [0.5781, 0.5108]],

        [[0.6809, 0.1000],
         [0.6240, 0.6535]],

        [[0.6650, 0.1049],
         [0.6718, 0.6685]],

        [[0.7106, 0.1166],
         [0.5562, 0.6446]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080762963757459
Global Adjusted Rand Index: 0.9137634276656456
Average Adjusted Rand Index: 0.9145856788277331
11353.179542585178
[0.03341228743467, 0.9137634276656456] [0.8825812257053997, 0.9145856788277331] [11367.167377890457, 11320.31818932058]
-------------------------------------
This iteration is 80
True Objective function: Loss = -11343.04741931403
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20974.261897966946
Iteration 100: Loss = -11654.211431331796
Iteration 200: Loss = -11650.52462573589
Iteration 300: Loss = -11645.501671066806
Iteration 400: Loss = -11623.432678804744
Iteration 500: Loss = -11471.009909158998
Iteration 600: Loss = -11354.49177009829
Iteration 700: Loss = -11333.726103601872
Iteration 800: Loss = -11332.047353724594
Iteration 900: Loss = -11331.87982527415
Iteration 1000: Loss = -11331.779297032
Iteration 1100: Loss = -11331.711643075507
Iteration 1200: Loss = -11331.662450939599
Iteration 1300: Loss = -11331.544893447708
Iteration 1400: Loss = -11331.207114753966
Iteration 1500: Loss = -11331.161111954332
Iteration 1600: Loss = -11331.105734782317
Iteration 1700: Loss = -11331.092708981356
Iteration 1800: Loss = -11331.081767783742
Iteration 1900: Loss = -11331.072310643614
Iteration 2000: Loss = -11331.064297164909
Iteration 2100: Loss = -11331.05524540494
Iteration 2200: Loss = -11330.259966403424
Iteration 2300: Loss = -11330.250988480293
Iteration 2400: Loss = -11330.24450122868
Iteration 2500: Loss = -11330.237238493964
Iteration 2600: Loss = -11330.22817890626
Iteration 2700: Loss = -11330.219849545525
Iteration 2800: Loss = -11330.214709197224
Iteration 2900: Loss = -11330.21150936985
Iteration 3000: Loss = -11330.208702028109
Iteration 3100: Loss = -11330.20545237867
Iteration 3200: Loss = -11330.201482114573
Iteration 3300: Loss = -11330.197190407971
Iteration 3400: Loss = -11330.192431844089
Iteration 3500: Loss = -11330.18882783313
Iteration 3600: Loss = -11330.175314112217
Iteration 3700: Loss = -11329.517158541588
Iteration 3800: Loss = -11329.513448095895
Iteration 3900: Loss = -11329.507810089903
Iteration 4000: Loss = -11329.525616348174
1
Iteration 4100: Loss = -11329.503044909346
Iteration 4200: Loss = -11329.502217125579
Iteration 4300: Loss = -11329.501481776715
Iteration 4400: Loss = -11329.500684865085
Iteration 4500: Loss = -11329.50146046778
1
Iteration 4600: Loss = -11329.498542046123
Iteration 4700: Loss = -11329.498634763871
Iteration 4800: Loss = -11329.496876869463
Iteration 4900: Loss = -11329.489151161752
Iteration 5000: Loss = -11329.488879747149
Iteration 5100: Loss = -11329.480693475956
Iteration 5200: Loss = -11329.479157544201
Iteration 5300: Loss = -11329.481849708245
1
Iteration 5400: Loss = -11329.481289174835
2
Iteration 5500: Loss = -11329.477089337859
Iteration 5600: Loss = -11329.476572406866
Iteration 5700: Loss = -11329.47636819589
Iteration 5800: Loss = -11329.476272774638
Iteration 5900: Loss = -11329.476711020572
1
Iteration 6000: Loss = -11329.475639771092
Iteration 6100: Loss = -11329.475529572836
Iteration 6200: Loss = -11329.475802911558
1
Iteration 6300: Loss = -11329.476170618458
2
Iteration 6400: Loss = -11329.475135635119
Iteration 6500: Loss = -11329.47940620504
1
Iteration 6600: Loss = -11329.483155314872
2
Iteration 6700: Loss = -11329.47487787539
Iteration 6800: Loss = -11329.474657315684
Iteration 6900: Loss = -11329.478179444093
1
Iteration 7000: Loss = -11329.474470129631
Iteration 7100: Loss = -11329.474402133834
Iteration 7200: Loss = -11329.491268495902
1
Iteration 7300: Loss = -11329.4746195566
2
Iteration 7400: Loss = -11329.474177368422
Iteration 7500: Loss = -11329.474482511607
1
Iteration 7600: Loss = -11329.475123344126
2
Iteration 7700: Loss = -11329.474072951434
Iteration 7800: Loss = -11329.47394904432
Iteration 7900: Loss = -11329.646094353384
1
Iteration 8000: Loss = -11329.47384615307
Iteration 8100: Loss = -11329.473957669283
1
Iteration 8200: Loss = -11329.473790532866
Iteration 8300: Loss = -11329.473700148475
Iteration 8400: Loss = -11329.4747251391
1
Iteration 8500: Loss = -11329.473598846289
Iteration 8600: Loss = -11329.601650247701
1
Iteration 8700: Loss = -11329.473315546593
Iteration 8800: Loss = -11329.472477217765
Iteration 8900: Loss = -11329.475398336772
1
Iteration 9000: Loss = -11329.472339466196
Iteration 9100: Loss = -11329.472246832709
Iteration 9200: Loss = -11329.475077382947
1
Iteration 9300: Loss = -11329.472137915458
Iteration 9400: Loss = -11329.471440055977
Iteration 9500: Loss = -11329.458510479924
Iteration 9600: Loss = -11329.42269911121
Iteration 9700: Loss = -11329.422405606678
Iteration 9800: Loss = -11329.42695149431
1
Iteration 9900: Loss = -11329.435541426603
2
Iteration 10000: Loss = -11329.422276313166
Iteration 10100: Loss = -11329.42291968609
1
Iteration 10200: Loss = -11329.422287550764
Iteration 10300: Loss = -11329.422299213811
Iteration 10400: Loss = -11329.66567738478
1
Iteration 10500: Loss = -11329.431511662453
2
Iteration 10600: Loss = -11329.584749845812
3
Iteration 10700: Loss = -11329.421051340028
Iteration 10800: Loss = -11329.421241800723
1
Iteration 10900: Loss = -11329.444644100595
2
Iteration 11000: Loss = -11329.42148356764
3
Iteration 11100: Loss = -11329.421538970746
4
Iteration 11200: Loss = -11329.426625532597
5
Iteration 11300: Loss = -11329.423232654335
6
Iteration 11400: Loss = -11329.42448940533
7
Iteration 11500: Loss = -11329.420900343708
Iteration 11600: Loss = -11329.423589689131
1
Iteration 11700: Loss = -11329.420874746405
Iteration 11800: Loss = -11329.432246981676
1
Iteration 11900: Loss = -11329.420850691648
Iteration 12000: Loss = -11329.803717608947
1
Iteration 12100: Loss = -11329.420832639338
Iteration 12200: Loss = -11329.420831055782
Iteration 12300: Loss = -11329.420848427393
Iteration 12400: Loss = -11329.42143336609
1
Iteration 12500: Loss = -11329.425733880278
2
Iteration 12600: Loss = -11329.420797065228
Iteration 12700: Loss = -11329.424660163615
1
Iteration 12800: Loss = -11329.42183108398
2
Iteration 12900: Loss = -11329.421163817799
3
Iteration 13000: Loss = -11329.430284490869
4
Iteration 13100: Loss = -11329.419213854435
Iteration 13200: Loss = -11329.419475943749
1
Iteration 13300: Loss = -11329.429815784177
2
Iteration 13400: Loss = -11329.41906255037
Iteration 13500: Loss = -11329.529673114059
1
Iteration 13600: Loss = -11329.419072282375
Iteration 13700: Loss = -11329.432588027095
1
Iteration 13800: Loss = -11329.419042583928
Iteration 13900: Loss = -11329.421109188079
1
Iteration 14000: Loss = -11329.419044339522
Iteration 14100: Loss = -11329.451487824685
1
Iteration 14200: Loss = -11329.419028552682
Iteration 14300: Loss = -11329.422836263606
1
Iteration 14400: Loss = -11329.41930693156
2
Iteration 14500: Loss = -11329.41917516094
3
Iteration 14600: Loss = -11329.424712351063
4
Iteration 14700: Loss = -11329.419013078781
Iteration 14800: Loss = -11329.431333112147
1
Iteration 14900: Loss = -11329.418993863803
Iteration 15000: Loss = -11329.423125284304
1
Iteration 15100: Loss = -11329.41943715508
2
Iteration 15200: Loss = -11329.419923827027
3
Iteration 15300: Loss = -11329.419808454253
4
Iteration 15400: Loss = -11329.654602105087
5
Iteration 15500: Loss = -11329.418246751773
Iteration 15600: Loss = -11329.422121289384
1
Iteration 15700: Loss = -11329.418321798341
Iteration 15800: Loss = -11329.418234519684
Iteration 15900: Loss = -11329.41872134405
1
Iteration 16000: Loss = -11329.41841764178
2
Iteration 16100: Loss = -11329.425053542438
3
Iteration 16200: Loss = -11329.419056156583
4
Iteration 16300: Loss = -11329.419142515664
5
Iteration 16400: Loss = -11329.434636089483
6
Iteration 16500: Loss = -11329.418495258378
7
Iteration 16600: Loss = -11329.419816213795
8
Iteration 16700: Loss = -11329.418834560847
9
Iteration 16800: Loss = -11329.418328030033
Iteration 16900: Loss = -11329.419359855465
1
Iteration 17000: Loss = -11329.418739620241
2
Iteration 17100: Loss = -11329.633390960731
3
Iteration 17200: Loss = -11329.417513084574
Iteration 17300: Loss = -11329.451645721114
1
Iteration 17400: Loss = -11329.420245475947
2
Iteration 17500: Loss = -11329.418104670442
3
Iteration 17600: Loss = -11329.458011079345
4
Iteration 17700: Loss = -11329.41792892867
5
Iteration 17800: Loss = -11329.417811071906
6
Iteration 17900: Loss = -11329.436713457342
7
Iteration 18000: Loss = -11329.430373897447
8
Iteration 18100: Loss = -11329.418947007744
9
Iteration 18200: Loss = -11329.417350797341
Iteration 18300: Loss = -11329.418656782413
1
Iteration 18400: Loss = -11329.418794492274
2
Iteration 18500: Loss = -11329.42975907691
3
Iteration 18600: Loss = -11329.419091370943
4
Iteration 18700: Loss = -11329.418259485377
5
Iteration 18800: Loss = -11329.424859389017
6
Iteration 18900: Loss = -11329.490661427728
7
Iteration 19000: Loss = -11329.417424322446
Iteration 19100: Loss = -11329.421789407968
1
Iteration 19200: Loss = -11329.419668621498
2
Iteration 19300: Loss = -11329.419512931576
3
Iteration 19400: Loss = -11329.417291212521
Iteration 19500: Loss = -11329.417436875276
1
Iteration 19600: Loss = -11329.582922012933
2
Iteration 19700: Loss = -11329.417628248699
3
Iteration 19800: Loss = -11329.418964824046
4
Iteration 19900: Loss = -11329.417886299769
5
pi: tensor([[0.6967, 0.3033],
        [0.2539, 0.7461]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4477, 0.5523], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2024, 0.0941],
         [0.7142, 0.2974]],

        [[0.5416, 0.1017],
         [0.6978, 0.7197]],

        [[0.5507, 0.1036],
         [0.6876, 0.5357]],

        [[0.5536, 0.0991],
         [0.5825, 0.5146]],

        [[0.6915, 0.1054],
         [0.7058, 0.6239]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.9760953167449541
Average Adjusted Rand Index: 0.9761618721313594
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20878.195723848057
Iteration 100: Loss = -11653.736410829568
Iteration 200: Loss = -11650.83702470712
Iteration 300: Loss = -11648.614932575161
Iteration 400: Loss = -11645.587386496993
Iteration 500: Loss = -11627.008430598888
Iteration 600: Loss = -11489.321130743514
Iteration 700: Loss = -11388.606387296786
Iteration 800: Loss = -11359.307715446354
Iteration 900: Loss = -11356.902699101554
Iteration 1000: Loss = -11346.522937891663
Iteration 1100: Loss = -11346.027917326532
Iteration 1200: Loss = -11335.827214800009
Iteration 1300: Loss = -11331.89601750598
Iteration 1400: Loss = -11331.836893586917
Iteration 1500: Loss = -11331.800956429266
Iteration 1600: Loss = -11331.765368025171
Iteration 1700: Loss = -11331.0414767326
Iteration 1800: Loss = -11329.777315181627
Iteration 1900: Loss = -11329.751526274746
Iteration 2000: Loss = -11329.652662243449
Iteration 2100: Loss = -11329.62301789286
Iteration 2200: Loss = -11329.565214030934
Iteration 2300: Loss = -11329.543356366501
Iteration 2400: Loss = -11329.531632887889
Iteration 2500: Loss = -11329.53996776331
1
Iteration 2600: Loss = -11329.523233659058
Iteration 2700: Loss = -11329.516034014685
Iteration 2800: Loss = -11329.521746020539
1
Iteration 2900: Loss = -11329.504940242148
Iteration 3000: Loss = -11329.502041078209
Iteration 3100: Loss = -11329.49969060287
Iteration 3200: Loss = -11329.49730967703
Iteration 3300: Loss = -11329.497155728961
Iteration 3400: Loss = -11329.495479062954
Iteration 3500: Loss = -11329.49366638108
Iteration 3600: Loss = -11329.493604186213
Iteration 3700: Loss = -11329.49189880469
Iteration 3800: Loss = -11329.491087012018
Iteration 3900: Loss = -11329.489019568467
Iteration 4000: Loss = -11329.490403131635
1
Iteration 4100: Loss = -11329.497628680521
2
Iteration 4200: Loss = -11329.486756413504
Iteration 4300: Loss = -11329.483824375844
Iteration 4400: Loss = -11329.453502876799
Iteration 4500: Loss = -11329.452838805008
Iteration 4600: Loss = -11329.45289719571
Iteration 4700: Loss = -11329.467673620871
1
Iteration 4800: Loss = -11329.452377369642
Iteration 4900: Loss = -11329.451261534967
Iteration 5000: Loss = -11329.44892822048
Iteration 5100: Loss = -11329.454939961883
1
Iteration 5200: Loss = -11329.44634983281
Iteration 5300: Loss = -11329.440156537066
Iteration 5400: Loss = -11329.439045870553
Iteration 5500: Loss = -11329.438581095268
Iteration 5600: Loss = -11329.438947089595
1
Iteration 5700: Loss = -11329.441298237398
2
Iteration 5800: Loss = -11329.445908128107
3
Iteration 5900: Loss = -11329.447855574012
4
Iteration 6000: Loss = -11329.437559011938
Iteration 6100: Loss = -11329.438389501776
1
Iteration 6200: Loss = -11329.437145562046
Iteration 6300: Loss = -11329.43838400203
1
Iteration 6400: Loss = -11329.43676672365
Iteration 6500: Loss = -11329.454730309835
1
Iteration 6600: Loss = -11329.435738898597
Iteration 6700: Loss = -11329.433401376054
Iteration 6800: Loss = -11329.435740869092
1
Iteration 6900: Loss = -11329.432747169722
Iteration 7000: Loss = -11329.432578965685
Iteration 7100: Loss = -11329.433934441115
1
Iteration 7200: Loss = -11329.430638301332
Iteration 7300: Loss = -11329.427546731291
Iteration 7400: Loss = -11329.42181626622
Iteration 7500: Loss = -11329.421612734037
Iteration 7600: Loss = -11329.438391093538
1
Iteration 7700: Loss = -11329.421456470722
Iteration 7800: Loss = -11329.421388175073
Iteration 7900: Loss = -11329.422114905483
1
Iteration 8000: Loss = -11329.421293287463
Iteration 8100: Loss = -11329.421215282597
Iteration 8200: Loss = -11329.425691894345
1
Iteration 8300: Loss = -11329.42112639754
Iteration 8400: Loss = -11329.420945961481
Iteration 8500: Loss = -11329.452381342016
1
Iteration 8600: Loss = -11329.420781183873
Iteration 8700: Loss = -11329.42079853424
Iteration 8800: Loss = -11329.424069276052
1
Iteration 8900: Loss = -11329.42077066435
Iteration 9000: Loss = -11329.420705610502
Iteration 9100: Loss = -11329.421355184932
1
Iteration 9200: Loss = -11329.420610518144
Iteration 9300: Loss = -11329.422060921936
1
Iteration 9400: Loss = -11329.420597349119
Iteration 9500: Loss = -11329.420582390365
Iteration 9600: Loss = -11329.420821864162
1
Iteration 9700: Loss = -11329.420474930503
Iteration 9800: Loss = -11329.413816569673
Iteration 9900: Loss = -11329.404815121672
Iteration 10000: Loss = -11329.406138004071
1
Iteration 10100: Loss = -11329.435546054641
2
Iteration 10200: Loss = -11329.552236684685
3
Iteration 10300: Loss = -11329.405219790935
4
Iteration 10400: Loss = -11329.404626093869
Iteration 10500: Loss = -11329.416103261276
1
Iteration 10600: Loss = -11329.404540032969
Iteration 10700: Loss = -11329.406307358035
1
Iteration 10800: Loss = -11329.404514273492
Iteration 10900: Loss = -11329.41808310965
1
Iteration 11000: Loss = -11329.404472643684
Iteration 11100: Loss = -11329.40502113899
1
Iteration 11200: Loss = -11329.404475905765
Iteration 11300: Loss = -11329.404430783128
Iteration 11400: Loss = -11329.460095571101
1
Iteration 11500: Loss = -11329.404448049005
Iteration 11600: Loss = -11329.404390343694
Iteration 11700: Loss = -11329.42069090506
1
Iteration 11800: Loss = -11329.404388200597
Iteration 11900: Loss = -11329.404389234089
Iteration 12000: Loss = -11329.404491546386
1
Iteration 12100: Loss = -11329.404360638535
Iteration 12200: Loss = -11329.404405564208
Iteration 12300: Loss = -11329.465062342135
1
Iteration 12400: Loss = -11329.404422395768
Iteration 12500: Loss = -11329.40465178038
1
Iteration 12600: Loss = -11329.479673576976
2
Iteration 12700: Loss = -11329.40434596922
Iteration 12800: Loss = -11329.404672025677
1
Iteration 12900: Loss = -11329.454587514356
2
Iteration 13000: Loss = -11329.405729162385
3
Iteration 13100: Loss = -11329.44764821033
4
Iteration 13200: Loss = -11329.404261565065
Iteration 13300: Loss = -11329.404283700727
Iteration 13400: Loss = -11329.634365519685
1
Iteration 13500: Loss = -11329.40421317581
Iteration 13600: Loss = -11329.40522065978
1
Iteration 13700: Loss = -11329.40501823003
2
Iteration 13800: Loss = -11329.410299538773
3
Iteration 13900: Loss = -11329.432052012076
4
Iteration 14000: Loss = -11329.407716482643
5
Iteration 14100: Loss = -11329.404675827207
6
Iteration 14200: Loss = -11329.404516623697
7
Iteration 14300: Loss = -11329.40536693127
8
Iteration 14400: Loss = -11329.404237944775
Iteration 14500: Loss = -11329.404776935904
1
Iteration 14600: Loss = -11329.439713957247
2
Iteration 14700: Loss = -11329.404256255812
Iteration 14800: Loss = -11329.410450441983
1
Iteration 14900: Loss = -11329.404197697162
Iteration 15000: Loss = -11329.407132259357
1
Iteration 15100: Loss = -11329.404171256558
Iteration 15200: Loss = -11329.427134113248
1
Iteration 15300: Loss = -11329.404183519078
Iteration 15400: Loss = -11329.461886215042
1
Iteration 15500: Loss = -11329.404159704985
Iteration 15600: Loss = -11329.40413208887
Iteration 15700: Loss = -11329.40419194688
Iteration 15800: Loss = -11329.403347264179
Iteration 15900: Loss = -11329.403310705857
Iteration 16000: Loss = -11329.403423846768
1
Iteration 16100: Loss = -11329.403310494858
Iteration 16200: Loss = -11329.403302545961
Iteration 16300: Loss = -11329.403478679193
1
Iteration 16400: Loss = -11329.403209121996
Iteration 16500: Loss = -11329.40361941605
1
Iteration 16600: Loss = -11329.403224767646
Iteration 16700: Loss = -11329.403512499743
1
Iteration 16800: Loss = -11329.462715217773
2
Iteration 16900: Loss = -11329.40316817937
Iteration 17000: Loss = -11329.404050178722
1
Iteration 17100: Loss = -11329.407365563193
2
Iteration 17200: Loss = -11329.403386321232
3
Iteration 17300: Loss = -11329.40447480536
4
Iteration 17400: Loss = -11329.403622656318
5
Iteration 17500: Loss = -11329.403195259438
Iteration 17600: Loss = -11329.407545504519
1
Iteration 17700: Loss = -11329.402395319083
Iteration 17800: Loss = -11329.405196726035
1
Iteration 17900: Loss = -11329.401956665686
Iteration 18000: Loss = -11329.416446111287
1
Iteration 18100: Loss = -11329.650123812551
2
Iteration 18200: Loss = -11329.402005676622
Iteration 18300: Loss = -11329.402779318654
1
Iteration 18400: Loss = -11329.40764757311
2
Iteration 18500: Loss = -11329.401985203684
Iteration 18600: Loss = -11329.402236888167
1
Iteration 18700: Loss = -11329.401966336905
Iteration 18800: Loss = -11329.43335783324
1
Iteration 18900: Loss = -11329.592619317014
2
Iteration 19000: Loss = -11329.401935524127
Iteration 19100: Loss = -11329.402230795002
1
Iteration 19200: Loss = -11329.401930071428
Iteration 19300: Loss = -11329.402063131796
1
Iteration 19400: Loss = -11329.401921411625
Iteration 19500: Loss = -11329.402369033065
1
Iteration 19600: Loss = -11329.401946893558
Iteration 19700: Loss = -11329.405561094623
1
Iteration 19800: Loss = -11329.401928800055
Iteration 19900: Loss = -11329.401918166255
pi: tensor([[0.6967, 0.3033],
        [0.2539, 0.7461]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4476, 0.5524], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2025, 0.0941],
         [0.7009, 0.2974]],

        [[0.6309, 0.1017],
         [0.6203, 0.5097]],

        [[0.6823, 0.1036],
         [0.5433, 0.7277]],

        [[0.6235, 0.0992],
         [0.5839, 0.6269]],

        [[0.6178, 0.1054],
         [0.6144, 0.6778]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.9760953167449541
Average Adjusted Rand Index: 0.9761618721313594
11343.04741931403
[0.9760953167449541, 0.9760953167449541] [0.9761618721313594, 0.9761618721313594] [11329.417277179955, 11329.402050915141]
-------------------------------------
This iteration is 81
True Objective function: Loss = -11238.079182409829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21711.259206207807
Iteration 100: Loss = -11554.433788959479
Iteration 200: Loss = -11552.0434073604
Iteration 300: Loss = -11540.925518074482
Iteration 400: Loss = -11532.37574707055
Iteration 500: Loss = -11511.984882970411
Iteration 600: Loss = -11475.612231219056
Iteration 700: Loss = -11386.931051904228
Iteration 800: Loss = -11336.270979542878
Iteration 900: Loss = -11330.020541824888
Iteration 1000: Loss = -11320.38304082767
Iteration 1100: Loss = -11320.130941918576
Iteration 1200: Loss = -11319.249631235005
Iteration 1300: Loss = -11319.173958768259
Iteration 1400: Loss = -11319.104612321516
Iteration 1500: Loss = -11318.827405501288
Iteration 1600: Loss = -11317.647838800907
Iteration 1700: Loss = -11317.367127033967
Iteration 1800: Loss = -11317.31605413959
Iteration 1900: Loss = -11316.613445977007
Iteration 2000: Loss = -11316.59420248432
Iteration 2100: Loss = -11316.578023110891
Iteration 2200: Loss = -11316.562863491303
Iteration 2300: Loss = -11316.550444570132
Iteration 2400: Loss = -11316.5388849583
Iteration 2500: Loss = -11316.526899203574
Iteration 2600: Loss = -11316.512659821548
Iteration 2700: Loss = -11316.492776693967
Iteration 2800: Loss = -11316.458723653946
Iteration 2900: Loss = -11316.308698934527
Iteration 3000: Loss = -11316.087107252988
Iteration 3100: Loss = -11254.848966874137
Iteration 3200: Loss = -11254.50176430914
Iteration 3300: Loss = -11216.966466119007
Iteration 3400: Loss = -11216.873347440527
Iteration 3500: Loss = -11216.863570232877
Iteration 3600: Loss = -11216.857553088996
Iteration 3700: Loss = -11216.853121138158
Iteration 3800: Loss = -11216.849599636693
Iteration 3900: Loss = -11216.846854924011
Iteration 4000: Loss = -11216.844434282839
Iteration 4100: Loss = -11216.841391112534
Iteration 4200: Loss = -11216.8409673742
Iteration 4300: Loss = -11214.194951505551
Iteration 4400: Loss = -11214.17324148193
Iteration 4500: Loss = -11214.165008304086
Iteration 4600: Loss = -11214.16975463017
1
Iteration 4700: Loss = -11214.161333958218
Iteration 4800: Loss = -11214.157952751204
Iteration 4900: Loss = -11214.127331537573
Iteration 5000: Loss = -11214.126584661011
Iteration 5100: Loss = -11214.12548986957
Iteration 5200: Loss = -11214.124898040633
Iteration 5300: Loss = -11214.124771751854
Iteration 5400: Loss = -11214.12741957965
1
Iteration 5500: Loss = -11214.123770459006
Iteration 5600: Loss = -11214.123566439854
Iteration 5700: Loss = -11214.122790001813
Iteration 5800: Loss = -11214.121748508598
Iteration 5900: Loss = -11214.119494842504
Iteration 6000: Loss = -11214.117996100947
Iteration 6100: Loss = -11214.118241217298
1
Iteration 6200: Loss = -11214.120087428919
2
Iteration 6300: Loss = -11214.117277420997
Iteration 6400: Loss = -11214.117508898164
1
Iteration 6500: Loss = -11214.119545680755
2
Iteration 6600: Loss = -11214.117422195814
3
Iteration 6700: Loss = -11214.111940317689
Iteration 6800: Loss = -11214.092941815508
Iteration 6900: Loss = -11214.092551135333
Iteration 7000: Loss = -11214.092360040846
Iteration 7100: Loss = -11214.092140014414
Iteration 7200: Loss = -11214.109471891923
1
Iteration 7300: Loss = -11214.091641609682
Iteration 7400: Loss = -11214.091242652117
Iteration 7500: Loss = -11214.092723781376
1
Iteration 7600: Loss = -11214.090568327974
Iteration 7700: Loss = -11214.090168855244
Iteration 7800: Loss = -11214.09235520461
1
Iteration 7900: Loss = -11214.089286953027
Iteration 8000: Loss = -11214.08915504355
Iteration 8100: Loss = -11214.089292338815
1
Iteration 8200: Loss = -11214.089003296578
Iteration 8300: Loss = -11214.088928942336
Iteration 8400: Loss = -11214.088902059857
Iteration 8500: Loss = -11214.08881403544
Iteration 8600: Loss = -11214.089238184517
1
Iteration 8700: Loss = -11214.088726680067
Iteration 8800: Loss = -11214.088702106266
Iteration 8900: Loss = -11214.18854805198
1
Iteration 9000: Loss = -11214.08858808712
Iteration 9100: Loss = -11214.088511994127
Iteration 9200: Loss = -11214.184117549283
1
Iteration 9300: Loss = -11214.088392260051
Iteration 9400: Loss = -11214.111612221192
1
Iteration 9500: Loss = -11214.088057469558
Iteration 9600: Loss = -11214.08788332767
Iteration 9700: Loss = -11214.08806014576
1
Iteration 9800: Loss = -11214.079432568666
Iteration 9900: Loss = -11214.083750090223
1
Iteration 10000: Loss = -11214.01496799857
Iteration 10100: Loss = -11214.001142898873
Iteration 10200: Loss = -11214.000917237476
Iteration 10300: Loss = -11214.007948269207
1
Iteration 10400: Loss = -11213.9978717701
Iteration 10500: Loss = -11213.99799381278
1
Iteration 10600: Loss = -11213.997790376981
Iteration 10700: Loss = -11213.997788764185
Iteration 10800: Loss = -11213.9977611248
Iteration 10900: Loss = -11213.99775101023
Iteration 11000: Loss = -11214.001324623488
1
Iteration 11100: Loss = -11213.999411389623
2
Iteration 11200: Loss = -11213.998003896513
3
Iteration 11300: Loss = -11213.997751844
Iteration 11400: Loss = -11213.999789861102
1
Iteration 11500: Loss = -11213.997400600234
Iteration 11600: Loss = -11213.997142106755
Iteration 11700: Loss = -11213.996885827299
Iteration 11800: Loss = -11213.99821331474
1
Iteration 11900: Loss = -11213.996891657727
Iteration 12000: Loss = -11213.99688943704
Iteration 12100: Loss = -11214.076280146402
1
Iteration 12200: Loss = -11213.996833407622
Iteration 12300: Loss = -11213.99898195999
1
Iteration 12400: Loss = -11213.997061398773
2
Iteration 12500: Loss = -11213.996636279595
Iteration 12600: Loss = -11213.99994282711
1
Iteration 12700: Loss = -11213.996608921823
Iteration 12800: Loss = -11213.996508737562
Iteration 12900: Loss = -11213.996910903776
1
Iteration 13000: Loss = -11213.996443772878
Iteration 13100: Loss = -11213.996328517247
Iteration 13200: Loss = -11213.996728915514
1
Iteration 13300: Loss = -11213.996292883874
Iteration 13400: Loss = -11214.029811103326
1
Iteration 13500: Loss = -11213.996274412539
Iteration 13600: Loss = -11213.996242330273
Iteration 13700: Loss = -11214.007184162258
1
Iteration 13800: Loss = -11213.99609574347
Iteration 13900: Loss = -11213.996091912823
Iteration 14000: Loss = -11213.997510096739
1
Iteration 14100: Loss = -11213.996064385576
Iteration 14200: Loss = -11214.03761325232
1
Iteration 14300: Loss = -11213.996016303925
Iteration 14400: Loss = -11213.996017208743
Iteration 14500: Loss = -11213.996285567157
1
Iteration 14600: Loss = -11213.99743290907
2
Iteration 14700: Loss = -11213.996816234478
3
Iteration 14800: Loss = -11214.024426129059
4
Iteration 14900: Loss = -11213.996017687663
Iteration 15000: Loss = -11213.997686240631
1
Iteration 15100: Loss = -11213.99669690576
2
Iteration 15200: Loss = -11214.001752970362
3
Iteration 15300: Loss = -11213.996555912103
4
Iteration 15400: Loss = -11213.995711344873
Iteration 15500: Loss = -11214.046423845773
1
Iteration 15600: Loss = -11213.995690465006
Iteration 15700: Loss = -11213.996163841231
1
Iteration 15800: Loss = -11213.995689016261
Iteration 15900: Loss = -11213.995647575048
Iteration 16000: Loss = -11213.995750100143
1
Iteration 16100: Loss = -11213.995644123252
Iteration 16200: Loss = -11213.996022378587
1
Iteration 16300: Loss = -11213.995657904065
Iteration 16400: Loss = -11213.99825613058
1
Iteration 16500: Loss = -11213.995629062803
Iteration 16600: Loss = -11214.001528301762
1
Iteration 16700: Loss = -11213.998350908563
2
Iteration 16800: Loss = -11214.016376968935
3
Iteration 16900: Loss = -11213.99564360434
Iteration 17000: Loss = -11213.996446490573
1
Iteration 17100: Loss = -11214.004156516261
2
Iteration 17200: Loss = -11214.010729124093
3
Iteration 17300: Loss = -11214.060278595605
4
Iteration 17400: Loss = -11213.995649801958
Iteration 17500: Loss = -11214.002131031119
1
Iteration 17600: Loss = -11214.005673112739
2
Iteration 17700: Loss = -11213.99612741078
3
Iteration 17800: Loss = -11213.995426992258
Iteration 17900: Loss = -11214.02458431213
1
Iteration 18000: Loss = -11213.995380596172
Iteration 18100: Loss = -11214.22228856196
1
Iteration 18200: Loss = -11213.995370677272
Iteration 18300: Loss = -11214.005311018991
1
Iteration 18400: Loss = -11213.995330645419
Iteration 18500: Loss = -11214.004053882292
1
Iteration 18600: Loss = -11213.995299209422
Iteration 18700: Loss = -11214.01243881243
1
Iteration 18800: Loss = -11213.995316878723
Iteration 18900: Loss = -11213.997424725492
1
Iteration 19000: Loss = -11213.995326928598
Iteration 19100: Loss = -11213.995299595952
Iteration 19200: Loss = -11213.995843642468
1
Iteration 19300: Loss = -11213.995269525081
Iteration 19400: Loss = -11213.995763875748
1
Iteration 19500: Loss = -11213.99562321906
2
Iteration 19600: Loss = -11214.061029478125
3
Iteration 19700: Loss = -11214.020859902936
4
Iteration 19800: Loss = -11214.162733322366
5
Iteration 19900: Loss = -11213.995328183217
pi: tensor([[0.8152, 0.1848],
        [0.2428, 0.7572]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4871, 0.5129], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2969, 0.0928],
         [0.5801, 0.1965]],

        [[0.6729, 0.0998],
         [0.6037, 0.6109]],

        [[0.5779, 0.1053],
         [0.6070, 0.5316]],

        [[0.6351, 0.0996],
         [0.6829, 0.5933]],

        [[0.6337, 0.1051],
         [0.5969, 0.5879]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721181988081385
Global Adjusted Rand Index: 0.913763085966186
Average Adjusted Rand Index: 0.9147453050953225
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21386.183597855816
Iteration 100: Loss = -11553.370327259978
Iteration 200: Loss = -11550.06059162529
Iteration 300: Loss = -11539.65941791858
Iteration 400: Loss = -11522.084173843428
Iteration 500: Loss = -11441.610507841939
Iteration 600: Loss = -11361.057250216849
Iteration 700: Loss = -11347.18603315153
Iteration 800: Loss = -11334.348836138932
Iteration 900: Loss = -11329.382579642197
Iteration 1000: Loss = -11319.237772267204
Iteration 1100: Loss = -11318.997516438367
Iteration 1200: Loss = -11318.837087986061
Iteration 1300: Loss = -11318.756244550204
Iteration 1400: Loss = -11318.458519172647
Iteration 1500: Loss = -11318.422984997671
Iteration 1600: Loss = -11318.40167099298
Iteration 1700: Loss = -11318.3824611801
Iteration 1800: Loss = -11317.338612187681
Iteration 1900: Loss = -11317.196587414104
Iteration 2000: Loss = -11317.18487017983
Iteration 2100: Loss = -11317.17620561044
Iteration 2200: Loss = -11317.169005904494
Iteration 2300: Loss = -11317.16276759199
Iteration 2400: Loss = -11317.15719900885
Iteration 2500: Loss = -11317.152129491998
Iteration 2600: Loss = -11317.146854555214
Iteration 2700: Loss = -11316.497786563472
Iteration 2800: Loss = -11316.447683146102
Iteration 2900: Loss = -11316.443345829366
Iteration 3000: Loss = -11316.439842022392
Iteration 3100: Loss = -11316.437094139532
Iteration 3200: Loss = -11316.434886828843
Iteration 3300: Loss = -11316.43266793741
Iteration 3400: Loss = -11316.435567186893
1
Iteration 3500: Loss = -11316.429071372828
Iteration 3600: Loss = -11316.427546049568
Iteration 3700: Loss = -11316.437566546334
1
Iteration 3800: Loss = -11316.424908793688
Iteration 3900: Loss = -11316.423843322953
Iteration 4000: Loss = -11316.428725303052
1
Iteration 4100: Loss = -11316.42172478834
Iteration 4200: Loss = -11316.420995858736
Iteration 4300: Loss = -11316.422221567964
1
Iteration 4400: Loss = -11316.419203137313
Iteration 4500: Loss = -11316.420215941404
1
Iteration 4600: Loss = -11316.426102799136
2
Iteration 4700: Loss = -11316.416477983437
Iteration 4800: Loss = -11316.41487057049
Iteration 4900: Loss = -11316.41413904772
Iteration 5000: Loss = -11316.415606163755
1
Iteration 5100: Loss = -11316.41308294306
Iteration 5200: Loss = -11316.412638967064
Iteration 5300: Loss = -11316.41219252932
Iteration 5400: Loss = -11316.418151056034
1
Iteration 5500: Loss = -11316.411401706195
Iteration 5600: Loss = -11316.411308949759
Iteration 5700: Loss = -11316.410788120951
Iteration 5800: Loss = -11316.410385054438
Iteration 5900: Loss = -11316.41754191781
1
Iteration 6000: Loss = -11316.409933102268
Iteration 6100: Loss = -11316.409353156681
Iteration 6200: Loss = -11316.409186899424
Iteration 6300: Loss = -11316.41101747568
1
Iteration 6400: Loss = -11316.408284209767
Iteration 6500: Loss = -11316.40796678149
Iteration 6600: Loss = -11316.413789111622
1
Iteration 6700: Loss = -11316.399376313404
Iteration 6800: Loss = -11316.401748235543
1
Iteration 6900: Loss = -11316.398838522731
Iteration 7000: Loss = -11316.39839345077
Iteration 7100: Loss = -11316.29193289331
Iteration 7200: Loss = -11316.288292478395
Iteration 7300: Loss = -11316.278884531674
Iteration 7400: Loss = -11316.278776098507
Iteration 7500: Loss = -11316.286401902378
1
Iteration 7600: Loss = -11316.283976088034
2
Iteration 7700: Loss = -11316.278868682473
Iteration 7800: Loss = -11316.277999828237
Iteration 7900: Loss = -11316.27785652929
Iteration 8000: Loss = -11316.413358712232
1
Iteration 8100: Loss = -11316.277498591959
Iteration 8200: Loss = -11316.423929600087
1
Iteration 8300: Loss = -11316.277181772295
Iteration 8400: Loss = -11316.271625223319
Iteration 8500: Loss = -11316.271134140308
Iteration 8600: Loss = -11316.269660413072
Iteration 8700: Loss = -11316.26763734071
Iteration 8800: Loss = -11316.268982666807
1
Iteration 8900: Loss = -11316.267488250509
Iteration 9000: Loss = -11316.267414113356
Iteration 9100: Loss = -11316.267717042057
1
Iteration 9200: Loss = -11316.26719550347
Iteration 9300: Loss = -11316.525157256687
1
Iteration 9400: Loss = -11316.266897544718
Iteration 9500: Loss = -11316.266867413213
Iteration 9600: Loss = -11316.266837679359
Iteration 9700: Loss = -11316.266488629682
Iteration 9800: Loss = -11316.270488302654
1
Iteration 9900: Loss = -11316.266430415668
Iteration 10000: Loss = -11316.266353316041
Iteration 10100: Loss = -11316.266339289657
Iteration 10200: Loss = -11316.266007460328
Iteration 10300: Loss = -11316.267930847263
1
Iteration 10400: Loss = -11316.265176906558
Iteration 10500: Loss = -11316.265255650107
Iteration 10600: Loss = -11316.265069955281
Iteration 10700: Loss = -11316.26496897813
Iteration 10800: Loss = -11316.264383835423
Iteration 10900: Loss = -11316.262118866965
Iteration 11000: Loss = -11316.262110796088
Iteration 11100: Loss = -11316.262356474836
1
Iteration 11200: Loss = -11316.26205019466
Iteration 11300: Loss = -11316.286058324853
1
Iteration 11400: Loss = -11316.26188749387
Iteration 11500: Loss = -11316.41000005551
1
Iteration 11600: Loss = -11316.261280644589
Iteration 11700: Loss = -11316.260236779437
Iteration 11800: Loss = -11314.956982083468
Iteration 11900: Loss = -11313.003211731826
Iteration 12000: Loss = -11311.188869608812
Iteration 12100: Loss = -11311.127803077423
Iteration 12200: Loss = -11311.085978676922
Iteration 12300: Loss = -11311.0819973117
Iteration 12400: Loss = -11311.081991641835
Iteration 12500: Loss = -11311.0798448397
Iteration 12600: Loss = -11311.079679517505
Iteration 12700: Loss = -11311.0795659251
Iteration 12800: Loss = -11311.079471304502
Iteration 12900: Loss = -11311.079789192103
1
Iteration 13000: Loss = -11311.070410629718
Iteration 13100: Loss = -11311.07991529685
1
Iteration 13200: Loss = -11311.069842656008
Iteration 13300: Loss = -11310.868915652092
Iteration 13400: Loss = -11310.850852035483
Iteration 13500: Loss = -11310.846751627794
Iteration 13600: Loss = -11310.848834728113
1
Iteration 13700: Loss = -11310.846690419306
Iteration 13800: Loss = -11310.846832435544
1
Iteration 13900: Loss = -11310.847841033643
2
Iteration 14000: Loss = -11307.72370643324
Iteration 14100: Loss = -11300.66867928788
Iteration 14200: Loss = -11300.447932521889
Iteration 14300: Loss = -11300.429064908138
Iteration 14400: Loss = -11300.401307056736
Iteration 14500: Loss = -11300.402578430176
1
Iteration 14600: Loss = -11300.37401902464
Iteration 14700: Loss = -11300.363190710497
Iteration 14800: Loss = -11300.366878881257
1
Iteration 14900: Loss = -11300.366411428764
2
Iteration 15000: Loss = -11300.23608550829
Iteration 15100: Loss = -11300.240270766088
1
Iteration 15200: Loss = -11300.235830142046
Iteration 15300: Loss = -11300.236310451635
1
Iteration 15400: Loss = -11300.235860490504
Iteration 15500: Loss = -11300.238845022002
1
Iteration 15600: Loss = -11300.235363243204
Iteration 15700: Loss = -11300.250607896221
1
Iteration 15800: Loss = -11300.235401655735
Iteration 15900: Loss = -11300.234406086716
Iteration 16000: Loss = -11300.25668853781
1
Iteration 16100: Loss = -11300.234242895356
Iteration 16200: Loss = -11300.234324014045
Iteration 16300: Loss = -11300.234716349632
1
Iteration 16400: Loss = -11300.234077837064
Iteration 16500: Loss = -11300.233435014432
Iteration 16600: Loss = -11300.23237320959
Iteration 16700: Loss = -11300.299166619418
1
Iteration 16800: Loss = -11300.232255045305
Iteration 16900: Loss = -11300.232360249627
1
Iteration 17000: Loss = -11300.237813625623
2
Iteration 17100: Loss = -11300.232302141063
Iteration 17200: Loss = -11300.232507702234
1
Iteration 17300: Loss = -11300.239067905917
2
Iteration 17400: Loss = -11300.232252240903
Iteration 17500: Loss = -11300.232914569955
1
Iteration 17600: Loss = -11300.233501311857
2
Iteration 17700: Loss = -11300.236618319466
3
Iteration 17800: Loss = -11300.2323326663
Iteration 17900: Loss = -11300.232870681204
1
Iteration 18000: Loss = -11300.263879908412
2
Iteration 18100: Loss = -11300.227542587927
Iteration 18200: Loss = -11300.247656241989
1
Iteration 18300: Loss = -11300.244845081934
2
Iteration 18400: Loss = -11300.226507360214
Iteration 18500: Loss = -11300.227242283523
1
Iteration 18600: Loss = -11300.2392235871
2
Iteration 18700: Loss = -11300.22629568104
Iteration 18800: Loss = -11300.27668380846
1
Iteration 18900: Loss = -11300.226292457364
Iteration 19000: Loss = -11300.226378015777
Iteration 19100: Loss = -11300.226311975728
Iteration 19200: Loss = -11300.229205290334
1
Iteration 19300: Loss = -11300.230594745026
2
Iteration 19400: Loss = -11300.226370217142
Iteration 19500: Loss = -11300.226385345619
Iteration 19600: Loss = -11300.258103281121
1
Iteration 19700: Loss = -11300.22626743699
Iteration 19800: Loss = -11300.234466894914
1
Iteration 19900: Loss = -11300.226245932383
pi: tensor([[0.6013, 0.3987],
        [0.2774, 0.7226]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6980, 0.3020], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2100, 0.0932],
         [0.6176, 0.2861]],

        [[0.5253, 0.0994],
         [0.5234, 0.5505]],

        [[0.7081, 0.1050],
         [0.6417, 0.6689]],

        [[0.6962, 0.0979],
         [0.5415, 0.5437]],

        [[0.6227, 0.1033],
         [0.5642, 0.6458]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 20
Adjusted Rand Index: 0.3545232273838631
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721248949511927
Global Adjusted Rand Index: 0.4135668456462788
Average Adjusted Rand Index: 0.8018144729518596
11238.079182409829
[0.913763085966186, 0.4135668456462788] [0.9147453050953225, 0.8018144729518596] [11213.995516724413, 11300.226310605334]
-------------------------------------
This iteration is 82
True Objective function: Loss = -11159.609980885527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20778.502420735935
Iteration 100: Loss = -11405.434096249319
Iteration 200: Loss = -11404.277372945551
Iteration 300: Loss = -11402.731820643818
Iteration 400: Loss = -11401.913865995017
Iteration 500: Loss = -11401.1413639815
Iteration 600: Loss = -11400.321720271939
Iteration 700: Loss = -11399.324615260963
Iteration 800: Loss = -11388.645342353226
Iteration 900: Loss = -11243.724635911894
Iteration 1000: Loss = -11213.927654504936
Iteration 1100: Loss = -11210.089591062335
Iteration 1200: Loss = -11207.011671553264
Iteration 1300: Loss = -11206.518499700805
Iteration 1400: Loss = -11199.673248416175
Iteration 1500: Loss = -11199.619384587439
Iteration 1600: Loss = -11199.58539349201
Iteration 1700: Loss = -11199.562577670278
Iteration 1800: Loss = -11199.546002105904
Iteration 1900: Loss = -11199.532806510959
Iteration 2000: Loss = -11199.522312065363
Iteration 2100: Loss = -11199.513938674629
Iteration 2200: Loss = -11199.507211939987
Iteration 2300: Loss = -11199.501610342073
Iteration 2400: Loss = -11199.496751450677
Iteration 2500: Loss = -11199.492457432469
Iteration 2600: Loss = -11199.488681393004
Iteration 2700: Loss = -11199.485266427106
Iteration 2800: Loss = -11199.481980342522
Iteration 2900: Loss = -11199.478356818268
Iteration 3000: Loss = -11199.473729143952
Iteration 3100: Loss = -11199.455906461446
Iteration 3200: Loss = -11198.59659245766
Iteration 3300: Loss = -11198.563812396033
Iteration 3400: Loss = -11198.5509041787
Iteration 3500: Loss = -11198.540901977214
Iteration 3600: Loss = -11198.539470027677
Iteration 3700: Loss = -11198.531905502512
Iteration 3800: Loss = -11198.530196064394
Iteration 3900: Loss = -11198.526985605411
Iteration 4000: Loss = -11198.525176438701
Iteration 4100: Loss = -11198.523857368607
Iteration 4200: Loss = -11198.522073833778
Iteration 4300: Loss = -11198.52447359455
1
Iteration 4400: Loss = -11198.520037963759
Iteration 4500: Loss = -11198.519042015441
Iteration 4600: Loss = -11198.518537096035
Iteration 4700: Loss = -11198.517780073573
Iteration 4800: Loss = -11198.517260654724
Iteration 4900: Loss = -11198.516773207368
Iteration 5000: Loss = -11198.516329834412
Iteration 5100: Loss = -11198.515899789736
Iteration 5200: Loss = -11198.515540244032
Iteration 5300: Loss = -11198.515167250032
Iteration 5400: Loss = -11198.52953217592
1
Iteration 5500: Loss = -11198.514497583124
Iteration 5600: Loss = -11198.514112194081
Iteration 5700: Loss = -11198.515521016325
1
Iteration 5800: Loss = -11198.513196519621
Iteration 5900: Loss = -11198.512845503585
Iteration 6000: Loss = -11198.512608766458
Iteration 6100: Loss = -11198.513093068106
1
Iteration 6200: Loss = -11198.512795091432
2
Iteration 6300: Loss = -11198.51205971055
Iteration 6400: Loss = -11198.511812846997
Iteration 6500: Loss = -11198.51176209537
Iteration 6600: Loss = -11198.511557182212
Iteration 6700: Loss = -11198.511389277552
Iteration 6800: Loss = -11198.511276878822
Iteration 6900: Loss = -11198.511213364058
Iteration 7000: Loss = -11198.511036589161
Iteration 7100: Loss = -11198.510879567722
Iteration 7200: Loss = -11198.510752735316
Iteration 7300: Loss = -11198.510338378754
Iteration 7400: Loss = -11198.461742479156
Iteration 7500: Loss = -11198.483755845464
1
Iteration 7600: Loss = -11198.460222478132
Iteration 7700: Loss = -11198.460177280936
Iteration 7800: Loss = -11198.460115291518
Iteration 7900: Loss = -11198.460053567342
Iteration 8000: Loss = -11198.459980643693
Iteration 8100: Loss = -11198.459882160254
Iteration 8200: Loss = -11198.459758895082
Iteration 8300: Loss = -11198.45957670382
Iteration 8400: Loss = -11198.459655414077
Iteration 8500: Loss = -11198.459421020503
Iteration 8600: Loss = -11198.459363663835
Iteration 8700: Loss = -11198.45953823429
1
Iteration 8800: Loss = -11198.45930818097
Iteration 8900: Loss = -11198.461175676135
1
Iteration 9000: Loss = -11198.459224869766
Iteration 9100: Loss = -11198.817431941321
1
Iteration 9200: Loss = -11198.45921037879
Iteration 9300: Loss = -11198.45920125529
Iteration 9400: Loss = -11198.46117167515
1
Iteration 9500: Loss = -11198.459131115722
Iteration 9600: Loss = -11198.459151103752
Iteration 9700: Loss = -11198.460952167701
1
Iteration 9800: Loss = -11198.459084467368
Iteration 9900: Loss = -11198.460167935946
1
Iteration 10000: Loss = -11198.463028932354
2
Iteration 10100: Loss = -11198.459273259557
3
Iteration 10200: Loss = -11198.459170875276
Iteration 10300: Loss = -11198.46050564689
1
Iteration 10400: Loss = -11198.459210284565
Iteration 10500: Loss = -11198.458888105379
Iteration 10600: Loss = -11198.474334419105
1
Iteration 10700: Loss = -11198.45878067268
Iteration 10800: Loss = -11198.458735229236
Iteration 10900: Loss = -11198.460814234386
1
Iteration 11000: Loss = -11198.45846562792
Iteration 11100: Loss = -11198.452648141634
Iteration 11200: Loss = -11198.453193189627
1
Iteration 11300: Loss = -11198.452360916941
Iteration 11400: Loss = -11198.452237507352
Iteration 11500: Loss = -11198.467856078829
1
Iteration 11600: Loss = -11198.452178313615
Iteration 11700: Loss = -11198.452199961548
Iteration 11800: Loss = -11199.075126096086
1
Iteration 11900: Loss = -11198.452169746059
Iteration 12000: Loss = -11198.452159327624
Iteration 12100: Loss = -11198.452158931785
Iteration 12200: Loss = -11198.452275414777
1
Iteration 12300: Loss = -11198.452055416008
Iteration 12400: Loss = -11198.452259190377
1
Iteration 12500: Loss = -11198.452047953559
Iteration 12600: Loss = -11198.452889441574
1
Iteration 12700: Loss = -11198.45200478032
Iteration 12800: Loss = -11198.45292727899
1
Iteration 12900: Loss = -11198.45204081531
Iteration 13000: Loss = -11198.460790828723
1
Iteration 13100: Loss = -11198.452002324515
Iteration 13200: Loss = -11198.452005582998
Iteration 13300: Loss = -11198.452062436581
Iteration 13400: Loss = -11198.45194677469
Iteration 13500: Loss = -11198.451463602978
Iteration 13600: Loss = -11198.451457875184
Iteration 13700: Loss = -11198.451393153731
Iteration 13800: Loss = -11198.451713568222
1
Iteration 13900: Loss = -11198.451446726058
Iteration 14000: Loss = -11198.451383160114
Iteration 14100: Loss = -11198.45137511647
Iteration 14200: Loss = -11198.488501289614
1
Iteration 14300: Loss = -11198.451409431646
Iteration 14400: Loss = -11198.455098965855
1
Iteration 14500: Loss = -11198.47205157239
2
Iteration 14600: Loss = -11198.45141606831
Iteration 14700: Loss = -11198.451492060356
Iteration 14800: Loss = -11198.546257350596
1
Iteration 14900: Loss = -11198.451375782013
Iteration 15000: Loss = -11198.472447483053
1
Iteration 15100: Loss = -11198.451351849468
Iteration 15200: Loss = -11198.643478404072
1
Iteration 15300: Loss = -11198.450748091349
Iteration 15400: Loss = -11198.450732233237
Iteration 15500: Loss = -11198.451222262005
1
Iteration 15600: Loss = -11198.450741191498
Iteration 15700: Loss = -11198.454154093806
1
Iteration 15800: Loss = -11198.450777872204
Iteration 15900: Loss = -11198.450718223885
Iteration 16000: Loss = -11198.4904692328
1
Iteration 16100: Loss = -11198.450740875485
Iteration 16200: Loss = -11198.450705150215
Iteration 16300: Loss = -11198.454065644128
1
Iteration 16400: Loss = -11198.450734204136
Iteration 16500: Loss = -11198.450705670097
Iteration 16600: Loss = -11198.450812639563
1
Iteration 16700: Loss = -11198.450685812615
Iteration 16800: Loss = -11198.457841875776
1
Iteration 16900: Loss = -11198.45070329663
Iteration 17000: Loss = -11198.450846633927
1
Iteration 17100: Loss = -11198.450720100682
Iteration 17200: Loss = -11198.450677946948
Iteration 17300: Loss = -11198.453143118017
1
Iteration 17400: Loss = -11198.450683209821
Iteration 17500: Loss = -11198.468839731
1
Iteration 17600: Loss = -11198.450723303433
Iteration 17700: Loss = -11198.450677954048
Iteration 17800: Loss = -11198.455524712434
1
Iteration 17900: Loss = -11198.450674097841
Iteration 18000: Loss = -11198.811599024428
1
Iteration 18100: Loss = -11198.450670149607
Iteration 18200: Loss = -11198.450682272218
Iteration 18300: Loss = -11198.450758416244
Iteration 18400: Loss = -11198.450671267086
Iteration 18500: Loss = -11198.45175752273
1
Iteration 18600: Loss = -11198.450656135428
Iteration 18700: Loss = -11198.45614519228
1
Iteration 18800: Loss = -11198.450657069612
Iteration 18900: Loss = -11198.490885309664
1
Iteration 19000: Loss = -11198.450647854463
Iteration 19100: Loss = -11198.450650519571
Iteration 19200: Loss = -11198.450729324408
Iteration 19300: Loss = -11198.45066334059
Iteration 19400: Loss = -11198.452447854603
1
Iteration 19500: Loss = -11198.45064220689
Iteration 19600: Loss = -11198.513980579026
1
Iteration 19700: Loss = -11198.450643013877
Iteration 19800: Loss = -11198.450654678729
Iteration 19900: Loss = -11198.452450200804
1
pi: tensor([[0.7018, 0.2982],
        [0.2706, 0.7294]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9789, 0.0211], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1862, 0.2791],
         [0.5624, 0.3094]],

        [[0.7052, 0.1011],
         [0.6154, 0.6171]],

        [[0.6837, 0.1058],
         [0.5671, 0.6244]],

        [[0.5951, 0.0912],
         [0.5647, 0.5273]],

        [[0.6888, 0.1058],
         [0.7208, 0.5971]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8073322325697639
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.5827568095325322
Average Adjusted Rand Index: 0.7147599473650765
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23074.02648450949
Iteration 100: Loss = -11405.38486900831
Iteration 200: Loss = -11402.85198953692
Iteration 300: Loss = -11402.12720051276
Iteration 400: Loss = -11401.481974802213
Iteration 500: Loss = -11400.727518922287
Iteration 600: Loss = -11399.213358395598
Iteration 700: Loss = -11324.336027394453
Iteration 800: Loss = -11156.19317777251
Iteration 900: Loss = -11151.080653147568
Iteration 1000: Loss = -11150.723844559087
Iteration 1100: Loss = -11147.542242721102
Iteration 1200: Loss = -11147.494942393796
Iteration 1300: Loss = -11140.020168889434
Iteration 1400: Loss = -11139.998053622325
Iteration 1500: Loss = -11139.980845855469
Iteration 1600: Loss = -11139.946165071171
Iteration 1700: Loss = -11139.92325965146
Iteration 1800: Loss = -11139.915215264016
Iteration 1900: Loss = -11139.908688722267
Iteration 2000: Loss = -11139.902870223648
Iteration 2100: Loss = -11139.889776158636
Iteration 2200: Loss = -11139.872285131702
Iteration 2300: Loss = -11139.86849466651
Iteration 2400: Loss = -11139.866496506065
Iteration 2500: Loss = -11139.858137879228
Iteration 2600: Loss = -11139.002611426686
Iteration 2700: Loss = -11138.999452938786
Iteration 2800: Loss = -11138.997477853527
Iteration 2900: Loss = -11138.995388343104
Iteration 3000: Loss = -11138.993682905339
Iteration 3100: Loss = -11138.992090711325
Iteration 3200: Loss = -11138.99046912438
Iteration 3300: Loss = -11138.98713149769
Iteration 3400: Loss = -11137.385308554807
Iteration 3500: Loss = -11137.3380270537
Iteration 3600: Loss = -11137.207219918688
Iteration 3700: Loss = -11137.20589919869
Iteration 3800: Loss = -11137.20521695324
Iteration 3900: Loss = -11137.20471834987
Iteration 4000: Loss = -11137.204236169808
Iteration 4100: Loss = -11137.203704535334
Iteration 4200: Loss = -11137.205544325088
1
Iteration 4300: Loss = -11137.2014341253
Iteration 4400: Loss = -11137.200822283045
Iteration 4500: Loss = -11137.20283782384
1
Iteration 4600: Loss = -11137.204686956686
2
Iteration 4700: Loss = -11137.200271970232
Iteration 4800: Loss = -11137.198560402696
Iteration 4900: Loss = -11137.19824472811
Iteration 5000: Loss = -11137.200174815565
1
Iteration 5100: Loss = -11137.194521994274
Iteration 5200: Loss = -11137.193962511223
Iteration 5300: Loss = -11137.194037551115
Iteration 5400: Loss = -11137.195302258146
1
Iteration 5500: Loss = -11137.188374954474
Iteration 5600: Loss = -11137.087849691936
Iteration 5700: Loss = -11137.086625112961
Iteration 5800: Loss = -11137.095232584743
1
Iteration 5900: Loss = -11137.085464470885
Iteration 6000: Loss = -11137.083900920912
Iteration 6100: Loss = -11137.085369235789
1
Iteration 6200: Loss = -11137.08220961863
Iteration 6300: Loss = -11137.08207118925
Iteration 6400: Loss = -11137.082141855326
Iteration 6500: Loss = -11137.081773406315
Iteration 6600: Loss = -11137.090661945727
1
Iteration 6700: Loss = -11137.082276331956
2
Iteration 6800: Loss = -11137.082788103115
3
Iteration 6900: Loss = -11137.082531498154
4
Iteration 7000: Loss = -11137.087707405832
5
Iteration 7100: Loss = -11137.091917727228
6
Iteration 7200: Loss = -11137.080975023515
Iteration 7300: Loss = -11137.081534471248
1
Iteration 7400: Loss = -11137.082377512912
2
Iteration 7500: Loss = -11137.080948189725
Iteration 7600: Loss = -11137.081236373291
1
Iteration 7700: Loss = -11137.111386095814
2
Iteration 7800: Loss = -11137.080778102274
Iteration 7900: Loss = -11137.083240075819
1
Iteration 8000: Loss = -11137.080686779695
Iteration 8100: Loss = -11137.081648278565
1
Iteration 8200: Loss = -11137.081419042035
2
Iteration 8300: Loss = -11137.080686123049
Iteration 8400: Loss = -11137.08056941586
Iteration 8500: Loss = -11137.08067876732
1
Iteration 8600: Loss = -11137.080482014364
Iteration 8700: Loss = -11137.081071256034
1
Iteration 8800: Loss = -11137.080492081452
Iteration 8900: Loss = -11137.080538676833
Iteration 9000: Loss = -11137.096158879349
1
Iteration 9100: Loss = -11137.082564466227
2
Iteration 9200: Loss = -11137.089952541333
3
Iteration 9300: Loss = -11137.080385688361
Iteration 9400: Loss = -11137.08048625301
1
Iteration 9500: Loss = -11137.117068710642
2
Iteration 9600: Loss = -11137.080358226664
Iteration 9700: Loss = -11137.081977137026
1
Iteration 9800: Loss = -11137.087673781876
2
Iteration 9900: Loss = -11137.082282966328
3
Iteration 10000: Loss = -11137.080904684366
4
Iteration 10100: Loss = -11137.093797450454
5
Iteration 10200: Loss = -11137.081935012577
6
Iteration 10300: Loss = -11137.086212479782
7
Iteration 10400: Loss = -11137.127390916286
8
Iteration 10500: Loss = -11137.080079117664
Iteration 10600: Loss = -11137.080128500153
Iteration 10700: Loss = -11137.101565246987
1
Iteration 10800: Loss = -11137.080019347144
Iteration 10900: Loss = -11137.08044451321
1
Iteration 11000: Loss = -11137.102140260635
2
Iteration 11100: Loss = -11137.080421431721
3
Iteration 11200: Loss = -11137.081561953843
4
Iteration 11300: Loss = -11137.08503226331
5
Iteration 11400: Loss = -11137.088793909172
6
Iteration 11500: Loss = -11137.083601738637
7
Iteration 11600: Loss = -11137.080025783962
Iteration 11700: Loss = -11137.080354431835
1
Iteration 11800: Loss = -11137.081974633822
2
Iteration 11900: Loss = -11137.08033095368
3
Iteration 12000: Loss = -11137.083300813694
4
Iteration 12100: Loss = -11137.142006758266
5
Iteration 12200: Loss = -11137.080239054538
6
Iteration 12300: Loss = -11137.07953603786
Iteration 12400: Loss = -11137.083677806842
1
Iteration 12500: Loss = -11137.110975555088
2
Iteration 12600: Loss = -11137.0792392303
Iteration 12700: Loss = -11137.07923964288
Iteration 12800: Loss = -11137.10050008765
1
Iteration 12900: Loss = -11137.084323316096
2
Iteration 13000: Loss = -11137.106453700238
3
Iteration 13100: Loss = -11137.080335168635
4
Iteration 13200: Loss = -11137.079225292886
Iteration 13300: Loss = -11137.07945037244
1
Iteration 13400: Loss = -11137.08281919544
2
Iteration 13500: Loss = -11137.105461866404
3
Iteration 13600: Loss = -11137.08017645585
4
Iteration 13700: Loss = -11137.07925060424
Iteration 13800: Loss = -11137.270486870502
1
Iteration 13900: Loss = -11137.07916336361
Iteration 14000: Loss = -11137.076241437004
Iteration 14100: Loss = -11137.074549096264
Iteration 14200: Loss = -11137.074605213018
Iteration 14300: Loss = -11137.076507522905
1
Iteration 14400: Loss = -11137.146938647973
2
Iteration 14500: Loss = -11137.05597590899
Iteration 14600: Loss = -11137.037119305836
Iteration 14700: Loss = -11137.040129607532
1
Iteration 14800: Loss = -11137.130869426454
2
Iteration 14900: Loss = -11137.031258904444
Iteration 15000: Loss = -11137.031192780258
Iteration 15100: Loss = -11137.089499837988
1
Iteration 15200: Loss = -11137.031174132671
Iteration 15300: Loss = -11137.031302779971
1
Iteration 15400: Loss = -11137.034096569852
2
Iteration 15500: Loss = -11137.031199817848
Iteration 15600: Loss = -11137.056950734941
1
Iteration 15700: Loss = -11137.06660156485
2
Iteration 15800: Loss = -11137.034619879696
3
Iteration 15900: Loss = -11137.031217767899
Iteration 16000: Loss = -11137.031479965599
1
Iteration 16100: Loss = -11137.032258769479
2
Iteration 16200: Loss = -11137.04425156389
3
Iteration 16300: Loss = -11137.03602893425
4
Iteration 16400: Loss = -11137.032344813006
5
Iteration 16500: Loss = -11137.035215747164
6
Iteration 16600: Loss = -11137.038692732562
7
Iteration 16700: Loss = -11137.04006176461
8
Iteration 16800: Loss = -11137.166219105206
9
Iteration 16900: Loss = -11137.032991969498
10
Iteration 17000: Loss = -11137.03316707646
11
Iteration 17100: Loss = -11137.036606818629
12
Iteration 17200: Loss = -11137.050079440058
13
Iteration 17300: Loss = -11137.035759598111
14
Iteration 17400: Loss = -11137.0311331028
Iteration 17500: Loss = -11137.060450661573
1
Iteration 17600: Loss = -11137.17973270569
2
Iteration 17700: Loss = -11137.03232799498
3
Iteration 17800: Loss = -11137.036147778015
4
Iteration 17900: Loss = -11137.03832080727
5
Iteration 18000: Loss = -11137.033076782367
6
Iteration 18100: Loss = -11137.033096115349
7
Iteration 18200: Loss = -11137.031521755103
8
Iteration 18300: Loss = -11137.03166664688
9
Iteration 18400: Loss = -11137.03170512042
10
Iteration 18500: Loss = -11137.031900014299
11
Iteration 18600: Loss = -11137.036377893499
12
Iteration 18700: Loss = -11137.151841486415
13
Iteration 18800: Loss = -11137.030991839934
Iteration 18900: Loss = -11137.032093310712
1
Iteration 19000: Loss = -11137.03183018542
2
Iteration 19100: Loss = -11137.031019235157
Iteration 19200: Loss = -11137.033255502747
1
Iteration 19300: Loss = -11137.034460456094
2
Iteration 19400: Loss = -11137.064286046023
3
Iteration 19500: Loss = -11137.03088594628
Iteration 19600: Loss = -11137.031697608942
1
Iteration 19700: Loss = -11137.030880196824
Iteration 19800: Loss = -11137.031678990697
1
Iteration 19900: Loss = -11137.036229201687
2
pi: tensor([[0.7601, 0.2399],
        [0.2967, 0.7033]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5150, 0.4850], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2011, 0.1018],
         [0.6773, 0.3044]],

        [[0.5526, 0.1004],
         [0.6228, 0.7308]],

        [[0.6043, 0.1055],
         [0.5833, 0.5105]],

        [[0.6081, 0.0909],
         [0.5846, 0.5318]],

        [[0.6736, 0.1053],
         [0.6859, 0.5714]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.9446724524402711
Average Adjusted Rand Index: 0.9451295145292231
11159.609980885527
[0.5827568095325322, 0.9446724524402711] [0.7147599473650765, 0.9451295145292231] [11198.450639856981, 11137.031017903308]
-------------------------------------
This iteration is 83
True Objective function: Loss = -11263.604244653518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21872.5918276728
Iteration 100: Loss = -11584.753808693287
Iteration 200: Loss = -11582.348970012608
Iteration 300: Loss = -11571.924531339837
Iteration 400: Loss = -11543.611747472918
Iteration 500: Loss = -11375.661822180646
Iteration 600: Loss = -11358.083909134248
Iteration 700: Loss = -11349.74532717857
Iteration 800: Loss = -11349.306895300546
Iteration 900: Loss = -11342.75654775436
Iteration 1000: Loss = -11340.923957067345
Iteration 1100: Loss = -11340.824768890086
Iteration 1200: Loss = -11340.763181172879
Iteration 1300: Loss = -11340.718140701045
Iteration 1400: Loss = -11340.677984294933
Iteration 1500: Loss = -11340.504354783125
Iteration 1600: Loss = -11340.479740549032
Iteration 1700: Loss = -11340.45969936357
Iteration 1800: Loss = -11340.445114756269
Iteration 1900: Loss = -11340.433320679862
Iteration 2000: Loss = -11340.423333859148
Iteration 2100: Loss = -11340.414556085161
Iteration 2200: Loss = -11340.405702780485
Iteration 2300: Loss = -11340.3647213756
Iteration 2400: Loss = -11340.344994097783
Iteration 2500: Loss = -11340.338455270714
Iteration 2600: Loss = -11340.297592482373
Iteration 2700: Loss = -11340.226797156005
Iteration 2800: Loss = -11340.222257553258
Iteration 2900: Loss = -11340.219058620127
Iteration 3000: Loss = -11340.217285679248
Iteration 3100: Loss = -11340.212948050419
Iteration 3200: Loss = -11340.20988457665
Iteration 3300: Loss = -11340.208754806094
Iteration 3400: Loss = -11340.208316133238
Iteration 3500: Loss = -11340.2009661693
Iteration 3600: Loss = -11340.186836875124
Iteration 3700: Loss = -11340.145546946336
Iteration 3800: Loss = -11340.126142272793
Iteration 3900: Loss = -11340.118703805803
Iteration 4000: Loss = -11340.118926892083
1
Iteration 4100: Loss = -11340.11596297897
Iteration 4200: Loss = -11340.114447404645
Iteration 4300: Loss = -11340.11316523829
Iteration 4400: Loss = -11340.111707874914
Iteration 4500: Loss = -11340.1099495487
Iteration 4600: Loss = -11340.108577294639
Iteration 4700: Loss = -11340.10726762499
Iteration 4800: Loss = -11340.119327606759
1
Iteration 4900: Loss = -11340.102655109906
Iteration 5000: Loss = -11340.097843557205
Iteration 5100: Loss = -11340.085756210583
Iteration 5200: Loss = -11340.044406157662
Iteration 5300: Loss = -11338.223677970434
Iteration 5400: Loss = -11337.809526816318
Iteration 5500: Loss = -11337.766443937759
Iteration 5600: Loss = -11337.738671770412
Iteration 5700: Loss = -11337.728742710362
Iteration 5800: Loss = -11337.738005532923
1
Iteration 5900: Loss = -11337.712613912756
Iteration 6000: Loss = -11337.683350306996
Iteration 6100: Loss = -11337.67954601
Iteration 6200: Loss = -11337.658403024176
Iteration 6300: Loss = -11337.656312653779
Iteration 6400: Loss = -11337.656662477628
1
Iteration 6500: Loss = -11337.655463313378
Iteration 6600: Loss = -11337.664119286452
1
Iteration 6700: Loss = -11337.654767192987
Iteration 6800: Loss = -11337.653918109292
Iteration 6900: Loss = -11337.65362514786
Iteration 7000: Loss = -11337.65345192986
Iteration 7100: Loss = -11337.653728635214
1
Iteration 7200: Loss = -11337.65305518696
Iteration 7300: Loss = -11337.652750559088
Iteration 7400: Loss = -11337.65203144721
Iteration 7500: Loss = -11337.650495192467
Iteration 7600: Loss = -11337.650091708136
Iteration 7700: Loss = -11337.649382981455
Iteration 7800: Loss = -11337.650484590276
1
Iteration 7900: Loss = -11337.65813083018
2
Iteration 8000: Loss = -11337.649174979904
Iteration 8100: Loss = -11337.649079367851
Iteration 8200: Loss = -11337.695103730704
1
Iteration 8300: Loss = -11337.64899378293
Iteration 8400: Loss = -11337.67831926034
1
Iteration 8500: Loss = -11337.64881702229
Iteration 8600: Loss = -11337.66221102734
1
Iteration 8700: Loss = -11337.648761457367
Iteration 8800: Loss = -11337.707616937863
1
Iteration 8900: Loss = -11337.648627304354
Iteration 9000: Loss = -11337.654028652498
1
Iteration 9100: Loss = -11337.64835661914
Iteration 9200: Loss = -11337.648722105714
1
Iteration 9300: Loss = -11337.651012379223
2
Iteration 9400: Loss = -11337.648093134583
Iteration 9500: Loss = -11337.670828521781
1
Iteration 9600: Loss = -11337.647844886775
Iteration 9700: Loss = -11337.65153134965
1
Iteration 9800: Loss = -11337.647452047573
Iteration 9900: Loss = -11337.64754228286
Iteration 10000: Loss = -11337.901133497593
1
Iteration 10100: Loss = -11337.647244923019
Iteration 10200: Loss = -11337.653219997177
1
Iteration 10300: Loss = -11337.652127395264
2
Iteration 10400: Loss = -11337.713974129865
3
Iteration 10500: Loss = -11337.646741775856
Iteration 10600: Loss = -11337.65026404729
1
Iteration 10700: Loss = -11337.646639436116
Iteration 10800: Loss = -11337.647188535655
1
Iteration 10900: Loss = -11337.646570981075
Iteration 11000: Loss = -11337.64661367535
Iteration 11100: Loss = -11337.68655609309
1
Iteration 11200: Loss = -11337.632726722464
Iteration 11300: Loss = -11337.633564481175
1
Iteration 11400: Loss = -11337.633310505966
2
Iteration 11500: Loss = -11337.633687722348
3
Iteration 11600: Loss = -11337.632591632828
Iteration 11700: Loss = -11337.63883063537
1
Iteration 11800: Loss = -11337.63257325338
Iteration 11900: Loss = -11337.680121475458
1
Iteration 12000: Loss = -11337.632527940039
Iteration 12100: Loss = -11337.632551960636
Iteration 12200: Loss = -11337.632914377644
1
Iteration 12300: Loss = -11337.63245588166
Iteration 12400: Loss = -11337.632448742612
Iteration 12500: Loss = -11337.632744341197
1
Iteration 12600: Loss = -11337.684589704326
2
Iteration 12700: Loss = -11337.632334247519
Iteration 12800: Loss = -11337.632570034433
1
Iteration 12900: Loss = -11337.632306246616
Iteration 13000: Loss = -11337.652795294603
1
Iteration 13100: Loss = -11337.632283432265
Iteration 13200: Loss = -11337.67343278696
1
Iteration 13300: Loss = -11337.632157786602
Iteration 13400: Loss = -11337.631522355225
Iteration 13500: Loss = -11337.632927224546
1
Iteration 13600: Loss = -11337.699736726034
2
Iteration 13700: Loss = -11337.63145454529
Iteration 13800: Loss = -11337.632284739926
1
Iteration 13900: Loss = -11337.631859352672
2
Iteration 14000: Loss = -11337.631516467001
Iteration 14100: Loss = -11337.631429313442
Iteration 14200: Loss = -11337.632703712665
1
Iteration 14300: Loss = -11337.631433245695
Iteration 14400: Loss = -11337.631453498441
Iteration 14500: Loss = -11337.653189370107
1
Iteration 14600: Loss = -11337.631439683853
Iteration 14700: Loss = -11337.634035255316
1
Iteration 14800: Loss = -11337.63915448352
2
Iteration 14900: Loss = -11337.632545530938
3
Iteration 15000: Loss = -11337.63147620835
Iteration 15100: Loss = -11337.631785444823
1
Iteration 15200: Loss = -11337.631447533651
Iteration 15300: Loss = -11337.631426007638
Iteration 15400: Loss = -11337.631537298814
1
Iteration 15500: Loss = -11337.636281015344
2
Iteration 15600: Loss = -11337.631419173325
Iteration 15700: Loss = -11337.633295023124
1
Iteration 15800: Loss = -11337.631389954995
Iteration 15900: Loss = -11337.635192642992
1
Iteration 16000: Loss = -11337.631021138825
Iteration 16100: Loss = -11337.651636649987
1
Iteration 16200: Loss = -11337.631032027044
Iteration 16300: Loss = -11337.631041038736
Iteration 16400: Loss = -11337.63126027193
1
Iteration 16500: Loss = -11337.633693879738
2
Iteration 16600: Loss = -11337.631270302025
3
Iteration 16700: Loss = -11337.647057164295
4
Iteration 16800: Loss = -11337.63102539589
Iteration 16900: Loss = -11337.63144785435
1
Iteration 17000: Loss = -11337.631023133918
Iteration 17100: Loss = -11337.631872378195
1
Iteration 17200: Loss = -11337.646373634218
2
Iteration 17300: Loss = -11337.631011848789
Iteration 17400: Loss = -11337.630618179335
Iteration 17500: Loss = -11337.632919178759
1
Iteration 17600: Loss = -11337.630505410509
Iteration 17700: Loss = -11337.631969133223
1
Iteration 17800: Loss = -11337.630442655722
Iteration 17900: Loss = -11337.630512246677
Iteration 18000: Loss = -11337.632649903868
1
Iteration 18100: Loss = -11337.630468806658
Iteration 18200: Loss = -11337.639266585078
1
Iteration 18300: Loss = -11337.630453634238
Iteration 18400: Loss = -11337.6404563405
1
Iteration 18500: Loss = -11337.664628920658
2
Iteration 18600: Loss = -11337.63050896435
Iteration 18700: Loss = -11337.6350338174
1
Iteration 18800: Loss = -11337.648663753373
2
Iteration 18900: Loss = -11337.630276781349
Iteration 19000: Loss = -11337.63314538257
1
Iteration 19100: Loss = -11337.630532734473
2
Iteration 19200: Loss = -11337.630255587834
Iteration 19300: Loss = -11337.672327088001
1
Iteration 19400: Loss = -11337.630210645839
Iteration 19500: Loss = -11337.630105716875
Iteration 19600: Loss = -11337.634246798601
1
Iteration 19700: Loss = -11337.633047479554
2
Iteration 19800: Loss = -11337.815912403215
3
Iteration 19900: Loss = -11337.630030160513
pi: tensor([[0.6207, 0.3793],
        [0.2701, 0.7299]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9513, 0.0487], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1857, 0.0964],
         [0.5266, 0.3118]],

        [[0.6088, 0.0971],
         [0.6005, 0.7077]],

        [[0.6579, 0.1033],
         [0.5939, 0.5170]],

        [[0.6065, 0.1068],
         [0.7029, 0.5738]],

        [[0.7186, 0.1098],
         [0.5249, 0.6538]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.5828547650232409
Average Adjusted Rand Index: 0.7446577089846341
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23189.508493530288
Iteration 100: Loss = -11591.653893122388
Iteration 200: Loss = -11578.82193149967
Iteration 300: Loss = -11566.429120847937
Iteration 400: Loss = -11496.282795475576
Iteration 500: Loss = -11420.129206479776
Iteration 600: Loss = -11376.000469464441
Iteration 700: Loss = -11345.309439872752
Iteration 800: Loss = -11343.860302384726
Iteration 900: Loss = -11343.459057699632
Iteration 1000: Loss = -11343.29886541426
Iteration 1100: Loss = -11343.19428971538
Iteration 1200: Loss = -11343.11921648543
Iteration 1300: Loss = -11343.06186644195
Iteration 1400: Loss = -11343.014790423982
Iteration 1500: Loss = -11342.93934917676
Iteration 1600: Loss = -11341.63985302016
Iteration 1700: Loss = -11340.30927554528
Iteration 1800: Loss = -11340.279905487421
Iteration 1900: Loss = -11340.259434779167
Iteration 2000: Loss = -11340.242843786933
Iteration 2100: Loss = -11340.228522785848
Iteration 2200: Loss = -11340.215381406755
Iteration 2300: Loss = -11340.202150226083
Iteration 2400: Loss = -11340.186485626844
Iteration 2500: Loss = -11340.161720246495
Iteration 2600: Loss = -11340.10148810743
Iteration 2700: Loss = -11339.960422909031
Iteration 2800: Loss = -11339.87130065779
Iteration 2900: Loss = -11339.840659878246
Iteration 3000: Loss = -11339.830111450467
Iteration 3100: Loss = -11339.81855984142
Iteration 3200: Loss = -11339.812684859731
Iteration 3300: Loss = -11339.815933293807
1
Iteration 3400: Loss = -11339.804882810262
Iteration 3500: Loss = -11339.803877460057
Iteration 3600: Loss = -11339.800124156063
Iteration 3700: Loss = -11339.797926458285
Iteration 3800: Loss = -11339.794108505399
Iteration 3900: Loss = -11339.775789184587
Iteration 4000: Loss = -11339.742774018665
Iteration 4100: Loss = -11339.725317460827
Iteration 4200: Loss = -11338.107742150058
Iteration 4300: Loss = -11337.993953265484
Iteration 4400: Loss = -11337.975056026287
Iteration 4500: Loss = -11337.967785851435
Iteration 4600: Loss = -11337.963419572365
Iteration 4700: Loss = -11337.96272025541
Iteration 4800: Loss = -11337.912295731823
Iteration 4900: Loss = -11337.909645096617
Iteration 5000: Loss = -11337.909207926143
Iteration 5100: Loss = -11337.906774000849
Iteration 5200: Loss = -11337.906794747934
Iteration 5300: Loss = -11337.897956871051
Iteration 5400: Loss = -11337.762182981416
Iteration 5500: Loss = -11337.76078502997
Iteration 5600: Loss = -11337.759747720282
Iteration 5700: Loss = -11337.753149528398
Iteration 5800: Loss = -11337.725958891328
Iteration 5900: Loss = -11337.72548795725
Iteration 6000: Loss = -11337.726784339045
1
Iteration 6100: Loss = -11337.724339405766
Iteration 6200: Loss = -11337.72401896114
Iteration 6300: Loss = -11337.723691375588
Iteration 6400: Loss = -11337.723351058714
Iteration 6500: Loss = -11337.72617341616
1
Iteration 6600: Loss = -11337.722716862176
Iteration 6700: Loss = -11337.72393546692
1
Iteration 6800: Loss = -11337.722038880924
Iteration 6900: Loss = -11337.72173099168
Iteration 7000: Loss = -11337.722318786706
1
Iteration 7100: Loss = -11337.721015856616
Iteration 7200: Loss = -11337.720244479446
Iteration 7300: Loss = -11337.719602969633
Iteration 7400: Loss = -11337.719239468155
Iteration 7500: Loss = -11337.71867588406
Iteration 7600: Loss = -11337.714553942304
Iteration 7700: Loss = -11337.696440984748
Iteration 7800: Loss = -11337.708761289277
1
Iteration 7900: Loss = -11337.69558401585
Iteration 8000: Loss = -11337.694565868087
Iteration 8100: Loss = -11337.678384030547
Iteration 8200: Loss = -11337.672138378834
Iteration 8300: Loss = -11337.671948306492
Iteration 8400: Loss = -11337.671808409204
Iteration 8500: Loss = -11337.671299172804
Iteration 8600: Loss = -11337.668670331397
Iteration 8700: Loss = -11337.647131946218
Iteration 8800: Loss = -11337.646748475529
Iteration 8900: Loss = -11337.821913352915
1
Iteration 9000: Loss = -11337.646475949241
Iteration 9100: Loss = -11337.64618989119
Iteration 9200: Loss = -11337.67515267497
1
Iteration 9300: Loss = -11337.64577057464
Iteration 9400: Loss = -11337.645686959431
Iteration 9500: Loss = -11337.647228563708
1
Iteration 9600: Loss = -11337.645508690455
Iteration 9700: Loss = -11337.660367630939
1
Iteration 9800: Loss = -11337.645150929297
Iteration 9900: Loss = -11337.644800176182
Iteration 10000: Loss = -11337.642953902343
Iteration 10100: Loss = -11337.642726969001
Iteration 10200: Loss = -11337.64216260955
Iteration 10300: Loss = -11337.636629887578
Iteration 10400: Loss = -11337.636326988999
Iteration 10500: Loss = -11337.636678794797
1
Iteration 10600: Loss = -11337.63625432662
Iteration 10700: Loss = -11337.63676012284
1
Iteration 10800: Loss = -11337.63626183476
Iteration 10900: Loss = -11337.636209603284
Iteration 11000: Loss = -11337.641469241724
1
Iteration 11100: Loss = -11337.636109338198
Iteration 11200: Loss = -11337.639863501108
1
Iteration 11300: Loss = -11337.63635895678
2
Iteration 11400: Loss = -11337.644318363895
3
Iteration 11500: Loss = -11337.636796333216
4
Iteration 11600: Loss = -11337.636052773385
Iteration 11700: Loss = -11337.636770265428
1
Iteration 11800: Loss = -11337.635918306962
Iteration 11900: Loss = -11337.63571792115
Iteration 12000: Loss = -11337.690056226646
1
Iteration 12100: Loss = -11337.651876269534
2
Iteration 12200: Loss = -11337.635535281643
Iteration 12300: Loss = -11337.637643662834
1
Iteration 12400: Loss = -11337.638402878949
2
Iteration 12500: Loss = -11337.635504708162
Iteration 12600: Loss = -11337.691520776682
1
Iteration 12700: Loss = -11337.635303360143
Iteration 12800: Loss = -11337.630702642437
Iteration 12900: Loss = -11337.693328610436
1
Iteration 13000: Loss = -11337.630576834037
Iteration 13100: Loss = -11337.693432354865
1
Iteration 13200: Loss = -11337.630511175896
Iteration 13300: Loss = -11337.694935347263
1
Iteration 13400: Loss = -11337.630516629093
Iteration 13500: Loss = -11337.630479026186
Iteration 13600: Loss = -11337.633986361061
1
Iteration 13700: Loss = -11337.630439188939
Iteration 13800: Loss = -11337.63039378684
Iteration 13900: Loss = -11337.6308331578
1
Iteration 14000: Loss = -11337.652394343464
2
Iteration 14100: Loss = -11337.630364527131
Iteration 14200: Loss = -11337.632439161662
1
Iteration 14300: Loss = -11337.630379975639
Iteration 14400: Loss = -11337.63069089317
1
Iteration 14500: Loss = -11337.631262591265
2
Iteration 14600: Loss = -11337.762159643968
3
Iteration 14700: Loss = -11337.630255228476
Iteration 14800: Loss = -11337.631359338764
1
Iteration 14900: Loss = -11337.823680210126
2
Iteration 15000: Loss = -11337.63020022821
Iteration 15100: Loss = -11337.644012100947
1
Iteration 15200: Loss = -11337.630175418983
Iteration 15300: Loss = -11337.63502918954
1
Iteration 15400: Loss = -11337.63017467226
Iteration 15500: Loss = -11337.635065186603
1
Iteration 15600: Loss = -11337.630132259123
Iteration 15700: Loss = -11337.630571094098
1
Iteration 15800: Loss = -11337.631155458901
2
Iteration 15900: Loss = -11337.630185625532
Iteration 16000: Loss = -11337.630437306067
1
Iteration 16100: Loss = -11337.635177459975
2
Iteration 16200: Loss = -11337.630319976979
3
Iteration 16300: Loss = -11337.630246012051
Iteration 16400: Loss = -11337.641463539918
1
Iteration 16500: Loss = -11337.630105254844
Iteration 16600: Loss = -11337.631975003334
1
Iteration 16700: Loss = -11337.630067559836
Iteration 16800: Loss = -11337.631187050489
1
Iteration 16900: Loss = -11337.630069971436
Iteration 17000: Loss = -11337.630216064521
1
Iteration 17100: Loss = -11337.632688739563
2
Iteration 17200: Loss = -11337.630009707063
Iteration 17300: Loss = -11337.630370587158
1
Iteration 17400: Loss = -11337.635962290628
2
Iteration 17500: Loss = -11337.629956970235
Iteration 17600: Loss = -11337.633563798554
1
Iteration 17700: Loss = -11337.629958647069
Iteration 17800: Loss = -11337.649622508614
1
Iteration 17900: Loss = -11337.629964745218
Iteration 18000: Loss = -11337.635204296494
1
Iteration 18100: Loss = -11337.629991314312
Iteration 18200: Loss = -11337.649658462988
1
Iteration 18300: Loss = -11337.629971394803
Iteration 18400: Loss = -11337.631110715625
1
Iteration 18500: Loss = -11337.62999810285
Iteration 18600: Loss = -11337.629968593212
Iteration 18700: Loss = -11337.630378621996
1
Iteration 18800: Loss = -11337.629989987541
Iteration 18900: Loss = -11337.630793951448
1
Iteration 19000: Loss = -11337.629971906435
Iteration 19100: Loss = -11337.654112657194
1
Iteration 19200: Loss = -11337.629971137305
Iteration 19300: Loss = -11337.641887965408
1
Iteration 19400: Loss = -11337.630009948778
Iteration 19500: Loss = -11337.629956547367
Iteration 19600: Loss = -11337.630594040256
1
Iteration 19700: Loss = -11337.629971366276
Iteration 19800: Loss = -11337.633151966658
1
Iteration 19900: Loss = -11337.629945850786
pi: tensor([[0.6142, 0.3858],
        [0.2702, 0.7298]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9510, 0.0490], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1864, 0.0968],
         [0.6278, 0.3088]],

        [[0.6173, 0.0959],
         [0.5377, 0.5616]],

        [[0.6906, 0.1020],
         [0.6920, 0.5397]],

        [[0.5970, 0.1056],
         [0.5573, 0.5512]],

        [[0.7206, 0.1085],
         [0.5531, 0.7254]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.5828547650232409
Average Adjusted Rand Index: 0.7446577089846341
11263.604244653518
[0.5828547650232409, 0.5828547650232409] [0.7446577089846341, 0.7446577089846341] [11337.632436124613, 11337.909546199695]
-------------------------------------
This iteration is 84
True Objective function: Loss = -11130.952815894521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21859.859850324603
Iteration 100: Loss = -11424.846075392486
Iteration 200: Loss = -11421.91951429132
Iteration 300: Loss = -11414.858137508845
Iteration 400: Loss = -11401.749391116571
Iteration 500: Loss = -11252.336574958787
Iteration 600: Loss = -11124.657162524645
Iteration 700: Loss = -11122.551916246044
Iteration 800: Loss = -11122.343828506948
Iteration 900: Loss = -11120.419355745607
Iteration 1000: Loss = -11120.18260376963
Iteration 1100: Loss = -11119.887029679892
Iteration 1200: Loss = -11119.850877210929
Iteration 1300: Loss = -11119.824819223588
Iteration 1400: Loss = -11119.795008997837
Iteration 1500: Loss = -11116.567563826371
Iteration 1600: Loss = -11116.544207707882
Iteration 1700: Loss = -11116.473821635598
Iteration 1800: Loss = -11116.42680526109
Iteration 1900: Loss = -11116.418800964499
Iteration 2000: Loss = -11116.412366386196
Iteration 2100: Loss = -11116.406299175776
Iteration 2200: Loss = -11116.400690082744
Iteration 2300: Loss = -11116.39577089461
Iteration 2400: Loss = -11116.390822065445
Iteration 2500: Loss = -11116.379183720213
Iteration 2600: Loss = -11116.25668976015
Iteration 2700: Loss = -11116.200433810624
Iteration 2800: Loss = -11116.167324260883
Iteration 2900: Loss = -11116.16049770543
Iteration 3000: Loss = -11116.129288984086
Iteration 3100: Loss = -11116.120478299454
Iteration 3200: Loss = -11115.861595746905
Iteration 3300: Loss = -11115.854623780915
Iteration 3400: Loss = -11115.852568522712
Iteration 3500: Loss = -11115.84695907886
Iteration 3600: Loss = -11115.843971774013
Iteration 3700: Loss = -11115.842498460086
Iteration 3800: Loss = -11115.854359849169
1
Iteration 3900: Loss = -11115.840745589216
Iteration 4000: Loss = -11115.848680937528
1
Iteration 4100: Loss = -11115.839214502477
Iteration 4200: Loss = -11115.838338219928
Iteration 4300: Loss = -11115.837778411558
Iteration 4400: Loss = -11115.833553828326
Iteration 4500: Loss = -11113.173777845077
Iteration 4600: Loss = -11113.171984840837
Iteration 4700: Loss = -11113.17018905236
Iteration 4800: Loss = -11113.169567303721
Iteration 4900: Loss = -11113.172699750749
1
Iteration 5000: Loss = -11113.168651602871
Iteration 5100: Loss = -11113.168287862934
Iteration 5200: Loss = -11113.168453864677
1
Iteration 5300: Loss = -11113.16846746959
2
Iteration 5400: Loss = -11113.167591506171
Iteration 5500: Loss = -11113.167161203752
Iteration 5600: Loss = -11113.16692577431
Iteration 5700: Loss = -11113.166470132266
Iteration 5800: Loss = -11113.165696581626
Iteration 5900: Loss = -11113.160721356386
Iteration 6000: Loss = -11113.167247106618
1
Iteration 6100: Loss = -11113.16005350052
Iteration 6200: Loss = -11113.15953377433
Iteration 6300: Loss = -11113.161254806117
1
Iteration 6400: Loss = -11113.162120097997
2
Iteration 6500: Loss = -11113.159020717178
Iteration 6600: Loss = -11113.158751511544
Iteration 6700: Loss = -11113.158604744085
Iteration 6800: Loss = -11113.15819654142
Iteration 6900: Loss = -11113.158018959268
Iteration 7000: Loss = -11113.157909862113
Iteration 7100: Loss = -11113.157836591972
Iteration 7200: Loss = -11113.15820836169
1
Iteration 7300: Loss = -11113.158535534323
2
Iteration 7400: Loss = -11113.157675177672
Iteration 7500: Loss = -11113.164768825503
1
Iteration 7600: Loss = -11113.157523737502
Iteration 7700: Loss = -11113.170320259971
1
Iteration 7800: Loss = -11113.157364484132
Iteration 7900: Loss = -11113.241065367914
1
Iteration 8000: Loss = -11113.157097048457
Iteration 8100: Loss = -11113.156532665012
Iteration 8200: Loss = -11113.155805493803
Iteration 8300: Loss = -11113.136586274175
Iteration 8400: Loss = -11113.136430754928
Iteration 8500: Loss = -11113.130552620965
Iteration 8600: Loss = -11113.126483283446
Iteration 8700: Loss = -11113.155212817523
1
Iteration 8800: Loss = -11113.126446391248
Iteration 8900: Loss = -11113.126399618253
Iteration 9000: Loss = -11113.126883008577
1
Iteration 9100: Loss = -11113.126351653884
Iteration 9200: Loss = -11113.137345202174
1
Iteration 9300: Loss = -11113.126331801139
Iteration 9400: Loss = -11113.12628223132
Iteration 9500: Loss = -11113.127051345653
1
Iteration 9600: Loss = -11113.126236894179
Iteration 9700: Loss = -11113.455999823007
1
Iteration 9800: Loss = -11113.126142288324
Iteration 9900: Loss = -11113.126004375392
Iteration 10000: Loss = -11113.143374705043
1
Iteration 10100: Loss = -11113.125865784106
Iteration 10200: Loss = -11113.125762114438
Iteration 10300: Loss = -11113.127279898352
1
Iteration 10400: Loss = -11113.124457938195
Iteration 10500: Loss = -11113.125989864182
1
Iteration 10600: Loss = -11113.124832990636
2
Iteration 10700: Loss = -11113.123989215672
Iteration 10800: Loss = -11113.124194995824
1
Iteration 10900: Loss = -11113.240788478432
2
Iteration 11000: Loss = -11113.122889866449
Iteration 11100: Loss = -11113.139320579141
1
Iteration 11200: Loss = -11113.12248733644
Iteration 11300: Loss = -11113.126639563137
1
Iteration 11400: Loss = -11113.122144970057
Iteration 11500: Loss = -11113.122070418969
Iteration 11600: Loss = -11113.126957770986
1
Iteration 11700: Loss = -11113.121292875756
Iteration 11800: Loss = -11113.12126877163
Iteration 11900: Loss = -11113.121261616416
Iteration 12000: Loss = -11113.120865654635
Iteration 12100: Loss = -11113.588155524065
1
Iteration 12200: Loss = -11113.120832301209
Iteration 12300: Loss = -11113.120805027655
Iteration 12400: Loss = -11113.149003578268
1
Iteration 12500: Loss = -11113.12053018185
Iteration 12600: Loss = -11113.120534498545
Iteration 12700: Loss = -11113.120594268772
Iteration 12800: Loss = -11113.120581354598
Iteration 12900: Loss = -11113.120485912821
Iteration 13000: Loss = -11113.16875488704
1
Iteration 13100: Loss = -11113.120390819875
Iteration 13200: Loss = -11113.126703463888
1
Iteration 13300: Loss = -11113.12039642307
Iteration 13400: Loss = -11113.120387332634
Iteration 13500: Loss = -11113.120764106534
1
Iteration 13600: Loss = -11113.122627042263
2
Iteration 13700: Loss = -11113.120308664593
Iteration 13800: Loss = -11113.120191841477
Iteration 13900: Loss = -11113.120614831576
1
Iteration 14000: Loss = -11113.120216723197
Iteration 14100: Loss = -11113.120538445753
1
Iteration 14200: Loss = -11113.120210723482
Iteration 14300: Loss = -11113.164751955706
1
Iteration 14400: Loss = -11113.120225704532
Iteration 14500: Loss = -11113.120233742875
Iteration 14600: Loss = -11113.120195714508
Iteration 14700: Loss = -11113.120153366068
Iteration 14800: Loss = -11113.120193421115
Iteration 14900: Loss = -11113.120129226609
Iteration 15000: Loss = -11113.121016942654
1
Iteration 15100: Loss = -11113.120113656356
Iteration 15200: Loss = -11113.172607665949
1
Iteration 15300: Loss = -11113.120115704949
Iteration 15400: Loss = -11113.120090322895
Iteration 15500: Loss = -11113.12011187216
Iteration 15600: Loss = -11113.120110977256
Iteration 15700: Loss = -11113.191109110574
1
Iteration 15800: Loss = -11113.120099955238
Iteration 15900: Loss = -11113.120070620229
Iteration 16000: Loss = -11113.16210370633
1
Iteration 16100: Loss = -11113.120078825516
Iteration 16200: Loss = -11113.120063520144
Iteration 16300: Loss = -11113.193176292367
1
Iteration 16400: Loss = -11113.11989995561
Iteration 16500: Loss = -11113.119918842664
Iteration 16600: Loss = -11113.266428465313
1
Iteration 16700: Loss = -11113.119907012047
Iteration 16800: Loss = -11113.119897560175
Iteration 16900: Loss = -11113.132019933937
1
Iteration 17000: Loss = -11113.119905626956
Iteration 17100: Loss = -11113.119900723399
Iteration 17200: Loss = -11113.12063444706
1
Iteration 17300: Loss = -11113.134343955433
2
Iteration 17400: Loss = -11113.119914631023
Iteration 17500: Loss = -11113.143688791593
1
Iteration 17600: Loss = -11113.119907807088
Iteration 17700: Loss = -11113.120135950778
1
Iteration 17800: Loss = -11113.119886671368
Iteration 17900: Loss = -11113.157111705954
1
Iteration 18000: Loss = -11113.119879117392
Iteration 18100: Loss = -11113.119853208937
Iteration 18200: Loss = -11113.119692481998
Iteration 18300: Loss = -11113.11802699739
Iteration 18400: Loss = -11113.11803205426
Iteration 18500: Loss = -11113.118123588274
Iteration 18600: Loss = -11113.11789359226
Iteration 18700: Loss = -11113.079396816454
Iteration 18800: Loss = -11113.07741181321
Iteration 18900: Loss = -11113.078394470878
1
Iteration 19000: Loss = -11113.07432366967
Iteration 19100: Loss = -11113.07177080267
Iteration 19200: Loss = -11113.071728404362
Iteration 19300: Loss = -11113.09387012089
1
Iteration 19400: Loss = -11113.071746592886
Iteration 19500: Loss = -11113.071733699231
Iteration 19600: Loss = -11113.382797901599
1
Iteration 19700: Loss = -11113.071730342632
Iteration 19800: Loss = -11113.071507100862
Iteration 19900: Loss = -11113.071636058401
1
pi: tensor([[0.7409, 0.2591],
        [0.2382, 0.7618]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5073, 0.4927], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1954, 0.0968],
         [0.5496, 0.2970]],

        [[0.6404, 0.0971],
         [0.5852, 0.5032]],

        [[0.5388, 0.0952],
         [0.6897, 0.6431]],

        [[0.5083, 0.0997],
         [0.6575, 0.6014]],

        [[0.5585, 0.1068],
         [0.6793, 0.6125]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.9368977075382423
Average Adjusted Rand Index: 0.9371260657489021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21354.934699850404
Iteration 100: Loss = -11425.878504406091
Iteration 200: Loss = -11424.588919207654
Iteration 300: Loss = -11421.70888464886
Iteration 400: Loss = -11397.050510414017
Iteration 500: Loss = -11269.829060836551
Iteration 600: Loss = -11207.622686272285
Iteration 700: Loss = -11206.599658164272
Iteration 800: Loss = -11206.360799451308
Iteration 900: Loss = -11205.707593467112
Iteration 1000: Loss = -11205.61678895425
Iteration 1100: Loss = -11205.560719815789
Iteration 1200: Loss = -11205.527596703967
Iteration 1300: Loss = -11205.503871923098
Iteration 1400: Loss = -11205.485684265223
Iteration 1500: Loss = -11205.47126417274
Iteration 1600: Loss = -11205.459294201088
Iteration 1700: Loss = -11205.450113474037
Iteration 1800: Loss = -11205.443091017481
Iteration 1900: Loss = -11205.437380740337
Iteration 2000: Loss = -11205.432513157624
Iteration 2100: Loss = -11205.428372953684
Iteration 2200: Loss = -11205.424795583454
Iteration 2300: Loss = -11205.421639561768
Iteration 2400: Loss = -11205.418906043975
Iteration 2500: Loss = -11205.41644094058
Iteration 2600: Loss = -11205.414595348742
Iteration 2700: Loss = -11205.412343392865
Iteration 2800: Loss = -11205.410621680941
Iteration 2900: Loss = -11205.409059760077
Iteration 3000: Loss = -11205.407709988753
Iteration 3100: Loss = -11205.406427179827
Iteration 3200: Loss = -11205.405281474772
Iteration 3300: Loss = -11205.40417109217
Iteration 3400: Loss = -11205.403657000523
Iteration 3500: Loss = -11205.402482244017
Iteration 3600: Loss = -11205.40162196435
Iteration 3700: Loss = -11205.403230596183
1
Iteration 3800: Loss = -11205.400200193233
Iteration 3900: Loss = -11205.408223774084
1
Iteration 4000: Loss = -11205.398879642744
Iteration 4100: Loss = -11205.401271443312
1
Iteration 4200: Loss = -11205.397443711165
Iteration 4300: Loss = -11205.40112927111
1
Iteration 4400: Loss = -11205.395464627274
Iteration 4500: Loss = -11205.393905900948
Iteration 4600: Loss = -11205.390205237096
Iteration 4700: Loss = -11205.369384623415
Iteration 4800: Loss = -11201.579757792471
Iteration 4900: Loss = -11199.128660986207
Iteration 5000: Loss = -11198.155003179965
Iteration 5100: Loss = -11197.971278209745
Iteration 5200: Loss = -11197.962097415797
Iteration 5300: Loss = -11197.958463650284
Iteration 5400: Loss = -11197.957542101713
Iteration 5500: Loss = -11197.956990166684
Iteration 5600: Loss = -11197.95653565244
Iteration 5700: Loss = -11197.956154220576
Iteration 5800: Loss = -11197.955974888522
Iteration 5900: Loss = -11197.955699311671
Iteration 6000: Loss = -11197.955649971844
Iteration 6100: Loss = -11197.955627076368
Iteration 6200: Loss = -11197.954746497486
Iteration 6300: Loss = -11197.953950424579
Iteration 6400: Loss = -11197.953542319368
Iteration 6500: Loss = -11197.953406491699
Iteration 6600: Loss = -11197.953308156691
Iteration 6700: Loss = -11197.953163203632
Iteration 6800: Loss = -11197.953075164805
Iteration 6900: Loss = -11197.962000993584
1
Iteration 7000: Loss = -11197.952874369388
Iteration 7100: Loss = -11197.953079577363
1
Iteration 7200: Loss = -11197.952570545454
Iteration 7300: Loss = -11197.956667179427
1
Iteration 7400: Loss = -11197.927943113993
Iteration 7500: Loss = -11197.921933297073
Iteration 7600: Loss = -11197.928101037953
1
Iteration 7700: Loss = -11197.921794434056
Iteration 7800: Loss = -11197.940710871371
1
Iteration 7900: Loss = -11197.92164073388
Iteration 8000: Loss = -11197.921443439453
Iteration 8100: Loss = -11197.92143786705
Iteration 8200: Loss = -11197.921022069992
Iteration 8300: Loss = -11197.919542712167
Iteration 8400: Loss = -11197.919562800704
Iteration 8500: Loss = -11197.949521665665
1
Iteration 8600: Loss = -11197.920724866959
2
Iteration 8700: Loss = -11197.92214735018
3
Iteration 8800: Loss = -11197.920730194966
4
Iteration 8900: Loss = -11197.921170340187
5
Iteration 9000: Loss = -11197.95397067182
6
Iteration 9100: Loss = -11197.918371143525
Iteration 9200: Loss = -11197.918709198893
1
Iteration 9300: Loss = -11197.921222136993
2
Iteration 9400: Loss = -11197.920307787712
3
Iteration 9500: Loss = -11197.924421627251
4
Iteration 9600: Loss = -11197.91708209508
Iteration 9700: Loss = -11197.915640937845
Iteration 9800: Loss = -11197.916557638673
1
Iteration 9900: Loss = -11197.916982111215
2
Iteration 10000: Loss = -11197.916060502694
3
Iteration 10100: Loss = -11197.918306812342
4
Iteration 10200: Loss = -11197.915822861065
5
Iteration 10300: Loss = -11197.918630493212
6
Iteration 10400: Loss = -11197.915496339367
Iteration 10500: Loss = -11197.915758407606
1
Iteration 10600: Loss = -11197.927172137433
2
Iteration 10700: Loss = -11198.128313008543
3
Iteration 10800: Loss = -11197.91557411794
Iteration 10900: Loss = -11197.91552484974
Iteration 11000: Loss = -11197.915456627932
Iteration 11100: Loss = -11197.915587265306
1
Iteration 11200: Loss = -11197.915390154983
Iteration 11300: Loss = -11197.91539621965
Iteration 11400: Loss = -11197.91539958882
Iteration 11500: Loss = -11197.915638121642
1
Iteration 11600: Loss = -11197.915203736757
Iteration 11700: Loss = -11197.916173903632
1
Iteration 11800: Loss = -11197.915241072313
Iteration 11900: Loss = -11198.03164300988
1
Iteration 12000: Loss = -11197.915543016567
2
Iteration 12100: Loss = -11197.915049677018
Iteration 12200: Loss = -11197.914479769724
Iteration 12300: Loss = -11197.914429218314
Iteration 12400: Loss = -11197.917782848639
1
Iteration 12500: Loss = -11197.915280337013
2
Iteration 12600: Loss = -11197.921356050882
3
Iteration 12700: Loss = -11197.914398118426
Iteration 12800: Loss = -11197.916392303721
1
Iteration 12900: Loss = -11197.918098762391
2
Iteration 13000: Loss = -11197.914445947477
Iteration 13100: Loss = -11197.914909785397
1
Iteration 13200: Loss = -11197.925140292247
2
Iteration 13300: Loss = -11197.915767124941
3
Iteration 13400: Loss = -11198.003522192352
4
Iteration 13500: Loss = -11197.91438714067
Iteration 13600: Loss = -11197.91326328507
Iteration 13700: Loss = -11197.923938449516
1
Iteration 13800: Loss = -11197.913250149231
Iteration 13900: Loss = -11197.915553008372
1
Iteration 14000: Loss = -11197.913237398086
Iteration 14100: Loss = -11198.05316389577
1
Iteration 14200: Loss = -11197.913229993617
Iteration 14300: Loss = -11197.915035913085
1
Iteration 14400: Loss = -11197.995137453365
2
Iteration 14500: Loss = -11197.917762822553
3
Iteration 14600: Loss = -11197.913269989276
Iteration 14700: Loss = -11197.913843301376
1
Iteration 14800: Loss = -11197.980700342723
2
Iteration 14900: Loss = -11197.91318223281
Iteration 15000: Loss = -11197.917091635036
1
Iteration 15100: Loss = -11197.913195668509
Iteration 15200: Loss = -11197.922840832056
1
Iteration 15300: Loss = -11197.913208038623
Iteration 15400: Loss = -11198.14175882227
1
Iteration 15500: Loss = -11197.911505958611
Iteration 15600: Loss = -11197.925108862019
1
Iteration 15700: Loss = -11197.911726431057
2
Iteration 15800: Loss = -11197.91338909347
3
Iteration 15900: Loss = -11197.91328260272
4
Iteration 16000: Loss = -11197.911608763123
5
Iteration 16100: Loss = -11197.913940859436
6
Iteration 16200: Loss = -11197.911580128703
Iteration 16300: Loss = -11197.931213612364
1
Iteration 16400: Loss = -11197.911773181799
2
Iteration 16500: Loss = -11197.911825533074
3
Iteration 16600: Loss = -11197.92060886977
4
Iteration 16700: Loss = -11197.911599822704
Iteration 16800: Loss = -11197.911271255889
Iteration 16900: Loss = -11197.912015398228
1
Iteration 17000: Loss = -11197.911174674746
Iteration 17100: Loss = -11197.912247397233
1
Iteration 17200: Loss = -11197.911182592821
Iteration 17300: Loss = -11198.104020861956
1
Iteration 17400: Loss = -11197.915331470482
2
Iteration 17500: Loss = -11197.911153432615
Iteration 17600: Loss = -11197.917778679413
1
Iteration 17700: Loss = -11197.911162308037
Iteration 17800: Loss = -11197.933236553139
1
Iteration 17900: Loss = -11197.920842911319
2
Iteration 18000: Loss = -11197.911224869677
Iteration 18100: Loss = -11197.912134806054
1
Iteration 18200: Loss = -11197.923835269192
2
Iteration 18300: Loss = -11197.911655715568
3
Iteration 18400: Loss = -11197.911277001824
Iteration 18500: Loss = -11197.916365048928
1
Iteration 18600: Loss = -11197.911503984136
2
Iteration 18700: Loss = -11197.911193537018
Iteration 18800: Loss = -11197.953665417344
1
Iteration 18900: Loss = -11197.912360284809
2
Iteration 19000: Loss = -11197.911558987127
3
Iteration 19100: Loss = -11197.911373395105
4
Iteration 19200: Loss = -11197.925868966786
5
Iteration 19300: Loss = -11197.911427570805
6
Iteration 19400: Loss = -11197.911803867744
7
Iteration 19500: Loss = -11198.051176965844
8
Iteration 19600: Loss = -11197.911185824496
Iteration 19700: Loss = -11197.912278480158
1
Iteration 19800: Loss = -11197.921824620093
2
Iteration 19900: Loss = -11197.91116775258
pi: tensor([[0.7432, 0.2568],
        [0.3601, 0.6399]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1217, 0.8783], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2895, 0.0959],
         [0.6365, 0.1964]],

        [[0.6346, 0.0977],
         [0.5663, 0.5479]],

        [[0.6830, 0.0945],
         [0.6391, 0.5791]],

        [[0.7264, 0.0997],
         [0.5879, 0.7281]],

        [[0.6785, 0.1056],
         [0.7202, 0.6866]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.04462094507161662
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.5526459047312486
Average Adjusted Rand Index: 0.7849233426747497
11130.952815894521
[0.9368977075382423, 0.5526459047312486] [0.9371260657489021, 0.7849233426747497] [11113.07153711392, 11197.914203223989]
-------------------------------------
This iteration is 85
True Objective function: Loss = -11126.658850058191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23771.397566584903
Iteration 100: Loss = -11442.998479007456
Iteration 200: Loss = -11439.360615212896
Iteration 300: Loss = -11434.61459676057
Iteration 400: Loss = -11426.216188414215
Iteration 500: Loss = -11387.052106457442
Iteration 600: Loss = -11224.774221688362
Iteration 700: Loss = -11197.73142138231
Iteration 800: Loss = -11177.728483348628
Iteration 900: Loss = -11149.168022967711
Iteration 1000: Loss = -11141.695618031834
Iteration 1100: Loss = -11129.68034468106
Iteration 1200: Loss = -11124.206062154326
Iteration 1300: Loss = -11124.160144165677
Iteration 1400: Loss = -11118.74678857215
Iteration 1500: Loss = -11113.539215560562
Iteration 1600: Loss = -11113.503561991489
Iteration 1700: Loss = -11113.446801744694
Iteration 1800: Loss = -11113.37673197111
Iteration 1900: Loss = -11110.972333520243
Iteration 2000: Loss = -11110.959590691755
Iteration 2100: Loss = -11110.962979410953
1
Iteration 2200: Loss = -11110.941929053317
Iteration 2300: Loss = -11110.933067383552
Iteration 2400: Loss = -11110.923895660637
Iteration 2500: Loss = -11110.855554595899
Iteration 2600: Loss = -11110.85198528371
Iteration 2700: Loss = -11110.823050744199
Iteration 2800: Loss = -11110.222792331506
Iteration 2900: Loss = -11105.991125261755
Iteration 3000: Loss = -11105.738773928233
Iteration 3100: Loss = -11105.72837577748
Iteration 3200: Loss = -11105.725783057296
Iteration 3300: Loss = -11105.724117516618
Iteration 3400: Loss = -11105.723161526483
Iteration 3500: Loss = -11105.723019075951
Iteration 3600: Loss = -11105.721652893702
Iteration 3700: Loss = -11105.720948858738
Iteration 3800: Loss = -11105.720724057106
Iteration 3900: Loss = -11105.719673345046
Iteration 4000: Loss = -11105.719010551713
Iteration 4100: Loss = -11105.720846559158
1
Iteration 4200: Loss = -11105.717295420714
Iteration 4300: Loss = -11105.714188869104
Iteration 4400: Loss = -11105.710283506729
Iteration 4500: Loss = -11105.70519867057
Iteration 4600: Loss = -11105.695843769276
Iteration 4700: Loss = -11105.694442073678
Iteration 4800: Loss = -11105.69251874653
Iteration 4900: Loss = -11105.691899990747
Iteration 5000: Loss = -11105.691440760744
Iteration 5100: Loss = -11105.690447771494
Iteration 5200: Loss = -11105.66412336559
Iteration 5300: Loss = -11105.667460328335
1
Iteration 5400: Loss = -11105.66188219652
Iteration 5500: Loss = -11105.657258008803
Iteration 5600: Loss = -11105.656269072415
Iteration 5700: Loss = -11105.65227696391
Iteration 5800: Loss = -11105.652309967243
Iteration 5900: Loss = -11105.652093963381
Iteration 6000: Loss = -11105.653697599364
1
Iteration 6100: Loss = -11105.66882440112
2
Iteration 6200: Loss = -11105.652861998824
3
Iteration 6300: Loss = -11105.652108649792
Iteration 6400: Loss = -11105.654018285568
1
Iteration 6500: Loss = -11105.651784435677
Iteration 6600: Loss = -11105.65187214463
Iteration 6700: Loss = -11105.655312366294
1
Iteration 6800: Loss = -11105.654956690689
2
Iteration 6900: Loss = -11105.671527563978
3
Iteration 7000: Loss = -11105.653479759996
4
Iteration 7100: Loss = -11105.651475409748
Iteration 7200: Loss = -11105.657385351677
1
Iteration 7300: Loss = -11105.651094699142
Iteration 7400: Loss = -11105.650868769566
Iteration 7500: Loss = -11105.651539406485
1
Iteration 7600: Loss = -11105.651310153671
2
Iteration 7700: Loss = -11105.73900985872
3
Iteration 7800: Loss = -11105.650688237349
Iteration 7900: Loss = -11105.655233573056
1
Iteration 8000: Loss = -11105.650762836689
Iteration 8100: Loss = -11105.650283068573
Iteration 8200: Loss = -11105.661462379347
1
Iteration 8300: Loss = -11105.649567097931
Iteration 8400: Loss = -11105.649517955257
Iteration 8500: Loss = -11105.650192198706
1
Iteration 8600: Loss = -11105.649431624548
Iteration 8700: Loss = -11105.666648751405
1
Iteration 8800: Loss = -11105.65073041541
2
Iteration 8900: Loss = -11105.649281413684
Iteration 9000: Loss = -11105.649452860964
1
Iteration 9100: Loss = -11105.649479953141
2
Iteration 9200: Loss = -11105.64900443968
Iteration 9300: Loss = -11105.762802294024
1
Iteration 9400: Loss = -11105.64891460582
Iteration 9500: Loss = -11105.650404342365
1
Iteration 9600: Loss = -11105.6489019834
Iteration 9700: Loss = -11105.648878791219
Iteration 9800: Loss = -11105.662531028307
1
Iteration 9900: Loss = -11105.661818785755
2
Iteration 10000: Loss = -11105.649647782186
3
Iteration 10100: Loss = -11105.731168176833
4
Iteration 10200: Loss = -11105.661810834921
5
Iteration 10300: Loss = -11105.648767995868
Iteration 10400: Loss = -11105.65051840961
1
Iteration 10500: Loss = -11105.65344741961
2
Iteration 10600: Loss = -11105.656103215137
3
Iteration 10700: Loss = -11105.65115025067
4
Iteration 10800: Loss = -11105.655615686283
5
Iteration 10900: Loss = -11105.784702193545
6
Iteration 11000: Loss = -11105.65001826647
7
Iteration 11100: Loss = -11105.648379965425
Iteration 11200: Loss = -11105.64846742688
Iteration 11300: Loss = -11105.659901809662
1
Iteration 11400: Loss = -11105.849549206625
2
Iteration 11500: Loss = -11105.648117176732
Iteration 11600: Loss = -11105.650269754058
1
Iteration 11700: Loss = -11105.648136137164
Iteration 11800: Loss = -11105.64865004449
1
Iteration 11900: Loss = -11105.661793291467
2
Iteration 12000: Loss = -11105.64839521786
3
Iteration 12100: Loss = -11105.648634569252
4
Iteration 12200: Loss = -11105.691109096704
5
Iteration 12300: Loss = -11105.654301432109
6
Iteration 12400: Loss = -11105.657388686528
7
Iteration 12500: Loss = -11105.64770219184
Iteration 12600: Loss = -11105.64786073108
1
Iteration 12700: Loss = -11105.669671582478
2
Iteration 12800: Loss = -11105.651861328957
3
Iteration 12900: Loss = -11105.647615663282
Iteration 13000: Loss = -11105.641864217014
Iteration 13100: Loss = -11105.839092404465
1
Iteration 13200: Loss = -11105.641135396454
Iteration 13300: Loss = -11105.642098946882
1
Iteration 13400: Loss = -11105.648948500439
2
Iteration 13500: Loss = -11105.640781175847
Iteration 13600: Loss = -11105.642754303986
1
Iteration 13700: Loss = -11105.647370623352
2
Iteration 13800: Loss = -11105.64072622943
Iteration 13900: Loss = -11105.632483575595
Iteration 14000: Loss = -11105.632679419496
1
Iteration 14100: Loss = -11105.633784341248
2
Iteration 14200: Loss = -11105.646771616872
3
Iteration 14300: Loss = -11105.63248211691
Iteration 14400: Loss = -11105.63240232186
Iteration 14500: Loss = -11105.66638921248
1
Iteration 14600: Loss = -11105.637312920826
2
Iteration 14700: Loss = -11105.626769975557
Iteration 14800: Loss = -11105.625648760932
Iteration 14900: Loss = -11105.632650253892
1
Iteration 15000: Loss = -11105.786500729273
2
Iteration 15100: Loss = -11105.635877312001
3
Iteration 15200: Loss = -11105.625698686592
Iteration 15300: Loss = -11105.62554444277
Iteration 15400: Loss = -11105.628712420828
1
Iteration 15500: Loss = -11105.625662205146
2
Iteration 15600: Loss = -11105.625662316004
3
Iteration 15700: Loss = -11105.625719521877
4
Iteration 15800: Loss = -11105.637571030838
5
Iteration 15900: Loss = -11105.625325707344
Iteration 16000: Loss = -11105.625336255001
Iteration 16100: Loss = -11105.627561000168
1
Iteration 16200: Loss = -11105.62681094242
2
Iteration 16300: Loss = -11105.62649050717
3
Iteration 16400: Loss = -11105.645652586685
4
Iteration 16500: Loss = -11105.625413840895
Iteration 16600: Loss = -11105.62546320308
Iteration 16700: Loss = -11105.626267487836
1
Iteration 16800: Loss = -11105.878189122677
2
Iteration 16900: Loss = -11105.62527116492
Iteration 17000: Loss = -11105.630973345238
1
Iteration 17100: Loss = -11105.625237611033
Iteration 17200: Loss = -11105.629214476481
1
Iteration 17300: Loss = -11105.625240319061
Iteration 17400: Loss = -11105.704066702736
1
Iteration 17500: Loss = -11105.62524331827
Iteration 17600: Loss = -11105.625336487816
Iteration 17700: Loss = -11105.625281614179
Iteration 17800: Loss = -11105.625114555714
Iteration 17900: Loss = -11105.62512616072
Iteration 18000: Loss = -11105.632212125396
1
Iteration 18100: Loss = -11105.634951346481
2
Iteration 18200: Loss = -11105.6353271808
3
Iteration 18300: Loss = -11105.630916624921
4
Iteration 18400: Loss = -11105.625208894664
Iteration 18500: Loss = -11105.625587117202
1
Iteration 18600: Loss = -11105.625305579739
Iteration 18700: Loss = -11105.625386252994
Iteration 18800: Loss = -11105.625942851879
1
Iteration 18900: Loss = -11105.625541692003
2
Iteration 19000: Loss = -11105.625276349754
Iteration 19100: Loss = -11105.626384807692
1
Iteration 19200: Loss = -11105.627138810083
2
Iteration 19300: Loss = -11105.640589028944
3
Iteration 19400: Loss = -11105.625137321853
Iteration 19500: Loss = -11105.625771894865
1
Iteration 19600: Loss = -11105.625719932232
2
Iteration 19700: Loss = -11105.625277503654
3
Iteration 19800: Loss = -11105.626027843393
4
Iteration 19900: Loss = -11105.676148253966
5
pi: tensor([[0.7354, 0.2646],
        [0.2510, 0.7490]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5057, 0.4943], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3081, 0.0986],
         [0.5134, 0.1989]],

        [[0.6436, 0.0925],
         [0.5019, 0.6180]],

        [[0.6326, 0.1058],
         [0.5159, 0.5907]],

        [[0.6890, 0.0953],
         [0.5053, 0.5886]],

        [[0.5348, 0.0968],
         [0.6396, 0.5789]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9524808303943679
Average Adjusted Rand Index: 0.952642787795647
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20585.946201577506
Iteration 100: Loss = -11442.889813742651
Iteration 200: Loss = -11439.602606948314
Iteration 300: Loss = -11429.426783705645
Iteration 400: Loss = -11270.842051272057
Iteration 500: Loss = -11190.048334550987
Iteration 600: Loss = -11174.51517969266
Iteration 700: Loss = -11173.460508556027
Iteration 800: Loss = -11173.180927291865
Iteration 900: Loss = -11172.891807271903
Iteration 1000: Loss = -11171.45460761652
Iteration 1100: Loss = -11171.349472305239
Iteration 1200: Loss = -11171.28592256739
Iteration 1300: Loss = -11171.234069759998
Iteration 1400: Loss = -11171.182439291244
Iteration 1500: Loss = -11171.117100491663
Iteration 1600: Loss = -11171.039333890949
Iteration 1700: Loss = -11170.925555411288
Iteration 1800: Loss = -11170.660065736545
Iteration 1900: Loss = -11137.680533084407
Iteration 2000: Loss = -11110.86854286806
Iteration 2100: Loss = -11110.060073890363
Iteration 2200: Loss = -11109.784012505308
Iteration 2300: Loss = -11105.733230001271
Iteration 2400: Loss = -11105.706921029025
Iteration 2500: Loss = -11105.690089956966
Iteration 2600: Loss = -11105.677814824174
Iteration 2700: Loss = -11105.668277963472
Iteration 2800: Loss = -11105.660923689968
Iteration 2900: Loss = -11105.654786112982
Iteration 3000: Loss = -11105.650099870898
Iteration 3100: Loss = -11105.647160258448
Iteration 3200: Loss = -11105.64531336527
Iteration 3300: Loss = -11105.642848363645
Iteration 3400: Loss = -11105.640909666901
Iteration 3500: Loss = -11105.640620697342
Iteration 3600: Loss = -11105.64042849015
Iteration 3700: Loss = -11105.6368415051
Iteration 3800: Loss = -11105.635229043792
Iteration 3900: Loss = -11105.634211132081
Iteration 4000: Loss = -11105.633862302193
Iteration 4100: Loss = -11105.632666548221
Iteration 4200: Loss = -11105.632158686729
Iteration 4300: Loss = -11105.631500723239
Iteration 4400: Loss = -11105.630982799621
Iteration 4500: Loss = -11105.630553577274
Iteration 4600: Loss = -11105.630687315454
1
Iteration 4700: Loss = -11105.629941837693
Iteration 4800: Loss = -11105.629474826068
Iteration 4900: Loss = -11105.649103734919
1
Iteration 5000: Loss = -11105.628929177425
Iteration 5100: Loss = -11105.628838724413
Iteration 5200: Loss = -11105.628458600415
Iteration 5300: Loss = -11105.629111018987
1
Iteration 5400: Loss = -11105.628017593808
Iteration 5500: Loss = -11105.639721244372
1
Iteration 5600: Loss = -11105.62769673622
Iteration 5700: Loss = -11105.63772151956
1
Iteration 5800: Loss = -11105.627372819074
Iteration 5900: Loss = -11105.627219128772
Iteration 6000: Loss = -11105.630087839127
1
Iteration 6100: Loss = -11105.626983259684
Iteration 6200: Loss = -11105.63036127995
1
Iteration 6300: Loss = -11105.628907802116
2
Iteration 6400: Loss = -11105.626686058647
Iteration 6500: Loss = -11105.629999042707
1
Iteration 6600: Loss = -11105.629398539999
2
Iteration 6700: Loss = -11105.62816698106
3
Iteration 6800: Loss = -11105.626498598378
Iteration 6900: Loss = -11105.626330622941
Iteration 7000: Loss = -11105.629191042699
1
Iteration 7100: Loss = -11105.637932146787
2
Iteration 7200: Loss = -11105.630046166258
3
Iteration 7300: Loss = -11105.647061915934
4
Iteration 7400: Loss = -11105.63565010703
5
Iteration 7500: Loss = -11105.631943850974
6
Iteration 7600: Loss = -11105.654664150641
7
Iteration 7700: Loss = -11105.632943893532
8
Iteration 7800: Loss = -11105.66005431101
9
Iteration 7900: Loss = -11105.625784635897
Iteration 8000: Loss = -11105.632368795421
1
Iteration 8100: Loss = -11105.626874339554
2
Iteration 8200: Loss = -11105.62672735729
3
Iteration 8300: Loss = -11105.628452366702
4
Iteration 8400: Loss = -11105.625651876375
Iteration 8500: Loss = -11105.625781780233
1
Iteration 8600: Loss = -11105.629472581828
2
Iteration 8700: Loss = -11105.625958953095
3
Iteration 8800: Loss = -11105.626062065865
4
Iteration 8900: Loss = -11105.677441926251
5
Iteration 9000: Loss = -11105.625572078958
Iteration 9100: Loss = -11105.625512663828
Iteration 9200: Loss = -11105.62555956514
Iteration 9300: Loss = -11105.630020194145
1
Iteration 9400: Loss = -11105.626109724719
2
Iteration 9500: Loss = -11105.625581199614
Iteration 9600: Loss = -11105.625582872666
Iteration 9700: Loss = -11105.62603514455
1
Iteration 9800: Loss = -11105.627357931802
2
Iteration 9900: Loss = -11105.662943759686
3
Iteration 10000: Loss = -11105.631679098507
4
Iteration 10100: Loss = -11105.626626613233
5
Iteration 10200: Loss = -11105.625465447774
Iteration 10300: Loss = -11105.626144546017
1
Iteration 10400: Loss = -11105.630366975083
2
Iteration 10500: Loss = -11105.625493285661
Iteration 10600: Loss = -11105.62552818377
Iteration 10700: Loss = -11105.627211670107
1
Iteration 10800: Loss = -11105.625495921555
Iteration 10900: Loss = -11105.626296079456
1
Iteration 11000: Loss = -11105.628892494113
2
Iteration 11100: Loss = -11105.67826876701
3
Iteration 11200: Loss = -11105.625277625933
Iteration 11300: Loss = -11105.626251759893
1
Iteration 11400: Loss = -11105.638256349583
2
Iteration 11500: Loss = -11105.625298505762
Iteration 11600: Loss = -11105.626477823464
1
Iteration 11700: Loss = -11105.6253381469
Iteration 11800: Loss = -11105.62876481441
1
Iteration 11900: Loss = -11105.625303017474
Iteration 12000: Loss = -11105.625318031854
Iteration 12100: Loss = -11105.625355726535
Iteration 12200: Loss = -11105.625320306945
Iteration 12300: Loss = -11105.625211815588
Iteration 12400: Loss = -11105.625757461139
1
Iteration 12500: Loss = -11105.625237962839
Iteration 12600: Loss = -11105.633557133071
1
Iteration 12700: Loss = -11105.625246284097
Iteration 12800: Loss = -11105.626399815412
1
Iteration 12900: Loss = -11105.625847061756
2
Iteration 13000: Loss = -11105.62827283506
3
Iteration 13100: Loss = -11105.625363520714
4
Iteration 13200: Loss = -11105.625216622413
Iteration 13300: Loss = -11105.625746967013
1
Iteration 13400: Loss = -11105.62612631269
2
Iteration 13500: Loss = -11105.724443141871
3
Iteration 13600: Loss = -11105.626043433504
4
Iteration 13700: Loss = -11105.626115174398
5
Iteration 13800: Loss = -11105.628307178553
6
Iteration 13900: Loss = -11105.717663736335
7
Iteration 14000: Loss = -11105.625006340955
Iteration 14100: Loss = -11105.634056896184
1
Iteration 14200: Loss = -11105.62519193258
2
Iteration 14300: Loss = -11105.721504121482
3
Iteration 14400: Loss = -11105.634286317358
4
Iteration 14500: Loss = -11105.625031276844
Iteration 14600: Loss = -11105.625082274677
Iteration 14700: Loss = -11105.625614861921
1
Iteration 14800: Loss = -11105.625057214616
Iteration 14900: Loss = -11105.638558833683
1
Iteration 15000: Loss = -11105.625002059895
Iteration 15100: Loss = -11105.62639946166
1
Iteration 15200: Loss = -11105.625144786887
2
Iteration 15300: Loss = -11105.625066804663
Iteration 15400: Loss = -11105.628530122633
1
Iteration 15500: Loss = -11105.667089537068
2
Iteration 15600: Loss = -11105.63095071054
3
Iteration 15700: Loss = -11105.62835513042
4
Iteration 15800: Loss = -11105.65390462496
5
Iteration 15900: Loss = -11105.643761477357
6
Iteration 16000: Loss = -11105.624881293501
Iteration 16100: Loss = -11105.625326063397
1
Iteration 16200: Loss = -11105.625070739801
2
Iteration 16300: Loss = -11105.630714646673
3
Iteration 16400: Loss = -11105.634844477685
4
Iteration 16500: Loss = -11105.63486926795
5
Iteration 16600: Loss = -11105.624992531002
6
Iteration 16700: Loss = -11105.6249566474
Iteration 16800: Loss = -11105.660199922417
1
Iteration 16900: Loss = -11105.624896657104
Iteration 17000: Loss = -11105.626699540413
1
Iteration 17100: Loss = -11105.703631677472
2
Iteration 17200: Loss = -11105.63870270362
3
Iteration 17300: Loss = -11105.62875200564
4
Iteration 17400: Loss = -11105.625024707959
5
Iteration 17500: Loss = -11105.62819448296
6
Iteration 17600: Loss = -11105.628006400493
7
Iteration 17700: Loss = -11105.624879150058
Iteration 17800: Loss = -11105.62628274216
1
Iteration 17900: Loss = -11105.625031453088
2
Iteration 18000: Loss = -11105.625626694273
3
Iteration 18100: Loss = -11105.63440408043
4
Iteration 18200: Loss = -11105.631025231894
5
Iteration 18300: Loss = -11105.645507858944
6
Iteration 18400: Loss = -11105.624865557584
Iteration 18500: Loss = -11105.625581148766
1
Iteration 18600: Loss = -11105.79352984918
2
Iteration 18700: Loss = -11105.626368386687
3
Iteration 18800: Loss = -11105.625906762089
4
Iteration 18900: Loss = -11105.624875195512
Iteration 19000: Loss = -11105.626092348371
1
Iteration 19100: Loss = -11105.686677416275
2
Iteration 19200: Loss = -11105.6248840467
Iteration 19300: Loss = -11105.720493424575
1
Iteration 19400: Loss = -11105.624861916589
Iteration 19500: Loss = -11105.753972344783
1
Iteration 19600: Loss = -11105.624903989417
Iteration 19700: Loss = -11105.624926059585
Iteration 19800: Loss = -11105.690835214857
1
Iteration 19900: Loss = -11105.62625177439
2
pi: tensor([[0.7352, 0.2648],
        [0.2504, 0.7496]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5062, 0.4938], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3078, 0.0986],
         [0.6416, 0.1991]],

        [[0.7261, 0.0925],
         [0.7054, 0.6902]],

        [[0.5715, 0.1058],
         [0.5664, 0.5147]],

        [[0.5960, 0.0954],
         [0.5737, 0.6870]],

        [[0.7229, 0.0967],
         [0.6976, 0.5271]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9524808303943679
Average Adjusted Rand Index: 0.952642787795647
11126.658850058191
[0.9524808303943679, 0.9524808303943679] [0.952642787795647, 0.952642787795647] [11105.625273890259, 11105.628838077773]
-------------------------------------
This iteration is 86
True Objective function: Loss = -11149.720558210178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21999.226718175585
Iteration 100: Loss = -11424.712049602045
Iteration 200: Loss = -11422.857021570328
Iteration 300: Loss = -11420.74693851071
Iteration 400: Loss = -11415.972327517824
Iteration 500: Loss = -11401.075130153426
Iteration 600: Loss = -11252.486455948449
Iteration 700: Loss = -11205.775247384112
Iteration 800: Loss = -11187.933775919828
Iteration 900: Loss = -11184.870432412368
Iteration 1000: Loss = -11184.341794382044
Iteration 1100: Loss = -11183.476110621416
Iteration 1200: Loss = -11182.637148539443
Iteration 1300: Loss = -11181.580554858188
Iteration 1400: Loss = -11181.50651441056
Iteration 1500: Loss = -11175.987863711498
Iteration 1600: Loss = -11174.918743554621
Iteration 1700: Loss = -11174.794004271154
Iteration 1800: Loss = -11174.748801567017
Iteration 1900: Loss = -11174.686514019382
Iteration 2000: Loss = -11174.494346181978
Iteration 2100: Loss = -11173.64036220545
Iteration 2200: Loss = -11173.479971461002
Iteration 2300: Loss = -11173.119524940294
Iteration 2400: Loss = -11172.875321300045
Iteration 2500: Loss = -11172.76186177098
Iteration 2600: Loss = -11172.732722770703
Iteration 2700: Loss = -11172.699567113073
Iteration 2800: Loss = -11172.671623924309
Iteration 2900: Loss = -11172.626676756285
Iteration 3000: Loss = -11172.407914123976
Iteration 3100: Loss = -11172.388857066931
Iteration 3200: Loss = -11172.343604166817
Iteration 3300: Loss = -11172.154089180318
Iteration 3400: Loss = -11172.145190058733
Iteration 3500: Loss = -11172.12292184958
Iteration 3600: Loss = -11172.120331688096
Iteration 3700: Loss = -11172.118847350135
Iteration 3800: Loss = -11172.114109683791
Iteration 3900: Loss = -11172.10839867664
Iteration 4000: Loss = -11172.102047601356
Iteration 4100: Loss = -11172.111029663803
1
Iteration 4200: Loss = -11172.098758262884
Iteration 4300: Loss = -11172.097304493655
Iteration 4400: Loss = -11172.096062893137
Iteration 4500: Loss = -11172.095381405708
Iteration 4600: Loss = -11172.093931011086
Iteration 4700: Loss = -11172.092313773985
Iteration 4800: Loss = -11172.09115742614
Iteration 4900: Loss = -11172.090077867026
Iteration 5000: Loss = -11172.088655108932
Iteration 5100: Loss = -11172.088156301714
Iteration 5200: Loss = -11172.086284092666
Iteration 5300: Loss = -11172.089328760809
1
Iteration 5400: Loss = -11172.085650712304
Iteration 5500: Loss = -11172.083732249217
Iteration 5600: Loss = -11172.082097827284
Iteration 5700: Loss = -11172.000658070467
Iteration 5800: Loss = -11171.995676010461
Iteration 5900: Loss = -11171.99563608541
Iteration 6000: Loss = -11171.99468610691
Iteration 6100: Loss = -11171.994224488219
Iteration 6200: Loss = -11172.002233417856
1
Iteration 6300: Loss = -11171.992330567757
Iteration 6400: Loss = -11171.961763730918
Iteration 6500: Loss = -11171.874489690674
Iteration 6600: Loss = -11171.87351324374
Iteration 6700: Loss = -11171.872519344095
Iteration 6800: Loss = -11171.678079101308
Iteration 6900: Loss = -11171.656366270965
Iteration 7000: Loss = -11171.655654649296
Iteration 7100: Loss = -11171.652161435137
Iteration 7200: Loss = -11171.652132863675
Iteration 7300: Loss = -11171.653796808998
1
Iteration 7400: Loss = -11171.651623208749
Iteration 7500: Loss = -11171.651369680836
Iteration 7600: Loss = -11171.651252954838
Iteration 7700: Loss = -11171.651895984687
1
Iteration 7800: Loss = -11171.651976588168
2
Iteration 7900: Loss = -11171.676501326541
3
Iteration 8000: Loss = -11171.650627089704
Iteration 8100: Loss = -11171.650691880235
Iteration 8200: Loss = -11171.651449207668
1
Iteration 8300: Loss = -11171.650211092137
Iteration 8400: Loss = -11171.667200410628
1
Iteration 8500: Loss = -11171.649729762457
Iteration 8600: Loss = -11171.649697922803
Iteration 8700: Loss = -11171.649616810188
Iteration 8800: Loss = -11171.64952729592
Iteration 8900: Loss = -11171.649474958587
Iteration 9000: Loss = -11171.649370351335
Iteration 9100: Loss = -11171.833445667755
1
Iteration 9200: Loss = -11171.649236326304
Iteration 9300: Loss = -11171.6536668735
1
Iteration 9400: Loss = -11171.649122273151
Iteration 9500: Loss = -11171.649030464298
Iteration 9600: Loss = -11171.649580082596
1
Iteration 9700: Loss = -11171.648589264913
Iteration 9800: Loss = -11171.679062160641
1
Iteration 9900: Loss = -11171.648400275093
Iteration 10000: Loss = -11171.650512340868
1
Iteration 10100: Loss = -11171.656870866946
2
Iteration 10200: Loss = -11171.70577548537
3
Iteration 10300: Loss = -11171.6498944191
4
Iteration 10400: Loss = -11171.650336249542
5
Iteration 10500: Loss = -11171.649492289056
6
Iteration 10600: Loss = -11171.667740310544
7
Iteration 10700: Loss = -11171.652213547648
8
Iteration 10800: Loss = -11171.647687845478
Iteration 10900: Loss = -11171.65040266344
1
Iteration 11000: Loss = -11171.647362959886
Iteration 11100: Loss = -11171.647651079096
1
Iteration 11200: Loss = -11171.647413931301
Iteration 11300: Loss = -11171.648094903701
1
Iteration 11400: Loss = -11171.674679208189
2
Iteration 11500: Loss = -11171.647318139903
Iteration 11600: Loss = -11171.679952895687
1
Iteration 11700: Loss = -11171.65173116988
2
Iteration 11800: Loss = -11171.647153275819
Iteration 11900: Loss = -11171.648208577553
1
Iteration 12000: Loss = -11171.648843923449
2
Iteration 12100: Loss = -11171.645959589388
Iteration 12200: Loss = -11171.645901918935
Iteration 12300: Loss = -11171.645873793594
Iteration 12400: Loss = -11171.645487160582
Iteration 12500: Loss = -11171.64647415703
1
Iteration 12600: Loss = -11171.648734573517
2
Iteration 12700: Loss = -11171.645558047618
Iteration 12800: Loss = -11171.655482444035
1
Iteration 12900: Loss = -11171.651828328428
2
Iteration 13000: Loss = -11171.645513772757
Iteration 13100: Loss = -11171.647253757159
1
Iteration 13200: Loss = -11171.670311458507
2
Iteration 13300: Loss = -11171.646621343734
3
Iteration 13400: Loss = -11171.647214698018
4
Iteration 13500: Loss = -11171.650659913468
5
Iteration 13600: Loss = -11171.654758572162
6
Iteration 13700: Loss = -11171.645463133755
Iteration 13800: Loss = -11171.659090775667
1
Iteration 13900: Loss = -11171.645702509577
2
Iteration 14000: Loss = -11171.774539349102
3
Iteration 14100: Loss = -11171.64490985729
Iteration 14200: Loss = -11171.718294789709
1
Iteration 14300: Loss = -11171.643367501723
Iteration 14400: Loss = -11171.61240667249
Iteration 14500: Loss = -11171.625480387616
1
Iteration 14600: Loss = -11171.739990484004
2
Iteration 14700: Loss = -11171.609743232762
Iteration 14800: Loss = -11171.61311171399
1
Iteration 14900: Loss = -11171.609655757053
Iteration 15000: Loss = -11171.612051961009
1
Iteration 15100: Loss = -11171.609675150805
Iteration 15200: Loss = -11171.609695206096
Iteration 15300: Loss = -11171.61506171344
1
Iteration 15400: Loss = -11171.610889899286
2
Iteration 15500: Loss = -11171.609992213744
3
Iteration 15600: Loss = -11171.61000216032
4
Iteration 15700: Loss = -11171.67091056819
5
Iteration 15800: Loss = -11171.614810181463
6
Iteration 15900: Loss = -11171.609910325396
7
Iteration 16000: Loss = -11171.640860321153
8
Iteration 16100: Loss = -11171.609681691689
Iteration 16200: Loss = -11171.609622682554
Iteration 16300: Loss = -11171.609739239662
1
Iteration 16400: Loss = -11171.609846368006
2
Iteration 16500: Loss = -11171.609959788791
3
Iteration 16600: Loss = -11171.61305686094
4
Iteration 16700: Loss = -11171.608473472443
Iteration 16800: Loss = -11171.609052476637
1
Iteration 16900: Loss = -11171.625777881196
2
Iteration 17000: Loss = -11171.610480772075
3
Iteration 17100: Loss = -11171.62939359257
4
Iteration 17200: Loss = -11171.608594239391
5
Iteration 17300: Loss = -11171.610207761361
6
Iteration 17400: Loss = -11171.608398712058
Iteration 17500: Loss = -11171.609988003396
1
Iteration 17600: Loss = -11171.608425206074
Iteration 17700: Loss = -11171.608768674434
1
Iteration 17800: Loss = -11171.608333668053
Iteration 17900: Loss = -11171.6160445199
1
Iteration 18000: Loss = -11171.608339723918
Iteration 18100: Loss = -11171.62777936363
1
Iteration 18200: Loss = -11171.6082798579
Iteration 18300: Loss = -11171.608537842825
1
Iteration 18400: Loss = -11171.608849245544
2
Iteration 18500: Loss = -11171.612394093216
3
Iteration 18600: Loss = -11171.608322198197
Iteration 18700: Loss = -11171.608233633286
Iteration 18800: Loss = -11171.613788705154
1
Iteration 18900: Loss = -11171.608180377447
Iteration 19000: Loss = -11171.608337581234
1
Iteration 19100: Loss = -11171.608293526539
2
Iteration 19200: Loss = -11171.608205859606
Iteration 19300: Loss = -11171.608316677897
1
Iteration 19400: Loss = -11171.60851036086
2
Iteration 19500: Loss = -11171.621818520038
3
Iteration 19600: Loss = -11171.831359718812
4
Iteration 19700: Loss = -11171.608177874417
Iteration 19800: Loss = -11171.610241145714
1
Iteration 19900: Loss = -11171.60832904068
2
pi: tensor([[0.6882, 0.3118],
        [0.2375, 0.7625]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9246, 0.0754], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1874, 0.0950],
         [0.5558, 0.3123]],

        [[0.6297, 0.1115],
         [0.6791, 0.5672]],

        [[0.5575, 0.1116],
         [0.5193, 0.7104]],

        [[0.5432, 0.1030],
         [0.7030, 0.7308]],

        [[0.5833, 0.0834],
         [0.6222, 0.6359]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0037746410354473374
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5951852217510649
Average Adjusted Rand Index: 0.7674042621112708
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23439.917727585485
Iteration 100: Loss = -11140.415780792093
Iteration 200: Loss = -11125.87785804987
Iteration 300: Loss = -11125.501389703339
Iteration 400: Loss = -11125.380827620233
Iteration 500: Loss = -11125.320302394535
Iteration 600: Loss = -11125.28496545053
Iteration 700: Loss = -11125.262494378258
Iteration 800: Loss = -11125.247379067836
Iteration 900: Loss = -11125.236569451301
Iteration 1000: Loss = -11125.228609481901
Iteration 1100: Loss = -11125.222535430932
Iteration 1200: Loss = -11125.217832647259
Iteration 1300: Loss = -11125.214081928969
Iteration 1400: Loss = -11125.211051872908
Iteration 1500: Loss = -11125.20858802179
Iteration 1600: Loss = -11125.206497655412
Iteration 1700: Loss = -11125.204735113324
Iteration 1800: Loss = -11125.203257888757
Iteration 1900: Loss = -11125.201977027333
Iteration 2000: Loss = -11125.200912791595
Iteration 2100: Loss = -11125.19995943044
Iteration 2200: Loss = -11125.199144011121
Iteration 2300: Loss = -11125.198412449066
Iteration 2400: Loss = -11125.197797769211
Iteration 2500: Loss = -11125.197149294496
Iteration 2600: Loss = -11125.196661765387
Iteration 2700: Loss = -11125.196215167038
Iteration 2800: Loss = -11125.195782907442
Iteration 2900: Loss = -11125.195386292988
Iteration 3000: Loss = -11125.195084315394
Iteration 3100: Loss = -11125.195295927157
1
Iteration 3200: Loss = -11125.194953022114
Iteration 3300: Loss = -11125.194423407775
Iteration 3400: Loss = -11125.199280741017
1
Iteration 3500: Loss = -11125.198071193592
2
Iteration 3600: Loss = -11125.193621619333
Iteration 3700: Loss = -11125.193458222662
Iteration 3800: Loss = -11125.193594244982
1
Iteration 3900: Loss = -11125.194879175522
2
Iteration 4000: Loss = -11125.192940543511
Iteration 4100: Loss = -11125.193606277217
1
Iteration 4200: Loss = -11125.19419205065
2
Iteration 4300: Loss = -11125.192608214713
Iteration 4400: Loss = -11125.193916481405
1
Iteration 4500: Loss = -11125.195228164304
2
Iteration 4600: Loss = -11125.192343093002
Iteration 4700: Loss = -11125.192256908356
Iteration 4800: Loss = -11125.192191613498
Iteration 4900: Loss = -11125.192130277543
Iteration 5000: Loss = -11125.192020371995
Iteration 5100: Loss = -11125.192069297022
Iteration 5200: Loss = -11125.192103684662
Iteration 5300: Loss = -11125.192238805967
1
Iteration 5400: Loss = -11125.197548631948
2
Iteration 5500: Loss = -11125.191747078301
Iteration 5600: Loss = -11125.1940591559
1
Iteration 5700: Loss = -11125.191666588275
Iteration 5800: Loss = -11125.19159991151
Iteration 5900: Loss = -11125.191811819308
1
Iteration 6000: Loss = -11125.191565182864
Iteration 6100: Loss = -11125.192481401422
1
Iteration 6200: Loss = -11125.191525768747
Iteration 6300: Loss = -11125.191515742636
Iteration 6400: Loss = -11125.191758577492
1
Iteration 6500: Loss = -11125.191741745579
2
Iteration 6600: Loss = -11125.191474652891
Iteration 6700: Loss = -11125.19173218503
1
Iteration 6800: Loss = -11125.192746772715
2
Iteration 6900: Loss = -11125.204150440188
3
Iteration 7000: Loss = -11125.192869013808
4
Iteration 7100: Loss = -11125.192051487656
5
Iteration 7200: Loss = -11125.194177217974
6
Iteration 7300: Loss = -11125.192162456406
7
Iteration 7400: Loss = -11125.192692067681
8
Iteration 7500: Loss = -11125.191888497719
9
Iteration 7600: Loss = -11125.19134931241
Iteration 7700: Loss = -11125.191295709976
Iteration 7800: Loss = -11125.196313630662
1
Iteration 7900: Loss = -11125.194072075672
2
Iteration 8000: Loss = -11125.191242653185
Iteration 8100: Loss = -11125.191251629407
Iteration 8200: Loss = -11125.191584502882
1
Iteration 8300: Loss = -11125.191421621286
2
Iteration 8400: Loss = -11125.19117759713
Iteration 8500: Loss = -11125.194548315289
1
Iteration 8600: Loss = -11125.190991988025
Iteration 8700: Loss = -11125.202059588313
1
Iteration 8800: Loss = -11125.190947018958
Iteration 8900: Loss = -11125.206127041154
1
Iteration 9000: Loss = -11125.218237049878
2
Iteration 9100: Loss = -11125.190974659183
Iteration 9200: Loss = -11125.193940819649
1
Iteration 9300: Loss = -11125.205843981536
2
Iteration 9400: Loss = -11125.19092334219
Iteration 9500: Loss = -11125.190957134379
Iteration 9600: Loss = -11125.196528392153
1
Iteration 9700: Loss = -11125.337005374642
2
Iteration 9800: Loss = -11125.191303501575
3
Iteration 9900: Loss = -11125.190945229104
Iteration 10000: Loss = -11125.191550994707
1
Iteration 10100: Loss = -11125.19541843159
2
Iteration 10200: Loss = -11125.236015217359
3
Iteration 10300: Loss = -11125.190935033472
Iteration 10400: Loss = -11125.512383462485
1
Iteration 10500: Loss = -11125.191531579681
2
Iteration 10600: Loss = -11125.234327091299
3
Iteration 10700: Loss = -11125.199032169114
4
Iteration 10800: Loss = -11125.364110879817
5
Iteration 10900: Loss = -11125.19091674416
Iteration 11000: Loss = -11125.221737495893
1
Iteration 11100: Loss = -11125.245160644174
2
Iteration 11200: Loss = -11125.19836903653
3
Iteration 11300: Loss = -11125.191755343822
4
Iteration 11400: Loss = -11125.19128367013
5
Iteration 11500: Loss = -11125.191895618533
6
Iteration 11600: Loss = -11125.191626375125
7
Iteration 11700: Loss = -11125.19090380602
Iteration 11800: Loss = -11125.190923326863
Iteration 11900: Loss = -11125.191258683744
1
Iteration 12000: Loss = -11125.19111898112
2
Iteration 12100: Loss = -11125.200725288681
3
Iteration 12200: Loss = -11125.192153294553
4
Iteration 12300: Loss = -11125.19095644876
Iteration 12400: Loss = -11125.197279146836
1
Iteration 12500: Loss = -11125.197243464112
2
Iteration 12600: Loss = -11125.191729276039
3
Iteration 12700: Loss = -11125.196937225845
4
Iteration 12800: Loss = -11125.191067388889
5
Iteration 12900: Loss = -11125.191001992622
Iteration 13000: Loss = -11125.20936002303
1
Iteration 13100: Loss = -11125.191056144653
Iteration 13200: Loss = -11125.199967376695
1
Iteration 13300: Loss = -11125.233035898436
2
Iteration 13400: Loss = -11125.190849942408
Iteration 13500: Loss = -11125.19219969049
1
Iteration 13600: Loss = -11125.265266502749
2
Iteration 13700: Loss = -11125.190847501606
Iteration 13800: Loss = -11125.20684151764
1
Iteration 13900: Loss = -11125.207480994153
2
Iteration 14000: Loss = -11125.191714030767
3
Iteration 14100: Loss = -11125.192355948884
4
Iteration 14200: Loss = -11125.361452487243
5
Iteration 14300: Loss = -11125.1908667616
Iteration 14400: Loss = -11125.194110925686
1
Iteration 14500: Loss = -11125.19087145214
Iteration 14600: Loss = -11125.191848251252
1
Iteration 14700: Loss = -11125.190911885285
Iteration 14800: Loss = -11125.191264067393
1
Iteration 14900: Loss = -11125.191853653108
2
Iteration 15000: Loss = -11125.194699344367
3
Iteration 15100: Loss = -11125.207712017345
4
Iteration 15200: Loss = -11125.19429594089
5
Iteration 15300: Loss = -11125.19487187422
6
Iteration 15400: Loss = -11125.191497503756
7
Iteration 15500: Loss = -11125.195612846146
8
Iteration 15600: Loss = -11125.193605294762
9
Iteration 15700: Loss = -11125.218886720282
10
Iteration 15800: Loss = -11125.192022708778
11
Iteration 15900: Loss = -11125.243518846677
12
Iteration 16000: Loss = -11125.19634846708
13
Iteration 16100: Loss = -11125.192207849348
14
Iteration 16200: Loss = -11125.193669566625
15
Stopping early at iteration 16200 due to no improvement.
pi: tensor([[0.7600, 0.2400],
        [0.2400, 0.7600]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5406, 0.4594], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1958, 0.1020],
         [0.6665, 0.3050]],

        [[0.6243, 0.1091],
         [0.5604, 0.5482]],

        [[0.5045, 0.1109],
         [0.5434, 0.5112]],

        [[0.6305, 0.1027],
         [0.7150, 0.6397]],

        [[0.5266, 0.0832],
         [0.5460, 0.5689]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446733085256775
Average Adjusted Rand Index: 0.9451292516044927
11149.720558210178
[0.5951852217510649, 0.9446733085256775] [0.7674042621112708, 0.9451292516044927] [11171.608222759523, 11125.193669566625]
-------------------------------------
This iteration is 87
True Objective function: Loss = -11167.822974129685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24939.23528526958
Iteration 100: Loss = -11451.530900440865
Iteration 200: Loss = -11450.385262224549
Iteration 300: Loss = -11449.53639405084
Iteration 400: Loss = -11449.030214834673
Iteration 500: Loss = -11448.357512167313
Iteration 600: Loss = -11445.129379353526
Iteration 700: Loss = -11444.44133728334
Iteration 800: Loss = -11444.227240771175
Iteration 900: Loss = -11444.105521488618
Iteration 1000: Loss = -11443.951586885114
Iteration 1100: Loss = -11443.840616300831
Iteration 1200: Loss = -11443.797595921766
Iteration 1300: Loss = -11443.774254827658
Iteration 1400: Loss = -11443.758620023627
Iteration 1500: Loss = -11443.746936527452
Iteration 1600: Loss = -11443.73770374553
Iteration 1700: Loss = -11443.730192172734
Iteration 1800: Loss = -11443.723866870476
Iteration 1900: Loss = -11443.718474143354
Iteration 2000: Loss = -11443.713846621133
Iteration 2100: Loss = -11443.709792655138
Iteration 2200: Loss = -11443.70623532633
Iteration 2300: Loss = -11443.703082286942
Iteration 2400: Loss = -11443.700255446442
Iteration 2500: Loss = -11443.697674160252
Iteration 2600: Loss = -11443.695323327705
Iteration 2700: Loss = -11443.693167085943
Iteration 2800: Loss = -11443.691136632477
Iteration 2900: Loss = -11443.689203253218
Iteration 3000: Loss = -11443.687329045484
Iteration 3100: Loss = -11443.685523359376
Iteration 3200: Loss = -11443.683695160373
Iteration 3300: Loss = -11443.68178132937
Iteration 3400: Loss = -11443.679675230062
Iteration 3500: Loss = -11443.677299020386
Iteration 3600: Loss = -11443.67435976329
Iteration 3700: Loss = -11443.670472824
Iteration 3800: Loss = -11443.664869958515
Iteration 3900: Loss = -11443.658352287666
Iteration 4000: Loss = -11443.6431113685
Iteration 4100: Loss = -11443.631986392278
Iteration 4200: Loss = -11443.638305968256
1
Iteration 4300: Loss = -11443.626338896656
Iteration 4400: Loss = -11443.625063590482
Iteration 4500: Loss = -11443.631772203638
1
Iteration 4600: Loss = -11443.623518762739
Iteration 4700: Loss = -11443.622986699022
Iteration 4800: Loss = -11443.622615432801
Iteration 4900: Loss = -11443.622196256678
Iteration 5000: Loss = -11443.623042827176
1
Iteration 5100: Loss = -11443.621632280683
Iteration 5200: Loss = -11443.621465046901
Iteration 5300: Loss = -11443.62117209073
Iteration 5400: Loss = -11443.620954272203
Iteration 5500: Loss = -11443.620794959761
Iteration 5600: Loss = -11443.62063744993
Iteration 5700: Loss = -11443.63889413889
1
Iteration 5800: Loss = -11443.620318694999
Iteration 5900: Loss = -11443.620200847288
Iteration 6000: Loss = -11443.620065144845
Iteration 6100: Loss = -11443.61998424609
Iteration 6200: Loss = -11443.619881085826
Iteration 6300: Loss = -11443.619746955528
Iteration 6400: Loss = -11443.61968585217
Iteration 6500: Loss = -11443.625441477488
1
Iteration 6600: Loss = -11443.619493920418
Iteration 6700: Loss = -11443.619448627975
Iteration 6800: Loss = -11443.61935024698
Iteration 6900: Loss = -11443.619287412355
Iteration 7000: Loss = -11443.61984486727
1
Iteration 7100: Loss = -11443.619105708065
Iteration 7200: Loss = -11443.62052486958
1
Iteration 7300: Loss = -11443.619025198766
Iteration 7400: Loss = -11443.618952088182
Iteration 7500: Loss = -11443.619109657278
1
Iteration 7600: Loss = -11443.618907792514
Iteration 7700: Loss = -11443.664680388165
1
Iteration 7800: Loss = -11443.618759667068
Iteration 7900: Loss = -11443.618748602332
Iteration 8000: Loss = -11443.620603120278
1
Iteration 8100: Loss = -11443.618641325616
Iteration 8200: Loss = -11443.653029283289
1
Iteration 8300: Loss = -11443.618600751632
Iteration 8400: Loss = -11443.62258495256
1
Iteration 8500: Loss = -11443.61853543645
Iteration 8600: Loss = -11443.618975701374
1
Iteration 8700: Loss = -11443.618473417757
Iteration 8800: Loss = -11443.644898077247
1
Iteration 8900: Loss = -11443.618408427312
Iteration 9000: Loss = -11443.618376443788
Iteration 9100: Loss = -11443.618348536625
Iteration 9200: Loss = -11443.61835935647
Iteration 9300: Loss = -11443.618535287487
1
Iteration 9400: Loss = -11443.618287495856
Iteration 9500: Loss = -11443.618276418161
Iteration 9600: Loss = -11443.621740210338
1
Iteration 9700: Loss = -11443.618259011284
Iteration 9800: Loss = -11443.618200199198
Iteration 9900: Loss = -11443.618202179123
Iteration 10000: Loss = -11443.618315575439
1
Iteration 10100: Loss = -11443.6181965649
Iteration 10200: Loss = -11443.618182560846
Iteration 10300: Loss = -11443.618279039929
Iteration 10400: Loss = -11443.61813466076
Iteration 10500: Loss = -11443.692412511824
1
Iteration 10600: Loss = -11443.618109622914
Iteration 10700: Loss = -11443.618087276178
Iteration 10800: Loss = -11443.618082652076
Iteration 10900: Loss = -11443.618062889529
Iteration 11000: Loss = -11443.618989279423
1
Iteration 11100: Loss = -11443.618452581793
2
Iteration 11200: Loss = -11443.619402026317
3
Iteration 11300: Loss = -11443.62396250613
4
Iteration 11400: Loss = -11443.618123435408
Iteration 11500: Loss = -11443.618064628063
Iteration 11600: Loss = -11443.622802625841
1
Iteration 11700: Loss = -11443.625644786822
2
Iteration 11800: Loss = -11443.618029936079
Iteration 11900: Loss = -11443.618294783604
1
Iteration 12000: Loss = -11443.618025279764
Iteration 12100: Loss = -11443.620646150528
1
Iteration 12200: Loss = -11443.619045366228
2
Iteration 12300: Loss = -11443.622774410764
3
Iteration 12400: Loss = -11443.618052301588
Iteration 12500: Loss = -11443.623563425443
1
Iteration 12600: Loss = -11443.618040845531
Iteration 12700: Loss = -11443.618738143798
1
Iteration 12800: Loss = -11443.61823918758
2
Iteration 12900: Loss = -11443.618004815835
Iteration 13000: Loss = -11443.753232253799
1
Iteration 13100: Loss = -11443.617936843886
Iteration 13200: Loss = -11443.643080182974
1
Iteration 13300: Loss = -11443.620366520568
2
Iteration 13400: Loss = -11443.617977512322
Iteration 13500: Loss = -11443.62194558478
1
Iteration 13600: Loss = -11443.619125844994
2
Iteration 13700: Loss = -11443.631701998764
3
Iteration 13800: Loss = -11443.637584373524
4
Iteration 13900: Loss = -11443.618453244104
5
Iteration 14000: Loss = -11443.61806807758
Iteration 14100: Loss = -11443.618445739705
1
Iteration 14200: Loss = -11443.66037914497
2
Iteration 14300: Loss = -11443.63227659038
3
Iteration 14400: Loss = -11443.637929384286
4
Iteration 14500: Loss = -11443.739081094112
5
Iteration 14600: Loss = -11443.664658486885
6
Iteration 14700: Loss = -11443.618054851988
Iteration 14800: Loss = -11443.618120509147
Iteration 14900: Loss = -11443.619984224279
1
Iteration 15000: Loss = -11443.61871785894
2
Iteration 15100: Loss = -11443.618224294769
3
Iteration 15200: Loss = -11443.627694287321
4
Iteration 15300: Loss = -11443.619437299452
5
Iteration 15400: Loss = -11443.619764166924
6
Iteration 15500: Loss = -11443.626520425794
7
Iteration 15600: Loss = -11443.617922888898
Iteration 15700: Loss = -11443.619964953316
1
Iteration 15800: Loss = -11443.628391000448
2
Iteration 15900: Loss = -11443.633624900513
3
Iteration 16000: Loss = -11443.618352308726
4
Iteration 16100: Loss = -11443.619443519401
5
Iteration 16200: Loss = -11443.64933839213
6
Iteration 16300: Loss = -11443.618042443692
7
Iteration 16400: Loss = -11443.618056598989
8
Iteration 16500: Loss = -11443.778406725361
9
Iteration 16600: Loss = -11443.618086553992
10
Iteration 16700: Loss = -11443.631593114926
11
Iteration 16800: Loss = -11443.618370514527
12
Iteration 16900: Loss = -11443.618056153142
13
Iteration 17000: Loss = -11443.6182766753
14
Iteration 17100: Loss = -11443.623139136253
15
Stopping early at iteration 17100 due to no improvement.
pi: tensor([[9.1863e-01, 8.1371e-02],
        [1.0000e+00, 8.1941e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9917, 0.0083], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1734, 0.1134],
         [0.7194, 0.3213]],

        [[0.7252, 0.2141],
         [0.6632, 0.7132]],

        [[0.5671, 0.2017],
         [0.5909, 0.6092]],

        [[0.6771, 0.0830],
         [0.6311, 0.5311]],

        [[0.5877, 0.2273],
         [0.5718, 0.5476]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.044214004900005696
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -0.0002625030891303614
Average Adjusted Rand Index: 0.009004545229993052
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23503.77429654762
Iteration 100: Loss = -11451.451367047923
Iteration 200: Loss = -11450.032491693291
Iteration 300: Loss = -11448.3954234471
Iteration 400: Loss = -11446.251254610432
Iteration 500: Loss = -11444.986265374226
Iteration 600: Loss = -11443.276001473152
Iteration 700: Loss = -11442.022220184977
Iteration 800: Loss = -11438.38678493069
Iteration 900: Loss = -11263.353385262368
Iteration 1000: Loss = -11226.98203441955
Iteration 1100: Loss = -11220.232568807414
Iteration 1200: Loss = -11219.586295982082
Iteration 1300: Loss = -11214.660947669467
Iteration 1400: Loss = -11214.507834753644
Iteration 1500: Loss = -11214.411719349522
Iteration 1600: Loss = -11214.347080142068
Iteration 1700: Loss = -11214.29760682589
Iteration 1800: Loss = -11214.200371867684
Iteration 1900: Loss = -11213.938223450627
Iteration 2000: Loss = -11213.907105246026
Iteration 2100: Loss = -11212.826200789046
Iteration 2200: Loss = -11212.800343856905
Iteration 2300: Loss = -11212.77957179014
Iteration 2400: Loss = -11212.764485657117
Iteration 2500: Loss = -11212.751384953577
Iteration 2600: Loss = -11212.737803055705
Iteration 2700: Loss = -11212.721152184093
Iteration 2800: Loss = -11212.703391200886
Iteration 2900: Loss = -11212.680607326143
Iteration 3000: Loss = -11212.641277825127
Iteration 3100: Loss = -11212.54872626527
Iteration 3200: Loss = -11212.16620025583
Iteration 3300: Loss = -11211.730472097905
Iteration 3400: Loss = -11211.443895361608
Iteration 3500: Loss = -11211.262126277366
Iteration 3600: Loss = -11211.106219884401
Iteration 3700: Loss = -11209.201017369325
Iteration 3800: Loss = -11208.935645974327
Iteration 3900: Loss = -11208.757781309681
Iteration 4000: Loss = -11208.733176690177
Iteration 4100: Loss = -11208.724771500936
Iteration 4200: Loss = -11208.679485868966
Iteration 4300: Loss = -11208.51649104088
Iteration 4400: Loss = -11208.503599696161
Iteration 4500: Loss = -11208.485947306872
Iteration 4600: Loss = -11208.483460053994
Iteration 4700: Loss = -11208.481447982975
Iteration 4800: Loss = -11208.480178904761
Iteration 4900: Loss = -11208.4850690608
1
Iteration 5000: Loss = -11208.477656668576
Iteration 5100: Loss = -11208.47724746605
Iteration 5200: Loss = -11208.479099265163
1
Iteration 5300: Loss = -11208.475040285499
Iteration 5400: Loss = -11208.474052599398
Iteration 5500: Loss = -11208.473301372476
Iteration 5600: Loss = -11208.470916228955
Iteration 5700: Loss = -11208.470361082045
Iteration 5800: Loss = -11208.472781813045
1
Iteration 5900: Loss = -11208.467684481297
Iteration 6000: Loss = -11208.462607482208
Iteration 6100: Loss = -11208.46328760994
1
Iteration 6200: Loss = -11208.460698143614
Iteration 6300: Loss = -11208.460239154358
Iteration 6400: Loss = -11208.465607006849
1
Iteration 6500: Loss = -11208.458877424748
Iteration 6600: Loss = -11208.458222067324
Iteration 6700: Loss = -11208.457768639299
Iteration 6800: Loss = -11208.458266363321
1
Iteration 6900: Loss = -11208.458089141337
2
Iteration 7000: Loss = -11208.45825423008
3
Iteration 7100: Loss = -11208.456884595384
Iteration 7200: Loss = -11208.45667635129
Iteration 7300: Loss = -11208.45656873595
Iteration 7400: Loss = -11208.45656636593
Iteration 7500: Loss = -11208.456566808178
Iteration 7600: Loss = -11208.458331754608
1
Iteration 7700: Loss = -11208.456294779971
Iteration 7800: Loss = -11208.459106550074
1
Iteration 7900: Loss = -11208.457715226175
2
Iteration 8000: Loss = -11208.45626193049
Iteration 8100: Loss = -11208.455359668209
Iteration 8200: Loss = -11208.456044960916
1
Iteration 8300: Loss = -11208.516791976415
2
Iteration 8400: Loss = -11208.454945525938
Iteration 8500: Loss = -11208.454811784128
Iteration 8600: Loss = -11208.45471717769
Iteration 8700: Loss = -11208.454494683856
Iteration 8800: Loss = -11208.660696918647
1
Iteration 8900: Loss = -11208.454260949138
Iteration 9000: Loss = -11208.454189291093
Iteration 9100: Loss = -11208.45599139677
1
Iteration 9200: Loss = -11208.454008152778
Iteration 9300: Loss = -11208.45404041441
Iteration 9400: Loss = -11208.453968899696
Iteration 9500: Loss = -11208.453884631157
Iteration 9600: Loss = -11208.453803211687
Iteration 9700: Loss = -11208.454062654677
1
Iteration 9800: Loss = -11208.453749569358
Iteration 9900: Loss = -11208.45370966427
Iteration 10000: Loss = -11208.453957921249
1
Iteration 10100: Loss = -11208.453613610285
Iteration 10200: Loss = -11208.453460915145
Iteration 10300: Loss = -11208.463799361407
1
Iteration 10400: Loss = -11208.453342564
Iteration 10500: Loss = -11208.453339499707
Iteration 10600: Loss = -11208.454788243593
1
Iteration 10700: Loss = -11208.453265224593
Iteration 10800: Loss = -11208.453248844728
Iteration 10900: Loss = -11208.454414542603
1
Iteration 11000: Loss = -11208.453196001421
Iteration 11100: Loss = -11208.475381048174
1
Iteration 11200: Loss = -11208.453151157053
Iteration 11300: Loss = -11208.453092138927
Iteration 11400: Loss = -11208.453213992272
1
Iteration 11500: Loss = -11208.46060895639
2
Iteration 11600: Loss = -11208.47579371631
3
Iteration 11700: Loss = -11208.52875396734
4
Iteration 11800: Loss = -11208.455524605784
5
Iteration 11900: Loss = -11208.478369368235
6
Iteration 12000: Loss = -11208.464814437964
7
Iteration 12100: Loss = -11208.45747522355
8
Iteration 12200: Loss = -11208.453217449702
9
Iteration 12300: Loss = -11208.456337636706
10
Iteration 12400: Loss = -11208.452994987554
Iteration 12500: Loss = -11208.452977287841
Iteration 12600: Loss = -11208.453333669237
1
Iteration 12700: Loss = -11208.453012526139
Iteration 12800: Loss = -11208.453021473864
Iteration 12900: Loss = -11208.453053470177
Iteration 13000: Loss = -11208.459470805468
1
Iteration 13100: Loss = -11208.45235629175
Iteration 13200: Loss = -11208.456221639692
1
Iteration 13300: Loss = -11208.452224182785
Iteration 13400: Loss = -11208.45310996034
1
Iteration 13500: Loss = -11208.456219676791
2
Iteration 13600: Loss = -11208.452678341604
3
Iteration 13700: Loss = -11208.452473029236
4
Iteration 13800: Loss = -11208.467665239737
5
Iteration 13900: Loss = -11208.45235807125
6
Iteration 14000: Loss = -11208.592744621752
7
Iteration 14100: Loss = -11208.45774218802
8
Iteration 14200: Loss = -11208.452494942503
9
Iteration 14300: Loss = -11208.456113695107
10
Iteration 14400: Loss = -11208.531993727882
11
Iteration 14500: Loss = -11208.4522082193
Iteration 14600: Loss = -11208.45370291865
1
Iteration 14700: Loss = -11208.452521463927
2
Iteration 14800: Loss = -11208.45226077148
Iteration 14900: Loss = -11208.468911417907
1
Iteration 15000: Loss = -11208.455718629153
2
Iteration 15100: Loss = -11208.452788359935
3
Iteration 15200: Loss = -11208.452349242756
Iteration 15300: Loss = -11208.45346479157
1
Iteration 15400: Loss = -11208.4566213944
2
Iteration 15500: Loss = -11208.621578499486
3
Iteration 15600: Loss = -11208.451720867526
Iteration 15700: Loss = -11208.452235738905
1
Iteration 15800: Loss = -11208.454361609496
2
Iteration 15900: Loss = -11208.471032158062
3
Iteration 16000: Loss = -11208.611462875817
4
Iteration 16100: Loss = -11208.454743094771
5
Iteration 16200: Loss = -11208.4562159427
6
Iteration 16300: Loss = -11208.45608427222
7
Iteration 16400: Loss = -11208.453525593042
8
Iteration 16500: Loss = -11208.452178345016
9
Iteration 16600: Loss = -11208.452395873652
10
Iteration 16700: Loss = -11208.509682902382
11
Iteration 16800: Loss = -11208.542694744774
12
Iteration 16900: Loss = -11208.505125424397
13
Iteration 17000: Loss = -11208.462478487834
14
Iteration 17100: Loss = -11208.456382062403
15
Stopping early at iteration 17100 due to no improvement.
pi: tensor([[0.7354, 0.2646],
        [0.3169, 0.6831]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1576, 0.8424], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3119, 0.1086],
         [0.6279, 0.1925]],

        [[0.6577, 0.1003],
         [0.6962, 0.6846]],

        [[0.5812, 0.1044],
         [0.5279, 0.6432]],

        [[0.5518, 0.1105],
         [0.5455, 0.6581]],

        [[0.5169, 0.0940],
         [0.5348, 0.6873]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.04196837565353897
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5174570272176271
Average Adjusted Rand Index: 0.7384902588024711
11167.822974129685
[-0.0002625030891303614, 0.5174570272176271] [0.009004545229993052, 0.7384902588024711] [11443.623139136253, 11208.456382062403]
-------------------------------------
This iteration is 88
True Objective function: Loss = -11273.743954662514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18777.39481521283
Iteration 100: Loss = -11588.916199672201
Iteration 200: Loss = -11584.733874143662
Iteration 300: Loss = -11577.5587622943
Iteration 400: Loss = -11543.455740592583
Iteration 500: Loss = -11420.87180356269
Iteration 600: Loss = -11394.795797720855
Iteration 700: Loss = -11371.711512558448
Iteration 800: Loss = -11360.363082841355
Iteration 900: Loss = -11359.412291931998
Iteration 1000: Loss = -11349.743307235545
Iteration 1100: Loss = -11348.96354594031
Iteration 1200: Loss = -11343.741684381359
Iteration 1300: Loss = -11343.701152124086
Iteration 1400: Loss = -11343.618723039333
Iteration 1500: Loss = -11343.593577989599
Iteration 1600: Loss = -11343.55647043939
Iteration 1700: Loss = -11343.509331410543
Iteration 1800: Loss = -11343.501322747747
Iteration 1900: Loss = -11343.4957395026
Iteration 2000: Loss = -11343.49024224707
Iteration 2100: Loss = -11343.485955319853
Iteration 2200: Loss = -11343.481913773694
Iteration 2300: Loss = -11343.475491370238
Iteration 2400: Loss = -11343.469867555079
Iteration 2500: Loss = -11343.468844987921
Iteration 2600: Loss = -11343.466537661174
Iteration 2700: Loss = -11343.465103379998
Iteration 2800: Loss = -11343.463799548634
Iteration 2900: Loss = -11343.462817616175
Iteration 3000: Loss = -11343.461537545736
Iteration 3100: Loss = -11343.460476603555
Iteration 3200: Loss = -11343.46926791848
1
Iteration 3300: Loss = -11343.45534837516
Iteration 3400: Loss = -11343.457305466394
1
Iteration 3500: Loss = -11343.454807322621
Iteration 3600: Loss = -11343.451003059196
Iteration 3700: Loss = -11343.45031519069
Iteration 3800: Loss = -11343.449727753989
Iteration 3900: Loss = -11343.449187365939
Iteration 4000: Loss = -11343.44828246611
Iteration 4100: Loss = -11343.447669134594
Iteration 4200: Loss = -11343.447098237435
Iteration 4300: Loss = -11343.445183343947
Iteration 4400: Loss = -11343.41959258644
Iteration 4500: Loss = -11343.4218698026
1
Iteration 4600: Loss = -11343.419011412216
Iteration 4700: Loss = -11343.418711236971
Iteration 4800: Loss = -11343.418443547076
Iteration 4900: Loss = -11343.418021140902
Iteration 5000: Loss = -11343.41744707787
Iteration 5100: Loss = -11343.416454132457
Iteration 5200: Loss = -11343.411979763401
Iteration 5300: Loss = -11343.41149332381
Iteration 5400: Loss = -11343.411444808162
Iteration 5500: Loss = -11343.411012016804
Iteration 5600: Loss = -11343.410882172653
Iteration 5700: Loss = -11343.410736212883
Iteration 5800: Loss = -11343.410686632946
Iteration 5900: Loss = -11343.410528542709
Iteration 6000: Loss = -11343.415305598795
1
Iteration 6100: Loss = -11343.410307210092
Iteration 6200: Loss = -11343.410258980719
Iteration 6300: Loss = -11343.410182875225
Iteration 6400: Loss = -11343.410073288058
Iteration 6500: Loss = -11343.411105985771
1
Iteration 6600: Loss = -11343.409945682366
Iteration 6700: Loss = -11343.410554203874
1
Iteration 6800: Loss = -11343.41020232705
2
Iteration 6900: Loss = -11343.409902228022
Iteration 7000: Loss = -11343.409642418039
Iteration 7100: Loss = -11343.410398599284
1
Iteration 7200: Loss = -11343.409577140354
Iteration 7300: Loss = -11343.409449661669
Iteration 7400: Loss = -11343.409444296065
Iteration 7500: Loss = -11343.4093775438
Iteration 7600: Loss = -11343.409338286436
Iteration 7700: Loss = -11343.596694509197
1
Iteration 7800: Loss = -11343.409261868144
Iteration 7900: Loss = -11343.409225687494
Iteration 8000: Loss = -11343.409175178025
Iteration 8100: Loss = -11343.414094773198
1
Iteration 8200: Loss = -11343.409131232069
Iteration 8300: Loss = -11343.40906585781
Iteration 8400: Loss = -11343.40859206526
Iteration 8500: Loss = -11343.399830795546
Iteration 8600: Loss = -11343.39973593675
Iteration 8700: Loss = -11343.399684195396
Iteration 8800: Loss = -11343.399742487098
Iteration 8900: Loss = -11343.399542704525
Iteration 9000: Loss = -11343.509255298512
1
Iteration 9100: Loss = -11343.39946231173
Iteration 9200: Loss = -11343.399421200169
Iteration 9300: Loss = -11343.439992522699
1
Iteration 9400: Loss = -11343.399405007089
Iteration 9500: Loss = -11343.399414068483
Iteration 9600: Loss = -11343.399762568892
1
Iteration 9700: Loss = -11343.399361874648
Iteration 9800: Loss = -11343.404861137762
1
Iteration 9900: Loss = -11343.399353705607
Iteration 10000: Loss = -11343.496465535927
1
Iteration 10100: Loss = -11343.399293563014
Iteration 10200: Loss = -11343.71788809531
1
Iteration 10300: Loss = -11343.399314133836
Iteration 10400: Loss = -11343.44430278443
1
Iteration 10500: Loss = -11343.39929708965
Iteration 10600: Loss = -11343.399383131606
Iteration 10700: Loss = -11343.399320148043
Iteration 10800: Loss = -11343.399283214203
Iteration 10900: Loss = -11343.399972669115
1
Iteration 11000: Loss = -11343.399262224952
Iteration 11100: Loss = -11343.409880941344
1
Iteration 11200: Loss = -11343.399227442982
Iteration 11300: Loss = -11343.451050817892
1
Iteration 11400: Loss = -11343.399058408615
Iteration 11500: Loss = -11343.430204997734
1
Iteration 11600: Loss = -11343.39907551034
Iteration 11700: Loss = -11343.492152191044
1
Iteration 11800: Loss = -11343.399076063402
Iteration 11900: Loss = -11343.415852361062
1
Iteration 12000: Loss = -11343.399030020953
Iteration 12100: Loss = -11343.454794719382
1
Iteration 12200: Loss = -11343.398977004354
Iteration 12300: Loss = -11343.416895830296
1
Iteration 12400: Loss = -11343.398983266386
Iteration 12500: Loss = -11343.702493522047
1
Iteration 12600: Loss = -11343.399001806529
Iteration 12700: Loss = -11343.399011992784
Iteration 12800: Loss = -11343.399037359643
Iteration 12900: Loss = -11343.398960528779
Iteration 13000: Loss = -11343.406099382399
1
Iteration 13100: Loss = -11343.398964442404
Iteration 13200: Loss = -11343.555372383265
1
Iteration 13300: Loss = -11343.39897986226
Iteration 13400: Loss = -11343.398920489277
Iteration 13500: Loss = -11343.399934555946
1
Iteration 13600: Loss = -11343.398864118528
Iteration 13700: Loss = -11343.589366940825
1
Iteration 13800: Loss = -11343.39869830429
Iteration 13900: Loss = -11343.397194502326
Iteration 14000: Loss = -11342.428480170978
Iteration 14100: Loss = -11341.653044830384
Iteration 14200: Loss = -11339.842400591253
Iteration 14300: Loss = -11331.160718550449
Iteration 14400: Loss = -11331.02540399197
Iteration 14500: Loss = -11324.692960416454
Iteration 14600: Loss = -11324.624989024453
Iteration 14700: Loss = -11319.077170072826
Iteration 14800: Loss = -11319.051441541968
Iteration 14900: Loss = -11319.035098230575
Iteration 15000: Loss = -11319.035166485135
Iteration 15100: Loss = -11319.038142466336
1
Iteration 15200: Loss = -11319.033820454018
Iteration 15300: Loss = -11319.046021451835
1
Iteration 15400: Loss = -11319.02804680887
Iteration 15500: Loss = -11319.027747735288
Iteration 15600: Loss = -11319.027054598784
Iteration 15700: Loss = -11319.026957912194
Iteration 15800: Loss = -11319.026452928174
Iteration 15900: Loss = -11319.021481325024
Iteration 16000: Loss = -11316.568782391994
Iteration 16100: Loss = -11316.846878584847
1
Iteration 16200: Loss = -11316.533248251377
Iteration 16300: Loss = -11316.533390963757
1
Iteration 16400: Loss = -11316.533066876123
Iteration 16500: Loss = -11316.533932131859
1
Iteration 16600: Loss = -11316.533518318178
2
Iteration 16700: Loss = -11316.603632476274
3
Iteration 16800: Loss = -11316.532551564798
Iteration 16900: Loss = -11316.532662294792
1
Iteration 17000: Loss = -11316.532551179987
Iteration 17100: Loss = -11316.532539573313
Iteration 17200: Loss = -11316.55217202332
1
Iteration 17300: Loss = -11316.532063622328
Iteration 17400: Loss = -11316.53140582547
Iteration 17500: Loss = -11316.516393498963
Iteration 17600: Loss = -11316.468578179374
Iteration 17700: Loss = -11316.567382828513
1
Iteration 17800: Loss = -11316.46753153785
Iteration 17900: Loss = -11316.466716099365
Iteration 18000: Loss = -11316.48020554402
1
Iteration 18100: Loss = -11316.466724102489
Iteration 18200: Loss = -11316.46668604896
Iteration 18300: Loss = -11316.554215251856
1
Iteration 18400: Loss = -11316.46669734064
Iteration 18500: Loss = -11316.466640522181
Iteration 18600: Loss = -11316.468081230016
1
Iteration 18700: Loss = -11316.464804699792
Iteration 18800: Loss = -11316.463193982267
Iteration 18900: Loss = -11316.562313644017
1
Iteration 19000: Loss = -11316.463194087906
Iteration 19100: Loss = -11316.470542071796
1
Iteration 19200: Loss = -11316.463157297349
Iteration 19300: Loss = -11316.464255352395
1
Iteration 19400: Loss = -11316.463228041812
Iteration 19500: Loss = -11316.463280990567
Iteration 19600: Loss = -11316.463206801418
Iteration 19700: Loss = -11316.463157450498
Iteration 19800: Loss = -11316.59336144181
1
Iteration 19900: Loss = -11316.463152305509
pi: tensor([[0.6624, 0.3376],
        [0.4242, 0.5758]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4211, 0.5789], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2850, 0.0929],
         [0.6312, 0.2226]],

        [[0.5699, 0.1037],
         [0.7090, 0.5805]],

        [[0.6732, 0.0990],
         [0.5118, 0.5188]],

        [[0.5125, 0.0957],
         [0.5392, 0.5125]],

        [[0.7216, 0.1001],
         [0.5064, 0.5999]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080965973782139
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.882389682918764
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.3397362176468412
Average Adjusted Rand Index: 0.8907407795893588
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24439.845768287112
Iteration 100: Loss = -11255.946804317626
Iteration 200: Loss = -11254.900494601305
Iteration 300: Loss = -11254.722211830976
Iteration 400: Loss = -11254.64759680187
Iteration 500: Loss = -11254.608620025585
Iteration 600: Loss = -11254.585491404525
Iteration 700: Loss = -11254.57059962664
Iteration 800: Loss = -11254.56041505132
Iteration 900: Loss = -11254.553053425649
Iteration 1000: Loss = -11254.547694094525
Iteration 1100: Loss = -11254.5435252976
Iteration 1200: Loss = -11254.540312017634
Iteration 1300: Loss = -11254.537712634843
Iteration 1400: Loss = -11254.53563707225
Iteration 1500: Loss = -11254.533959849161
Iteration 1600: Loss = -11254.532538724527
Iteration 1700: Loss = -11254.531358383147
Iteration 1800: Loss = -11254.533404470767
1
Iteration 1900: Loss = -11254.529605226011
Iteration 2000: Loss = -11254.532084844557
1
Iteration 2100: Loss = -11254.528159898484
Iteration 2200: Loss = -11254.527568256994
Iteration 2300: Loss = -11254.531073175638
1
Iteration 2400: Loss = -11254.52686676724
Iteration 2500: Loss = -11254.526581689803
Iteration 2600: Loss = -11254.536700451103
1
Iteration 2700: Loss = -11254.525609659546
Iteration 2800: Loss = -11254.525479357724
Iteration 2900: Loss = -11254.527534218074
1
Iteration 3000: Loss = -11254.524882295009
Iteration 3100: Loss = -11254.525802578764
1
Iteration 3200: Loss = -11254.524977379411
Iteration 3300: Loss = -11254.524440782478
Iteration 3400: Loss = -11254.52422221172
Iteration 3500: Loss = -11254.524028384012
Iteration 3600: Loss = -11254.52403007381
Iteration 3700: Loss = -11254.524100818582
Iteration 3800: Loss = -11254.523661226714
Iteration 3900: Loss = -11254.524785916437
1
Iteration 4000: Loss = -11254.523458188563
Iteration 4100: Loss = -11254.523529460574
Iteration 4200: Loss = -11254.523326011
Iteration 4300: Loss = -11254.523312656136
Iteration 4400: Loss = -11254.528482249285
1
Iteration 4500: Loss = -11254.523331490127
Iteration 4600: Loss = -11254.523085296278
Iteration 4700: Loss = -11254.523035224567
Iteration 4800: Loss = -11254.523100557224
Iteration 4900: Loss = -11254.522893367937
Iteration 5000: Loss = -11254.522928599345
Iteration 5100: Loss = -11254.522884643966
Iteration 5200: Loss = -11254.523036806842
1
Iteration 5300: Loss = -11254.523670304809
2
Iteration 5400: Loss = -11254.522799431
Iteration 5500: Loss = -11254.532269179854
1
Iteration 5600: Loss = -11254.522701587346
Iteration 5700: Loss = -11254.52274622516
Iteration 5800: Loss = -11254.52787068415
1
Iteration 5900: Loss = -11254.524213899014
2
Iteration 6000: Loss = -11254.522803677493
Iteration 6100: Loss = -11254.52258607868
Iteration 6200: Loss = -11254.52743234988
1
Iteration 6300: Loss = -11254.52282602351
2
Iteration 6400: Loss = -11254.522549591942
Iteration 6500: Loss = -11254.522533650603
Iteration 6600: Loss = -11254.522707122245
1
Iteration 6700: Loss = -11254.522769531444
2
Iteration 6800: Loss = -11254.5235297075
3
Iteration 6900: Loss = -11254.52245736521
Iteration 7000: Loss = -11254.525809254796
1
Iteration 7100: Loss = -11254.522445411687
Iteration 7200: Loss = -11254.530811279426
1
Iteration 7300: Loss = -11254.522419490615
Iteration 7400: Loss = -11254.522402596162
Iteration 7500: Loss = -11254.527937101062
1
Iteration 7600: Loss = -11254.522397885561
Iteration 7700: Loss = -11254.522415773621
Iteration 7800: Loss = -11254.52265497254
1
Iteration 7900: Loss = -11254.522384030133
Iteration 8000: Loss = -11254.526396649655
1
Iteration 8100: Loss = -11254.523205390806
2
Iteration 8200: Loss = -11254.522603910453
3
Iteration 8300: Loss = -11254.525641142505
4
Iteration 8400: Loss = -11254.531656331857
5
Iteration 8500: Loss = -11254.522387176283
Iteration 8600: Loss = -11254.522379378112
Iteration 8700: Loss = -11254.557943946393
1
Iteration 8800: Loss = -11254.522361499176
Iteration 8900: Loss = -11254.53133409247
1
Iteration 9000: Loss = -11254.522307694302
Iteration 9100: Loss = -11254.538840065099
1
Iteration 9200: Loss = -11254.522337413733
Iteration 9300: Loss = -11254.52235858781
Iteration 9400: Loss = -11254.522389206953
Iteration 9500: Loss = -11254.522337799133
Iteration 9600: Loss = -11254.529360232124
1
Iteration 9700: Loss = -11254.522300816883
Iteration 9800: Loss = -11254.522296099749
Iteration 9900: Loss = -11254.52253834241
1
Iteration 10000: Loss = -11254.52233548635
Iteration 10100: Loss = -11254.522320147422
Iteration 10200: Loss = -11254.534551424855
1
Iteration 10300: Loss = -11254.522315614155
Iteration 10400: Loss = -11254.522317467841
Iteration 10500: Loss = -11254.523096895802
1
Iteration 10600: Loss = -11254.522308655896
Iteration 10700: Loss = -11254.522788874883
1
Iteration 10800: Loss = -11254.543812474567
2
Iteration 10900: Loss = -11254.522306306277
Iteration 11000: Loss = -11254.526200379625
1
Iteration 11100: Loss = -11254.522309907125
Iteration 11200: Loss = -11254.557164790442
1
Iteration 11300: Loss = -11254.522335766782
Iteration 11400: Loss = -11254.522323879622
Iteration 11500: Loss = -11254.523540803066
1
Iteration 11600: Loss = -11254.522312546527
Iteration 11700: Loss = -11254.522296640742
Iteration 11800: Loss = -11254.522377771053
Iteration 11900: Loss = -11254.52231749663
Iteration 12000: Loss = -11254.589597035489
1
Iteration 12100: Loss = -11254.522298875147
Iteration 12200: Loss = -11254.52231087496
Iteration 12300: Loss = -11254.522438326063
1
Iteration 12400: Loss = -11254.522298232903
Iteration 12500: Loss = -11254.538624686527
1
Iteration 12600: Loss = -11254.522316594715
Iteration 12700: Loss = -11254.52229423843
Iteration 12800: Loss = -11254.522628376944
1
Iteration 12900: Loss = -11254.522307143076
Iteration 13000: Loss = -11254.617694494733
1
Iteration 13100: Loss = -11254.522314169471
Iteration 13200: Loss = -11254.522299531916
Iteration 13300: Loss = -11254.52834443148
1
Iteration 13400: Loss = -11254.522309569022
Iteration 13500: Loss = -11254.522306124405
Iteration 13600: Loss = -11254.522438492339
1
Iteration 13700: Loss = -11254.522318964688
Iteration 13800: Loss = -11254.566970881775
1
Iteration 13900: Loss = -11254.522318553127
Iteration 14000: Loss = -11254.52401463144
1
Iteration 14100: Loss = -11254.522321633198
Iteration 14200: Loss = -11254.522437711415
1
Iteration 14300: Loss = -11254.522563217648
2
Iteration 14400: Loss = -11254.525711958146
3
Iteration 14500: Loss = -11254.52239542214
Iteration 14600: Loss = -11254.52243029302
Iteration 14700: Loss = -11254.811899845652
1
Iteration 14800: Loss = -11254.522322227918
Iteration 14900: Loss = -11254.62674676775
1
Iteration 15000: Loss = -11254.522306986733
Iteration 15100: Loss = -11254.539231877054
1
Iteration 15200: Loss = -11254.52232880657
Iteration 15300: Loss = -11254.522587163246
1
Iteration 15400: Loss = -11254.549462210893
2
Iteration 15500: Loss = -11254.52230452159
Iteration 15600: Loss = -11254.524364416391
1
Iteration 15700: Loss = -11254.523773760033
2
Iteration 15800: Loss = -11254.522401423203
Iteration 15900: Loss = -11254.52570499137
1
Iteration 16000: Loss = -11254.525643556843
2
Iteration 16100: Loss = -11254.522302707888
Iteration 16200: Loss = -11254.522713826045
1
Iteration 16300: Loss = -11254.63650451646
2
Iteration 16400: Loss = -11254.522287008174
Iteration 16500: Loss = -11254.522875774268
1
Iteration 16600: Loss = -11254.552950902611
2
Iteration 16700: Loss = -11254.522293132126
Iteration 16800: Loss = -11254.549116452952
1
Iteration 16900: Loss = -11254.522304704216
Iteration 17000: Loss = -11254.595345656564
1
Iteration 17100: Loss = -11254.522320429955
Iteration 17200: Loss = -11254.522324061985
Iteration 17300: Loss = -11254.522877989337
1
Iteration 17400: Loss = -11254.522295154315
Iteration 17500: Loss = -11254.631174637789
1
Iteration 17600: Loss = -11254.52230545067
Iteration 17700: Loss = -11254.522308320202
Iteration 17800: Loss = -11254.539157980811
1
Iteration 17900: Loss = -11254.522291808864
Iteration 18000: Loss = -11254.522309693453
Iteration 18100: Loss = -11254.522395621927
Iteration 18200: Loss = -11254.5672137425
1
Iteration 18300: Loss = -11254.522351219923
Iteration 18400: Loss = -11254.524937330481
1
Iteration 18500: Loss = -11254.52471183904
2
Iteration 18600: Loss = -11254.522299383032
Iteration 18700: Loss = -11254.556845254456
1
Iteration 18800: Loss = -11254.522301979074
Iteration 18900: Loss = -11254.686795147429
1
Iteration 19000: Loss = -11254.522299597123
Iteration 19100: Loss = -11254.522307061241
Iteration 19200: Loss = -11254.522588288577
1
Iteration 19300: Loss = -11254.522301772879
Iteration 19400: Loss = -11254.611238477737
1
Iteration 19500: Loss = -11254.522306801182
Iteration 19600: Loss = -11254.522317173958
Iteration 19700: Loss = -11254.52233427221
Iteration 19800: Loss = -11254.524822346579
1
Iteration 19900: Loss = -11254.552532236326
2
pi: tensor([[0.7383, 0.2617],
        [0.2229, 0.7771]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4760, 0.5240], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2081, 0.0923],
         [0.6089, 0.2937]],

        [[0.5930, 0.1037],
         [0.6023, 0.6546]],

        [[0.5815, 0.1010],
         [0.5611, 0.5253]],

        [[0.6273, 0.0965],
         [0.6389, 0.6356]],

        [[0.5876, 0.1006],
         [0.6698, 0.6864]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
Global Adjusted Rand Index: 0.9214424092538381
Average Adjusted Rand Index: 0.921124160393884
11273.743954662514
[0.3397362176468412, 0.9214424092538381] [0.8907407795893588, 0.921124160393884] [11316.46314098574, 11254.532713076593]
-------------------------------------
This iteration is 89
True Objective function: Loss = -11150.371617540199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20799.74230490944
Iteration 100: Loss = -11351.352684434083
Iteration 200: Loss = -11294.545340837516
Iteration 300: Loss = -11263.15030188172
Iteration 400: Loss = -11252.567044456573
Iteration 500: Loss = -11234.478475906844
Iteration 600: Loss = -11181.005748936665
Iteration 700: Loss = -11135.6450538863
Iteration 800: Loss = -11119.499025002338
Iteration 900: Loss = -11118.690108220942
Iteration 1000: Loss = -11118.255312619187
Iteration 1100: Loss = -11118.14722421396
Iteration 1200: Loss = -11118.020768876084
Iteration 1300: Loss = -11117.99522223093
Iteration 1400: Loss = -11117.9328637564
Iteration 1500: Loss = -11117.90114648973
Iteration 1600: Loss = -11117.890675716304
Iteration 1700: Loss = -11117.881711250899
Iteration 1800: Loss = -11117.873052817891
Iteration 1900: Loss = -11117.862902456773
Iteration 2000: Loss = -11117.842811472705
Iteration 2100: Loss = -11117.815991353407
Iteration 2200: Loss = -11117.806842808299
Iteration 2300: Loss = -11117.789736142304
Iteration 2400: Loss = -11117.782063069233
Iteration 2500: Loss = -11117.772568928916
Iteration 2600: Loss = -11117.769649051985
Iteration 2700: Loss = -11117.767077145141
Iteration 2800: Loss = -11117.764693579928
Iteration 2900: Loss = -11117.762324817837
Iteration 3000: Loss = -11117.759974692975
Iteration 3100: Loss = -11117.75722344018
Iteration 3200: Loss = -11116.635747597013
Iteration 3300: Loss = -11116.578007129976
Iteration 3400: Loss = -11116.439499395106
Iteration 3500: Loss = -11116.433499810848
Iteration 3600: Loss = -11116.41435534435
Iteration 3700: Loss = -11116.412752358894
Iteration 3800: Loss = -11116.411391313239
Iteration 3900: Loss = -11116.41075068562
Iteration 4000: Loss = -11116.413050701602
1
Iteration 4100: Loss = -11116.406778126102
Iteration 4200: Loss = -11116.40894965409
1
Iteration 4300: Loss = -11116.407998364255
2
Iteration 4400: Loss = -11116.406433815882
Iteration 4500: Loss = -11116.40390472384
Iteration 4600: Loss = -11116.403372479308
Iteration 4700: Loss = -11116.40741763154
1
Iteration 4800: Loss = -11116.402026136364
Iteration 4900: Loss = -11116.400950700267
Iteration 5000: Loss = -11116.400965025781
Iteration 5100: Loss = -11116.398048893314
Iteration 5200: Loss = -11116.39180118687
Iteration 5300: Loss = -11115.589757577645
Iteration 5400: Loss = -11115.537566838047
Iteration 5500: Loss = -11115.536316320127
Iteration 5600: Loss = -11115.543374775149
1
Iteration 5700: Loss = -11115.534466927998
Iteration 5800: Loss = -11115.558930276902
1
Iteration 5900: Loss = -11115.520916344154
Iteration 6000: Loss = -11115.519506562921
Iteration 6100: Loss = -11115.514560145359
Iteration 6200: Loss = -11115.515276685648
1
Iteration 6300: Loss = -11115.5167203772
2
Iteration 6400: Loss = -11115.514317322
Iteration 6500: Loss = -11115.512511056819
Iteration 6600: Loss = -11115.508500259972
Iteration 6700: Loss = -11115.50178817886
Iteration 6800: Loss = -11115.500808805751
Iteration 6900: Loss = -11115.500732011948
Iteration 7000: Loss = -11115.50060016068
Iteration 7100: Loss = -11115.500579662461
Iteration 7200: Loss = -11115.500502513278
Iteration 7300: Loss = -11115.504846288464
1
Iteration 7400: Loss = -11115.50042981639
Iteration 7500: Loss = -11115.500325405703
Iteration 7600: Loss = -11115.5004823546
1
Iteration 7700: Loss = -11115.50450998536
2
Iteration 7800: Loss = -11115.500594061545
3
Iteration 7900: Loss = -11115.500148387135
Iteration 8000: Loss = -11115.501010229447
1
Iteration 8100: Loss = -11115.499636052547
Iteration 8200: Loss = -11115.44032134075
Iteration 8300: Loss = -11115.440057834863
Iteration 8400: Loss = -11115.472655365673
1
Iteration 8500: Loss = -11115.406888425927
Iteration 8600: Loss = -11115.40683643592
Iteration 8700: Loss = -11115.409392971258
1
Iteration 8800: Loss = -11115.395650863415
Iteration 8900: Loss = -11115.372869932919
Iteration 9000: Loss = -11115.372845313894
Iteration 9100: Loss = -11115.372733269933
Iteration 9200: Loss = -11115.374428776728
1
Iteration 9300: Loss = -11115.372681767538
Iteration 9400: Loss = -11115.46232240052
1
Iteration 9500: Loss = -11115.372799142502
2
Iteration 9600: Loss = -11115.372492004184
Iteration 9700: Loss = -11115.733230173795
1
Iteration 9800: Loss = -11115.363811624658
Iteration 9900: Loss = -11115.391725118305
1
Iteration 10000: Loss = -11115.368887532983
2
Iteration 10100: Loss = -11115.3637735719
Iteration 10200: Loss = -11115.36398731867
1
Iteration 10300: Loss = -11115.371004299472
2
Iteration 10400: Loss = -11115.363322484856
Iteration 10500: Loss = -11115.366387560764
1
Iteration 10600: Loss = -11115.289788556187
Iteration 10700: Loss = -11115.288211720757
Iteration 10800: Loss = -11115.288092562165
Iteration 10900: Loss = -11115.287704291728
Iteration 11000: Loss = -11115.28784476908
1
Iteration 11100: Loss = -11115.28764989193
Iteration 11200: Loss = -11115.28992031771
1
Iteration 11300: Loss = -11115.287625634382
Iteration 11400: Loss = -11115.288508725966
1
Iteration 11500: Loss = -11115.287532781464
Iteration 11600: Loss = -11115.283842983074
Iteration 11700: Loss = -11115.28104623512
Iteration 11800: Loss = -11115.281076935446
Iteration 11900: Loss = -11115.281150963694
Iteration 12000: Loss = -11115.281033764932
Iteration 12100: Loss = -11115.306359997378
1
Iteration 12200: Loss = -11115.280559081255
Iteration 12300: Loss = -11115.291397471798
1
Iteration 12400: Loss = -11115.280534009733
Iteration 12500: Loss = -11115.280454660873
Iteration 12600: Loss = -11115.32862744084
1
Iteration 12700: Loss = -11115.277477354035
Iteration 12800: Loss = -11115.277440141637
Iteration 12900: Loss = -11115.314157848281
1
Iteration 13000: Loss = -11115.359761639182
2
Iteration 13100: Loss = -11115.290713639142
3
Iteration 13200: Loss = -11115.277508276591
Iteration 13300: Loss = -11115.289207876882
1
Iteration 13400: Loss = -11115.3039692138
2
Iteration 13500: Loss = -11115.277888305838
3
Iteration 13600: Loss = -11115.277500496319
Iteration 13700: Loss = -11115.287695409083
1
Iteration 13800: Loss = -11115.27745122956
Iteration 13900: Loss = -11115.277358599551
Iteration 14000: Loss = -11115.27748953601
1
Iteration 14100: Loss = -11115.278537170776
2
Iteration 14200: Loss = -11115.359520683032
3
Iteration 14300: Loss = -11115.277432010094
Iteration 14400: Loss = -11115.227942132235
Iteration 14500: Loss = -11115.231888320484
1
Iteration 14600: Loss = -11115.229198339426
2
Iteration 14700: Loss = -11115.22776513542
Iteration 14800: Loss = -11115.22790396545
1
Iteration 14900: Loss = -11115.232873765981
2
Iteration 15000: Loss = -11115.22668101206
Iteration 15100: Loss = -11115.218620668944
Iteration 15200: Loss = -11115.209551533255
Iteration 15300: Loss = -11115.24631614659
1
Iteration 15400: Loss = -11115.209234313863
Iteration 15500: Loss = -11115.20926424765
Iteration 15600: Loss = -11115.208583732652
Iteration 15700: Loss = -11115.231974002261
1
Iteration 15800: Loss = -11115.208863564001
2
Iteration 15900: Loss = -11115.211747904741
3
Iteration 16000: Loss = -11115.273157003938
4
Iteration 16100: Loss = -11115.208272266234
Iteration 16200: Loss = -11115.201916340462
Iteration 16300: Loss = -11115.197856492154
Iteration 16400: Loss = -11115.195325514367
Iteration 16500: Loss = -11115.195171815343
Iteration 16600: Loss = -11115.195162295717
Iteration 16700: Loss = -11115.195294920071
1
Iteration 16800: Loss = -11115.1944562716
Iteration 16900: Loss = -11115.195239014469
1
Iteration 17000: Loss = -11115.194352628198
Iteration 17100: Loss = -11115.194637611823
1
Iteration 17200: Loss = -11115.194355477528
Iteration 17300: Loss = -11115.194546124168
1
Iteration 17400: Loss = -11115.194364148447
Iteration 17500: Loss = -11115.197742551145
1
Iteration 17600: Loss = -11115.194320568067
Iteration 17700: Loss = -11115.211750819408
1
Iteration 17800: Loss = -11115.194331568911
Iteration 17900: Loss = -11115.194554964004
1
Iteration 18000: Loss = -11115.194382048006
Iteration 18100: Loss = -11115.194325170894
Iteration 18200: Loss = -11115.197303264089
1
Iteration 18300: Loss = -11115.205001075905
2
Iteration 18400: Loss = -11115.194276908593
Iteration 18500: Loss = -11115.194377911039
1
Iteration 18600: Loss = -11115.194767576038
2
Iteration 18700: Loss = -11115.194390731232
3
Iteration 18800: Loss = -11115.194256972814
Iteration 18900: Loss = -11115.19959908975
1
Iteration 19000: Loss = -11115.194267277973
Iteration 19100: Loss = -11115.246755915396
1
Iteration 19200: Loss = -11115.194261877285
Iteration 19300: Loss = -11115.193182316036
Iteration 19400: Loss = -11115.192856851028
Iteration 19500: Loss = -11115.192872674203
Iteration 19600: Loss = -11115.192938184298
Iteration 19700: Loss = -11115.276165004325
1
Iteration 19800: Loss = -11115.19282545547
Iteration 19900: Loss = -11115.220162341495
1
pi: tensor([[0.7551, 0.2449],
        [0.3167, 0.6833]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4352, 0.5648], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1895, 0.1018],
         [0.7023, 0.3000]],

        [[0.5226, 0.1015],
         [0.5599, 0.6741]],

        [[0.6659, 0.1111],
         [0.6477, 0.5383]],

        [[0.5156, 0.0995],
         [0.7002, 0.5203]],

        [[0.5178, 0.1026],
         [0.6408, 0.5995]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369243452069851
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080762963757459
Global Adjusted Rand Index: 0.8609058856634724
Average Adjusted Rand Index: 0.861644519379577
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21661.35407208701
Iteration 100: Loss = -11353.5584475329
Iteration 200: Loss = -11116.916038000325
Iteration 300: Loss = -11115.423461914861
Iteration 400: Loss = -11115.296861428675
Iteration 500: Loss = -11115.249758074044
Iteration 600: Loss = -11115.224492932572
Iteration 700: Loss = -11115.209363503318
Iteration 800: Loss = -11115.199415425875
Iteration 900: Loss = -11115.192555024865
Iteration 1000: Loss = -11115.187526326656
Iteration 1100: Loss = -11115.183778642664
Iteration 1200: Loss = -11115.180890944825
Iteration 1300: Loss = -11115.17858123181
Iteration 1400: Loss = -11115.176705854648
Iteration 1500: Loss = -11115.175050500122
Iteration 1600: Loss = -11115.173367880174
Iteration 1700: Loss = -11115.171934736481
Iteration 1800: Loss = -11115.17099696883
Iteration 1900: Loss = -11115.170249359317
Iteration 2000: Loss = -11115.172516795668
1
Iteration 2100: Loss = -11115.169090618687
Iteration 2200: Loss = -11115.168581458305
Iteration 2300: Loss = -11115.17192248539
1
Iteration 2400: Loss = -11115.16781249363
Iteration 2500: Loss = -11115.16744290144
Iteration 2600: Loss = -11115.167143386403
Iteration 2700: Loss = -11115.166871760535
Iteration 2800: Loss = -11115.166953515982
Iteration 2900: Loss = -11115.166424674919
Iteration 3000: Loss = -11115.166642572314
1
Iteration 3100: Loss = -11115.166130623762
Iteration 3200: Loss = -11115.168727222841
1
Iteration 3300: Loss = -11115.165837492124
Iteration 3400: Loss = -11115.169567734714
1
Iteration 3500: Loss = -11115.165590058206
Iteration 3600: Loss = -11115.165473144098
Iteration 3700: Loss = -11115.1653999914
Iteration 3800: Loss = -11115.165332222974
Iteration 3900: Loss = -11115.170757270273
1
Iteration 4000: Loss = -11115.165160242494
Iteration 4100: Loss = -11115.165064402527
Iteration 4200: Loss = -11115.165061013471
Iteration 4300: Loss = -11115.164958992213
Iteration 4400: Loss = -11115.164931369114
Iteration 4500: Loss = -11115.16486122986
Iteration 4600: Loss = -11115.164759468635
Iteration 4700: Loss = -11115.165427198135
1
Iteration 4800: Loss = -11115.16471902166
Iteration 4900: Loss = -11115.164637090102
Iteration 5000: Loss = -11115.196980691097
1
Iteration 5100: Loss = -11115.164557176086
Iteration 5200: Loss = -11115.164563515547
Iteration 5300: Loss = -11115.16768654555
1
Iteration 5400: Loss = -11115.164514482038
Iteration 5500: Loss = -11115.16448225335
Iteration 5600: Loss = -11115.164478687399
Iteration 5700: Loss = -11115.164526802902
Iteration 5800: Loss = -11115.165798574855
1
Iteration 5900: Loss = -11115.165099620184
2
Iteration 6000: Loss = -11115.16440797851
Iteration 6100: Loss = -11115.164350577685
Iteration 6200: Loss = -11115.18504931477
1
Iteration 6300: Loss = -11115.164225498029
Iteration 6400: Loss = -11115.16427104757
Iteration 6500: Loss = -11115.165597743518
1
Iteration 6600: Loss = -11115.164186619138
Iteration 6700: Loss = -11115.171274864728
1
Iteration 6800: Loss = -11115.164143930428
Iteration 6900: Loss = -11115.164243575773
Iteration 7000: Loss = -11115.163915721358
Iteration 7100: Loss = -11115.163817786704
Iteration 7200: Loss = -11115.165250541424
1
Iteration 7300: Loss = -11115.163813852188
Iteration 7400: Loss = -11115.163781809431
Iteration 7500: Loss = -11115.163829994295
Iteration 7600: Loss = -11115.163765294605
Iteration 7700: Loss = -11115.164347631182
1
Iteration 7800: Loss = -11115.163758766788
Iteration 7900: Loss = -11115.1681987699
1
Iteration 8000: Loss = -11115.16585558177
2
Iteration 8100: Loss = -11115.173808574913
3
Iteration 8200: Loss = -11115.163937737912
4
Iteration 8300: Loss = -11115.163772275107
Iteration 8400: Loss = -11115.164653038257
1
Iteration 8500: Loss = -11115.165019535007
2
Iteration 8600: Loss = -11115.16372156464
Iteration 8700: Loss = -11115.16379699548
Iteration 8800: Loss = -11115.163811161076
Iteration 8900: Loss = -11115.163758960283
Iteration 9000: Loss = -11115.163726365958
Iteration 9100: Loss = -11115.163817819624
Iteration 9200: Loss = -11115.16371528499
Iteration 9300: Loss = -11115.164424845168
1
Iteration 9400: Loss = -11115.16369761368
Iteration 9500: Loss = -11115.172670351756
1
Iteration 9600: Loss = -11115.16370739378
Iteration 9700: Loss = -11115.163967341387
1
Iteration 9800: Loss = -11115.163748925484
Iteration 9900: Loss = -11115.17331371267
1
Iteration 10000: Loss = -11115.16635103627
2
Iteration 10100: Loss = -11115.163685470023
Iteration 10200: Loss = -11115.163913188126
1
Iteration 10300: Loss = -11115.178057134972
2
Iteration 10400: Loss = -11115.163651044797
Iteration 10500: Loss = -11115.163995179557
1
Iteration 10600: Loss = -11115.163699993649
Iteration 10700: Loss = -11115.197094939767
1
Iteration 10800: Loss = -11115.163693185132
Iteration 10900: Loss = -11115.163694047134
Iteration 11000: Loss = -11115.165814494974
1
Iteration 11100: Loss = -11115.163719555154
Iteration 11200: Loss = -11115.163706016609
Iteration 11300: Loss = -11115.16376416759
Iteration 11400: Loss = -11115.281562879485
1
Iteration 11500: Loss = -11115.163749841786
Iteration 11600: Loss = -11115.174912643466
1
Iteration 11700: Loss = -11115.181265483321
2
Iteration 11800: Loss = -11115.163700927227
Iteration 11900: Loss = -11115.164579483215
1
Iteration 12000: Loss = -11115.16418961381
2
Iteration 12100: Loss = -11115.163703250024
Iteration 12200: Loss = -11115.163765104757
Iteration 12300: Loss = -11115.163732020499
Iteration 12400: Loss = -11115.163688039487
Iteration 12500: Loss = -11115.168967611673
1
Iteration 12600: Loss = -11115.16370649427
Iteration 12700: Loss = -11115.163700413083
Iteration 12800: Loss = -11115.163794232638
Iteration 12900: Loss = -11115.163808816516
Iteration 13000: Loss = -11115.163845283027
Iteration 13100: Loss = -11115.163941496752
Iteration 13200: Loss = -11115.16524482728
1
Iteration 13300: Loss = -11115.252265852028
2
Iteration 13400: Loss = -11115.163695302525
Iteration 13500: Loss = -11115.16409745265
1
Iteration 13600: Loss = -11115.163701174348
Iteration 13700: Loss = -11115.173613972616
1
Iteration 13800: Loss = -11115.163859833288
2
Iteration 13900: Loss = -11115.546306996077
3
Iteration 14000: Loss = -11115.16369143057
Iteration 14100: Loss = -11115.189673033316
1
Iteration 14200: Loss = -11115.18034776164
2
Iteration 14300: Loss = -11115.166945867066
3
Iteration 14400: Loss = -11115.165770457083
4
Iteration 14500: Loss = -11115.174257863646
5
Iteration 14600: Loss = -11115.191249647407
6
Iteration 14700: Loss = -11115.163713609967
Iteration 14800: Loss = -11115.167297778342
1
Iteration 14900: Loss = -11115.163693131568
Iteration 15000: Loss = -11115.165730835222
1
Iteration 15100: Loss = -11115.39737306885
2
Iteration 15200: Loss = -11115.163692207338
Iteration 15300: Loss = -11115.170141812294
1
Iteration 15400: Loss = -11115.163678523157
Iteration 15500: Loss = -11115.165658127948
1
Iteration 15600: Loss = -11115.16369108891
Iteration 15700: Loss = -11115.274063731908
1
Iteration 15800: Loss = -11115.16366829702
Iteration 15900: Loss = -11115.163692210093
Iteration 16000: Loss = -11115.16400504203
1
Iteration 16100: Loss = -11115.16365530585
Iteration 16200: Loss = -11115.17606382226
1
Iteration 16300: Loss = -11115.176266657192
2
Iteration 16400: Loss = -11115.164945420582
3
Iteration 16500: Loss = -11115.163714457196
Iteration 16600: Loss = -11115.164983245575
1
Iteration 16700: Loss = -11115.19583105598
2
Iteration 16800: Loss = -11115.163701214204
Iteration 16900: Loss = -11115.164605883629
1
Iteration 17000: Loss = -11115.163671916998
Iteration 17100: Loss = -11115.174391915809
1
Iteration 17200: Loss = -11115.163662842719
Iteration 17300: Loss = -11115.163655662589
Iteration 17400: Loss = -11115.164270204996
1
Iteration 17500: Loss = -11115.163653645212
Iteration 17600: Loss = -11115.163686100492
Iteration 17700: Loss = -11115.164886799166
1
Iteration 17800: Loss = -11115.163678000836
Iteration 17900: Loss = -11115.166805985484
1
Iteration 18000: Loss = -11115.163939165994
2
Iteration 18100: Loss = -11115.16752232311
3
Iteration 18200: Loss = -11115.180153881707
4
Iteration 18300: Loss = -11115.236190088459
5
Iteration 18400: Loss = -11115.163686751068
Iteration 18500: Loss = -11115.166147858177
1
Iteration 18600: Loss = -11115.165531888628
2
Iteration 18700: Loss = -11115.163708957694
Iteration 18800: Loss = -11115.186250221395
1
Iteration 18900: Loss = -11115.163665783692
Iteration 19000: Loss = -11115.16428313055
1
Iteration 19100: Loss = -11115.163702903925
Iteration 19200: Loss = -11115.16932719388
1
Iteration 19300: Loss = -11115.163680235515
Iteration 19400: Loss = -11115.513185882233
1
Iteration 19500: Loss = -11115.163705298442
Iteration 19600: Loss = -11115.163668948524
Iteration 19700: Loss = -11115.165841008953
1
Iteration 19800: Loss = -11115.16370121221
Iteration 19900: Loss = -11115.227969593985
1
pi: tensor([[0.7551, 0.2449],
        [0.3168, 0.6832]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4352, 0.5648], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1894, 0.1018],
         [0.5158, 0.3000]],

        [[0.5835, 0.1015],
         [0.5645, 0.5320]],

        [[0.6269, 0.1112],
         [0.6799, 0.5703]],

        [[0.5092, 0.0995],
         [0.6307, 0.5751]],

        [[0.5638, 0.1027],
         [0.7126, 0.6200]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 92
Adjusted Rand Index: 0.7025862068965517
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080762963757459
Global Adjusted Rand Index: 0.8534829495126053
Average Adjusted Rand Index: 0.8547768917174903
11150.371617540199
[0.8609058856634724, 0.8534829495126053] [0.861644519379577, 0.8547768917174903] [11115.192832390645, 11115.163716494362]
-------------------------------------
This iteration is 90
True Objective function: Loss = -11272.879573707021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22359.851126278823
Iteration 100: Loss = -11599.620284257497
Iteration 200: Loss = -11598.559183639563
Iteration 300: Loss = -11597.879808609745
Iteration 400: Loss = -11596.446631583505
Iteration 500: Loss = -11591.250560582532
Iteration 600: Loss = -11575.931052407956
Iteration 700: Loss = -11442.82315688274
Iteration 800: Loss = -11283.417053647718
Iteration 900: Loss = -11268.55490549012
Iteration 1000: Loss = -11263.647719053637
Iteration 1100: Loss = -11263.17324983291
Iteration 1200: Loss = -11262.993246773709
Iteration 1300: Loss = -11262.763460713964
Iteration 1400: Loss = -11256.794582014192
Iteration 1500: Loss = -11256.697546268924
Iteration 1600: Loss = -11256.610166384173
Iteration 1700: Loss = -11256.560659917699
Iteration 1800: Loss = -11256.529567471911
Iteration 1900: Loss = -11256.501514513997
Iteration 2000: Loss = -11256.46777635591
Iteration 2100: Loss = -11256.183552639472
Iteration 2200: Loss = -11255.9796374352
Iteration 2300: Loss = -11255.956712545982
Iteration 2400: Loss = -11255.93579102093
Iteration 2500: Loss = -11255.925416444808
Iteration 2600: Loss = -11255.917708913683
Iteration 2700: Loss = -11255.907564486739
Iteration 2800: Loss = -11255.862104036225
Iteration 2900: Loss = -11255.753158448675
Iteration 3000: Loss = -11255.746672674884
Iteration 3100: Loss = -11255.74116125932
Iteration 3200: Loss = -11255.736614132724
Iteration 3300: Loss = -11255.741043663229
1
Iteration 3400: Loss = -11255.72896304474
Iteration 3500: Loss = -11255.725463807818
Iteration 3600: Loss = -11255.729465555687
1
Iteration 3700: Loss = -11255.718462885132
Iteration 3800: Loss = -11255.721157154921
1
Iteration 3900: Loss = -11255.711997620952
Iteration 4000: Loss = -11255.7135379038
1
Iteration 4100: Loss = -11255.706447576362
Iteration 4200: Loss = -11255.707428629119
1
Iteration 4300: Loss = -11255.7031914569
Iteration 4400: Loss = -11255.702066176622
Iteration 4500: Loss = -11255.706970437179
1
Iteration 4600: Loss = -11255.700487168251
Iteration 4700: Loss = -11255.699349922887
Iteration 4800: Loss = -11255.706275553499
1
Iteration 4900: Loss = -11255.699705647581
2
Iteration 5000: Loss = -11255.695388042392
Iteration 5100: Loss = -11255.695284731255
Iteration 5200: Loss = -11255.693694603002
Iteration 5300: Loss = -11255.692931196
Iteration 5400: Loss = -11255.692125077014
Iteration 5500: Loss = -11255.691427545302
Iteration 5600: Loss = -11255.690712239932
Iteration 5700: Loss = -11255.689945846181
Iteration 5800: Loss = -11255.689121684281
Iteration 5900: Loss = -11255.688685942763
Iteration 6000: Loss = -11255.686380284089
Iteration 6100: Loss = -11255.688079259537
1
Iteration 6200: Loss = -11255.688603466979
2
Iteration 6300: Loss = -11255.685057427178
Iteration 6400: Loss = -11255.683911425813
Iteration 6500: Loss = -11255.684134689493
1
Iteration 6600: Loss = -11255.693456597142
2
Iteration 6700: Loss = -11255.682735538223
Iteration 6800: Loss = -11255.682347947886
Iteration 6900: Loss = -11255.681969972893
Iteration 7000: Loss = -11255.681418060341
Iteration 7100: Loss = -11255.681617563967
1
Iteration 7200: Loss = -11255.68039931739
Iteration 7300: Loss = -11255.681751731667
1
Iteration 7400: Loss = -11255.705655126862
2
Iteration 7500: Loss = -11255.676699893038
Iteration 7600: Loss = -11255.676497835668
Iteration 7700: Loss = -11255.67662690893
1
Iteration 7800: Loss = -11255.682680617954
2
Iteration 7900: Loss = -11255.676295104182
Iteration 8000: Loss = -11255.685008848606
1
Iteration 8100: Loss = -11255.681849918665
2
Iteration 8200: Loss = -11255.676877566953
3
Iteration 8300: Loss = -11255.675328362215
Iteration 8400: Loss = -11255.675308081301
Iteration 8500: Loss = -11255.827745010922
1
Iteration 8600: Loss = -11255.680826592732
2
Iteration 8700: Loss = -11255.675214859508
Iteration 8800: Loss = -11255.6747141946
Iteration 8900: Loss = -11255.675205611898
1
Iteration 9000: Loss = -11255.657419888546
Iteration 9100: Loss = -11255.656651795964
Iteration 9200: Loss = -11255.656853064116
1
Iteration 9300: Loss = -11255.775027404516
2
Iteration 9400: Loss = -11255.659568352343
3
Iteration 9500: Loss = -11255.656822263707
4
Iteration 9600: Loss = -11255.653821967346
Iteration 9700: Loss = -11255.653503265423
Iteration 9800: Loss = -11255.65338344458
Iteration 9900: Loss = -11255.653973629323
1
Iteration 10000: Loss = -11255.653228412457
Iteration 10100: Loss = -11255.652010113172
Iteration 10200: Loss = -11255.651899179456
Iteration 10300: Loss = -11255.651980059902
Iteration 10400: Loss = -11255.651796673175
Iteration 10500: Loss = -11255.652266186009
1
Iteration 10600: Loss = -11255.651741691056
Iteration 10700: Loss = -11255.651853373203
1
Iteration 10800: Loss = -11255.651686480312
Iteration 10900: Loss = -11255.652587530038
1
Iteration 11000: Loss = -11255.651601162035
Iteration 11100: Loss = -11255.666428448923
1
Iteration 11200: Loss = -11255.651487158386
Iteration 11300: Loss = -11255.648725671512
Iteration 11400: Loss = -11255.64913378044
1
Iteration 11500: Loss = -11255.650130300648
2
Iteration 11600: Loss = -11255.815766774213
3
Iteration 11700: Loss = -11255.647895186707
Iteration 11800: Loss = -11255.648961001001
1
Iteration 11900: Loss = -11255.647766511234
Iteration 12000: Loss = -11255.647184968371
Iteration 12100: Loss = -11255.646802693695
Iteration 12200: Loss = -11255.649132823475
1
Iteration 12300: Loss = -11255.646731237417
Iteration 12400: Loss = -11255.647308169457
1
Iteration 12500: Loss = -11255.646826036873
Iteration 12600: Loss = -11255.646853508926
Iteration 12700: Loss = -11255.648203325807
1
Iteration 12800: Loss = -11255.646709082836
Iteration 12900: Loss = -11255.646804645645
Iteration 13000: Loss = -11255.834242841844
1
Iteration 13100: Loss = -11255.646678328414
Iteration 13200: Loss = -11255.666598904443
1
Iteration 13300: Loss = -11255.646652464846
Iteration 13400: Loss = -11255.65451058848
1
Iteration 13500: Loss = -11255.646689948799
Iteration 13600: Loss = -11255.65657143893
1
Iteration 13700: Loss = -11255.646942344452
2
Iteration 13800: Loss = -11255.646711527219
Iteration 13900: Loss = -11255.689375549826
1
Iteration 14000: Loss = -11255.652542993796
2
Iteration 14100: Loss = -11255.646653784865
Iteration 14200: Loss = -11255.647695067457
1
Iteration 14300: Loss = -11255.654158943376
2
Iteration 14400: Loss = -11255.64658283311
Iteration 14500: Loss = -11255.646675268556
Iteration 14600: Loss = -11255.646595775559
Iteration 14700: Loss = -11255.64682403874
1
Iteration 14800: Loss = -11255.646600052343
Iteration 14900: Loss = -11255.657095807415
1
Iteration 15000: Loss = -11255.646556077994
Iteration 15100: Loss = -11255.647657540725
1
Iteration 15200: Loss = -11255.646555050658
Iteration 15300: Loss = -11255.646454468671
Iteration 15400: Loss = -11255.664218867518
1
Iteration 15500: Loss = -11255.646445300721
Iteration 15600: Loss = -11255.646422911866
Iteration 15700: Loss = -11255.648442843984
1
Iteration 15800: Loss = -11255.646445110295
Iteration 15900: Loss = -11255.646424607383
Iteration 16000: Loss = -11255.647483241693
1
Iteration 16100: Loss = -11255.64645072669
Iteration 16200: Loss = -11255.662441221079
1
Iteration 16300: Loss = -11255.649122434636
2
Iteration 16400: Loss = -11255.703179719534
3
Iteration 16500: Loss = -11255.646371143395
Iteration 16600: Loss = -11255.648610214692
1
Iteration 16700: Loss = -11255.646363002772
Iteration 16800: Loss = -11255.66005860671
1
Iteration 16900: Loss = -11255.64638413113
Iteration 17000: Loss = -11255.646386935376
Iteration 17100: Loss = -11255.646469272797
Iteration 17200: Loss = -11255.64629123925
Iteration 17300: Loss = -11255.646287610209
Iteration 17400: Loss = -11255.646448366118
1
Iteration 17500: Loss = -11255.646254263665
Iteration 17600: Loss = -11255.6480382773
1
Iteration 17700: Loss = -11255.65001485703
2
Iteration 17800: Loss = -11255.671876036884
3
Iteration 17900: Loss = -11255.646328676283
Iteration 18000: Loss = -11255.646336046186
Iteration 18100: Loss = -11255.790623438585
1
Iteration 18200: Loss = -11255.646204667944
Iteration 18300: Loss = -11255.645849688055
Iteration 18400: Loss = -11255.647734351121
1
Iteration 18500: Loss = -11255.645860835906
Iteration 18600: Loss = -11255.64596562169
1
Iteration 18700: Loss = -11255.645905963573
Iteration 18800: Loss = -11255.645869109158
Iteration 18900: Loss = -11256.20961763252
1
Iteration 19000: Loss = -11255.645889176825
Iteration 19100: Loss = -11255.645868038962
Iteration 19200: Loss = -11256.055313432313
1
Iteration 19300: Loss = -11255.64586971373
Iteration 19400: Loss = -11255.64588177891
Iteration 19500: Loss = -11255.657160451821
1
Iteration 19600: Loss = -11255.64585634539
Iteration 19700: Loss = -11255.645977606078
1
Iteration 19800: Loss = -11255.645883939535
Iteration 19900: Loss = -11255.646128960014
1
pi: tensor([[0.7625, 0.2375],
        [0.2598, 0.7402]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5441, 0.4559], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3047, 0.0935],
         [0.7175, 0.1935]],

        [[0.7231, 0.1102],
         [0.6465, 0.6171]],

        [[0.5368, 0.1025],
         [0.6088, 0.6747]],

        [[0.5175, 0.1085],
         [0.6704, 0.5785]],

        [[0.5326, 0.0926],
         [0.6383, 0.6449]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524809382781522
Average Adjusted Rand Index: 0.952643817179708
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21667.05812823048
Iteration 100: Loss = -11598.772808570946
Iteration 200: Loss = -11597.607374025423
Iteration 300: Loss = -11596.043845111635
Iteration 400: Loss = -11590.65946718074
Iteration 500: Loss = -11526.474951756263
Iteration 600: Loss = -11403.369517238947
Iteration 700: Loss = -11263.375946771597
Iteration 800: Loss = -11257.72855364428
Iteration 900: Loss = -11257.391879244806
Iteration 1000: Loss = -11257.266568219946
Iteration 1100: Loss = -11257.184782962277
Iteration 1200: Loss = -11257.119132841275
Iteration 1300: Loss = -11256.446891957446
Iteration 1400: Loss = -11256.420062329424
Iteration 1500: Loss = -11256.40027840022
Iteration 1600: Loss = -11256.384088654924
Iteration 1700: Loss = -11256.369704425942
Iteration 1800: Loss = -11256.355903923863
Iteration 1900: Loss = -11256.33723826119
Iteration 2000: Loss = -11256.309915539627
Iteration 2100: Loss = -11256.299198240438
Iteration 2200: Loss = -11256.289911171525
Iteration 2300: Loss = -11256.278830894153
Iteration 2400: Loss = -11256.26549603629
Iteration 2500: Loss = -11256.25718604108
Iteration 2600: Loss = -11256.207445647246
Iteration 2700: Loss = -11256.203224868455
Iteration 2800: Loss = -11256.20027130803
Iteration 2900: Loss = -11256.197701909658
Iteration 3000: Loss = -11256.195885079946
Iteration 3100: Loss = -11256.192062655253
Iteration 3200: Loss = -11256.186910686973
Iteration 3300: Loss = -11256.183841403199
Iteration 3400: Loss = -11256.181872969197
Iteration 3500: Loss = -11256.179792764531
Iteration 3600: Loss = -11256.17742085351
Iteration 3700: Loss = -11256.175646560558
Iteration 3800: Loss = -11256.174298212185
Iteration 3900: Loss = -11256.174120229698
Iteration 4000: Loss = -11256.171808689394
Iteration 4100: Loss = -11256.16805390274
Iteration 4200: Loss = -11256.172338637944
1
Iteration 4300: Loss = -11256.177678922879
2
Iteration 4400: Loss = -11256.164464287514
Iteration 4500: Loss = -11256.108792748882
Iteration 4600: Loss = -11255.954047897276
Iteration 4700: Loss = -11255.954296407992
1
Iteration 4800: Loss = -11255.957397923015
2
Iteration 4900: Loss = -11255.952545527818
Iteration 5000: Loss = -11255.952155529882
Iteration 5100: Loss = -11255.95262035412
1
Iteration 5200: Loss = -11255.951375427305
Iteration 5300: Loss = -11255.951743878035
1
Iteration 5400: Loss = -11255.950543526395
Iteration 5500: Loss = -11255.950188611832
Iteration 5600: Loss = -11255.950936621839
1
Iteration 5700: Loss = -11255.949811217377
Iteration 5800: Loss = -11255.949145337749
Iteration 5900: Loss = -11255.9485560273
Iteration 6000: Loss = -11255.948094320851
Iteration 6100: Loss = -11255.946611868656
Iteration 6200: Loss = -11255.944874807486
Iteration 6300: Loss = -11255.944424115805
Iteration 6400: Loss = -11255.944032085985
Iteration 6500: Loss = -11255.941880124265
Iteration 6600: Loss = -11255.911784251191
Iteration 6700: Loss = -11255.907223315675
Iteration 6800: Loss = -11255.9070499772
Iteration 6900: Loss = -11255.919189520318
1
Iteration 7000: Loss = -11255.906729241115
Iteration 7100: Loss = -11255.906665722363
Iteration 7200: Loss = -11255.917474399046
1
Iteration 7300: Loss = -11255.906203603363
Iteration 7400: Loss = -11255.914793940554
1
Iteration 7500: Loss = -11255.904619947525
Iteration 7600: Loss = -11255.90198373016
Iteration 7700: Loss = -11255.900850080076
Iteration 7800: Loss = -11255.900388398171
Iteration 7900: Loss = -11255.90024047853
Iteration 8000: Loss = -11255.900656290534
1
Iteration 8100: Loss = -11255.899946249821
Iteration 8200: Loss = -11255.899354624373
Iteration 8300: Loss = -11255.895316001679
Iteration 8400: Loss = -11255.890995534532
Iteration 8500: Loss = -11255.892825496287
1
Iteration 8600: Loss = -11255.889621051207
Iteration 8700: Loss = -11255.8923867445
1
Iteration 8800: Loss = -11255.888326805009
Iteration 8900: Loss = -11255.887538454836
Iteration 9000: Loss = -11255.899592708394
1
Iteration 9100: Loss = -11255.886950008793
Iteration 9200: Loss = -11255.748684717386
Iteration 9300: Loss = -11255.723240182782
Iteration 9400: Loss = -11255.903871294955
1
Iteration 9500: Loss = -11255.72317422749
Iteration 9600: Loss = -11255.723142447452
Iteration 9700: Loss = -11255.723167643462
Iteration 9800: Loss = -11255.733503593465
1
Iteration 9900: Loss = -11255.738251989324
2
Iteration 10000: Loss = -11255.727119255838
3
Iteration 10100: Loss = -11255.722925802518
Iteration 10200: Loss = -11255.72263043041
Iteration 10300: Loss = -11255.728215577055
1
Iteration 10400: Loss = -11255.721219260377
Iteration 10500: Loss = -11255.723471092442
1
Iteration 10600: Loss = -11255.708122057695
Iteration 10700: Loss = -11255.709831964043
1
Iteration 10800: Loss = -11255.708118882016
Iteration 10900: Loss = -11255.734868675421
1
Iteration 11000: Loss = -11255.708076535355
Iteration 11100: Loss = -11255.70805910472
Iteration 11200: Loss = -11255.746339990386
1
Iteration 11300: Loss = -11255.676267334418
Iteration 11400: Loss = -11255.94347359752
1
Iteration 11500: Loss = -11255.676251452307
Iteration 11600: Loss = -11255.726260351728
1
Iteration 11700: Loss = -11255.676196431034
Iteration 11800: Loss = -11255.686954358756
1
Iteration 11900: Loss = -11255.676322103993
2
Iteration 12000: Loss = -11255.676263615847
Iteration 12100: Loss = -11255.676835982485
1
Iteration 12200: Loss = -11255.676166762849
Iteration 12300: Loss = -11255.707046078142
1
Iteration 12400: Loss = -11255.676154184768
Iteration 12500: Loss = -11256.064098775685
1
Iteration 12600: Loss = -11255.676183038118
Iteration 12700: Loss = -11255.676237857451
Iteration 12800: Loss = -11255.67619055057
Iteration 12900: Loss = -11255.677405323351
1
Iteration 13000: Loss = -11255.75437088309
2
Iteration 13100: Loss = -11255.676068081077
Iteration 13200: Loss = -11255.676824475488
1
Iteration 13300: Loss = -11255.676393874923
2
Iteration 13400: Loss = -11255.67597673923
Iteration 13500: Loss = -11255.676786733871
1
Iteration 13600: Loss = -11255.675908801692
Iteration 13700: Loss = -11255.675902644549
Iteration 13800: Loss = -11255.676638088113
1
Iteration 13900: Loss = -11255.675756737459
Iteration 14000: Loss = -11255.675793396606
Iteration 14100: Loss = -11255.675804357563
Iteration 14200: Loss = -11255.675685997583
Iteration 14300: Loss = -11255.6742220678
Iteration 14400: Loss = -11255.756069767174
1
Iteration 14500: Loss = -11255.671077037225
Iteration 14600: Loss = -11255.67140561681
1
Iteration 14700: Loss = -11255.68280328216
2
Iteration 14800: Loss = -11255.66988269683
Iteration 14900: Loss = -11255.67060675533
1
Iteration 15000: Loss = -11255.669875660611
Iteration 15100: Loss = -11255.685540304512
1
Iteration 15200: Loss = -11255.669704119111
Iteration 15300: Loss = -11255.668861329676
Iteration 15400: Loss = -11255.652332603624
Iteration 15500: Loss = -11255.650712096312
Iteration 15600: Loss = -11255.650652337603
Iteration 15700: Loss = -11255.65937831664
1
Iteration 15800: Loss = -11255.65064161165
Iteration 15900: Loss = -11255.650840763738
1
Iteration 16000: Loss = -11255.65106670343
2
Iteration 16100: Loss = -11255.746442886124
3
Iteration 16200: Loss = -11255.650626765086
Iteration 16300: Loss = -11255.68188043803
1
Iteration 16400: Loss = -11255.69358128131
2
Iteration 16500: Loss = -11255.654013130914
3
Iteration 16600: Loss = -11255.650697253657
Iteration 16700: Loss = -11255.6542969795
1
Iteration 16800: Loss = -11255.650429887046
Iteration 16900: Loss = -11255.65042986569
Iteration 17000: Loss = -11255.650497074334
Iteration 17100: Loss = -11255.65040326255
Iteration 17200: Loss = -11255.650706285047
1
Iteration 17300: Loss = -11255.71299860396
2
Iteration 17400: Loss = -11255.650411524859
Iteration 17500: Loss = -11255.650642515131
1
Iteration 17600: Loss = -11255.650364958734
Iteration 17700: Loss = -11255.913690939316
1
Iteration 17800: Loss = -11255.650083984561
Iteration 17900: Loss = -11255.660310984691
1
Iteration 18000: Loss = -11255.650057149705
Iteration 18100: Loss = -11255.727880441984
1
Iteration 18200: Loss = -11255.649649123725
Iteration 18300: Loss = -11255.649642809332
Iteration 18400: Loss = -11255.651744136912
1
Iteration 18500: Loss = -11255.649640907322
Iteration 18600: Loss = -11255.649614580154
Iteration 18700: Loss = -11255.651844228285
1
Iteration 18800: Loss = -11255.649335636817
Iteration 18900: Loss = -11255.649237916481
Iteration 19000: Loss = -11255.649432798118
1
Iteration 19100: Loss = -11255.64933593034
Iteration 19200: Loss = -11255.650394009042
1
Iteration 19300: Loss = -11255.64997936045
2
Iteration 19400: Loss = -11255.648560187197
Iteration 19500: Loss = -11255.648537905337
Iteration 19600: Loss = -11255.647959385915
Iteration 19700: Loss = -11255.64795414804
Iteration 19800: Loss = -11255.647675112617
Iteration 19900: Loss = -11255.646943272164
pi: tensor([[0.7434, 0.2566],
        [0.2433, 0.7567]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4636, 0.5364], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1910, 0.0946],
         [0.7073, 0.3081]],

        [[0.5085, 0.1117],
         [0.5903, 0.6824]],

        [[0.6675, 0.1037],
         [0.5689, 0.5044]],

        [[0.7030, 0.1100],
         [0.7264, 0.6646]],

        [[0.7221, 0.0937],
         [0.6554, 0.6662]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524809382781522
Average Adjusted Rand Index: 0.952643817179708
11272.879573707021
[0.9524809382781522, 0.9524809382781522] [0.952643817179708, 0.952643817179708] [11255.646294932476, 11256.068751707875]
-------------------------------------
This iteration is 91
True Objective function: Loss = -11084.339780930504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21812.50117605591
Iteration 100: Loss = -11443.504004177714
Iteration 200: Loss = -11438.507305035026
Iteration 300: Loss = -11421.215561674022
Iteration 400: Loss = -11346.612211805048
Iteration 500: Loss = -11261.01486327799
Iteration 600: Loss = -11161.245776725937
Iteration 700: Loss = -11134.24119140409
Iteration 800: Loss = -11133.54155455793
Iteration 900: Loss = -11133.23260522244
Iteration 1000: Loss = -11133.030880890674
Iteration 1100: Loss = -11132.897746547995
Iteration 1200: Loss = -11132.787867664169
Iteration 1300: Loss = -11132.710166875026
Iteration 1400: Loss = -11132.639625148506
Iteration 1500: Loss = -11131.903161614688
Iteration 1600: Loss = -11131.831737502855
Iteration 1700: Loss = -11131.802965441295
Iteration 1800: Loss = -11131.778052657037
Iteration 1900: Loss = -11131.753675631588
Iteration 2000: Loss = -11131.724229825859
Iteration 2100: Loss = -11131.662578875777
Iteration 2200: Loss = -11131.571675646524
Iteration 2300: Loss = -11131.301539850656
Iteration 2400: Loss = -11130.9955388896
Iteration 2500: Loss = -11130.73362675179
Iteration 2600: Loss = -11130.66798958247
Iteration 2700: Loss = -11130.635958240075
Iteration 2800: Loss = -11130.609149267266
Iteration 2900: Loss = -11130.590204519332
Iteration 3000: Loss = -11130.57965738636
Iteration 3100: Loss = -11130.572315148978
Iteration 3200: Loss = -11130.585336785678
1
Iteration 3300: Loss = -11130.557818128447
Iteration 3400: Loss = -11130.548242102634
Iteration 3500: Loss = -11130.54207102362
Iteration 3600: Loss = -11130.528185317231
Iteration 3700: Loss = -11130.443604800628
Iteration 3800: Loss = -11130.427085169038
Iteration 3900: Loss = -11130.416254248854
Iteration 4000: Loss = -11130.411655177219
Iteration 4100: Loss = -11130.403765934405
Iteration 4200: Loss = -11130.358419214386
Iteration 4300: Loss = -11130.325057815615
Iteration 4400: Loss = -11130.310273401257
Iteration 4500: Loss = -11130.24771818748
Iteration 4600: Loss = -11130.224149868982
Iteration 4700: Loss = -11130.218868042772
Iteration 4800: Loss = -11130.216623342032
Iteration 4900: Loss = -11130.222293063993
1
Iteration 5000: Loss = -11130.210877176643
Iteration 5100: Loss = -11130.209898408762
Iteration 5200: Loss = -11130.209431119347
Iteration 5300: Loss = -11130.205543423494
Iteration 5400: Loss = -11130.203348604147
Iteration 5500: Loss = -11130.205442679486
1
Iteration 5600: Loss = -11130.20056410652
Iteration 5700: Loss = -11130.201543598556
1
Iteration 5800: Loss = -11130.199459407027
Iteration 5900: Loss = -11130.20407035426
1
Iteration 6000: Loss = -11130.19861852057
Iteration 6100: Loss = -11130.200325689964
1
Iteration 6200: Loss = -11130.197912122494
Iteration 6300: Loss = -11130.197559057662
Iteration 6400: Loss = -11130.197234789046
Iteration 6500: Loss = -11130.199789005925
1
Iteration 6600: Loss = -11130.196653514853
Iteration 6700: Loss = -11130.19819808525
1
Iteration 6800: Loss = -11130.195938007482
Iteration 6900: Loss = -11130.19558152301
Iteration 7000: Loss = -11130.195585977728
Iteration 7100: Loss = -11130.195117785346
Iteration 7200: Loss = -11130.19496457307
Iteration 7300: Loss = -11130.194796598786
Iteration 7400: Loss = -11130.194593159304
Iteration 7500: Loss = -11130.194391702806
Iteration 7600: Loss = -11130.194203429435
Iteration 7700: Loss = -11130.195043576246
1
Iteration 7800: Loss = -11130.193180355975
Iteration 7900: Loss = -11130.19330175249
1
Iteration 8000: Loss = -11130.192705664986
Iteration 8100: Loss = -11130.192914550533
1
Iteration 8200: Loss = -11130.192483001107
Iteration 8300: Loss = -11130.192665488525
1
Iteration 8400: Loss = -11130.192586889223
2
Iteration 8500: Loss = -11130.194671445784
3
Iteration 8600: Loss = -11130.192178732894
Iteration 8700: Loss = -11130.21408240426
1
Iteration 8800: Loss = -11130.19201193667
Iteration 8900: Loss = -11130.302776497043
1
Iteration 9000: Loss = -11130.19188225023
Iteration 9100: Loss = -11130.19320847091
1
Iteration 9200: Loss = -11130.220159526601
2
Iteration 9300: Loss = -11130.190440196324
Iteration 9400: Loss = -11130.190579205433
1
Iteration 9500: Loss = -11130.193868811031
2
Iteration 9600: Loss = -11130.190356547548
Iteration 9700: Loss = -11130.365106186195
1
Iteration 9800: Loss = -11130.190285822391
Iteration 9900: Loss = -11130.19395426483
1
Iteration 10000: Loss = -11130.190243403316
Iteration 10100: Loss = -11130.190888309551
1
Iteration 10200: Loss = -11130.190206864703
Iteration 10300: Loss = -11130.190403779845
1
Iteration 10400: Loss = -11130.228543911657
2
Iteration 10500: Loss = -11130.190093138497
Iteration 10600: Loss = -11130.197450054653
1
Iteration 10700: Loss = -11130.190104353296
Iteration 10800: Loss = -11130.192752559518
1
Iteration 10900: Loss = -11130.190026462258
Iteration 11000: Loss = -11130.274351800725
1
Iteration 11100: Loss = -11130.18998278698
Iteration 11200: Loss = -11130.248620422386
1
Iteration 11300: Loss = -11130.208775183763
2
Iteration 11400: Loss = -11130.189886924289
Iteration 11500: Loss = -11130.189848516591
Iteration 11600: Loss = -11130.189959688983
1
Iteration 11700: Loss = -11130.195553479214
2
Iteration 11800: Loss = -11130.19977553381
3
Iteration 11900: Loss = -11130.189823386328
Iteration 12000: Loss = -11130.190129757357
1
Iteration 12100: Loss = -11130.204396840614
2
Iteration 12200: Loss = -11130.189764734698
Iteration 12300: Loss = -11130.189904687108
1
Iteration 12400: Loss = -11130.21580827803
2
Iteration 12500: Loss = -11130.189694502884
Iteration 12600: Loss = -11130.201979128324
1
Iteration 12700: Loss = -11130.189696857038
Iteration 12800: Loss = -11130.223903572218
1
Iteration 12900: Loss = -11130.18969577166
Iteration 13000: Loss = -11130.189951481812
1
Iteration 13100: Loss = -11130.282227451708
2
Iteration 13200: Loss = -11130.190302071383
3
Iteration 13300: Loss = -11130.189677552253
Iteration 13400: Loss = -11130.191589968434
1
Iteration 13500: Loss = -11130.18964823177
Iteration 13600: Loss = -11130.19237649292
1
Iteration 13700: Loss = -11130.189615092691
Iteration 13800: Loss = -11130.287107150189
1
Iteration 13900: Loss = -11130.189633071004
Iteration 14000: Loss = -11130.212322615687
1
Iteration 14100: Loss = -11130.189607358063
Iteration 14200: Loss = -11130.21078219578
1
Iteration 14300: Loss = -11130.202801391504
2
Iteration 14400: Loss = -11130.200562019201
3
Iteration 14500: Loss = -11130.195267911628
4
Iteration 14600: Loss = -11130.195228883951
5
Iteration 14700: Loss = -11130.189719400023
6
Iteration 14800: Loss = -11130.189623722783
Iteration 14900: Loss = -11130.389076178224
1
Iteration 15000: Loss = -11130.192403162973
2
Iteration 15100: Loss = -11130.206208850908
3
Iteration 15200: Loss = -11130.267447975573
4
Iteration 15300: Loss = -11130.18948704718
Iteration 15400: Loss = -11130.19117202398
1
Iteration 15500: Loss = -11130.189481615438
Iteration 15600: Loss = -11130.189615511017
1
Iteration 15700: Loss = -11130.189469305933
Iteration 15800: Loss = -11130.190510463968
1
Iteration 15900: Loss = -11130.189449988424
Iteration 16000: Loss = -11130.193788044362
1
Iteration 16100: Loss = -11130.189452918801
Iteration 16200: Loss = -11130.190355810348
1
Iteration 16300: Loss = -11130.18957309277
2
Iteration 16400: Loss = -11130.189589221463
3
Iteration 16500: Loss = -11130.194155106576
4
Iteration 16600: Loss = -11130.192600812792
5
Iteration 16700: Loss = -11130.201639250683
6
Iteration 16800: Loss = -11130.30844260125
7
Iteration 16900: Loss = -11130.190114751174
8
Iteration 17000: Loss = -11130.189504507058
Iteration 17100: Loss = -11130.204426519107
1
Iteration 17200: Loss = -11130.196315620913
2
Iteration 17300: Loss = -11130.190545155434
3
Iteration 17400: Loss = -11130.1950729178
4
Iteration 17500: Loss = -11130.189472410168
Iteration 17600: Loss = -11130.194802456333
1
Iteration 17700: Loss = -11130.189464685389
Iteration 17800: Loss = -11130.27916856508
1
Iteration 17900: Loss = -11130.189449179878
Iteration 18000: Loss = -11130.19159667542
1
Iteration 18100: Loss = -11130.190325082445
2
Iteration 18200: Loss = -11130.192539269756
3
Iteration 18300: Loss = -11130.190353875263
4
Iteration 18400: Loss = -11130.189867254521
5
Iteration 18500: Loss = -11130.189453606523
Iteration 18600: Loss = -11130.18959128858
1
Iteration 18700: Loss = -11130.189460649124
Iteration 18800: Loss = -11130.19075069559
1
Iteration 18900: Loss = -11130.189485660354
Iteration 19000: Loss = -11130.199719262024
1
Iteration 19100: Loss = -11130.196913656353
2
Iteration 19200: Loss = -11130.18944348873
Iteration 19300: Loss = -11130.19359500466
1
Iteration 19400: Loss = -11130.189440160324
Iteration 19500: Loss = -11130.189497673753
Iteration 19600: Loss = -11130.228607095669
1
Iteration 19700: Loss = -11130.189391138885
Iteration 19800: Loss = -11130.355265300173
1
Iteration 19900: Loss = -11130.18938310479
pi: tensor([[0.6746, 0.3254],
        [0.1736, 0.8264]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9529, 0.0471], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1828, 0.1226],
         [0.6744, 0.3094]],

        [[0.5039, 0.0931],
         [0.5752, 0.5193]],

        [[0.7289, 0.1056],
         [0.5630, 0.7073]],

        [[0.6455, 0.0916],
         [0.5587, 0.5201]],

        [[0.6580, 0.0964],
         [0.6018, 0.6982]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6457223393920053
Average Adjusted Rand Index: 0.7839989028908529
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20641.34968995471
Iteration 100: Loss = -11441.408729026978
Iteration 200: Loss = -11433.865604896215
Iteration 300: Loss = -11383.86048398166
Iteration 400: Loss = -11209.95131035267
Iteration 500: Loss = -11145.755340658234
Iteration 600: Loss = -11133.76453428709
Iteration 700: Loss = -11133.403656838387
Iteration 800: Loss = -11133.29207150149
Iteration 900: Loss = -11133.197489426104
Iteration 1000: Loss = -11132.509756695774
Iteration 1100: Loss = -11132.479637730534
Iteration 1200: Loss = -11132.463290859507
Iteration 1300: Loss = -11132.450600987195
Iteration 1400: Loss = -11132.440383559906
Iteration 1500: Loss = -11132.43148156585
Iteration 1600: Loss = -11132.422673954246
Iteration 1700: Loss = -11132.408990088565
Iteration 1800: Loss = -11132.40004123057
Iteration 1900: Loss = -11132.394391508466
Iteration 2000: Loss = -11132.386286217057
Iteration 2100: Loss = -11132.372614873499
Iteration 2200: Loss = -11132.3683003348
Iteration 2300: Loss = -11132.36682546385
Iteration 2400: Loss = -11132.364022952748
Iteration 2500: Loss = -11132.362039822008
Iteration 2600: Loss = -11132.402166102029
1
Iteration 2700: Loss = -11131.979530910114
Iteration 2800: Loss = -11131.97789133904
Iteration 2900: Loss = -11131.976724892718
Iteration 3000: Loss = -11131.97582359863
Iteration 3100: Loss = -11131.974868987229
Iteration 3200: Loss = -11131.974038836255
Iteration 3300: Loss = -11131.990463281116
1
Iteration 3400: Loss = -11131.972650443435
Iteration 3500: Loss = -11131.971986833836
Iteration 3600: Loss = -11131.971863966257
Iteration 3700: Loss = -11131.948689609433
Iteration 3800: Loss = -11131.859847134208
Iteration 3900: Loss = -11131.697243079729
Iteration 4000: Loss = -11131.69651114221
Iteration 4100: Loss = -11131.695546770276
Iteration 4200: Loss = -11131.694181896099
Iteration 4300: Loss = -11131.691608179204
Iteration 4400: Loss = -11131.671311189328
Iteration 4500: Loss = -11131.392427313025
Iteration 4600: Loss = -11131.333793602818
Iteration 4700: Loss = -11131.321947007575
Iteration 4800: Loss = -11131.323905750756
1
Iteration 4900: Loss = -11131.290743301794
Iteration 5000: Loss = -11131.237116627342
Iteration 5100: Loss = -11131.23508789152
Iteration 5200: Loss = -11131.234463926525
Iteration 5300: Loss = -11131.233839523475
Iteration 5400: Loss = -11131.234029748453
1
Iteration 5500: Loss = -11131.233342436242
Iteration 5600: Loss = -11131.233288842053
Iteration 5700: Loss = -11131.232999740081
Iteration 5800: Loss = -11131.232895995587
Iteration 5900: Loss = -11131.234903135897
1
Iteration 6000: Loss = -11131.232536913038
Iteration 6100: Loss = -11131.232050571545
Iteration 6200: Loss = -11131.223659113437
Iteration 6300: Loss = -11131.102813143405
Iteration 6400: Loss = -11130.977326348922
Iteration 6500: Loss = -11130.976963545856
Iteration 6600: Loss = -11130.9765577694
Iteration 6700: Loss = -11130.976369259935
Iteration 6800: Loss = -11130.97628488054
Iteration 6900: Loss = -11130.976234741776
Iteration 7000: Loss = -11130.976090729213
Iteration 7100: Loss = -11130.975978135973
Iteration 7200: Loss = -11130.973334038992
Iteration 7300: Loss = -11130.831295656091
Iteration 7400: Loss = -11130.831632357702
1
Iteration 7500: Loss = -11130.831033110137
Iteration 7600: Loss = -11130.76411119331
Iteration 7700: Loss = -11130.759302360348
Iteration 7800: Loss = -11130.750773127294
Iteration 7900: Loss = -11130.749746129999
Iteration 8000: Loss = -11130.74878525751
Iteration 8100: Loss = -11130.748201057628
Iteration 8200: Loss = -11130.776593287072
1
Iteration 8300: Loss = -11130.746953375878
Iteration 8400: Loss = -11130.74785989647
1
Iteration 8500: Loss = -11130.745975827609
Iteration 8600: Loss = -11130.647569964382
Iteration 8700: Loss = -11130.651789117166
1
Iteration 8800: Loss = -11130.647193261684
Iteration 8900: Loss = -11130.64740337941
1
Iteration 9000: Loss = -11130.647123866127
Iteration 9100: Loss = -11130.647105855824
Iteration 9200: Loss = -11130.646970891961
Iteration 9300: Loss = -11130.64597281952
Iteration 9400: Loss = -11130.645695502657
Iteration 9500: Loss = -11130.677827284866
1
Iteration 9600: Loss = -11130.64569640318
Iteration 9700: Loss = -11130.67073000739
1
Iteration 9800: Loss = -11130.66190468557
2
Iteration 9900: Loss = -11130.644946645654
Iteration 10000: Loss = -11130.64494625252
Iteration 10100: Loss = -11130.64921110433
1
Iteration 10200: Loss = -11130.644598694782
Iteration 10300: Loss = -11130.652830137435
1
Iteration 10400: Loss = -11130.644475262447
Iteration 10500: Loss = -11130.644458856706
Iteration 10600: Loss = -11130.644470000318
Iteration 10700: Loss = -11130.644370621514
Iteration 10800: Loss = -11130.644414721766
Iteration 10900: Loss = -11130.644257325086
Iteration 11000: Loss = -11130.628515998354
Iteration 11100: Loss = -11130.929561956067
1
Iteration 11200: Loss = -11130.616150848096
Iteration 11300: Loss = -11130.634455730507
1
Iteration 11400: Loss = -11130.564812023951
Iteration 11500: Loss = -11130.640869193467
1
Iteration 11600: Loss = -11130.508063423016
Iteration 11700: Loss = -11130.492361608362
Iteration 11800: Loss = -11130.483310385735
Iteration 11900: Loss = -11130.48121850591
Iteration 12000: Loss = -11130.418362228504
Iteration 12100: Loss = -11130.413929011665
Iteration 12200: Loss = -11130.407805987106
Iteration 12300: Loss = -11130.40696842633
Iteration 12400: Loss = -11130.403287274381
Iteration 12500: Loss = -11130.403222747447
Iteration 12600: Loss = -11130.405579157567
1
Iteration 12700: Loss = -11130.400338286752
Iteration 12800: Loss = -11130.34908805498
Iteration 12900: Loss = -11130.34739634229
Iteration 13000: Loss = -11130.347471715077
Iteration 13100: Loss = -11130.347392094167
Iteration 13200: Loss = -11130.337344183825
Iteration 13300: Loss = -11130.336975279952
Iteration 13400: Loss = -11130.338011551989
1
Iteration 13500: Loss = -11130.430300845266
2
Iteration 13600: Loss = -11130.336111761031
Iteration 13700: Loss = -11130.459468701278
1
Iteration 13800: Loss = -11130.336131852957
Iteration 13900: Loss = -11130.336561760912
1
Iteration 14000: Loss = -11130.328948344406
Iteration 14100: Loss = -11130.341832949332
1
Iteration 14200: Loss = -11130.318668098953
Iteration 14300: Loss = -11130.631676043315
1
Iteration 14400: Loss = -11130.291504838973
Iteration 14500: Loss = -11130.29150771371
Iteration 14600: Loss = -11130.307106150596
1
Iteration 14700: Loss = -11130.369823466563
2
Iteration 14800: Loss = -11130.292622267927
3
Iteration 14900: Loss = -11130.290574276047
Iteration 15000: Loss = -11130.310039544813
1
Iteration 15100: Loss = -11130.284824086802
Iteration 15200: Loss = -11130.286778367776
1
Iteration 15300: Loss = -11130.28592324158
2
Iteration 15400: Loss = -11130.284745037183
Iteration 15500: Loss = -11130.293946426227
1
Iteration 15600: Loss = -11130.284699487522
Iteration 15700: Loss = -11130.28682443406
1
Iteration 15800: Loss = -11130.287054758426
2
Iteration 15900: Loss = -11130.289865860123
3
Iteration 16000: Loss = -11130.290273676172
4
Iteration 16100: Loss = -11130.28831630503
5
Iteration 16200: Loss = -11130.28919125535
6
Iteration 16300: Loss = -11130.284602504922
Iteration 16400: Loss = -11130.284673869173
Iteration 16500: Loss = -11130.325815099412
1
Iteration 16600: Loss = -11130.284594915776
Iteration 16700: Loss = -11130.285382757345
1
Iteration 16800: Loss = -11130.2839034153
Iteration 16900: Loss = -11130.28395754733
Iteration 17000: Loss = -11130.28890358333
1
Iteration 17100: Loss = -11130.2838109002
Iteration 17200: Loss = -11130.283840456474
Iteration 17300: Loss = -11130.23204221722
Iteration 17400: Loss = -11130.355109427941
1
Iteration 17500: Loss = -11130.236894641672
2
Iteration 17600: Loss = -11130.216268231676
Iteration 17700: Loss = -11130.215860317114
Iteration 17800: Loss = -11130.23464255167
1
Iteration 17900: Loss = -11130.231080802429
2
Iteration 18000: Loss = -11130.20923952107
Iteration 18100: Loss = -11130.210117499373
1
Iteration 18200: Loss = -11130.209221155561
Iteration 18300: Loss = -11130.213004865216
1
Iteration 18400: Loss = -11130.200786425032
Iteration 18500: Loss = -11130.204904153677
1
Iteration 18600: Loss = -11130.204360218862
2
Iteration 18700: Loss = -11130.199098028314
Iteration 18800: Loss = -11130.257024741855
1
Iteration 18900: Loss = -11130.198334090843
Iteration 19000: Loss = -11130.198688706521
1
Iteration 19100: Loss = -11130.202524520435
2
Iteration 19200: Loss = -11130.198303250307
Iteration 19300: Loss = -11130.413460091266
1
Iteration 19400: Loss = -11130.198340989737
Iteration 19500: Loss = -11130.211226460608
1
Iteration 19600: Loss = -11130.198339554508
Iteration 19700: Loss = -11130.211992166036
1
Iteration 19800: Loss = -11130.207225201237
2
Iteration 19900: Loss = -11130.1992716189
3
pi: tensor([[0.6729, 0.3271],
        [0.1747, 0.8253]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9533, 0.0467], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1826, 0.1219],
         [0.5835, 0.3097]],

        [[0.6399, 0.0932],
         [0.6378, 0.7027]],

        [[0.5137, 0.1057],
         [0.7255, 0.7275]],

        [[0.5472, 0.0917],
         [0.6720, 0.6338]],

        [[0.5764, 0.0964],
         [0.6282, 0.5340]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6457223393920053
Average Adjusted Rand Index: 0.7839989028908529
11084.339780930504
[0.6457223393920053, 0.6457223393920053] [0.7839989028908529, 0.7839989028908529] [11130.189409962275, 11130.202494754383]
-------------------------------------
This iteration is 92
True Objective function: Loss = -11063.415993853168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22646.475573690877
Iteration 100: Loss = -11354.074954612335
Iteration 200: Loss = -11352.607466427626
Iteration 300: Loss = -11351.737375016826
Iteration 400: Loss = -11350.763403608245
Iteration 500: Loss = -11348.33331995939
Iteration 600: Loss = -11329.802992950914
Iteration 700: Loss = -11276.980492420791
Iteration 800: Loss = -11276.151451327733
Iteration 900: Loss = -11275.927238836026
Iteration 1000: Loss = -11275.828924088442
Iteration 1100: Loss = -11275.77085228044
Iteration 1200: Loss = -11275.630897526804
Iteration 1300: Loss = -11268.499964193703
Iteration 1400: Loss = -11135.2097913245
Iteration 1500: Loss = -11116.390848102254
Iteration 1600: Loss = -11111.593486301124
Iteration 1700: Loss = -11106.254426690686
Iteration 1800: Loss = -11104.741018937324
Iteration 1900: Loss = -11103.737535393928
Iteration 2000: Loss = -11099.802657855902
Iteration 2100: Loss = -11099.660559442345
Iteration 2200: Loss = -11099.394971005393
Iteration 2300: Loss = -11099.279625576464
Iteration 2400: Loss = -11099.244216620824
Iteration 2500: Loss = -11099.229500520714
Iteration 2600: Loss = -11099.223523456003
Iteration 2700: Loss = -11099.218512049625
Iteration 2800: Loss = -11099.213927003839
Iteration 2900: Loss = -11099.20945690506
Iteration 3000: Loss = -11099.203385787223
Iteration 3100: Loss = -11093.022518671067
Iteration 3200: Loss = -11090.213417010757
Iteration 3300: Loss = -11090.17826569215
Iteration 3400: Loss = -11090.10624050522
Iteration 3500: Loss = -11089.580388095264
Iteration 3600: Loss = -11089.572241767324
Iteration 3700: Loss = -11089.561486522069
Iteration 3800: Loss = -11089.513844893898
Iteration 3900: Loss = -11089.511542341917
Iteration 4000: Loss = -11089.507280464146
Iteration 4100: Loss = -11087.021087094541
Iteration 4200: Loss = -11086.947882064733
Iteration 4300: Loss = -11086.946391081845
Iteration 4400: Loss = -11086.942636138641
Iteration 4500: Loss = -11086.941479549365
Iteration 4600: Loss = -11086.941162059113
Iteration 4700: Loss = -11086.939417242547
Iteration 4800: Loss = -11086.937921799386
Iteration 4900: Loss = -11086.933561371328
Iteration 5000: Loss = -11086.912043827284
Iteration 5100: Loss = -11086.90641442571
Iteration 5200: Loss = -11086.727021136256
Iteration 5300: Loss = -11086.725209391047
Iteration 5400: Loss = -11086.722977736126
Iteration 5500: Loss = -11086.704076568632
Iteration 5600: Loss = -11086.654422109163
Iteration 5700: Loss = -11086.653545197369
Iteration 5800: Loss = -11086.653054572433
Iteration 5900: Loss = -11086.652803730898
Iteration 6000: Loss = -11086.652464964065
Iteration 6100: Loss = -11086.652254147732
Iteration 6200: Loss = -11086.649131859931
Iteration 6300: Loss = -11086.64765477928
Iteration 6400: Loss = -11086.605049806503
Iteration 6500: Loss = -11085.552716029075
Iteration 6600: Loss = -11085.495033956107
Iteration 6700: Loss = -11085.49006838256
Iteration 6800: Loss = -11085.49312661243
1
Iteration 6900: Loss = -11085.488738447993
Iteration 7000: Loss = -11085.488239413531
Iteration 7100: Loss = -11085.499717820037
1
Iteration 7200: Loss = -11085.486944470926
Iteration 7300: Loss = -11085.406580663504
Iteration 7400: Loss = -11085.407312033014
1
Iteration 7500: Loss = -11085.406094300955
Iteration 7600: Loss = -11085.405884586413
Iteration 7700: Loss = -11085.40899621977
1
Iteration 7800: Loss = -11085.4091904425
2
Iteration 7900: Loss = -11085.411851611138
3
Iteration 8000: Loss = -11085.405556767664
Iteration 8100: Loss = -11085.429263203532
1
Iteration 8200: Loss = -11085.407354552375
2
Iteration 8300: Loss = -11085.412827976483
3
Iteration 8400: Loss = -11085.414298505188
4
Iteration 8500: Loss = -11085.403729347767
Iteration 8600: Loss = -11085.404048500932
1
Iteration 8700: Loss = -11085.403570004248
Iteration 8800: Loss = -11085.409798545907
1
Iteration 8900: Loss = -11085.403472212247
Iteration 9000: Loss = -11085.41870524126
1
Iteration 9100: Loss = -11085.403423071391
Iteration 9200: Loss = -11085.403383745821
Iteration 9300: Loss = -11085.41172899057
1
Iteration 9400: Loss = -11085.403299128935
Iteration 9500: Loss = -11085.403277133966
Iteration 9600: Loss = -11085.43445931671
1
Iteration 9700: Loss = -11085.404383036619
2
Iteration 9800: Loss = -11085.267435123416
Iteration 9900: Loss = -11085.267404679811
Iteration 10000: Loss = -11085.313928793008
1
Iteration 10100: Loss = -11085.25183440373
Iteration 10200: Loss = -11085.294639249765
1
Iteration 10300: Loss = -11085.248489936426
Iteration 10400: Loss = -11085.291084508239
1
Iteration 10500: Loss = -11085.40591560546
2
Iteration 10600: Loss = -11085.251284887625
3
Iteration 10700: Loss = -11085.248437992253
Iteration 10800: Loss = -11085.248411636332
Iteration 10900: Loss = -11085.445723357549
1
Iteration 11000: Loss = -11085.248277037981
Iteration 11100: Loss = -11085.249787868403
1
Iteration 11200: Loss = -11085.245582854539
Iteration 11300: Loss = -11085.244586720157
Iteration 11400: Loss = -11085.244435793204
Iteration 11500: Loss = -11085.25864826751
1
Iteration 11600: Loss = -11085.243699992465
Iteration 11700: Loss = -11085.241367434573
Iteration 11800: Loss = -11085.437970789177
1
Iteration 11900: Loss = -11085.244263956696
2
Iteration 12000: Loss = -11085.302081498825
3
Iteration 12100: Loss = -11085.246566419835
4
Iteration 12200: Loss = -11085.242608128143
5
Iteration 12300: Loss = -11085.240584729207
Iteration 12400: Loss = -11085.239804301194
Iteration 12500: Loss = -11085.24269324643
1
Iteration 12600: Loss = -11085.267804844521
2
Iteration 12700: Loss = -11085.284970075701
3
Iteration 12800: Loss = -11085.239960259863
4
Iteration 12900: Loss = -11085.23977789549
Iteration 13000: Loss = -11085.296621207724
1
Iteration 13100: Loss = -11085.240334501186
2
Iteration 13200: Loss = -11085.237328513449
Iteration 13300: Loss = -11085.237359349152
Iteration 13400: Loss = -11085.370063212136
1
Iteration 13500: Loss = -11085.240865187745
2
Iteration 13600: Loss = -11085.237235652025
Iteration 13700: Loss = -11085.237461974008
1
Iteration 13800: Loss = -11085.238159313842
2
Iteration 13900: Loss = -11085.238291082927
3
Iteration 14000: Loss = -11085.237038837107
Iteration 14100: Loss = -11085.256815475994
1
Iteration 14200: Loss = -11085.26843421813
2
Iteration 14300: Loss = -11085.239504687497
3
Iteration 14400: Loss = -11085.242007918898
4
Iteration 14500: Loss = -11085.237173491409
5
Iteration 14600: Loss = -11085.237356171308
6
Iteration 14700: Loss = -11085.239238330701
7
Iteration 14800: Loss = -11085.236894306787
Iteration 14900: Loss = -11085.236499345165
Iteration 15000: Loss = -11085.236520782548
Iteration 15100: Loss = -11085.236459755384
Iteration 15200: Loss = -11085.236655151155
1
Iteration 15300: Loss = -11085.239458774202
2
Iteration 15400: Loss = -11085.236499361054
Iteration 15500: Loss = -11085.23711353729
1
Iteration 15600: Loss = -11085.25452391268
2
Iteration 15700: Loss = -11085.260199740735
3
Iteration 15800: Loss = -11085.426312550378
4
Iteration 15900: Loss = -11085.236406037215
Iteration 16000: Loss = -11085.236559310828
1
Iteration 16100: Loss = -11085.270589373149
2
Iteration 16200: Loss = -11085.236379909613
Iteration 16300: Loss = -11085.248225756048
1
Iteration 16400: Loss = -11085.236305321263
Iteration 16500: Loss = -11085.236266130221
Iteration 16600: Loss = -11085.2496346422
1
Iteration 16700: Loss = -11085.236219564129
Iteration 16800: Loss = -11085.557484944518
1
Iteration 16900: Loss = -11085.23632548613
2
Iteration 17000: Loss = -11085.243956835668
3
Iteration 17100: Loss = -11085.242242849521
4
Iteration 17200: Loss = -11085.237138722256
5
Iteration 17300: Loss = -11085.236362299082
6
Iteration 17400: Loss = -11085.236502441798
7
Iteration 17500: Loss = -11085.24312124275
8
Iteration 17600: Loss = -11085.23619813837
Iteration 17700: Loss = -11085.236306768762
1
Iteration 17800: Loss = -11085.23752728827
2
Iteration 17900: Loss = -11085.246388520072
3
Iteration 18000: Loss = -11085.236202369437
Iteration 18100: Loss = -11085.23624272092
Iteration 18200: Loss = -11085.251533555664
1
Iteration 18300: Loss = -11085.235929489358
Iteration 18400: Loss = -11085.235902969454
Iteration 18500: Loss = -11085.236331413947
1
Iteration 18600: Loss = -11085.235877977957
Iteration 18700: Loss = -11085.806200190937
1
Iteration 18800: Loss = -11085.23581153646
Iteration 18900: Loss = -11085.235677730921
Iteration 19000: Loss = -11085.306930005763
1
Iteration 19100: Loss = -11085.2394501343
2
Iteration 19200: Loss = -11085.23552824722
Iteration 19300: Loss = -11085.235710683302
1
Iteration 19400: Loss = -11085.235553103481
Iteration 19500: Loss = -11085.235780982204
1
Iteration 19600: Loss = -11085.235813495738
2
Iteration 19700: Loss = -11085.479254607966
3
Iteration 19800: Loss = -11085.235396540169
Iteration 19900: Loss = -11085.245267272705
1
pi: tensor([[0.2415, 0.7585],
        [0.7926, 0.2074]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4043, 0.5957], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2405, 0.0907],
         [0.5839, 0.2535]],

        [[0.6983, 0.0987],
         [0.5961, 0.5888]],

        [[0.6063, 0.0932],
         [0.6002, 0.5942]],

        [[0.6064, 0.1083],
         [0.6212, 0.6483]],

        [[0.5246, 0.0854],
         [0.6915, 0.6550]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080740404436667
time is 2
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7026660163709976
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080725364594837
Global Adjusted Rand Index: 0.022378679959674967
Average Adjusted Rand Index: 0.8095398077401216
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24694.563626613886
Iteration 100: Loss = -11354.435644
Iteration 200: Loss = -11353.370388092591
Iteration 300: Loss = -11352.85599111165
Iteration 400: Loss = -11352.203050090025
Iteration 500: Loss = -11351.220930422618
Iteration 600: Loss = -11346.070914661817
Iteration 700: Loss = -11272.381886535652
Iteration 800: Loss = -11042.291037611647
Iteration 900: Loss = -11039.751455651847
Iteration 1000: Loss = -11036.990999476102
Iteration 1100: Loss = -11036.63961534028
Iteration 1200: Loss = -11036.5077436652
Iteration 1300: Loss = -11036.188645235146
Iteration 1400: Loss = -11036.082361093411
Iteration 1500: Loss = -11035.737180519112
Iteration 1600: Loss = -11034.489510603273
Iteration 1700: Loss = -11034.45946845642
Iteration 1800: Loss = -11034.42514703015
Iteration 1900: Loss = -11034.391011025917
Iteration 2000: Loss = -11034.379002666694
Iteration 2100: Loss = -11034.369016789049
Iteration 2200: Loss = -11034.361480964133
Iteration 2300: Loss = -11034.354977788364
Iteration 2400: Loss = -11034.348581585853
Iteration 2500: Loss = -11034.341420179084
Iteration 2600: Loss = -11034.332442007542
Iteration 2700: Loss = -11034.271073289787
Iteration 2800: Loss = -11034.209018246782
Iteration 2900: Loss = -11034.204083376815
Iteration 3000: Loss = -11034.199362707424
Iteration 3100: Loss = -11034.19498562157
Iteration 3200: Loss = -11034.188966525096
Iteration 3300: Loss = -11034.169381462598
Iteration 3400: Loss = -11034.16774989669
Iteration 3500: Loss = -11034.164871770232
Iteration 3600: Loss = -11034.164087597712
Iteration 3700: Loss = -11034.162587222463
Iteration 3800: Loss = -11034.1560541471
Iteration 3900: Loss = -11034.149742712176
Iteration 4000: Loss = -11034.14792591252
Iteration 4100: Loss = -11034.146855135708
Iteration 4200: Loss = -11034.145951371125
Iteration 4300: Loss = -11034.145166049539
Iteration 4400: Loss = -11034.144289096392
Iteration 4500: Loss = -11034.143562411138
Iteration 4600: Loss = -11034.142805704641
Iteration 4700: Loss = -11034.142361777445
Iteration 4800: Loss = -11034.141892141479
Iteration 4900: Loss = -11034.142198056898
1
Iteration 5000: Loss = -11034.141192831215
Iteration 5100: Loss = -11034.143840291163
1
Iteration 5200: Loss = -11034.140901658113
Iteration 5300: Loss = -11034.150738243288
1
Iteration 5400: Loss = -11034.140071592095
Iteration 5500: Loss = -11034.139793141754
Iteration 5600: Loss = -11034.139617650206
Iteration 5700: Loss = -11034.1395792299
Iteration 5800: Loss = -11034.139242329427
Iteration 5900: Loss = -11034.138967401455
Iteration 6000: Loss = -11034.138964619675
Iteration 6100: Loss = -11034.144358922855
1
Iteration 6200: Loss = -11034.138417966587
Iteration 6300: Loss = -11034.138468767502
Iteration 6400: Loss = -11034.13813483613
Iteration 6500: Loss = -11034.138690405476
1
Iteration 6600: Loss = -11034.137824717529
Iteration 6700: Loss = -11034.138551195201
1
Iteration 6800: Loss = -11034.137532983232
Iteration 6900: Loss = -11034.137247286915
Iteration 7000: Loss = -11034.136907148415
Iteration 7100: Loss = -11034.136477016831
Iteration 7200: Loss = -11034.139469853639
1
Iteration 7300: Loss = -11034.141840392804
2
Iteration 7400: Loss = -11034.143149954845
3
Iteration 7500: Loss = -11034.140927195504
4
Iteration 7600: Loss = -11034.136674680114
5
Iteration 7700: Loss = -11034.135874337504
Iteration 7800: Loss = -11034.13561627713
Iteration 7900: Loss = -11034.135546526679
Iteration 8000: Loss = -11034.135705986137
1
Iteration 8100: Loss = -11034.13584837817
2
Iteration 8200: Loss = -11034.136814297437
3
Iteration 8300: Loss = -11034.136173732832
4
Iteration 8400: Loss = -11034.135600099165
Iteration 8500: Loss = -11034.148967503246
1
Iteration 8600: Loss = -11034.135294889216
Iteration 8700: Loss = -11034.135184043787
Iteration 8800: Loss = -11034.13569820742
1
Iteration 8900: Loss = -11034.135236824191
Iteration 9000: Loss = -11034.135081510794
Iteration 9100: Loss = -11034.137630071491
1
Iteration 9200: Loss = -11034.135132253947
Iteration 9300: Loss = -11034.135068254063
Iteration 9400: Loss = -11034.134887012076
Iteration 9500: Loss = -11034.135453711626
1
Iteration 9600: Loss = -11034.134495061318
Iteration 9700: Loss = -11034.329151041378
1
Iteration 9800: Loss = -11034.134325913503
Iteration 9900: Loss = -11034.135774288827
1
Iteration 10000: Loss = -11034.134293062292
Iteration 10100: Loss = -11034.143601161855
1
Iteration 10200: Loss = -11034.138711927211
2
Iteration 10300: Loss = -11034.148091724568
3
Iteration 10400: Loss = -11034.15469853783
4
Iteration 10500: Loss = -11034.134298359895
Iteration 10600: Loss = -11034.136315035432
1
Iteration 10700: Loss = -11034.138962813893
2
Iteration 10800: Loss = -11034.144994207149
3
Iteration 10900: Loss = -11034.134381459979
Iteration 11000: Loss = -11034.13413308832
Iteration 11100: Loss = -11034.139723231443
1
Iteration 11200: Loss = -11034.133840199176
Iteration 11300: Loss = -11034.13372629113
Iteration 11400: Loss = -11034.181157299172
1
Iteration 11500: Loss = -11034.133754959512
Iteration 11600: Loss = -11034.135155705291
1
Iteration 11700: Loss = -11034.133877101318
2
Iteration 11800: Loss = -11034.142171132631
3
Iteration 11900: Loss = -11034.134376317059
4
Iteration 12000: Loss = -11034.1433357623
5
Iteration 12100: Loss = -11034.146868339307
6
Iteration 12200: Loss = -11034.133070951833
Iteration 12300: Loss = -11034.133488937136
1
Iteration 12400: Loss = -11034.13446318433
2
Iteration 12500: Loss = -11034.133624107792
3
Iteration 12600: Loss = -11034.27716022849
4
Iteration 12700: Loss = -11034.13433272115
5
Iteration 12800: Loss = -11034.143227622653
6
Iteration 12900: Loss = -11034.133006225213
Iteration 13000: Loss = -11034.235375815246
1
Iteration 13100: Loss = -11034.133332749398
2
Iteration 13200: Loss = -11034.152572327723
3
Iteration 13300: Loss = -11034.132636603777
Iteration 13400: Loss = -11034.133044630074
1
Iteration 13500: Loss = -11034.133703198242
2
Iteration 13600: Loss = -11034.2092413905
3
Iteration 13700: Loss = -11034.142120044871
4
Iteration 13800: Loss = -11034.132489669564
Iteration 13900: Loss = -11034.137330660278
1
Iteration 14000: Loss = -11034.134488660631
2
Iteration 14100: Loss = -11034.135695247276
3
Iteration 14200: Loss = -11034.159721194259
4
Iteration 14300: Loss = -11034.132878450542
5
Iteration 14400: Loss = -11034.134667902123
6
Iteration 14500: Loss = -11034.136135937157
7
Iteration 14600: Loss = -11034.133296707803
8
Iteration 14700: Loss = -11034.132855676691
9
Iteration 14800: Loss = -11034.13297985488
10
Iteration 14900: Loss = -11034.14620613368
11
Iteration 15000: Loss = -11034.135635272743
12
Iteration 15100: Loss = -11034.132649742327
13
Iteration 15200: Loss = -11034.136890001684
14
Iteration 15300: Loss = -11034.134225282978
15
Stopping early at iteration 15300 due to no improvement.
pi: tensor([[0.7750, 0.2250],
        [0.2256, 0.7744]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4534, 0.5466], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1963, 0.0946],
         [0.6319, 0.2944]],

        [[0.6900, 0.0992],
         [0.5106, 0.5945]],

        [[0.5902, 0.0944],
         [0.6771, 0.6370]],

        [[0.7046, 0.1103],
         [0.5927, 0.6699]],

        [[0.6779, 0.0879],
         [0.6406, 0.6090]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
Global Adjusted Rand Index: 0.9137632854537449
Average Adjusted Rand Index: 0.9152316649596133
11063.415993853168
[0.022378679959674967, 0.9137632854537449] [0.8095398077401216, 0.9152316649596133] [11085.235437197502, 11034.134225282978]
-------------------------------------
This iteration is 93
True Objective function: Loss = -11154.634579662514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20360.597864214218
Iteration 100: Loss = -11421.304730167769
Iteration 200: Loss = -11420.898661094634
Iteration 300: Loss = -11420.765348382207
Iteration 400: Loss = -11420.596422952813
Iteration 500: Loss = -11420.050336097896
Iteration 600: Loss = -11417.359458199775
Iteration 700: Loss = -11412.159489682883
Iteration 800: Loss = -11307.505290834064
Iteration 900: Loss = -11138.378366125991
Iteration 1000: Loss = -11130.941394612668
Iteration 1100: Loss = -11130.70801389118
Iteration 1200: Loss = -11129.20172380768
Iteration 1300: Loss = -11129.095759213418
Iteration 1400: Loss = -11128.816705545507
Iteration 1500: Loss = -11128.779746818687
Iteration 1600: Loss = -11127.56979973865
Iteration 1700: Loss = -11127.418753566511
Iteration 1800: Loss = -11127.398759763768
Iteration 1900: Loss = -11127.385525508922
Iteration 2000: Loss = -11127.377397571232
Iteration 2100: Loss = -11127.37192932786
Iteration 2200: Loss = -11127.367225676135
Iteration 2300: Loss = -11127.363012777463
Iteration 2400: Loss = -11127.359249789244
Iteration 2500: Loss = -11127.355664012219
Iteration 2600: Loss = -11127.35227668816
Iteration 2700: Loss = -11127.34899793884
Iteration 2800: Loss = -11127.345786672515
Iteration 2900: Loss = -11127.342733451085
Iteration 3000: Loss = -11127.339697723137
Iteration 3100: Loss = -11127.336525840634
Iteration 3200: Loss = -11127.33369558891
Iteration 3300: Loss = -11127.332068173713
Iteration 3400: Loss = -11127.32811205861
Iteration 3500: Loss = -11127.171881713048
Iteration 3600: Loss = -11127.171927055642
Iteration 3700: Loss = -11127.170374781806
Iteration 3800: Loss = -11127.167145670817
Iteration 3900: Loss = -11127.1664363389
Iteration 4000: Loss = -11127.168282002738
1
Iteration 4100: Loss = -11127.16552962569
Iteration 4200: Loss = -11127.164877660363
Iteration 4300: Loss = -11127.164515874128
Iteration 4400: Loss = -11127.164103257077
Iteration 4500: Loss = -11127.163818470672
Iteration 4600: Loss = -11127.163375054906
Iteration 4700: Loss = -11127.16322057699
Iteration 4800: Loss = -11127.163010031956
Iteration 4900: Loss = -11127.16230450022
Iteration 5000: Loss = -11127.161869499187
Iteration 5100: Loss = -11127.161862488618
Iteration 5200: Loss = -11127.159796565275
Iteration 5300: Loss = -11127.159859179606
Iteration 5400: Loss = -11127.159202553838
Iteration 5500: Loss = -11127.158980680884
Iteration 5600: Loss = -11127.160947649523
1
Iteration 5700: Loss = -11127.15865794186
Iteration 5800: Loss = -11127.15852977846
Iteration 5900: Loss = -11127.158391081866
Iteration 6000: Loss = -11127.158324000588
Iteration 6100: Loss = -11127.158182052828
Iteration 6200: Loss = -11127.158188401614
Iteration 6300: Loss = -11127.158137811537
Iteration 6400: Loss = -11127.157808554111
Iteration 6500: Loss = -11127.15771155949
Iteration 6600: Loss = -11127.15757484495
Iteration 6700: Loss = -11127.158085102486
1
Iteration 6800: Loss = -11127.157312107849
Iteration 6900: Loss = -11127.157028236987
Iteration 7000: Loss = -11127.156836239297
Iteration 7100: Loss = -11127.156726896807
Iteration 7200: Loss = -11127.160012063592
1
Iteration 7300: Loss = -11127.154473250197
Iteration 7400: Loss = -11127.141026369369
Iteration 7500: Loss = -11127.141419743513
1
Iteration 7600: Loss = -11127.240929232436
2
Iteration 7700: Loss = -11127.137590419166
Iteration 7800: Loss = -11127.138079107366
1
Iteration 7900: Loss = -11127.13743944307
Iteration 8000: Loss = -11127.138620367044
1
Iteration 8100: Loss = -11127.1397588351
2
Iteration 8200: Loss = -11127.144833487539
3
Iteration 8300: Loss = -11127.137671771723
4
Iteration 8400: Loss = -11127.138142005633
5
Iteration 8500: Loss = -11127.13705393643
Iteration 8600: Loss = -11127.137246567503
1
Iteration 8700: Loss = -11127.139667764119
2
Iteration 8800: Loss = -11127.136789875054
Iteration 8900: Loss = -11127.137099011734
1
Iteration 9000: Loss = -11127.136862951125
Iteration 9100: Loss = -11127.132379054994
Iteration 9200: Loss = -11127.116995893613
Iteration 9300: Loss = -11127.113809516708
Iteration 9400: Loss = -11127.114126449105
1
Iteration 9500: Loss = -11127.116086269236
2
Iteration 9600: Loss = -11127.114721752618
3
Iteration 9700: Loss = -11127.341266610432
4
Iteration 9800: Loss = -11127.112966090375
Iteration 9900: Loss = -11127.117035741663
1
Iteration 10000: Loss = -11127.11322103705
2
Iteration 10100: Loss = -11127.114335533688
3
Iteration 10200: Loss = -11127.113995410442
4
Iteration 10300: Loss = -11127.110607408782
Iteration 10400: Loss = -11127.110860080764
1
Iteration 10500: Loss = -11127.122990611326
2
Iteration 10600: Loss = -11127.111383811023
3
Iteration 10700: Loss = -11127.110601711089
Iteration 10800: Loss = -11127.112269118705
1
Iteration 10900: Loss = -11127.120221124223
2
Iteration 11000: Loss = -11127.14424961753
3
Iteration 11100: Loss = -11127.151854687236
4
Iteration 11200: Loss = -11127.111777517028
5
Iteration 11300: Loss = -11127.114505499574
6
Iteration 11400: Loss = -11127.110240398586
Iteration 11500: Loss = -11127.109685179485
Iteration 11600: Loss = -11127.109997304873
1
Iteration 11700: Loss = -11127.112586063484
2
Iteration 11800: Loss = -11127.112254017238
3
Iteration 11900: Loss = -11127.23978044922
4
Iteration 12000: Loss = -11127.109185032055
Iteration 12100: Loss = -11127.109322741271
1
Iteration 12200: Loss = -11127.124536282456
2
Iteration 12300: Loss = -11127.199066001747
3
Iteration 12400: Loss = -11127.108807951592
Iteration 12500: Loss = -11127.110504285816
1
Iteration 12600: Loss = -11127.108784684744
Iteration 12700: Loss = -11127.111738082363
1
Iteration 12800: Loss = -11127.246971796976
2
Iteration 12900: Loss = -11127.112030898594
3
Iteration 13000: Loss = -11127.109384580375
4
Iteration 13100: Loss = -11127.108926475392
5
Iteration 13200: Loss = -11127.11264080107
6
Iteration 13300: Loss = -11127.114244535042
7
Iteration 13400: Loss = -11127.108758545764
Iteration 13500: Loss = -11127.11254217471
1
Iteration 13600: Loss = -11127.108709708134
Iteration 13700: Loss = -11127.108802129953
Iteration 13800: Loss = -11127.22710687259
1
Iteration 13900: Loss = -11127.114566271903
2
Iteration 14000: Loss = -11127.1092474525
3
Iteration 14100: Loss = -11127.108936790768
4
Iteration 14200: Loss = -11127.22672577274
5
Iteration 14300: Loss = -11127.108735729624
Iteration 14400: Loss = -11127.316432203108
1
Iteration 14500: Loss = -11127.10873419324
Iteration 14600: Loss = -11127.120446642493
1
Iteration 14700: Loss = -11127.108811641825
Iteration 14800: Loss = -11127.110787984204
1
Iteration 14900: Loss = -11127.10942509536
2
Iteration 15000: Loss = -11127.11174214308
3
Iteration 15100: Loss = -11127.137907399689
4
Iteration 15200: Loss = -11127.157253979907
5
Iteration 15300: Loss = -11127.114396063092
6
Iteration 15400: Loss = -11127.13086439054
7
Iteration 15500: Loss = -11127.282019591861
8
Iteration 15600: Loss = -11127.109005087768
9
Iteration 15700: Loss = -11127.10868810143
Iteration 15800: Loss = -11127.112277677019
1
Iteration 15900: Loss = -11127.112134516461
2
Iteration 16000: Loss = -11127.10845697812
Iteration 16100: Loss = -11127.10882353077
1
Iteration 16200: Loss = -11127.108785186116
2
Iteration 16300: Loss = -11127.108764591012
3
Iteration 16400: Loss = -11127.112999365028
4
Iteration 16500: Loss = -11127.127513630832
5
Iteration 16600: Loss = -11127.108154604748
Iteration 16700: Loss = -11127.15370731688
1
Iteration 16800: Loss = -11127.123942791623
2
Iteration 16900: Loss = -11127.120974456633
3
Iteration 17000: Loss = -11127.25669273672
4
Iteration 17100: Loss = -11127.108322143891
5
Iteration 17200: Loss = -11127.109406736367
6
Iteration 17300: Loss = -11127.109374908643
7
Iteration 17400: Loss = -11127.110286662957
8
Iteration 17500: Loss = -11127.112122204775
9
Iteration 17600: Loss = -11127.10910786597
10
Iteration 17700: Loss = -11127.118791634806
11
Iteration 17800: Loss = -11127.108346031322
12
Iteration 17900: Loss = -11127.109666255577
13
Iteration 18000: Loss = -11127.10992657308
14
Iteration 18100: Loss = -11127.108788344201
15
Stopping early at iteration 18100 due to no improvement.
pi: tensor([[0.7687, 0.2313],
        [0.2290, 0.7710]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4527, 0.5473], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2019, 0.0969],
         [0.5420, 0.2880]],

        [[0.6258, 0.0950],
         [0.6616, 0.6911]],

        [[0.6702, 0.1029],
         [0.5002, 0.5734]],

        [[0.7259, 0.0958],
         [0.6612, 0.6123]],

        [[0.6335, 0.1044],
         [0.6001, 0.6934]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291543100011382
Average Adjusted Rand Index: 0.9289668011508706
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20994.10668895231
Iteration 100: Loss = -11420.591849362143
Iteration 200: Loss = -11419.38422487866
Iteration 300: Loss = -11419.197120617891
Iteration 400: Loss = -11419.083451128587
Iteration 500: Loss = -11418.979025004786
Iteration 600: Loss = -11418.885401036276
Iteration 700: Loss = -11418.705982106972
Iteration 800: Loss = -11418.471307159678
Iteration 900: Loss = -11418.367003133117
Iteration 1000: Loss = -11418.322029545458
Iteration 1100: Loss = -11418.301858060215
Iteration 1200: Loss = -11418.291606257046
Iteration 1300: Loss = -11418.285597443162
Iteration 1400: Loss = -11418.281787572027
Iteration 1500: Loss = -11418.279198645087
Iteration 1600: Loss = -11418.27733128357
Iteration 1700: Loss = -11418.275950802488
Iteration 1800: Loss = -11418.274913533862
Iteration 1900: Loss = -11418.274086516318
Iteration 2000: Loss = -11418.273417309123
Iteration 2100: Loss = -11418.272929730241
Iteration 2200: Loss = -11418.272455457845
Iteration 2300: Loss = -11418.27209930601
Iteration 2400: Loss = -11418.271793224212
Iteration 2500: Loss = -11418.271525686057
Iteration 2600: Loss = -11418.271319219577
Iteration 2700: Loss = -11418.27111541634
Iteration 2800: Loss = -11418.270931147345
Iteration 2900: Loss = -11418.270806967244
Iteration 3000: Loss = -11418.270662549498
Iteration 3100: Loss = -11418.270555787354
Iteration 3200: Loss = -11418.27047028698
Iteration 3300: Loss = -11418.270336849717
Iteration 3400: Loss = -11418.27027268851
Iteration 3500: Loss = -11418.270191595178
Iteration 3600: Loss = -11418.270113830851
Iteration 3700: Loss = -11418.270061469408
Iteration 3800: Loss = -11418.269971662205
Iteration 3900: Loss = -11418.269905830199
Iteration 4000: Loss = -11418.269867889208
Iteration 4100: Loss = -11418.269856339866
Iteration 4200: Loss = -11418.269752079796
Iteration 4300: Loss = -11418.269767825139
Iteration 4400: Loss = -11418.26971256205
Iteration 4500: Loss = -11418.269698624572
Iteration 4600: Loss = -11418.269631873116
Iteration 4700: Loss = -11418.269605186159
Iteration 4800: Loss = -11418.26959073858
Iteration 4900: Loss = -11418.269534483728
Iteration 5000: Loss = -11418.269554987864
Iteration 5100: Loss = -11418.269527772214
Iteration 5200: Loss = -11418.269531589083
Iteration 5300: Loss = -11418.269495952121
Iteration 5400: Loss = -11418.269473347937
Iteration 5500: Loss = -11418.269596885086
1
Iteration 5600: Loss = -11418.269430422206
Iteration 5700: Loss = -11418.269431159106
Iteration 5800: Loss = -11418.269394083456
Iteration 5900: Loss = -11418.269396066347
Iteration 6000: Loss = -11418.269392319275
Iteration 6100: Loss = -11418.269526790942
1
Iteration 6200: Loss = -11418.269341592846
Iteration 6300: Loss = -11418.269431375555
Iteration 6400: Loss = -11418.269348290878
Iteration 6500: Loss = -11418.269544398076
1
Iteration 6600: Loss = -11418.26931284883
Iteration 6700: Loss = -11418.269408355409
Iteration 6800: Loss = -11418.269302589793
Iteration 6900: Loss = -11418.269321553831
Iteration 7000: Loss = -11418.269360695305
Iteration 7100: Loss = -11418.26978682519
1
Iteration 7200: Loss = -11418.269357695252
Iteration 7300: Loss = -11418.269335431522
Iteration 7400: Loss = -11418.269389280673
Iteration 7500: Loss = -11418.27134513164
1
Iteration 7600: Loss = -11418.317223998687
2
Iteration 7700: Loss = -11418.270155277009
3
Iteration 7800: Loss = -11418.269309175736
Iteration 7900: Loss = -11418.27061761964
1
Iteration 8000: Loss = -11418.269291063481
Iteration 8100: Loss = -11418.269281151048
Iteration 8200: Loss = -11418.269238928464
Iteration 8300: Loss = -11418.269359178008
1
Iteration 8400: Loss = -11418.269281199815
Iteration 8500: Loss = -11418.273972901281
1
Iteration 8600: Loss = -11418.26922532959
Iteration 8700: Loss = -11418.26924519496
Iteration 8800: Loss = -11418.269303711115
Iteration 8900: Loss = -11418.269276966366
Iteration 9000: Loss = -11418.26923648503
Iteration 9100: Loss = -11418.26928389834
Iteration 9200: Loss = -11418.269271030123
Iteration 9300: Loss = -11418.26920365932
Iteration 9400: Loss = -11418.269446640985
1
Iteration 9500: Loss = -11418.269240563197
Iteration 9600: Loss = -11418.269262935817
Iteration 9700: Loss = -11418.269334082932
Iteration 9800: Loss = -11418.269231508919
Iteration 9900: Loss = -11418.26978172869
1
Iteration 10000: Loss = -11418.270417780093
2
Iteration 10100: Loss = -11418.27202314308
3
Iteration 10200: Loss = -11418.295045108878
4
Iteration 10300: Loss = -11418.269247658809
Iteration 10400: Loss = -11418.269385544325
1
Iteration 10500: Loss = -11418.270689180325
2
Iteration 10600: Loss = -11418.269262298576
Iteration 10700: Loss = -11418.269220284896
Iteration 10800: Loss = -11418.269232556007
Iteration 10900: Loss = -11418.269235753205
Iteration 11000: Loss = -11418.44483739072
1
Iteration 11100: Loss = -11418.26924216276
Iteration 11200: Loss = -11418.269225115553
Iteration 11300: Loss = -11418.30943153633
1
Iteration 11400: Loss = -11418.269191768371
Iteration 11500: Loss = -11418.269226915258
Iteration 11600: Loss = -11418.26957158103
1
Iteration 11700: Loss = -11418.269235046097
Iteration 11800: Loss = -11418.353307853662
1
Iteration 11900: Loss = -11418.269240584465
Iteration 12000: Loss = -11418.317937890708
1
Iteration 12100: Loss = -11418.269894160672
2
Iteration 12200: Loss = -11418.269351520663
3
Iteration 12300: Loss = -11418.269228101934
Iteration 12400: Loss = -11418.373317441961
1
Iteration 12500: Loss = -11418.269244649708
Iteration 12600: Loss = -11418.271233021904
1
Iteration 12700: Loss = -11418.271716725985
2
Iteration 12800: Loss = -11418.28889508824
3
Iteration 12900: Loss = -11418.269408078548
4
Iteration 13000: Loss = -11418.27417915225
5
Iteration 13100: Loss = -11418.319984796444
6
Iteration 13200: Loss = -11418.389477098634
7
Iteration 13300: Loss = -11418.290067916136
8
Iteration 13400: Loss = -11418.269618865437
9
Iteration 13500: Loss = -11418.27452638137
10
Iteration 13600: Loss = -11418.271758117175
11
Iteration 13700: Loss = -11418.307653931675
12
Iteration 13800: Loss = -11418.274398941297
13
Iteration 13900: Loss = -11418.26922216882
Iteration 14000: Loss = -11418.333687197426
1
Iteration 14100: Loss = -11418.269204282402
Iteration 14200: Loss = -11418.269418840788
1
Iteration 14300: Loss = -11418.2692383799
Iteration 14400: Loss = -11418.269238458728
Iteration 14500: Loss = -11418.269826219926
1
Iteration 14600: Loss = -11418.269237730063
Iteration 14700: Loss = -11418.271607074827
1
Iteration 14800: Loss = -11418.28236690764
2
Iteration 14900: Loss = -11418.307840753943
3
Iteration 15000: Loss = -11418.273824912932
4
Iteration 15100: Loss = -11418.29163788918
5
Iteration 15200: Loss = -11418.300182814262
6
Iteration 15300: Loss = -11418.269433592717
7
Iteration 15400: Loss = -11418.269614725397
8
Iteration 15500: Loss = -11418.291897500241
9
Iteration 15600: Loss = -11418.269263000604
Iteration 15700: Loss = -11418.269450319287
1
Iteration 15800: Loss = -11418.275237501388
2
Iteration 15900: Loss = -11418.273504170056
3
Iteration 16000: Loss = -11418.278970238594
4
Iteration 16100: Loss = -11418.2711011294
5
Iteration 16200: Loss = -11418.269243074503
Iteration 16300: Loss = -11418.26966176366
1
Iteration 16400: Loss = -11418.310536341349
2
Iteration 16500: Loss = -11418.270232306586
3
Iteration 16600: Loss = -11418.27428680814
4
Iteration 16700: Loss = -11418.270513776713
5
Iteration 16800: Loss = -11418.269563987615
6
Iteration 16900: Loss = -11418.26971700137
7
Iteration 17000: Loss = -11418.271088052757
8
Iteration 17100: Loss = -11418.269209408198
Iteration 17200: Loss = -11418.269650733768
1
Iteration 17300: Loss = -11418.269315055855
2
Iteration 17400: Loss = -11418.269346486064
3
Iteration 17500: Loss = -11418.278652642148
4
Iteration 17600: Loss = -11418.269281039631
Iteration 17700: Loss = -11418.269264229204
Iteration 17800: Loss = -11418.33054571455
1
Iteration 17900: Loss = -11418.269195228811
Iteration 18000: Loss = -11418.269287036279
Iteration 18100: Loss = -11418.269535952753
1
Iteration 18200: Loss = -11418.269507760719
2
Iteration 18300: Loss = -11418.269456952901
3
Iteration 18400: Loss = -11418.486412862634
4
Iteration 18500: Loss = -11418.2692191585
Iteration 18600: Loss = -11418.27433417656
1
Iteration 18700: Loss = -11418.272342007758
2
Iteration 18800: Loss = -11418.284934860481
3
Iteration 18900: Loss = -11418.272018349164
4
Iteration 19000: Loss = -11418.277655224943
5
Iteration 19100: Loss = -11418.26922583102
Iteration 19200: Loss = -11418.280652229412
1
Iteration 19300: Loss = -11418.269406644911
2
Iteration 19400: Loss = -11418.269258039809
Iteration 19500: Loss = -11418.314958131608
1
Iteration 19600: Loss = -11418.269256568825
Iteration 19700: Loss = -11418.270734937178
1
Iteration 19800: Loss = -11418.269227189157
Iteration 19900: Loss = -11418.278464548352
1
pi: tensor([[0.9837, 0.0163],
        [0.9868, 0.0132]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0420, 0.9580], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1724, 0.2616],
         [0.6404, 0.1654]],

        [[0.6176, 0.1138],
         [0.6676, 0.6229]],

        [[0.5333, 0.2555],
         [0.5404, 0.6493]],

        [[0.7186, 0.2661],
         [0.6977, 0.5454]],

        [[0.5470, 0.1954],
         [0.6640, 0.7127]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00185992198747466
Average Adjusted Rand Index: -0.0021499162589125485
11154.634579662514
[0.9291543100011382, -0.00185992198747466] [0.9289668011508706, -0.0021499162589125485] [11127.108788344201, 11418.269450547627]
-------------------------------------
This iteration is 94
True Objective function: Loss = -11383.230180168353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22684.665796054225
Iteration 100: Loss = -11647.17239162164
Iteration 200: Loss = -11642.481791321132
Iteration 300: Loss = -11638.427942557184
Iteration 400: Loss = -11635.463438050463
Iteration 500: Loss = -11626.612269667767
Iteration 600: Loss = -11498.236887454748
Iteration 700: Loss = -11462.459324689042
Iteration 800: Loss = -11452.63247556904
Iteration 900: Loss = -11452.262909830284
Iteration 1000: Loss = -11452.084606309088
Iteration 1100: Loss = -11447.564066912399
Iteration 1200: Loss = -11447.517360254678
Iteration 1300: Loss = -11447.113496698592
Iteration 1400: Loss = -11447.083064690072
Iteration 1500: Loss = -11446.316462982033
Iteration 1600: Loss = -11445.888825593818
Iteration 1700: Loss = -11445.83592924872
Iteration 1800: Loss = -11445.82400912228
Iteration 1900: Loss = -11445.813365315003
Iteration 2000: Loss = -11445.789798402546
Iteration 2100: Loss = -11445.606436911805
Iteration 2200: Loss = -11445.599778016312
Iteration 2300: Loss = -11445.593394307278
Iteration 2400: Loss = -11445.589061426263
Iteration 2500: Loss = -11445.58263506907
Iteration 2600: Loss = -11445.578932199182
Iteration 2700: Loss = -11443.45946753447
Iteration 2800: Loss = -11443.4361157993
Iteration 2900: Loss = -11443.432813093857
Iteration 3000: Loss = -11443.428389812134
Iteration 3100: Loss = -11442.924355863155
Iteration 3200: Loss = -11442.85223643411
Iteration 3300: Loss = -11442.848666905587
Iteration 3400: Loss = -11442.84795451618
Iteration 3500: Loss = -11442.84347535273
Iteration 3600: Loss = -11442.839699232396
Iteration 3700: Loss = -11442.837979533026
Iteration 3800: Loss = -11442.834985410249
Iteration 3900: Loss = -11442.833733528025
Iteration 4000: Loss = -11442.83280016339
Iteration 4100: Loss = -11442.831230402246
Iteration 4200: Loss = -11442.751042560765
Iteration 4300: Loss = -11442.680780491804
Iteration 4400: Loss = -11442.680200762734
Iteration 4500: Loss = -11442.679580366686
Iteration 4600: Loss = -11442.679098640552
Iteration 4700: Loss = -11442.682607691124
1
Iteration 4800: Loss = -11442.677894470664
Iteration 4900: Loss = -11442.676833508938
Iteration 5000: Loss = -11442.675380149256
Iteration 5100: Loss = -11442.669728279565
Iteration 5200: Loss = -11442.633714188343
Iteration 5300: Loss = -11442.639602088955
1
Iteration 5400: Loss = -11442.63098624824
Iteration 5500: Loss = -11442.630474305632
Iteration 5600: Loss = -11442.629738796877
Iteration 5700: Loss = -11442.621093362979
Iteration 5800: Loss = -11442.609842668882
Iteration 5900: Loss = -11442.591124862893
Iteration 6000: Loss = -11442.59198231495
1
Iteration 6100: Loss = -11442.586748061554
Iteration 6200: Loss = -11442.542221514968
Iteration 6300: Loss = -11442.538852451527
Iteration 6400: Loss = -11442.538580248238
Iteration 6500: Loss = -11442.538412767979
Iteration 6600: Loss = -11442.538033434603
Iteration 6700: Loss = -11442.553330218321
1
Iteration 6800: Loss = -11442.537056031668
Iteration 6900: Loss = -11442.533926946933
Iteration 7000: Loss = -11442.530803688933
Iteration 7100: Loss = -11442.527521402837
Iteration 7200: Loss = -11442.527155321008
Iteration 7300: Loss = -11442.518558191394
Iteration 7400: Loss = -11442.510847847065
Iteration 7500: Loss = -11442.5056391392
Iteration 7600: Loss = -11442.503869614193
Iteration 7700: Loss = -11442.503193904031
Iteration 7800: Loss = -11442.503124774412
Iteration 7900: Loss = -11442.5239863842
1
Iteration 8000: Loss = -11442.502980590212
Iteration 8100: Loss = -11442.50294957461
Iteration 8200: Loss = -11442.503340467365
1
Iteration 8300: Loss = -11442.501389351386
Iteration 8400: Loss = -11442.488062496754
Iteration 8500: Loss = -11442.487599986936
Iteration 8600: Loss = -11442.490129197695
1
Iteration 8700: Loss = -11442.486456615725
Iteration 8800: Loss = -11442.486245151893
Iteration 8900: Loss = -11442.486140339188
Iteration 9000: Loss = -11442.483815151914
Iteration 9100: Loss = -11442.483245214336
Iteration 9200: Loss = -11442.483010366239
Iteration 9300: Loss = -11442.481884222576
Iteration 9400: Loss = -11442.48949219296
1
Iteration 9500: Loss = -11442.481480657503
Iteration 9600: Loss = -11442.481367983495
Iteration 9700: Loss = -11442.481332185424
Iteration 9800: Loss = -11442.47967233575
Iteration 9900: Loss = -11442.4783115264
Iteration 10000: Loss = -11442.478373883152
Iteration 10100: Loss = -11442.477214012211
Iteration 10200: Loss = -11442.476705788262
Iteration 10300: Loss = -11442.4770627252
1
Iteration 10400: Loss = -11442.476497711043
Iteration 10500: Loss = -11442.476363668135
Iteration 10600: Loss = -11442.475973408398
Iteration 10700: Loss = -11442.475681698355
Iteration 10800: Loss = -11442.544340278966
1
Iteration 10900: Loss = -11442.475492894166
Iteration 11000: Loss = -11442.475313233565
Iteration 11100: Loss = -11442.475284989729
Iteration 11200: Loss = -11442.47517468331
Iteration 11300: Loss = -11442.477484874911
1
Iteration 11400: Loss = -11442.473657893435
Iteration 11500: Loss = -11442.479850650641
1
Iteration 11600: Loss = -11442.473600075573
Iteration 11700: Loss = -11442.472183180664
Iteration 11800: Loss = -11442.472420472088
1
Iteration 11900: Loss = -11442.47168328735
Iteration 12000: Loss = -11442.481336089055
1
Iteration 12100: Loss = -11442.471667403324
Iteration 12200: Loss = -11442.543433979732
1
Iteration 12300: Loss = -11442.471497113836
Iteration 12400: Loss = -11442.511087971348
1
Iteration 12500: Loss = -11442.47146645842
Iteration 12600: Loss = -11442.475269166327
1
Iteration 12700: Loss = -11442.471367734352
Iteration 12800: Loss = -11442.488291976342
1
Iteration 12900: Loss = -11442.459586485273
Iteration 13000: Loss = -11442.466330230038
1
Iteration 13100: Loss = -11442.459450050455
Iteration 13200: Loss = -11442.525277319772
1
Iteration 13300: Loss = -11442.459429456334
Iteration 13400: Loss = -11442.459349948456
Iteration 13500: Loss = -11442.459391168399
Iteration 13600: Loss = -11442.459355669906
Iteration 13700: Loss = -11442.470304222812
1
Iteration 13800: Loss = -11442.459316420065
Iteration 13900: Loss = -11442.477041946406
1
Iteration 14000: Loss = -11442.458955825588
Iteration 14100: Loss = -11442.458888412404
Iteration 14200: Loss = -11442.459403770834
1
Iteration 14300: Loss = -11442.458925910461
Iteration 14400: Loss = -11442.503879407388
1
Iteration 14500: Loss = -11442.458907712034
Iteration 14600: Loss = -11442.470966474544
1
Iteration 14700: Loss = -11442.45889652812
Iteration 14800: Loss = -11442.458829184325
Iteration 14900: Loss = -11442.459140195519
1
Iteration 15000: Loss = -11442.458808532007
Iteration 15100: Loss = -11442.47105950931
1
Iteration 15200: Loss = -11442.4588361148
Iteration 15300: Loss = -11442.76415225333
1
Iteration 15400: Loss = -11442.458824845922
Iteration 15500: Loss = -11442.782245189608
1
Iteration 15600: Loss = -11442.458639642336
Iteration 15700: Loss = -11442.465849994347
1
Iteration 15800: Loss = -11442.458662651772
Iteration 15900: Loss = -11442.473634702128
1
Iteration 16000: Loss = -11442.455497855866
Iteration 16100: Loss = -11442.67129584964
1
Iteration 16200: Loss = -11442.455510832338
Iteration 16300: Loss = -11442.711634790985
1
Iteration 16400: Loss = -11442.455525363292
Iteration 16500: Loss = -11442.455577995734
Iteration 16600: Loss = -11442.455535226954
Iteration 16700: Loss = -11442.456742317414
1
Iteration 16800: Loss = -11442.455539784309
Iteration 16900: Loss = -11442.455862638813
1
Iteration 17000: Loss = -11442.455503084786
Iteration 17100: Loss = -11442.458856907548
1
Iteration 17200: Loss = -11442.45525760107
Iteration 17300: Loss = -11442.455857616143
1
Iteration 17400: Loss = -11442.455253699054
Iteration 17500: Loss = -11442.455343032285
Iteration 17600: Loss = -11442.455148163097
Iteration 17700: Loss = -11442.455129645521
Iteration 17800: Loss = -11442.455261926147
1
Iteration 17900: Loss = -11442.45508148418
Iteration 18000: Loss = -11442.457766043855
1
Iteration 18100: Loss = -11442.45510676652
Iteration 18200: Loss = -11442.458620101783
1
Iteration 18300: Loss = -11442.455086980632
Iteration 18400: Loss = -11442.460165649185
1
Iteration 18500: Loss = -11442.45510332383
Iteration 18600: Loss = -11442.4654523111
1
Iteration 18700: Loss = -11442.455074661786
Iteration 18800: Loss = -11442.458976395443
1
Iteration 18900: Loss = -11442.455015174643
Iteration 19000: Loss = -11442.456697542157
1
Iteration 19100: Loss = -11442.455004604093
Iteration 19200: Loss = -11442.455184028484
1
Iteration 19300: Loss = -11442.455001614273
Iteration 19400: Loss = -11442.456080484884
1
Iteration 19500: Loss = -11442.454989170183
Iteration 19600: Loss = -11442.456865306216
1
Iteration 19700: Loss = -11442.454995546714
Iteration 19800: Loss = -11442.45948688887
1
Iteration 19900: Loss = -11442.454982383992
pi: tensor([[0.6359, 0.3641],
        [0.3046, 0.6954]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9385, 0.0615], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1795, 0.2789],
         [0.5509, 0.3101]],

        [[0.5856, 0.1177],
         [0.7183, 0.5781]],

        [[0.6127, 0.1023],
         [0.5520, 0.7071]],

        [[0.6701, 0.1057],
         [0.5335, 0.6603]],

        [[0.7199, 0.1119],
         [0.6770, 0.5876]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.01333633701284073
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7026262626262626
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
Global Adjusted Rand Index: 0.5466776687496071
Average Adjusted Rand Index: 0.690826394096025
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23653.132848337325
Iteration 100: Loss = -11647.189583194384
Iteration 200: Loss = -11642.749490945553
Iteration 300: Loss = -11636.46980029885
Iteration 400: Loss = -11567.914157004887
Iteration 500: Loss = -11537.547790094623
Iteration 600: Loss = -11358.04115453321
Iteration 700: Loss = -11356.467257716398
Iteration 800: Loss = -11356.189904039784
Iteration 900: Loss = -11355.797441478426
Iteration 1000: Loss = -11354.7277264137
Iteration 1100: Loss = -11354.699632379004
Iteration 1200: Loss = -11354.677992004632
Iteration 1300: Loss = -11354.647706842532
Iteration 1400: Loss = -11354.633603898761
Iteration 1500: Loss = -11354.619770559731
Iteration 1600: Loss = -11354.611788975242
Iteration 1700: Loss = -11354.605384028266
Iteration 1800: Loss = -11354.59880063383
Iteration 1900: Loss = -11354.594017015384
Iteration 2000: Loss = -11354.590974454253
Iteration 2100: Loss = -11354.588297562159
Iteration 2200: Loss = -11354.585781646796
Iteration 2300: Loss = -11354.582776763482
Iteration 2400: Loss = -11354.57875215132
Iteration 2500: Loss = -11354.57672011953
Iteration 2600: Loss = -11354.575037845963
Iteration 2700: Loss = -11354.572692538526
Iteration 2800: Loss = -11354.571257310898
Iteration 2900: Loss = -11354.570410785454
Iteration 3000: Loss = -11354.570005263231
Iteration 3100: Loss = -11354.568897006284
Iteration 3200: Loss = -11354.568266246268
Iteration 3300: Loss = -11354.57828027976
1
Iteration 3400: Loss = -11354.566967762523
Iteration 3500: Loss = -11354.566269404866
Iteration 3600: Loss = -11354.569983801095
1
Iteration 3700: Loss = -11354.56450512783
Iteration 3800: Loss = -11354.563805973992
Iteration 3900: Loss = -11354.56346832536
Iteration 4000: Loss = -11354.562722845547
Iteration 4100: Loss = -11354.56216357715
Iteration 4200: Loss = -11354.561661408265
Iteration 4300: Loss = -11354.560702425968
Iteration 4400: Loss = -11354.559499606918
Iteration 4500: Loss = -11354.55832701265
Iteration 4600: Loss = -11354.550608350994
Iteration 4700: Loss = -11354.550117472969
Iteration 4800: Loss = -11354.549463406398
Iteration 4900: Loss = -11354.548737389
Iteration 5000: Loss = -11354.548199346087
Iteration 5100: Loss = -11354.551108990454
1
Iteration 5200: Loss = -11354.547850345136
Iteration 5300: Loss = -11354.547746434531
Iteration 5400: Loss = -11354.547666105693
Iteration 5500: Loss = -11354.548602872148
1
Iteration 5600: Loss = -11354.548335631449
2
Iteration 5700: Loss = -11354.54728808202
Iteration 5800: Loss = -11354.557014509608
1
Iteration 5900: Loss = -11354.547046136919
Iteration 6000: Loss = -11354.5466248342
Iteration 6100: Loss = -11354.546187299382
Iteration 6200: Loss = -11354.546052239752
Iteration 6300: Loss = -11354.546011020095
Iteration 6400: Loss = -11354.542188343703
Iteration 6500: Loss = -11354.493245834328
Iteration 6600: Loss = -11354.518633482907
1
Iteration 6700: Loss = -11354.493136912492
Iteration 6800: Loss = -11354.493138402919
Iteration 6900: Loss = -11354.49317369312
Iteration 7000: Loss = -11354.493048085042
Iteration 7100: Loss = -11354.50444685438
1
Iteration 7200: Loss = -11354.492903152151
Iteration 7300: Loss = -11354.492853636795
Iteration 7400: Loss = -11354.492662719385
Iteration 7500: Loss = -11354.492479876304
Iteration 7600: Loss = -11354.502261994872
1
Iteration 7700: Loss = -11354.493072592217
2
Iteration 7800: Loss = -11354.496942170677
3
Iteration 7900: Loss = -11354.491159134966
Iteration 8000: Loss = -11354.495729850747
1
Iteration 8100: Loss = -11354.49398389733
2
Iteration 8200: Loss = -11354.492102212464
3
Iteration 8300: Loss = -11354.49089729758
Iteration 8400: Loss = -11354.490860332608
Iteration 8500: Loss = -11354.513895274862
1
Iteration 8600: Loss = -11354.490778477088
Iteration 8700: Loss = -11354.491946268194
1
Iteration 8800: Loss = -11354.490660501808
Iteration 8900: Loss = -11354.494159022704
1
Iteration 9000: Loss = -11354.484733457442
Iteration 9100: Loss = -11354.484806351742
Iteration 9200: Loss = -11354.484733783964
Iteration 9300: Loss = -11354.484671809656
Iteration 9400: Loss = -11354.507122777117
1
Iteration 9500: Loss = -11354.484696815694
Iteration 9600: Loss = -11354.484656557535
Iteration 9700: Loss = -11354.484989314209
1
Iteration 9800: Loss = -11354.484665649947
Iteration 9900: Loss = -11354.485817392606
1
Iteration 10000: Loss = -11354.484661162433
Iteration 10100: Loss = -11354.48463999898
Iteration 10200: Loss = -11354.48714387778
1
Iteration 10300: Loss = -11354.484251622227
Iteration 10400: Loss = -11354.607429148462
1
Iteration 10500: Loss = -11354.484187536027
Iteration 10600: Loss = -11354.727977272392
1
Iteration 10700: Loss = -11354.484106791475
Iteration 10800: Loss = -11354.483836738971
Iteration 10900: Loss = -11354.484021703902
1
Iteration 11000: Loss = -11354.4837921063
Iteration 11100: Loss = -11354.496828696001
1
Iteration 11200: Loss = -11354.483633284019
Iteration 11300: Loss = -11354.483616182088
Iteration 11400: Loss = -11354.483906142576
1
Iteration 11500: Loss = -11354.483611924254
Iteration 11600: Loss = -11354.562015690628
1
Iteration 11700: Loss = -11354.483626945987
Iteration 11800: Loss = -11354.483612148648
Iteration 11900: Loss = -11354.48780038027
1
Iteration 12000: Loss = -11354.483624408502
Iteration 12100: Loss = -11354.483595314317
Iteration 12200: Loss = -11354.483632323132
Iteration 12300: Loss = -11354.483590712574
Iteration 12400: Loss = -11354.64186784094
1
Iteration 12500: Loss = -11354.483589926025
Iteration 12600: Loss = -11354.483579196612
Iteration 12700: Loss = -11354.485440980123
1
Iteration 12800: Loss = -11354.4815956652
Iteration 12900: Loss = -11354.481599000006
Iteration 13000: Loss = -11354.481925809829
1
Iteration 13100: Loss = -11354.481594816469
Iteration 13200: Loss = -11354.48155861215
Iteration 13300: Loss = -11354.495122016842
1
Iteration 13400: Loss = -11354.481443013598
Iteration 13500: Loss = -11354.481441178676
Iteration 13600: Loss = -11354.974491196494
1
Iteration 13700: Loss = -11354.481042582627
Iteration 13800: Loss = -11354.481053493379
Iteration 13900: Loss = -11354.484933885631
1
Iteration 14000: Loss = -11354.481065712715
Iteration 14100: Loss = -11354.496398227258
1
Iteration 14200: Loss = -11354.481092219245
Iteration 14300: Loss = -11354.48103437568
Iteration 14400: Loss = -11354.491111076532
1
Iteration 14500: Loss = -11354.481009425122
Iteration 14600: Loss = -11354.481032994401
Iteration 14700: Loss = -11354.481489372876
1
Iteration 14800: Loss = -11354.48104790856
Iteration 14900: Loss = -11354.507672546637
1
Iteration 15000: Loss = -11354.479930171363
Iteration 15100: Loss = -11354.479892014737
Iteration 15200: Loss = -11354.734277726437
1
Iteration 15300: Loss = -11354.479886692658
Iteration 15400: Loss = -11354.47937358037
Iteration 15500: Loss = -11354.74196075229
1
Iteration 15600: Loss = -11354.47936237136
Iteration 15700: Loss = -11354.479388615457
Iteration 15800: Loss = -11354.479793932922
1
Iteration 15900: Loss = -11354.479411816008
Iteration 16000: Loss = -11354.479354994066
Iteration 16100: Loss = -11354.617501841736
1
Iteration 16200: Loss = -11354.479327766034
Iteration 16300: Loss = -11354.479339422915
Iteration 16400: Loss = -11354.498459620874
1
Iteration 16500: Loss = -11354.479335609962
Iteration 16600: Loss = -11354.479336045366
Iteration 16700: Loss = -11354.505531033154
1
Iteration 16800: Loss = -11354.47934664542
Iteration 16900: Loss = -11354.479343002384
Iteration 17000: Loss = -11354.50474768555
1
Iteration 17100: Loss = -11354.479114341568
Iteration 17200: Loss = -11354.479104265218
Iteration 17300: Loss = -11354.522403034314
1
Iteration 17400: Loss = -11354.479126646775
Iteration 17500: Loss = -11354.47911258317
Iteration 17600: Loss = -11354.480502304268
1
Iteration 17700: Loss = -11354.479121104312
Iteration 17800: Loss = -11354.479115971686
Iteration 17900: Loss = -11354.47926928511
1
Iteration 18000: Loss = -11354.4774979918
Iteration 18100: Loss = -11354.485812670944
1
Iteration 18200: Loss = -11354.477390752118
Iteration 18300: Loss = -11354.477410320418
Iteration 18400: Loss = -11354.65362742953
1
Iteration 18500: Loss = -11354.477377532648
Iteration 18600: Loss = -11354.477392534247
Iteration 18700: Loss = -11354.501535937516
1
Iteration 18800: Loss = -11354.477399624
Iteration 18900: Loss = -11354.477397781242
Iteration 19000: Loss = -11354.478421061882
1
Iteration 19100: Loss = -11354.477378023937
Iteration 19200: Loss = -11354.477256886068
Iteration 19300: Loss = -11354.477365404524
1
Iteration 19400: Loss = -11354.477229266875
Iteration 19500: Loss = -11354.665261946988
1
Iteration 19600: Loss = -11354.477230624034
Iteration 19700: Loss = -11354.477239916057
Iteration 19800: Loss = -11354.507726983114
1
Iteration 19900: Loss = -11354.477244224996
pi: tensor([[0.7350, 0.2650],
        [0.2699, 0.7301]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4470, 0.5530], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1961, 0.0963],
         [0.6906, 0.3013]],

        [[0.5465, 0.1105],
         [0.5684, 0.7088]],

        [[0.7186, 0.1013],
         [0.7219, 0.6636]],

        [[0.6388, 0.1045],
         [0.5262, 0.6812]],

        [[0.7052, 0.1115],
         [0.5431, 0.6749]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.9291543137886643
Average Adjusted Rand Index: 0.9289538718000039
11383.230180168353
[0.5466776687496071, 0.9291543137886643] [0.690826394096025, 0.9289538718000039] [11442.492228901041, 11354.477229504304]
-------------------------------------
This iteration is 95
True Objective function: Loss = -11196.908685867535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23080.592071530973
Iteration 100: Loss = -11404.399427198603
Iteration 200: Loss = -11399.084043169474
Iteration 300: Loss = -11397.195184724213
Iteration 400: Loss = -11396.568812122458
Iteration 500: Loss = -11396.298935746563
Iteration 600: Loss = -11396.152327077813
Iteration 700: Loss = -11396.0287716117
Iteration 800: Loss = -11395.9248795235
Iteration 900: Loss = -11395.834971120259
Iteration 1000: Loss = -11395.758934056506
Iteration 1100: Loss = -11395.69782507385
Iteration 1200: Loss = -11395.64860336671
Iteration 1300: Loss = -11395.607956057416
Iteration 1400: Loss = -11395.573599626552
Iteration 1500: Loss = -11395.542840399741
Iteration 1600: Loss = -11395.50671130042
Iteration 1700: Loss = -11374.242974941939
Iteration 1800: Loss = -11368.828808354065
Iteration 1900: Loss = -11368.782058110477
Iteration 2000: Loss = -11368.761366967969
Iteration 2100: Loss = -11368.72960535863
Iteration 2200: Loss = -11368.724760307983
Iteration 2300: Loss = -11368.721230581466
Iteration 2400: Loss = -11368.71834748881
Iteration 2500: Loss = -11368.71597496285
Iteration 2600: Loss = -11368.713963027705
Iteration 2700: Loss = -11368.712451573329
Iteration 2800: Loss = -11368.711410393498
Iteration 2900: Loss = -11368.710496855287
Iteration 3000: Loss = -11368.709767339375
Iteration 3100: Loss = -11368.70908278467
Iteration 3200: Loss = -11368.708552956294
Iteration 3300: Loss = -11368.708045029756
Iteration 3400: Loss = -11368.707580751785
Iteration 3500: Loss = -11368.707216311943
Iteration 3600: Loss = -11368.706833968217
Iteration 3700: Loss = -11368.706492600482
Iteration 3800: Loss = -11368.706224845937
Iteration 3900: Loss = -11368.705946207425
Iteration 4000: Loss = -11368.70571024856
Iteration 4100: Loss = -11368.705461879825
Iteration 4200: Loss = -11368.705293905907
Iteration 4300: Loss = -11368.705079649506
Iteration 4400: Loss = -11368.704939576059
Iteration 4500: Loss = -11368.704779461466
Iteration 4600: Loss = -11368.704648840383
Iteration 4700: Loss = -11368.70449049612
Iteration 4800: Loss = -11368.70441548725
Iteration 4900: Loss = -11368.70424760015
Iteration 5000: Loss = -11368.70414590274
Iteration 5100: Loss = -11368.704077138758
Iteration 5200: Loss = -11368.70394721485
Iteration 5300: Loss = -11368.703837016217
Iteration 5400: Loss = -11368.703774936666
Iteration 5500: Loss = -11368.703689313623
Iteration 5600: Loss = -11368.703632156066
Iteration 5700: Loss = -11368.70353788489
Iteration 5800: Loss = -11368.703981704746
1
Iteration 5900: Loss = -11368.70340691392
Iteration 6000: Loss = -11368.703389894219
Iteration 6100: Loss = -11368.704650946602
1
Iteration 6200: Loss = -11368.703344478205
Iteration 6300: Loss = -11368.703232086007
Iteration 6400: Loss = -11368.703372655189
1
Iteration 6500: Loss = -11368.703143103561
Iteration 6600: Loss = -11368.703085046795
Iteration 6700: Loss = -11368.703061651702
Iteration 6800: Loss = -11368.703033225855
Iteration 6900: Loss = -11368.704832705138
1
Iteration 7000: Loss = -11368.70299571013
Iteration 7100: Loss = -11368.706511936429
1
Iteration 7200: Loss = -11368.710545495862
2
Iteration 7300: Loss = -11368.704208918109
3
Iteration 7400: Loss = -11368.70352139217
4
Iteration 7500: Loss = -11368.702855385918
Iteration 7600: Loss = -11368.710679899077
1
Iteration 7700: Loss = -11368.70728533544
2
Iteration 7800: Loss = -11368.703650743297
3
Iteration 7900: Loss = -11368.702814030945
Iteration 8000: Loss = -11368.702950555828
1
Iteration 8100: Loss = -11368.73160185832
2
Iteration 8200: Loss = -11368.702678728125
Iteration 8300: Loss = -11368.702920276257
1
Iteration 8400: Loss = -11368.702654308008
Iteration 8500: Loss = -11368.702662090443
Iteration 8600: Loss = -11368.702665662611
Iteration 8700: Loss = -11368.70262623146
Iteration 8800: Loss = -11369.152840757431
1
Iteration 8900: Loss = -11368.70262527916
Iteration 9000: Loss = -11368.702545466813
Iteration 9100: Loss = -11368.72459297892
1
Iteration 9200: Loss = -11368.702580855059
Iteration 9300: Loss = -11368.702567439612
Iteration 9400: Loss = -11368.733612963488
1
Iteration 9500: Loss = -11368.7024723014
Iteration 9600: Loss = -11368.702536721321
Iteration 9700: Loss = -11368.83776007643
1
Iteration 9800: Loss = -11368.702508330236
Iteration 9900: Loss = -11368.702496881413
Iteration 10000: Loss = -11368.999533531132
1
Iteration 10100: Loss = -11368.702478039091
Iteration 10200: Loss = -11368.703401901095
1
Iteration 10300: Loss = -11368.70247126772
Iteration 10400: Loss = -11368.707947646151
1
Iteration 10500: Loss = -11368.702458175367
Iteration 10600: Loss = -11368.70283374737
1
Iteration 10700: Loss = -11368.70524815591
2
Iteration 10800: Loss = -11368.702453381033
Iteration 10900: Loss = -11368.702586053227
1
Iteration 11000: Loss = -11368.702621651277
2
Iteration 11100: Loss = -11368.702484499996
Iteration 11200: Loss = -11368.702441211106
Iteration 11300: Loss = -11368.737116120199
1
Iteration 11400: Loss = -11368.702451866931
Iteration 11500: Loss = -11368.702415101023
Iteration 11600: Loss = -11368.706604316814
1
Iteration 11700: Loss = -11368.702444526672
Iteration 11800: Loss = -11368.70243834729
Iteration 11900: Loss = -11368.703136816008
1
Iteration 12000: Loss = -11368.702394433038
Iteration 12100: Loss = -11368.702390010383
Iteration 12200: Loss = -11368.703167646483
1
Iteration 12300: Loss = -11368.702421784727
Iteration 12400: Loss = -11368.702389551934
Iteration 12500: Loss = -11368.702801337527
1
Iteration 12600: Loss = -11368.70283840768
2
Iteration 12700: Loss = -11368.702500049232
3
Iteration 12800: Loss = -11368.703836487513
4
Iteration 12900: Loss = -11368.713102767948
5
Iteration 13000: Loss = -11368.702361369895
Iteration 13100: Loss = -11368.702789051405
1
Iteration 13200: Loss = -11368.702667745856
2
Iteration 13300: Loss = -11368.702505365321
3
Iteration 13400: Loss = -11368.70250675585
4
Iteration 13500: Loss = -11368.702493646819
5
Iteration 13600: Loss = -11368.70339347239
6
Iteration 13700: Loss = -11368.705509965559
7
Iteration 13800: Loss = -11368.709346737698
8
Iteration 13900: Loss = -11368.702505255602
9
Iteration 14000: Loss = -11368.70503898288
10
Iteration 14100: Loss = -11368.771735776521
11
Iteration 14200: Loss = -11368.702354415022
Iteration 14300: Loss = -11368.704314373252
1
Iteration 14400: Loss = -11368.702414019028
Iteration 14500: Loss = -11368.702495416377
Iteration 14600: Loss = -11368.775194744303
1
Iteration 14700: Loss = -11368.763979966638
2
Iteration 14800: Loss = -11368.70640408622
3
Iteration 14900: Loss = -11368.769629504579
4
Iteration 15000: Loss = -11368.70689201713
5
Iteration 15100: Loss = -11368.715469136036
6
Iteration 15200: Loss = -11368.726325327536
7
Iteration 15300: Loss = -11368.702406811235
Iteration 15400: Loss = -11368.738915798403
1
Iteration 15500: Loss = -11368.702382704374
Iteration 15600: Loss = -11368.70289678184
1
Iteration 15700: Loss = -11368.7023267267
Iteration 15800: Loss = -11368.702454224242
1
Iteration 15900: Loss = -11368.702379058932
Iteration 16000: Loss = -11368.702514116181
1
Iteration 16100: Loss = -11368.704139470914
2
Iteration 16200: Loss = -11368.702341300037
Iteration 16300: Loss = -11368.702349071198
Iteration 16400: Loss = -11368.702529727942
1
Iteration 16500: Loss = -11368.715924034039
2
Iteration 16600: Loss = -11368.70231779374
Iteration 16700: Loss = -11368.745211232661
1
Iteration 16800: Loss = -11368.702333077526
Iteration 16900: Loss = -11368.703604243812
1
Iteration 17000: Loss = -11368.739775246202
2
Iteration 17100: Loss = -11368.70242120946
Iteration 17200: Loss = -11368.702375735429
Iteration 17300: Loss = -11368.906345804773
1
Iteration 17400: Loss = -11368.702729815195
2
Iteration 17500: Loss = -11368.703729133556
3
Iteration 17600: Loss = -11368.706588173196
4
Iteration 17700: Loss = -11368.703760694125
5
Iteration 17800: Loss = -11368.702430400466
Iteration 17900: Loss = -11368.703242965285
1
Iteration 18000: Loss = -11368.76191339268
2
Iteration 18100: Loss = -11368.7023459286
Iteration 18200: Loss = -11368.704809826344
1
Iteration 18300: Loss = -11368.702330572518
Iteration 18400: Loss = -11368.70492569637
1
Iteration 18500: Loss = -11368.70242620528
Iteration 18600: Loss = -11368.820599420143
1
Iteration 18700: Loss = -11368.702328290527
Iteration 18800: Loss = -11368.706305340871
1
Iteration 18900: Loss = -11368.706057266
2
Iteration 19000: Loss = -11368.845121578823
3
Iteration 19100: Loss = -11368.70233984482
Iteration 19200: Loss = -11368.704920390075
1
Iteration 19300: Loss = -11368.70412077556
2
Iteration 19400: Loss = -11368.702393818447
Iteration 19500: Loss = -11368.702564535122
1
Iteration 19600: Loss = -11368.706901333622
2
Iteration 19700: Loss = -11368.711676060213
3
Iteration 19800: Loss = -11368.702539017868
4
Iteration 19900: Loss = -11368.702377442234
pi: tensor([[9.7055e-01, 2.9453e-02],
        [1.0000e+00, 4.4596e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6112, 0.3888], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1732, 0.1221],
         [0.5754, 0.3433]],

        [[0.5027, 0.1436],
         [0.6057, 0.7140]],

        [[0.5245, 0.2080],
         [0.6550, 0.6648]],

        [[0.7090, 0.0483],
         [0.5525, 0.6594]],

        [[0.6996, 0.2432],
         [0.6161, 0.5392]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 84
Adjusted Rand Index: 0.4570646234120315
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.034730514477578335
Average Adjusted Rand Index: 0.08971595498543661
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21101.499374354975
Iteration 100: Loss = -11405.474517508623
Iteration 200: Loss = -11404.962952926897
Iteration 300: Loss = -11404.140691224742
Iteration 400: Loss = -11398.064457683478
Iteration 500: Loss = -11396.415059360155
Iteration 600: Loss = -11396.054506437602
Iteration 700: Loss = -11395.833239526111
Iteration 800: Loss = -11395.682524936707
Iteration 900: Loss = -11395.582366359287
Iteration 1000: Loss = -11395.492833367269
Iteration 1100: Loss = -11369.180747857905
Iteration 1200: Loss = -11368.859918497175
Iteration 1300: Loss = -11368.809417613986
Iteration 1400: Loss = -11368.767771606243
Iteration 1500: Loss = -11368.744454274969
Iteration 1600: Loss = -11368.734872687055
Iteration 1700: Loss = -11368.72718469847
Iteration 1800: Loss = -11368.72268449403
Iteration 1900: Loss = -11368.71974153249
Iteration 2000: Loss = -11368.717515398537
Iteration 2100: Loss = -11368.715701773312
Iteration 2200: Loss = -11368.714251500141
Iteration 2300: Loss = -11368.712983504429
Iteration 2400: Loss = -11368.711956893188
Iteration 2500: Loss = -11368.711048835734
Iteration 2600: Loss = -11368.710273559282
Iteration 2700: Loss = -11368.709599647802
Iteration 2800: Loss = -11368.708987631537
Iteration 2900: Loss = -11368.708429005008
Iteration 3000: Loss = -11368.707952369556
Iteration 3100: Loss = -11368.707551387748
Iteration 3200: Loss = -11368.707167422359
Iteration 3300: Loss = -11368.706781473444
Iteration 3400: Loss = -11368.70646520874
Iteration 3500: Loss = -11368.70620310635
Iteration 3600: Loss = -11368.705954935507
Iteration 3700: Loss = -11368.705721246542
Iteration 3800: Loss = -11368.705537263695
Iteration 3900: Loss = -11368.705287818335
Iteration 4000: Loss = -11368.705118905471
Iteration 4100: Loss = -11368.704955563891
Iteration 4200: Loss = -11368.704824022561
Iteration 4300: Loss = -11368.704642561765
Iteration 4400: Loss = -11368.70452010092
Iteration 4500: Loss = -11368.704387981421
Iteration 4600: Loss = -11368.704285993928
Iteration 4700: Loss = -11368.710113951121
1
Iteration 4800: Loss = -11368.7040208489
Iteration 4900: Loss = -11368.711164611308
1
Iteration 5000: Loss = -11368.704847299987
2
Iteration 5100: Loss = -11368.707005563256
3
Iteration 5200: Loss = -11368.70600715399
4
Iteration 5300: Loss = -11368.704948598239
5
Iteration 5400: Loss = -11368.703520235842
Iteration 5500: Loss = -11368.703482488865
Iteration 5600: Loss = -11368.703471997283
Iteration 5700: Loss = -11368.70545843891
1
Iteration 5800: Loss = -11368.703279403098
Iteration 5900: Loss = -11368.70327796719
Iteration 6000: Loss = -11368.703332234003
Iteration 6100: Loss = -11368.703137650129
Iteration 6200: Loss = -11368.704441892876
1
Iteration 6300: Loss = -11368.70395716237
2
Iteration 6400: Loss = -11368.703056447592
Iteration 6500: Loss = -11368.703020859655
Iteration 6600: Loss = -11368.707634536962
1
Iteration 6700: Loss = -11368.702927258337
Iteration 6800: Loss = -11368.70359367422
1
Iteration 6900: Loss = -11368.7082817481
2
Iteration 7000: Loss = -11368.70605156956
3
Iteration 7100: Loss = -11368.702835543414
Iteration 7200: Loss = -11368.702814073942
Iteration 7300: Loss = -11368.702972745405
1
Iteration 7400: Loss = -11368.703560916158
2
Iteration 7500: Loss = -11368.702770023101
Iteration 7600: Loss = -11368.702715553936
Iteration 7700: Loss = -11368.702686609802
Iteration 7800: Loss = -11368.702679749245
Iteration 7900: Loss = -11368.703865337391
1
Iteration 8000: Loss = -11368.702640877922
Iteration 8100: Loss = -11368.707523819174
1
Iteration 8200: Loss = -11368.7030899966
2
Iteration 8300: Loss = -11368.702659727207
Iteration 8400: Loss = -11368.703143325043
1
Iteration 8500: Loss = -11368.929989426551
2
Iteration 8600: Loss = -11368.702563858998
Iteration 8700: Loss = -11368.705434182963
1
Iteration 8800: Loss = -11368.702552575915
Iteration 8900: Loss = -11368.702547334044
Iteration 9000: Loss = -11368.703155913161
1
Iteration 9100: Loss = -11368.702478834783
Iteration 9200: Loss = -11368.711126939104
1
Iteration 9300: Loss = -11368.702490227892
Iteration 9400: Loss = -11368.702491227996
Iteration 9500: Loss = -11368.955924835745
1
Iteration 9600: Loss = -11368.702495342417
Iteration 9700: Loss = -11368.70244983401
Iteration 9800: Loss = -11369.189067996209
1
Iteration 9900: Loss = -11368.702468426785
Iteration 10000: Loss = -11368.702465943943
Iteration 10100: Loss = -11369.026766298135
1
Iteration 10200: Loss = -11368.702456148292
Iteration 10300: Loss = -11368.702426892698
Iteration 10400: Loss = -11368.720165442242
1
Iteration 10500: Loss = -11368.702412137343
Iteration 10600: Loss = -11368.702384961256
Iteration 10700: Loss = -11368.704384689256
1
Iteration 10800: Loss = -11368.70242626767
Iteration 10900: Loss = -11368.702395970231
Iteration 11000: Loss = -11368.70322529339
1
Iteration 11100: Loss = -11368.702405098235
Iteration 11200: Loss = -11368.73424906316
1
Iteration 11300: Loss = -11368.702406286484
Iteration 11400: Loss = -11368.702433476456
Iteration 11500: Loss = -11368.702421869757
Iteration 11600: Loss = -11368.704536162706
1
Iteration 11700: Loss = -11368.70239190235
Iteration 11800: Loss = -11368.702579530527
1
Iteration 11900: Loss = -11368.705353965746
2
Iteration 12000: Loss = -11368.70933205356
3
Iteration 12100: Loss = -11368.749787954084
4
Iteration 12200: Loss = -11368.729867947031
5
Iteration 12300: Loss = -11368.721785135438
6
Iteration 12400: Loss = -11368.702356364554
Iteration 12500: Loss = -11368.702585924813
1
Iteration 12600: Loss = -11368.702376780202
Iteration 12700: Loss = -11368.70245705366
Iteration 12800: Loss = -11368.702369476674
Iteration 12900: Loss = -11368.722435534986
1
Iteration 13000: Loss = -11368.704180408517
2
Iteration 13100: Loss = -11368.702514471732
3
Iteration 13200: Loss = -11368.702451853376
Iteration 13300: Loss = -11368.72471582848
1
Iteration 13400: Loss = -11368.702360775424
Iteration 13500: Loss = -11368.702517744363
1
Iteration 13600: Loss = -11368.917475747612
2
Iteration 13700: Loss = -11368.702366596694
Iteration 13800: Loss = -11368.718605186628
1
Iteration 13900: Loss = -11368.707698430744
2
Iteration 14000: Loss = -11368.70418448013
3
Iteration 14100: Loss = -11368.702353369094
Iteration 14200: Loss = -11368.7086400146
1
Iteration 14300: Loss = -11368.702303707882
Iteration 14400: Loss = -11368.723784960324
1
Iteration 14500: Loss = -11368.756825058093
2
Iteration 14600: Loss = -11368.702355848502
Iteration 14700: Loss = -11368.702338593417
Iteration 14800: Loss = -11368.704111671621
1
Iteration 14900: Loss = -11368.747161153758
2
Iteration 15000: Loss = -11368.706179204079
3
Iteration 15100: Loss = -11368.705513028255
4
Iteration 15200: Loss = -11368.713230977955
5
Iteration 15300: Loss = -11368.702439132774
6
Iteration 15400: Loss = -11368.702878856991
7
Iteration 15500: Loss = -11368.787752990584
8
Iteration 15600: Loss = -11368.702371133531
Iteration 15700: Loss = -11368.70292155537
1
Iteration 15800: Loss = -11368.70540200422
2
Iteration 15900: Loss = -11368.718468509544
3
Iteration 16000: Loss = -11368.749746419402
4
Iteration 16100: Loss = -11368.70235527817
Iteration 16200: Loss = -11368.961622202398
1
Iteration 16300: Loss = -11368.702345868518
Iteration 16400: Loss = -11368.71207316232
1
Iteration 16500: Loss = -11368.702608269201
2
Iteration 16600: Loss = -11368.704386862166
3
Iteration 16700: Loss = -11368.709601285384
4
Iteration 16800: Loss = -11368.716103083425
5
Iteration 16900: Loss = -11368.703060563432
6
Iteration 17000: Loss = -11368.704156497324
7
Iteration 17100: Loss = -11368.719816300545
8
Iteration 17200: Loss = -11368.70233905573
Iteration 17300: Loss = -11368.703362079788
1
Iteration 17400: Loss = -11368.900973461637
2
Iteration 17500: Loss = -11368.70234722071
Iteration 17600: Loss = -11368.735082526986
1
Iteration 17700: Loss = -11368.702326857105
Iteration 17800: Loss = -11368.702782085415
1
Iteration 17900: Loss = -11368.702395927718
Iteration 18000: Loss = -11368.702357897555
Iteration 18100: Loss = -11368.703866721044
1
Iteration 18200: Loss = -11368.703989773647
2
Iteration 18300: Loss = -11368.703194591417
3
Iteration 18400: Loss = -11368.702369926019
Iteration 18500: Loss = -11368.721032973934
1
Iteration 18600: Loss = -11368.702391232306
Iteration 18700: Loss = -11368.702310502911
Iteration 18800: Loss = -11368.717214990644
1
Iteration 18900: Loss = -11368.706316704593
2
Iteration 19000: Loss = -11368.702361857097
Iteration 19100: Loss = -11368.706160027608
1
Iteration 19200: Loss = -11368.702378748922
Iteration 19300: Loss = -11368.769326354899
1
Iteration 19400: Loss = -11368.702368980023
Iteration 19500: Loss = -11368.70265866595
1
Iteration 19600: Loss = -11368.70322572014
2
Iteration 19700: Loss = -11368.843461940054
3
Iteration 19800: Loss = -11368.702334387823
Iteration 19900: Loss = -11368.961518186989
1
pi: tensor([[4.0703e-08, 1.0000e+00],
        [2.9444e-02, 9.7056e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3897, 0.6103], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3428, 0.1219],
         [0.6978, 0.1733]],

        [[0.6993, 0.1435],
         [0.6398, 0.5291]],

        [[0.6994, 0.2077],
         [0.6617, 0.5998]],

        [[0.5348, 0.0484],
         [0.5492, 0.5211]],

        [[0.5018, 0.2432],
         [0.6315, 0.6696]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 16
Adjusted Rand Index: 0.4570646234120315
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.034730514477578335
Average Adjusted Rand Index: 0.08971595498543661
11196.908685867535
[0.034730514477578335, 0.034730514477578335] [0.08971595498543661, 0.08971595498543661] [11368.708175170166, 11368.702343535926]
-------------------------------------
This iteration is 96
True Objective function: Loss = -11275.81335217151
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20164.630175795417
Iteration 100: Loss = -11549.886003868534
Iteration 200: Loss = -11549.2204987084
Iteration 300: Loss = -11547.231355910404
Iteration 400: Loss = -11546.005177427092
Iteration 500: Loss = -11544.90567317934
Iteration 600: Loss = -11538.652642802002
Iteration 700: Loss = -11364.743251382799
Iteration 800: Loss = -11360.39391761669
Iteration 900: Loss = -11360.159454538963
Iteration 1000: Loss = -11359.908711348833
Iteration 1100: Loss = -11357.697503923318
Iteration 1200: Loss = -11356.057189163397
Iteration 1300: Loss = -11355.930710915121
Iteration 1400: Loss = -11355.147891645642
Iteration 1500: Loss = -11353.095618458687
Iteration 1600: Loss = -11352.977944299624
Iteration 1700: Loss = -11352.968835863005
Iteration 1800: Loss = -11352.963077370528
Iteration 1900: Loss = -11352.958769135179
Iteration 2000: Loss = -11352.955130346
Iteration 2100: Loss = -11352.951702165568
Iteration 2200: Loss = -11352.947667548773
Iteration 2300: Loss = -11352.944176692805
Iteration 2400: Loss = -11352.941882073927
Iteration 2500: Loss = -11352.940865564491
Iteration 2600: Loss = -11352.93978491718
Iteration 2700: Loss = -11352.938615553358
Iteration 2800: Loss = -11352.938072551982
Iteration 2900: Loss = -11352.936829795151
Iteration 3000: Loss = -11352.93592638121
Iteration 3100: Loss = -11352.941067469066
1
Iteration 3200: Loss = -11352.931606305108
Iteration 3300: Loss = -11352.930398205304
Iteration 3400: Loss = -11352.929738913876
Iteration 3500: Loss = -11352.92926153162
Iteration 3600: Loss = -11352.932076850684
1
Iteration 3700: Loss = -11352.928413170339
Iteration 3800: Loss = -11352.927933203435
Iteration 3900: Loss = -11352.924556607428
Iteration 4000: Loss = -11352.92352914058
Iteration 4100: Loss = -11352.923719759134
1
Iteration 4200: Loss = -11352.923117915134
Iteration 4300: Loss = -11352.922932459765
Iteration 4400: Loss = -11352.922788787902
Iteration 4500: Loss = -11352.922643201151
Iteration 4600: Loss = -11352.922527636027
Iteration 4700: Loss = -11352.922487793925
Iteration 4800: Loss = -11352.92226701233
Iteration 4900: Loss = -11352.922102655364
Iteration 5000: Loss = -11352.922000218805
Iteration 5100: Loss = -11352.926548244646
1
Iteration 5200: Loss = -11352.921781493169
Iteration 5300: Loss = -11352.921673272689
Iteration 5400: Loss = -11352.921647706264
Iteration 5500: Loss = -11352.921554187984
Iteration 5600: Loss = -11352.922101465083
1
Iteration 5700: Loss = -11352.921446548748
Iteration 5800: Loss = -11352.921349321505
Iteration 5900: Loss = -11352.921336951511
Iteration 6000: Loss = -11352.921248183635
Iteration 6100: Loss = -11352.92162526403
1
Iteration 6200: Loss = -11352.921320707326
Iteration 6300: Loss = -11352.940821845836
1
Iteration 6400: Loss = -11352.921142264304
Iteration 6500: Loss = -11352.921093991881
Iteration 6600: Loss = -11352.921078193927
Iteration 6700: Loss = -11352.92105016962
Iteration 6800: Loss = -11352.921021282726
Iteration 6900: Loss = -11352.921183735427
1
Iteration 7000: Loss = -11352.94485648043
2
Iteration 7100: Loss = -11352.920968621576
Iteration 7200: Loss = -11352.920961392612
Iteration 7300: Loss = -11352.922560511424
1
Iteration 7400: Loss = -11352.920877940125
Iteration 7500: Loss = -11352.921277698932
1
Iteration 7600: Loss = -11352.920864844855
Iteration 7700: Loss = -11352.920806800348
Iteration 7800: Loss = -11352.920888483237
Iteration 7900: Loss = -11352.920785527864
Iteration 8000: Loss = -11352.921703510812
1
Iteration 8100: Loss = -11352.920749823365
Iteration 8200: Loss = -11352.998304669994
1
Iteration 8300: Loss = -11352.920731199434
Iteration 8400: Loss = -11352.920708566664
Iteration 8500: Loss = -11352.923033541994
1
Iteration 8600: Loss = -11352.920708698244
Iteration 8700: Loss = -11352.92092050712
1
Iteration 8800: Loss = -11352.92131554309
2
Iteration 8900: Loss = -11352.921133843507
3
Iteration 9000: Loss = -11352.921952215844
4
Iteration 9100: Loss = -11352.931616758195
5
Iteration 9200: Loss = -11352.920700011648
Iteration 9300: Loss = -11352.922057541035
1
Iteration 9400: Loss = -11352.922774807352
2
Iteration 9500: Loss = -11352.92998797382
3
Iteration 9600: Loss = -11352.920709569968
Iteration 9700: Loss = -11352.920718847357
Iteration 9800: Loss = -11352.996259835583
1
Iteration 9900: Loss = -11352.931213136992
2
Iteration 10000: Loss = -11352.920648349378
Iteration 10100: Loss = -11352.997285148145
1
Iteration 10200: Loss = -11352.920601735465
Iteration 10300: Loss = -11352.920605100007
Iteration 10400: Loss = -11352.920634674654
Iteration 10500: Loss = -11352.920566316201
Iteration 10600: Loss = -11352.944774918607
1
Iteration 10700: Loss = -11352.92055697887
Iteration 10800: Loss = -11352.920571607727
Iteration 10900: Loss = -11352.921386118865
1
Iteration 11000: Loss = -11352.920562569994
Iteration 11100: Loss = -11352.920873880332
1
Iteration 11200: Loss = -11352.920576122675
Iteration 11300: Loss = -11352.928597575337
1
Iteration 11400: Loss = -11352.92201703551
2
Iteration 11500: Loss = -11352.920480449
Iteration 11600: Loss = -11352.922144375447
1
Iteration 11700: Loss = -11352.920484305967
Iteration 11800: Loss = -11352.920868786008
1
Iteration 11900: Loss = -11352.928928260702
2
Iteration 12000: Loss = -11352.920484518587
Iteration 12100: Loss = -11352.920604415034
1
Iteration 12200: Loss = -11352.938094114821
2
Iteration 12300: Loss = -11352.92045924324
Iteration 12400: Loss = -11353.109656845741
1
Iteration 12500: Loss = -11352.920447761959
Iteration 12600: Loss = -11352.94161842077
1
Iteration 12700: Loss = -11352.920452830309
Iteration 12800: Loss = -11352.926170127837
1
Iteration 12900: Loss = -11352.92095705276
2
Iteration 13000: Loss = -11352.921667956933
3
Iteration 13100: Loss = -11352.921901309055
4
Iteration 13200: Loss = -11352.92095402075
5
Iteration 13300: Loss = -11353.01906926288
6
Iteration 13400: Loss = -11352.923234733087
7
Iteration 13500: Loss = -11352.920497503108
Iteration 13600: Loss = -11352.922845111461
1
Iteration 13700: Loss = -11352.922678645
2
Iteration 13800: Loss = -11352.920491374338
Iteration 13900: Loss = -11352.924537145356
1
Iteration 14000: Loss = -11352.921856139357
2
Iteration 14100: Loss = -11352.920513546269
Iteration 14200: Loss = -11352.962179632657
1
Iteration 14300: Loss = -11352.920435414646
Iteration 14400: Loss = -11353.002864543008
1
Iteration 14500: Loss = -11352.92039907117
Iteration 14600: Loss = -11352.920402576636
Iteration 14700: Loss = -11352.920627612482
1
Iteration 14800: Loss = -11352.952838733156
2
Iteration 14900: Loss = -11352.923833518948
3
Iteration 15000: Loss = -11352.984692923457
4
Iteration 15100: Loss = -11352.920391959402
Iteration 15200: Loss = -11352.924701921589
1
Iteration 15300: Loss = -11352.936242246518
2
Iteration 15400: Loss = -11353.030452932522
3
Iteration 15500: Loss = -11353.00356099728
4
Iteration 15600: Loss = -11353.073778686767
5
Iteration 15700: Loss = -11352.927982265977
6
Iteration 15800: Loss = -11352.920580268912
7
Iteration 15900: Loss = -11352.921180622576
8
Iteration 16000: Loss = -11352.920995239481
9
Iteration 16100: Loss = -11353.001767357015
10
Iteration 16200: Loss = -11352.920357339166
Iteration 16300: Loss = -11352.921570664159
1
Iteration 16400: Loss = -11352.92034923985
Iteration 16500: Loss = -11352.920685851264
1
Iteration 16600: Loss = -11352.92197148306
2
Iteration 16700: Loss = -11352.920508450432
3
Iteration 16800: Loss = -11352.9447302863
4
Iteration 16900: Loss = -11352.920933904188
5
Iteration 17000: Loss = -11352.925485780623
6
Iteration 17100: Loss = -11352.922447658853
7
Iteration 17200: Loss = -11353.17090018218
8
Iteration 17300: Loss = -11352.920246256064
Iteration 17400: Loss = -11352.93174430317
1
Iteration 17500: Loss = -11352.918890724253
Iteration 17600: Loss = -11352.918738121693
Iteration 17700: Loss = -11352.919956085074
1
Iteration 17800: Loss = -11352.920554312817
2
Iteration 17900: Loss = -11352.919947391763
3
Iteration 18000: Loss = -11352.91931155824
4
Iteration 18100: Loss = -11352.919177888101
5
Iteration 18200: Loss = -11352.936399517286
6
Iteration 18300: Loss = -11352.935193943842
7
Iteration 18400: Loss = -11352.920309834182
8
Iteration 18500: Loss = -11352.918752835381
Iteration 18600: Loss = -11352.921205177221
1
Iteration 18700: Loss = -11352.918735189596
Iteration 18800: Loss = -11352.91880825037
Iteration 18900: Loss = -11352.919470227193
1
Iteration 19000: Loss = -11352.91874427037
Iteration 19100: Loss = -11352.941614907746
1
Iteration 19200: Loss = -11352.918713141911
Iteration 19300: Loss = -11352.918940517666
1
Iteration 19400: Loss = -11352.933757756855
2
Iteration 19500: Loss = -11352.918739395882
Iteration 19600: Loss = -11352.919756589536
1
Iteration 19700: Loss = -11352.918681424042
Iteration 19800: Loss = -11352.921214950218
1
Iteration 19900: Loss = -11352.91869374037
pi: tensor([[0.6927, 0.3073],
        [0.3554, 0.6446]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1409, 0.8591], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2998, 0.1076],
         [0.6196, 0.1992]],

        [[0.5679, 0.1131],
         [0.6542, 0.5665]],

        [[0.5202, 0.0997],
         [0.6353, 0.6863]],

        [[0.6560, 0.1078],
         [0.6978, 0.5728]],

        [[0.5564, 0.0991],
         [0.7297, 0.7023]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.026235421038789564
time is 1
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.523247349092223
Average Adjusted Rand Index: 0.7271850829135569
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21247.924694877318
Iteration 100: Loss = -11550.217904485862
Iteration 200: Loss = -11549.49225807726
Iteration 300: Loss = -11549.1469161586
Iteration 400: Loss = -11547.84241474713
Iteration 500: Loss = -11545.878864305903
Iteration 600: Loss = -11544.164356283232
Iteration 700: Loss = -11542.137333097966
Iteration 800: Loss = -11412.17914578619
Iteration 900: Loss = -11366.532466860748
Iteration 1000: Loss = -11361.239792496906
Iteration 1100: Loss = -11360.817994227298
Iteration 1200: Loss = -11360.615543624446
Iteration 1300: Loss = -11360.272042614606
Iteration 1400: Loss = -11360.195324232142
Iteration 1500: Loss = -11360.115090588766
Iteration 1600: Loss = -11359.930826961821
Iteration 1700: Loss = -11358.915996781456
Iteration 1800: Loss = -11357.837583387769
Iteration 1900: Loss = -11356.512518338715
Iteration 2000: Loss = -11356.046246581169
Iteration 2100: Loss = -11356.021596038237
Iteration 2200: Loss = -11356.006803748982
Iteration 2300: Loss = -11355.992835597706
Iteration 2400: Loss = -11355.984298727713
Iteration 2500: Loss = -11355.972584450874
Iteration 2600: Loss = -11355.773486953403
Iteration 2700: Loss = -11353.673057415599
Iteration 2800: Loss = -11353.16865850137
Iteration 2900: Loss = -11353.004368114101
Iteration 3000: Loss = -11352.995807015384
Iteration 3100: Loss = -11352.991437358676
Iteration 3200: Loss = -11352.990136085404
Iteration 3300: Loss = -11352.984450886726
Iteration 3400: Loss = -11352.987539413054
1
Iteration 3500: Loss = -11352.97494622943
Iteration 3600: Loss = -11352.952726474396
Iteration 3700: Loss = -11352.95141957056
Iteration 3800: Loss = -11352.950383042295
Iteration 3900: Loss = -11352.949027347568
Iteration 4000: Loss = -11352.95024690183
1
Iteration 4100: Loss = -11352.946461879077
Iteration 4200: Loss = -11352.945425701557
Iteration 4300: Loss = -11352.944639396264
Iteration 4400: Loss = -11352.944808537135
1
Iteration 4500: Loss = -11352.942584544568
Iteration 4600: Loss = -11352.953175436365
1
Iteration 4700: Loss = -11352.938562589869
Iteration 4800: Loss = -11352.944259676866
1
Iteration 4900: Loss = -11352.937610675628
Iteration 5000: Loss = -11352.937517703467
Iteration 5100: Loss = -11352.93696651737
Iteration 5200: Loss = -11352.93663919837
Iteration 5300: Loss = -11352.936342137926
Iteration 5400: Loss = -11352.93657057285
1
Iteration 5500: Loss = -11352.939490132765
2
Iteration 5600: Loss = -11352.93542782256
Iteration 5700: Loss = -11352.93513411197
Iteration 5800: Loss = -11352.96281229703
1
Iteration 5900: Loss = -11352.937167697219
2
Iteration 6000: Loss = -11352.941971120024
3
Iteration 6100: Loss = -11352.928642396459
Iteration 6200: Loss = -11352.94575696206
1
Iteration 6300: Loss = -11352.927576203203
Iteration 6400: Loss = -11352.928038990902
1
Iteration 6500: Loss = -11352.948398126351
2
Iteration 6600: Loss = -11352.92620580655
Iteration 6700: Loss = -11352.922807657194
Iteration 6800: Loss = -11352.92435581814
1
Iteration 6900: Loss = -11352.922475950494
Iteration 7000: Loss = -11352.922692735641
1
Iteration 7100: Loss = -11352.973200683957
2
Iteration 7200: Loss = -11352.922364677941
Iteration 7300: Loss = -11352.922238813046
Iteration 7400: Loss = -11352.922713166394
1
Iteration 7500: Loss = -11352.938716773155
2
Iteration 7600: Loss = -11352.922542008484
3
Iteration 7700: Loss = -11352.922384342284
4
Iteration 7800: Loss = -11352.925474253592
5
Iteration 7900: Loss = -11352.921708314476
Iteration 8000: Loss = -11352.921752579989
Iteration 8100: Loss = -11352.921798526242
Iteration 8200: Loss = -11352.926327472895
1
Iteration 8300: Loss = -11352.921539787043
Iteration 8400: Loss = -11352.922052842307
1
Iteration 8500: Loss = -11352.921455341608
Iteration 8600: Loss = -11352.921767966189
1
Iteration 8700: Loss = -11352.922144999795
2
Iteration 8800: Loss = -11352.92136897371
Iteration 8900: Loss = -11352.921619265835
1
Iteration 9000: Loss = -11352.921336050615
Iteration 9100: Loss = -11352.922054422892
1
Iteration 9200: Loss = -11352.926602901282
2
Iteration 9300: Loss = -11352.921279982187
Iteration 9400: Loss = -11352.922204901657
1
Iteration 9500: Loss = -11352.921108280785
Iteration 9600: Loss = -11352.93043017132
1
Iteration 9700: Loss = -11352.920899794572
Iteration 9800: Loss = -11352.920831520947
Iteration 9900: Loss = -11352.921806917937
1
Iteration 10000: Loss = -11353.035200992992
2
Iteration 10100: Loss = -11352.922206806988
3
Iteration 10200: Loss = -11352.922404102686
4
Iteration 10300: Loss = -11352.92261360004
5
Iteration 10400: Loss = -11352.921165697915
6
Iteration 10500: Loss = -11352.923540202148
7
Iteration 10600: Loss = -11352.920749716935
Iteration 10700: Loss = -11352.938084746196
1
Iteration 10800: Loss = -11352.920716303051
Iteration 10900: Loss = -11352.921488148508
1
Iteration 11000: Loss = -11352.920742996705
Iteration 11100: Loss = -11352.920730693284
Iteration 11200: Loss = -11352.920764208862
Iteration 11300: Loss = -11352.920653876987
Iteration 11400: Loss = -11353.08230740372
1
Iteration 11500: Loss = -11352.92145371616
2
Iteration 11600: Loss = -11352.920564271835
Iteration 11700: Loss = -11352.922482469967
1
Iteration 11800: Loss = -11352.920526567314
Iteration 11900: Loss = -11352.920608648887
Iteration 12000: Loss = -11352.921617349979
1
Iteration 12100: Loss = -11352.92052543353
Iteration 12200: Loss = -11352.920606082265
Iteration 12300: Loss = -11352.920540316341
Iteration 12400: Loss = -11352.920522174885
Iteration 12500: Loss = -11352.930846801082
1
Iteration 12600: Loss = -11352.920485687551
Iteration 12700: Loss = -11352.920506687784
Iteration 12800: Loss = -11352.920596715736
Iteration 12900: Loss = -11352.920469381095
Iteration 13000: Loss = -11352.921898443548
1
Iteration 13100: Loss = -11352.971393304124
2
Iteration 13200: Loss = -11352.929188192447
3
Iteration 13300: Loss = -11352.957901831063
4
Iteration 13400: Loss = -11352.922522752984
5
Iteration 13500: Loss = -11353.231377804934
6
Iteration 13600: Loss = -11352.92047722379
Iteration 13700: Loss = -11352.951158396942
1
Iteration 13800: Loss = -11352.920462211936
Iteration 13900: Loss = -11352.920620276453
1
Iteration 14000: Loss = -11352.947043109538
2
Iteration 14100: Loss = -11352.920480565544
Iteration 14200: Loss = -11352.920861973127
1
Iteration 14300: Loss = -11352.924394194917
2
Iteration 14400: Loss = -11352.92045887335
Iteration 14500: Loss = -11352.921676505772
1
Iteration 14600: Loss = -11352.920500994896
Iteration 14700: Loss = -11352.92058290693
Iteration 14800: Loss = -11353.011596005137
1
Iteration 14900: Loss = -11352.921022177345
2
Iteration 15000: Loss = -11352.923057828479
3
Iteration 15100: Loss = -11352.920478818087
Iteration 15200: Loss = -11352.920676440592
1
Iteration 15300: Loss = -11353.235768883818
2
Iteration 15400: Loss = -11352.920449305968
Iteration 15500: Loss = -11353.02910652445
1
Iteration 15600: Loss = -11352.920438195282
Iteration 15700: Loss = -11352.920472401358
Iteration 15800: Loss = -11352.920554607315
Iteration 15900: Loss = -11352.920447169534
Iteration 16000: Loss = -11352.921524146006
1
Iteration 16100: Loss = -11352.920518343637
Iteration 16200: Loss = -11352.92044979651
Iteration 16300: Loss = -11352.924039951762
1
Iteration 16400: Loss = -11352.920459017521
Iteration 16500: Loss = -11352.920934929754
1
Iteration 16600: Loss = -11352.920467352564
Iteration 16700: Loss = -11352.922961015329
1
Iteration 16800: Loss = -11352.920473071137
Iteration 16900: Loss = -11352.923957025261
1
Iteration 17000: Loss = -11352.931275208204
2
Iteration 17100: Loss = -11352.963730899863
3
Iteration 17200: Loss = -11352.92047151838
Iteration 17300: Loss = -11352.92267620302
1
Iteration 17400: Loss = -11352.920458222938
Iteration 17500: Loss = -11352.925239780569
1
Iteration 17600: Loss = -11352.920432732712
Iteration 17700: Loss = -11352.921846370673
1
Iteration 17800: Loss = -11352.92047898788
Iteration 17900: Loss = -11352.923183719004
1
Iteration 18000: Loss = -11352.920736972412
2
Iteration 18100: Loss = -11352.968441845593
3
Iteration 18200: Loss = -11352.920460703635
Iteration 18300: Loss = -11352.920668239123
1
Iteration 18400: Loss = -11352.920670551253
2
Iteration 18500: Loss = -11352.928926963083
3
Iteration 18600: Loss = -11352.927022540716
4
Iteration 18700: Loss = -11352.92081551997
5
Iteration 18800: Loss = -11352.922453298961
6
Iteration 18900: Loss = -11352.921487464926
7
Iteration 19000: Loss = -11352.920464060964
Iteration 19100: Loss = -11352.98480447538
1
Iteration 19200: Loss = -11352.920398048202
Iteration 19300: Loss = -11352.920823805307
1
Iteration 19400: Loss = -11352.920423525029
Iteration 19500: Loss = -11352.920400939644
Iteration 19600: Loss = -11352.921934528502
1
Iteration 19700: Loss = -11352.920369219095
Iteration 19800: Loss = -11353.029299943613
1
Iteration 19900: Loss = -11352.920393050339
pi: tensor([[0.6921, 0.3079],
        [0.3572, 0.6428]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1413, 0.8587], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2993, 0.1077],
         [0.6726, 0.1994]],

        [[0.7304, 0.1128],
         [0.6741, 0.7113]],

        [[0.5398, 0.0995],
         [0.5244, 0.5377]],

        [[0.6183, 0.1082],
         [0.5561, 0.5142]],

        [[0.5993, 0.0989],
         [0.5645, 0.5173]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.026235421038789564
time is 1
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.523247349092223
Average Adjusted Rand Index: 0.7271850829135569
11275.81335217151
[0.523247349092223, 0.523247349092223] [0.7271850829135569, 0.7271850829135569] [11352.933940835901, 11352.920610744675]
-------------------------------------
This iteration is 97
True Objective function: Loss = -11156.24391361485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23675.23514124355
Iteration 100: Loss = -11457.233084346994
Iteration 200: Loss = -11456.489901438052
Iteration 300: Loss = -11455.472960633311
Iteration 400: Loss = -11451.252382474006
Iteration 500: Loss = -11450.447269623752
Iteration 600: Loss = -11445.355651759024
Iteration 700: Loss = -11353.886079118916
Iteration 800: Loss = -11345.25916552262
Iteration 900: Loss = -11344.542758243524
Iteration 1000: Loss = -11343.998298472008
Iteration 1100: Loss = -11343.542563343104
Iteration 1200: Loss = -11343.258887827396
Iteration 1300: Loss = -11339.105480705457
Iteration 1400: Loss = -11263.813641492427
Iteration 1500: Loss = -11255.008500257985
Iteration 1600: Loss = -11248.200136127349
Iteration 1700: Loss = -11246.004376925024
Iteration 1800: Loss = -11241.926665838146
Iteration 1900: Loss = -11241.67869025947
Iteration 2000: Loss = -11241.604438928041
Iteration 2100: Loss = -11239.243166950437
Iteration 2200: Loss = -11239.220778355995
Iteration 2300: Loss = -11239.097209444271
Iteration 2400: Loss = -11239.043416648405
Iteration 2500: Loss = -11238.9721639976
Iteration 2600: Loss = -11235.782324064732
Iteration 2700: Loss = -11228.744452183515
Iteration 2800: Loss = -11228.716560759012
Iteration 2900: Loss = -11228.707563381566
Iteration 3000: Loss = -11224.123590156267
Iteration 3100: Loss = -11223.643221855555
Iteration 3200: Loss = -11223.649604384138
1
Iteration 3300: Loss = -11223.640331614375
Iteration 3400: Loss = -11223.638003244045
Iteration 3500: Loss = -11223.636704688115
Iteration 3600: Loss = -11221.036094508607
Iteration 3700: Loss = -11221.018719670683
Iteration 3800: Loss = -11220.86518893171
Iteration 3900: Loss = -11220.79878886553
Iteration 4000: Loss = -11220.800710833806
1
Iteration 4100: Loss = -11220.809666924126
2
Iteration 4200: Loss = -11220.794291403048
Iteration 4300: Loss = -11220.672533782974
Iteration 4400: Loss = -11220.675127267803
1
Iteration 4500: Loss = -11220.670782566769
Iteration 4600: Loss = -11220.670477598045
Iteration 4700: Loss = -11220.669679949633
Iteration 4800: Loss = -11220.669483280333
Iteration 4900: Loss = -11220.669237212122
Iteration 5000: Loss = -11220.669061215785
Iteration 5100: Loss = -11220.668872882416
Iteration 5200: Loss = -11220.66883273745
Iteration 5300: Loss = -11220.668555574248
Iteration 5400: Loss = -11220.668631687495
Iteration 5500: Loss = -11220.668130728263
Iteration 5600: Loss = -11220.670491282968
1
Iteration 5700: Loss = -11220.666899518646
Iteration 5800: Loss = -11220.645776583488
Iteration 5900: Loss = -11220.620864986155
Iteration 6000: Loss = -11220.620338295386
Iteration 6100: Loss = -11220.68532325355
1
Iteration 6200: Loss = -11220.609572676563
Iteration 6300: Loss = -11220.598808327566
Iteration 6400: Loss = -11220.60962583746
1
Iteration 6500: Loss = -11220.598594306035
Iteration 6600: Loss = -11220.598551045825
Iteration 6700: Loss = -11220.598747248052
1
Iteration 6800: Loss = -11220.59842331406
Iteration 6900: Loss = -11220.607142677698
1
Iteration 7000: Loss = -11220.598431517277
Iteration 7100: Loss = -11220.602621827124
1
Iteration 7200: Loss = -11220.598224542982
Iteration 7300: Loss = -11220.59736104918
Iteration 7400: Loss = -11220.595328912386
Iteration 7500: Loss = -11220.602982476295
1
Iteration 7600: Loss = -11220.552791156559
Iteration 7700: Loss = -11220.552748825325
Iteration 7800: Loss = -11220.610351602894
1
Iteration 7900: Loss = -11220.552600271134
Iteration 8000: Loss = -11220.552088610408
Iteration 8100: Loss = -11217.875989429922
Iteration 8200: Loss = -11217.805023577375
Iteration 8300: Loss = -11217.804874409883
Iteration 8400: Loss = -11217.80779740078
1
Iteration 8500: Loss = -11217.804727787618
Iteration 8600: Loss = -11217.804174310802
Iteration 8700: Loss = -11217.928610484629
1
Iteration 8800: Loss = -11217.803894707398
Iteration 8900: Loss = -11217.927845083062
1
Iteration 9000: Loss = -11217.8032308237
Iteration 9100: Loss = -11217.803173438539
Iteration 9200: Loss = -11217.813599467057
1
Iteration 9300: Loss = -11217.801394114786
Iteration 9400: Loss = -11217.801385486982
Iteration 9500: Loss = -11217.80603475562
1
Iteration 9600: Loss = -11217.80133982479
Iteration 9700: Loss = -11217.801304600363
Iteration 9800: Loss = -11217.799074322009
Iteration 9900: Loss = -11217.799115597329
Iteration 10000: Loss = -11217.827909779626
1
Iteration 10100: Loss = -11217.79839952112
Iteration 10200: Loss = -11217.799031766423
1
Iteration 10300: Loss = -11217.79959937074
2
Iteration 10400: Loss = -11217.835671408355
3
Iteration 10500: Loss = -11217.817352108514
4
Iteration 10600: Loss = -11217.798093216785
Iteration 10700: Loss = -11217.798315245675
1
Iteration 10800: Loss = -11217.809310468516
2
Iteration 10900: Loss = -11217.797998911054
Iteration 11000: Loss = -11217.798048997229
Iteration 11100: Loss = -11217.797799048973
Iteration 11200: Loss = -11217.798254858331
1
Iteration 11300: Loss = -11217.797577404353
Iteration 11400: Loss = -11218.130756395765
1
Iteration 11500: Loss = -11217.766129026162
Iteration 11600: Loss = -11217.766450966068
1
Iteration 11700: Loss = -11217.76596790929
Iteration 11800: Loss = -11217.767350485374
1
Iteration 11900: Loss = -11217.770925411323
2
Iteration 12000: Loss = -11217.770259482397
3
Iteration 12100: Loss = -11217.765844816837
Iteration 12200: Loss = -11217.770267184304
1
Iteration 12300: Loss = -11217.853207704757
2
Iteration 12400: Loss = -11217.766804304501
3
Iteration 12500: Loss = -11217.766836168184
4
Iteration 12600: Loss = -11217.773917145547
5
Iteration 12700: Loss = -11217.765666712452
Iteration 12800: Loss = -11217.76617901985
1
Iteration 12900: Loss = -11217.817362886224
2
Iteration 13000: Loss = -11217.775914389571
3
Iteration 13100: Loss = -11217.790317810264
4
Iteration 13200: Loss = -11217.77049123283
5
Iteration 13300: Loss = -11217.76611347138
6
Iteration 13400: Loss = -11217.762798489508
Iteration 13500: Loss = -11217.060254627791
Iteration 13600: Loss = -11217.062429503385
1
Iteration 13700: Loss = -11217.057546266282
Iteration 13800: Loss = -11217.071823487368
1
Iteration 13900: Loss = -11217.057315154027
Iteration 14000: Loss = -11217.06727901366
1
Iteration 14100: Loss = -11217.05307728433
Iteration 14200: Loss = -11217.053981105797
1
Iteration 14300: Loss = -11217.087812714191
2
Iteration 14400: Loss = -11217.045245545309
Iteration 14500: Loss = -11217.031248437816
Iteration 14600: Loss = -11217.032363444017
1
Iteration 14700: Loss = -11217.10189114598
2
Iteration 14800: Loss = -11217.030942474568
Iteration 14900: Loss = -11217.048120186273
1
Iteration 15000: Loss = -11217.074600810902
2
Iteration 15100: Loss = -11217.046720188537
3
Iteration 15200: Loss = -11217.044086734333
4
Iteration 15300: Loss = -11217.03279194695
5
Iteration 15400: Loss = -11217.031057930799
6
Iteration 15500: Loss = -11217.040242761886
7
Iteration 15600: Loss = -11217.151912384468
8
Iteration 15700: Loss = -11217.041856355394
9
Iteration 15800: Loss = -11217.030273932884
Iteration 15900: Loss = -11217.03769441697
1
Iteration 16000: Loss = -11217.030110479416
Iteration 16100: Loss = -11217.030481616515
1
Iteration 16200: Loss = -11217.0331094739
2
Iteration 16300: Loss = -11217.032950711062
3
Iteration 16400: Loss = -11217.165448441663
4
Iteration 16500: Loss = -11217.018862936062
Iteration 16600: Loss = -11217.023636378473
1
Iteration 16700: Loss = -11217.018355213168
Iteration 16800: Loss = -11217.014788662615
Iteration 16900: Loss = -11217.014866313128
Iteration 17000: Loss = -11217.014692444567
Iteration 17100: Loss = -11217.01248596891
Iteration 17200: Loss = -11217.064037481081
1
Iteration 17300: Loss = -11217.02557935346
2
Iteration 17400: Loss = -11217.012486716805
Iteration 17500: Loss = -11217.058581251242
1
Iteration 17600: Loss = -11217.01401582728
2
Iteration 17700: Loss = -11217.012493552804
Iteration 17800: Loss = -11217.012971256558
1
Iteration 17900: Loss = -11217.01233795601
Iteration 18000: Loss = -11217.012903102994
1
Iteration 18100: Loss = -11217.012345529796
Iteration 18200: Loss = -11217.017446663527
1
Iteration 18300: Loss = -11217.013235983417
2
Iteration 18400: Loss = -11217.015210274982
3
Iteration 18500: Loss = -11217.0123105954
Iteration 18600: Loss = -11217.017773200123
1
Iteration 18700: Loss = -11217.011388988749
Iteration 18800: Loss = -11217.013333438186
1
Iteration 18900: Loss = -11217.025334521182
2
Iteration 19000: Loss = -11217.011392049142
Iteration 19100: Loss = -11217.024565639023
1
Iteration 19200: Loss = -11217.011946924571
2
Iteration 19300: Loss = -11217.074614310268
3
Iteration 19400: Loss = -11217.028811032334
4
Iteration 19500: Loss = -11217.021224680071
5
Iteration 19600: Loss = -11217.00764324254
Iteration 19700: Loss = -11217.009051219964
1
Iteration 19800: Loss = -11217.11072002677
2
Iteration 19900: Loss = -11217.007695766635
pi: tensor([[0.6196, 0.3804],
        [0.3345, 0.6655]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4643, 0.5357], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2471, 0.0974],
         [0.6957, 0.2565]],

        [[0.6622, 0.0940],
         [0.7166, 0.6644]],

        [[0.7145, 0.1008],
         [0.6014, 0.5189]],

        [[0.6381, 0.1017],
         [0.5512, 0.5368]],

        [[0.6795, 0.0879],
         [0.5670, 0.6994]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080682750429576
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
Global Adjusted Rand Index: 0.029036977800507038
Average Adjusted Rand Index: 0.8605104018199026
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21554.502415231585
Iteration 100: Loss = -11214.074944534821
Iteration 200: Loss = -11208.526361635679
Iteration 300: Loss = -11208.382495192523
Iteration 400: Loss = -11208.280803985483
Iteration 500: Loss = -11207.724600383639
Iteration 600: Loss = -11207.704437863995
Iteration 700: Loss = -11207.693567335617
Iteration 800: Loss = -11207.686525187217
Iteration 900: Loss = -11207.681588141353
Iteration 1000: Loss = -11207.67804384942
Iteration 1100: Loss = -11207.675377975293
Iteration 1200: Loss = -11207.673313644353
Iteration 1300: Loss = -11207.67162517923
Iteration 1400: Loss = -11207.670334394255
Iteration 1500: Loss = -11207.66922120599
Iteration 1600: Loss = -11207.66834220313
Iteration 1700: Loss = -11207.667587152413
Iteration 1800: Loss = -11207.666874077357
Iteration 1900: Loss = -11207.666290367766
Iteration 2000: Loss = -11207.66573594474
Iteration 2100: Loss = -11207.666824178898
1
Iteration 2200: Loss = -11207.66364777285
Iteration 2300: Loss = -11207.66260802445
Iteration 2400: Loss = -11207.66255335234
Iteration 2500: Loss = -11207.66200343023
Iteration 2600: Loss = -11207.66466439978
1
Iteration 2700: Loss = -11207.662637043406
2
Iteration 2800: Loss = -11207.661560400087
Iteration 2900: Loss = -11207.66222471249
1
Iteration 3000: Loss = -11207.673679981044
2
Iteration 3100: Loss = -11207.66100338611
Iteration 3200: Loss = -11207.66157153944
1
Iteration 3300: Loss = -11207.661508769448
2
Iteration 3400: Loss = -11207.664628870903
3
Iteration 3500: Loss = -11207.660406248942
Iteration 3600: Loss = -11207.661217635163
1
Iteration 3700: Loss = -11207.661876149965
2
Iteration 3800: Loss = -11207.664307923933
3
Iteration 3900: Loss = -11207.660375839085
Iteration 4000: Loss = -11207.659989914424
Iteration 4100: Loss = -11207.66014211737
1
Iteration 4200: Loss = -11207.66037048339
2
Iteration 4300: Loss = -11207.659831769035
Iteration 4400: Loss = -11207.659618958363
Iteration 4500: Loss = -11207.66121197797
1
Iteration 4600: Loss = -11207.659810521342
2
Iteration 4700: Loss = -11207.660656318641
3
Iteration 4800: Loss = -11207.659461919658
Iteration 4900: Loss = -11207.671413170478
1
Iteration 5000: Loss = -11207.659380489707
Iteration 5100: Loss = -11207.65938800035
Iteration 5200: Loss = -11207.659905793962
1
Iteration 5300: Loss = -11207.669822167103
2
Iteration 5400: Loss = -11207.66012454515
3
Iteration 5500: Loss = -11207.661139722806
4
Iteration 5600: Loss = -11207.659855850154
5
Iteration 5700: Loss = -11207.659294328581
Iteration 5800: Loss = -11207.661307458073
1
Iteration 5900: Loss = -11207.659565319596
2
Iteration 6000: Loss = -11207.66138988619
3
Iteration 6100: Loss = -11207.659632529938
4
Iteration 6200: Loss = -11207.661218579631
5
Iteration 6300: Loss = -11207.662121201502
6
Iteration 6400: Loss = -11207.661286548117
7
Iteration 6500: Loss = -11207.659817672537
8
Iteration 6600: Loss = -11207.659158455092
Iteration 6700: Loss = -11207.659101100522
Iteration 6800: Loss = -11207.659170689083
Iteration 6900: Loss = -11207.659188865011
Iteration 7000: Loss = -11207.659661736923
1
Iteration 7100: Loss = -11207.662355638726
2
Iteration 7200: Loss = -11207.659240752073
Iteration 7300: Loss = -11207.659240169274
Iteration 7400: Loss = -11207.659481602923
1
Iteration 7500: Loss = -11207.659569711792
2
Iteration 7600: Loss = -11207.659014484627
Iteration 7700: Loss = -11207.659923183806
1
Iteration 7800: Loss = -11207.659217643486
2
Iteration 7900: Loss = -11207.660741149406
3
Iteration 8000: Loss = -11207.66856679273
4
Iteration 8100: Loss = -11207.661543523056
5
Iteration 8200: Loss = -11207.65902873779
Iteration 8300: Loss = -11207.65900968241
Iteration 8400: Loss = -11207.665469159554
1
Iteration 8500: Loss = -11207.65899063706
Iteration 8600: Loss = -11207.658996785218
Iteration 8700: Loss = -11207.66051037416
1
Iteration 8800: Loss = -11207.65898428375
Iteration 8900: Loss = -11207.662163195793
1
Iteration 9000: Loss = -11207.658986532628
Iteration 9100: Loss = -11207.658991492184
Iteration 9200: Loss = -11207.65909083429
Iteration 9300: Loss = -11207.658964785722
Iteration 9400: Loss = -11207.659696506815
1
Iteration 9500: Loss = -11207.658941032856
Iteration 9600: Loss = -11207.659599556062
1
Iteration 9700: Loss = -11207.65915770261
2
Iteration 9800: Loss = -11207.670171937805
3
Iteration 9900: Loss = -11207.659174329172
4
Iteration 10000: Loss = -11207.66063274203
5
Iteration 10100: Loss = -11207.658922703798
Iteration 10200: Loss = -11207.659152465152
1
Iteration 10300: Loss = -11207.658949561595
Iteration 10400: Loss = -11207.659486549195
1
Iteration 10500: Loss = -11207.658934228795
Iteration 10600: Loss = -11207.66116241445
1
Iteration 10700: Loss = -11207.659498574512
2
Iteration 10800: Loss = -11207.659180027791
3
Iteration 10900: Loss = -11207.659208118941
4
Iteration 11000: Loss = -11207.659376186162
5
Iteration 11100: Loss = -11207.694304626275
6
Iteration 11200: Loss = -11207.659896874455
7
Iteration 11300: Loss = -11207.65895049348
Iteration 11400: Loss = -11207.65932810868
1
Iteration 11500: Loss = -11207.696464950335
2
Iteration 11600: Loss = -11207.784654654874
3
Iteration 11700: Loss = -11207.661932381458
4
Iteration 11800: Loss = -11207.659307484308
5
Iteration 11900: Loss = -11207.744254812458
6
Iteration 12000: Loss = -11207.659078877448
7
Iteration 12100: Loss = -11207.65904967306
Iteration 12200: Loss = -11207.845835866363
1
Iteration 12300: Loss = -11207.658977159863
Iteration 12400: Loss = -11207.665621153892
1
Iteration 12500: Loss = -11207.66454139159
2
Iteration 12600: Loss = -11207.774530962008
3
Iteration 12700: Loss = -11207.665203486553
4
Iteration 12800: Loss = -11207.659472437223
5
Iteration 12900: Loss = -11207.660356226048
6
Iteration 13000: Loss = -11207.667130853171
7
Iteration 13100: Loss = -11207.658901944662
Iteration 13200: Loss = -11207.676307667945
1
Iteration 13300: Loss = -11207.658925023537
Iteration 13400: Loss = -11207.695341741668
1
Iteration 13500: Loss = -11207.659112435755
2
Iteration 13600: Loss = -11207.659549007714
3
Iteration 13700: Loss = -11207.660149165835
4
Iteration 13800: Loss = -11207.658967130996
Iteration 13900: Loss = -11207.667822136033
1
Iteration 14000: Loss = -11207.817673057103
2
Iteration 14100: Loss = -11207.65891105541
Iteration 14200: Loss = -11207.660035984416
1
Iteration 14300: Loss = -11207.681264421242
2
Iteration 14400: Loss = -11207.659119251039
3
Iteration 14500: Loss = -11207.659491281538
4
Iteration 14600: Loss = -11207.659468568072
5
Iteration 14700: Loss = -11207.7500733517
6
Iteration 14800: Loss = -11207.658907732099
Iteration 14900: Loss = -11207.662824623938
1
Iteration 15000: Loss = -11207.677695639577
2
Iteration 15100: Loss = -11207.65965720286
3
Iteration 15200: Loss = -11207.667312963482
4
Iteration 15300: Loss = -11207.664991208718
5
Iteration 15400: Loss = -11207.658962270763
Iteration 15500: Loss = -11207.732338521759
1
Iteration 15600: Loss = -11207.690779862278
2
Iteration 15700: Loss = -11207.660095098103
3
Iteration 15800: Loss = -11207.659025335259
Iteration 15900: Loss = -11207.66203519783
1
Iteration 16000: Loss = -11207.661036348649
2
Iteration 16100: Loss = -11207.686455186307
3
Iteration 16200: Loss = -11207.66012380229
4
Iteration 16300: Loss = -11207.65986937592
5
Iteration 16400: Loss = -11207.789610446946
6
Iteration 16500: Loss = -11207.659628050054
7
Iteration 16600: Loss = -11207.659132280316
8
Iteration 16700: Loss = -11207.66272491678
9
Iteration 16800: Loss = -11207.662059592667
10
Iteration 16900: Loss = -11207.71708193586
11
Iteration 17000: Loss = -11207.685445065656
12
Iteration 17100: Loss = -11207.661889310424
13
Iteration 17200: Loss = -11207.659239650437
14
Iteration 17300: Loss = -11207.661136892993
15
Stopping early at iteration 17300 due to no improvement.
pi: tensor([[0.6534, 0.3466],
        [0.3676, 0.6324]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5695, 0.4305], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2314, 0.0971],
         [0.6540, 0.2769]],

        [[0.5702, 0.0953],
         [0.6007, 0.6164]],

        [[0.6330, 0.1015],
         [0.5625, 0.7054]],

        [[0.7048, 0.1000],
         [0.6369, 0.5410]],

        [[0.5222, 0.0872],
         [0.5828, 0.7237]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369954580512469
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6690909090909091
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
Global Adjusted Rand Index: 0.06162819899716971
Average Adjusted Rand Index: 0.8573741911954013
11156.24391361485
[0.029036977800507038, 0.06162819899716971] [0.8605104018199026, 0.8573741911954013] [11217.01214271327, 11207.661136892993]
-------------------------------------
This iteration is 98
True Objective function: Loss = -11182.205909772678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23289.071317484264
Iteration 100: Loss = -11227.960755040545
Iteration 200: Loss = -11225.51044767063
Iteration 300: Loss = -11225.22550380212
Iteration 400: Loss = -11225.090530346239
Iteration 500: Loss = -11224.963552316762
Iteration 600: Loss = -11224.672297073126
Iteration 700: Loss = -11176.565749558073
Iteration 800: Loss = -11155.494400280424
Iteration 900: Loss = -11152.104649478015
Iteration 1000: Loss = -11152.014784328898
Iteration 1100: Loss = -11151.99738209739
Iteration 1200: Loss = -11151.986506603404
Iteration 1300: Loss = -11151.977552778708
Iteration 1400: Loss = -11151.968701611038
Iteration 1500: Loss = -11151.961650352554
Iteration 1600: Loss = -11151.958413608241
Iteration 1700: Loss = -11151.956138795296
Iteration 1800: Loss = -11151.95435787737
Iteration 1900: Loss = -11151.952866581336
Iteration 2000: Loss = -11151.951654789604
Iteration 2100: Loss = -11151.951597062183
Iteration 2200: Loss = -11151.949754436275
Iteration 2300: Loss = -11151.9489898173
Iteration 2400: Loss = -11151.94836084571
Iteration 2500: Loss = -11151.94779816439
Iteration 2600: Loss = -11151.947283398818
Iteration 2700: Loss = -11151.947054238779
Iteration 2800: Loss = -11151.946902753127
Iteration 2900: Loss = -11151.946316856924
Iteration 3000: Loss = -11151.946186367564
Iteration 3100: Loss = -11151.946652831039
1
Iteration 3200: Loss = -11151.945618903985
Iteration 3300: Loss = -11151.946086617798
1
Iteration 3400: Loss = -11151.945308190283
Iteration 3500: Loss = -11151.944744675928
Iteration 3600: Loss = -11151.946680759436
1
Iteration 3700: Loss = -11151.944549532353
Iteration 3800: Loss = -11151.945492992472
1
Iteration 3900: Loss = -11151.948439813137
2
Iteration 4000: Loss = -11151.944032688445
Iteration 4100: Loss = -11151.943879598019
Iteration 4200: Loss = -11151.943850283124
Iteration 4300: Loss = -11151.9458585669
1
Iteration 4400: Loss = -11151.944374108694
2
Iteration 4500: Loss = -11151.946700424533
3
Iteration 4600: Loss = -11151.943427179911
Iteration 4700: Loss = -11151.943540585586
1
Iteration 4800: Loss = -11151.943278721916
Iteration 4900: Loss = -11151.94409964675
1
Iteration 5000: Loss = -11151.943296787176
Iteration 5100: Loss = -11151.944602578968
1
Iteration 5200: Loss = -11151.944384623186
2
Iteration 5300: Loss = -11151.943415328631
3
Iteration 5400: Loss = -11151.94292446988
Iteration 5500: Loss = -11151.943070731115
1
Iteration 5600: Loss = -11151.944164733231
2
Iteration 5700: Loss = -11151.94279791045
Iteration 5800: Loss = -11151.942709191497
Iteration 5900: Loss = -11151.94302411672
1
Iteration 6000: Loss = -11151.942663662037
Iteration 6100: Loss = -11151.942653223701
Iteration 6200: Loss = -11151.942631037593
Iteration 6300: Loss = -11151.942614282105
Iteration 6400: Loss = -11151.956839806493
1
Iteration 6500: Loss = -11151.942383275493
Iteration 6600: Loss = -11151.942323420402
Iteration 6700: Loss = -11151.94231810546
Iteration 6800: Loss = -11151.9469315221
1
Iteration 6900: Loss = -11151.942416351862
Iteration 7000: Loss = -11151.942498778038
Iteration 7100: Loss = -11151.949578466765
1
Iteration 7200: Loss = -11151.943078076718
2
Iteration 7300: Loss = -11151.942301455068
Iteration 7400: Loss = -11151.942574339193
1
Iteration 7500: Loss = -11151.942425128636
2
Iteration 7600: Loss = -11151.944780355318
3
Iteration 7700: Loss = -11151.942260014788
Iteration 7800: Loss = -11151.942240447606
Iteration 7900: Loss = -11151.942485959009
1
Iteration 8000: Loss = -11151.942223355922
Iteration 8100: Loss = -11151.942176552684
Iteration 8200: Loss = -11151.942657259458
1
Iteration 8300: Loss = -11151.942210237601
Iteration 8400: Loss = -11151.94331931299
1
Iteration 8500: Loss = -11151.9421539496
Iteration 8600: Loss = -11151.94702774827
1
Iteration 8700: Loss = -11151.942168107598
Iteration 8800: Loss = -11151.949223409514
1
Iteration 8900: Loss = -11151.942134233728
Iteration 9000: Loss = -11151.94584542141
1
Iteration 9100: Loss = -11151.94212541628
Iteration 9200: Loss = -11151.942347612821
1
Iteration 9300: Loss = -11151.945795407013
2
Iteration 9400: Loss = -11151.942142528556
Iteration 9500: Loss = -11151.942481736116
1
Iteration 9600: Loss = -11151.965607340104
2
Iteration 9700: Loss = -11151.942273532286
3
Iteration 9800: Loss = -11151.94214244931
Iteration 9900: Loss = -11151.957001730236
1
Iteration 10000: Loss = -11151.942131085414
Iteration 10100: Loss = -11152.064799449092
1
Iteration 10200: Loss = -11151.941334765239
Iteration 10300: Loss = -11151.948408328715
1
Iteration 10400: Loss = -11151.941586760864
2
Iteration 10500: Loss = -11151.941891937637
3
Iteration 10600: Loss = -11151.941354933613
Iteration 10700: Loss = -11151.94211589598
1
Iteration 10800: Loss = -11151.941341891143
Iteration 10900: Loss = -11151.944141199374
1
Iteration 11000: Loss = -11151.941820236623
2
Iteration 11100: Loss = -11151.941329707337
Iteration 11200: Loss = -11152.162713677308
1
Iteration 11300: Loss = -11151.941314024369
Iteration 11400: Loss = -11151.941298669144
Iteration 11500: Loss = -11151.94146117729
1
Iteration 11600: Loss = -11151.941658215792
2
Iteration 11700: Loss = -11151.95912755591
3
Iteration 11800: Loss = -11151.942798986276
4
Iteration 11900: Loss = -11151.946386659523
5
Iteration 12000: Loss = -11151.941303440193
Iteration 12100: Loss = -11151.941503592614
1
Iteration 12200: Loss = -11152.044111950388
2
Iteration 12300: Loss = -11151.94228424073
3
Iteration 12400: Loss = -11151.996358870922
4
Iteration 12500: Loss = -11151.941252213877
Iteration 12600: Loss = -11151.952245371682
1
Iteration 12700: Loss = -11151.967914621348
2
Iteration 12800: Loss = -11151.94130165402
Iteration 12900: Loss = -11151.943442484777
1
Iteration 13000: Loss = -11151.941279192677
Iteration 13100: Loss = -11151.94153505453
1
Iteration 13200: Loss = -11151.941249234422
Iteration 13300: Loss = -11151.942477059825
1
Iteration 13400: Loss = -11151.941237884943
Iteration 13500: Loss = -11151.94599612562
1
Iteration 13600: Loss = -11151.9412568394
Iteration 13700: Loss = -11151.942510307852
1
Iteration 13800: Loss = -11151.941256035736
Iteration 13900: Loss = -11151.942354901837
1
Iteration 14000: Loss = -11151.94130214426
Iteration 14100: Loss = -11151.941470735304
1
Iteration 14200: Loss = -11151.941270122787
Iteration 14300: Loss = -11151.941645394878
1
Iteration 14400: Loss = -11151.94126180734
Iteration 14500: Loss = -11151.957558232938
1
Iteration 14600: Loss = -11151.941262202241
Iteration 14700: Loss = -11152.091252220025
1
Iteration 14800: Loss = -11151.941280927706
Iteration 14900: Loss = -11151.94236460523
1
Iteration 15000: Loss = -11152.299420845995
2
Iteration 15100: Loss = -11151.941287549327
Iteration 15200: Loss = -11151.942771064523
1
Iteration 15300: Loss = -11151.941339177705
Iteration 15400: Loss = -11151.941247516297
Iteration 15500: Loss = -11151.941852031667
1
Iteration 15600: Loss = -11151.941272182734
Iteration 15700: Loss = -11152.002685079415
1
Iteration 15800: Loss = -11151.94128177737
Iteration 15900: Loss = -11151.941273678714
Iteration 16000: Loss = -11151.941521747325
1
Iteration 16100: Loss = -11151.941295539944
Iteration 16200: Loss = -11151.963378500654
1
Iteration 16300: Loss = -11151.941267777205
Iteration 16400: Loss = -11151.966782152078
1
Iteration 16500: Loss = -11151.944642555625
2
Iteration 16600: Loss = -11151.944074683815
3
Iteration 16700: Loss = -11151.943416045557
4
Iteration 16800: Loss = -11151.950201476973
5
Iteration 16900: Loss = -11151.941300039534
Iteration 17000: Loss = -11151.941607371253
1
Iteration 17100: Loss = -11151.941292966047
Iteration 17200: Loss = -11151.941707688722
1
Iteration 17300: Loss = -11151.970984419157
2
Iteration 17400: Loss = -11151.944974454682
3
Iteration 17500: Loss = -11151.947068922878
4
Iteration 17600: Loss = -11151.941292947156
Iteration 17700: Loss = -11151.947063928767
1
Iteration 17800: Loss = -11151.941291832907
Iteration 17900: Loss = -11151.941672897896
1
Iteration 18000: Loss = -11151.946241048967
2
Iteration 18100: Loss = -11151.942894684924
3
Iteration 18200: Loss = -11151.94214467511
4
Iteration 18300: Loss = -11151.9883935191
5
Iteration 18400: Loss = -11151.941719240405
6
Iteration 18500: Loss = -11151.994473251461
7
Iteration 18600: Loss = -11151.941393183413
8
Iteration 18700: Loss = -11152.166243036014
9
Iteration 18800: Loss = -11151.941316935474
Iteration 18900: Loss = -11151.990945950985
1
Iteration 19000: Loss = -11151.941294729064
Iteration 19100: Loss = -11151.941255035079
Iteration 19200: Loss = -11151.941761997386
1
Iteration 19300: Loss = -11151.951927110145
2
Iteration 19400: Loss = -11151.949850864361
3
Iteration 19500: Loss = -11152.015941978903
4
Iteration 19600: Loss = -11151.945191706163
5
Iteration 19700: Loss = -11151.941356489737
6
Iteration 19800: Loss = -11151.942268794843
7
Iteration 19900: Loss = -11151.946371438784
8
pi: tensor([[0.7572, 0.2428],
        [0.2437, 0.7563]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5181, 0.4819], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2962, 0.0972],
         [0.7022, 0.2021]],

        [[0.5789, 0.1000],
         [0.6665, 0.6039]],

        [[0.6723, 0.1019],
         [0.5122, 0.5533]],

        [[0.6232, 0.1006],
         [0.7294, 0.5412]],

        [[0.5402, 0.0981],
         [0.5459, 0.5101]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.9214427761527915
Average Adjusted Rand Index: 0.9216121053571843
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22792.793695088505
Iteration 100: Loss = -11458.059571576703
Iteration 200: Loss = -11457.067917184617
Iteration 300: Loss = -11456.852307790581
Iteration 400: Loss = -11456.691597519415
Iteration 500: Loss = -11456.335529459058
Iteration 600: Loss = -11452.951502900505
Iteration 700: Loss = -11391.126700610987
Iteration 800: Loss = -11270.344856549822
Iteration 900: Loss = -11234.982586193939
Iteration 1000: Loss = -11226.285775184971
Iteration 1100: Loss = -11225.5802566152
Iteration 1200: Loss = -11225.415337596825
Iteration 1300: Loss = -11225.314541768468
Iteration 1400: Loss = -11225.25434813308
Iteration 1500: Loss = -11225.212284236695
Iteration 1600: Loss = -11225.180928301736
Iteration 1700: Loss = -11225.156739110816
Iteration 1800: Loss = -11225.133402456779
Iteration 1900: Loss = -11225.105267087954
Iteration 2000: Loss = -11225.066619957546
Iteration 2100: Loss = -11224.995237020321
Iteration 2200: Loss = -11224.564802563536
Iteration 2300: Loss = -11194.491590653628
Iteration 2400: Loss = -11186.990082950635
Iteration 2500: Loss = -11186.223373967314
Iteration 2600: Loss = -11170.541654720553
Iteration 2700: Loss = -11164.453373681803
Iteration 2800: Loss = -11164.207400772917
Iteration 2900: Loss = -11164.19505027704
Iteration 3000: Loss = -11164.176663732324
Iteration 3100: Loss = -11158.785939782942
Iteration 3200: Loss = -11158.777952337507
Iteration 3300: Loss = -11158.772660771047
Iteration 3400: Loss = -11158.768005955615
Iteration 3500: Loss = -11158.764980500458
Iteration 3600: Loss = -11158.762161930325
Iteration 3700: Loss = -11158.76037297149
Iteration 3800: Loss = -11158.758468691607
Iteration 3900: Loss = -11158.755933390072
Iteration 4000: Loss = -11158.751789365568
Iteration 4100: Loss = -11158.750629961865
Iteration 4200: Loss = -11158.75230906852
1
Iteration 4300: Loss = -11158.751617735596
2
Iteration 4400: Loss = -11158.72940394384
Iteration 4500: Loss = -11151.968758185985
Iteration 4600: Loss = -11151.9621445347
Iteration 4700: Loss = -11151.961080803489
Iteration 4800: Loss = -11151.960003559941
Iteration 4900: Loss = -11151.959337478482
Iteration 5000: Loss = -11151.95946230036
1
Iteration 5100: Loss = -11151.961190560263
2
Iteration 5200: Loss = -11151.957362912903
Iteration 5300: Loss = -11151.957391848095
Iteration 5400: Loss = -11151.961018939364
1
Iteration 5500: Loss = -11151.957135687591
Iteration 5600: Loss = -11151.955006091572
Iteration 5700: Loss = -11151.954561777558
Iteration 5800: Loss = -11151.953954344186
Iteration 5900: Loss = -11151.953528534817
Iteration 6000: Loss = -11151.954561313634
1
Iteration 6100: Loss = -11151.950423894612
Iteration 6200: Loss = -11151.948782141737
Iteration 6300: Loss = -11151.948448016114
Iteration 6400: Loss = -11151.948716833645
1
Iteration 6500: Loss = -11151.947530034424
Iteration 6600: Loss = -11151.946560386838
Iteration 6700: Loss = -11151.949251867694
1
Iteration 6800: Loss = -11151.945000045496
Iteration 6900: Loss = -11151.948427291843
1
Iteration 7000: Loss = -11151.945552465662
2
Iteration 7100: Loss = -11151.944667073425
Iteration 7200: Loss = -11151.946069818914
1
Iteration 7300: Loss = -11151.99593279426
2
Iteration 7400: Loss = -11151.944331197632
Iteration 7500: Loss = -11151.944465274286
1
Iteration 7600: Loss = -11151.943966727082
Iteration 7700: Loss = -11151.944293540422
1
Iteration 7800: Loss = -11151.943057509958
Iteration 7900: Loss = -11151.983736502287
1
Iteration 8000: Loss = -11151.942895593847
Iteration 8100: Loss = -11151.942869932918
Iteration 8200: Loss = -11151.948688335275
1
Iteration 8300: Loss = -11151.942767147832
Iteration 8400: Loss = -11151.94275357681
Iteration 8500: Loss = -11151.942825095171
Iteration 8600: Loss = -11151.942706424099
Iteration 8700: Loss = -11151.94266840175
Iteration 8800: Loss = -11151.946086897866
1
Iteration 8900: Loss = -11151.942647073425
Iteration 9000: Loss = -11151.961571857339
1
Iteration 9100: Loss = -11151.95497533922
2
Iteration 9200: Loss = -11151.942660863093
Iteration 9300: Loss = -11151.942557823166
Iteration 9400: Loss = -11151.949263123372
1
Iteration 9500: Loss = -11151.9424776593
Iteration 9600: Loss = -11151.943202053242
1
Iteration 9700: Loss = -11151.942470892975
Iteration 9800: Loss = -11151.94263355861
1
Iteration 9900: Loss = -11151.94241705
Iteration 10000: Loss = -11151.942560429028
1
Iteration 10100: Loss = -11151.942385261491
Iteration 10200: Loss = -11151.943231214047
1
Iteration 10300: Loss = -11151.942521185534
2
Iteration 10400: Loss = -11151.951648454462
3
Iteration 10500: Loss = -11151.942464626834
Iteration 10600: Loss = -11151.944986981785
1
Iteration 10700: Loss = -11151.94234166286
Iteration 10800: Loss = -11151.94570949967
1
Iteration 10900: Loss = -11151.946050913479
2
Iteration 11000: Loss = -11151.942697262763
3
Iteration 11100: Loss = -11151.953929659054
4
Iteration 11200: Loss = -11151.944640591984
5
Iteration 11300: Loss = -11151.942940607525
6
Iteration 11400: Loss = -11152.163394372117
7
Iteration 11500: Loss = -11151.942203240376
Iteration 11600: Loss = -11151.946477997666
1
Iteration 11700: Loss = -11151.942163709224
Iteration 11800: Loss = -11151.942120129455
Iteration 11900: Loss = -11152.227473697494
1
Iteration 12000: Loss = -11151.941711603671
Iteration 12100: Loss = -11151.95996175412
1
Iteration 12200: Loss = -11151.94429017105
2
Iteration 12300: Loss = -11151.942003248432
3
Iteration 12400: Loss = -11152.01613784249
4
Iteration 12500: Loss = -11151.943077105423
5
Iteration 12600: Loss = -11151.94356789531
6
Iteration 12700: Loss = -11151.941673822088
Iteration 12800: Loss = -11151.941756456437
Iteration 12900: Loss = -11151.941696960372
Iteration 13000: Loss = -11151.945551627325
1
Iteration 13100: Loss = -11152.180643030733
2
Iteration 13200: Loss = -11151.943322442758
3
Iteration 13300: Loss = -11152.007488725478
4
Iteration 13400: Loss = -11151.955654788728
5
Iteration 13500: Loss = -11152.069113482823
6
Iteration 13600: Loss = -11151.945860708523
7
Iteration 13700: Loss = -11151.944981024613
8
Iteration 13800: Loss = -11151.942163690192
9
Iteration 13900: Loss = -11151.941650925104
Iteration 14000: Loss = -11151.942305442535
1
Iteration 14100: Loss = -11151.941482408265
Iteration 14200: Loss = -11151.941716948055
1
Iteration 14300: Loss = -11151.941423365244
Iteration 14400: Loss = -11151.957181896769
1
Iteration 14500: Loss = -11151.941491862997
Iteration 14600: Loss = -11152.003310605587
1
Iteration 14700: Loss = -11151.941369607577
Iteration 14800: Loss = -11152.366800534723
1
Iteration 14900: Loss = -11151.941377790281
Iteration 15000: Loss = -11151.941351305419
Iteration 15100: Loss = -11151.942059282424
1
Iteration 15200: Loss = -11151.941581241485
2
Iteration 15300: Loss = -11151.941726073632
3
Iteration 15400: Loss = -11151.958946995246
4
Iteration 15500: Loss = -11151.941393730414
Iteration 15600: Loss = -11151.952017295036
1
Iteration 15700: Loss = -11151.94136765754
Iteration 15800: Loss = -11151.946219254314
1
Iteration 15900: Loss = -11152.017413427056
2
Iteration 16000: Loss = -11151.94146852117
3
Iteration 16100: Loss = -11151.942026192648
4
Iteration 16200: Loss = -11151.94877335115
5
Iteration 16300: Loss = -11151.941420805288
Iteration 16400: Loss = -11151.943252209521
1
Iteration 16500: Loss = -11151.949777182044
2
Iteration 16600: Loss = -11151.941425546569
Iteration 16700: Loss = -11151.961252883704
1
Iteration 16800: Loss = -11151.941365064416
Iteration 16900: Loss = -11151.941737088513
1
Iteration 17000: Loss = -11151.967185074789
2
Iteration 17100: Loss = -11151.941333722703
Iteration 17200: Loss = -11151.941743056817
1
Iteration 17300: Loss = -11151.97768702341
2
Iteration 17400: Loss = -11152.050211143407
3
Iteration 17500: Loss = -11151.941497985168
4
Iteration 17600: Loss = -11151.941452926862
5
Iteration 17700: Loss = -11151.948083714833
6
Iteration 17800: Loss = -11151.941330654936
Iteration 17900: Loss = -11151.941538595109
1
Iteration 18000: Loss = -11151.960548545483
2
Iteration 18100: Loss = -11151.941328666993
Iteration 18200: Loss = -11151.94741002409
1
Iteration 18300: Loss = -11151.941322636118
Iteration 18400: Loss = -11151.943952037667
1
Iteration 18500: Loss = -11151.941311094246
Iteration 18600: Loss = -11151.941640967309
1
Iteration 18700: Loss = -11151.941304204696
Iteration 18800: Loss = -11151.943377573361
1
Iteration 18900: Loss = -11151.997717712924
2
Iteration 19000: Loss = -11151.941330454387
Iteration 19100: Loss = -11152.16956644809
1
Iteration 19200: Loss = -11151.941306549239
Iteration 19300: Loss = -11151.94128133296
Iteration 19400: Loss = -11151.942254822341
1
Iteration 19500: Loss = -11151.941284521563
Iteration 19600: Loss = -11152.24326794229
1
Iteration 19700: Loss = -11151.941286993551
Iteration 19800: Loss = -11151.958539244684
1
Iteration 19900: Loss = -11151.941649367946
2
pi: tensor([[0.7598, 0.2402],
        [0.2454, 0.7546]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5218, 0.4782], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2945, 0.0968],
         [0.7170, 0.2031]],

        [[0.6892, 0.0993],
         [0.6413, 0.7284]],

        [[0.5028, 0.1012],
         [0.7201, 0.5866]],

        [[0.5849, 0.1002],
         [0.6132, 0.6555]],

        [[0.7073, 0.0974],
         [0.6822, 0.7294]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.9214427761527915
Average Adjusted Rand Index: 0.9216121053571843
11182.205909772678
[0.9214427761527915, 0.9214427761527915] [0.9216121053571843, 0.9216121053571843] [11152.05774848788, 11151.947256286481]
-------------------------------------
This iteration is 99
True Objective function: Loss = -11179.296791656672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23142.16170078118
Iteration 100: Loss = -11427.363980194716
Iteration 200: Loss = -11424.915770616873
Iteration 300: Loss = -11421.86212009137
Iteration 400: Loss = -11276.131931725704
Iteration 500: Loss = -11252.455159079875
Iteration 600: Loss = -11249.701442701038
Iteration 700: Loss = -11249.049453735766
Iteration 800: Loss = -11248.905012052595
Iteration 900: Loss = -11248.830448661252
Iteration 1000: Loss = -11248.782970614635
Iteration 1100: Loss = -11248.708430467113
Iteration 1200: Loss = -11248.680015886515
Iteration 1300: Loss = -11248.658539434748
Iteration 1400: Loss = -11248.609163139801
Iteration 1500: Loss = -11248.585136187907
Iteration 1600: Loss = -11248.57439281973
Iteration 1700: Loss = -11248.563916949439
Iteration 1800: Loss = -11248.40607095271
Iteration 1900: Loss = -11248.385092602533
Iteration 2000: Loss = -11248.315428441003
Iteration 2100: Loss = -11248.28723130975
Iteration 2200: Loss = -11244.71894725987
Iteration 2300: Loss = -11239.507220866286
Iteration 2400: Loss = -11232.114203444027
Iteration 2500: Loss = -11231.586460014627
Iteration 2600: Loss = -11231.569980954773
Iteration 2700: Loss = -11227.530623690498
Iteration 2800: Loss = -11227.398533501397
Iteration 2900: Loss = -11224.564560319115
Iteration 3000: Loss = -11223.482089074902
Iteration 3100: Loss = -11217.714127089235
Iteration 3200: Loss = -11217.638221836993
Iteration 3300: Loss = -11217.63239736671
Iteration 3400: Loss = -11217.630661248933
Iteration 3500: Loss = -11217.625488506295
Iteration 3600: Loss = -11217.611280499486
Iteration 3700: Loss = -11212.545608415418
Iteration 3800: Loss = -11212.544969714876
Iteration 3900: Loss = -11212.544130468454
Iteration 4000: Loss = -11212.539271584354
Iteration 4100: Loss = -11212.537456676731
Iteration 4200: Loss = -11212.53549878549
Iteration 4300: Loss = -11212.53506633153
Iteration 4400: Loss = -11212.533628957151
Iteration 4500: Loss = -11212.539277579173
1
Iteration 4600: Loss = -11212.532035426879
Iteration 4700: Loss = -11212.531327038512
Iteration 4800: Loss = -11212.5306792698
Iteration 4900: Loss = -11212.530051767779
Iteration 5000: Loss = -11212.531532577612
1
Iteration 5100: Loss = -11212.529159449437
Iteration 5200: Loss = -11212.528762083864
Iteration 5300: Loss = -11212.528440524155
Iteration 5400: Loss = -11212.528081226725
Iteration 5500: Loss = -11212.534532382784
1
Iteration 5600: Loss = -11212.527109866664
Iteration 5700: Loss = -11212.524799260247
Iteration 5800: Loss = -11212.524259912496
Iteration 5900: Loss = -11212.52358273859
Iteration 6000: Loss = -11212.523498005337
Iteration 6100: Loss = -11212.523385960749
Iteration 6200: Loss = -11212.523181473274
Iteration 6300: Loss = -11212.523030449503
Iteration 6400: Loss = -11212.52290369076
Iteration 6500: Loss = -11212.522773236637
Iteration 6600: Loss = -11212.522675963375
Iteration 6700: Loss = -11212.522566587153
Iteration 6800: Loss = -11212.52242095032
Iteration 6900: Loss = -11212.52308469639
1
Iteration 7000: Loss = -11212.522238151249
Iteration 7100: Loss = -11212.522121508378
Iteration 7200: Loss = -11212.522074024271
Iteration 7300: Loss = -11212.521876409091
Iteration 7400: Loss = -11212.564141646339
1
Iteration 7500: Loss = -11212.521674343277
Iteration 7600: Loss = -11212.521487440928
Iteration 7700: Loss = -11212.521492740289
Iteration 7800: Loss = -11212.521088743213
Iteration 7900: Loss = -11212.520848888835
Iteration 8000: Loss = -11212.520469673165
Iteration 8100: Loss = -11212.519876507293
Iteration 8200: Loss = -11212.518734277062
Iteration 8300: Loss = -11212.513888023734
Iteration 8400: Loss = -11212.242487867428
Iteration 8500: Loss = -11212.108469494156
Iteration 8600: Loss = -11212.074859589566
Iteration 8700: Loss = -11211.940743703197
Iteration 8800: Loss = -11211.940335841537
Iteration 8900: Loss = -11211.884378551127
Iteration 9000: Loss = -11211.86697041469
Iteration 9100: Loss = -11211.91322741149
1
Iteration 9200: Loss = -11211.768511740416
Iteration 9300: Loss = -11211.761876590026
Iteration 9400: Loss = -11211.76088666646
Iteration 9500: Loss = -11211.76533322069
1
Iteration 9600: Loss = -11211.760679903524
Iteration 9700: Loss = -11211.750491805235
Iteration 9800: Loss = -11211.750336356052
Iteration 9900: Loss = -11211.751085324511
1
Iteration 10000: Loss = -11211.750211709417
Iteration 10100: Loss = -11211.758834490825
1
Iteration 10200: Loss = -11211.74807785057
Iteration 10300: Loss = -11211.772085432083
1
Iteration 10400: Loss = -11211.74722658795
Iteration 10500: Loss = -11211.77471390631
1
Iteration 10600: Loss = -11211.744363549547
Iteration 10700: Loss = -11211.725480018644
Iteration 10800: Loss = -11211.72736340419
1
Iteration 10900: Loss = -11211.722580776193
Iteration 11000: Loss = -11211.723298126042
1
Iteration 11100: Loss = -11211.731742779572
2
Iteration 11200: Loss = -11211.758982198564
3
Iteration 11300: Loss = -11211.698175962829
Iteration 11400: Loss = -11211.693604996424
Iteration 11500: Loss = -11211.700415771535
1
Iteration 11600: Loss = -11211.693176797844
Iteration 11700: Loss = -11211.729295610785
1
Iteration 11800: Loss = -11211.722013147148
2
Iteration 11900: Loss = -11211.693136044574
Iteration 12000: Loss = -11211.694872093381
1
Iteration 12100: Loss = -11211.705993918109
2
Iteration 12200: Loss = -11211.693153611233
Iteration 12300: Loss = -11211.692906059578
Iteration 12400: Loss = -11211.763450940089
1
Iteration 12500: Loss = -11211.940857993033
2
Iteration 12600: Loss = -11211.693004885665
Iteration 12700: Loss = -11211.698263097924
1
Iteration 12800: Loss = -11211.700507735022
2
Iteration 12900: Loss = -11211.694696700075
3
Iteration 13000: Loss = -11211.698923722754
4
Iteration 13100: Loss = -11211.742183837468
5
Iteration 13200: Loss = -11211.828372629718
6
Iteration 13300: Loss = -11211.713257823543
7
Iteration 13400: Loss = -11211.692833642725
Iteration 13500: Loss = -11211.693078576041
1
Iteration 13600: Loss = -11211.693960534105
2
Iteration 13700: Loss = -11211.6938931364
3
Iteration 13800: Loss = -11211.693220803005
4
Iteration 13900: Loss = -11211.692986395521
5
Iteration 14000: Loss = -11211.712480843782
6
Iteration 14100: Loss = -11211.765484323227
7
Iteration 14200: Loss = -11211.710233123476
8
Iteration 14300: Loss = -11211.692820999824
Iteration 14400: Loss = -11211.692985034591
1
Iteration 14500: Loss = -11211.701281357758
2
Iteration 14600: Loss = -11211.712316036743
3
Iteration 14700: Loss = -11211.791950194141
4
Iteration 14800: Loss = -11211.771195177695
5
Iteration 14900: Loss = -11211.73315035342
6
Iteration 15000: Loss = -11211.694411304643
7
Iteration 15100: Loss = -11211.689675228141
Iteration 15200: Loss = -11211.700432728449
1
Iteration 15300: Loss = -11211.689563711328
Iteration 15400: Loss = -11211.691616596061
1
Iteration 15500: Loss = -11211.808846915657
2
Iteration 15600: Loss = -11211.689574639255
Iteration 15700: Loss = -11211.69104381641
1
Iteration 15800: Loss = -11211.689538911929
Iteration 15900: Loss = -11211.691639526496
1
Iteration 16000: Loss = -11211.690351038666
2
Iteration 16100: Loss = -11211.68727993488
Iteration 16200: Loss = -11211.690436012921
1
Iteration 16300: Loss = -11211.697131181627
2
Iteration 16400: Loss = -11211.687890761119
3
Iteration 16500: Loss = -11211.739842757781
4
Iteration 16600: Loss = -11211.687194994274
Iteration 16700: Loss = -11211.688477511527
1
Iteration 16800: Loss = -11211.720956509476
2
Iteration 16900: Loss = -11211.78879103601
3
Iteration 17000: Loss = -11211.779342525875
4
Iteration 17100: Loss = -11211.687160486385
Iteration 17200: Loss = -11211.688668806139
1
Iteration 17300: Loss = -11211.894580140994
2
Iteration 17400: Loss = -11211.68696468222
Iteration 17500: Loss = -11211.695011162255
1
Iteration 17600: Loss = -11211.793373988141
2
Iteration 17700: Loss = -11211.787543583268
3
Iteration 17800: Loss = -11211.766287500212
4
Iteration 17900: Loss = -11211.705027525868
5
Iteration 18000: Loss = -11211.686307147527
Iteration 18100: Loss = -11211.694391128689
1
Iteration 18200: Loss = -11211.68624109235
Iteration 18300: Loss = -11211.686537588846
1
Iteration 18400: Loss = -11211.688801824666
2
Iteration 18500: Loss = -11211.686545813593
3
Iteration 18600: Loss = -11211.686305961848
Iteration 18700: Loss = -11211.686311905069
Iteration 18800: Loss = -11211.687118970156
1
Iteration 18900: Loss = -11211.696818621858
2
Iteration 19000: Loss = -11211.686536205161
3
Iteration 19100: Loss = -11211.686245360188
Iteration 19200: Loss = -11211.687080096928
1
Iteration 19300: Loss = -11211.709855727137
2
Iteration 19400: Loss = -11211.718995944231
3
Iteration 19500: Loss = -11211.684204024195
Iteration 19600: Loss = -11211.683779127963
Iteration 19700: Loss = -11211.684650519077
1
Iteration 19800: Loss = -11211.688035614769
2
Iteration 19900: Loss = -11211.683906573715
3
pi: tensor([[0.7281, 0.2719],
        [0.2086, 0.7914]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9617, 0.0383], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1853, 0.1234],
         [0.5091, 0.3180]],

        [[0.5452, 0.1090],
         [0.6431, 0.5289]],

        [[0.6148, 0.0992],
         [0.6676, 0.6611]],

        [[0.5925, 0.1018],
         [0.5175, 0.6416]],

        [[0.6051, 0.1114],
         [0.7228, 0.5608]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8079912862954653
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.882389682918764
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6263834353230214
Average Adjusted Rand Index: 0.738076193842846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19161.47387920265
Iteration 100: Loss = -11429.134815180741
Iteration 200: Loss = -11428.223376990594
Iteration 300: Loss = -11426.974914282015
Iteration 400: Loss = -11425.796514261678
Iteration 500: Loss = -11328.63487162367
Iteration 600: Loss = -11261.084192555725
Iteration 700: Loss = -11244.228822715731
Iteration 800: Loss = -11232.504393869855
Iteration 900: Loss = -11223.82461720727
Iteration 1000: Loss = -11213.514381776877
Iteration 1100: Loss = -11212.91323511403
Iteration 1200: Loss = -11212.721974279939
Iteration 1300: Loss = -11212.648137146514
Iteration 1400: Loss = -11212.628282384143
Iteration 1500: Loss = -11212.613971758787
Iteration 1600: Loss = -11212.602881760597
Iteration 1700: Loss = -11212.593715978168
Iteration 1800: Loss = -11212.585967682755
Iteration 1900: Loss = -11212.579634257734
Iteration 2000: Loss = -11212.574113220666
Iteration 2100: Loss = -11212.569042584671
Iteration 2200: Loss = -11212.564302460341
Iteration 2300: Loss = -11212.560114128875
Iteration 2400: Loss = -11212.556430667957
Iteration 2500: Loss = -11212.55245412642
Iteration 2600: Loss = -11212.547304942587
Iteration 2700: Loss = -11212.543893650092
Iteration 2800: Loss = -11212.53981115791
Iteration 2900: Loss = -11212.53544713802
Iteration 3000: Loss = -11212.534810028266
Iteration 3100: Loss = -11212.532603399712
Iteration 3200: Loss = -11212.531616505445
Iteration 3300: Loss = -11212.5307942791
Iteration 3400: Loss = -11212.529966641525
Iteration 3500: Loss = -11212.530657910454
1
Iteration 3600: Loss = -11212.528614128583
Iteration 3700: Loss = -11212.528022758293
Iteration 3800: Loss = -11212.527689226328
Iteration 3900: Loss = -11212.526942553668
Iteration 4000: Loss = -11212.52649740043
Iteration 4100: Loss = -11212.52613262787
Iteration 4200: Loss = -11212.525671459309
Iteration 4300: Loss = -11212.525792689978
1
Iteration 4400: Loss = -11212.524962206002
Iteration 4500: Loss = -11212.557669173171
1
Iteration 4600: Loss = -11212.524328262569
Iteration 4700: Loss = -11212.524008114351
Iteration 4800: Loss = -11212.523516414441
Iteration 4900: Loss = -11212.522812637277
Iteration 5000: Loss = -11212.531038950303
1
Iteration 5100: Loss = -11212.522228385851
Iteration 5200: Loss = -11212.521931240757
Iteration 5300: Loss = -11212.52944072653
1
Iteration 5400: Loss = -11212.521134472536
Iteration 5500: Loss = -11212.520859279084
Iteration 5600: Loss = -11212.521303854455
1
Iteration 5700: Loss = -11212.520527753428
Iteration 5800: Loss = -11212.520400562882
Iteration 5900: Loss = -11212.52028665804
Iteration 6000: Loss = -11212.520163437292
Iteration 6100: Loss = -11212.522837009868
1
Iteration 6200: Loss = -11212.5199364579
Iteration 6300: Loss = -11212.519803564686
Iteration 6400: Loss = -11212.520064586417
1
Iteration 6500: Loss = -11212.519642261286
Iteration 6600: Loss = -11212.519574734379
Iteration 6700: Loss = -11212.519467722199
Iteration 6800: Loss = -11212.519398785022
Iteration 6900: Loss = -11212.519396548952
Iteration 7000: Loss = -11212.536857633115
1
Iteration 7100: Loss = -11212.51977917701
2
Iteration 7200: Loss = -11212.519114752675
Iteration 7300: Loss = -11212.5191789213
Iteration 7400: Loss = -11212.518992288557
Iteration 7500: Loss = -11212.52054102872
1
Iteration 7600: Loss = -11212.518865453934
Iteration 7700: Loss = -11212.52669965375
1
Iteration 7800: Loss = -11212.51879007222
Iteration 7900: Loss = -11212.519381319864
1
Iteration 8000: Loss = -11212.518689777444
Iteration 8100: Loss = -11212.518783539585
Iteration 8200: Loss = -11212.518619483908
Iteration 8300: Loss = -11212.545434827443
1
Iteration 8400: Loss = -11212.518498217667
Iteration 8500: Loss = -11212.51864036951
1
Iteration 8600: Loss = -11212.518506652908
Iteration 8700: Loss = -11212.519424666636
1
Iteration 8800: Loss = -11212.519018035773
2
Iteration 8900: Loss = -11212.518855330056
3
Iteration 9000: Loss = -11212.519003539377
4
Iteration 9100: Loss = -11212.545983075028
5
Iteration 9200: Loss = -11212.520605580801
6
Iteration 9300: Loss = -11212.518471782994
Iteration 9400: Loss = -11212.518451512506
Iteration 9500: Loss = -11212.521358808573
1
Iteration 9600: Loss = -11212.518281305778
Iteration 9700: Loss = -11212.570606925956
1
Iteration 9800: Loss = -11212.518253414173
Iteration 9900: Loss = -11212.518573604957
1
Iteration 10000: Loss = -11212.518640125563
2
Iteration 10100: Loss = -11212.531997232358
3
Iteration 10200: Loss = -11212.524934669531
4
Iteration 10300: Loss = -11212.518350396844
Iteration 10400: Loss = -11212.518272498091
Iteration 10500: Loss = -11212.519210077273
1
Iteration 10600: Loss = -11212.570302752094
2
Iteration 10700: Loss = -11212.529096182578
3
Iteration 10800: Loss = -11212.535579450187
4
Iteration 10900: Loss = -11212.519142384692
5
Iteration 11000: Loss = -11212.538980768692
6
Iteration 11100: Loss = -11212.518119269274
Iteration 11200: Loss = -11212.521709376635
1
Iteration 11300: Loss = -11212.520631560565
2
Iteration 11400: Loss = -11212.518284087593
3
Iteration 11500: Loss = -11212.546104796136
4
Iteration 11600: Loss = -11212.518010803842
Iteration 11700: Loss = -11212.519368705538
1
Iteration 11800: Loss = -11212.517329704962
Iteration 11900: Loss = -11212.323033967778
Iteration 12000: Loss = -11212.300301429901
Iteration 12100: Loss = -11212.262241073484
Iteration 12200: Loss = -11212.250049914392
Iteration 12300: Loss = -11212.195759599075
Iteration 12400: Loss = -11212.194073348655
Iteration 12500: Loss = -11212.195757937181
1
Iteration 12600: Loss = -11212.19960947849
2
Iteration 12700: Loss = -11211.9501117467
Iteration 12800: Loss = -11211.797688939578
Iteration 12900: Loss = -11211.809224651204
1
Iteration 13000: Loss = -11211.803565042363
2
Iteration 13100: Loss = -11211.79394586927
Iteration 13200: Loss = -11211.78118049984
Iteration 13300: Loss = -11211.780896005015
Iteration 13400: Loss = -11211.781400149042
1
Iteration 13500: Loss = -11211.824509526428
2
Iteration 13600: Loss = -11211.780765821382
Iteration 13700: Loss = -11211.778840103188
Iteration 13800: Loss = -11211.718567346654
Iteration 13900: Loss = -11211.729472758781
1
Iteration 14000: Loss = -11211.72100759449
2
Iteration 14100: Loss = -11211.85868990808
3
Iteration 14200: Loss = -11211.705354335576
Iteration 14300: Loss = -11211.699849019049
Iteration 14400: Loss = -11211.710429682156
1
Iteration 14500: Loss = -11211.846306916314
2
Iteration 14600: Loss = -11211.706511927172
3
Iteration 14700: Loss = -11211.698694616032
Iteration 14800: Loss = -11211.693061145077
Iteration 14900: Loss = -11211.693660209468
1
Iteration 15000: Loss = -11211.694569049656
2
Iteration 15100: Loss = -11211.702411111302
3
Iteration 15200: Loss = -11211.774636600328
4
Iteration 15300: Loss = -11211.712638597544
5
Iteration 15400: Loss = -11211.692596044984
Iteration 15500: Loss = -11211.743635165292
1
Iteration 15600: Loss = -11211.692483404744
Iteration 15700: Loss = -11211.785193025386
1
Iteration 15800: Loss = -11211.707894870307
2
Iteration 15900: Loss = -11211.688570442166
Iteration 16000: Loss = -11211.69484548376
1
Iteration 16100: Loss = -11211.700804894108
2
Iteration 16200: Loss = -11211.710227808402
3
Iteration 16300: Loss = -11211.702572186263
4
Iteration 16400: Loss = -11211.858292961373
5
Iteration 16500: Loss = -11211.706763188718
6
Iteration 16600: Loss = -11211.728167571782
7
Iteration 16700: Loss = -11211.723107910397
8
Iteration 16800: Loss = -11211.688673285476
9
Iteration 16900: Loss = -11211.688302138937
Iteration 17000: Loss = -11211.69107161175
1
Iteration 17100: Loss = -11211.688263061056
Iteration 17200: Loss = -11211.688273035967
Iteration 17300: Loss = -11211.691643169304
1
Iteration 17400: Loss = -11211.767498626668
2
Iteration 17500: Loss = -11211.713374230194
3
Iteration 17600: Loss = -11211.685442517975
Iteration 17700: Loss = -11211.682694593359
Iteration 17800: Loss = -11211.683273702589
1
Iteration 17900: Loss = -11211.751171139982
2
Iteration 18000: Loss = -11211.683704212262
3
Iteration 18100: Loss = -11211.682657317358
Iteration 18200: Loss = -11211.678171233867
Iteration 18300: Loss = -11211.693835856793
1
Iteration 18400: Loss = -11211.863536529934
2
Iteration 18500: Loss = -11211.677992363246
Iteration 18600: Loss = -11211.678253927092
1
Iteration 18700: Loss = -11211.678553783566
2
Iteration 18800: Loss = -11211.824386379029
3
Iteration 18900: Loss = -11211.67798532239
Iteration 19000: Loss = -11211.678266451805
1
Iteration 19100: Loss = -11211.805199922392
2
Iteration 19200: Loss = -11211.677966257585
Iteration 19300: Loss = -11211.678867250143
1
Iteration 19400: Loss = -11211.699420279105
2
Iteration 19500: Loss = -11211.68073197251
3
Iteration 19600: Loss = -11211.678517749473
4
Iteration 19700: Loss = -11211.678876498565
5
Iteration 19800: Loss = -11211.684489589761
6
Iteration 19900: Loss = -11211.67836703546
7
pi: tensor([[0.7914, 0.2086],
        [0.2719, 0.7281]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0383, 0.9617], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3179, 0.1234],
         [0.6912, 0.1854]],

        [[0.5225, 0.1090],
         [0.5148, 0.6822]],

        [[0.6535, 0.0992],
         [0.5031, 0.5190]],

        [[0.5035, 0.1019],
         [0.6032, 0.6215]],

        [[0.7051, 0.1114],
         [0.5437, 0.7208]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8079912862954653
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.882389682918764
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6263834353230214
Average Adjusted Rand Index: 0.738076193842846
11179.296791656672
[0.6263834353230214, 0.6263834353230214] [0.738076193842846, 0.738076193842846] [11211.678095190022, 11211.678324359742]
