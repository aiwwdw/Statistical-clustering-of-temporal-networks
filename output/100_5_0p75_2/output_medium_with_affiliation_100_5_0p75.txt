nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [21:30<35:29:03, 1290.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [34:31<26:58:05, 990.67s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [55:30<30:00:06, 1113.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [1:18:35<32:33:00, 1220.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [1:40:16<32:58:10, 1249.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [1:55:36<29:42:21, 1137.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [2:16:58<30:36:10, 1184.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [2:31:18<27:37:58, 1081.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [2:52:40<28:55:18, 1144.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [3:15:12<30:12:33, 1208.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [3:36:32<30:24:55, 1230.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [3:51:30<27:36:02, 1129.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [4:08:26<26:27:29, 1094.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [4:21:25<23:52:26, 999.37s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [4:38:56<23:57:47, 1014.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [4:58:07<24:38:12, 1055.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [5:15:16<24:09:29, 1047.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [5:36:35<25:27:07, 1117.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [5:55:39<25:19:11, 1125.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [6:16:58<26:01:54, 1171.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [6:38:17<26:25:05, 1203.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [6:59:35<26:33:59, 1226.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [7:20:53<26:33:25, 1241.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [7:39:13<25:18:39, 1198.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [7:58:36<24:45:24, 1188.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [8:18:32<24:28:28, 1190.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [8:39:46<24:38:49, 1215.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [9:01:36<24:52:35, 1243.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [9:22:56<24:44:55, 1254.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|███       | 30/100 [9:41:30<23:34:48, 1212.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 31%|███       | 31/100 [10:01:58<23:19:36, 1217.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 32%|███▏      | 32/100 [10:21:19<22:40:24, 1200.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 33%|███▎      | 33/100 [10:42:40<22:47:27, 1224.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 34%|███▍      | 34/100 [11:04:05<22:46:49, 1242.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 35%|███▌      | 35/100 [11:25:51<22:46:58, 1261.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 36%|███▌      | 36/100 [11:47:49<22:43:38, 1278.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 37%|███▋      | 37/100 [12:07:17<21:47:34, 1245.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 38%|███▊      | 38/100 [12:28:38<21:37:50, 1255.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 39%|███▉      | 39/100 [12:46:33<20:21:48, 1201.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 40%|████      | 40/100 [13:08:45<20:40:51, 1240.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 41%|████      | 41/100 [13:30:59<20:47:36, 1268.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -11830.412674212863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20551.690043206385
Iteration 100: Loss = -12354.6305739609
Iteration 200: Loss = -12354.370423670527
Iteration 300: Loss = -12354.301251739686
Iteration 400: Loss = -12354.199618016688
Iteration 500: Loss = -12354.002077961188
Iteration 600: Loss = -12353.770630204244
Iteration 700: Loss = -12353.633148697963
Iteration 800: Loss = -12353.543866628597
Iteration 900: Loss = -12353.500856954002
Iteration 1000: Loss = -12353.486631416476
Iteration 1100: Loss = -12353.479916930302
Iteration 1200: Loss = -12353.475110163961
Iteration 1300: Loss = -12353.470912307668
Iteration 1400: Loss = -12353.466923840946
Iteration 1500: Loss = -12353.462900515846
Iteration 1600: Loss = -12353.458865427452
Iteration 1700: Loss = -12353.45473282721
Iteration 1800: Loss = -12353.450641890684
Iteration 1900: Loss = -12353.446619322065
Iteration 2000: Loss = -12353.442868374146
Iteration 2100: Loss = -12353.43948142267
Iteration 2200: Loss = -12353.436360393036
Iteration 2300: Loss = -12353.433534704858
Iteration 2400: Loss = -12353.430980308936
Iteration 2500: Loss = -12353.428661114376
Iteration 2600: Loss = -12353.426571503367
Iteration 2700: Loss = -12353.424674146163
Iteration 2800: Loss = -12353.422905640122
Iteration 2900: Loss = -12353.42130333936
Iteration 3000: Loss = -12353.419823134123
Iteration 3100: Loss = -12353.418464036497
Iteration 3200: Loss = -12353.417172552772
Iteration 3300: Loss = -12353.416004571856
Iteration 3400: Loss = -12353.414910642256
Iteration 3500: Loss = -12353.413942115656
Iteration 3600: Loss = -12353.412963044837
Iteration 3700: Loss = -12353.412122661555
Iteration 3800: Loss = -12353.411278056961
Iteration 3900: Loss = -12353.41053129944
Iteration 4000: Loss = -12353.409862536364
Iteration 4100: Loss = -12353.409196262735
Iteration 4200: Loss = -12353.408620645188
Iteration 4300: Loss = -12353.408043651432
Iteration 4400: Loss = -12353.407535562848
Iteration 4500: Loss = -12353.406993817378
Iteration 4600: Loss = -12353.406615773822
Iteration 4700: Loss = -12353.40617911571
Iteration 4800: Loss = -12353.405797316778
Iteration 4900: Loss = -12353.405406120195
Iteration 5000: Loss = -12353.405114875464
Iteration 5100: Loss = -12353.404757619544
Iteration 5200: Loss = -12353.40448702614
Iteration 5300: Loss = -12353.40423578873
Iteration 5400: Loss = -12353.40393486987
Iteration 5500: Loss = -12353.403766591538
Iteration 5600: Loss = -12353.403504384678
Iteration 5700: Loss = -12353.403309796413
Iteration 5800: Loss = -12353.403699817409
1
Iteration 5900: Loss = -12353.402985465662
Iteration 6000: Loss = -12353.404177263277
1
Iteration 6100: Loss = -12353.402651292276
Iteration 6200: Loss = -12353.402477674206
Iteration 6300: Loss = -12353.402323251998
Iteration 6400: Loss = -12353.402219428952
Iteration 6500: Loss = -12353.402351907082
1
Iteration 6600: Loss = -12353.40198223504
Iteration 6700: Loss = -12353.407599963917
1
Iteration 6800: Loss = -12353.401809861116
Iteration 6900: Loss = -12353.402346914883
1
Iteration 7000: Loss = -12353.40158922772
Iteration 7100: Loss = -12353.404916427755
1
Iteration 7200: Loss = -12353.40145117684
Iteration 7300: Loss = -12353.401432230365
Iteration 7400: Loss = -12353.401700739163
1
Iteration 7500: Loss = -12353.415186624024
2
Iteration 7600: Loss = -12353.401188569256
Iteration 7700: Loss = -12353.401401239982
1
Iteration 7800: Loss = -12353.401052528734
Iteration 7900: Loss = -12353.40112945738
Iteration 8000: Loss = -12353.400930471913
Iteration 8100: Loss = -12353.400890811887
Iteration 8200: Loss = -12353.403055259743
1
Iteration 8300: Loss = -12353.400830866938
Iteration 8400: Loss = -12353.400743800616
Iteration 8500: Loss = -12353.400935923517
1
Iteration 8600: Loss = -12353.400702797597
Iteration 8700: Loss = -12353.40067817551
Iteration 8800: Loss = -12353.400655370167
Iteration 8900: Loss = -12353.400680834635
Iteration 9000: Loss = -12353.400576779577
Iteration 9100: Loss = -12353.400579280957
Iteration 9200: Loss = -12353.400682545138
1
Iteration 9300: Loss = -12353.4005133653
Iteration 9400: Loss = -12353.400475954686
Iteration 9500: Loss = -12353.401890034727
1
Iteration 9600: Loss = -12353.40043826593
Iteration 9700: Loss = -12353.40043638882
Iteration 9800: Loss = -12353.4057553967
1
Iteration 9900: Loss = -12353.400398592223
Iteration 10000: Loss = -12353.400384534756
Iteration 10100: Loss = -12353.400362285814
Iteration 10200: Loss = -12353.400399034834
Iteration 10300: Loss = -12353.400323894431
Iteration 10400: Loss = -12353.40032718406
Iteration 10500: Loss = -12353.400512954095
1
Iteration 10600: Loss = -12353.400272794663
Iteration 10700: Loss = -12353.40028020282
Iteration 10800: Loss = -12353.409249122167
1
Iteration 10900: Loss = -12353.400267009445
Iteration 11000: Loss = -12353.400265849654
Iteration 11100: Loss = -12353.545824763472
1
Iteration 11200: Loss = -12353.400267123036
Iteration 11300: Loss = -12353.400204515096
Iteration 11400: Loss = -12353.420204914435
1
Iteration 11500: Loss = -12353.40025194729
Iteration 11600: Loss = -12353.400216853632
Iteration 11700: Loss = -12353.40018709833
Iteration 11800: Loss = -12353.401549176397
1
Iteration 11900: Loss = -12353.400186731616
Iteration 12000: Loss = -12353.400191175935
Iteration 12100: Loss = -12353.434301175284
1
Iteration 12200: Loss = -12353.400171427582
Iteration 12300: Loss = -12353.400345020389
1
Iteration 12400: Loss = -12353.413868113781
2
Iteration 12500: Loss = -12353.400300062709
3
Iteration 12600: Loss = -12353.400150516425
Iteration 12700: Loss = -12353.421838735
1
Iteration 12800: Loss = -12353.400146302902
Iteration 12900: Loss = -12353.400116821222
Iteration 13000: Loss = -12353.421171286982
1
Iteration 13100: Loss = -12353.400113705115
Iteration 13200: Loss = -12353.400121123992
Iteration 13300: Loss = -12353.40020081519
Iteration 13400: Loss = -12353.400165653373
Iteration 13500: Loss = -12353.492868933743
1
Iteration 13600: Loss = -12353.400119379961
Iteration 13700: Loss = -12353.401365520345
1
Iteration 13800: Loss = -12353.400122292807
Iteration 13900: Loss = -12353.410383313048
1
Iteration 14000: Loss = -12353.400088016446
Iteration 14100: Loss = -12353.400077348559
Iteration 14200: Loss = -12353.402247946522
1
Iteration 14300: Loss = -12353.40012301284
Iteration 14400: Loss = -12353.400082199161
Iteration 14500: Loss = -12353.410435596083
1
Iteration 14600: Loss = -12353.400077051965
Iteration 14700: Loss = -12353.400131735607
Iteration 14800: Loss = -12353.400510627805
1
Iteration 14900: Loss = -12353.40008590694
Iteration 15000: Loss = -12353.40031712253
1
Iteration 15100: Loss = -12353.400079885258
Iteration 15200: Loss = -12353.40066207441
1
Iteration 15300: Loss = -12353.400081075644
Iteration 15400: Loss = -12353.64296405149
1
Iteration 15500: Loss = -12353.400075147689
Iteration 15600: Loss = -12353.409528737122
1
Iteration 15700: Loss = -12353.400087257036
Iteration 15800: Loss = -12353.401644503188
1
Iteration 15900: Loss = -12353.400073086175
Iteration 16000: Loss = -12353.41999282872
1
Iteration 16100: Loss = -12353.40009288131
Iteration 16200: Loss = -12353.400082771544
Iteration 16300: Loss = -12353.403938631442
1
Iteration 16400: Loss = -12353.400090199482
Iteration 16500: Loss = -12353.421047532534
1
Iteration 16600: Loss = -12353.400157191225
Iteration 16700: Loss = -12353.405813958114
1
Iteration 16800: Loss = -12353.400093506765
Iteration 16900: Loss = -12353.777993767657
1
Iteration 17000: Loss = -12353.40005686812
Iteration 17100: Loss = -12353.400072773928
Iteration 17200: Loss = -12353.40057497699
1
Iteration 17300: Loss = -12353.400082185044
Iteration 17400: Loss = -12353.402861596815
1
Iteration 17500: Loss = -12353.400360464972
2
Iteration 17600: Loss = -12353.400094783996
Iteration 17700: Loss = -12353.400101584773
Iteration 17800: Loss = -12353.40011784108
Iteration 17900: Loss = -12353.400090497456
Iteration 18000: Loss = -12353.40007746602
Iteration 18100: Loss = -12353.419301588729
1
Iteration 18200: Loss = -12353.400063637107
Iteration 18300: Loss = -12353.400085068692
Iteration 18400: Loss = -12353.452393027274
1
Iteration 18500: Loss = -12353.400106784722
Iteration 18600: Loss = -12353.405873686292
1
Iteration 18700: Loss = -12353.400125647106
Iteration 18800: Loss = -12353.631773235176
1
Iteration 18900: Loss = -12353.400087621589
Iteration 19000: Loss = -12353.40752264769
1
Iteration 19100: Loss = -12353.400065178761
Iteration 19200: Loss = -12353.400176946114
1
Iteration 19300: Loss = -12353.400080482834
Iteration 19400: Loss = -12353.400051455083
Iteration 19500: Loss = -12353.401937456129
1
Iteration 19600: Loss = -12353.400094940096
Iteration 19700: Loss = -12353.40004564711
Iteration 19800: Loss = -12353.402851676365
1
Iteration 19900: Loss = -12353.400043514694
pi: tensor([[9.7460e-01, 2.5399e-02],
        [1.0000e+00, 2.0043e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9848, 0.0152], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.1448],
         [0.6500, 0.2651]],

        [[0.5767, 0.2915],
         [0.5849, 0.5139]],

        [[0.5249, 0.2079],
         [0.6761, 0.6256]],

        [[0.5628, 0.2389],
         [0.5521, 0.6075]],

        [[0.5068, 0.2354],
         [0.5230, 0.5998]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 9.716743485269148e-05
Average Adjusted Rand Index: -0.0011027420863486436
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21288.48268605224
Iteration 100: Loss = -12355.152896620759
Iteration 200: Loss = -12354.739329373819
Iteration 300: Loss = -12354.593023945758
Iteration 400: Loss = -12354.492835125797
Iteration 500: Loss = -12354.397051892882
Iteration 600: Loss = -12354.25654382398
Iteration 700: Loss = -12354.025074257177
Iteration 800: Loss = -12353.801178962987
Iteration 900: Loss = -12353.637394062436
Iteration 1000: Loss = -12353.545493366391
Iteration 1100: Loss = -12353.511455012953
Iteration 1200: Loss = -12353.49621314828
Iteration 1300: Loss = -12353.486404122841
Iteration 1400: Loss = -12353.47715230036
Iteration 1500: Loss = -12353.46261173051
Iteration 1600: Loss = -12353.40434403686
Iteration 1700: Loss = -12352.598520078442
Iteration 1800: Loss = -12352.348963944632
Iteration 1900: Loss = -12352.199793626392
Iteration 2000: Loss = -12352.100287217712
Iteration 2100: Loss = -12352.036299693747
Iteration 2200: Loss = -12351.995564353232
Iteration 2300: Loss = -12351.969250253385
Iteration 2400: Loss = -12351.951757953202
Iteration 2500: Loss = -12351.93979417074
Iteration 2600: Loss = -12351.931351716406
Iteration 2700: Loss = -12351.925215549836
Iteration 2800: Loss = -12351.92063104953
Iteration 2900: Loss = -12351.917195066637
Iteration 3000: Loss = -12351.914508436012
Iteration 3100: Loss = -12351.912423297497
Iteration 3200: Loss = -12351.910738235323
Iteration 3300: Loss = -12351.909351824257
Iteration 3400: Loss = -12351.908250345108
Iteration 3500: Loss = -12351.907336744503
Iteration 3600: Loss = -12351.906582575122
Iteration 3700: Loss = -12351.9059029505
Iteration 3800: Loss = -12351.905387088274
Iteration 3900: Loss = -12351.904912848358
Iteration 4000: Loss = -12351.904487079746
Iteration 4100: Loss = -12351.904186498736
Iteration 4200: Loss = -12351.903841234807
Iteration 4300: Loss = -12351.903587643235
Iteration 4400: Loss = -12351.903361847417
Iteration 4500: Loss = -12351.903133762375
Iteration 4600: Loss = -12351.902973615826
Iteration 4700: Loss = -12351.902860560587
Iteration 4800: Loss = -12351.902705669576
Iteration 4900: Loss = -12351.902580551041
Iteration 5000: Loss = -12351.902414425964
Iteration 5100: Loss = -12351.902389854513
Iteration 5200: Loss = -12351.902257481137
Iteration 5300: Loss = -12351.902204714292
Iteration 5400: Loss = -12351.902146573348
Iteration 5500: Loss = -12351.902074218053
Iteration 5600: Loss = -12351.902009563644
Iteration 5700: Loss = -12351.901929233743
Iteration 5800: Loss = -12351.901940780357
Iteration 5900: Loss = -12351.90185242606
Iteration 6000: Loss = -12351.90179593576
Iteration 6100: Loss = -12351.90179833953
Iteration 6200: Loss = -12351.901750491064
Iteration 6300: Loss = -12351.90177872125
Iteration 6400: Loss = -12351.901683547412
Iteration 6500: Loss = -12351.901669373341
Iteration 6600: Loss = -12351.902039674673
1
Iteration 6700: Loss = -12351.901631177947
Iteration 6800: Loss = -12351.901630797052
Iteration 6900: Loss = -12351.901617074716
Iteration 7000: Loss = -12351.901569155607
Iteration 7100: Loss = -12351.901748722592
1
Iteration 7200: Loss = -12351.901529695639
Iteration 7300: Loss = -12351.90158012277
Iteration 7400: Loss = -12351.901543683247
Iteration 7500: Loss = -12351.901710199589
1
Iteration 7600: Loss = -12351.908148573499
2
Iteration 7700: Loss = -12351.901489347165
Iteration 7800: Loss = -12351.902122217974
1
Iteration 7900: Loss = -12351.901516628894
Iteration 8000: Loss = -12351.902830881572
1
Iteration 8100: Loss = -12351.901516868365
Iteration 8200: Loss = -12351.902492251178
1
Iteration 8300: Loss = -12351.90146314808
Iteration 8400: Loss = -12351.901491943123
Iteration 8500: Loss = -12351.960327532024
1
Iteration 8600: Loss = -12351.901499806387
Iteration 8700: Loss = -12351.901449575551
Iteration 8800: Loss = -12351.902013294342
1
Iteration 8900: Loss = -12351.901458346689
Iteration 9000: Loss = -12351.901429216852
Iteration 9100: Loss = -12352.284933397148
1
Iteration 9200: Loss = -12351.901474798
Iteration 9300: Loss = -12351.901462412112
Iteration 9400: Loss = -12351.973784960785
1
Iteration 9500: Loss = -12351.901434657879
Iteration 9600: Loss = -12351.901431289049
Iteration 9700: Loss = -12351.910827083238
1
Iteration 9800: Loss = -12351.901478605576
Iteration 9900: Loss = -12351.90140620151
Iteration 10000: Loss = -12351.92064393322
1
Iteration 10100: Loss = -12351.901455816667
Iteration 10200: Loss = -12351.901441462223
Iteration 10300: Loss = -12352.025569123083
1
Iteration 10400: Loss = -12351.90147385077
Iteration 10500: Loss = -12351.901449315654
Iteration 10600: Loss = -12352.039917784297
1
Iteration 10700: Loss = -12351.90140933998
Iteration 10800: Loss = -12351.901398444765
Iteration 10900: Loss = -12351.90527467295
1
Iteration 11000: Loss = -12351.901427676441
Iteration 11100: Loss = -12351.901409521823
Iteration 11200: Loss = -12351.948119752084
1
Iteration 11300: Loss = -12351.901409695316
Iteration 11400: Loss = -12351.901392120348
Iteration 11500: Loss = -12351.907076221112
1
Iteration 11600: Loss = -12351.901435699923
Iteration 11700: Loss = -12351.901413613743
Iteration 11800: Loss = -12351.93658978271
1
Iteration 11900: Loss = -12351.901461073212
Iteration 12000: Loss = -12351.901404373913
Iteration 12100: Loss = -12351.9021353449
1
Iteration 12200: Loss = -12351.901415810018
Iteration 12300: Loss = -12352.151413240967
1
Iteration 12400: Loss = -12351.90139890705
Iteration 12500: Loss = -12351.999031797672
1
Iteration 12600: Loss = -12351.901429263862
Iteration 12700: Loss = -12351.919482616926
1
Iteration 12800: Loss = -12351.901454555737
Iteration 12900: Loss = -12352.027469639988
1
Iteration 13000: Loss = -12351.90141595342
Iteration 13100: Loss = -12351.909642740962
1
Iteration 13200: Loss = -12351.901429257383
Iteration 13300: Loss = -12351.901567402418
1
Iteration 13400: Loss = -12351.901453373659
Iteration 13500: Loss = -12351.901409678381
Iteration 13600: Loss = -12351.902074577401
1
Iteration 13700: Loss = -12351.9038675543
2
Iteration 13800: Loss = -12351.90142579967
Iteration 13900: Loss = -12351.901505841237
Iteration 14000: Loss = -12351.901642297875
1
Iteration 14100: Loss = -12351.982777243815
2
Iteration 14200: Loss = -12351.901428742083
Iteration 14300: Loss = -12351.949795649367
1
Iteration 14400: Loss = -12351.90363637669
2
Iteration 14500: Loss = -12351.90148545758
Iteration 14600: Loss = -12351.920182180207
1
Iteration 14700: Loss = -12351.901422891455
Iteration 14800: Loss = -12352.274708892295
1
Iteration 14900: Loss = -12351.901424158572
Iteration 15000: Loss = -12351.901426170964
Iteration 15100: Loss = -12351.901511185139
Iteration 15200: Loss = -12351.90143815948
Iteration 15300: Loss = -12351.920424965449
1
Iteration 15400: Loss = -12351.902049098984
2
Iteration 15500: Loss = -12351.90148421496
Iteration 15600: Loss = -12351.928033062735
1
Iteration 15700: Loss = -12351.901446969554
Iteration 15800: Loss = -12352.220776052474
1
Iteration 15900: Loss = -12351.901429530626
Iteration 16000: Loss = -12351.909374899818
1
Iteration 16100: Loss = -12351.910869971156
2
Iteration 16200: Loss = -12351.974941338618
3
Iteration 16300: Loss = -12351.90144739795
Iteration 16400: Loss = -12351.910630302456
1
Iteration 16500: Loss = -12351.90145317162
Iteration 16600: Loss = -12351.906846917802
1
Iteration 16700: Loss = -12351.90144846952
Iteration 16800: Loss = -12351.901560983846
1
Iteration 16900: Loss = -12351.916733233125
2
Iteration 17000: Loss = -12351.902704501952
3
Iteration 17100: Loss = -12351.901419887557
Iteration 17200: Loss = -12351.906159485457
1
Iteration 17300: Loss = -12351.927472202904
2
Iteration 17400: Loss = -12351.902457630044
3
Iteration 17500: Loss = -12351.901493766974
Iteration 17600: Loss = -12351.973709187208
1
Iteration 17700: Loss = -12351.90143130429
Iteration 17800: Loss = -12351.90762627453
1
Iteration 17900: Loss = -12351.923130571819
2
Iteration 18000: Loss = -12351.902983730344
3
Iteration 18100: Loss = -12351.915393293335
4
Iteration 18200: Loss = -12351.931100323609
5
Iteration 18300: Loss = -12351.905012606372
6
Iteration 18400: Loss = -12351.901534877203
7
Iteration 18500: Loss = -12351.909037357187
8
Iteration 18600: Loss = -12351.901863098354
9
Iteration 18700: Loss = -12351.901590521715
10
Iteration 18800: Loss = -12351.901410944676
Iteration 18900: Loss = -12351.912663270568
1
Iteration 19000: Loss = -12351.901406200166
Iteration 19100: Loss = -12351.901788590616
1
Iteration 19200: Loss = -12351.901534557339
2
Iteration 19300: Loss = -12351.901550717366
3
Iteration 19400: Loss = -12351.907928228957
4
Iteration 19500: Loss = -12351.901468411246
Iteration 19600: Loss = -12352.034255103981
1
Iteration 19700: Loss = -12351.901411777262
Iteration 19800: Loss = -12351.912060706687
1
Iteration 19900: Loss = -12351.901433098712
pi: tensor([[0.9811, 0.0189],
        [0.4004, 0.5996]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9878, 0.0122], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1964, 0.3100],
         [0.6454, 0.2558]],

        [[0.6554, 0.3007],
         [0.5873, 0.6234]],

        [[0.5990, 0.1996],
         [0.5444, 0.6653]],

        [[0.5396, 0.2392],
         [0.5769, 0.5232]],

        [[0.6286, 0.2304],
         [0.6810, 0.6143]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00038770756378241793
Average Adjusted Rand Index: -0.0024956371249350872
11830.412674212863
[9.716743485269148e-05, 0.00038770756378241793] [-0.0011027420863486436, -0.0024956371249350872] [12353.40006023944, 12351.916523201664]
-------------------------------------
This iteration is 1
True Objective function: Loss = -11960.981279873846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20300.33951439241
Iteration 100: Loss = -12434.326358475897
Iteration 200: Loss = -12433.725685872007
Iteration 300: Loss = -12433.554368815698
Iteration 400: Loss = -12433.439036217853
Iteration 500: Loss = -12433.337588778955
Iteration 600: Loss = -12433.223000109867
Iteration 700: Loss = -12433.052382822912
Iteration 800: Loss = -12432.48150800526
Iteration 900: Loss = -12430.238663751348
Iteration 1000: Loss = -12429.420108773249
Iteration 1100: Loss = -12428.807015938757
Iteration 1200: Loss = -12428.156446857172
Iteration 1300: Loss = -12427.527126088533
Iteration 1400: Loss = -12425.964415437345
Iteration 1500: Loss = -12422.131442671463
Iteration 1600: Loss = -12420.13049244515
Iteration 1700: Loss = -12406.887102270208
Iteration 1800: Loss = -12357.851625941164
Iteration 1900: Loss = -12334.66566009646
Iteration 2000: Loss = -12095.699525965134
Iteration 2100: Loss = -11992.183856793312
Iteration 2200: Loss = -11991.709663098214
Iteration 2300: Loss = -11991.309208659139
Iteration 2400: Loss = -11991.239787691926
Iteration 2500: Loss = -11984.10573878724
Iteration 2600: Loss = -11979.113865267642
Iteration 2700: Loss = -11978.742881739885
Iteration 2800: Loss = -11978.72249333493
Iteration 2900: Loss = -11978.700674459385
Iteration 3000: Loss = -11978.56649142378
Iteration 3100: Loss = -11978.583047161319
1
Iteration 3200: Loss = -11978.54994521941
Iteration 3300: Loss = -11978.543621991203
Iteration 3400: Loss = -11978.538424658493
Iteration 3500: Loss = -11978.538399379662
Iteration 3600: Loss = -11978.529572353033
Iteration 3700: Loss = -11978.527557786418
Iteration 3800: Loss = -11978.523165079101
Iteration 3900: Loss = -11978.532171782803
1
Iteration 4000: Loss = -11978.51757460838
Iteration 4100: Loss = -11978.515095646166
Iteration 4200: Loss = -11978.512803811931
Iteration 4300: Loss = -11978.509268818661
Iteration 4400: Loss = -11978.507249625283
Iteration 4500: Loss = -11978.505656871886
Iteration 4600: Loss = -11978.505386670238
Iteration 4700: Loss = -11978.502202845464
Iteration 4800: Loss = -11978.503772933114
1
Iteration 4900: Loss = -11978.495584166196
Iteration 5000: Loss = -11978.494649857788
Iteration 5100: Loss = -11978.495887362169
1
Iteration 5200: Loss = -11978.49410050274
Iteration 5300: Loss = -11978.492227642539
Iteration 5400: Loss = -11978.491819638693
Iteration 5500: Loss = -11978.493003231091
1
Iteration 5600: Loss = -11978.490183578193
Iteration 5700: Loss = -11978.489623665708
Iteration 5800: Loss = -11978.488768862333
Iteration 5900: Loss = -11978.488425546773
Iteration 6000: Loss = -11978.49256869299
1
Iteration 6100: Loss = -11978.487534287551
Iteration 6200: Loss = -11978.486453346124
Iteration 6300: Loss = -11978.486497520924
Iteration 6400: Loss = -11978.4860858527
Iteration 6500: Loss = -11978.479818281176
Iteration 6600: Loss = -11978.479605717668
Iteration 6700: Loss = -11978.479130897458
Iteration 6800: Loss = -11978.478678614516
Iteration 6900: Loss = -11978.478111518816
Iteration 7000: Loss = -11978.475618312104
Iteration 7100: Loss = -11978.47360929753
Iteration 7200: Loss = -11978.472960436853
Iteration 7300: Loss = -11978.500314286393
1
Iteration 7400: Loss = -11978.474539982242
2
Iteration 7500: Loss = -11978.472156739204
Iteration 7600: Loss = -11978.476173621397
1
Iteration 7700: Loss = -11978.470068818415
Iteration 7800: Loss = -11978.469840013704
Iteration 7900: Loss = -11978.469906738896
Iteration 8000: Loss = -11978.469355342408
Iteration 8100: Loss = -11978.469420820413
Iteration 8200: Loss = -11978.471407200685
1
Iteration 8300: Loss = -11978.477211703146
2
Iteration 8400: Loss = -11978.471081283009
3
Iteration 8500: Loss = -11978.485490865598
4
Iteration 8600: Loss = -11978.472544208054
5
Iteration 8700: Loss = -11978.46838764469
Iteration 8800: Loss = -11978.467944572429
Iteration 8900: Loss = -11978.469312458965
1
Iteration 9000: Loss = -11978.474839232684
2
Iteration 9100: Loss = -11978.468062992073
3
Iteration 9200: Loss = -11978.467861046202
Iteration 9300: Loss = -11978.57944463757
1
Iteration 9400: Loss = -11978.477171723216
2
Iteration 9500: Loss = -11978.51472584622
3
Iteration 9600: Loss = -11978.504251424534
4
Iteration 9700: Loss = -11978.467457018518
Iteration 9800: Loss = -11978.467200580575
Iteration 9900: Loss = -11978.46738471484
1
Iteration 10000: Loss = -11978.4267473785
Iteration 10100: Loss = -11978.422043420758
Iteration 10200: Loss = -11978.456548471902
1
Iteration 10300: Loss = -11978.421533322187
Iteration 10400: Loss = -11978.421567562835
Iteration 10500: Loss = -11978.439828519764
1
Iteration 10600: Loss = -11978.42100905728
Iteration 10700: Loss = -11978.428631034865
1
Iteration 10800: Loss = -11978.423587969068
2
Iteration 10900: Loss = -11978.47141432647
3
Iteration 11000: Loss = -11978.435295013353
4
Iteration 11100: Loss = -11978.42766903563
5
Iteration 11200: Loss = -11978.441165579081
6
Iteration 11300: Loss = -11978.47031956845
7
Iteration 11400: Loss = -11978.419413393984
Iteration 11500: Loss = -11978.418983975975
Iteration 11600: Loss = -11978.421988899921
1
Iteration 11700: Loss = -11978.517004184221
2
Iteration 11800: Loss = -11978.421715641813
3
Iteration 11900: Loss = -11978.424923308241
4
Iteration 12000: Loss = -11978.557425536039
5
Iteration 12100: Loss = -11978.419959582008
6
Iteration 12200: Loss = -11978.43116417122
7
Iteration 12300: Loss = -11978.420409333983
8
Iteration 12400: Loss = -11978.42510520214
9
Iteration 12500: Loss = -11978.419279165853
10
Iteration 12600: Loss = -11978.42091255754
11
Iteration 12700: Loss = -11978.44420028621
12
Iteration 12800: Loss = -11978.418642033696
Iteration 12900: Loss = -11978.436803879798
1
Iteration 13000: Loss = -11978.451773758197
2
Iteration 13100: Loss = -11978.463933062072
3
Iteration 13200: Loss = -11978.427548725174
4
Iteration 13300: Loss = -11978.422736385237
5
Iteration 13400: Loss = -11978.425099346367
6
Iteration 13500: Loss = -11978.570825790868
7
Iteration 13600: Loss = -11978.426103917738
8
Iteration 13700: Loss = -11978.423293397962
9
Iteration 13800: Loss = -11978.422394528741
10
Iteration 13900: Loss = -11978.424350354317
11
Iteration 14000: Loss = -11978.41886872805
12
Iteration 14100: Loss = -11978.420442307895
13
Iteration 14200: Loss = -11978.446289648125
14
Iteration 14300: Loss = -11978.420599981238
15
Stopping early at iteration 14300 due to no improvement.
pi: tensor([[0.7260, 0.2740],
        [0.1787, 0.8213]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5636, 0.4364], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2940, 0.0993],
         [0.5334, 0.2965]],

        [[0.6016, 0.1046],
         [0.5974, 0.7189]],

        [[0.7098, 0.1175],
         [0.5166, 0.7072]],

        [[0.6689, 0.0960],
         [0.5190, 0.6109]],

        [[0.5680, 0.1063],
         [0.6995, 0.6237]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961941296443
Average Adjusted Rand Index: 0.976161616161616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19109.362225601566
Iteration 100: Loss = -12433.591699267272
Iteration 200: Loss = -12433.279793106209
Iteration 300: Loss = -12433.167243386375
Iteration 400: Loss = -12432.990844932383
Iteration 500: Loss = -12432.675077806503
Iteration 600: Loss = -12432.23607212225
Iteration 700: Loss = -12431.780671324432
Iteration 800: Loss = -12428.335214485098
Iteration 900: Loss = -12422.800860915244
Iteration 1000: Loss = -12418.768384496407
Iteration 1100: Loss = -12354.984723205143
Iteration 1200: Loss = -12123.113111823193
Iteration 1300: Loss = -11991.842746761813
Iteration 1400: Loss = -11983.808105238564
Iteration 1500: Loss = -11983.717207244943
Iteration 1600: Loss = -11983.662424537632
Iteration 1700: Loss = -11983.63036876282
Iteration 1800: Loss = -11978.711459063132
Iteration 1900: Loss = -11978.69668084462
Iteration 2000: Loss = -11978.686171074054
Iteration 2100: Loss = -11978.67775352895
Iteration 2200: Loss = -11978.671056664763
Iteration 2300: Loss = -11978.665501274914
Iteration 2400: Loss = -11978.660626827103
Iteration 2500: Loss = -11978.656116818878
Iteration 2600: Loss = -11978.65233146196
Iteration 2700: Loss = -11978.64948797529
Iteration 2800: Loss = -11978.64703354386
Iteration 2900: Loss = -11978.644726151633
Iteration 3000: Loss = -11978.642534384651
Iteration 3100: Loss = -11978.640298504672
Iteration 3200: Loss = -11978.649563713823
1
Iteration 3300: Loss = -11978.637620108337
Iteration 3400: Loss = -11978.636454312074
Iteration 3500: Loss = -11978.634615182
Iteration 3600: Loss = -11978.633100067267
Iteration 3700: Loss = -11978.631994657473
Iteration 3800: Loss = -11978.627357924535
Iteration 3900: Loss = -11978.497064457537
Iteration 4000: Loss = -11978.509433761985
1
Iteration 4100: Loss = -11978.496766725142
Iteration 4200: Loss = -11978.494899564408
Iteration 4300: Loss = -11978.494539332662
Iteration 4400: Loss = -11978.49401309252
Iteration 4500: Loss = -11978.493443700076
Iteration 4600: Loss = -11978.49666515018
1
Iteration 4700: Loss = -11978.493708535712
2
Iteration 4800: Loss = -11978.492319895278
Iteration 4900: Loss = -11978.490578053816
Iteration 5000: Loss = -11978.486731569557
Iteration 5100: Loss = -11978.490680160176
1
Iteration 5200: Loss = -11978.49389109738
2
Iteration 5300: Loss = -11978.485145149976
Iteration 5400: Loss = -11978.485738049854
1
Iteration 5500: Loss = -11978.485808718156
2
Iteration 5600: Loss = -11978.485723926065
3
Iteration 5700: Loss = -11978.484390605234
Iteration 5800: Loss = -11978.484516857252
1
Iteration 5900: Loss = -11978.48455392954
2
Iteration 6000: Loss = -11978.479518379796
Iteration 6100: Loss = -11978.487221305682
1
Iteration 6200: Loss = -11978.477792206944
Iteration 6300: Loss = -11978.47762956116
Iteration 6400: Loss = -11978.487330072394
1
Iteration 6500: Loss = -11978.477216155441
Iteration 6600: Loss = -11978.478384896307
1
Iteration 6700: Loss = -11978.502966268768
2
Iteration 6800: Loss = -11978.47884602421
3
Iteration 6900: Loss = -11978.476410063773
Iteration 7000: Loss = -11978.476054882714
Iteration 7100: Loss = -11978.481902796626
1
Iteration 7200: Loss = -11978.474742557823
Iteration 7300: Loss = -11978.47453140225
Iteration 7400: Loss = -11978.473507239167
Iteration 7500: Loss = -11978.515687961812
1
Iteration 7600: Loss = -11978.472584111709
Iteration 7700: Loss = -11978.477284852075
1
Iteration 7800: Loss = -11978.47244703053
Iteration 7900: Loss = -11978.47268220208
1
Iteration 8000: Loss = -11978.472314388875
Iteration 8100: Loss = -11978.478028672384
1
Iteration 8200: Loss = -11978.472475542703
2
Iteration 8300: Loss = -11978.473521541082
3
Iteration 8400: Loss = -11978.474871982246
4
Iteration 8500: Loss = -11978.530974678653
5
Iteration 8600: Loss = -11978.47198401025
Iteration 8700: Loss = -11978.472636197288
1
Iteration 8800: Loss = -11978.490623899697
2
Iteration 8900: Loss = -11978.478421181499
3
Iteration 9000: Loss = -11978.474764860248
4
Iteration 9100: Loss = -11978.484733541813
5
Iteration 9200: Loss = -11978.496889743516
6
Iteration 9300: Loss = -11978.477776345671
7
Iteration 9400: Loss = -11978.47728828001
8
Iteration 9500: Loss = -11978.494979445355
9
Iteration 9600: Loss = -11978.47261901178
10
Iteration 9700: Loss = -11978.472497687286
11
Iteration 9800: Loss = -11978.47341119402
12
Iteration 9900: Loss = -11978.485909574782
13
Iteration 10000: Loss = -11978.473065845146
14
Iteration 10100: Loss = -11978.473944558586
15
Stopping early at iteration 10100 due to no improvement.
pi: tensor([[0.7257, 0.2743],
        [0.1784, 0.8216]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5632, 0.4368], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2942, 0.0994],
         [0.5604, 0.2966]],

        [[0.5074, 0.1052],
         [0.5274, 0.6944]],

        [[0.5550, 0.1173],
         [0.5683, 0.5618]],

        [[0.6647, 0.0962],
         [0.6393, 0.6669]],

        [[0.6411, 0.1062],
         [0.6867, 0.7211]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961941296443
Average Adjusted Rand Index: 0.976161616161616
11960.981279873846
[0.9760961941296443, 0.9760961941296443] [0.976161616161616, 0.976161616161616] [11978.420599981238, 11978.473944558586]
-------------------------------------
This iteration is 2
True Objective function: Loss = -11921.074032787514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22551.30832982931
Iteration 100: Loss = -12429.376665447602
Iteration 200: Loss = -12428.747168152406
Iteration 300: Loss = -12427.802054185036
Iteration 400: Loss = -12427.011093503692
Iteration 500: Loss = -12426.570471790068
Iteration 600: Loss = -12426.262297617355
Iteration 700: Loss = -12426.019605788586
Iteration 800: Loss = -12425.820715859487
Iteration 900: Loss = -12425.657861145835
Iteration 1000: Loss = -12425.526124664217
Iteration 1100: Loss = -12425.41658428683
Iteration 1200: Loss = -12425.31586041815
Iteration 1300: Loss = -12425.207273869775
Iteration 1400: Loss = -12425.070440308398
Iteration 1500: Loss = -12424.885388686522
Iteration 1600: Loss = -12424.650760648272
Iteration 1700: Loss = -12424.3890563846
Iteration 1800: Loss = -12424.137619452938
Iteration 1900: Loss = -12423.932058005748
Iteration 2000: Loss = -12423.784027953609
Iteration 2100: Loss = -12423.684108233967
Iteration 2200: Loss = -12423.617485527022
Iteration 2300: Loss = -12423.572281715788
Iteration 2400: Loss = -12423.540686372851
Iteration 2500: Loss = -12423.517783814885
Iteration 2600: Loss = -12423.50076660664
Iteration 2700: Loss = -12423.487643347979
Iteration 2800: Loss = -12423.477300068822
Iteration 2900: Loss = -12423.46880143014
Iteration 3000: Loss = -12423.461788978999
Iteration 3100: Loss = -12423.45575470472
Iteration 3200: Loss = -12423.450371651816
Iteration 3300: Loss = -12423.44535783491
Iteration 3400: Loss = -12423.440391215858
Iteration 3500: Loss = -12423.435010886555
Iteration 3600: Loss = -12423.42827341143
Iteration 3700: Loss = -12423.418328385605
Iteration 3800: Loss = -12423.400927677123
Iteration 3900: Loss = -12423.371394519509
Iteration 4000: Loss = -12423.340979563796
Iteration 4100: Loss = -12423.325640974908
Iteration 4200: Loss = -12423.31947964543
Iteration 4300: Loss = -12423.31683752113
Iteration 4400: Loss = -12423.315265748432
Iteration 4500: Loss = -12423.314129670252
Iteration 4600: Loss = -12423.313222550025
Iteration 4700: Loss = -12423.327067250864
1
Iteration 4800: Loss = -12423.31173564417
Iteration 4900: Loss = -12423.311179468345
Iteration 5000: Loss = -12423.31065046708
Iteration 5100: Loss = -12423.31023707395
Iteration 5200: Loss = -12423.314581932886
1
Iteration 5300: Loss = -12423.309522435417
Iteration 5400: Loss = -12423.309159199609
Iteration 5500: Loss = -12423.308957042382
Iteration 5600: Loss = -12423.308662376494
Iteration 5700: Loss = -12423.308439843377
Iteration 5800: Loss = -12423.308244751217
Iteration 5900: Loss = -12423.308038047375
Iteration 6000: Loss = -12423.30898262942
1
Iteration 6100: Loss = -12423.307753435867
Iteration 6200: Loss = -12423.307566498075
Iteration 6300: Loss = -12423.30743454509
Iteration 6400: Loss = -12423.307336153746
Iteration 6500: Loss = -12423.307216065788
Iteration 6600: Loss = -12423.307093091802
Iteration 6700: Loss = -12423.307015930042
Iteration 6800: Loss = -12423.309394837317
1
Iteration 6900: Loss = -12423.306820904285
Iteration 7000: Loss = -12423.306745089087
Iteration 7100: Loss = -12423.306687196566
Iteration 7200: Loss = -12423.307479506171
1
Iteration 7300: Loss = -12423.306575822038
Iteration 7400: Loss = -12423.306507381285
Iteration 7500: Loss = -12423.306880058926
1
Iteration 7600: Loss = -12423.306412148835
Iteration 7700: Loss = -12423.306340107227
Iteration 7800: Loss = -12423.307320637072
1
Iteration 7900: Loss = -12423.306492258638
2
Iteration 8000: Loss = -12423.306219304432
Iteration 8100: Loss = -12423.309145505029
1
Iteration 8200: Loss = -12423.313420537997
2
Iteration 8300: Loss = -12423.381889178296
3
Iteration 8400: Loss = -12423.333696963615
4
Iteration 8500: Loss = -12423.306344214363
5
Iteration 8600: Loss = -12423.308150797318
6
Iteration 8700: Loss = -12423.315763225813
7
Iteration 8800: Loss = -12423.306414434144
8
Iteration 8900: Loss = -12423.306033715875
Iteration 9000: Loss = -12423.314876419734
1
Iteration 9100: Loss = -12423.306912295238
2
Iteration 9200: Loss = -12423.308382920572
3
Iteration 9300: Loss = -12423.305882818753
Iteration 9400: Loss = -12423.308643123679
1
Iteration 9500: Loss = -12423.359396529351
2
Iteration 9600: Loss = -12423.30594128727
Iteration 9700: Loss = -12423.34199271672
1
Iteration 9800: Loss = -12423.310070808717
2
Iteration 9900: Loss = -12423.306028184896
Iteration 10000: Loss = -12423.30679286094
1
Iteration 10100: Loss = -12423.353650789817
2
Iteration 10200: Loss = -12423.308241131384
3
Iteration 10300: Loss = -12423.310390870278
4
Iteration 10400: Loss = -12423.30578246492
Iteration 10500: Loss = -12423.305888679199
1
Iteration 10600: Loss = -12423.305769170911
Iteration 10700: Loss = -12423.31240739615
1
Iteration 10800: Loss = -12423.305672264718
Iteration 10900: Loss = -12423.308455566223
1
Iteration 11000: Loss = -12423.305711120338
Iteration 11100: Loss = -12423.313649478361
1
Iteration 11200: Loss = -12423.30565498052
Iteration 11300: Loss = -12423.317628830104
1
Iteration 11400: Loss = -12423.305617377979
Iteration 11500: Loss = -12423.306107559038
1
Iteration 11600: Loss = -12423.307696044412
2
Iteration 11700: Loss = -12423.315206251244
3
Iteration 11800: Loss = -12423.305664449785
Iteration 11900: Loss = -12423.308257456152
1
Iteration 12000: Loss = -12423.30559104974
Iteration 12100: Loss = -12423.30590711494
1
Iteration 12200: Loss = -12423.305683299574
Iteration 12300: Loss = -12423.305913694423
1
Iteration 12400: Loss = -12423.369312813498
2
Iteration 12500: Loss = -12423.30560045608
Iteration 12600: Loss = -12423.30865411902
1
Iteration 12700: Loss = -12423.309166645438
2
Iteration 12800: Loss = -12423.305884707752
3
Iteration 12900: Loss = -12423.305817983113
4
Iteration 13000: Loss = -12423.30591033802
5
Iteration 13100: Loss = -12423.447605481535
6
Iteration 13200: Loss = -12423.311951752843
7
Iteration 13300: Loss = -12423.305624111577
Iteration 13400: Loss = -12423.306687947541
1
Iteration 13500: Loss = -12423.310934279918
2
Iteration 13600: Loss = -12423.31158351766
3
Iteration 13700: Loss = -12423.490008112854
4
Iteration 13800: Loss = -12423.305631818706
Iteration 13900: Loss = -12423.31242439095
1
Iteration 14000: Loss = -12423.328789243564
2
Iteration 14100: Loss = -12423.307105250162
3
Iteration 14200: Loss = -12423.691284914172
4
Iteration 14300: Loss = -12423.3055210354
Iteration 14400: Loss = -12423.306621122745
1
Iteration 14500: Loss = -12423.305505825438
Iteration 14600: Loss = -12423.306836026091
1
Iteration 14700: Loss = -12423.34199227717
2
Iteration 14800: Loss = -12423.305519971123
Iteration 14900: Loss = -12423.305528693021
Iteration 15000: Loss = -12423.305477242535
Iteration 15100: Loss = -12423.305717339203
1
Iteration 15200: Loss = -12423.305508913867
Iteration 15300: Loss = -12423.305862785242
1
Iteration 15400: Loss = -12423.305488946036
Iteration 15500: Loss = -12423.309916463253
1
Iteration 15600: Loss = -12423.377981700984
2
Iteration 15700: Loss = -12423.305599659629
3
Iteration 15800: Loss = -12423.305664188107
4
Iteration 15900: Loss = -12423.30573110968
5
Iteration 16000: Loss = -12423.313569065092
6
Iteration 16100: Loss = -12423.56052381242
7
Iteration 16200: Loss = -12423.305493896534
Iteration 16300: Loss = -12423.307705180147
1
Iteration 16400: Loss = -12423.30737748079
2
Iteration 16500: Loss = -12423.307270335165
3
Iteration 16600: Loss = -12423.315783532102
4
Iteration 16700: Loss = -12423.305690970765
5
Iteration 16800: Loss = -12423.306985124602
6
Iteration 16900: Loss = -12423.305755075813
7
Iteration 17000: Loss = -12423.306505060682
8
Iteration 17100: Loss = -12423.37106705051
9
Iteration 17200: Loss = -12423.319734725768
10
Iteration 17300: Loss = -12423.305936149793
11
Iteration 17400: Loss = -12423.311607153399
12
Iteration 17500: Loss = -12423.306265040519
13
Iteration 17600: Loss = -12423.306699917839
14
Iteration 17700: Loss = -12423.309774722937
15
Stopping early at iteration 17700 due to no improvement.
pi: tensor([[0.8701, 0.1299],
        [0.9692, 0.0308]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.1611e-04, 9.9968e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1964, 0.2127],
         [0.5292, 0.2112]],

        [[0.7288, 0.3160],
         [0.6274, 0.5848]],

        [[0.5839, 0.1782],
         [0.6117, 0.5566]],

        [[0.5187, 0.2034],
         [0.6915, 0.5778]],

        [[0.6658, 0.2154],
         [0.6658, 0.7197]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014055496838748832
Average Adjusted Rand Index: -0.0015521208230856337
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20921.447641923143
Iteration 100: Loss = -12429.69598270455
Iteration 200: Loss = -12429.12337845516
Iteration 300: Loss = -12428.535261144614
Iteration 400: Loss = -12427.65403920212
Iteration 500: Loss = -12426.995466704302
Iteration 600: Loss = -12426.360720325532
Iteration 700: Loss = -12425.919355801338
Iteration 800: Loss = -12425.566653696209
Iteration 900: Loss = -12425.294553796313
Iteration 1000: Loss = -12425.022966711447
Iteration 1100: Loss = -12424.707084859376
Iteration 1200: Loss = -12424.351634721794
Iteration 1300: Loss = -12424.030443720674
Iteration 1400: Loss = -12423.805533617337
Iteration 1500: Loss = -12423.671393850445
Iteration 1600: Loss = -12423.594389384982
Iteration 1700: Loss = -12423.54830568092
Iteration 1800: Loss = -12423.51887653012
Iteration 1900: Loss = -12423.49891574368
Iteration 2000: Loss = -12423.484609498986
Iteration 2100: Loss = -12423.473869653553
Iteration 2200: Loss = -12423.465494065465
Iteration 2300: Loss = -12423.458711530198
Iteration 2400: Loss = -12423.4530320275
Iteration 2500: Loss = -12423.448222623585
Iteration 2600: Loss = -12423.443988230823
Iteration 2700: Loss = -12423.440094893505
Iteration 2800: Loss = -12423.436310323304
Iteration 2900: Loss = -12423.432328370396
Iteration 3000: Loss = -12423.427791667431
Iteration 3100: Loss = -12423.421896697364
Iteration 3200: Loss = -12423.412998211654
Iteration 3300: Loss = -12423.397708228336
Iteration 3400: Loss = -12423.371364985895
Iteration 3500: Loss = -12423.340456515667
Iteration 3600: Loss = -12423.32232428576
Iteration 3700: Loss = -12423.32529163311
1
Iteration 3800: Loss = -12423.313681531426
Iteration 3900: Loss = -12423.312378172533
Iteration 4000: Loss = -12423.311534119113
Iteration 4100: Loss = -12423.316936738544
1
Iteration 4200: Loss = -12423.310276134882
Iteration 4300: Loss = -12423.30976466282
Iteration 4400: Loss = -12423.309432575767
Iteration 4500: Loss = -12423.308999812203
Iteration 4600: Loss = -12423.31919183138
1
Iteration 4700: Loss = -12423.308433249249
Iteration 4800: Loss = -12423.308151138797
Iteration 4900: Loss = -12423.307942843312
Iteration 5000: Loss = -12423.307773477232
Iteration 5100: Loss = -12423.307586561847
Iteration 5200: Loss = -12423.30748552522
Iteration 5300: Loss = -12423.307308271545
Iteration 5400: Loss = -12423.307427563208
1
Iteration 5500: Loss = -12423.307084607903
Iteration 5600: Loss = -12423.307028880623
Iteration 5700: Loss = -12423.306914419136
Iteration 5800: Loss = -12423.306768789209
Iteration 5900: Loss = -12423.306813242863
Iteration 6000: Loss = -12423.30665188215
Iteration 6100: Loss = -12423.306748474944
Iteration 6200: Loss = -12423.306516551796
Iteration 6300: Loss = -12423.307170428325
1
Iteration 6400: Loss = -12423.306430815008
Iteration 6500: Loss = -12423.30646275403
Iteration 6600: Loss = -12423.306306058419
Iteration 6700: Loss = -12423.306259422288
Iteration 6800: Loss = -12423.306203056405
Iteration 6900: Loss = -12423.306164155834
Iteration 7000: Loss = -12423.306162491763
Iteration 7100: Loss = -12423.309164272256
1
Iteration 7200: Loss = -12423.306068772268
Iteration 7300: Loss = -12423.306299845919
1
Iteration 7400: Loss = -12423.306169950703
2
Iteration 7500: Loss = -12423.30598703973
Iteration 7600: Loss = -12423.30682429819
1
Iteration 7700: Loss = -12423.309540990453
2
Iteration 7800: Loss = -12423.305960100795
Iteration 7900: Loss = -12423.305939337202
Iteration 8000: Loss = -12423.305952880339
Iteration 8100: Loss = -12423.305833769715
Iteration 8200: Loss = -12423.306110904841
1
Iteration 8300: Loss = -12423.305835401334
Iteration 8400: Loss = -12423.305976743868
1
Iteration 8500: Loss = -12423.305846585596
Iteration 8600: Loss = -12423.470420439706
1
Iteration 8700: Loss = -12423.30580554722
Iteration 8800: Loss = -12423.306132218077
1
Iteration 8900: Loss = -12423.405483735649
2
Iteration 9000: Loss = -12423.30597888499
3
Iteration 9100: Loss = -12423.307998862963
4
Iteration 9200: Loss = -12423.307698521055
5
Iteration 9300: Loss = -12423.307666132398
6
Iteration 9400: Loss = -12423.30579577837
Iteration 9500: Loss = -12423.308826420784
1
Iteration 9600: Loss = -12423.319005974983
2
Iteration 9700: Loss = -12423.30620167553
3
Iteration 9800: Loss = -12423.306496555359
4
Iteration 9900: Loss = -12423.323533930414
5
Iteration 10000: Loss = -12423.30985817069
6
Iteration 10100: Loss = -12423.315813015412
7
Iteration 10200: Loss = -12423.305977784494
8
Iteration 10300: Loss = -12423.30569076387
Iteration 10400: Loss = -12423.309300189101
1
Iteration 10500: Loss = -12423.315266337573
2
Iteration 10600: Loss = -12423.308227529082
3
Iteration 10700: Loss = -12423.311297560145
4
Iteration 10800: Loss = -12423.307603168434
5
Iteration 10900: Loss = -12423.311592422546
6
Iteration 11000: Loss = -12423.307392953702
7
Iteration 11100: Loss = -12423.327097738947
8
Iteration 11200: Loss = -12423.30645677106
9
Iteration 11300: Loss = -12423.330487910627
10
Iteration 11400: Loss = -12423.30566713047
Iteration 11500: Loss = -12423.32405975136
1
Iteration 11600: Loss = -12423.343901317567
2
Iteration 11700: Loss = -12423.430616161111
3
Iteration 11800: Loss = -12423.305539554214
Iteration 11900: Loss = -12423.311759660359
1
Iteration 12000: Loss = -12423.30554464539
Iteration 12100: Loss = -12423.305874769283
1
Iteration 12200: Loss = -12423.305907801772
2
Iteration 12300: Loss = -12423.305940420021
3
Iteration 12400: Loss = -12423.395705046016
4
Iteration 12500: Loss = -12423.308839867846
5
Iteration 12600: Loss = -12423.305624537796
Iteration 12700: Loss = -12423.305545651721
Iteration 12800: Loss = -12423.311425539554
1
Iteration 12900: Loss = -12423.305542750917
Iteration 13000: Loss = -12423.306229505803
1
Iteration 13100: Loss = -12423.422985763436
2
Iteration 13200: Loss = -12423.318473675796
3
Iteration 13300: Loss = -12423.313520975189
4
Iteration 13400: Loss = -12423.345599167897
5
Iteration 13500: Loss = -12423.30550825915
Iteration 13600: Loss = -12423.314405021007
1
Iteration 13700: Loss = -12423.305480465795
Iteration 13800: Loss = -12423.305797372692
1
Iteration 13900: Loss = -12423.305485338109
Iteration 14000: Loss = -12423.306176233658
1
Iteration 14100: Loss = -12423.30550414083
Iteration 14200: Loss = -12423.306689777533
1
Iteration 14300: Loss = -12423.305500034257
Iteration 14400: Loss = -12423.315215120716
1
Iteration 14500: Loss = -12423.361974798845
2
Iteration 14600: Loss = -12423.467820300211
3
Iteration 14700: Loss = -12423.305430636481
Iteration 14800: Loss = -12423.306030695534
1
Iteration 14900: Loss = -12423.710139658584
2
Iteration 15000: Loss = -12423.305475434325
Iteration 15100: Loss = -12423.40777503187
1
Iteration 15200: Loss = -12423.30955764839
2
Iteration 15300: Loss = -12423.309251347855
3
Iteration 15400: Loss = -12423.310860987493
4
Iteration 15500: Loss = -12423.308608044683
5
Iteration 15600: Loss = -12423.320101262187
6
Iteration 15700: Loss = -12423.305507371784
Iteration 15800: Loss = -12423.307615755572
1
Iteration 15900: Loss = -12423.341595639757
2
Iteration 16000: Loss = -12423.305520128939
Iteration 16100: Loss = -12423.305653116557
1
Iteration 16200: Loss = -12423.305491262956
Iteration 16300: Loss = -12423.31100617247
1
Iteration 16400: Loss = -12423.321476798938
2
Iteration 16500: Loss = -12423.305590328753
Iteration 16600: Loss = -12423.31825186698
1
Iteration 16700: Loss = -12423.307660684679
2
Iteration 16800: Loss = -12423.309385912846
3
Iteration 16900: Loss = -12423.30872170255
4
Iteration 17000: Loss = -12423.30559984984
Iteration 17100: Loss = -12423.30666515774
1
Iteration 17200: Loss = -12423.306786755895
2
Iteration 17300: Loss = -12423.322474718047
3
Iteration 17400: Loss = -12423.34630238613
4
Iteration 17500: Loss = -12423.312001719165
5
Iteration 17600: Loss = -12423.305478514889
Iteration 17700: Loss = -12423.337961402596
1
Iteration 17800: Loss = -12423.305425666593
Iteration 17900: Loss = -12423.338417323826
1
Iteration 18000: Loss = -12423.318755700551
2
Iteration 18100: Loss = -12423.320043210028
3
Iteration 18200: Loss = -12423.306238571286
4
Iteration 18300: Loss = -12423.33678953766
5
Iteration 18400: Loss = -12423.305408063536
Iteration 18500: Loss = -12423.31655770665
1
Iteration 18600: Loss = -12423.305420942754
Iteration 18700: Loss = -12423.305431373252
Iteration 18800: Loss = -12423.305539694178
1
Iteration 18900: Loss = -12423.30546313662
Iteration 19000: Loss = -12423.306097350152
1
Iteration 19100: Loss = -12423.30662304458
2
Iteration 19200: Loss = -12423.305428599753
Iteration 19300: Loss = -12423.31111422204
1
Iteration 19400: Loss = -12423.305519239912
Iteration 19500: Loss = -12423.317869765477
1
Iteration 19600: Loss = -12423.305445642254
Iteration 19700: Loss = -12423.440281870764
1
Iteration 19800: Loss = -12423.305449304797
Iteration 19900: Loss = -12423.305459086292
pi: tensor([[0.0307, 0.9693],
        [0.1302, 0.8698]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9994e-01, 6.3252e-05], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2109, 0.2128],
         [0.6564, 0.1967]],

        [[0.6045, 0.3164],
         [0.6476, 0.6827]],

        [[0.6037, 0.1780],
         [0.7212, 0.5051]],

        [[0.7236, 0.2031],
         [0.5279, 0.6547]],

        [[0.6443, 0.2151],
         [0.5651, 0.5292]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014055496838748832
Average Adjusted Rand Index: -0.0015521208230856337
11921.074032787514
[-0.0014055496838748832, -0.0014055496838748832] [-0.0015521208230856337, -0.0015521208230856337] [12423.309774722937, 12423.305721255436]
-------------------------------------
This iteration is 3
True Objective function: Loss = -11912.63649173985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22667.699348873106
Iteration 100: Loss = -12446.824923700933
Iteration 200: Loss = -12446.166998886745
Iteration 300: Loss = -12445.935036680914
Iteration 400: Loss = -12445.833668314022
Iteration 500: Loss = -12445.774300282977
Iteration 600: Loss = -12445.730806153482
Iteration 700: Loss = -12445.694407784693
Iteration 800: Loss = -12445.661233488623
Iteration 900: Loss = -12445.62935649554
Iteration 1000: Loss = -12445.597556113478
Iteration 1100: Loss = -12445.565132369042
Iteration 1200: Loss = -12445.531601713537
Iteration 1300: Loss = -12445.496093527629
Iteration 1400: Loss = -12445.45777474691
Iteration 1500: Loss = -12445.415637544505
Iteration 1600: Loss = -12445.368680112459
Iteration 1700: Loss = -12445.316012631674
Iteration 1800: Loss = -12445.253701448728
Iteration 1900: Loss = -12445.151900894072
Iteration 2000: Loss = -12444.4541802457
Iteration 2100: Loss = -12443.978545463813
Iteration 2200: Loss = -12443.768019632644
Iteration 2300: Loss = -12443.64892499602
Iteration 2400: Loss = -12443.576682538773
Iteration 2500: Loss = -12443.529820446514
Iteration 2600: Loss = -12443.497152805792
Iteration 2700: Loss = -12443.472683797769
Iteration 2800: Loss = -12443.45294552372
Iteration 2900: Loss = -12443.435942707585
Iteration 3000: Loss = -12443.420534934248
Iteration 3100: Loss = -12443.406083710306
Iteration 3200: Loss = -12443.392221898559
Iteration 3300: Loss = -12443.37871486771
Iteration 3400: Loss = -12443.365714372443
Iteration 3500: Loss = -12443.353063544693
Iteration 3600: Loss = -12443.340766983802
Iteration 3700: Loss = -12443.329251723868
Iteration 3800: Loss = -12443.319559740868
Iteration 3900: Loss = -12443.310525845118
Iteration 4000: Loss = -12443.318317124922
1
Iteration 4100: Loss = -12443.296470156365
Iteration 4200: Loss = -12443.29076994827
Iteration 4300: Loss = -12443.286083249839
Iteration 4400: Loss = -12443.281728417545
Iteration 4500: Loss = -12443.345756667688
1
Iteration 4600: Loss = -12443.2746257266
Iteration 4700: Loss = -12443.271419179166
Iteration 4800: Loss = -12443.268665950447
Iteration 4900: Loss = -12443.26554995241
Iteration 5000: Loss = -12443.26871456218
1
Iteration 5100: Loss = -12443.25938344965
Iteration 5200: Loss = -12443.255320255283
Iteration 5300: Loss = -12443.250376745582
Iteration 5400: Loss = -12443.242272284424
Iteration 5500: Loss = -12443.226376435967
Iteration 5600: Loss = -12443.179967422577
Iteration 5700: Loss = -12442.974489798331
Iteration 5800: Loss = -12441.07281486377
Iteration 5900: Loss = -12440.783900476572
Iteration 6000: Loss = -12440.7041092199
Iteration 6100: Loss = -12440.670301634274
Iteration 6200: Loss = -12440.652258731581
Iteration 6300: Loss = -12440.641206930359
Iteration 6400: Loss = -12440.633779981874
Iteration 6500: Loss = -12440.628505162333
Iteration 6600: Loss = -12440.62446859932
Iteration 6700: Loss = -12440.621365200397
Iteration 6800: Loss = -12440.61888711345
Iteration 6900: Loss = -12440.616877067854
Iteration 7000: Loss = -12440.615120311793
Iteration 7100: Loss = -12440.613773801884
Iteration 7200: Loss = -12440.61252413153
Iteration 7300: Loss = -12440.611526276261
Iteration 7400: Loss = -12440.610598730816
Iteration 7500: Loss = -12440.609814291314
Iteration 7600: Loss = -12440.60911603471
Iteration 7700: Loss = -12440.608489894814
Iteration 7800: Loss = -12440.675126842598
1
Iteration 7900: Loss = -12440.60738842117
Iteration 8000: Loss = -12440.607168253257
Iteration 8100: Loss = -12440.606535702826
Iteration 8200: Loss = -12440.606217441195
Iteration 8300: Loss = -12440.60881867691
1
Iteration 8400: Loss = -12440.605469905908
Iteration 8500: Loss = -12440.606538023068
1
Iteration 8600: Loss = -12440.60501975297
Iteration 8700: Loss = -12440.604695031197
Iteration 8800: Loss = -12440.615079852727
1
Iteration 8900: Loss = -12440.60432547343
Iteration 9000: Loss = -12440.74787063256
1
Iteration 9100: Loss = -12440.604241928799
Iteration 9200: Loss = -12440.61047421517
1
Iteration 9300: Loss = -12440.603594956901
Iteration 9400: Loss = -12440.648556154432
1
Iteration 9500: Loss = -12440.60345347277
Iteration 9600: Loss = -12440.603169777563
Iteration 9700: Loss = -12440.603290228422
1
Iteration 9800: Loss = -12440.60296549789
Iteration 9900: Loss = -12440.602901608965
Iteration 10000: Loss = -12440.60308926916
1
Iteration 10100: Loss = -12440.602662784775
Iteration 10200: Loss = -12440.611122495216
1
Iteration 10300: Loss = -12440.602532025447
Iteration 10400: Loss = -12440.618485914578
1
Iteration 10500: Loss = -12440.602371432768
Iteration 10600: Loss = -12440.639364754588
1
Iteration 10700: Loss = -12440.602260200176
Iteration 10800: Loss = -12440.602471975117
1
Iteration 10900: Loss = -12440.60211028313
Iteration 11000: Loss = -12440.659966947966
1
Iteration 11100: Loss = -12440.602057765014
Iteration 11200: Loss = -12440.685479286443
1
Iteration 11300: Loss = -12440.60192938917
Iteration 11400: Loss = -12440.601880238357
Iteration 11500: Loss = -12440.602461082328
1
Iteration 11600: Loss = -12440.601835992788
Iteration 11700: Loss = -12440.611293221831
1
Iteration 11800: Loss = -12440.601768630953
Iteration 11900: Loss = -12440.602531508926
1
Iteration 12000: Loss = -12440.601758645565
Iteration 12100: Loss = -12440.601743499425
Iteration 12200: Loss = -12440.633638869946
1
Iteration 12300: Loss = -12440.601632374483
Iteration 12400: Loss = -12440.60171327969
Iteration 12500: Loss = -12440.601571219433
Iteration 12600: Loss = -12440.603563566263
1
Iteration 12700: Loss = -12440.601543328328
Iteration 12800: Loss = -12440.615112160029
1
Iteration 12900: Loss = -12440.60148836462
Iteration 13000: Loss = -12440.681130187639
1
Iteration 13100: Loss = -12440.601496924415
Iteration 13200: Loss = -12440.607717067162
1
Iteration 13300: Loss = -12440.601384373897
Iteration 13400: Loss = -12440.602285946632
1
Iteration 13500: Loss = -12440.601721759016
2
Iteration 13600: Loss = -12440.6014341582
Iteration 13700: Loss = -12440.642834098813
1
Iteration 13800: Loss = -12440.603132320739
2
Iteration 13900: Loss = -12440.601599505297
3
Iteration 14000: Loss = -12440.601747142011
4
Iteration 14100: Loss = -12440.615961078069
5
Iteration 14200: Loss = -12440.662783024643
6
Iteration 14300: Loss = -12440.602236248267
7
Iteration 14400: Loss = -12440.601391299033
Iteration 14500: Loss = -12440.648838015548
1
Iteration 14600: Loss = -12440.601308133178
Iteration 14700: Loss = -12440.60524622641
1
Iteration 14800: Loss = -12440.690186399768
2
Iteration 14900: Loss = -12440.601374185906
Iteration 15000: Loss = -12440.60129690787
Iteration 15100: Loss = -12440.603587021487
1
Iteration 15200: Loss = -12440.62160550703
2
Iteration 15300: Loss = -12440.603049374937
3
Iteration 15400: Loss = -12440.60167566931
4
Iteration 15500: Loss = -12440.601346845808
Iteration 15600: Loss = -12440.601712650296
1
Iteration 15700: Loss = -12440.601259884055
Iteration 15800: Loss = -12440.601346098816
Iteration 15900: Loss = -12440.601556375545
1
Iteration 16000: Loss = -12440.601815077187
2
Iteration 16100: Loss = -12440.60133919
Iteration 16200: Loss = -12440.602598015932
1
Iteration 16300: Loss = -12440.60121717116
Iteration 16400: Loss = -12440.622945934916
1
Iteration 16500: Loss = -12440.681039307341
2
Iteration 16600: Loss = -12440.603286191325
3
Iteration 16700: Loss = -12440.601711914089
4
Iteration 16800: Loss = -12440.601210453002
Iteration 16900: Loss = -12440.628033078741
1
Iteration 17000: Loss = -12440.601519117894
2
Iteration 17100: Loss = -12440.61322750783
3
Iteration 17200: Loss = -12440.64323800986
4
Iteration 17300: Loss = -12440.601617808636
5
Iteration 17400: Loss = -12440.60142591808
6
Iteration 17500: Loss = -12440.60751265271
7
Iteration 17600: Loss = -12440.614470338232
8
Iteration 17700: Loss = -12440.602307949326
9
Iteration 17800: Loss = -12440.662823730943
10
Iteration 17900: Loss = -12440.60116139241
Iteration 18000: Loss = -12440.601407457096
1
Iteration 18100: Loss = -12440.606318717319
2
Iteration 18200: Loss = -12440.601270403251
3
Iteration 18300: Loss = -12440.60322079117
4
Iteration 18400: Loss = -12440.601187773034
Iteration 18500: Loss = -12440.601306843693
1
Iteration 18600: Loss = -12440.601809991987
2
Iteration 18700: Loss = -12440.601167051356
Iteration 18800: Loss = -12440.661131325109
1
Iteration 18900: Loss = -12440.601319620895
2
Iteration 19000: Loss = -12440.601258996774
Iteration 19100: Loss = -12440.60419935955
1
Iteration 19200: Loss = -12440.603445142477
2
Iteration 19300: Loss = -12440.632077958258
3
Iteration 19400: Loss = -12440.605288098053
4
Iteration 19500: Loss = -12440.602610819076
5
Iteration 19600: Loss = -12440.610087900704
6
Iteration 19700: Loss = -12440.602412862147
7
Iteration 19800: Loss = -12440.601523497206
8
Iteration 19900: Loss = -12440.601689165089
9
pi: tensor([[1.3654e-07, 1.0000e+00],
        [1.0000e+00, 1.0781e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9796, 0.0204], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1997, 0.0839],
         [0.6595, 0.2074]],

        [[0.6241, 0.1577],
         [0.5861, 0.7131]],

        [[0.5675, 0.2543],
         [0.6933, 0.7158]],

        [[0.5190, 0.2044],
         [0.6874, 0.7250]],

        [[0.5314, 0.2074],
         [0.5232, 0.7131]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: -0.0018350314903527825
Average Adjusted Rand Index: 0.001219409938710761
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21635.32065475873
Iteration 100: Loss = -12446.276406575
Iteration 200: Loss = -12445.885582144485
Iteration 300: Loss = -12445.819809783146
Iteration 400: Loss = -12445.777190540615
Iteration 500: Loss = -12445.744004938104
Iteration 600: Loss = -12445.713645537287
Iteration 700: Loss = -12445.680160135485
Iteration 800: Loss = -12445.634934291707
Iteration 900: Loss = -12445.562746837857
Iteration 1000: Loss = -12445.458110236377
Iteration 1100: Loss = -12445.360302484352
Iteration 1200: Loss = -12445.289219168293
Iteration 1300: Loss = -12445.223869927075
Iteration 1400: Loss = -12445.11060530676
Iteration 1500: Loss = -12443.765832322168
Iteration 1600: Loss = -12443.47394734162
Iteration 1700: Loss = -12443.42129326554
Iteration 1800: Loss = -12443.391794228088
Iteration 1900: Loss = -12443.368245667185
Iteration 2000: Loss = -12443.347108108912
Iteration 2100: Loss = -12443.328231037944
Iteration 2200: Loss = -12443.312289111629
Iteration 2300: Loss = -12443.29950866979
Iteration 2400: Loss = -12443.289519890119
Iteration 2500: Loss = -12443.281871084608
Iteration 2600: Loss = -12443.276384991275
Iteration 2700: Loss = -12443.271240828588
Iteration 2800: Loss = -12443.267961988415
Iteration 2900: Loss = -12443.2630882547
Iteration 3000: Loss = -12443.286870613347
1
Iteration 3100: Loss = -12443.254327089431
Iteration 3200: Loss = -12443.252957334997
Iteration 3300: Loss = -12443.235735535874
Iteration 3400: Loss = -12443.200505061843
Iteration 3500: Loss = -12442.99578847229
Iteration 3600: Loss = -12440.917836016706
Iteration 3700: Loss = -12440.709147906899
Iteration 3800: Loss = -12440.661309188788
Iteration 3900: Loss = -12440.641817785683
Iteration 4000: Loss = -12440.631403899572
Iteration 4100: Loss = -12440.624958542643
Iteration 4200: Loss = -12440.620631985967
Iteration 4300: Loss = -12440.617462496242
Iteration 4400: Loss = -12440.615117984009
Iteration 4500: Loss = -12440.613271639068
Iteration 4600: Loss = -12440.611804070464
Iteration 4700: Loss = -12440.610585251565
Iteration 4800: Loss = -12440.60956117998
Iteration 4900: Loss = -12440.608730247764
Iteration 5000: Loss = -12440.608003349844
Iteration 5100: Loss = -12440.607359115855
Iteration 5200: Loss = -12440.60681246804
Iteration 5300: Loss = -12440.606340018998
Iteration 5400: Loss = -12440.605940781279
Iteration 5500: Loss = -12440.605518080105
Iteration 5600: Loss = -12440.605198262176
Iteration 5700: Loss = -12440.604917366263
Iteration 5800: Loss = -12440.604613658557
Iteration 5900: Loss = -12440.604363049566
Iteration 6000: Loss = -12440.604177801339
Iteration 6100: Loss = -12440.603925705309
Iteration 6200: Loss = -12440.60375256884
Iteration 6300: Loss = -12440.603583625798
Iteration 6400: Loss = -12440.664418738188
1
Iteration 6500: Loss = -12440.603250489356
Iteration 6600: Loss = -12440.603168514874
Iteration 6700: Loss = -12440.602997155693
Iteration 6800: Loss = -12440.611582438443
1
Iteration 6900: Loss = -12440.602792805688
Iteration 7000: Loss = -12440.602674761336
Iteration 7100: Loss = -12440.602574544337
Iteration 7200: Loss = -12440.603539923382
1
Iteration 7300: Loss = -12440.602410128346
Iteration 7400: Loss = -12440.602356282401
Iteration 7500: Loss = -12440.634024908071
1
Iteration 7600: Loss = -12440.602251409446
Iteration 7700: Loss = -12440.60218493918
Iteration 7800: Loss = -12440.602127125652
Iteration 7900: Loss = -12440.722164456804
1
Iteration 8000: Loss = -12440.601989713448
Iteration 8100: Loss = -12440.601918393466
Iteration 8200: Loss = -12440.601943832442
Iteration 8300: Loss = -12440.601917663642
Iteration 8400: Loss = -12440.601858379643
Iteration 8500: Loss = -12440.601786102026
Iteration 8600: Loss = -12440.601768727956
Iteration 8700: Loss = -12440.601871905817
1
Iteration 8800: Loss = -12440.601696900216
Iteration 8900: Loss = -12440.601724438424
Iteration 9000: Loss = -12440.74946384659
1
Iteration 9100: Loss = -12440.601615875301
Iteration 9200: Loss = -12440.601673194313
Iteration 9300: Loss = -12440.60164009021
Iteration 9400: Loss = -12440.607995552728
1
Iteration 9500: Loss = -12440.60543182795
2
Iteration 9600: Loss = -12440.647221314337
3
Iteration 9700: Loss = -12440.602257374221
4
Iteration 9800: Loss = -12440.618682856786
5
Iteration 9900: Loss = -12440.60148463607
Iteration 10000: Loss = -12440.733038353672
1
Iteration 10100: Loss = -12440.601422684122
Iteration 10200: Loss = -12440.603325194284
1
Iteration 10300: Loss = -12440.601398047298
Iteration 10400: Loss = -12440.601709635233
1
Iteration 10500: Loss = -12440.601382686762
Iteration 10600: Loss = -12440.602046859472
1
Iteration 10700: Loss = -12440.601326143991
Iteration 10800: Loss = -12440.602543703308
1
Iteration 10900: Loss = -12440.602520605727
2
Iteration 11000: Loss = -12440.601304940745
Iteration 11100: Loss = -12440.605519483363
1
Iteration 11200: Loss = -12440.601313902103
Iteration 11300: Loss = -12440.602741526143
1
Iteration 11400: Loss = -12440.601277273186
Iteration 11500: Loss = -12440.606930629356
1
Iteration 11600: Loss = -12440.601256161333
Iteration 11700: Loss = -12440.601257813301
Iteration 11800: Loss = -12440.60186659921
1
Iteration 11900: Loss = -12440.602558229284
2
Iteration 12000: Loss = -12440.624333742366
3
Iteration 12100: Loss = -12440.62822544054
4
Iteration 12200: Loss = -12440.601512350491
5
Iteration 12300: Loss = -12440.601221130615
Iteration 12400: Loss = -12440.601279139435
Iteration 12500: Loss = -12440.601200648025
Iteration 12600: Loss = -12440.601209446135
Iteration 12700: Loss = -12440.601535929649
1
Iteration 12800: Loss = -12440.606910766912
2
Iteration 12900: Loss = -12440.60118082158
Iteration 13000: Loss = -12440.617478088925
1
Iteration 13100: Loss = -12440.60117877026
Iteration 13200: Loss = -12440.662970193505
1
Iteration 13300: Loss = -12440.601199344137
Iteration 13400: Loss = -12440.60117367691
Iteration 13500: Loss = -12440.601709122113
1
Iteration 13600: Loss = -12440.601169125383
Iteration 13700: Loss = -12440.603445588662
1
Iteration 13800: Loss = -12440.603902547602
2
Iteration 13900: Loss = -12440.602553641995
3
Iteration 14000: Loss = -12440.602238318641
4
Iteration 14100: Loss = -12440.601327388751
5
Iteration 14200: Loss = -12440.634762988595
6
Iteration 14300: Loss = -12440.601199364453
Iteration 14400: Loss = -12440.604400271528
1
Iteration 14500: Loss = -12440.683859744753
2
Iteration 14600: Loss = -12440.6059945377
3
Iteration 14700: Loss = -12440.60115735084
Iteration 14800: Loss = -12440.665588700023
1
Iteration 14900: Loss = -12440.60115003156
Iteration 15000: Loss = -12440.602834153622
1
Iteration 15100: Loss = -12440.60209104755
2
Iteration 15200: Loss = -12440.601817599621
3
Iteration 15300: Loss = -12440.662944837775
4
Iteration 15400: Loss = -12440.601251693304
5
Iteration 15500: Loss = -12440.684245498305
6
Iteration 15600: Loss = -12440.601159018212
Iteration 15700: Loss = -12440.605532502525
1
Iteration 15800: Loss = -12440.604124134523
2
Iteration 15900: Loss = -12440.601538136525
3
Iteration 16000: Loss = -12440.60659456023
4
Iteration 16100: Loss = -12440.601225189637
Iteration 16200: Loss = -12440.602492066242
1
Iteration 16300: Loss = -12440.608433270265
2
Iteration 16400: Loss = -12440.601225151473
Iteration 16500: Loss = -12440.66264137754
1
Iteration 16600: Loss = -12440.601149358072
Iteration 16700: Loss = -12440.601729370323
1
Iteration 16800: Loss = -12440.603319593036
2
Iteration 16900: Loss = -12440.601134098417
Iteration 17000: Loss = -12440.602815637827
1
Iteration 17100: Loss = -12440.603002722299
2
Iteration 17200: Loss = -12440.602870343186
3
Iteration 17300: Loss = -12440.601482901038
4
Iteration 17400: Loss = -12440.685060720783
5
Iteration 17500: Loss = -12440.604801454385
6
Iteration 17600: Loss = -12440.601480169938
7
Iteration 17700: Loss = -12440.601495411072
8
Iteration 17800: Loss = -12440.601195164461
Iteration 17900: Loss = -12440.65065875123
1
Iteration 18000: Loss = -12440.616629958742
2
Iteration 18100: Loss = -12440.602070570261
3
Iteration 18200: Loss = -12440.627483090399
4
Iteration 18300: Loss = -12440.631513816048
5
Iteration 18400: Loss = -12440.601141751775
Iteration 18500: Loss = -12440.602931118861
1
Iteration 18600: Loss = -12440.60144945409
2
Iteration 18700: Loss = -12440.601249864054
3
Iteration 18800: Loss = -12440.710717394295
4
Iteration 18900: Loss = -12440.601204643523
Iteration 19000: Loss = -12440.601597082496
1
Iteration 19100: Loss = -12440.616259233164
2
Iteration 19200: Loss = -12440.622958884653
3
Iteration 19300: Loss = -12440.602219799523
4
Iteration 19400: Loss = -12440.60208038842
5
Iteration 19500: Loss = -12440.601312843834
6
Iteration 19600: Loss = -12440.60131836485
7
Iteration 19700: Loss = -12440.603250227698
8
Iteration 19800: Loss = -12440.616493093097
9
Iteration 19900: Loss = -12440.601924265817
10
pi: tensor([[1.9193e-08, 1.0000e+00],
        [1.0000e+00, 3.6888e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0205, 0.9795], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2077, 0.0841],
         [0.6728, 0.1991]],

        [[0.6967, 0.1577],
         [0.6360, 0.5276]],

        [[0.6842, 0.2532],
         [0.6436, 0.5308]],

        [[0.5745, 0.2038],
         [0.5895, 0.5913]],

        [[0.7264, 0.2075],
         [0.5379, 0.7040]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: -0.0018350314903527825
Average Adjusted Rand Index: 0.001219409938710761
11912.63649173985
[-0.0018350314903527825, -0.0018350314903527825] [0.001219409938710761, 0.001219409938710761] [12440.602100885617, 12440.609667457908]
-------------------------------------
This iteration is 4
True Objective function: Loss = -11773.836522861695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22778.40854622023
Iteration 100: Loss = -12255.096920449001
Iteration 200: Loss = -12254.60123577463
Iteration 300: Loss = -12254.375474911541
Iteration 400: Loss = -12254.143857792838
Iteration 500: Loss = -12253.94494628344
Iteration 600: Loss = -12253.809588242151
Iteration 700: Loss = -12253.703678679887
Iteration 800: Loss = -12253.59567583472
Iteration 900: Loss = -12253.385809017864
Iteration 1000: Loss = -12251.945279488764
Iteration 1100: Loss = -12251.135441125602
Iteration 1200: Loss = -12250.360595655591
Iteration 1300: Loss = -12250.030196686463
Iteration 1400: Loss = -12249.928467118594
Iteration 1500: Loss = -12249.874988090103
Iteration 1600: Loss = -12249.832714078995
Iteration 1700: Loss = -12249.807473118186
Iteration 1800: Loss = -12249.792125519349
Iteration 1900: Loss = -12249.780744877038
Iteration 2000: Loss = -12249.772089654221
Iteration 2100: Loss = -12249.76532706565
Iteration 2200: Loss = -12249.759779425998
Iteration 2300: Loss = -12249.755200984098
Iteration 2400: Loss = -12249.751470806897
Iteration 2500: Loss = -12249.748327099769
Iteration 2600: Loss = -12249.745598434518
Iteration 2700: Loss = -12249.743066789848
Iteration 2800: Loss = -12249.740460410394
Iteration 2900: Loss = -12249.736925403393
Iteration 3000: Loss = -12249.735101036471
Iteration 3100: Loss = -12249.73387532407
Iteration 3200: Loss = -12249.732867234394
Iteration 3300: Loss = -12249.731971139609
Iteration 3400: Loss = -12249.731181535188
Iteration 3500: Loss = -12249.73045960956
Iteration 3600: Loss = -12249.729744215067
Iteration 3700: Loss = -12249.72910116649
Iteration 3800: Loss = -12249.72848210073
Iteration 3900: Loss = -12249.727889214788
Iteration 4000: Loss = -12249.727279312092
Iteration 4100: Loss = -12249.726739956444
Iteration 4200: Loss = -12249.726133552793
Iteration 4300: Loss = -12249.725462372928
Iteration 4400: Loss = -12249.724850845434
Iteration 4500: Loss = -12249.724373221781
Iteration 4600: Loss = -12249.72393677567
Iteration 4700: Loss = -12249.72343260006
Iteration 4800: Loss = -12249.722883904282
Iteration 4900: Loss = -12249.7221411902
Iteration 5000: Loss = -12249.721578110813
Iteration 5100: Loss = -12249.721114929418
Iteration 5200: Loss = -12249.720517983602
Iteration 5300: Loss = -12249.719741463328
Iteration 5400: Loss = -12249.718923015327
Iteration 5500: Loss = -12249.717972757202
Iteration 5600: Loss = -12249.717499294273
Iteration 5700: Loss = -12249.717265736288
Iteration 5800: Loss = -12249.717129044895
Iteration 5900: Loss = -12249.716988794944
Iteration 6000: Loss = -12249.716814975414
Iteration 6100: Loss = -12249.71665313559
Iteration 6200: Loss = -12249.716419273876
Iteration 6300: Loss = -12249.715980548564
Iteration 6400: Loss = -12249.715559617676
Iteration 6500: Loss = -12249.715295107955
Iteration 6600: Loss = -12249.71521998589
Iteration 6700: Loss = -12249.71513026147
Iteration 6800: Loss = -12249.715051626747
Iteration 6900: Loss = -12249.714893356579
Iteration 7000: Loss = -12249.714876584427
Iteration 7100: Loss = -12249.714756662222
Iteration 7200: Loss = -12249.714701976241
Iteration 7300: Loss = -12249.714675724537
Iteration 7400: Loss = -12249.714619760434
Iteration 7500: Loss = -12249.714492162138
Iteration 7600: Loss = -12249.714524827232
Iteration 7700: Loss = -12249.716394394736
1
Iteration 7800: Loss = -12249.715359780095
2
Iteration 7900: Loss = -12249.714173498622
Iteration 8000: Loss = -12249.714106314752
Iteration 8100: Loss = -12249.754726578918
1
Iteration 8200: Loss = -12249.713469978962
Iteration 8300: Loss = -12249.717611037471
1
Iteration 8400: Loss = -12249.712991811613
Iteration 8500: Loss = -12249.721159437153
1
Iteration 8600: Loss = -12249.71278345221
Iteration 8700: Loss = -12249.728025517124
1
Iteration 8800: Loss = -12249.712555780185
Iteration 8900: Loss = -12249.711886922783
Iteration 9000: Loss = -12249.717305932236
1
Iteration 9100: Loss = -12249.711492290086
Iteration 9200: Loss = -12249.711301392173
Iteration 9300: Loss = -12249.71116683216
Iteration 9400: Loss = -12249.710312165864
Iteration 9500: Loss = -12249.710178285472
Iteration 9600: Loss = -12249.767928986665
1
Iteration 9700: Loss = -12249.70909403909
Iteration 9800: Loss = -12249.708616054964
Iteration 9900: Loss = -12249.708496750856
Iteration 10000: Loss = -12249.71170322877
1
Iteration 10100: Loss = -12249.706376173745
Iteration 10200: Loss = -12249.705982234911
Iteration 10300: Loss = -12249.741660046919
1
Iteration 10400: Loss = -12249.704498332534
Iteration 10500: Loss = -12249.70423530983
Iteration 10600: Loss = -12249.727192108712
1
Iteration 10700: Loss = -12249.704063220945
Iteration 10800: Loss = -12249.704016741189
Iteration 10900: Loss = -12249.703970780974
Iteration 11000: Loss = -12249.703934756808
Iteration 11100: Loss = -12249.703931715303
Iteration 11200: Loss = -12249.790236377605
1
Iteration 11300: Loss = -12249.703474616008
Iteration 11400: Loss = -12249.703442607315
Iteration 11500: Loss = -12249.709030571854
1
Iteration 11600: Loss = -12249.702863083865
Iteration 11700: Loss = -12249.703165130304
1
Iteration 11800: Loss = -12249.702508973105
Iteration 11900: Loss = -12249.702360558369
Iteration 12000: Loss = -12249.702329435346
Iteration 12100: Loss = -12249.703599620325
1
Iteration 12200: Loss = -12249.702199681044
Iteration 12300: Loss = -12249.70218712774
Iteration 12400: Loss = -12249.711849893549
1
Iteration 12500: Loss = -12249.702187641753
Iteration 12600: Loss = -12249.702150459369
Iteration 12700: Loss = -12249.702072220929
Iteration 12800: Loss = -12249.702202238572
1
Iteration 12900: Loss = -12249.702002243983
Iteration 13000: Loss = -12249.702003911732
Iteration 13100: Loss = -12249.702383815818
1
Iteration 13200: Loss = -12249.701783602157
Iteration 13300: Loss = -12249.721190388565
1
Iteration 13400: Loss = -12249.701792602857
Iteration 13500: Loss = -12249.70178439457
Iteration 13600: Loss = -12249.756045632037
1
Iteration 13700: Loss = -12249.701655801342
Iteration 13800: Loss = -12249.771299094913
1
Iteration 13900: Loss = -12249.701690654874
Iteration 14000: Loss = -12249.70274446084
1
Iteration 14100: Loss = -12249.701738531734
Iteration 14200: Loss = -12249.701683642275
Iteration 14300: Loss = -12249.701646603608
Iteration 14400: Loss = -12249.701579329285
Iteration 14500: Loss = -12249.70222762911
1
Iteration 14600: Loss = -12249.701580037674
Iteration 14700: Loss = -12250.058917503362
1
Iteration 14800: Loss = -12249.701551961758
Iteration 14900: Loss = -12249.701456165221
Iteration 15000: Loss = -12249.701526269242
Iteration 15100: Loss = -12249.701417462935
Iteration 15200: Loss = -12249.700939754566
Iteration 15300: Loss = -12249.700761474776
Iteration 15400: Loss = -12249.700657601228
Iteration 15500: Loss = -12249.70059463163
Iteration 15600: Loss = -12249.70167588894
1
Iteration 15700: Loss = -12249.700527732259
Iteration 15800: Loss = -12249.700492803764
Iteration 15900: Loss = -12249.701214395704
1
Iteration 16000: Loss = -12249.700142591348
Iteration 16100: Loss = -12249.700169509366
Iteration 16200: Loss = -12249.722045481914
1
Iteration 16300: Loss = -12249.70015301755
Iteration 16400: Loss = -12249.700142531201
Iteration 16500: Loss = -12249.70052674114
1
Iteration 16600: Loss = -12249.699317941928
Iteration 16700: Loss = -12249.756064631989
1
Iteration 16800: Loss = -12249.69923716134
Iteration 16900: Loss = -12249.700907368067
1
Iteration 17000: Loss = -12249.699201131014
Iteration 17100: Loss = -12249.699155975899
Iteration 17200: Loss = -12249.820758057322
1
Iteration 17300: Loss = -12249.699165162465
Iteration 17400: Loss = -12249.70247964436
1
Iteration 17500: Loss = -12249.699127006617
Iteration 17600: Loss = -12249.700166451144
1
Iteration 17700: Loss = -12249.699119473458
Iteration 17800: Loss = -12249.700339381556
1
Iteration 17900: Loss = -12249.699157240811
Iteration 18000: Loss = -12249.699120962068
Iteration 18100: Loss = -12249.739265949238
1
Iteration 18200: Loss = -12249.698957305573
Iteration 18300: Loss = -12249.698940513834
Iteration 18400: Loss = -12249.69930564619
1
Iteration 18500: Loss = -12249.698762903732
Iteration 18600: Loss = -12249.698781719095
Iteration 18700: Loss = -12249.701518242078
1
Iteration 18800: Loss = -12249.698699234583
Iteration 18900: Loss = -12249.698677019187
Iteration 19000: Loss = -12249.70419761836
1
Iteration 19100: Loss = -12249.698542823626
Iteration 19200: Loss = -12249.698572260691
Iteration 19300: Loss = -12249.782688760575
1
Iteration 19400: Loss = -12249.698566902245
Iteration 19500: Loss = -12249.698481989171
Iteration 19600: Loss = -12249.69847217706
Iteration 19700: Loss = -12249.705067066474
1
Iteration 19800: Loss = -12249.787607995479
2
Iteration 19900: Loss = -12249.69836908135
pi: tensor([[1.0000e+00, 5.0653e-07],
        [5.0213e-03, 9.9498e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0100, 0.9900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3406, 0.0930],
         [0.6365, 0.1923]],

        [[0.5083, 0.2190],
         [0.6895, 0.5051]],

        [[0.6289, 0.2422],
         [0.6894, 0.6169]],

        [[0.6213, 0.2455],
         [0.5909, 0.5837]],

        [[0.5268, 0.3023],
         [0.6565, 0.5855]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.00877236457352431
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0012662455124815629
Global Adjusted Rand Index: -0.002619348064797456
Average Adjusted Rand Index: 0.001422931099184298
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21851.115517258342
Iteration 100: Loss = -12255.758375257337
Iteration 200: Loss = -12255.237855412437
Iteration 300: Loss = -12255.07564872573
Iteration 400: Loss = -12255.01017833278
Iteration 500: Loss = -12254.973157014574
Iteration 600: Loss = -12254.946933601928
Iteration 700: Loss = -12254.922433717684
Iteration 800: Loss = -12254.884448964736
Iteration 900: Loss = -12254.717886569339
Iteration 1000: Loss = -12252.039785225863
Iteration 1100: Loss = -12251.308995979896
Iteration 1200: Loss = -12250.998032841226
Iteration 1300: Loss = -12250.738778375238
Iteration 1400: Loss = -12250.529292365869
Iteration 1500: Loss = -12250.376144646882
Iteration 1600: Loss = -12250.265275122772
Iteration 1700: Loss = -12250.181588545975
Iteration 1800: Loss = -12250.117770765704
Iteration 1900: Loss = -12250.070551215165
Iteration 2000: Loss = -12250.03492687638
Iteration 2100: Loss = -12250.006666254103
Iteration 2200: Loss = -12249.983438049236
Iteration 2300: Loss = -12249.963973014339
Iteration 2400: Loss = -12249.947500080432
Iteration 2500: Loss = -12249.933396190927
Iteration 2600: Loss = -12249.92122517118
Iteration 2700: Loss = -12249.910728396897
Iteration 2800: Loss = -12249.901544293598
Iteration 2900: Loss = -12249.89346509512
Iteration 3000: Loss = -12249.886323794039
Iteration 3100: Loss = -12249.880028810498
Iteration 3200: Loss = -12249.87438189906
Iteration 3300: Loss = -12249.869369627435
Iteration 3400: Loss = -12249.86487166614
Iteration 3500: Loss = -12249.860782732281
Iteration 3600: Loss = -12249.857070012282
Iteration 3700: Loss = -12249.853775431688
Iteration 3800: Loss = -12249.850730571463
Iteration 3900: Loss = -12249.847932921895
Iteration 4000: Loss = -12249.845380579907
Iteration 4100: Loss = -12249.8430743973
Iteration 4200: Loss = -12249.840888210447
Iteration 4300: Loss = -12249.838875033274
Iteration 4400: Loss = -12249.837016007848
Iteration 4500: Loss = -12249.83530736121
Iteration 4600: Loss = -12249.833713596314
Iteration 4700: Loss = -12249.832180867375
Iteration 4800: Loss = -12249.830850227909
Iteration 4900: Loss = -12249.82950148959
Iteration 5000: Loss = -12249.82826001722
Iteration 5100: Loss = -12249.827085451636
Iteration 5200: Loss = -12249.826053982428
Iteration 5300: Loss = -12249.82501162878
Iteration 5400: Loss = -12249.82405110605
Iteration 5500: Loss = -12249.823165182468
Iteration 5600: Loss = -12249.822327913544
Iteration 5700: Loss = -12249.821537214464
Iteration 5800: Loss = -12249.820815907777
Iteration 5900: Loss = -12249.820042167612
Iteration 6000: Loss = -12249.819390189426
Iteration 6100: Loss = -12249.818793830786
Iteration 6200: Loss = -12249.818184830969
Iteration 6300: Loss = -12249.817602953948
Iteration 6400: Loss = -12249.817101409108
Iteration 6500: Loss = -12249.816636926571
Iteration 6600: Loss = -12249.81617376215
Iteration 6700: Loss = -12249.815713389975
Iteration 6800: Loss = -12249.815301751554
Iteration 6900: Loss = -12249.814954503943
Iteration 7000: Loss = -12249.814571835628
Iteration 7100: Loss = -12249.814249706686
Iteration 7200: Loss = -12249.813975327002
Iteration 7300: Loss = -12249.813687332458
Iteration 7400: Loss = -12249.813445852966
Iteration 7500: Loss = -12249.816165134982
1
Iteration 7600: Loss = -12249.81293079706
Iteration 7700: Loss = -12249.812743644017
Iteration 7800: Loss = -12249.812526213747
Iteration 7900: Loss = -12249.812722860112
1
Iteration 8000: Loss = -12249.812231794314
Iteration 8100: Loss = -12249.81203902197
Iteration 8200: Loss = -12249.817112485518
1
Iteration 8300: Loss = -12249.811804331825
Iteration 8400: Loss = -12249.81160095668
Iteration 8500: Loss = -12249.811476066649
Iteration 8600: Loss = -12249.846126011453
1
Iteration 8700: Loss = -12249.811277503697
Iteration 8800: Loss = -12249.811208700337
Iteration 8900: Loss = -12249.811098323646
Iteration 9000: Loss = -12249.816280901037
1
Iteration 9100: Loss = -12249.81096678676
Iteration 9200: Loss = -12249.810901485427
Iteration 9300: Loss = -12250.072829914721
1
Iteration 9400: Loss = -12249.810750709572
Iteration 9500: Loss = -12249.810692379002
Iteration 9600: Loss = -12249.810612177636
Iteration 9700: Loss = -12249.810540686765
Iteration 9800: Loss = -12249.810491300588
Iteration 9900: Loss = -12249.810473399204
Iteration 10000: Loss = -12249.810901189965
1
Iteration 10100: Loss = -12249.810336596656
Iteration 10200: Loss = -12249.810342106359
Iteration 10300: Loss = -12249.810319392478
Iteration 10400: Loss = -12249.810339752608
Iteration 10500: Loss = -12249.810216752545
Iteration 10600: Loss = -12249.81017747631
Iteration 10700: Loss = -12249.810421355622
1
Iteration 10800: Loss = -12249.810168798425
Iteration 10900: Loss = -12249.810094547114
Iteration 11000: Loss = -12249.810018945731
Iteration 11100: Loss = -12249.959837676453
1
Iteration 11200: Loss = -12249.810023123297
Iteration 11300: Loss = -12249.809996716705
Iteration 11400: Loss = -12249.809936118487
Iteration 11500: Loss = -12249.80992901786
Iteration 11600: Loss = -12249.809954714043
Iteration 11700: Loss = -12249.80991828318
Iteration 11800: Loss = -12249.809877461998
Iteration 11900: Loss = -12250.26699286368
1
Iteration 12000: Loss = -12249.809886303128
Iteration 12100: Loss = -12249.809848361516
Iteration 12200: Loss = -12249.809808708556
Iteration 12300: Loss = -12249.812048601558
1
Iteration 12400: Loss = -12249.809786058964
Iteration 12500: Loss = -12249.809796150741
Iteration 12600: Loss = -12249.867189529215
1
Iteration 12700: Loss = -12249.809802531183
Iteration 12800: Loss = -12249.809745203738
Iteration 12900: Loss = -12249.809752604227
Iteration 13000: Loss = -12249.811203705514
1
Iteration 13100: Loss = -12249.8097253414
Iteration 13200: Loss = -12249.809714271336
Iteration 13300: Loss = -12249.809717973427
Iteration 13400: Loss = -12249.809663935028
Iteration 13500: Loss = -12249.809663708143
Iteration 13600: Loss = -12249.80966277784
Iteration 13700: Loss = -12249.834868004098
1
Iteration 13800: Loss = -12249.809672551555
Iteration 13900: Loss = -12249.82407476516
1
Iteration 14000: Loss = -12249.809638811295
Iteration 14100: Loss = -12249.992766260708
1
Iteration 14200: Loss = -12249.809641778158
Iteration 14300: Loss = -12249.875857542555
1
Iteration 14400: Loss = -12249.809546088998
Iteration 14500: Loss = -12249.833760991434
1
Iteration 14600: Loss = -12249.809596350966
Iteration 14700: Loss = -12249.809616625516
Iteration 14800: Loss = -12249.81316053183
1
Iteration 14900: Loss = -12249.809601927798
Iteration 15000: Loss = -12249.809606834799
Iteration 15100: Loss = -12249.809899479946
1
Iteration 15200: Loss = -12249.809629242029
Iteration 15300: Loss = -12249.8100036068
1
Iteration 15400: Loss = -12249.809638257659
Iteration 15500: Loss = -12249.809657202632
Iteration 15600: Loss = -12249.812918980479
1
Iteration 15700: Loss = -12249.809688855235
Iteration 15800: Loss = -12249.84421490627
1
Iteration 15900: Loss = -12249.8095951576
Iteration 16000: Loss = -12249.809634275525
Iteration 16100: Loss = -12249.80959889384
Iteration 16200: Loss = -12249.810071976673
1
Iteration 16300: Loss = -12249.809918589359
2
Iteration 16400: Loss = -12249.80977422368
3
Iteration 16500: Loss = -12250.015606489917
4
Iteration 16600: Loss = -12249.809615226834
Iteration 16700: Loss = -12249.84627719707
1
Iteration 16800: Loss = -12249.809598665475
Iteration 16900: Loss = -12249.810152113865
1
Iteration 17000: Loss = -12249.810238623455
2
Iteration 17100: Loss = -12249.809850923299
3
Iteration 17200: Loss = -12249.809997780556
4
Iteration 17300: Loss = -12249.81149675163
5
Iteration 17400: Loss = -12249.809560979911
Iteration 17500: Loss = -12249.809599608798
Iteration 17600: Loss = -12249.810089525672
1
Iteration 17700: Loss = -12249.810087042115
2
Iteration 17800: Loss = -12249.809640185464
Iteration 17900: Loss = -12250.081119764121
1
Iteration 18000: Loss = -12249.80955792746
Iteration 18100: Loss = -12249.81050894975
1
Iteration 18200: Loss = -12249.80966372143
2
Iteration 18300: Loss = -12249.809565815109
Iteration 18400: Loss = -12249.809877738007
1
Iteration 18500: Loss = -12249.81045652664
2
Iteration 18600: Loss = -12249.809795526033
3
Iteration 18700: Loss = -12249.810497137092
4
Iteration 18800: Loss = -12249.812756020096
5
Iteration 18900: Loss = -12249.8098170073
6
Iteration 19000: Loss = -12249.809684747328
7
Iteration 19100: Loss = -12250.110723732307
8
Iteration 19200: Loss = -12249.809573934717
Iteration 19300: Loss = -12249.819883468917
1
Iteration 19400: Loss = -12249.80978385368
2
Iteration 19500: Loss = -12249.810749335475
3
Iteration 19600: Loss = -12249.80957351803
Iteration 19700: Loss = -12249.813111616271
1
Iteration 19800: Loss = -12249.809743711538
2
Iteration 19900: Loss = -12249.827108798434
3
pi: tensor([[1.0000e+00, 1.4771e-06],
        [8.9055e-04, 9.9911e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0091, 0.9909], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1044, 0.2224],
         [0.5810, 0.1945]],

        [[0.7153, 0.2419],
         [0.6602, 0.5156]],

        [[0.6680, 0.2914],
         [0.7072, 0.6221]],

        [[0.6886, 0.3102],
         [0.6373, 0.5758]],

        [[0.5015, 0.3035],
         [0.6947, 0.6530]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
Global Adjusted Rand Index: -0.001682918208106234
Average Adjusted Rand Index: -0.00023123120452624296
11773.836522861695
[-0.002619348064797456, -0.001682918208106234] [0.001422931099184298, -0.00023123120452624296] [12249.922202870508, 12249.810007830734]
-------------------------------------
This iteration is 5
True Objective function: Loss = -11945.322059138682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21521.010634288123
Iteration 100: Loss = -12467.315251102233
Iteration 200: Loss = -12466.638244173415
Iteration 300: Loss = -12466.447352754401
Iteration 400: Loss = -12466.360575557052
Iteration 500: Loss = -12466.306045993262
Iteration 600: Loss = -12466.262876588766
Iteration 700: Loss = -12466.223385935838
Iteration 800: Loss = -12466.184011330724
Iteration 900: Loss = -12466.142513665858
Iteration 1000: Loss = -12466.09721436178
Iteration 1100: Loss = -12466.046065367193
Iteration 1200: Loss = -12465.987937830554
Iteration 1300: Loss = -12465.923103785799
Iteration 1400: Loss = -12465.85213012626
Iteration 1500: Loss = -12465.774600259183
Iteration 1600: Loss = -12465.688251672946
Iteration 1700: Loss = -12465.591654411577
Iteration 1800: Loss = -12465.492973094042
Iteration 1900: Loss = -12465.406373448717
Iteration 2000: Loss = -12465.331481403991
Iteration 2100: Loss = -12465.262922634553
Iteration 2200: Loss = -12465.205118798784
Iteration 2300: Loss = -12465.155928541239
Iteration 2400: Loss = -12465.109484586623
Iteration 2500: Loss = -12465.061542020057
Iteration 2600: Loss = -12465.000668158804
Iteration 2700: Loss = -12464.913594397209
Iteration 2800: Loss = -12464.765290485399
Iteration 2900: Loss = -12464.546719213637
Iteration 3000: Loss = -12464.389609205726
Iteration 3100: Loss = -12464.328740433299
Iteration 3200: Loss = -12464.303423135403
Iteration 3300: Loss = -12464.289197936003
Iteration 3400: Loss = -12464.278274910277
Iteration 3500: Loss = -12464.265889823211
Iteration 3600: Loss = -12464.237408222178
Iteration 3700: Loss = -12463.620045852344
Iteration 3800: Loss = -12075.060396621717
Iteration 3900: Loss = -11997.12489597784
Iteration 4000: Loss = -11982.697830861543
Iteration 4100: Loss = -11973.110748155777
Iteration 4200: Loss = -11973.097507344944
Iteration 4300: Loss = -11973.086309542405
Iteration 4400: Loss = -11973.063257387488
Iteration 4500: Loss = -11962.297284900755
Iteration 4600: Loss = -11961.76518600079
Iteration 4700: Loss = -11961.756681439578
Iteration 4800: Loss = -11961.754435034132
Iteration 4900: Loss = -11961.754779334353
1
Iteration 5000: Loss = -11960.040759947393
Iteration 5100: Loss = -11960.035784824064
Iteration 5200: Loss = -11951.930390983696
Iteration 5300: Loss = -11938.181935337443
Iteration 5400: Loss = -11938.173296353238
Iteration 5500: Loss = -11938.168333180007
Iteration 5600: Loss = -11938.16379241081
Iteration 5700: Loss = -11938.161806821934
Iteration 5800: Loss = -11938.165044120946
1
Iteration 5900: Loss = -11938.161724383073
Iteration 6000: Loss = -11938.159961233796
Iteration 6100: Loss = -11938.165827099201
1
Iteration 6200: Loss = -11938.158396050243
Iteration 6300: Loss = -11938.157996068436
Iteration 6400: Loss = -11938.159714264657
1
Iteration 6500: Loss = -11938.150181497494
Iteration 6600: Loss = -11938.132832406169
Iteration 6700: Loss = -11938.137492270747
1
Iteration 6800: Loss = -11938.13671456824
2
Iteration 6900: Loss = -11938.132242454232
Iteration 7000: Loss = -11938.135416148683
1
Iteration 7100: Loss = -11938.196118998658
2
Iteration 7200: Loss = -11938.131505488138
Iteration 7300: Loss = -11938.154103447738
1
Iteration 7400: Loss = -11938.131194046598
Iteration 7500: Loss = -11938.131940405612
1
Iteration 7600: Loss = -11938.134660910096
2
Iteration 7700: Loss = -11938.136594133186
3
Iteration 7800: Loss = -11938.140932027958
4
Iteration 7900: Loss = -11938.130654426726
Iteration 8000: Loss = -11938.131897140962
1
Iteration 8100: Loss = -11938.136619420693
2
Iteration 8200: Loss = -11938.130167450008
Iteration 8300: Loss = -11938.129821661596
Iteration 8400: Loss = -11938.128875364184
Iteration 8500: Loss = -11938.12853646007
Iteration 8600: Loss = -11938.130637172937
1
Iteration 8700: Loss = -11938.128245584378
Iteration 8800: Loss = -11938.288680747912
1
Iteration 8900: Loss = -11938.128121514663
Iteration 9000: Loss = -11938.128418725402
1
Iteration 9100: Loss = -11938.128120851965
Iteration 9200: Loss = -11938.128422465725
1
Iteration 9300: Loss = -11938.127926916944
Iteration 9400: Loss = -11938.141686852723
1
Iteration 9500: Loss = -11938.134599883108
2
Iteration 9600: Loss = -11938.149716425554
3
Iteration 9700: Loss = -11938.13504331351
4
Iteration 9800: Loss = -11938.129179508875
5
Iteration 9900: Loss = -11938.138688524958
6
Iteration 10000: Loss = -11938.143725471666
7
Iteration 10100: Loss = -11938.127162078406
Iteration 10200: Loss = -11938.127487792222
1
Iteration 10300: Loss = -11938.136988123586
2
Iteration 10400: Loss = -11938.12684983349
Iteration 10500: Loss = -11938.126114144705
Iteration 10600: Loss = -11938.132281755692
1
Iteration 10700: Loss = -11938.126210096429
Iteration 10800: Loss = -11938.129401419656
1
Iteration 10900: Loss = -11938.183313973615
2
Iteration 11000: Loss = -11938.124948580562
Iteration 11100: Loss = -11938.142067159662
1
Iteration 11200: Loss = -11938.125051965833
2
Iteration 11300: Loss = -11938.127543146082
3
Iteration 11400: Loss = -11938.125490281971
4
Iteration 11500: Loss = -11938.128675121487
5
Iteration 11600: Loss = -11938.1274106961
6
Iteration 11700: Loss = -11938.147177146971
7
Iteration 11800: Loss = -11938.125130528113
8
Iteration 11900: Loss = -11938.125363048608
9
Iteration 12000: Loss = -11938.126435528255
10
Iteration 12100: Loss = -11938.256102420688
11
Iteration 12200: Loss = -11938.129949187394
12
Iteration 12300: Loss = -11938.127050497485
13
Iteration 12400: Loss = -11938.218509969025
14
Iteration 12500: Loss = -11938.124951418944
Iteration 12600: Loss = -11938.124656948366
Iteration 12700: Loss = -11938.13512348201
1
Iteration 12800: Loss = -11938.131769760472
2
Iteration 12900: Loss = -11938.141937397777
3
Iteration 13000: Loss = -11938.131999629828
4
Iteration 13100: Loss = -11938.132530954666
5
Iteration 13200: Loss = -11938.196449677096
6
Iteration 13300: Loss = -11938.213190439381
7
Iteration 13400: Loss = -11938.160672580394
8
Iteration 13500: Loss = -11938.138028833666
9
Iteration 13600: Loss = -11938.140240228639
10
Iteration 13700: Loss = -11938.12713735457
11
Iteration 13800: Loss = -11938.129942174593
12
Iteration 13900: Loss = -11938.12474700307
Iteration 14000: Loss = -11938.124421583208
Iteration 14100: Loss = -11938.12841903345
1
Iteration 14200: Loss = -11938.125363986384
2
Iteration 14300: Loss = -11938.156736122968
3
Iteration 14400: Loss = -11938.124254155311
Iteration 14500: Loss = -11938.260899595505
1
Iteration 14600: Loss = -11938.124389682323
2
Iteration 14700: Loss = -11938.143857306928
3
Iteration 14800: Loss = -11938.1265379837
4
Iteration 14900: Loss = -11938.12467559647
5
Iteration 15000: Loss = -11938.129492354601
6
Iteration 15100: Loss = -11938.127405203375
7
Iteration 15200: Loss = -11938.211188279285
8
Iteration 15300: Loss = -11938.12508906885
9
Iteration 15400: Loss = -11938.127247897826
10
Iteration 15500: Loss = -11938.124645095237
11
Iteration 15600: Loss = -11938.170733427516
12
Iteration 15700: Loss = -11938.132689015118
13
Iteration 15800: Loss = -11938.143112857812
14
Iteration 15900: Loss = -11938.130115841803
15
Stopping early at iteration 15900 due to no improvement.
pi: tensor([[0.2561, 0.7439],
        [0.7759, 0.2241]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4299, 0.5701], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3024, 0.0943],
         [0.7012, 0.3035]],

        [[0.6120, 0.1080],
         [0.6816, 0.6653]],

        [[0.5071, 0.0999],
         [0.6680, 0.5068]],

        [[0.5439, 0.1051],
         [0.6934, 0.5017]],

        [[0.6475, 0.0997],
         [0.6259, 0.5461]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.038095866609034974
Average Adjusted Rand Index: 0.9839992163675584
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21901.654677985898
Iteration 100: Loss = -12467.192484120878
Iteration 200: Loss = -12466.610214233531
Iteration 300: Loss = -12466.467552631235
Iteration 400: Loss = -12466.390102408026
Iteration 500: Loss = -12466.332319574294
Iteration 600: Loss = -12466.282735506053
Iteration 700: Loss = -12466.237179585558
Iteration 800: Loss = -12466.193670121136
Iteration 900: Loss = -12466.150801971953
Iteration 1000: Loss = -12466.107339500631
Iteration 1100: Loss = -12466.062126399738
Iteration 1200: Loss = -12466.013658567766
Iteration 1300: Loss = -12465.961156076011
Iteration 1400: Loss = -12465.904552317365
Iteration 1500: Loss = -12465.844455174796
Iteration 1600: Loss = -12465.781124375573
Iteration 1700: Loss = -12465.714635835688
Iteration 1800: Loss = -12465.64588592475
Iteration 1900: Loss = -12465.5774155222
Iteration 2000: Loss = -12465.512328001469
Iteration 2100: Loss = -12465.452660444966
Iteration 2200: Loss = -12465.398265614991
Iteration 2300: Loss = -12465.34673533383
Iteration 2400: Loss = -12465.297202341157
Iteration 2500: Loss = -12465.251824488667
Iteration 2600: Loss = -12465.211558499232
Iteration 2700: Loss = -12465.175313658376
Iteration 2800: Loss = -12465.141250435841
Iteration 2900: Loss = -12465.109115963325
Iteration 3000: Loss = -12465.072779258016
Iteration 3100: Loss = -12465.03283626636
Iteration 3200: Loss = -12464.982040660803
Iteration 3300: Loss = -12464.940638480595
Iteration 3400: Loss = -12464.806120307607
Iteration 3500: Loss = -12464.65023215716
Iteration 3600: Loss = -12464.484233489848
Iteration 3700: Loss = -12464.381747000014
Iteration 3800: Loss = -12464.334017980915
Iteration 3900: Loss = -12464.310302772526
Iteration 4000: Loss = -12464.296272103045
Iteration 4100: Loss = -12464.286242188444
Iteration 4200: Loss = -12464.277361715422
Iteration 4300: Loss = -12464.267127026877
Iteration 4400: Loss = -12464.247794342042
Iteration 4500: Loss = -12464.147810689517
Iteration 4600: Loss = -12166.978772002907
Iteration 4700: Loss = -12014.710920476493
Iteration 4800: Loss = -11981.519294948215
Iteration 4900: Loss = -11973.32888956068
Iteration 5000: Loss = -11973.253835470561
Iteration 5100: Loss = -11962.434002027832
Iteration 5200: Loss = -11961.799232463776
Iteration 5300: Loss = -11960.08503317707
Iteration 5400: Loss = -11960.076279296882
Iteration 5500: Loss = -11959.976729848855
Iteration 5600: Loss = -11946.336417055292
Iteration 5700: Loss = -11946.32581214614
Iteration 5800: Loss = -11946.324939919581
Iteration 5900: Loss = -11946.298723398959
Iteration 6000: Loss = -11938.19358333735
Iteration 6100: Loss = -11938.170474785204
Iteration 6200: Loss = -11938.167182207939
Iteration 6300: Loss = -11938.164986381365
Iteration 6400: Loss = -11938.163905540483
Iteration 6500: Loss = -11938.169241013815
1
Iteration 6600: Loss = -11938.141058349767
Iteration 6700: Loss = -11938.138853139639
Iteration 6800: Loss = -11938.140049079802
1
Iteration 6900: Loss = -11938.139733032705
2
Iteration 7000: Loss = -11938.138378610207
Iteration 7100: Loss = -11938.140404420108
1
Iteration 7200: Loss = -11938.135712881858
Iteration 7300: Loss = -11938.138537718562
1
Iteration 7400: Loss = -11938.16633035045
2
Iteration 7500: Loss = -11938.13900352771
3
Iteration 7600: Loss = -11938.145734716707
4
Iteration 7700: Loss = -11938.219958690606
5
Iteration 7800: Loss = -11938.1335564959
Iteration 7900: Loss = -11938.136723794276
1
Iteration 8000: Loss = -11938.133177556832
Iteration 8100: Loss = -11938.143263236663
1
Iteration 8200: Loss = -11938.13287203257
Iteration 8300: Loss = -11938.132738483013
Iteration 8400: Loss = -11938.132762076226
Iteration 8500: Loss = -11938.132475216844
Iteration 8600: Loss = -11938.132537239291
Iteration 8700: Loss = -11938.132336414234
Iteration 8800: Loss = -11938.13206496809
Iteration 8900: Loss = -11938.145726611492
1
Iteration 9000: Loss = -11938.131747166344
Iteration 9100: Loss = -11938.13145000291
Iteration 9200: Loss = -11938.13088590244
Iteration 9300: Loss = -11938.13182800369
1
Iteration 9400: Loss = -11938.178218098652
2
Iteration 9500: Loss = -11938.211858614646
3
Iteration 9600: Loss = -11938.133655923188
4
Iteration 9700: Loss = -11938.129678546911
Iteration 9800: Loss = -11938.182993310575
1
Iteration 9900: Loss = -11938.142906631616
2
Iteration 10000: Loss = -11938.233920644781
3
Iteration 10100: Loss = -11938.1685586341
4
Iteration 10200: Loss = -11938.145022095672
5
Iteration 10300: Loss = -11938.1298936641
6
Iteration 10400: Loss = -11938.135777081461
7
Iteration 10500: Loss = -11938.128992939968
Iteration 10600: Loss = -11938.138211488602
1
Iteration 10700: Loss = -11938.127979379491
Iteration 10800: Loss = -11938.190495392868
1
Iteration 10900: Loss = -11938.148017077798
2
Iteration 11000: Loss = -11938.198743468702
3
Iteration 11100: Loss = -11938.14068216345
4
Iteration 11200: Loss = -11938.126233783678
Iteration 11300: Loss = -11938.130854980163
1
Iteration 11400: Loss = -11938.125458477114
Iteration 11500: Loss = -11938.127908206525
1
Iteration 11600: Loss = -11938.126899062258
2
Iteration 11700: Loss = -11938.126527267103
3
Iteration 11800: Loss = -11938.128620336056
4
Iteration 11900: Loss = -11938.162623462085
5
Iteration 12000: Loss = -11938.13197842967
6
Iteration 12100: Loss = -11938.161854487711
7
Iteration 12200: Loss = -11938.12830380217
8
Iteration 12300: Loss = -11938.127941844277
9
Iteration 12400: Loss = -11938.164875037124
10
Iteration 12500: Loss = -11938.13089878288
11
Iteration 12600: Loss = -11938.14302090884
12
Iteration 12700: Loss = -11938.14582820575
13
Iteration 12800: Loss = -11938.210710811427
14
Iteration 12900: Loss = -11938.127153197187
15
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[0.2561, 0.7439],
        [0.7759, 0.2241]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4298, 0.5702], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3024, 0.0943],
         [0.6379, 0.3035]],

        [[0.5841, 0.1080],
         [0.7125, 0.5383]],

        [[0.6431, 0.0994],
         [0.6836, 0.5821]],

        [[0.5896, 0.1051],
         [0.5877, 0.7057]],

        [[0.5282, 0.0989],
         [0.6258, 0.5180]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.038095866609034974
Average Adjusted Rand Index: 0.9839992163675584
11945.322059138682
[0.038095866609034974, 0.038095866609034974] [0.9839992163675584, 0.9839992163675584] [11938.130115841803, 11938.127153197187]
-------------------------------------
This iteration is 6
True Objective function: Loss = -12056.058284644521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21656.661540216825
Iteration 100: Loss = -12573.341067665997
Iteration 200: Loss = -12572.785795930104
Iteration 300: Loss = -12572.648208731789
Iteration 400: Loss = -12572.580448757022
Iteration 500: Loss = -12572.53559336142
Iteration 600: Loss = -12572.498373435154
Iteration 700: Loss = -12572.46183000384
Iteration 800: Loss = -12572.42042048556
Iteration 900: Loss = -12572.36703150155
Iteration 1000: Loss = -12572.29221523259
Iteration 1100: Loss = -12572.187060324475
Iteration 1200: Loss = -12572.060877965107
Iteration 1300: Loss = -12571.95212044921
Iteration 1400: Loss = -12571.857549659244
Iteration 1500: Loss = -12571.723820008629
Iteration 1600: Loss = -12571.535330919292
Iteration 1700: Loss = -12571.407180267608
Iteration 1800: Loss = -12571.341033711818
Iteration 1900: Loss = -12571.302434998695
Iteration 2000: Loss = -12571.275722667278
Iteration 2100: Loss = -12571.253385692733
Iteration 2200: Loss = -12571.22894358021
Iteration 2300: Loss = -12571.18986313002
Iteration 2400: Loss = -12571.127450905073
Iteration 2500: Loss = -12571.08284908933
Iteration 2600: Loss = -12571.065453684763
Iteration 2700: Loss = -12571.056683042581
Iteration 2800: Loss = -12571.050368866878
Iteration 2900: Loss = -12571.045145944134
Iteration 3000: Loss = -12571.040514225842
Iteration 3100: Loss = -12571.036111556903
Iteration 3200: Loss = -12571.031721805002
Iteration 3300: Loss = -12571.026971222504
Iteration 3400: Loss = -12571.021227699657
Iteration 3500: Loss = -12571.013011496616
Iteration 3600: Loss = -12570.99778729262
Iteration 3700: Loss = -12570.954351599219
Iteration 3800: Loss = -12570.683156759073
Iteration 3900: Loss = -12200.763627061662
Iteration 4000: Loss = -12140.808544193333
Iteration 4100: Loss = -12112.631165096036
Iteration 4200: Loss = -12111.788604751502
Iteration 4300: Loss = -12100.188289220932
Iteration 4400: Loss = -12090.972137094855
Iteration 4500: Loss = -12081.674697179567
Iteration 4600: Loss = -12081.585420259205
Iteration 4700: Loss = -12081.580073554636
Iteration 4800: Loss = -12081.576361384765
Iteration 4900: Loss = -12081.573982104454
Iteration 5000: Loss = -12081.5720102404
Iteration 5100: Loss = -12081.570310231196
Iteration 5200: Loss = -12081.568824959513
Iteration 5300: Loss = -12081.567438777154
Iteration 5400: Loss = -12081.566212662638
Iteration 5500: Loss = -12081.564557436242
Iteration 5600: Loss = -12065.6736666342
Iteration 5700: Loss = -12065.640468513739
Iteration 5800: Loss = -12057.113402572268
Iteration 5900: Loss = -12057.125182499773
1
Iteration 6000: Loss = -12048.323612326289
Iteration 6100: Loss = -12048.270450791988
Iteration 6200: Loss = -12048.269417813457
Iteration 6300: Loss = -12048.268863157078
Iteration 6400: Loss = -12048.270659738395
1
Iteration 6500: Loss = -12048.2679309899
Iteration 6600: Loss = -12048.267534299066
Iteration 6700: Loss = -12048.271868352398
1
Iteration 6800: Loss = -12048.26673912865
Iteration 6900: Loss = -12048.269089096739
1
Iteration 7000: Loss = -12048.26829187614
2
Iteration 7100: Loss = -12048.265011515778
Iteration 7200: Loss = -12048.27593545775
1
Iteration 7300: Loss = -12048.265756307575
2
Iteration 7400: Loss = -12048.057558936815
Iteration 7500: Loss = -12048.057543777706
Iteration 7600: Loss = -12048.059919696136
1
Iteration 7700: Loss = -12048.056786731979
Iteration 7800: Loss = -12048.067572140335
1
Iteration 7900: Loss = -12048.05861868139
2
Iteration 8000: Loss = -12048.056309755955
Iteration 8100: Loss = -12048.056551521753
1
Iteration 8200: Loss = -12048.056559781753
2
Iteration 8300: Loss = -12048.055911992538
Iteration 8400: Loss = -12048.056062230737
1
Iteration 8500: Loss = -12048.055776567491
Iteration 8600: Loss = -12048.058751621376
1
Iteration 8700: Loss = -12048.05953748568
2
Iteration 8800: Loss = -12048.058544726093
3
Iteration 8900: Loss = -12048.055458104114
Iteration 9000: Loss = -12048.05595211294
1
Iteration 9100: Loss = -12048.150195412898
2
Iteration 9200: Loss = -12048.055232200899
Iteration 9300: Loss = -12048.055457163258
1
Iteration 9400: Loss = -12048.055094721714
Iteration 9500: Loss = -12048.05523751898
1
Iteration 9600: Loss = -12048.058766150483
2
Iteration 9700: Loss = -12048.054076769185
Iteration 9800: Loss = -12048.060048387697
1
Iteration 9900: Loss = -12048.053701191373
Iteration 10000: Loss = -12048.054076961154
1
Iteration 10100: Loss = -12048.250364654632
2
Iteration 10200: Loss = -12048.056296497894
3
Iteration 10300: Loss = -12048.067748310905
4
Iteration 10400: Loss = -12048.053192157604
Iteration 10500: Loss = -12048.05429146593
1
Iteration 10600: Loss = -12046.646771122096
Iteration 10700: Loss = -12046.602033665287
Iteration 10800: Loss = -12046.607510747639
1
Iteration 10900: Loss = -12046.600794546612
Iteration 11000: Loss = -12046.601513301828
1
Iteration 11100: Loss = -12046.600688254715
Iteration 11200: Loss = -12046.601226084209
1
Iteration 11300: Loss = -12046.603827475536
2
Iteration 11400: Loss = -12046.601039863126
3
Iteration 11500: Loss = -12046.599493150557
Iteration 11600: Loss = -12046.819814589004
1
Iteration 11700: Loss = -12046.59884138077
Iteration 11800: Loss = -12046.632532576674
1
Iteration 11900: Loss = -12046.61472831268
2
Iteration 12000: Loss = -12046.602554066194
3
Iteration 12100: Loss = -12046.600947525714
4
Iteration 12200: Loss = -12046.61297937188
5
Iteration 12300: Loss = -12046.651016861982
6
Iteration 12400: Loss = -12046.616180821455
7
Iteration 12500: Loss = -12046.618366098774
8
Iteration 12600: Loss = -12046.598851815274
Iteration 12700: Loss = -12046.59998185981
1
Iteration 12800: Loss = -12046.600169694882
2
Iteration 12900: Loss = -12046.761806438963
3
Iteration 13000: Loss = -12046.600374137734
4
Iteration 13100: Loss = -12046.599887276092
5
Iteration 13200: Loss = -12046.603289930397
6
Iteration 13300: Loss = -12046.611402284683
7
Iteration 13400: Loss = -12046.599910372253
8
Iteration 13500: Loss = -12046.603069386436
9
Iteration 13600: Loss = -12046.610101603115
10
Iteration 13700: Loss = -12046.755191330762
11
Iteration 13800: Loss = -12046.584037559676
Iteration 13900: Loss = -12046.590381004758
1
Iteration 14000: Loss = -12046.64598358486
2
Iteration 14100: Loss = -12046.584615027507
3
Iteration 14200: Loss = -12046.591499340087
4
Iteration 14300: Loss = -12046.607867614637
5
Iteration 14400: Loss = -12046.58396700918
Iteration 14500: Loss = -12046.5832048108
Iteration 14600: Loss = -12046.59535034422
1
Iteration 14700: Loss = -12046.583147154586
Iteration 14800: Loss = -12046.624748055392
1
Iteration 14900: Loss = -12046.586956256222
2
Iteration 15000: Loss = -12046.600227833658
3
Iteration 15100: Loss = -12046.582840037658
Iteration 15200: Loss = -12046.584349546196
1
Iteration 15300: Loss = -12046.58580007576
2
Iteration 15400: Loss = -12046.58293779975
Iteration 15500: Loss = -12046.61477950802
1
Iteration 15600: Loss = -12046.585481606935
2
Iteration 15700: Loss = -12046.589187578276
3
Iteration 15800: Loss = -12046.59170147835
4
Iteration 15900: Loss = -12046.583696226066
5
Iteration 16000: Loss = -12046.583892637826
6
Iteration 16100: Loss = -12046.606945117144
7
Iteration 16200: Loss = -12046.582974779301
Iteration 16300: Loss = -12046.59121184971
1
Iteration 16400: Loss = -12046.622099734586
2
Iteration 16500: Loss = -12046.583084622875
3
Iteration 16600: Loss = -12046.583722153384
4
Iteration 16700: Loss = -12046.593075411725
5
Iteration 16800: Loss = -12046.591262361095
6
Iteration 16900: Loss = -12046.582935597095
Iteration 17000: Loss = -12046.583288509013
1
Iteration 17100: Loss = -12046.617651307093
2
Iteration 17200: Loss = -12046.584326861659
3
Iteration 17300: Loss = -12046.583552943881
4
Iteration 17400: Loss = -12046.588568442143
5
Iteration 17500: Loss = -12046.690520138676
6
Iteration 17600: Loss = -12046.588377046914
7
Iteration 17700: Loss = -12046.585841794371
8
Iteration 17800: Loss = -12046.5905008884
9
Iteration 17900: Loss = -12046.626701332494
10
Iteration 18000: Loss = -12046.587577021668
11
Iteration 18100: Loss = -12046.582775716313
Iteration 18200: Loss = -12046.59037374804
1
Iteration 18300: Loss = -12046.599356019344
2
Iteration 18400: Loss = -12046.595054904652
3
Iteration 18500: Loss = -12046.592749243819
4
Iteration 18600: Loss = -12046.584263055227
5
Iteration 18700: Loss = -12046.588393390875
6
Iteration 18800: Loss = -12046.617900044568
7
Iteration 18900: Loss = -12046.587056822065
8
Iteration 19000: Loss = -12046.590412035008
9
Iteration 19100: Loss = -12046.583076519077
10
Iteration 19200: Loss = -12046.58309918534
11
Iteration 19300: Loss = -12046.685505688249
12
Iteration 19400: Loss = -12046.592465321351
13
Iteration 19500: Loss = -12046.58701410421
14
Iteration 19600: Loss = -12046.582887843946
15
Stopping early at iteration 19600 due to no improvement.
pi: tensor([[0.2288, 0.7712],
        [0.7169, 0.2831]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4204, 0.5796], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3112, 0.1071],
         [0.6951, 0.3027]],

        [[0.5905, 0.1045],
         [0.5565, 0.5077]],

        [[0.5585, 0.1025],
         [0.6684, 0.6138]],

        [[0.7102, 0.1057],
         [0.5254, 0.5123]],

        [[0.6580, 0.0953],
         [0.6146, 0.6786]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03805587815811412
Average Adjusted Rand Index: 0.9839979937370392
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19481.947803844258
Iteration 100: Loss = -12572.82383768939
Iteration 200: Loss = -12572.503763313181
Iteration 300: Loss = -12572.44873766534
Iteration 400: Loss = -12572.398021610705
Iteration 500: Loss = -12572.325855962203
Iteration 600: Loss = -12572.191813765025
Iteration 700: Loss = -12571.920721157354
Iteration 800: Loss = -12571.646052135355
Iteration 900: Loss = -12571.514339465697
Iteration 1000: Loss = -12571.448336957399
Iteration 1100: Loss = -12571.39530629842
Iteration 1200: Loss = -12571.28825919893
Iteration 1300: Loss = -12570.406154639146
Iteration 1400: Loss = -12568.386969838959
Iteration 1500: Loss = -12567.336275155241
Iteration 1600: Loss = -12566.629344554223
Iteration 1700: Loss = -12566.08253528636
Iteration 1800: Loss = -12565.711281661499
Iteration 1900: Loss = -12565.476932983975
Iteration 2000: Loss = -12565.332119305358
Iteration 2100: Loss = -12565.249882713124
Iteration 2200: Loss = -12565.202439369114
Iteration 2300: Loss = -12565.174129627001
Iteration 2400: Loss = -12565.153269968927
Iteration 2500: Loss = -12565.13877069714
Iteration 2600: Loss = -12565.128807122166
Iteration 2700: Loss = -12565.12095641413
Iteration 2800: Loss = -12565.114379611477
Iteration 2900: Loss = -12565.108618837721
Iteration 3000: Loss = -12565.103896851122
Iteration 3100: Loss = -12565.10009245003
Iteration 3200: Loss = -12565.096998834118
Iteration 3300: Loss = -12565.094380938643
Iteration 3400: Loss = -12565.092071269346
Iteration 3500: Loss = -12565.090103676523
Iteration 3600: Loss = -12565.088353800658
Iteration 3700: Loss = -12565.08681591476
Iteration 3800: Loss = -12565.085429241153
Iteration 3900: Loss = -12565.084234029
Iteration 4000: Loss = -12565.083088537654
Iteration 4100: Loss = -12565.082054762628
Iteration 4200: Loss = -12565.081112429907
Iteration 4300: Loss = -12565.080211832008
Iteration 4400: Loss = -12565.079484479787
Iteration 4500: Loss = -12565.078767745681
Iteration 4600: Loss = -12565.078073167011
Iteration 4700: Loss = -12565.077440607503
Iteration 4800: Loss = -12565.07686318567
Iteration 4900: Loss = -12565.07632212484
Iteration 5000: Loss = -12565.07584863273
Iteration 5100: Loss = -12565.075308302043
Iteration 5200: Loss = -12565.07484657511
Iteration 5300: Loss = -12565.074298908034
Iteration 5400: Loss = -12565.07355711113
Iteration 5500: Loss = -12565.072598430776
Iteration 5600: Loss = -12565.072224356549
Iteration 5700: Loss = -12565.071943129891
Iteration 5800: Loss = -12565.071621147345
Iteration 5900: Loss = -12565.071383768936
Iteration 6000: Loss = -12565.071109407982
Iteration 6100: Loss = -12565.070877063254
Iteration 6200: Loss = -12565.070627124242
Iteration 6300: Loss = -12565.07040280455
Iteration 6400: Loss = -12565.07020737144
Iteration 6500: Loss = -12565.070053764808
Iteration 6600: Loss = -12565.06983383628
Iteration 6700: Loss = -12565.06972024084
Iteration 6800: Loss = -12565.07052213811
1
Iteration 6900: Loss = -12565.069326364108
Iteration 7000: Loss = -12565.069206530581
Iteration 7100: Loss = -12565.078123872772
1
Iteration 7200: Loss = -12565.068935130981
Iteration 7300: Loss = -12565.068844225632
Iteration 7400: Loss = -12565.068666435589
Iteration 7500: Loss = -12565.10756225442
1
Iteration 7600: Loss = -12565.068456327348
Iteration 7700: Loss = -12565.068374071428
Iteration 7800: Loss = -12565.068244510803
Iteration 7900: Loss = -12565.068236018604
Iteration 8000: Loss = -12565.068088966493
Iteration 8100: Loss = -12565.067951926681
Iteration 8200: Loss = -12565.068004104603
Iteration 8300: Loss = -12565.06777641747
Iteration 8400: Loss = -12565.067625495354
Iteration 8500: Loss = -12565.067570704432
Iteration 8600: Loss = -12565.07622611532
1
Iteration 8700: Loss = -12565.067421473837
Iteration 8800: Loss = -12565.067369254002
Iteration 8900: Loss = -12565.067521387975
1
Iteration 9000: Loss = -12565.067341039337
Iteration 9100: Loss = -12565.067209743445
Iteration 9200: Loss = -12565.067146905942
Iteration 9300: Loss = -12565.067382688032
1
Iteration 9400: Loss = -12565.067123202436
Iteration 9500: Loss = -12565.067087891022
Iteration 9600: Loss = -12565.121524723147
1
Iteration 9700: Loss = -12565.066992395263
Iteration 9800: Loss = -12565.06694856533
Iteration 9900: Loss = -12565.0669285544
Iteration 10000: Loss = -12565.06768956773
1
Iteration 10100: Loss = -12565.066852834898
Iteration 10200: Loss = -12565.066843446144
Iteration 10300: Loss = -12565.53213556191
1
Iteration 10400: Loss = -12565.066736480128
Iteration 10500: Loss = -12565.066745885619
Iteration 10600: Loss = -12565.06667697614
Iteration 10700: Loss = -12565.187638780646
1
Iteration 10800: Loss = -12565.066693200322
Iteration 10900: Loss = -12565.066639967183
Iteration 11000: Loss = -12565.067292897646
1
Iteration 11100: Loss = -12565.066604187881
Iteration 11200: Loss = -12565.06658813279
Iteration 11300: Loss = -12565.069792277125
1
Iteration 11400: Loss = -12565.066567856346
Iteration 11500: Loss = -12565.066542295397
Iteration 11600: Loss = -12565.088194309388
1
Iteration 11700: Loss = -12565.066505085104
Iteration 11800: Loss = -12565.066480806414
Iteration 11900: Loss = -12565.721173713488
1
Iteration 12000: Loss = -12565.06644010175
Iteration 12100: Loss = -12565.06673560453
1
Iteration 12200: Loss = -12565.066382811014
Iteration 12300: Loss = -12565.069923482031
1
Iteration 12400: Loss = -12565.06647377991
Iteration 12500: Loss = -12565.066389338752
Iteration 12600: Loss = -12565.073835310037
1
Iteration 12700: Loss = -12565.066379547046
Iteration 12800: Loss = -12565.06654798762
1
Iteration 12900: Loss = -12565.066448630094
Iteration 13000: Loss = -12565.066422508024
Iteration 13100: Loss = -12565.06637206813
Iteration 13200: Loss = -12565.067660477072
1
Iteration 13300: Loss = -12565.067238363728
2
Iteration 13400: Loss = -12565.066429115548
Iteration 13500: Loss = -12565.066414823048
Iteration 13600: Loss = -12565.06664748139
1
Iteration 13700: Loss = -12565.07647547229
2
Iteration 13800: Loss = -12565.06643282353
Iteration 13900: Loss = -12565.070027967191
1
Iteration 14000: Loss = -12565.066370114222
Iteration 14100: Loss = -12565.066306866083
Iteration 14200: Loss = -12565.066509367729
1
Iteration 14300: Loss = -12565.066309560903
Iteration 14400: Loss = -12565.082998227537
1
Iteration 14500: Loss = -12565.066326406468
Iteration 14600: Loss = -12565.066739805243
1
Iteration 14700: Loss = -12565.084592517702
2
Iteration 14800: Loss = -12565.066388347028
Iteration 14900: Loss = -12565.06631624362
Iteration 15000: Loss = -12565.066433488662
1
Iteration 15100: Loss = -12565.066343326267
Iteration 15200: Loss = -12565.067608155196
1
Iteration 15300: Loss = -12565.06682839286
2
Iteration 15400: Loss = -12565.109719983402
3
Iteration 15500: Loss = -12565.066765710126
4
Iteration 15600: Loss = -12565.06650100327
5
Iteration 15700: Loss = -12565.084354987608
6
Iteration 15800: Loss = -12565.067519896751
7
Iteration 15900: Loss = -12565.06682068147
8
Iteration 16000: Loss = -12565.083970644548
9
Iteration 16100: Loss = -12565.06631688334
Iteration 16200: Loss = -12565.066557795704
1
Iteration 16300: Loss = -12565.282254853148
2
Iteration 16400: Loss = -12565.066385630582
Iteration 16500: Loss = -12565.067665746297
1
Iteration 16600: Loss = -12565.085499089857
2
Iteration 16700: Loss = -12565.076242139234
3
Iteration 16800: Loss = -12565.06625713456
Iteration 16900: Loss = -12565.071298957153
1
Iteration 17000: Loss = -12565.216148286427
2
Iteration 17100: Loss = -12565.066327132728
Iteration 17200: Loss = -12565.066361095449
Iteration 17300: Loss = -12565.105743195762
1
Iteration 17400: Loss = -12565.066254489724
Iteration 17500: Loss = -12565.066901706232
1
Iteration 17600: Loss = -12565.066262615439
Iteration 17700: Loss = -12565.066558748453
1
Iteration 17800: Loss = -12565.06682725604
2
Iteration 17900: Loss = -12565.091580216778
3
Iteration 18000: Loss = -12565.066440405753
4
Iteration 18100: Loss = -12565.111830916068
5
Iteration 18200: Loss = -12565.066353568589
Iteration 18300: Loss = -12565.076899269592
1
Iteration 18400: Loss = -12565.06632635432
Iteration 18500: Loss = -12565.067486340919
1
Iteration 18600: Loss = -12565.081063298296
2
Iteration 18700: Loss = -12565.066275766418
Iteration 18800: Loss = -12565.07031250614
1
Iteration 18900: Loss = -12565.223129937162
2
Iteration 19000: Loss = -12565.066264644007
Iteration 19100: Loss = -12565.066788409646
1
Iteration 19200: Loss = -12565.133200041453
2
Iteration 19300: Loss = -12565.066232730705
Iteration 19400: Loss = -12565.067702362147
1
Iteration 19500: Loss = -12565.066407403698
2
Iteration 19600: Loss = -12565.067344332247
3
Iteration 19700: Loss = -12565.139808651285
4
Iteration 19800: Loss = -12565.066285675832
Iteration 19900: Loss = -12565.067352591635
1
pi: tensor([[1.0000e+00, 1.5349e-07],
        [1.3994e-03, 9.9860e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0200, 0.9800], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5840, 0.1017],
         [0.7096, 0.2071]],

        [[0.5697, 0.1292],
         [0.6708, 0.5382]],

        [[0.5159, 0.2569],
         [0.5084, 0.7184]],

        [[0.5987, 0.1520],
         [0.7109, 0.5226]],

        [[0.6762, 0.1914],
         [0.7009, 0.5401]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.025967371000118884
Global Adjusted Rand Index: 0.01032522015942593
Average Adjusted Rand Index: 0.01048800095140955
12056.058284644521
[0.03805587815811412, 0.01032522015942593] [0.9839979937370392, 0.01048800095140955] [12046.582887843946, 12565.066926477599]
-------------------------------------
This iteration is 7
True Objective function: Loss = -11986.680313909359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23278.327410328555
Iteration 100: Loss = -12514.8819681925
Iteration 200: Loss = -12513.908907788817
Iteration 300: Loss = -12512.995106021086
Iteration 400: Loss = -12510.944679323338
Iteration 500: Loss = -12510.06116795603
Iteration 600: Loss = -12509.56666123231
Iteration 700: Loss = -12509.252170492728
Iteration 800: Loss = -12509.061790853055
Iteration 900: Loss = -12508.93497100853
Iteration 1000: Loss = -12508.53193399582
Iteration 1100: Loss = -12362.36617787876
Iteration 1200: Loss = -12164.20948038769
Iteration 1300: Loss = -12069.205700204055
Iteration 1400: Loss = -12038.037970782123
Iteration 1500: Loss = -12033.682634190043
Iteration 1600: Loss = -12033.600510984661
Iteration 1700: Loss = -12007.665570635032
Iteration 1800: Loss = -12007.625548190144
Iteration 1900: Loss = -12007.60202104442
Iteration 2000: Loss = -12007.585576119887
Iteration 2100: Loss = -12007.57345315761
Iteration 2200: Loss = -12007.563800298238
Iteration 2300: Loss = -12007.555990894647
Iteration 2400: Loss = -12007.552522264428
Iteration 2500: Loss = -12007.54372311179
Iteration 2600: Loss = -12007.538438057414
Iteration 2700: Loss = -12007.533374112456
Iteration 2800: Loss = -12007.528858155849
Iteration 2900: Loss = -12007.525135922968
Iteration 3000: Loss = -12007.522413504514
Iteration 3100: Loss = -12007.520974170538
Iteration 3200: Loss = -12007.51888680454
Iteration 3300: Loss = -12007.517143145376
Iteration 3400: Loss = -12007.515075764795
Iteration 3500: Loss = -12007.522022719591
1
Iteration 3600: Loss = -12007.512474233605
Iteration 3700: Loss = -12007.511646650248
Iteration 3800: Loss = -12007.51932723335
1
Iteration 3900: Loss = -12007.509805217775
Iteration 4000: Loss = -12007.50860391032
Iteration 4100: Loss = -12007.508915806098
1
Iteration 4200: Loss = -12007.507023334605
Iteration 4300: Loss = -12007.509087650627
1
Iteration 4400: Loss = -12007.505694657008
Iteration 4500: Loss = -12007.50518066127
Iteration 4600: Loss = -12007.5045039072
Iteration 4700: Loss = -12007.503981430247
Iteration 4800: Loss = -12007.504621219457
1
Iteration 4900: Loss = -12007.502987869082
Iteration 5000: Loss = -12007.517951151445
1
Iteration 5100: Loss = -12007.503451106271
2
Iteration 5200: Loss = -12007.506047196002
3
Iteration 5300: Loss = -12007.501640068387
Iteration 5400: Loss = -12007.501285738337
Iteration 5500: Loss = -12007.500919763825
Iteration 5600: Loss = -12007.500629359376
Iteration 5700: Loss = -12007.500904634302
1
Iteration 5800: Loss = -12007.500261604693
Iteration 5900: Loss = -12007.500512975595
1
Iteration 6000: Loss = -12007.500826097319
2
Iteration 6100: Loss = -12007.505232159632
3
Iteration 6200: Loss = -12007.499473174377
Iteration 6300: Loss = -12007.499416966879
Iteration 6400: Loss = -12007.511546670024
1
Iteration 6500: Loss = -12007.49893786227
Iteration 6600: Loss = -12007.499373264549
1
Iteration 6700: Loss = -12007.503017566823
2
Iteration 6800: Loss = -12007.498999225683
Iteration 6900: Loss = -12007.498432887756
Iteration 7000: Loss = -12007.498407936657
Iteration 7100: Loss = -12007.498268162119
Iteration 7200: Loss = -12007.49823867429
Iteration 7300: Loss = -12007.499276114677
1
Iteration 7400: Loss = -12007.502867091833
2
Iteration 7500: Loss = -12007.500766406836
3
Iteration 7600: Loss = -12007.498425089361
4
Iteration 7700: Loss = -12007.497851820557
Iteration 7800: Loss = -12007.49769243255
Iteration 7900: Loss = -12007.49826560935
1
Iteration 8000: Loss = -12007.502229614594
2
Iteration 8100: Loss = -12007.506425565361
3
Iteration 8200: Loss = -12007.53353526415
4
Iteration 8300: Loss = -12007.498021173395
5
Iteration 8400: Loss = -12007.497456549381
Iteration 8500: Loss = -12007.497565881575
1
Iteration 8600: Loss = -12007.498810336836
2
Iteration 8700: Loss = -12007.500110638612
3
Iteration 8800: Loss = -12007.497283319904
Iteration 8900: Loss = -12007.497433843784
1
Iteration 9000: Loss = -12007.521242663983
2
Iteration 9100: Loss = -12007.497179931124
Iteration 9200: Loss = -12007.497501856344
1
Iteration 9300: Loss = -12007.518124984756
2
Iteration 9400: Loss = -12007.499506621547
3
Iteration 9500: Loss = -12007.500921283285
4
Iteration 9600: Loss = -12007.511291448813
5
Iteration 9700: Loss = -12007.498717853989
6
Iteration 9800: Loss = -12007.52018205926
7
Iteration 9900: Loss = -12007.499100517754
8
Iteration 10000: Loss = -12007.49833533313
9
Iteration 10100: Loss = -12007.502602112789
10
Iteration 10200: Loss = -12007.562763136188
11
Iteration 10300: Loss = -12007.49869942359
12
Iteration 10400: Loss = -12007.497581698284
13
Iteration 10500: Loss = -12007.500084938953
14
Iteration 10600: Loss = -12007.501424297789
15
Stopping early at iteration 10600 due to no improvement.
pi: tensor([[0.6289, 0.3711],
        [0.3680, 0.6320]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4770, 0.5230], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2960, 0.0938],
         [0.5519, 0.3153]],

        [[0.5894, 0.1157],
         [0.6899, 0.6609]],

        [[0.5512, 0.1115],
         [0.5515, 0.6836]],

        [[0.7093, 0.0873],
         [0.7024, 0.6488]],

        [[0.7058, 0.1007],
         [0.6992, 0.6589]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.3539216032332437
Average Adjusted Rand Index: 0.9919945110819786
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19824.79140128962
Iteration 100: Loss = -12514.124185895542
Iteration 200: Loss = -12513.337222310176
Iteration 300: Loss = -12512.711338283058
Iteration 400: Loss = -12511.249675051788
Iteration 500: Loss = -12510.059572522381
Iteration 600: Loss = -12509.656836026146
Iteration 700: Loss = -12509.430489805836
Iteration 800: Loss = -12509.265253578564
Iteration 900: Loss = -12509.13636227556
Iteration 1000: Loss = -12509.037616194404
Iteration 1100: Loss = -12508.955851917244
Iteration 1200: Loss = -12508.867505136968
Iteration 1300: Loss = -12507.28633092205
Iteration 1400: Loss = -12274.43875613536
Iteration 1500: Loss = -12149.234315265654
Iteration 1600: Loss = -12114.081301152108
Iteration 1700: Loss = -12061.433149361888
Iteration 1800: Loss = -12016.795617567226
Iteration 1900: Loss = -12010.269033331906
Iteration 2000: Loss = -11989.857371379569
Iteration 2100: Loss = -11975.012798363872
Iteration 2200: Loss = -11974.98702567046
Iteration 2300: Loss = -11974.970148830447
Iteration 2400: Loss = -11974.958076965846
Iteration 2500: Loss = -11974.948402194375
Iteration 2600: Loss = -11974.941284255368
Iteration 2700: Loss = -11974.935585488887
Iteration 2800: Loss = -11974.930578722036
Iteration 2900: Loss = -11974.926336276558
Iteration 3000: Loss = -11974.934940080855
1
Iteration 3100: Loss = -11974.919689503593
Iteration 3200: Loss = -11974.917534957644
Iteration 3300: Loss = -11974.915024311791
Iteration 3400: Loss = -11974.91316463376
Iteration 3500: Loss = -11974.914106044675
1
Iteration 3600: Loss = -11974.912984149942
Iteration 3700: Loss = -11974.90863725076
Iteration 3800: Loss = -11974.912882744673
1
Iteration 3900: Loss = -11974.906723504688
Iteration 4000: Loss = -11974.905453852003
Iteration 4100: Loss = -11974.906726682779
1
Iteration 4200: Loss = -11974.91104875461
2
Iteration 4300: Loss = -11974.903165048176
Iteration 4400: Loss = -11974.909853007626
1
Iteration 4500: Loss = -11974.901657899314
Iteration 4600: Loss = -11974.902371796084
1
Iteration 4700: Loss = -11974.900576804903
Iteration 4800: Loss = -11974.900080166652
Iteration 4900: Loss = -11974.900538031277
1
Iteration 5000: Loss = -11974.89916063116
Iteration 5100: Loss = -11974.909812531867
1
Iteration 5200: Loss = -11974.89834210234
Iteration 5300: Loss = -11974.898144216777
Iteration 5400: Loss = -11974.904052699138
1
Iteration 5500: Loss = -11974.897373307245
Iteration 5600: Loss = -11974.897045221991
Iteration 5700: Loss = -11974.896772864644
Iteration 5800: Loss = -11974.89658210393
Iteration 5900: Loss = -11974.896242598228
Iteration 6000: Loss = -11974.89912446699
1
Iteration 6100: Loss = -11974.896332809803
Iteration 6200: Loss = -11974.895628951064
Iteration 6300: Loss = -11974.904731936007
1
Iteration 6400: Loss = -11974.89505752669
Iteration 6500: Loss = -11974.894918680653
Iteration 6600: Loss = -11974.897456463626
1
Iteration 6700: Loss = -11974.89460209339
Iteration 6800: Loss = -11974.894447004615
Iteration 6900: Loss = -11974.895699955297
1
Iteration 7000: Loss = -11974.894154587893
Iteration 7100: Loss = -11974.893895065927
Iteration 7200: Loss = -11974.893815115222
Iteration 7300: Loss = -11974.893605837122
Iteration 7400: Loss = -11974.893508497396
Iteration 7500: Loss = -11974.893390187277
Iteration 7600: Loss = -11974.893261532376
Iteration 7700: Loss = -11974.893216382457
Iteration 7800: Loss = -11974.893290736007
Iteration 7900: Loss = -11974.893049973049
Iteration 8000: Loss = -11974.89299581202
Iteration 8100: Loss = -11974.892916644969
Iteration 8200: Loss = -11974.892955238685
Iteration 8300: Loss = -11974.89281932664
Iteration 8400: Loss = -11974.892723730885
Iteration 8500: Loss = -11974.892951583895
1
Iteration 8600: Loss = -11974.899437687884
2
Iteration 8700: Loss = -11974.94696458259
3
Iteration 8800: Loss = -11974.930695933532
4
Iteration 8900: Loss = -11974.892563792293
Iteration 9000: Loss = -11974.89284072019
1
Iteration 9100: Loss = -11974.892691505902
2
Iteration 9200: Loss = -11974.892456224326
Iteration 9300: Loss = -11974.904499882716
1
Iteration 9400: Loss = -11974.89252490278
Iteration 9500: Loss = -11974.893822192815
1
Iteration 9600: Loss = -11974.929432676621
2
Iteration 9700: Loss = -11974.89486637369
3
Iteration 9800: Loss = -11974.893546717156
4
Iteration 9900: Loss = -11974.892345326378
Iteration 10000: Loss = -11974.907646574911
1
Iteration 10100: Loss = -11974.89422271534
2
Iteration 10200: Loss = -11974.912494244065
3
Iteration 10300: Loss = -11974.903359314136
4
Iteration 10400: Loss = -11974.893948131097
5
Iteration 10500: Loss = -11974.892240985662
Iteration 10600: Loss = -11974.915480176169
1
Iteration 10700: Loss = -11974.8921257143
Iteration 10800: Loss = -11974.89208481508
Iteration 10900: Loss = -11974.897159558606
1
Iteration 11000: Loss = -11974.893260470326
2
Iteration 11100: Loss = -11974.893322122261
3
Iteration 11200: Loss = -11974.907893344112
4
Iteration 11300: Loss = -11974.893629891674
5
Iteration 11400: Loss = -11974.89218446258
Iteration 11500: Loss = -11974.893324866825
1
Iteration 11600: Loss = -11974.917267763045
2
Iteration 11700: Loss = -11974.902198566519
3
Iteration 11800: Loss = -11974.892204129763
Iteration 11900: Loss = -11974.895964508465
1
Iteration 12000: Loss = -11975.050970983759
2
Iteration 12100: Loss = -11974.89375659014
3
Iteration 12200: Loss = -11974.896688262772
4
Iteration 12300: Loss = -11974.893067716303
5
Iteration 12400: Loss = -11974.893842683392
6
Iteration 12500: Loss = -11974.907046511531
7
Iteration 12600: Loss = -11974.903967576438
8
Iteration 12700: Loss = -11974.894974855473
9
Iteration 12800: Loss = -11974.89954020368
10
Iteration 12900: Loss = -11974.90828927051
11
Iteration 13000: Loss = -11974.89197421927
Iteration 13100: Loss = -11974.927678647526
1
Iteration 13200: Loss = -11974.906322399833
2
Iteration 13300: Loss = -11974.892071369335
Iteration 13400: Loss = -11974.905643910606
1
Iteration 13500: Loss = -11975.057349438392
2
Iteration 13600: Loss = -11974.893557655882
3
Iteration 13700: Loss = -11974.893636384195
4
Iteration 13800: Loss = -11974.911627704018
5
Iteration 13900: Loss = -11974.892618870706
6
Iteration 14000: Loss = -11974.913111530683
7
Iteration 14100: Loss = -11974.894792896112
8
Iteration 14200: Loss = -11974.89821963986
9
Iteration 14300: Loss = -11974.893414027207
10
Iteration 14400: Loss = -11974.892247471336
11
Iteration 14500: Loss = -11974.893984122868
12
Iteration 14600: Loss = -11974.894317392931
13
Iteration 14700: Loss = -11975.108950236445
14
Iteration 14800: Loss = -11974.89193007421
Iteration 14900: Loss = -11974.892991878027
1
Iteration 15000: Loss = -11974.892573567156
2
Iteration 15100: Loss = -11974.906735496343
3
Iteration 15200: Loss = -11974.898934570136
4
Iteration 15300: Loss = -11974.892038360305
5
Iteration 15400: Loss = -11974.89339571308
6
Iteration 15500: Loss = -11974.985104882546
7
Iteration 15600: Loss = -11974.892453477909
8
Iteration 15700: Loss = -11974.89901481555
9
Iteration 15800: Loss = -11974.893362067893
10
Iteration 15900: Loss = -11974.898138244309
11
Iteration 16000: Loss = -11975.037675549964
12
Iteration 16100: Loss = -11974.8966566369
13
Iteration 16200: Loss = -11975.15327545753
14
Iteration 16300: Loss = -11974.90876890709
15
Stopping early at iteration 16300 due to no improvement.
pi: tensor([[0.7429, 0.2571],
        [0.2668, 0.7332]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4742, 0.5258], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3149, 0.0939],
         [0.7304, 0.2968]],

        [[0.6755, 0.1156],
         [0.6143, 0.7085]],

        [[0.7167, 0.1116],
         [0.5699, 0.6637]],

        [[0.6309, 0.0883],
         [0.6500, 0.5523]],

        [[0.5982, 0.1010],
         [0.6278, 0.5731]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11986.680313909359
[0.3539216032332437, 1.0] [0.9919945110819786, 1.0] [12007.501424297789, 11974.90876890709]
-------------------------------------
This iteration is 8
True Objective function: Loss = -11950.512119397208
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23793.929287952047
Iteration 100: Loss = -12529.776977940834
Iteration 200: Loss = -12529.232496814779
Iteration 300: Loss = -12529.120570993226
Iteration 400: Loss = -12529.06343649914
Iteration 500: Loss = -12529.02186242608
Iteration 600: Loss = -12528.986669263197
Iteration 700: Loss = -12528.954346825029
Iteration 800: Loss = -12528.923003671662
Iteration 900: Loss = -12528.891358005314
Iteration 1000: Loss = -12528.858238470424
Iteration 1100: Loss = -12528.822481956508
Iteration 1200: Loss = -12528.782585499222
Iteration 1300: Loss = -12528.736401528588
Iteration 1400: Loss = -12528.680669801854
Iteration 1500: Loss = -12528.610859159067
Iteration 1600: Loss = -12528.521242905115
Iteration 1700: Loss = -12528.404075989505
Iteration 1800: Loss = -12528.252527556297
Iteration 1900: Loss = -12528.06912311382
Iteration 2000: Loss = -12527.878850627898
Iteration 2100: Loss = -12527.717195964651
Iteration 2200: Loss = -12527.601643417214
Iteration 2300: Loss = -12527.527711888843
Iteration 2400: Loss = -12527.482500301228
Iteration 2500: Loss = -12527.452876638807
Iteration 2600: Loss = -12527.433296646479
Iteration 2700: Loss = -12527.420339397338
Iteration 2800: Loss = -12527.411072339406
Iteration 2900: Loss = -12527.404501114852
Iteration 3000: Loss = -12527.400093311791
Iteration 3100: Loss = -12527.396203550878
Iteration 3200: Loss = -12527.393517572675
Iteration 3300: Loss = -12527.391437779179
Iteration 3400: Loss = -12527.38958304372
Iteration 3500: Loss = -12527.388106131415
Iteration 3600: Loss = -12527.386850553838
Iteration 3700: Loss = -12527.385679341378
Iteration 3800: Loss = -12527.384630038292
Iteration 3900: Loss = -12527.383776601684
Iteration 4000: Loss = -12527.382809914903
Iteration 4100: Loss = -12527.38261947821
Iteration 4200: Loss = -12527.381036050912
Iteration 4300: Loss = -12527.37992418326
Iteration 4400: Loss = -12527.379178107167
Iteration 4500: Loss = -12527.377856973126
Iteration 4600: Loss = -12527.381088604488
1
Iteration 4700: Loss = -12527.375351024784
Iteration 4800: Loss = -12527.395024358879
1
Iteration 4900: Loss = -12527.372589099408
Iteration 5000: Loss = -12527.371055330577
Iteration 5100: Loss = -12527.370055758689
Iteration 5200: Loss = -12527.368173637407
Iteration 5300: Loss = -12527.370456431876
1
Iteration 5400: Loss = -12527.365776063996
Iteration 5500: Loss = -12527.364750790606
Iteration 5600: Loss = -12527.373585252179
1
Iteration 5700: Loss = -12527.362754866319
Iteration 5800: Loss = -12527.361789901428
Iteration 5900: Loss = -12527.444368139677
1
Iteration 6000: Loss = -12527.35920227262
Iteration 6100: Loss = -12527.35692912121
Iteration 6200: Loss = -12527.35641981277
Iteration 6300: Loss = -12527.343173670337
Iteration 6400: Loss = -12527.314087553015
Iteration 6500: Loss = -12527.24680053652
Iteration 6600: Loss = -12527.193344966985
Iteration 6700: Loss = -12527.164679781925
Iteration 6800: Loss = -12527.130239742743
Iteration 6900: Loss = -12527.059799944085
Iteration 7000: Loss = -12527.016174378039
Iteration 7100: Loss = -12527.005360361607
Iteration 7200: Loss = -12527.00149205071
Iteration 7300: Loss = -12526.999418791167
Iteration 7400: Loss = -12526.99843070874
Iteration 7500: Loss = -12526.997207813623
Iteration 7600: Loss = -12526.997517823094
1
Iteration 7700: Loss = -12526.99590212779
Iteration 7800: Loss = -12527.00120300772
1
Iteration 7900: Loss = -12526.995051055366
Iteration 8000: Loss = -12526.99865584747
1
Iteration 8100: Loss = -12526.994392800023
Iteration 8200: Loss = -12526.994333872748
Iteration 8300: Loss = -12526.993958587991
Iteration 8400: Loss = -12527.000526877291
1
Iteration 8500: Loss = -12527.003086830126
2
Iteration 8600: Loss = -12527.01615092415
3
Iteration 8700: Loss = -12526.993310801232
Iteration 8800: Loss = -12526.993398867446
Iteration 8900: Loss = -12527.134117019323
1
Iteration 9000: Loss = -12526.993494796898
Iteration 9100: Loss = -12526.994484738694
1
Iteration 9200: Loss = -12526.993791980189
2
Iteration 9300: Loss = -12526.993083788233
Iteration 9400: Loss = -12526.992735624457
Iteration 9500: Loss = -12526.995340143794
1
Iteration 9600: Loss = -12526.992824116704
Iteration 9700: Loss = -12526.994001636687
1
Iteration 9800: Loss = -12526.994723853351
2
Iteration 9900: Loss = -12526.992310876516
Iteration 10000: Loss = -12526.997524090195
1
Iteration 10100: Loss = -12527.001225869431
2
Iteration 10200: Loss = -12527.095539112639
3
Iteration 10300: Loss = -12526.996534198208
4
Iteration 10400: Loss = -12526.994287690493
5
Iteration 10500: Loss = -12526.992213720923
Iteration 10600: Loss = -12527.024531418676
1
Iteration 10700: Loss = -12526.993692891461
2
Iteration 10800: Loss = -12526.998020777597
3
Iteration 10900: Loss = -12526.994366140743
4
Iteration 11000: Loss = -12526.991912290197
Iteration 11100: Loss = -12526.992023190132
1
Iteration 11200: Loss = -12526.991860044649
Iteration 11300: Loss = -12526.992725553398
1
Iteration 11400: Loss = -12526.991880335809
Iteration 11500: Loss = -12527.01467444537
1
Iteration 11600: Loss = -12527.002124327557
2
Iteration 11700: Loss = -12527.003870929962
3
Iteration 11800: Loss = -12527.002782927551
4
Iteration 11900: Loss = -12526.99242399833
5
Iteration 12000: Loss = -12527.041628761424
6
Iteration 12100: Loss = -12526.992232484312
7
Iteration 12200: Loss = -12527.304552980171
8
Iteration 12300: Loss = -12526.996227368365
9
Iteration 12400: Loss = -12527.033391247942
10
Iteration 12500: Loss = -12526.991845712695
Iteration 12600: Loss = -12527.064018527044
1
Iteration 12700: Loss = -12526.991968990673
2
Iteration 12800: Loss = -12526.999603327773
3
Iteration 12900: Loss = -12527.022052535933
4
Iteration 13000: Loss = -12527.002997311889
5
Iteration 13100: Loss = -12527.003124789333
6
Iteration 13200: Loss = -12526.99319141103
7
Iteration 13300: Loss = -12526.993110869595
8
Iteration 13400: Loss = -12527.00385269024
9
Iteration 13500: Loss = -12526.993854797964
10
Iteration 13600: Loss = -12526.991626121317
Iteration 13700: Loss = -12526.991734169162
1
Iteration 13800: Loss = -12526.999314546903
2
Iteration 13900: Loss = -12526.99285021365
3
Iteration 14000: Loss = -12527.020949056412
4
Iteration 14100: Loss = -12526.99345703984
5
Iteration 14200: Loss = -12527.05253586926
6
Iteration 14300: Loss = -12526.995675558197
7
Iteration 14400: Loss = -12526.997867725082
8
Iteration 14500: Loss = -12527.015150172525
9
Iteration 14600: Loss = -12527.000301022592
10
Iteration 14700: Loss = -12526.993085676604
11
Iteration 14800: Loss = -12526.991617897307
Iteration 14900: Loss = -12527.02774259264
1
Iteration 15000: Loss = -12527.013988864575
2
Iteration 15100: Loss = -12527.129741998366
3
Iteration 15200: Loss = -12527.01724190061
4
Iteration 15300: Loss = -12527.039892721632
5
Iteration 15400: Loss = -12527.027650172902
6
Iteration 15500: Loss = -12526.999662450533
7
Iteration 15600: Loss = -12526.996741321134
8
Iteration 15700: Loss = -12526.991578410334
Iteration 15800: Loss = -12526.991536695621
Iteration 15900: Loss = -12526.992153455272
1
Iteration 16000: Loss = -12526.99178750266
2
Iteration 16100: Loss = -12526.993157444165
3
Iteration 16200: Loss = -12526.99285261057
4
Iteration 16300: Loss = -12526.991680290676
5
Iteration 16400: Loss = -12526.99185495932
6
Iteration 16500: Loss = -12527.010734646985
7
Iteration 16600: Loss = -12526.991709151564
8
Iteration 16700: Loss = -12526.99224090597
9
Iteration 16800: Loss = -12526.991606117706
Iteration 16900: Loss = -12526.992186136697
1
Iteration 17000: Loss = -12527.115077964776
2
Iteration 17100: Loss = -12526.99150738498
Iteration 17200: Loss = -12526.991493987733
Iteration 17300: Loss = -12526.99264769077
1
Iteration 17400: Loss = -12527.064068453
2
Iteration 17500: Loss = -12526.997645526286
3
Iteration 17600: Loss = -12526.993178047147
4
Iteration 17700: Loss = -12526.99216322215
5
Iteration 17800: Loss = -12526.992414498043
6
Iteration 17900: Loss = -12526.991518340372
Iteration 18000: Loss = -12526.992067174318
1
Iteration 18100: Loss = -12526.993488503093
2
Iteration 18200: Loss = -12526.991795113398
3
Iteration 18300: Loss = -12526.991763731246
4
Iteration 18400: Loss = -12527.006990471213
5
Iteration 18500: Loss = -12526.991505576369
Iteration 18600: Loss = -12526.992174387879
1
Iteration 18700: Loss = -12527.012380508093
2
Iteration 18800: Loss = -12526.991740250503
3
Iteration 18900: Loss = -12526.993199288248
4
Iteration 19000: Loss = -12527.000048515714
5
Iteration 19100: Loss = -12527.00477129802
6
Iteration 19200: Loss = -12527.030203248996
7
Iteration 19300: Loss = -12526.992228947394
8
Iteration 19400: Loss = -12526.991740507003
9
Iteration 19500: Loss = -12527.021773083849
10
Iteration 19600: Loss = -12526.996923762565
11
Iteration 19700: Loss = -12526.996794831053
12
Iteration 19800: Loss = -12526.992152443745
13
Iteration 19900: Loss = -12527.0009954572
14
pi: tensor([[1.0000e+00, 4.2851e-06],
        [1.5665e-01, 8.4335e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0109, 0.9891], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1945, 0.1216],
         [0.6577, 0.2080]],

        [[0.5254, 0.2036],
         [0.6283, 0.6898]],

        [[0.6649, 0.2046],
         [0.6014, 0.6502]],

        [[0.5921, 0.2102],
         [0.6738, 0.5416]],

        [[0.5355, 0.1909],
         [0.7218, 0.6915]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.01405713152211082
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006547462490945077
Global Adjusted Rand Index: 0.0025450948906085083
Average Adjusted Rand Index: -0.0032320299137222905
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23961.91845677922
Iteration 100: Loss = -12529.989514165596
Iteration 200: Loss = -12529.414187877988
Iteration 300: Loss = -12529.282191613509
Iteration 400: Loss = -12529.214012924404
Iteration 500: Loss = -12529.164762904835
Iteration 600: Loss = -12529.122391767767
Iteration 700: Loss = -12529.082953802179
Iteration 800: Loss = -12529.04572668077
Iteration 900: Loss = -12529.010425004462
Iteration 1000: Loss = -12528.975867865225
Iteration 1100: Loss = -12528.941030042977
Iteration 1200: Loss = -12528.905114775793
Iteration 1300: Loss = -12528.867449448573
Iteration 1400: Loss = -12528.827380436698
Iteration 1500: Loss = -12528.783362037615
Iteration 1600: Loss = -12528.732680664987
Iteration 1700: Loss = -12528.670993692174
Iteration 1800: Loss = -12528.592770099325
Iteration 1900: Loss = -12528.490725464915
Iteration 2000: Loss = -12528.356887797048
Iteration 2100: Loss = -12528.186957245769
Iteration 2200: Loss = -12527.993300007307
Iteration 2300: Loss = -12527.809636632324
Iteration 2400: Loss = -12527.666052053906
Iteration 2500: Loss = -12527.56928515152
Iteration 2600: Loss = -12527.509708428934
Iteration 2700: Loss = -12527.470780890602
Iteration 2800: Loss = -12527.445676421197
Iteration 2900: Loss = -12527.429137210751
Iteration 3000: Loss = -12527.417566213746
Iteration 3100: Loss = -12527.409512088641
Iteration 3200: Loss = -12527.403751064483
Iteration 3300: Loss = -12527.39942752811
Iteration 3400: Loss = -12527.396272648204
Iteration 3500: Loss = -12527.393767736457
Iteration 3600: Loss = -12527.391721367509
Iteration 3700: Loss = -12527.390106455465
Iteration 3800: Loss = -12527.388777157956
Iteration 3900: Loss = -12527.387502247764
Iteration 4000: Loss = -12527.386439361655
Iteration 4100: Loss = -12527.385644763172
Iteration 4200: Loss = -12527.384614998387
Iteration 4300: Loss = -12527.384358515417
Iteration 4400: Loss = -12527.382931544775
Iteration 4500: Loss = -12527.382113986814
Iteration 4600: Loss = -12527.39155177605
1
Iteration 4700: Loss = -12527.380427009171
Iteration 4800: Loss = -12527.379707498652
Iteration 4900: Loss = -12527.378486128002
Iteration 5000: Loss = -12527.377316318392
Iteration 5100: Loss = -12527.376645925107
Iteration 5200: Loss = -12527.374783245774
Iteration 5300: Loss = -12527.373447674598
Iteration 5400: Loss = -12527.37176769466
Iteration 5500: Loss = -12527.371004446377
Iteration 5600: Loss = -12527.368758765098
Iteration 5700: Loss = -12527.372355635625
1
Iteration 5800: Loss = -12527.36613180751
Iteration 5900: Loss = -12527.364946383761
Iteration 6000: Loss = -12527.364073355262
Iteration 6100: Loss = -12527.36296857322
Iteration 6200: Loss = -12527.392924368547
1
Iteration 6300: Loss = -12527.360839963163
Iteration 6400: Loss = -12527.359342108643
Iteration 6500: Loss = -12527.357301680511
Iteration 6600: Loss = -12527.353198535686
Iteration 6700: Loss = -12527.35692153286
1
Iteration 6800: Loss = -12527.314298981111
Iteration 6900: Loss = -12527.244828474291
Iteration 7000: Loss = -12527.191419122639
Iteration 7100: Loss = -12527.16128219089
Iteration 7200: Loss = -12527.096168416596
Iteration 7300: Loss = -12527.023069527066
Iteration 7400: Loss = -12527.00730932358
Iteration 7500: Loss = -12527.002608216795
Iteration 7600: Loss = -12527.000302314262
Iteration 7700: Loss = -12526.998811423899
Iteration 7800: Loss = -12526.997752490724
Iteration 7900: Loss = -12526.996973439265
Iteration 8000: Loss = -12526.997032720326
Iteration 8100: Loss = -12526.995774113431
Iteration 8200: Loss = -12527.026244070716
1
Iteration 8300: Loss = -12526.994999644421
Iteration 8400: Loss = -12526.994708535032
Iteration 8500: Loss = -12526.99976013259
1
Iteration 8600: Loss = -12527.015762397563
2
Iteration 8700: Loss = -12527.007138870571
3
Iteration 8800: Loss = -12527.135167282482
4
Iteration 8900: Loss = -12527.00632999975
5
Iteration 9000: Loss = -12527.034041207204
6
Iteration 9100: Loss = -12527.164808298745
7
Iteration 9200: Loss = -12526.99347400755
Iteration 9300: Loss = -12526.99360115504
1
Iteration 9400: Loss = -12527.03081862773
2
Iteration 9500: Loss = -12526.992935358017
Iteration 9600: Loss = -12526.992824358611
Iteration 9700: Loss = -12526.992924685477
1
Iteration 9800: Loss = -12526.996192568888
2
Iteration 9900: Loss = -12526.99876563238
3
Iteration 10000: Loss = -12526.994865165521
4
Iteration 10100: Loss = -12527.031558246217
5
Iteration 10200: Loss = -12526.992966528323
6
Iteration 10300: Loss = -12526.992301431601
Iteration 10400: Loss = -12526.992220373313
Iteration 10500: Loss = -12526.996177693678
1
Iteration 10600: Loss = -12526.992350215094
2
Iteration 10700: Loss = -12527.30779596595
3
Iteration 10800: Loss = -12526.99408532781
4
Iteration 10900: Loss = -12527.012859097631
5
Iteration 11000: Loss = -12526.994498691267
6
Iteration 11100: Loss = -12526.995362350057
7
Iteration 11200: Loss = -12526.997198629137
8
Iteration 11300: Loss = -12526.992080753666
Iteration 11400: Loss = -12526.992060908136
Iteration 11500: Loss = -12526.997215057498
1
Iteration 11600: Loss = -12526.991975953664
Iteration 11700: Loss = -12527.014721111473
1
Iteration 11800: Loss = -12527.10323693817
2
Iteration 11900: Loss = -12527.124746436646
3
Iteration 12000: Loss = -12526.99221144964
4
Iteration 12100: Loss = -12526.99374880561
5
Iteration 12200: Loss = -12526.997154436687
6
Iteration 12300: Loss = -12526.99277433765
7
Iteration 12400: Loss = -12526.991872646755
Iteration 12500: Loss = -12526.994261067739
1
Iteration 12600: Loss = -12527.008272100618
2
Iteration 12700: Loss = -12526.99176971009
Iteration 12800: Loss = -12526.994060000985
1
Iteration 12900: Loss = -12526.993032321696
2
Iteration 13000: Loss = -12527.053475585897
3
Iteration 13100: Loss = -12526.991787652954
Iteration 13200: Loss = -12527.062118908443
1
Iteration 13300: Loss = -12526.991634523933
Iteration 13400: Loss = -12526.9926553846
1
Iteration 13500: Loss = -12526.995889719883
2
Iteration 13600: Loss = -12526.993429882588
3
Iteration 13700: Loss = -12526.99173547718
4
Iteration 13800: Loss = -12526.99178548255
5
Iteration 13900: Loss = -12526.99258100076
6
Iteration 14000: Loss = -12526.992076645525
7
Iteration 14100: Loss = -12526.991674746734
Iteration 14200: Loss = -12526.992079633439
1
Iteration 14300: Loss = -12526.992042710503
2
Iteration 14400: Loss = -12526.99390621702
3
Iteration 14500: Loss = -12526.99157857422
Iteration 14600: Loss = -12526.991706382385
1
Iteration 14700: Loss = -12526.991603072309
Iteration 14800: Loss = -12526.991736903401
1
Iteration 14900: Loss = -12526.992575996275
2
Iteration 15000: Loss = -12526.991885561183
3
Iteration 15100: Loss = -12526.991649657082
Iteration 15200: Loss = -12527.02893646177
1
Iteration 15300: Loss = -12526.995641668946
2
Iteration 15400: Loss = -12526.99156754712
Iteration 15500: Loss = -12526.994008883921
1
Iteration 15600: Loss = -12526.991930641449
2
Iteration 15700: Loss = -12526.99161252724
Iteration 15800: Loss = -12527.321970475778
1
Iteration 15900: Loss = -12527.00202158973
2
Iteration 16000: Loss = -12527.029760141355
3
Iteration 16100: Loss = -12526.991610057124
Iteration 16200: Loss = -12527.00535332911
1
Iteration 16300: Loss = -12526.996450290804
2
Iteration 16400: Loss = -12527.041861648104
3
Iteration 16500: Loss = -12526.99543471413
4
Iteration 16600: Loss = -12527.433372421845
5
Iteration 16700: Loss = -12526.991517530241
Iteration 16800: Loss = -12526.9931851224
1
Iteration 16900: Loss = -12526.991553193879
Iteration 17000: Loss = -12526.991486304192
Iteration 17100: Loss = -12527.00260091878
1
Iteration 17200: Loss = -12527.002979301336
2
Iteration 17300: Loss = -12526.991483603188
Iteration 17400: Loss = -12526.991778342528
1
Iteration 17500: Loss = -12527.005176442652
2
Iteration 17600: Loss = -12526.992316094782
3
Iteration 17700: Loss = -12527.010334272974
4
Iteration 17800: Loss = -12526.992836578189
5
Iteration 17900: Loss = -12527.001189967355
6
Iteration 18000: Loss = -12526.991597912813
7
Iteration 18100: Loss = -12527.007248407102
8
Iteration 18200: Loss = -12526.996028728538
9
Iteration 18300: Loss = -12526.991544949511
Iteration 18400: Loss = -12526.996781756025
1
Iteration 18500: Loss = -12526.991496071894
Iteration 18600: Loss = -12526.991601149635
1
Iteration 18700: Loss = -12527.183908388679
2
Iteration 18800: Loss = -12527.02380524942
3
Iteration 18900: Loss = -12527.022457265843
4
Iteration 19000: Loss = -12526.99156980832
Iteration 19100: Loss = -12526.992020978036
1
Iteration 19200: Loss = -12526.991541686795
Iteration 19300: Loss = -12526.992460010213
1
Iteration 19400: Loss = -12527.07373399392
2
Iteration 19500: Loss = -12527.006112601626
3
Iteration 19600: Loss = -12526.992074495973
4
Iteration 19700: Loss = -12526.993396269547
5
Iteration 19800: Loss = -12526.99237672139
6
Iteration 19900: Loss = -12526.992145891514
7
pi: tensor([[8.4309e-01, 1.5691e-01],
        [5.1759e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9891, 0.0109], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2081, 0.1217],
         [0.7165, 0.1946]],

        [[0.5706, 0.2036],
         [0.7106, 0.7189]],

        [[0.5514, 0.2047],
         [0.5088, 0.5389]],

        [[0.6675, 0.2104],
         [0.6538, 0.5460]],

        [[0.6316, 0.1911],
         [0.6567, 0.6475]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.01405713152211082
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.006547462490945077
Global Adjusted Rand Index: 0.0025450948906085083
Average Adjusted Rand Index: -0.0032320299137222905
11950.512119397208
[0.0025450948906085083, 0.0025450948906085083] [-0.0032320299137222905, -0.0032320299137222905] [12526.992906596079, 12526.991982872483]
-------------------------------------
This iteration is 9
True Objective function: Loss = -11980.979244653518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20199.171363445952
Iteration 100: Loss = -12454.979014752967
Iteration 200: Loss = -12453.420956121097
Iteration 300: Loss = -12452.898392771125
Iteration 400: Loss = -12452.622930249987
Iteration 500: Loss = -12452.448563434627
Iteration 600: Loss = -12452.326063698996
Iteration 700: Loss = -12452.156908936571
Iteration 800: Loss = -12451.444741167734
Iteration 900: Loss = -12451.240431062308
Iteration 1000: Loss = -12451.145575567327
Iteration 1100: Loss = -12451.07875050943
Iteration 1200: Loss = -12451.034506856886
Iteration 1300: Loss = -12451.003603484995
Iteration 1400: Loss = -12450.980865071066
Iteration 1500: Loss = -12450.96344388738
Iteration 1600: Loss = -12450.949637514246
Iteration 1700: Loss = -12450.938578243626
Iteration 1800: Loss = -12450.929513360043
Iteration 1900: Loss = -12450.922117931963
Iteration 2000: Loss = -12450.916080280667
Iteration 2100: Loss = -12450.911065010703
Iteration 2200: Loss = -12450.906878774887
Iteration 2300: Loss = -12450.903453977407
Iteration 2400: Loss = -12450.90051345093
Iteration 2500: Loss = -12450.898062823691
Iteration 2600: Loss = -12450.895979107707
Iteration 2700: Loss = -12450.89414336184
Iteration 2800: Loss = -12450.892553894315
Iteration 2900: Loss = -12450.891205162558
Iteration 3000: Loss = -12450.889926531847
Iteration 3100: Loss = -12450.888825242639
Iteration 3200: Loss = -12450.887872952146
Iteration 3300: Loss = -12450.886998485365
Iteration 3400: Loss = -12450.886213347814
Iteration 3500: Loss = -12450.885501148823
Iteration 3600: Loss = -12450.88482269342
Iteration 3700: Loss = -12450.884227206538
Iteration 3800: Loss = -12450.8836621844
Iteration 3900: Loss = -12450.88318743274
Iteration 4000: Loss = -12450.88268748274
Iteration 4100: Loss = -12450.88223125451
Iteration 4200: Loss = -12450.881858289054
Iteration 4300: Loss = -12450.881473105092
Iteration 4400: Loss = -12450.881132255967
Iteration 4500: Loss = -12450.880826507184
Iteration 4600: Loss = -12450.880561604723
Iteration 4700: Loss = -12450.880278895056
Iteration 4800: Loss = -12450.88003542663
Iteration 4900: Loss = -12450.879809479045
Iteration 5000: Loss = -12450.879558592284
Iteration 5100: Loss = -12450.879379920305
Iteration 5200: Loss = -12450.879192007915
Iteration 5300: Loss = -12450.879018996291
Iteration 5400: Loss = -12450.878870151428
Iteration 5500: Loss = -12450.878765002764
Iteration 5600: Loss = -12450.878521899565
Iteration 5700: Loss = -12450.87844867376
Iteration 5800: Loss = -12450.878304054351
Iteration 5900: Loss = -12450.878187092836
Iteration 6000: Loss = -12450.882459946102
1
Iteration 6100: Loss = -12450.87795585817
Iteration 6200: Loss = -12450.877870992508
Iteration 6300: Loss = -12450.87776800009
Iteration 6400: Loss = -12450.87770036772
Iteration 6500: Loss = -12450.877572939444
Iteration 6600: Loss = -12450.877518366797
Iteration 6700: Loss = -12450.87744029971
Iteration 6800: Loss = -12450.877352023237
Iteration 6900: Loss = -12450.87733502995
Iteration 7000: Loss = -12450.878860157763
1
Iteration 7100: Loss = -12450.879082854804
2
Iteration 7200: Loss = -12450.877257540595
Iteration 7300: Loss = -12450.877780923347
1
Iteration 7400: Loss = -12450.877178035407
Iteration 7500: Loss = -12450.877104770449
Iteration 7600: Loss = -12450.876968046861
Iteration 7700: Loss = -12450.877007215788
Iteration 7800: Loss = -12450.876866178342
Iteration 7900: Loss = -12450.88039487365
1
Iteration 8000: Loss = -12450.87682549291
Iteration 8100: Loss = -12450.876861449757
Iteration 8200: Loss = -12450.87672746874
Iteration 8300: Loss = -12450.876716841662
Iteration 8400: Loss = -12451.224104742028
1
Iteration 8500: Loss = -12450.876652384934
Iteration 8600: Loss = -12450.87661742689
Iteration 8700: Loss = -12451.01914204925
1
Iteration 8800: Loss = -12450.876564083786
Iteration 8900: Loss = -12450.876505609285
Iteration 9000: Loss = -12450.896649144292
1
Iteration 9100: Loss = -12450.876480994086
Iteration 9200: Loss = -12450.876469511923
Iteration 9300: Loss = -12451.190066119932
1
Iteration 9400: Loss = -12450.876448889207
Iteration 9500: Loss = -12450.876393530094
Iteration 9600: Loss = -12450.876379538933
Iteration 9700: Loss = -12450.877353918764
1
Iteration 9800: Loss = -12450.876344481969
Iteration 9900: Loss = -12450.876341396244
Iteration 10000: Loss = -12450.914011453277
1
Iteration 10100: Loss = -12450.876306523178
Iteration 10200: Loss = -12450.876312292685
Iteration 10300: Loss = -12450.910830284629
1
Iteration 10400: Loss = -12450.876264327739
Iteration 10500: Loss = -12450.87626731279
Iteration 10600: Loss = -12451.26417194499
1
Iteration 10700: Loss = -12450.876282396792
Iteration 10800: Loss = -12450.87622443605
Iteration 10900: Loss = -12450.876239515548
Iteration 11000: Loss = -12450.87626574276
Iteration 11100: Loss = -12450.876225302865
Iteration 11200: Loss = -12450.876189139075
Iteration 11300: Loss = -12450.876372162982
1
Iteration 11400: Loss = -12450.87619987364
Iteration 11500: Loss = -12450.876155386955
Iteration 11600: Loss = -12450.876573939524
1
Iteration 11700: Loss = -12450.87619378002
Iteration 11800: Loss = -12450.87614887997
Iteration 11900: Loss = -12450.876669484764
1
Iteration 12000: Loss = -12450.876170377995
Iteration 12100: Loss = -12450.876171802549
Iteration 12200: Loss = -12450.937409374714
1
Iteration 12300: Loss = -12450.876174597064
Iteration 12400: Loss = -12450.876138493044
Iteration 12500: Loss = -12450.876245217318
1
Iteration 12600: Loss = -12450.876246413829
2
Iteration 12700: Loss = -12450.87614135389
Iteration 12800: Loss = -12450.878138125483
1
Iteration 12900: Loss = -12450.876394075915
2
Iteration 13000: Loss = -12450.876145498996
Iteration 13100: Loss = -12450.884402137874
1
Iteration 13200: Loss = -12450.896063036678
2
Iteration 13300: Loss = -12450.876371520144
3
Iteration 13400: Loss = -12450.876334911854
4
Iteration 13500: Loss = -12450.914380803028
5
Iteration 13600: Loss = -12450.876832419886
6
Iteration 13700: Loss = -12450.87643172329
7
Iteration 13800: Loss = -12450.876150636439
Iteration 13900: Loss = -12450.876277928135
1
Iteration 14000: Loss = -12450.886223123056
2
Iteration 14100: Loss = -12450.876087044355
Iteration 14200: Loss = -12450.954485264
1
Iteration 14300: Loss = -12450.876137940884
Iteration 14400: Loss = -12450.88399896768
1
Iteration 14500: Loss = -12450.876769567383
2
Iteration 14600: Loss = -12450.876108560262
Iteration 14700: Loss = -12450.879865381703
1
Iteration 14800: Loss = -12450.884622262407
2
Iteration 14900: Loss = -12450.8761584709
Iteration 15000: Loss = -12450.878292729534
1
Iteration 15100: Loss = -12450.876145617563
Iteration 15200: Loss = -12450.876296532075
1
Iteration 15300: Loss = -12450.876788181553
2
Iteration 15400: Loss = -12450.87650248619
3
Iteration 15500: Loss = -12450.877060670826
4
Iteration 15600: Loss = -12450.905228747448
5
Iteration 15700: Loss = -12450.877315514172
6
Iteration 15800: Loss = -12450.879923984658
7
Iteration 15900: Loss = -12450.876088391147
Iteration 16000: Loss = -12450.88031545
1
Iteration 16100: Loss = -12450.876233151452
2
Iteration 16200: Loss = -12450.876241829419
3
Iteration 16300: Loss = -12450.876104920795
Iteration 16400: Loss = -12450.876277601068
1
Iteration 16500: Loss = -12450.876084397654
Iteration 16600: Loss = -12450.876316450562
1
Iteration 16700: Loss = -12451.200774720592
2
Iteration 16800: Loss = -12450.876071791772
Iteration 16900: Loss = -12450.894406276147
1
Iteration 17000: Loss = -12450.877676519536
2
Iteration 17100: Loss = -12450.876133015912
Iteration 17200: Loss = -12450.897605285058
1
Iteration 17300: Loss = -12450.876055685902
Iteration 17400: Loss = -12450.876318907372
1
Iteration 17500: Loss = -12450.87604928294
Iteration 17600: Loss = -12450.88207024951
1
Iteration 17700: Loss = -12450.876083091145
Iteration 17800: Loss = -12450.88711177468
1
Iteration 17900: Loss = -12450.876057664278
Iteration 18000: Loss = -12450.876075561098
Iteration 18100: Loss = -12450.876645478207
1
Iteration 18200: Loss = -12450.87606456494
Iteration 18300: Loss = -12451.039647847823
1
Iteration 18400: Loss = -12450.876545938667
2
Iteration 18500: Loss = -12450.889573800341
3
Iteration 18600: Loss = -12450.876221879553
4
Iteration 18700: Loss = -12450.909559584417
5
Iteration 18800: Loss = -12450.87606810899
Iteration 18900: Loss = -12450.877685914998
1
Iteration 19000: Loss = -12450.878310822336
2
Iteration 19100: Loss = -12450.876798008085
3
Iteration 19200: Loss = -12450.877485771123
4
Iteration 19300: Loss = -12450.876114787017
Iteration 19400: Loss = -12450.89175205589
1
Iteration 19500: Loss = -12450.876568975802
2
Iteration 19600: Loss = -12450.90831621384
3
Iteration 19700: Loss = -12450.876567898766
4
Iteration 19800: Loss = -12450.878580579014
5
Iteration 19900: Loss = -12450.876143677642
pi: tensor([[1.9388e-07, 1.0000e+00],
        [1.8829e-02, 9.8117e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0015, 0.9985], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3428, 0.1855],
         [0.6649, 0.1993]],

        [[0.6303, 0.2353],
         [0.6030, 0.6162]],

        [[0.6550, 0.3674],
         [0.7133, 0.5120]],

        [[0.5385, 0.2759],
         [0.7097, 0.6097]],

        [[0.5091, 0.2992],
         [0.5442, 0.5269]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
Global Adjusted Rand Index: -0.0004863746247697297
Average Adjusted Rand Index: -0.0004872441303393567
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22112.31720196251
Iteration 100: Loss = -12454.569807714797
Iteration 200: Loss = -12452.97065862047
Iteration 300: Loss = -12452.311866499274
Iteration 400: Loss = -12451.926711357726
Iteration 500: Loss = -12451.703116392508
Iteration 600: Loss = -12451.558139491277
Iteration 700: Loss = -12451.448013959853
Iteration 800: Loss = -12451.357912367926
Iteration 900: Loss = -12451.28206886094
Iteration 1000: Loss = -12451.218478827464
Iteration 1100: Loss = -12451.166747661164
Iteration 1200: Loss = -12451.125412112284
Iteration 1300: Loss = -12451.092262538416
Iteration 1400: Loss = -12451.06503988323
Iteration 1500: Loss = -12451.042352944041
Iteration 1600: Loss = -12451.023198559078
Iteration 1700: Loss = -12451.00686292216
Iteration 1800: Loss = -12450.992887458084
Iteration 1900: Loss = -12450.980746705523
Iteration 2000: Loss = -12450.970336139779
Iteration 2100: Loss = -12450.96123937569
Iteration 2200: Loss = -12450.953324293961
Iteration 2300: Loss = -12450.946356566978
Iteration 2400: Loss = -12450.940123222712
Iteration 2500: Loss = -12450.934718859065
Iteration 2600: Loss = -12450.929850004872
Iteration 2700: Loss = -12450.925470844033
Iteration 2800: Loss = -12450.92163138799
Iteration 2900: Loss = -12450.918098714445
Iteration 3000: Loss = -12450.914960229804
Iteration 3100: Loss = -12450.912152987858
Iteration 3200: Loss = -12450.909443619821
Iteration 3300: Loss = -12450.907118970668
Iteration 3400: Loss = -12450.904913432076
Iteration 3500: Loss = -12450.90297052781
Iteration 3600: Loss = -12450.90114431395
Iteration 3700: Loss = -12450.899452953969
Iteration 3800: Loss = -12450.89793679277
Iteration 3900: Loss = -12450.896497304459
Iteration 4000: Loss = -12450.89518149486
Iteration 4100: Loss = -12450.894006697516
Iteration 4200: Loss = -12450.89286499194
Iteration 4300: Loss = -12450.891877875112
Iteration 4400: Loss = -12450.890818399426
Iteration 4500: Loss = -12450.889959614298
Iteration 4600: Loss = -12450.889147289517
Iteration 4700: Loss = -12450.88833625859
Iteration 4800: Loss = -12450.887603104717
Iteration 4900: Loss = -12450.88692558367
Iteration 5000: Loss = -12450.886288249916
Iteration 5100: Loss = -12450.885706109673
Iteration 5200: Loss = -12450.885177252596
Iteration 5300: Loss = -12450.88459082792
Iteration 5400: Loss = -12450.884160105608
Iteration 5500: Loss = -12450.88370458195
Iteration 5600: Loss = -12450.883236503782
Iteration 5700: Loss = -12450.882825292918
Iteration 5800: Loss = -12450.882469453574
Iteration 5900: Loss = -12450.882092372085
Iteration 6000: Loss = -12450.881721603406
Iteration 6100: Loss = -12450.881468435022
Iteration 6200: Loss = -12450.88118583983
Iteration 6300: Loss = -12450.880968715524
Iteration 6400: Loss = -12450.880629846173
Iteration 6500: Loss = -12450.880574400631
Iteration 6600: Loss = -12450.880127604607
Iteration 6700: Loss = -12450.880146164314
Iteration 6800: Loss = -12450.884751075246
1
Iteration 6900: Loss = -12450.88387436994
2
Iteration 7000: Loss = -12450.89583777005
3
Iteration 7100: Loss = -12450.879172021898
Iteration 7200: Loss = -12450.879743909434
1
Iteration 7300: Loss = -12450.878856225223
Iteration 7400: Loss = -12450.881904604223
1
Iteration 7500: Loss = -12450.878592618452
Iteration 7600: Loss = -12450.878822506831
1
Iteration 7700: Loss = -12450.87838179443
Iteration 7800: Loss = -12450.878218171028
Iteration 7900: Loss = -12450.878530392552
1
Iteration 8000: Loss = -12450.8780175387
Iteration 8100: Loss = -12450.877893034976
Iteration 8200: Loss = -12450.877813898622
Iteration 8300: Loss = -12450.877809936508
Iteration 8400: Loss = -12450.877630610647
Iteration 8500: Loss = -12450.877537185555
Iteration 8600: Loss = -12450.87875360513
1
Iteration 8700: Loss = -12450.877391325776
Iteration 8800: Loss = -12450.877353480164
Iteration 8900: Loss = -12450.90954451887
1
Iteration 9000: Loss = -12450.8772555441
Iteration 9100: Loss = -12450.877180374428
Iteration 9200: Loss = -12450.886978828587
1
Iteration 9300: Loss = -12450.877111332838
Iteration 9400: Loss = -12450.877038831051
Iteration 9500: Loss = -12450.876985810552
Iteration 9600: Loss = -12450.877022941186
Iteration 9700: Loss = -12450.876905287247
Iteration 9800: Loss = -12450.876834394696
Iteration 9900: Loss = -12450.877084282796
1
Iteration 10000: Loss = -12450.876785539553
Iteration 10100: Loss = -12450.876749605812
Iteration 10200: Loss = -12450.884853493522
1
Iteration 10300: Loss = -12450.87668758001
Iteration 10400: Loss = -12450.876619569543
Iteration 10500: Loss = -12451.017731722093
1
Iteration 10600: Loss = -12450.876640463293
Iteration 10700: Loss = -12450.876552758285
Iteration 10800: Loss = -12450.901157692339
1
Iteration 10900: Loss = -12450.876519837579
Iteration 11000: Loss = -12450.876524450447
Iteration 11100: Loss = -12450.876512408604
Iteration 11200: Loss = -12450.87903465758
1
Iteration 11300: Loss = -12450.876419992755
Iteration 11400: Loss = -12450.876418130423
Iteration 11500: Loss = -12450.876406516847
Iteration 11600: Loss = -12450.88749943271
1
Iteration 11700: Loss = -12450.87635644229
Iteration 11800: Loss = -12450.87634526161
Iteration 11900: Loss = -12451.649820954175
1
Iteration 12000: Loss = -12450.876342729789
Iteration 12100: Loss = -12450.876312053939
Iteration 12200: Loss = -12450.876277529469
Iteration 12300: Loss = -12450.881863468021
1
Iteration 12400: Loss = -12450.87628324019
Iteration 12500: Loss = -12450.876295608763
Iteration 12600: Loss = -12451.563106564246
1
Iteration 12700: Loss = -12450.876248605342
Iteration 12800: Loss = -12450.87619981449
Iteration 12900: Loss = -12450.876229451462
Iteration 13000: Loss = -12450.876831379837
1
Iteration 13100: Loss = -12450.876246053978
Iteration 13200: Loss = -12450.876605800311
1
Iteration 13300: Loss = -12450.876657415021
2
Iteration 13400: Loss = -12450.87617324416
Iteration 13500: Loss = -12450.884698132078
1
Iteration 13600: Loss = -12450.87668167497
2
Iteration 13700: Loss = -12450.876206643017
Iteration 13800: Loss = -12450.876279136151
Iteration 13900: Loss = -12450.87620743317
Iteration 14000: Loss = -12450.98512480498
1
Iteration 14100: Loss = -12450.876151821629
Iteration 14200: Loss = -12450.876145129905
Iteration 14300: Loss = -12450.876145552389
Iteration 14400: Loss = -12450.877441514564
1
Iteration 14500: Loss = -12450.877336017047
2
Iteration 14600: Loss = -12450.876512993458
3
Iteration 14700: Loss = -12450.882284939958
4
Iteration 14800: Loss = -12450.876147073035
Iteration 14900: Loss = -12450.876137556312
Iteration 15000: Loss = -12450.882815595112
1
Iteration 15100: Loss = -12450.876120909228
Iteration 15200: Loss = -12450.876607540284
1
Iteration 15300: Loss = -12450.876256027224
2
Iteration 15400: Loss = -12450.876127499047
Iteration 15500: Loss = -12450.876230721771
1
Iteration 15600: Loss = -12450.87615225725
Iteration 15700: Loss = -12450.87828058548
1
Iteration 15800: Loss = -12450.876122096719
Iteration 15900: Loss = -12450.938398630913
1
Iteration 16000: Loss = -12450.876106154934
Iteration 16100: Loss = -12450.87838734332
1
Iteration 16200: Loss = -12450.876222592367
2
Iteration 16300: Loss = -12450.87609601613
Iteration 16400: Loss = -12450.877528126879
1
Iteration 16500: Loss = -12450.87811722554
2
Iteration 16600: Loss = -12450.879936480464
3
Iteration 16700: Loss = -12450.87611457797
Iteration 16800: Loss = -12450.876381838156
1
Iteration 16900: Loss = -12450.876296941198
2
Iteration 17000: Loss = -12450.890389819584
3
Iteration 17100: Loss = -12450.87607891864
Iteration 17200: Loss = -12450.87683023326
1
Iteration 17300: Loss = -12450.876072769657
Iteration 17400: Loss = -12450.876827095753
1
Iteration 17500: Loss = -12450.87950988706
2
Iteration 17600: Loss = -12450.91105628373
3
Iteration 17700: Loss = -12450.87617410169
4
Iteration 17800: Loss = -12450.89702670398
5
Iteration 17900: Loss = -12450.87796014354
6
Iteration 18000: Loss = -12450.876479067136
7
Iteration 18100: Loss = -12450.897015087143
8
Iteration 18200: Loss = -12450.876622805721
9
Iteration 18300: Loss = -12450.879432354077
10
Iteration 18400: Loss = -12450.87606498017
Iteration 18500: Loss = -12450.87691677473
1
Iteration 18600: Loss = -12450.877150257355
2
Iteration 18700: Loss = -12450.876503148436
3
Iteration 18800: Loss = -12450.974484639764
4
Iteration 18900: Loss = -12450.876086554012
Iteration 19000: Loss = -12450.91262914321
1
Iteration 19100: Loss = -12450.876995166183
2
Iteration 19200: Loss = -12450.895319504996
3
Iteration 19300: Loss = -12450.8761266299
Iteration 19400: Loss = -12450.876220815464
Iteration 19500: Loss = -12450.8760822927
Iteration 19600: Loss = -12450.876362997908
1
Iteration 19700: Loss = -12450.876064162665
Iteration 19800: Loss = -12450.87673726716
1
Iteration 19900: Loss = -12450.87614955791
pi: tensor([[5.1831e-07, 1.0000e+00],
        [1.8835e-02, 9.8117e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0015, 0.9985], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3424, 0.1846],
         [0.6364, 0.1995]],

        [[0.6324, 0.2352],
         [0.5205, 0.5829]],

        [[0.5642, 0.3676],
         [0.6348, 0.6368]],

        [[0.5648, 0.2744],
         [0.5365, 0.6502]],

        [[0.5535, 0.3001],
         [0.6129, 0.5038]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
Global Adjusted Rand Index: -0.0004863746247697297
Average Adjusted Rand Index: -0.0004872441303393567
11980.979244653518
[-0.0004863746247697297, -0.0004863746247697297] [-0.0004872441303393567, -0.0004872441303393567] [12450.878732660529, 12450.87616323046]
-------------------------------------
This iteration is 10
True Objective function: Loss = -11940.883623623846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22313.424980634158
Iteration 100: Loss = -12495.921989546348
Iteration 200: Loss = -12495.225096769502
Iteration 300: Loss = -12495.030108081864
Iteration 400: Loss = -12494.8477623851
Iteration 500: Loss = -12494.57297405351
Iteration 600: Loss = -12494.29917665223
Iteration 700: Loss = -12494.192692128147
Iteration 800: Loss = -12494.128664938495
Iteration 900: Loss = -12494.068560301053
Iteration 1000: Loss = -12493.9952740007
Iteration 1100: Loss = -12493.86191688896
Iteration 1200: Loss = -12493.550397171264
Iteration 1300: Loss = -12493.168088447876
Iteration 1400: Loss = -12492.90923464719
Iteration 1500: Loss = -12492.759162211581
Iteration 1600: Loss = -12492.660075321444
Iteration 1700: Loss = -12492.590021895292
Iteration 1800: Loss = -12492.540380003356
Iteration 1900: Loss = -12492.50508324088
Iteration 2000: Loss = -12492.479257254035
Iteration 2100: Loss = -12492.458931770569
Iteration 2200: Loss = -12492.440558438377
Iteration 2300: Loss = -12492.419374203137
Iteration 2400: Loss = -12492.387193025857
Iteration 2500: Loss = -12492.338419706304
Iteration 2600: Loss = -12492.287860650134
Iteration 2700: Loss = -12492.254282888918
Iteration 2800: Loss = -12492.232843100854
Iteration 2900: Loss = -12492.218080186267
Iteration 3000: Loss = -12492.207327051203
Iteration 3100: Loss = -12492.1992972429
Iteration 3200: Loss = -12492.193180949482
Iteration 3300: Loss = -12492.188308379407
Iteration 3400: Loss = -12492.184250185768
Iteration 3500: Loss = -12492.180877462908
Iteration 3600: Loss = -12492.178011952941
Iteration 3700: Loss = -12492.17553773127
Iteration 3800: Loss = -12492.173447072111
Iteration 3900: Loss = -12492.171560690798
Iteration 4000: Loss = -12492.169907033864
Iteration 4100: Loss = -12492.168435419382
Iteration 4200: Loss = -12492.167103837675
Iteration 4300: Loss = -12492.16591484717
Iteration 4400: Loss = -12492.164814973645
Iteration 4500: Loss = -12492.16389118559
Iteration 4600: Loss = -12492.16295971634
Iteration 4700: Loss = -12492.162174747747
Iteration 4800: Loss = -12492.16144596779
Iteration 4900: Loss = -12492.160732306356
Iteration 5000: Loss = -12492.160081874983
Iteration 5100: Loss = -12492.159539922974
Iteration 5200: Loss = -12492.158966328394
Iteration 5300: Loss = -12492.158482310577
Iteration 5400: Loss = -12492.157991046823
Iteration 5500: Loss = -12492.157555177886
Iteration 5600: Loss = -12492.157134905778
Iteration 5700: Loss = -12492.156753497567
Iteration 5800: Loss = -12492.156605569364
Iteration 5900: Loss = -12492.156118081415
Iteration 6000: Loss = -12492.155786998757
Iteration 6100: Loss = -12492.155517118252
Iteration 6200: Loss = -12492.155226746025
Iteration 6300: Loss = -12492.155003024034
Iteration 6400: Loss = -12492.154770895733
Iteration 6500: Loss = -12492.154593657078
Iteration 6600: Loss = -12492.154274463637
Iteration 6700: Loss = -12492.154255234344
Iteration 6800: Loss = -12492.153953343905
Iteration 6900: Loss = -12492.153799701804
Iteration 7000: Loss = -12492.153592822418
Iteration 7100: Loss = -12492.153470628642
Iteration 7200: Loss = -12492.15333507473
Iteration 7300: Loss = -12492.153146854916
Iteration 7400: Loss = -12492.153033158293
Iteration 7500: Loss = -12492.15291758119
Iteration 7600: Loss = -12492.152798012288
Iteration 7700: Loss = -12492.153290863866
1
Iteration 7800: Loss = -12492.152593829702
Iteration 7900: Loss = -12492.152590632348
Iteration 8000: Loss = -12492.154324118932
1
Iteration 8100: Loss = -12492.15230548099
Iteration 8200: Loss = -12492.15266811924
1
Iteration 8300: Loss = -12492.152201607976
Iteration 8400: Loss = -12492.152072408207
Iteration 8500: Loss = -12492.277474806297
1
Iteration 8600: Loss = -12492.151964321829
Iteration 8700: Loss = -12492.15189820774
Iteration 8800: Loss = -12492.1908530583
1
Iteration 8900: Loss = -12492.151822677253
Iteration 9000: Loss = -12492.15171560563
Iteration 9100: Loss = -12492.152199378306
1
Iteration 9200: Loss = -12492.151621535359
Iteration 9300: Loss = -12492.151582843437
Iteration 9400: Loss = -12492.151526080715
Iteration 9500: Loss = -12492.15285318697
1
Iteration 9600: Loss = -12492.151462012247
Iteration 9700: Loss = -12492.151427530282
Iteration 9800: Loss = -12492.155082808198
1
Iteration 9900: Loss = -12492.151353847117
Iteration 10000: Loss = -12492.151328821343
Iteration 10100: Loss = -12492.160956500153
1
Iteration 10200: Loss = -12492.151290116795
Iteration 10300: Loss = -12492.151241590242
Iteration 10400: Loss = -12492.163472214552
1
Iteration 10500: Loss = -12492.151184177057
Iteration 10600: Loss = -12492.151181456278
Iteration 10700: Loss = -12492.151732801163
1
Iteration 10800: Loss = -12492.151111635425
Iteration 10900: Loss = -12492.151150889289
Iteration 11000: Loss = -12492.397022417961
1
Iteration 11100: Loss = -12492.151055956972
Iteration 11200: Loss = -12492.151045901466
Iteration 11300: Loss = -12492.151982461593
1
Iteration 11400: Loss = -12492.151007377373
Iteration 11500: Loss = -12492.154545095334
1
Iteration 11600: Loss = -12492.151051297025
Iteration 11700: Loss = -12492.166566059552
1
Iteration 11800: Loss = -12492.150994012723
Iteration 11900: Loss = -12492.150989286798
Iteration 12000: Loss = -12492.150962914558
Iteration 12100: Loss = -12492.15088615149
Iteration 12200: Loss = -12492.151128549964
1
Iteration 12300: Loss = -12492.150922334189
Iteration 12400: Loss = -12492.15959568779
1
Iteration 12500: Loss = -12492.150929467336
Iteration 12600: Loss = -12492.150849659984
Iteration 12700: Loss = -12492.22207211334
1
Iteration 12800: Loss = -12492.15086107422
Iteration 12900: Loss = -12492.150837251826
Iteration 13000: Loss = -12492.150857915127
Iteration 13100: Loss = -12492.1510706079
1
Iteration 13200: Loss = -12492.150845922033
Iteration 13300: Loss = -12492.150836318053
Iteration 13400: Loss = -12492.159505622494
1
Iteration 13500: Loss = -12492.150843991085
Iteration 13600: Loss = -12492.150818381067
Iteration 13700: Loss = -12492.151401597856
1
Iteration 13800: Loss = -12492.150801352132
Iteration 13900: Loss = -12492.151025675972
1
Iteration 14000: Loss = -12492.15079897111
Iteration 14100: Loss = -12492.15781249978
1
Iteration 14200: Loss = -12492.150821008812
Iteration 14300: Loss = -12492.150789444608
Iteration 14400: Loss = -12492.151050800574
1
Iteration 14500: Loss = -12492.15077698331
Iteration 14600: Loss = -12492.273199036616
1
Iteration 14700: Loss = -12492.150747082618
Iteration 14800: Loss = -12492.150754382326
Iteration 14900: Loss = -12492.163434873675
1
Iteration 15000: Loss = -12492.1507796518
Iteration 15100: Loss = -12492.150750911938
Iteration 15200: Loss = -12492.150765180122
Iteration 15300: Loss = -12492.151781930725
1
Iteration 15400: Loss = -12492.151355693293
2
Iteration 15500: Loss = -12492.150764847764
Iteration 15600: Loss = -12492.150786236818
Iteration 15700: Loss = -12492.150956980577
1
Iteration 15800: Loss = -12492.150824897488
Iteration 15900: Loss = -12492.150712978206
Iteration 16000: Loss = -12492.176259156076
1
Iteration 16100: Loss = -12492.150772848729
Iteration 16200: Loss = -12492.150749149145
Iteration 16300: Loss = -12492.154325023508
1
Iteration 16400: Loss = -12492.150756731502
Iteration 16500: Loss = -12492.380849336525
1
Iteration 16600: Loss = -12492.150724256126
Iteration 16700: Loss = -12492.150722876417
Iteration 16800: Loss = -12492.155650854944
1
Iteration 16900: Loss = -12492.150697875379
Iteration 17000: Loss = -12492.150720931231
Iteration 17100: Loss = -12492.15404586396
1
Iteration 17200: Loss = -12492.150683477832
Iteration 17300: Loss = -12492.15069475339
Iteration 17400: Loss = -12492.151060238422
1
Iteration 17500: Loss = -12492.150719328985
Iteration 17600: Loss = -12492.151058329142
1
Iteration 17700: Loss = -12492.150721976914
Iteration 17800: Loss = -12492.150942003305
1
Iteration 17900: Loss = -12492.175576391086
2
Iteration 18000: Loss = -12492.15071892484
Iteration 18100: Loss = -12492.15494220983
1
Iteration 18200: Loss = -12492.150690192531
Iteration 18300: Loss = -12492.16303308599
1
Iteration 18400: Loss = -12492.150716786336
Iteration 18500: Loss = -12492.151595374393
1
Iteration 18600: Loss = -12492.150772716886
Iteration 18700: Loss = -12492.150716513685
Iteration 18800: Loss = -12492.22181171408
1
Iteration 18900: Loss = -12492.150713287063
Iteration 19000: Loss = -12492.150732398186
Iteration 19100: Loss = -12492.16913638799
1
Iteration 19200: Loss = -12492.15076807048
Iteration 19300: Loss = -12492.1643571836
1
Iteration 19400: Loss = -12492.150740237836
Iteration 19500: Loss = -12492.152841939069
1
Iteration 19600: Loss = -12492.150750692892
Iteration 19700: Loss = -12492.197207806144
1
Iteration 19800: Loss = -12492.150718265957
Iteration 19900: Loss = -12492.150746463072
pi: tensor([[1.0000e+00, 3.4976e-07],
        [1.0157e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0502, 0.9498], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2562, 0.1826],
         [0.5669, 0.2041]],

        [[0.6096, 0.1690],
         [0.6435, 0.6276]],

        [[0.6114, 0.2744],
         [0.6890, 0.6616]],

        [[0.7191, 0.1977],
         [0.6132, 0.5845]],

        [[0.6970, 0.1451],
         [0.7294, 0.5458]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.007493599095377873
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.019488297385266844
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
Global Adjusted Rand Index: -0.004146535427433695
Average Adjusted Rand Index: -0.004391948416363759
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21156.65569069357
Iteration 100: Loss = -12495.68497397031
Iteration 200: Loss = -12495.276293276422
Iteration 300: Loss = -12495.137673405308
Iteration 400: Loss = -12495.025026049076
Iteration 500: Loss = -12494.906632507598
Iteration 600: Loss = -12494.77170143482
Iteration 700: Loss = -12494.620981413102
Iteration 800: Loss = -12494.464865036807
Iteration 900: Loss = -12494.343165595918
Iteration 1000: Loss = -12494.261137484775
Iteration 1100: Loss = -12494.199809993603
Iteration 1200: Loss = -12494.150930516342
Iteration 1300: Loss = -12494.111398569075
Iteration 1400: Loss = -12494.077934881278
Iteration 1500: Loss = -12494.041319780003
Iteration 1600: Loss = -12493.961885728895
Iteration 1700: Loss = -12493.781642017011
Iteration 1800: Loss = -12493.68392670072
Iteration 1900: Loss = -12493.636706395171
Iteration 2000: Loss = -12493.609227352186
Iteration 2100: Loss = -12493.587211864125
Iteration 2200: Loss = -12493.56037414195
Iteration 2300: Loss = -12493.510722203115
Iteration 2400: Loss = -12493.377618948207
Iteration 2500: Loss = -12493.033940539204
Iteration 2600: Loss = -12492.778167985625
Iteration 2700: Loss = -12492.647282171443
Iteration 2800: Loss = -12492.56804268269
Iteration 2900: Loss = -12492.520225809641
Iteration 3000: Loss = -12492.488499778168
Iteration 3100: Loss = -12492.465497622748
Iteration 3200: Loss = -12492.450768584915
Iteration 3300: Loss = -12492.439296743081
Iteration 3400: Loss = -12492.428945928203
Iteration 3500: Loss = -12492.415637030299
Iteration 3600: Loss = -12492.388617918185
Iteration 3700: Loss = -12492.342247007557
Iteration 3800: Loss = -12492.295342881067
Iteration 3900: Loss = -12492.250385528869
Iteration 4000: Loss = -12492.22689891613
Iteration 4100: Loss = -12492.21170192725
Iteration 4200: Loss = -12492.200268823331
Iteration 4300: Loss = -12492.19241306541
Iteration 4400: Loss = -12492.18648069309
Iteration 4500: Loss = -12492.18191692755
Iteration 4600: Loss = -12492.177899656437
Iteration 4700: Loss = -12492.174139614317
Iteration 4800: Loss = -12492.171740751566
Iteration 4900: Loss = -12492.169816221342
Iteration 5000: Loss = -12492.168040609542
Iteration 5100: Loss = -12492.166574277971
Iteration 5200: Loss = -12492.16528111525
Iteration 5300: Loss = -12492.16410866707
Iteration 5400: Loss = -12492.163115927351
Iteration 5500: Loss = -12492.162210304541
Iteration 5600: Loss = -12492.161567414858
Iteration 5700: Loss = -12492.160726776117
Iteration 5800: Loss = -12492.160056539438
Iteration 5900: Loss = -12492.159408064608
Iteration 6000: Loss = -12492.158907104133
Iteration 6100: Loss = -12492.158387733887
Iteration 6200: Loss = -12492.157899989466
Iteration 6300: Loss = -12492.157570057947
Iteration 6400: Loss = -12492.157010048204
Iteration 6500: Loss = -12492.15662509714
Iteration 6600: Loss = -12492.15626025161
Iteration 6700: Loss = -12492.155895154387
Iteration 6800: Loss = -12492.155695832264
Iteration 6900: Loss = -12492.155325336877
Iteration 7000: Loss = -12492.155046967267
Iteration 7100: Loss = -12492.154786788054
Iteration 7200: Loss = -12492.156382736
1
Iteration 7300: Loss = -12492.154365858754
Iteration 7400: Loss = -12492.154643872163
1
Iteration 7500: Loss = -12492.160091760037
2
Iteration 7600: Loss = -12492.155046324244
3
Iteration 7700: Loss = -12492.153803119545
Iteration 7800: Loss = -12492.154134270142
1
Iteration 7900: Loss = -12492.153331694546
Iteration 8000: Loss = -12492.171409491113
1
Iteration 8100: Loss = -12492.15299506412
Iteration 8200: Loss = -12492.171144340376
1
Iteration 8300: Loss = -12492.152767833039
Iteration 8400: Loss = -12492.153220044347
1
Iteration 8500: Loss = -12492.152529427793
Iteration 8600: Loss = -12492.152450201736
Iteration 8700: Loss = -12492.15369003374
1
Iteration 8800: Loss = -12492.152263203754
Iteration 8900: Loss = -12492.15217099386
Iteration 9000: Loss = -12492.152226541188
Iteration 9100: Loss = -12492.152007392931
Iteration 9200: Loss = -12492.152228386893
1
Iteration 9300: Loss = -12492.151924191923
Iteration 9400: Loss = -12492.15181457269
Iteration 9500: Loss = -12492.151761991845
Iteration 9600: Loss = -12492.151980674143
1
Iteration 9700: Loss = -12492.151635665581
Iteration 9800: Loss = -12492.151641795319
Iteration 9900: Loss = -12492.152488343456
1
Iteration 10000: Loss = -12492.15152049538
Iteration 10100: Loss = -12492.15152466752
Iteration 10200: Loss = -12492.152834430226
1
Iteration 10300: Loss = -12492.15140944291
Iteration 10400: Loss = -12492.151378297687
Iteration 10500: Loss = -12492.264615316759
1
Iteration 10600: Loss = -12492.151299679173
Iteration 10700: Loss = -12492.151304969295
Iteration 10800: Loss = -12492.15149391155
1
Iteration 10900: Loss = -12492.151319498129
Iteration 11000: Loss = -12492.151312044602
Iteration 11100: Loss = -12492.151595544208
1
Iteration 11200: Loss = -12492.151205096685
Iteration 11300: Loss = -12492.154973411225
1
Iteration 11400: Loss = -12492.151110835866
Iteration 11500: Loss = -12492.248562979797
1
Iteration 11600: Loss = -12492.151129384163
Iteration 11700: Loss = -12492.151068711482
Iteration 11800: Loss = -12492.154528876528
1
Iteration 11900: Loss = -12492.151019523324
Iteration 12000: Loss = -12492.151006813703
Iteration 12100: Loss = -12492.16728753626
1
Iteration 12200: Loss = -12492.150980148754
Iteration 12300: Loss = -12492.150968148799
Iteration 12400: Loss = -12492.150991484894
Iteration 12500: Loss = -12492.151100509238
1
Iteration 12600: Loss = -12492.150942724842
Iteration 12700: Loss = -12492.150903522992
Iteration 12800: Loss = -12492.151250717998
1
Iteration 12900: Loss = -12492.150885978846
Iteration 13000: Loss = -12492.150893203683
Iteration 13100: Loss = -12492.15134929257
1
Iteration 13200: Loss = -12492.150863161738
Iteration 13300: Loss = -12492.150867046394
Iteration 13400: Loss = -12492.1509447712
Iteration 13500: Loss = -12492.15082929669
Iteration 13600: Loss = -12492.150834656959
Iteration 13700: Loss = -12492.15153572852
1
Iteration 13800: Loss = -12492.150837094649
Iteration 13900: Loss = -12492.15081827787
Iteration 14000: Loss = -12492.151191694602
1
Iteration 14100: Loss = -12492.150814090948
Iteration 14200: Loss = -12492.15082889924
Iteration 14300: Loss = -12492.152141833902
1
Iteration 14400: Loss = -12492.150798506145
Iteration 14500: Loss = -12492.15080700165
Iteration 14600: Loss = -12492.151341988323
1
Iteration 14700: Loss = -12492.15077045298
Iteration 14800: Loss = -12492.150784002004
Iteration 14900: Loss = -12492.15151693084
1
Iteration 15000: Loss = -12492.15075692165
Iteration 15100: Loss = -12492.15076359949
Iteration 15200: Loss = -12492.157161584595
1
Iteration 15300: Loss = -12492.150743387092
Iteration 15400: Loss = -12492.150748829405
Iteration 15500: Loss = -12492.150752554093
Iteration 15600: Loss = -12492.151071909504
1
Iteration 15700: Loss = -12492.15075177385
Iteration 15800: Loss = -12492.150765404824
Iteration 15900: Loss = -12492.150919431779
1
Iteration 16000: Loss = -12492.150722985929
Iteration 16100: Loss = -12492.311126553452
1
Iteration 16200: Loss = -12492.150725958592
Iteration 16300: Loss = -12492.151893452497
1
Iteration 16400: Loss = -12492.150729647645
Iteration 16500: Loss = -12492.150749143158
Iteration 16600: Loss = -12492.153598227336
1
Iteration 16700: Loss = -12492.150725442614
Iteration 16800: Loss = -12492.15071290269
Iteration 16900: Loss = -12492.150830589817
1
Iteration 17000: Loss = -12492.150716479342
Iteration 17100: Loss = -12492.150756351848
Iteration 17200: Loss = -12492.150768760808
Iteration 17300: Loss = -12492.150728532532
Iteration 17400: Loss = -12492.152150363236
1
Iteration 17500: Loss = -12492.150758663092
Iteration 17600: Loss = -12492.150729091716
Iteration 17700: Loss = -12492.31159205264
1
Iteration 17800: Loss = -12492.150736227506
Iteration 17900: Loss = -12492.151317071319
1
Iteration 18000: Loss = -12492.150721529742
Iteration 18100: Loss = -12492.150959735163
1
Iteration 18200: Loss = -12492.150697504625
Iteration 18300: Loss = -12492.153548186523
1
Iteration 18400: Loss = -12492.15070292231
Iteration 18500: Loss = -12492.359706607227
1
Iteration 18600: Loss = -12492.150730857467
Iteration 18700: Loss = -12492.151081162125
1
Iteration 18800: Loss = -12492.151925931144
2
Iteration 18900: Loss = -12492.150787711573
Iteration 19000: Loss = -12492.15117083019
1
Iteration 19100: Loss = -12492.215678688613
2
Iteration 19200: Loss = -12492.1507463975
Iteration 19300: Loss = -12492.22093877508
1
Iteration 19400: Loss = -12492.150695144637
Iteration 19500: Loss = -12492.176269661524
1
Iteration 19600: Loss = -12492.150740686771
Iteration 19700: Loss = -12492.19912810874
1
Iteration 19800: Loss = -12492.15075068114
Iteration 19900: Loss = -12492.150757196943
pi: tensor([[1.0000e+00, 4.7694e-07],
        [1.3471e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0502, 0.9498], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2562, 0.1824],
         [0.5444, 0.2040]],

        [[0.6267, 0.1691],
         [0.6537, 0.5934]],

        [[0.5300, 0.2746],
         [0.5823, 0.5690]],

        [[0.5063, 0.1977],
         [0.7090, 0.6054]],

        [[0.5124, 0.1453],
         [0.5760, 0.6788]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.007493599095377873
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.019488297385266844
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
Global Adjusted Rand Index: -0.004146535427433695
Average Adjusted Rand Index: -0.004391948416363759
11940.883623623846
[-0.004146535427433695, -0.004146535427433695] [-0.004391948416363759, -0.004391948416363759] [12492.15079030259, 12492.151628920023]
-------------------------------------
This iteration is 11
True Objective function: Loss = -11868.489236040668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19017.823445160764
Iteration 100: Loss = -12414.108743377667
Iteration 200: Loss = -12413.958717548583
Iteration 300: Loss = -12413.896233447322
Iteration 400: Loss = -12413.682475964675
Iteration 500: Loss = -12412.77869323105
Iteration 600: Loss = -12412.502744952497
Iteration 700: Loss = -12378.802564638621
Iteration 800: Loss = -12042.114277958299
Iteration 900: Loss = -12005.536358983685
Iteration 1000: Loss = -11939.381503094399
Iteration 1100: Loss = -11908.974608791696
Iteration 1200: Loss = -11908.781735435527
Iteration 1300: Loss = -11908.769474164854
Iteration 1400: Loss = -11908.762014623822
Iteration 1500: Loss = -11908.756637572793
Iteration 1600: Loss = -11908.75213982324
Iteration 1700: Loss = -11908.74752299798
Iteration 1800: Loss = -11908.742058074122
Iteration 1900: Loss = -11908.560119199397
Iteration 2000: Loss = -11908.27447809488
Iteration 2100: Loss = -11908.272915746325
Iteration 2200: Loss = -11908.271829901843
Iteration 2300: Loss = -11908.270912679234
Iteration 2400: Loss = -11908.27013033039
Iteration 2500: Loss = -11908.27016010573
Iteration 2600: Loss = -11908.268974179222
Iteration 2700: Loss = -11908.269618298627
1
Iteration 2800: Loss = -11908.267946374448
Iteration 2900: Loss = -11908.276147792001
1
Iteration 3000: Loss = -11908.266771169878
Iteration 3100: Loss = -11908.26639587961
Iteration 3200: Loss = -11908.266114298676
Iteration 3300: Loss = -11908.265845618012
Iteration 3400: Loss = -11908.265091010859
Iteration 3500: Loss = -11908.264728087046
Iteration 3600: Loss = -11908.264356323367
Iteration 3700: Loss = -11908.263937987062
Iteration 3800: Loss = -11908.268248521046
1
Iteration 3900: Loss = -11908.26792854433
2
Iteration 4000: Loss = -11908.263919744255
Iteration 4100: Loss = -11908.263612378161
Iteration 4200: Loss = -11908.261453524676
Iteration 4300: Loss = -11908.261325065032
Iteration 4400: Loss = -11908.261192053804
Iteration 4500: Loss = -11908.261390290732
1
Iteration 4600: Loss = -11908.26195963997
2
Iteration 4700: Loss = -11908.266328326874
3
Iteration 4800: Loss = -11908.261220895982
Iteration 4900: Loss = -11908.263462351026
1
Iteration 5000: Loss = -11908.26076428944
Iteration 5100: Loss = -11908.260761557274
Iteration 5200: Loss = -11908.26042622069
Iteration 5300: Loss = -11908.260200156896
Iteration 5400: Loss = -11908.260314190156
1
Iteration 5500: Loss = -11908.260924741357
2
Iteration 5600: Loss = -11908.260500018248
3
Iteration 5700: Loss = -11908.265274258358
4
Iteration 5800: Loss = -11908.260475098787
5
Iteration 5900: Loss = -11908.260262632759
Iteration 6000: Loss = -11908.2599558069
Iteration 6100: Loss = -11908.26074323783
1
Iteration 6200: Loss = -11908.260854667891
2
Iteration 6300: Loss = -11908.29286404904
3
Iteration 6400: Loss = -11908.3086419969
4
Iteration 6500: Loss = -11908.259565047703
Iteration 6600: Loss = -11908.259621143103
Iteration 6700: Loss = -11908.259423922298
Iteration 6800: Loss = -11908.25952371156
Iteration 6900: Loss = -11908.25914561701
Iteration 7000: Loss = -11908.279899583315
1
Iteration 7100: Loss = -11908.259061002334
Iteration 7200: Loss = -11908.259016016478
Iteration 7300: Loss = -11908.258996488212
Iteration 7400: Loss = -11908.258954771358
Iteration 7500: Loss = -11908.342550081446
1
Iteration 7600: Loss = -11908.25890860339
Iteration 7700: Loss = -11908.258864783445
Iteration 7800: Loss = -11908.260543191967
1
Iteration 7900: Loss = -11908.258867913959
Iteration 8000: Loss = -11908.258835014256
Iteration 8100: Loss = -11908.266808621322
1
Iteration 8200: Loss = -11908.258811586606
Iteration 8300: Loss = -11908.258789361418
Iteration 8400: Loss = -11908.261684693558
1
Iteration 8500: Loss = -11908.258807270731
Iteration 8600: Loss = -11908.25874207695
Iteration 8700: Loss = -11908.26208383604
1
Iteration 8800: Loss = -11908.25873480387
Iteration 8900: Loss = -11908.258721879112
Iteration 9000: Loss = -11908.259967783313
1
Iteration 9100: Loss = -11908.260831880518
2
Iteration 9200: Loss = -11908.26161945748
3
Iteration 9300: Loss = -11908.265324261845
4
Iteration 9400: Loss = -11908.260643364496
5
Iteration 9500: Loss = -11908.25787665724
Iteration 9600: Loss = -11908.261415510106
1
Iteration 9700: Loss = -11908.276004619227
2
Iteration 9800: Loss = -11908.257547037076
Iteration 9900: Loss = -11908.250744761908
Iteration 10000: Loss = -11908.250461094032
Iteration 10100: Loss = -11908.274422998216
1
Iteration 10200: Loss = -11908.252499159376
2
Iteration 10300: Loss = -11908.250534997975
Iteration 10400: Loss = -11908.267494573567
1
Iteration 10500: Loss = -11908.264128859779
2
Iteration 10600: Loss = -11908.337770198985
3
Iteration 10700: Loss = -11908.2593072823
4
Iteration 10800: Loss = -11908.251330962112
5
Iteration 10900: Loss = -11908.253162734838
6
Iteration 11000: Loss = -11908.25326901244
7
Iteration 11100: Loss = -11908.250673736195
8
Iteration 11200: Loss = -11908.251231177384
9
Iteration 11300: Loss = -11908.250839522827
10
Iteration 11400: Loss = -11908.252634051347
11
Iteration 11500: Loss = -11908.261863754991
12
Iteration 11600: Loss = -11908.25093787428
13
Iteration 11700: Loss = -11908.251305453728
14
Iteration 11800: Loss = -11908.262686837135
15
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[0.3415, 0.6585],
        [0.6158, 0.3842]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5503, 0.4497], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3071, 0.0971],
         [0.5509, 0.2968]],

        [[0.6669, 0.0955],
         [0.6239, 0.7154]],

        [[0.6838, 0.0960],
         [0.5487, 0.5524]],

        [[0.5940, 0.1076],
         [0.5531, 0.5264]],

        [[0.5953, 0.0964],
         [0.5768, 0.5477]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03342897826153057
Average Adjusted Rand Index: 0.9759996238662728
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23544.903448678953
Iteration 100: Loss = -12414.707302827019
Iteration 200: Loss = -12414.092423191194
Iteration 300: Loss = -12413.759633959246
Iteration 400: Loss = -12413.39363304622
Iteration 500: Loss = -12413.186678885784
Iteration 600: Loss = -12413.04350641515
Iteration 700: Loss = -12412.92045766379
Iteration 800: Loss = -12412.804425298633
Iteration 900: Loss = -12412.694152615319
Iteration 1000: Loss = -12412.5987265713
Iteration 1100: Loss = -12412.533493584227
Iteration 1200: Loss = -12412.496706571112
Iteration 1300: Loss = -12412.47597206137
Iteration 1400: Loss = -12412.463072939478
Iteration 1500: Loss = -12412.454348780493
Iteration 1600: Loss = -12412.447934964935
Iteration 1700: Loss = -12412.443082736518
Iteration 1800: Loss = -12412.439298093508
Iteration 1900: Loss = -12412.436149631534
Iteration 2000: Loss = -12412.433602061066
Iteration 2100: Loss = -12412.431477139758
Iteration 2200: Loss = -12412.429622416303
Iteration 2300: Loss = -12412.428009842366
Iteration 2400: Loss = -12412.426639737347
Iteration 2500: Loss = -12412.425365514931
Iteration 2600: Loss = -12412.424280617668
Iteration 2700: Loss = -12412.423230466782
Iteration 2800: Loss = -12412.422300633363
Iteration 2900: Loss = -12412.42151141266
Iteration 3000: Loss = -12412.420619370332
Iteration 3100: Loss = -12412.41982974017
Iteration 3200: Loss = -12412.41904760226
Iteration 3300: Loss = -12412.418263752712
Iteration 3400: Loss = -12412.417438462115
Iteration 3500: Loss = -12412.41658135607
Iteration 3600: Loss = -12412.415695035032
Iteration 3700: Loss = -12412.414715939107
Iteration 3800: Loss = -12412.413611750382
Iteration 3900: Loss = -12412.412334230004
Iteration 4000: Loss = -12412.410659372195
Iteration 4100: Loss = -12412.40863826998
Iteration 4200: Loss = -12412.405590237347
Iteration 4300: Loss = -12412.400476019806
Iteration 4400: Loss = -12412.389557513587
Iteration 4500: Loss = -12412.351209947205
Iteration 4600: Loss = -12296.70524218599
Iteration 4700: Loss = -11937.646417417172
Iteration 4800: Loss = -11900.427414666443
Iteration 4900: Loss = -11892.049991914304
Iteration 5000: Loss = -11884.085407540872
Iteration 5100: Loss = -11863.30772320906
Iteration 5200: Loss = -11863.254036023818
Iteration 5300: Loss = -11863.250794138363
Iteration 5400: Loss = -11863.247044270856
Iteration 5500: Loss = -11863.232060432074
Iteration 5600: Loss = -11863.22713424856
Iteration 5700: Loss = -11863.221405510196
Iteration 5800: Loss = -11863.221447602844
Iteration 5900: Loss = -11862.935063611034
Iteration 6000: Loss = -11862.896286806636
Iteration 6100: Loss = -11862.890280129955
Iteration 6200: Loss = -11862.889429666224
Iteration 6300: Loss = -11862.888090006962
Iteration 6400: Loss = -11862.883477972928
Iteration 6500: Loss = -11862.882175513598
Iteration 6600: Loss = -11862.88289692059
1
Iteration 6700: Loss = -11862.881635134625
Iteration 6800: Loss = -11862.881240370361
Iteration 6900: Loss = -11862.880962398172
Iteration 7000: Loss = -11862.880512483473
Iteration 7100: Loss = -11862.88027112478
Iteration 7200: Loss = -11862.880833039088
1
Iteration 7300: Loss = -11862.87546321971
Iteration 7400: Loss = -11862.875207150719
Iteration 7500: Loss = -11862.875120594394
Iteration 7600: Loss = -11862.874828151247
Iteration 7700: Loss = -11862.874554609874
Iteration 7800: Loss = -11862.875704458278
1
Iteration 7900: Loss = -11862.873944572093
Iteration 8000: Loss = -11862.870713977267
Iteration 8100: Loss = -11862.869275423618
Iteration 8200: Loss = -11862.866120721474
Iteration 8300: Loss = -11862.86597786029
Iteration 8400: Loss = -11862.86674244679
1
Iteration 8500: Loss = -11862.865325427141
Iteration 8600: Loss = -11862.86299010857
Iteration 8700: Loss = -11862.68680377551
Iteration 8800: Loss = -11862.645686032502
Iteration 8900: Loss = -11862.624918946007
Iteration 9000: Loss = -11862.624719633312
Iteration 9100: Loss = -11862.635504457194
1
Iteration 9200: Loss = -11862.625458957233
2
Iteration 9300: Loss = -11862.626839921611
3
Iteration 9400: Loss = -11862.625601487389
4
Iteration 9500: Loss = -11862.628668737485
5
Iteration 9600: Loss = -11862.630638213628
6
Iteration 9700: Loss = -11862.629245765576
7
Iteration 9800: Loss = -11862.642103267808
8
Iteration 9900: Loss = -11862.695215054327
9
Iteration 10000: Loss = -11862.625289606956
10
Iteration 10100: Loss = -11862.625793460209
11
Iteration 10200: Loss = -11862.639502305454
12
Iteration 10300: Loss = -11862.623880325338
Iteration 10400: Loss = -11862.623407431805
Iteration 10500: Loss = -11862.630719478251
1
Iteration 10600: Loss = -11862.623474842954
Iteration 10700: Loss = -11862.629502053507
1
Iteration 10800: Loss = -11862.62428921874
2
Iteration 10900: Loss = -11862.631567652608
3
Iteration 11000: Loss = -11862.632220496402
4
Iteration 11100: Loss = -11862.624675072871
5
Iteration 11200: Loss = -11862.625997406556
6
Iteration 11300: Loss = -11862.655028363077
7
Iteration 11400: Loss = -11862.62183754026
Iteration 11500: Loss = -11862.623147248301
1
Iteration 11600: Loss = -11862.620578542412
Iteration 11700: Loss = -11862.620781781692
1
Iteration 11800: Loss = -11862.634669995803
2
Iteration 11900: Loss = -11862.631120247343
3
Iteration 12000: Loss = -11862.625569255744
4
Iteration 12100: Loss = -11862.622501520382
5
Iteration 12200: Loss = -11862.620147185848
Iteration 12300: Loss = -11862.619348728893
Iteration 12400: Loss = -11862.658111105296
1
Iteration 12500: Loss = -11862.625728130448
2
Iteration 12600: Loss = -11862.633265583998
3
Iteration 12700: Loss = -11862.756762492492
4
Iteration 12800: Loss = -11862.621554318317
5
Iteration 12900: Loss = -11862.628657196954
6
Iteration 13000: Loss = -11862.638200331707
7
Iteration 13100: Loss = -11862.638355510133
8
Iteration 13200: Loss = -11862.637139584433
9
Iteration 13300: Loss = -11862.620923892395
10
Iteration 13400: Loss = -11862.617489840417
Iteration 13500: Loss = -11862.622519005192
1
Iteration 13600: Loss = -11862.619058714361
2
Iteration 13700: Loss = -11862.619147709975
3
Iteration 13800: Loss = -11862.624759163797
4
Iteration 13900: Loss = -11862.623211261482
5
Iteration 14000: Loss = -11862.618558741362
6
Iteration 14100: Loss = -11862.623196132521
7
Iteration 14200: Loss = -11862.631385941737
8
Iteration 14300: Loss = -11862.624746998836
9
Iteration 14400: Loss = -11862.61999601204
10
Iteration 14500: Loss = -11862.61948748981
11
Iteration 14600: Loss = -11862.61702222066
Iteration 14700: Loss = -11862.638758133475
1
Iteration 14800: Loss = -11862.64720965881
2
Iteration 14900: Loss = -11862.619541456748
3
Iteration 15000: Loss = -11862.61879564668
4
Iteration 15100: Loss = -11862.620137575315
5
Iteration 15200: Loss = -11862.686594626495
6
Iteration 15300: Loss = -11862.670501666033
7
Iteration 15400: Loss = -11862.630415541082
8
Iteration 15500: Loss = -11862.636232438297
9
Iteration 15600: Loss = -11862.639438070635
10
Iteration 15700: Loss = -11862.665346481595
11
Iteration 15800: Loss = -11862.623656998905
12
Iteration 15900: Loss = -11862.644171569913
13
Iteration 16000: Loss = -11862.621731498557
14
Iteration 16100: Loss = -11862.62656314579
15
Stopping early at iteration 16100 due to no improvement.
pi: tensor([[0.7937, 0.2063],
        [0.2496, 0.7504]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5519, 0.4481], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2997, 0.0972],
         [0.5929, 0.3046]],

        [[0.6575, 0.0948],
         [0.6495, 0.6981]],

        [[0.6223, 0.0963],
         [0.6627, 0.5205]],

        [[0.6161, 0.1071],
         [0.5694, 0.5985]],

        [[0.7008, 0.0960],
         [0.5883, 0.5607]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840305667129629
Average Adjusted Rand Index: 0.983977557788841
11868.489236040668
[0.03342897826153057, 0.9840305667129629] [0.9759996238662728, 0.983977557788841] [11908.262686837135, 11862.62656314579]
-------------------------------------
This iteration is 12
True Objective function: Loss = -11832.624752466018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20376.54519399538
Iteration 100: Loss = -12404.78940307147
Iteration 200: Loss = -12404.419506220926
Iteration 300: Loss = -12404.341915919118
Iteration 400: Loss = -12404.288846656787
Iteration 500: Loss = -12404.204928710558
Iteration 600: Loss = -12404.045638974732
Iteration 700: Loss = -12403.894553532837
Iteration 800: Loss = -12403.807640937706
Iteration 900: Loss = -12403.749828362867
Iteration 1000: Loss = -12403.69938552422
Iteration 1100: Loss = -12403.649425795065
Iteration 1200: Loss = -12403.601131206891
Iteration 1300: Loss = -12403.56249031326
Iteration 1400: Loss = -12403.538656746869
Iteration 1500: Loss = -12403.524798481256
Iteration 1600: Loss = -12403.515115105127
Iteration 1700: Loss = -12403.506830416998
Iteration 1800: Loss = -12403.498275853572
Iteration 1900: Loss = -12403.48786979497
Iteration 2000: Loss = -12403.472850066186
Iteration 2100: Loss = -12403.447123980433
Iteration 2200: Loss = -12403.39412417463
Iteration 2300: Loss = -12403.26544175323
Iteration 2400: Loss = -12402.770641790425
Iteration 2500: Loss = -12400.377606930977
Iteration 2600: Loss = -12391.986358134449
Iteration 2700: Loss = -12391.772709713545
Iteration 2800: Loss = -12391.740758759652
Iteration 2900: Loss = -12391.726908841869
Iteration 3000: Loss = -12391.719131828866
Iteration 3100: Loss = -12391.714116624818
Iteration 3200: Loss = -12391.71059306495
Iteration 3300: Loss = -12391.708113573339
Iteration 3400: Loss = -12391.70622659535
Iteration 3500: Loss = -12391.704750962717
Iteration 3600: Loss = -12391.703542954316
Iteration 3700: Loss = -12391.702593694514
Iteration 3800: Loss = -12391.701905791286
Iteration 3900: Loss = -12391.701238344243
Iteration 4000: Loss = -12391.700757617918
Iteration 4100: Loss = -12391.70031588992
Iteration 4200: Loss = -12391.70003998793
Iteration 4300: Loss = -12391.699629930332
Iteration 4400: Loss = -12391.699386949644
Iteration 4500: Loss = -12391.700659281592
1
Iteration 4600: Loss = -12391.698925140729
Iteration 4700: Loss = -12391.698746134183
Iteration 4800: Loss = -12391.700958910567
1
Iteration 4900: Loss = -12391.698433638192
Iteration 5000: Loss = -12391.698301021142
Iteration 5100: Loss = -12391.698570166323
1
Iteration 5200: Loss = -12391.698061690711
Iteration 5300: Loss = -12391.69798682229
Iteration 5400: Loss = -12391.698013512254
Iteration 5500: Loss = -12391.697819569343
Iteration 5600: Loss = -12391.697758708751
Iteration 5700: Loss = -12391.697768722579
Iteration 5800: Loss = -12391.697630942124
Iteration 5900: Loss = -12391.700856360168
1
Iteration 6000: Loss = -12391.6975484583
Iteration 6100: Loss = -12391.697494752549
Iteration 6200: Loss = -12391.697474686523
Iteration 6300: Loss = -12391.697420111446
Iteration 6400: Loss = -12391.69746015505
Iteration 6500: Loss = -12391.69737073078
Iteration 6600: Loss = -12391.69733830219
Iteration 6700: Loss = -12391.697281625895
Iteration 6800: Loss = -12391.697258147962
Iteration 6900: Loss = -12391.697312701162
Iteration 7000: Loss = -12391.69721964188
Iteration 7100: Loss = -12391.69857914355
1
Iteration 7200: Loss = -12391.697194048711
Iteration 7300: Loss = -12391.697167099597
Iteration 7400: Loss = -12391.704102467824
1
Iteration 7500: Loss = -12391.697116745641
Iteration 7600: Loss = -12391.697118483311
Iteration 7700: Loss = -12391.697095657304
Iteration 7800: Loss = -12391.6972127063
1
Iteration 7900: Loss = -12391.697071781795
Iteration 8000: Loss = -12391.697042168918
Iteration 8100: Loss = -12391.698059114555
1
Iteration 8200: Loss = -12391.697011498532
Iteration 8300: Loss = -12391.696991820265
Iteration 8400: Loss = -12391.697013253593
Iteration 8500: Loss = -12391.696996637229
Iteration 8600: Loss = -12391.70651131687
1
Iteration 8700: Loss = -12391.697034281076
Iteration 8800: Loss = -12391.728459131307
1
Iteration 8900: Loss = -12391.696990531744
Iteration 9000: Loss = -12391.696960405889
Iteration 9100: Loss = -12391.721642663206
1
Iteration 9200: Loss = -12391.696970530656
Iteration 9300: Loss = -12391.696910135954
Iteration 9400: Loss = -12391.73687194984
1
Iteration 9500: Loss = -12391.696955307089
Iteration 9600: Loss = -12391.696987798001
Iteration 9700: Loss = -12391.69697131732
Iteration 9800: Loss = -12391.69722686971
1
Iteration 9900: Loss = -12391.69693613077
Iteration 10000: Loss = -12391.69704369594
1
Iteration 10100: Loss = -12391.697080601842
2
Iteration 10200: Loss = -12391.724779679244
3
Iteration 10300: Loss = -12391.696972758602
Iteration 10400: Loss = -12391.829460846635
1
Iteration 10500: Loss = -12391.696948668989
Iteration 10600: Loss = -12391.696958161621
Iteration 10700: Loss = -12391.6971292834
1
Iteration 10800: Loss = -12391.696908439282
Iteration 10900: Loss = -12391.701167952762
1
Iteration 11000: Loss = -12391.696902225027
Iteration 11100: Loss = -12391.697136132057
1
Iteration 11200: Loss = -12391.696931190549
Iteration 11300: Loss = -12391.696938474026
Iteration 11400: Loss = -12391.701384568853
1
Iteration 11500: Loss = -12391.69695791459
Iteration 11600: Loss = -12391.696921047514
Iteration 11700: Loss = -12391.701167738363
1
Iteration 11800: Loss = -12391.715988610818
2
Iteration 11900: Loss = -12391.78002084239
3
Iteration 12000: Loss = -12391.69687847791
Iteration 12100: Loss = -12391.699413285003
1
Iteration 12200: Loss = -12391.69689317682
Iteration 12300: Loss = -12391.697327143886
1
Iteration 12400: Loss = -12391.696921587878
Iteration 12500: Loss = -12391.72233444187
1
Iteration 12600: Loss = -12391.696888040007
Iteration 12700: Loss = -12391.696903377828
Iteration 12800: Loss = -12391.949531543807
1
Iteration 12900: Loss = -12391.696885022051
Iteration 13000: Loss = -12391.696893339496
Iteration 13100: Loss = -12391.70034626184
1
Iteration 13200: Loss = -12391.697200553694
2
Iteration 13300: Loss = -12391.874666628053
3
Iteration 13400: Loss = -12391.696919609098
Iteration 13500: Loss = -12391.773523424743
1
Iteration 13600: Loss = -12391.69688007551
Iteration 13700: Loss = -12391.697360987575
1
Iteration 13800: Loss = -12391.705226190525
2
Iteration 13900: Loss = -12391.697233640125
3
Iteration 14000: Loss = -12391.742728270789
4
Iteration 14100: Loss = -12391.696904437853
Iteration 14200: Loss = -12391.70858595607
1
Iteration 14300: Loss = -12391.698177684386
2
Iteration 14400: Loss = -12391.697706475326
3
Iteration 14500: Loss = -12391.720341342521
4
Iteration 14600: Loss = -12391.696894277293
Iteration 14700: Loss = -12391.69772362614
1
Iteration 14800: Loss = -12391.699892378268
2
Iteration 14900: Loss = -12391.698263745795
3
Iteration 15000: Loss = -12391.767918913509
4
Iteration 15100: Loss = -12391.697114177085
5
Iteration 15200: Loss = -12391.701388877866
6
Iteration 15300: Loss = -12391.697255027768
7
Iteration 15400: Loss = -12391.696899478136
Iteration 15500: Loss = -12391.706842760128
1
Iteration 15600: Loss = -12391.696890234201
Iteration 15700: Loss = -12391.697620353008
1
Iteration 15800: Loss = -12391.697268793912
2
Iteration 15900: Loss = -12391.697146340375
3
Iteration 16000: Loss = -12391.699884332373
4
Iteration 16100: Loss = -12391.710511107121
5
Iteration 16200: Loss = -12391.6968981867
Iteration 16300: Loss = -12391.706981051013
1
Iteration 16400: Loss = -12391.696863377023
Iteration 16500: Loss = -12391.698256216143
1
Iteration 16600: Loss = -12391.707464833567
2
Iteration 16700: Loss = -12391.707409375042
3
Iteration 16800: Loss = -12391.696889722363
Iteration 16900: Loss = -12391.696947188617
Iteration 17000: Loss = -12391.703080847103
1
Iteration 17100: Loss = -12391.697223389236
2
Iteration 17200: Loss = -12391.69710765332
3
Iteration 17300: Loss = -12391.697254512084
4
Iteration 17400: Loss = -12391.70585857566
5
Iteration 17500: Loss = -12391.696898065891
Iteration 17600: Loss = -12391.697881299358
1
Iteration 17700: Loss = -12391.69688309472
Iteration 17800: Loss = -12391.719763609564
1
Iteration 17900: Loss = -12391.705563209878
2
Iteration 18000: Loss = -12391.69690647206
Iteration 18100: Loss = -12391.702260912336
1
Iteration 18200: Loss = -12391.973281814888
2
Iteration 18300: Loss = -12391.696905665189
Iteration 18400: Loss = -12391.714257026784
1
Iteration 18500: Loss = -12391.696884442452
Iteration 18600: Loss = -12391.697528579105
1
Iteration 18700: Loss = -12391.696968387225
Iteration 18800: Loss = -12391.696917366344
Iteration 18900: Loss = -12391.764745626839
1
Iteration 19000: Loss = -12391.704229877001
2
Iteration 19100: Loss = -12391.697786281668
3
Iteration 19200: Loss = -12391.696941084614
Iteration 19300: Loss = -12391.697475591816
1
Iteration 19400: Loss = -12391.702891825815
2
Iteration 19500: Loss = -12391.696936432998
Iteration 19600: Loss = -12391.706780174422
1
Iteration 19700: Loss = -12391.696877340184
Iteration 19800: Loss = -12391.69693469667
Iteration 19900: Loss = -12391.73848152375
1
pi: tensor([[0.6220, 0.3780],
        [0.0060, 0.9940]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0986, 0.9014], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.6342, 0.1726],
         [0.5458, 0.1996]],

        [[0.5900, 0.1450],
         [0.6062, 0.7134]],

        [[0.7108, 0.2585],
         [0.7053, 0.7210]],

        [[0.6283, 0.2737],
         [0.5108, 0.6761]],

        [[0.6458, 0.2206],
         [0.5713, 0.7291]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.02269374787246114
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.006528075836638138
Average Adjusted Rand Index: 0.0013353894992507293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20735.69286804531
Iteration 100: Loss = -12404.806457724311
Iteration 200: Loss = -12404.444468944992
Iteration 300: Loss = -12404.302073836718
Iteration 400: Loss = -12404.102229631511
Iteration 500: Loss = -12403.766404978118
Iteration 600: Loss = -12403.565677343968
Iteration 700: Loss = -12401.901747848573
Iteration 800: Loss = -12399.533336310498
Iteration 900: Loss = -12398.688706563562
Iteration 1000: Loss = -12397.673988929002
Iteration 1100: Loss = -12394.9955860407
Iteration 1200: Loss = -12070.048904896465
Iteration 1300: Loss = -11850.760437806353
Iteration 1400: Loss = -11823.618607776933
Iteration 1500: Loss = -11823.510654108492
Iteration 1600: Loss = -11823.458156979465
Iteration 1700: Loss = -11823.42391469713
Iteration 1800: Loss = -11823.394409000535
Iteration 1900: Loss = -11823.378647801132
Iteration 2000: Loss = -11823.366814882485
Iteration 2100: Loss = -11823.35747240428
Iteration 2200: Loss = -11823.352311062621
Iteration 2300: Loss = -11823.343580434273
Iteration 2400: Loss = -11823.338123282128
Iteration 2500: Loss = -11823.348211929304
1
Iteration 2600: Loss = -11823.328183334115
Iteration 2700: Loss = -11823.324609814414
Iteration 2800: Loss = -11823.321769194308
Iteration 2900: Loss = -11823.319240254996
Iteration 3000: Loss = -11823.316883608448
Iteration 3100: Loss = -11823.314805027203
Iteration 3200: Loss = -11823.313084298536
Iteration 3300: Loss = -11823.311262220319
Iteration 3400: Loss = -11823.310161079897
Iteration 3500: Loss = -11823.308543707015
Iteration 3600: Loss = -11823.307346658907
Iteration 3700: Loss = -11823.306283316202
Iteration 3800: Loss = -11823.305106291145
Iteration 3900: Loss = -11823.304418286869
Iteration 4000: Loss = -11823.301207387654
Iteration 4100: Loss = -11823.300332996292
Iteration 4200: Loss = -11823.31302259101
1
Iteration 4300: Loss = -11823.30240242762
2
Iteration 4400: Loss = -11823.298452368666
Iteration 4500: Loss = -11823.29810041828
Iteration 4600: Loss = -11823.298245488964
1
Iteration 4700: Loss = -11823.297176933413
Iteration 4800: Loss = -11823.296532273638
Iteration 4900: Loss = -11823.296104503965
Iteration 5000: Loss = -11823.295799717866
Iteration 5100: Loss = -11823.295825308367
Iteration 5200: Loss = -11823.294509883968
Iteration 5300: Loss = -11823.304495413997
1
Iteration 5400: Loss = -11823.293697515273
Iteration 5500: Loss = -11823.295192766001
1
Iteration 5600: Loss = -11823.29297509869
Iteration 5700: Loss = -11823.292579229368
Iteration 5800: Loss = -11823.294516972888
1
Iteration 5900: Loss = -11823.291017540523
Iteration 6000: Loss = -11823.291846314032
1
Iteration 6100: Loss = -11823.29032174261
Iteration 6200: Loss = -11823.291513886506
1
Iteration 6300: Loss = -11823.289843881566
Iteration 6400: Loss = -11823.293162329965
1
Iteration 6500: Loss = -11823.290628973176
2
Iteration 6600: Loss = -11823.293918217318
3
Iteration 6700: Loss = -11823.290814110385
4
Iteration 6800: Loss = -11823.289373111382
Iteration 6900: Loss = -11823.291756825647
1
Iteration 7000: Loss = -11823.28876218386
Iteration 7100: Loss = -11823.293790797357
1
Iteration 7200: Loss = -11823.288548886623
Iteration 7300: Loss = -11823.295896120857
1
Iteration 7400: Loss = -11823.28835809915
Iteration 7500: Loss = -11823.288226809256
Iteration 7600: Loss = -11823.2879567114
Iteration 7700: Loss = -11823.287866591168
Iteration 7800: Loss = -11823.28779153745
Iteration 7900: Loss = -11823.28944872722
1
Iteration 8000: Loss = -11823.289201056268
2
Iteration 8100: Loss = -11823.28745016831
Iteration 8200: Loss = -11823.292413247249
1
Iteration 8300: Loss = -11823.28733826428
Iteration 8400: Loss = -11823.287985841333
1
Iteration 8500: Loss = -11823.291249838248
2
Iteration 8600: Loss = -11823.29873607053
3
Iteration 8700: Loss = -11823.28759372784
4
Iteration 8800: Loss = -11823.28731158138
Iteration 8900: Loss = -11823.287168487308
Iteration 9000: Loss = -11823.28737627768
1
Iteration 9100: Loss = -11823.286990250055
Iteration 9200: Loss = -11823.287373602183
1
Iteration 9300: Loss = -11823.287582602503
2
Iteration 9400: Loss = -11823.30126929709
3
Iteration 9500: Loss = -11823.290362559888
4
Iteration 9600: Loss = -11823.292345164866
5
Iteration 9700: Loss = -11823.291171340046
6
Iteration 9800: Loss = -11823.309406325387
7
Iteration 9900: Loss = -11823.336706231055
8
Iteration 10000: Loss = -11823.310372737309
9
Iteration 10100: Loss = -11823.290858677783
10
Iteration 10200: Loss = -11823.286821805583
Iteration 10300: Loss = -11823.288034335488
1
Iteration 10400: Loss = -11823.289702762073
2
Iteration 10500: Loss = -11823.288638233842
3
Iteration 10600: Loss = -11823.28756369395
4
Iteration 10700: Loss = -11823.3055629785
5
Iteration 10800: Loss = -11823.28970816384
6
Iteration 10900: Loss = -11823.292799392171
7
Iteration 11000: Loss = -11823.288444421092
8
Iteration 11100: Loss = -11823.29437347371
9
Iteration 11200: Loss = -11823.287203079944
10
Iteration 11300: Loss = -11823.294604812907
11
Iteration 11400: Loss = -11823.298686148972
12
Iteration 11500: Loss = -11823.295417201542
13
Iteration 11600: Loss = -11823.286982432463
14
Iteration 11700: Loss = -11823.359469358074
15
Stopping early at iteration 11700 due to no improvement.
pi: tensor([[0.7820, 0.2180],
        [0.2880, 0.7120]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4500, 0.5500], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3048, 0.0990],
         [0.6617, 0.3037]],

        [[0.6039, 0.0912],
         [0.6669, 0.6687]],

        [[0.5584, 0.0937],
         [0.5480, 0.6838]],

        [[0.7143, 0.1000],
         [0.6229, 0.6724]],

        [[0.5719, 0.0983],
         [0.5236, 0.6166]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320648505942
Average Adjusted Rand Index: 0.9841602586080228
11832.624752466018
[0.006528075836638138, 0.9840320648505942] [0.0013353894992507293, 0.9841602586080228] [12391.69744895745, 11823.359469358074]
-------------------------------------
This iteration is 13
True Objective function: Loss = -11903.701900903518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20995.326665499884
Iteration 100: Loss = -12356.138164460379
Iteration 200: Loss = -12355.529076081566
Iteration 300: Loss = -12354.815933591268
Iteration 400: Loss = -12354.220838098096
Iteration 500: Loss = -12353.309776620305
Iteration 600: Loss = -12351.211333395022
Iteration 700: Loss = -12329.565612655932
Iteration 800: Loss = -12247.533716174486
Iteration 900: Loss = -11931.618634418413
Iteration 1000: Loss = -11896.291998189896
Iteration 1100: Loss = -11890.25561677964
Iteration 1200: Loss = -11890.101719462735
Iteration 1300: Loss = -11889.986492819571
Iteration 1400: Loss = -11889.680607277243
Iteration 1500: Loss = -11889.636600392281
Iteration 1600: Loss = -11889.608313088822
Iteration 1700: Loss = -11889.587017091182
Iteration 1800: Loss = -11889.570267585244
Iteration 1900: Loss = -11889.556793001599
Iteration 2000: Loss = -11889.545685075664
Iteration 2100: Loss = -11889.536362777066
Iteration 2200: Loss = -11889.528311337432
Iteration 2300: Loss = -11889.521270358198
Iteration 2400: Loss = -11889.51562086606
Iteration 2500: Loss = -11889.509323341272
Iteration 2600: Loss = -11889.504534675076
Iteration 2700: Loss = -11889.500473722837
Iteration 2800: Loss = -11889.496708424713
Iteration 2900: Loss = -11889.4931149285
Iteration 3000: Loss = -11889.490562895677
Iteration 3100: Loss = -11889.485343045988
Iteration 3200: Loss = -11889.479356614584
Iteration 3300: Loss = -11889.46632125225
Iteration 3400: Loss = -11889.42088172407
Iteration 3500: Loss = -11889.396953185149
Iteration 3600: Loss = -11889.394720443599
Iteration 3700: Loss = -11889.395672251201
1
Iteration 3800: Loss = -11889.388605558968
Iteration 3900: Loss = -11889.38155278069
Iteration 4000: Loss = -11889.350097028766
Iteration 4100: Loss = -11889.347366111522
Iteration 4200: Loss = -11889.346486782075
Iteration 4300: Loss = -11889.34505237207
Iteration 4400: Loss = -11889.344151735164
Iteration 4500: Loss = -11889.34395905253
Iteration 4600: Loss = -11889.341895875821
Iteration 4700: Loss = -11889.340259669007
Iteration 4800: Loss = -11889.338138867924
Iteration 4900: Loss = -11889.337407732355
Iteration 5000: Loss = -11889.336975860479
Iteration 5100: Loss = -11889.336511407168
Iteration 5200: Loss = -11889.336066048972
Iteration 5300: Loss = -11889.33603386188
Iteration 5400: Loss = -11889.335151833495
Iteration 5500: Loss = -11889.335126341695
Iteration 5600: Loss = -11889.335961127275
1
Iteration 5700: Loss = -11889.394732828305
2
Iteration 5800: Loss = -11889.334705462956
Iteration 5900: Loss = -11889.334966055143
1
Iteration 6000: Loss = -11889.351789050546
2
Iteration 6100: Loss = -11889.333065959598
Iteration 6200: Loss = -11889.334238151398
1
Iteration 6300: Loss = -11889.332617352513
Iteration 6400: Loss = -11889.346431938178
1
Iteration 6500: Loss = -11889.332148001935
Iteration 6600: Loss = -11889.363615275004
1
Iteration 6700: Loss = -11889.331763309634
Iteration 6800: Loss = -11889.33172722746
Iteration 6900: Loss = -11889.331345788572
Iteration 7000: Loss = -11889.33132072577
Iteration 7100: Loss = -11889.330999052923
Iteration 7200: Loss = -11889.333730826036
1
Iteration 7300: Loss = -11889.330740840487
Iteration 7400: Loss = -11889.377622513748
1
Iteration 7500: Loss = -11889.332081987524
2
Iteration 7600: Loss = -11889.33044590114
Iteration 7700: Loss = -11889.331114989382
1
Iteration 7800: Loss = -11889.334597527859
2
Iteration 7900: Loss = -11889.330173163115
Iteration 8000: Loss = -11889.330654565627
1
Iteration 8100: Loss = -11889.340284122922
2
Iteration 8200: Loss = -11889.32992800692
Iteration 8300: Loss = -11889.35493625191
1
Iteration 8400: Loss = -11889.329779758124
Iteration 8500: Loss = -11889.329821942021
Iteration 8600: Loss = -11889.341513803229
1
Iteration 8700: Loss = -11889.329426496393
Iteration 8800: Loss = -11889.329233737639
Iteration 8900: Loss = -11889.328987732595
Iteration 9000: Loss = -11889.329102931431
1
Iteration 9100: Loss = -11889.32894899619
Iteration 9200: Loss = -11889.51814611697
1
Iteration 9300: Loss = -11889.3454815507
2
Iteration 9400: Loss = -11889.329797163613
3
Iteration 9500: Loss = -11889.333343784925
4
Iteration 9600: Loss = -11889.33079638905
5
Iteration 9700: Loss = -11889.32958300474
6
Iteration 9800: Loss = -11889.355176128134
7
Iteration 9900: Loss = -11889.348171450478
8
Iteration 10000: Loss = -11889.328398483154
Iteration 10100: Loss = -11889.328970758494
1
Iteration 10200: Loss = -11889.329841731771
2
Iteration 10300: Loss = -11889.3292009845
3
Iteration 10400: Loss = -11889.381490408843
4
Iteration 10500: Loss = -11889.33342318105
5
Iteration 10600: Loss = -11889.496007769249
6
Iteration 10700: Loss = -11889.334270399191
7
Iteration 10800: Loss = -11889.328350152755
Iteration 10900: Loss = -11889.383996025077
1
Iteration 11000: Loss = -11889.324883065485
Iteration 11100: Loss = -11889.324137430673
Iteration 11200: Loss = -11889.334962736115
1
Iteration 11300: Loss = -11889.472843777967
2
Iteration 11400: Loss = -11889.323505134189
Iteration 11500: Loss = -11889.324010494644
1
Iteration 11600: Loss = -11889.342640631075
2
Iteration 11700: Loss = -11889.354987496474
3
Iteration 11800: Loss = -11889.32569512808
4
Iteration 11900: Loss = -11889.324453641679
5
Iteration 12000: Loss = -11889.364620361168
6
Iteration 12100: Loss = -11889.328416321812
7
Iteration 12200: Loss = -11889.331057653642
8
Iteration 12300: Loss = -11889.341657093577
9
Iteration 12400: Loss = -11889.411214480218
10
Iteration 12500: Loss = -11889.337125577676
11
Iteration 12600: Loss = -11889.338804328385
12
Iteration 12700: Loss = -11889.370573362798
13
Iteration 12800: Loss = -11889.357675047857
14
Iteration 12900: Loss = -11889.424661888796
15
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[0.7097, 0.2903],
        [0.2236, 0.7764]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3724, 0.6276], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2857, 0.1009],
         [0.5090, 0.2974]],

        [[0.5517, 0.0964],
         [0.7065, 0.5365]],

        [[0.5094, 0.1117],
         [0.5836, 0.5821]],

        [[0.6113, 0.0958],
         [0.6083, 0.6497]],

        [[0.5376, 0.0981],
         [0.6213, 0.5266]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840172519118843
Average Adjusted Rand Index: 0.9839932884892886
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22256.093176351667
Iteration 100: Loss = -12236.41533891111
Iteration 200: Loss = -11916.009022857561
Iteration 300: Loss = -11894.085017488436
Iteration 400: Loss = -11893.52698189875
Iteration 500: Loss = -11893.287236303613
Iteration 600: Loss = -11892.759426925484
Iteration 700: Loss = -11892.356490215654
Iteration 800: Loss = -11892.312357393059
Iteration 900: Loss = -11892.281967230601
Iteration 1000: Loss = -11892.259675077275
Iteration 1100: Loss = -11892.242674692387
Iteration 1200: Loss = -11892.229284489636
Iteration 1300: Loss = -11892.218685021977
Iteration 1400: Loss = -11892.210083759694
Iteration 1500: Loss = -11892.203021654144
Iteration 1600: Loss = -11892.197115464862
Iteration 1700: Loss = -11892.192142404754
Iteration 1800: Loss = -11892.187815689282
Iteration 1900: Loss = -11892.184050660848
Iteration 2000: Loss = -11892.180865520524
Iteration 2100: Loss = -11892.17818960392
Iteration 2200: Loss = -11892.175776958098
Iteration 2300: Loss = -11892.173703220245
Iteration 2400: Loss = -11892.171809169156
Iteration 2500: Loss = -11892.170191268639
Iteration 2600: Loss = -11892.168680947014
Iteration 2700: Loss = -11892.167368741579
Iteration 2800: Loss = -11892.166131119197
Iteration 2900: Loss = -11892.164966254848
Iteration 3000: Loss = -11892.164094185875
Iteration 3100: Loss = -11892.16306468418
Iteration 3200: Loss = -11892.16215993369
Iteration 3300: Loss = -11892.16743017208
1
Iteration 3400: Loss = -11892.160082284337
Iteration 3500: Loss = -11892.159202723778
Iteration 3600: Loss = -11892.170555025577
1
Iteration 3700: Loss = -11892.15795781382
Iteration 3800: Loss = -11892.157472262186
Iteration 3900: Loss = -11892.157217458918
Iteration 4000: Loss = -11892.156656341858
Iteration 4100: Loss = -11892.156120631716
Iteration 4200: Loss = -11892.155747702316
Iteration 4300: Loss = -11892.155344396117
Iteration 4400: Loss = -11892.15499666506
Iteration 4500: Loss = -11892.154599235986
Iteration 4600: Loss = -11892.155284260283
1
Iteration 4700: Loss = -11892.143737740846
Iteration 4800: Loss = -11892.139569878907
Iteration 4900: Loss = -11892.137454410002
Iteration 5000: Loss = -11889.394211533072
Iteration 5100: Loss = -11889.393510706526
Iteration 5200: Loss = -11889.389372914802
Iteration 5300: Loss = -11889.38729025351
Iteration 5400: Loss = -11889.385681331387
Iteration 5500: Loss = -11889.387428281068
1
Iteration 5600: Loss = -11889.385062527854
Iteration 5700: Loss = -11889.38449672834
Iteration 5800: Loss = -11889.383596084901
Iteration 5900: Loss = -11889.382477583047
Iteration 6000: Loss = -11889.381727168347
Iteration 6100: Loss = -11889.381519425266
Iteration 6200: Loss = -11889.381412629022
Iteration 6300: Loss = -11889.3814252689
Iteration 6400: Loss = -11889.381175462524
Iteration 6500: Loss = -11889.381331389362
1
Iteration 6600: Loss = -11889.39331140588
2
Iteration 6700: Loss = -11889.384836762263
3
Iteration 6800: Loss = -11889.3880789457
4
Iteration 6900: Loss = -11889.380933553819
Iteration 7000: Loss = -11889.380581204754
Iteration 7100: Loss = -11889.380453671562
Iteration 7200: Loss = -11889.382913471918
1
Iteration 7300: Loss = -11889.399284164134
2
Iteration 7400: Loss = -11889.382347213914
3
Iteration 7500: Loss = -11889.381102123692
4
Iteration 7600: Loss = -11889.379962042558
Iteration 7700: Loss = -11889.379939643139
Iteration 7800: Loss = -11889.380652411395
1
Iteration 7900: Loss = -11889.37980273429
Iteration 8000: Loss = -11889.37970223924
Iteration 8100: Loss = -11889.392503472696
1
Iteration 8200: Loss = -11889.380146617446
2
Iteration 8300: Loss = -11889.376271524177
Iteration 8400: Loss = -11889.343108429668
Iteration 8500: Loss = -11889.343981503682
1
Iteration 8600: Loss = -11889.34284063062
Iteration 8700: Loss = -11889.355026250138
1
Iteration 8800: Loss = -11889.341986479209
Iteration 8900: Loss = -11889.345791013227
1
Iteration 9000: Loss = -11889.342274205868
2
Iteration 9100: Loss = -11889.345407256982
3
Iteration 9200: Loss = -11889.332631462737
Iteration 9300: Loss = -11889.33593642389
1
Iteration 9400: Loss = -11889.340517601626
2
Iteration 9500: Loss = -11889.332277773185
Iteration 9600: Loss = -11889.331379679988
Iteration 9700: Loss = -11889.331800828493
1
Iteration 9800: Loss = -11889.332013090734
2
Iteration 9900: Loss = -11889.345072741013
3
Iteration 10000: Loss = -11889.334904273213
4
Iteration 10100: Loss = -11889.356200124246
5
Iteration 10200: Loss = -11889.348253363005
6
Iteration 10300: Loss = -11889.331853601747
7
Iteration 10400: Loss = -11889.3334502877
8
Iteration 10500: Loss = -11889.332253529283
9
Iteration 10600: Loss = -11889.337208727047
10
Iteration 10700: Loss = -11889.346699792712
11
Iteration 10800: Loss = -11889.554130095468
12
Iteration 10900: Loss = -11889.344912584802
13
Iteration 11000: Loss = -11889.36015216836
14
Iteration 11100: Loss = -11889.339267154157
15
Stopping early at iteration 11100 due to no improvement.
pi: tensor([[0.7112, 0.2888],
        [0.2218, 0.7782]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3701, 0.6299], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2873, 0.1008],
         [0.6791, 0.2957]],

        [[0.5712, 0.0955],
         [0.5797, 0.6375]],

        [[0.5786, 0.1113],
         [0.6155, 0.6697]],

        [[0.6161, 0.0958],
         [0.6999, 0.5726]],

        [[0.5491, 0.0989],
         [0.5148, 0.6727]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840172519118843
Average Adjusted Rand Index: 0.9839932884892886
11903.701900903518
[0.9840172519118843, 0.9840172519118843] [0.9839932884892886, 0.9839932884892886] [11889.424661888796, 11889.339267154157]
-------------------------------------
This iteration is 14
True Objective function: Loss = -11913.81237560901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23384.020802618044
Iteration 100: Loss = -11971.934133498473
Iteration 200: Loss = -11943.190048208482
Iteration 300: Loss = -11942.670861141398
Iteration 400: Loss = -11942.472576225164
Iteration 500: Loss = -11942.369555631692
Iteration 600: Loss = -11942.308846945762
Iteration 700: Loss = -11942.269410760346
Iteration 800: Loss = -11942.242053229897
Iteration 900: Loss = -11942.222297460494
Iteration 1000: Loss = -11942.207579592567
Iteration 1100: Loss = -11942.196292380268
Iteration 1200: Loss = -11942.187435152677
Iteration 1300: Loss = -11942.18032741875
Iteration 1400: Loss = -11942.174531366585
Iteration 1500: Loss = -11942.169758230171
Iteration 1600: Loss = -11942.165725034449
Iteration 1700: Loss = -11942.162375272972
Iteration 1800: Loss = -11942.159458800768
Iteration 1900: Loss = -11942.157012945678
Iteration 2000: Loss = -11942.154865827297
Iteration 2100: Loss = -11942.152963871911
Iteration 2200: Loss = -11942.151348328876
Iteration 2300: Loss = -11942.1499284397
Iteration 2400: Loss = -11942.148613089272
Iteration 2500: Loss = -11942.147492427348
Iteration 2600: Loss = -11942.14646431614
Iteration 2700: Loss = -11942.145978126968
Iteration 2800: Loss = -11942.146034769588
Iteration 2900: Loss = -11942.143991594909
Iteration 3000: Loss = -11942.15981764014
1
Iteration 3100: Loss = -11942.142697067726
Iteration 3200: Loss = -11942.14215291089
Iteration 3300: Loss = -11942.141721883487
Iteration 3400: Loss = -11942.141412036024
Iteration 3500: Loss = -11942.148594740454
1
Iteration 3600: Loss = -11942.140419762909
Iteration 3700: Loss = -11942.140267891073
Iteration 3800: Loss = -11942.139684284266
Iteration 3900: Loss = -11942.139421279273
Iteration 4000: Loss = -11942.139127430419
Iteration 4100: Loss = -11942.138854863184
Iteration 4200: Loss = -11942.138962097135
1
Iteration 4300: Loss = -11942.139062931241
2
Iteration 4400: Loss = -11942.13903619919
3
Iteration 4500: Loss = -11942.13801968986
Iteration 4600: Loss = -11942.138147993302
1
Iteration 4700: Loss = -11942.140671843767
2
Iteration 4800: Loss = -11942.13824994987
3
Iteration 4900: Loss = -11942.137933548476
Iteration 5000: Loss = -11942.137325184269
Iteration 5100: Loss = -11942.138760161297
1
Iteration 5200: Loss = -11942.13745984223
2
Iteration 5300: Loss = -11942.142175435894
3
Iteration 5400: Loss = -11942.140612939025
4
Iteration 5500: Loss = -11942.136700950574
Iteration 5600: Loss = -11942.14198399379
1
Iteration 5700: Loss = -11942.136497020556
Iteration 5800: Loss = -11942.148275032065
1
Iteration 5900: Loss = -11942.136662774132
2
Iteration 6000: Loss = -11942.136545438785
Iteration 6100: Loss = -11942.136370151
Iteration 6200: Loss = -11942.136199719327
Iteration 6300: Loss = -11942.136994320197
1
Iteration 6400: Loss = -11942.136115923435
Iteration 6500: Loss = -11942.13658877104
1
Iteration 6600: Loss = -11942.137889090836
2
Iteration 6700: Loss = -11942.135922506492
Iteration 6800: Loss = -11942.136017944771
Iteration 6900: Loss = -11942.13679859339
1
Iteration 7000: Loss = -11942.136565282191
2
Iteration 7100: Loss = -11942.13968104216
3
Iteration 7200: Loss = -11942.137261284734
4
Iteration 7300: Loss = -11942.13720675902
5
Iteration 7400: Loss = -11942.149919198044
6
Iteration 7500: Loss = -11942.135664582062
Iteration 7600: Loss = -11942.135657467254
Iteration 7700: Loss = -11942.148912998506
1
Iteration 7800: Loss = -11942.135670472728
Iteration 7900: Loss = -11942.13559747908
Iteration 8000: Loss = -11942.13653713436
1
Iteration 8100: Loss = -11942.136164609943
2
Iteration 8200: Loss = -11942.135545146817
Iteration 8300: Loss = -11942.135564546452
Iteration 8400: Loss = -11942.13547367637
Iteration 8500: Loss = -11942.138590468283
1
Iteration 8600: Loss = -11942.1362009094
2
Iteration 8700: Loss = -11942.13555115805
Iteration 8800: Loss = -11942.135632548338
Iteration 8900: Loss = -11942.135610620526
Iteration 9000: Loss = -11942.158085146419
1
Iteration 9100: Loss = -11942.135382049168
Iteration 9200: Loss = -11942.143870669846
1
Iteration 9300: Loss = -11942.135371633767
Iteration 9400: Loss = -11942.13536077428
Iteration 9500: Loss = -11942.137858883747
1
Iteration 9600: Loss = -11942.135370565986
Iteration 9700: Loss = -11942.135286345407
Iteration 9800: Loss = -11942.135747908576
1
Iteration 9900: Loss = -11942.135294436104
Iteration 10000: Loss = -11942.15047415403
1
Iteration 10100: Loss = -11942.148345357164
2
Iteration 10200: Loss = -11942.135585670825
3
Iteration 10300: Loss = -11942.1354354369
4
Iteration 10400: Loss = -11942.170657330647
5
Iteration 10500: Loss = -11942.150049619566
6
Iteration 10600: Loss = -11942.135309240319
Iteration 10700: Loss = -11942.144197440035
1
Iteration 10800: Loss = -11942.327121568858
2
Iteration 10900: Loss = -11942.137048032639
3
Iteration 11000: Loss = -11942.140549865782
4
Iteration 11100: Loss = -11942.14822118474
5
Iteration 11200: Loss = -11942.138559957644
6
Iteration 11300: Loss = -11942.13526037082
Iteration 11400: Loss = -11942.137506788727
1
Iteration 11500: Loss = -11942.15060987282
2
Iteration 11600: Loss = -11942.139939777939
3
Iteration 11700: Loss = -11942.226221200817
4
Iteration 11800: Loss = -11942.144634006778
5
Iteration 11900: Loss = -11942.135363911506
6
Iteration 12000: Loss = -11942.141456158013
7
Iteration 12100: Loss = -11942.162376532227
8
Iteration 12200: Loss = -11942.138125877967
9
Iteration 12300: Loss = -11942.210971223116
10
Iteration 12400: Loss = -11942.16745483903
11
Iteration 12500: Loss = -11942.135600968562
12
Iteration 12600: Loss = -11942.136071921588
13
Iteration 12700: Loss = -11942.220913381734
14
Iteration 12800: Loss = -11942.139427401964
15
Stopping early at iteration 12800 due to no improvement.
pi: tensor([[0.6284, 0.3716],
        [0.3303, 0.6697]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5927, 0.4073], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3181, 0.0955],
         [0.5158, 0.2890]],

        [[0.5864, 0.1008],
         [0.6158, 0.5104]],

        [[0.5481, 0.0985],
         [0.6009, 0.6250]],

        [[0.7026, 0.1003],
         [0.6208, 0.5781]],

        [[0.7278, 0.0988],
         [0.5515, 0.6980]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.036514803032027816
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21468.748012531967
Iteration 100: Loss = -12467.036512529707
Iteration 200: Loss = -12466.466341544372
Iteration 300: Loss = -12466.382526035002
Iteration 400: Loss = -12466.33869079215
Iteration 500: Loss = -12466.29951438015
Iteration 600: Loss = -12466.248129910711
Iteration 700: Loss = -12466.148754783622
Iteration 800: Loss = -12465.801698406554
Iteration 900: Loss = -12463.701226947524
Iteration 1000: Loss = -12457.531202521064
Iteration 1100: Loss = -12457.034198934425
Iteration 1200: Loss = -12456.868759533703
Iteration 1300: Loss = -12456.772516540232
Iteration 1400: Loss = -12456.703817452726
Iteration 1500: Loss = -12456.645639684248
Iteration 1600: Loss = -12456.590679532572
Iteration 1700: Loss = -12456.53518794117
Iteration 1800: Loss = -12456.475087780233
Iteration 1900: Loss = -12456.40180567166
Iteration 2000: Loss = -12456.301495484973
Iteration 2100: Loss = -12456.178560906186
Iteration 2200: Loss = -12456.06318416577
Iteration 2300: Loss = -12455.96798136983
Iteration 2400: Loss = -12455.892686490928
Iteration 2500: Loss = -12455.835055190833
Iteration 2600: Loss = -12455.79195774237
Iteration 2700: Loss = -12455.759785474473
Iteration 2800: Loss = -12455.735735171687
Iteration 2900: Loss = -12455.717592104591
Iteration 3000: Loss = -12455.703505670168
Iteration 3100: Loss = -12455.69245658069
Iteration 3200: Loss = -12455.683530624847
Iteration 3300: Loss = -12455.676309046537
Iteration 3400: Loss = -12455.670292617535
Iteration 3500: Loss = -12455.665220438028
Iteration 3600: Loss = -12455.660899306007
Iteration 3700: Loss = -12455.657172375957
Iteration 3800: Loss = -12455.653941952076
Iteration 3900: Loss = -12455.651140593349
Iteration 4000: Loss = -12455.648585898201
Iteration 4100: Loss = -12455.646395633232
Iteration 4200: Loss = -12455.644450297521
Iteration 4300: Loss = -12455.642746128542
Iteration 4400: Loss = -12455.641132738474
Iteration 4500: Loss = -12455.639704204254
Iteration 4600: Loss = -12455.638404505537
Iteration 4700: Loss = -12455.637194772129
Iteration 4800: Loss = -12455.636065489452
Iteration 4900: Loss = -12455.635124868228
Iteration 5000: Loss = -12455.634253782213
Iteration 5100: Loss = -12455.633397869293
Iteration 5200: Loss = -12455.632669007662
Iteration 5300: Loss = -12455.631994821679
Iteration 5400: Loss = -12455.631333558362
Iteration 5500: Loss = -12455.630739569013
Iteration 5600: Loss = -12455.63017953589
Iteration 5700: Loss = -12455.629626601149
Iteration 5800: Loss = -12455.629205578676
Iteration 5900: Loss = -12455.628779479253
Iteration 6000: Loss = -12455.628341113832
Iteration 6100: Loss = -12455.627991412923
Iteration 6200: Loss = -12455.627634831162
Iteration 6300: Loss = -12455.627277624115
Iteration 6400: Loss = -12455.626948584491
Iteration 6500: Loss = -12455.62667205043
Iteration 6600: Loss = -12455.62645033096
Iteration 6700: Loss = -12455.626184481967
Iteration 6800: Loss = -12456.153326168287
1
Iteration 6900: Loss = -12455.625683533362
Iteration 7000: Loss = -12455.625451322558
Iteration 7100: Loss = -12455.625284993017
Iteration 7200: Loss = -12455.625172598788
Iteration 7300: Loss = -12455.624922848356
Iteration 7400: Loss = -12455.62473363768
Iteration 7500: Loss = -12455.664048024637
1
Iteration 7600: Loss = -12455.62447053003
Iteration 7700: Loss = -12455.624298267823
Iteration 7800: Loss = -12455.624214996833
Iteration 7900: Loss = -12455.624453550767
1
Iteration 8000: Loss = -12455.623935001275
Iteration 8100: Loss = -12455.623809646237
Iteration 8200: Loss = -12455.625782529316
1
Iteration 8300: Loss = -12455.623604327033
Iteration 8400: Loss = -12455.623547816202
Iteration 8500: Loss = -12455.623416778091
Iteration 8600: Loss = -12456.366456092146
1
Iteration 8700: Loss = -12455.623277857001
Iteration 8800: Loss = -12455.623212419441
Iteration 8900: Loss = -12455.62313522232
Iteration 9000: Loss = -12455.62392349578
1
Iteration 9100: Loss = -12455.622955655288
Iteration 9200: Loss = -12455.622944002853
Iteration 9300: Loss = -12455.623030767681
Iteration 9400: Loss = -12455.622945031151
Iteration 9500: Loss = -12455.62276130769
Iteration 9600: Loss = -12455.690631989377
1
Iteration 9700: Loss = -12455.622632564324
Iteration 9800: Loss = -12455.622663167786
Iteration 9900: Loss = -12455.817005334697
1
Iteration 10000: Loss = -12455.622585127447
Iteration 10100: Loss = -12455.622536677094
Iteration 10200: Loss = -12455.6228932136
1
Iteration 10300: Loss = -12455.622920556865
2
Iteration 10400: Loss = -12455.622366068614
Iteration 10500: Loss = -12455.622381674348
Iteration 10600: Loss = -12455.625006099326
1
Iteration 10700: Loss = -12455.622342934052
Iteration 10800: Loss = -12455.753918258733
1
Iteration 10900: Loss = -12455.6222707908
Iteration 11000: Loss = -12455.622255942131
Iteration 11100: Loss = -12455.622315896253
Iteration 11200: Loss = -12455.622200524404
Iteration 11300: Loss = -12455.667194987538
1
Iteration 11400: Loss = -12455.622183204396
Iteration 11500: Loss = -12455.622182666042
Iteration 11600: Loss = -12455.622171578696
Iteration 11700: Loss = -12455.622105607652
Iteration 11800: Loss = -12456.325339156494
1
Iteration 11900: Loss = -12455.622119916765
Iteration 12000: Loss = -12455.622067232489
Iteration 12100: Loss = -12455.622238181431
1
Iteration 12200: Loss = -12455.622073277013
Iteration 12300: Loss = -12455.679001498786
1
Iteration 12400: Loss = -12455.622032790334
Iteration 12500: Loss = -12455.646773135328
1
Iteration 12600: Loss = -12455.622025709898
Iteration 12700: Loss = -12455.62199975112
Iteration 12800: Loss = -12455.622128932639
1
Iteration 12900: Loss = -12455.621963809142
Iteration 13000: Loss = -12455.62196228941
Iteration 13100: Loss = -12455.622035619106
Iteration 13200: Loss = -12455.621920957585
Iteration 13300: Loss = -12455.621962948566
Iteration 13400: Loss = -12455.622391535691
1
Iteration 13500: Loss = -12455.621988264937
Iteration 13600: Loss = -12455.623004765703
1
Iteration 13700: Loss = -12455.62195662231
Iteration 13800: Loss = -12455.62194376287
Iteration 13900: Loss = -12455.622093005695
1
Iteration 14000: Loss = -12455.6219196893
Iteration 14100: Loss = -12456.023355618541
1
Iteration 14200: Loss = -12455.621921837119
Iteration 14300: Loss = -12455.621916007933
Iteration 14400: Loss = -12455.622347483126
1
Iteration 14500: Loss = -12455.621910731088
Iteration 14600: Loss = -12455.622783506364
1
Iteration 14700: Loss = -12455.621956185045
Iteration 14800: Loss = -12455.62184918036
Iteration 14900: Loss = -12455.85914881167
1
Iteration 15000: Loss = -12455.621863266528
Iteration 15100: Loss = -12455.621870994019
Iteration 15200: Loss = -12455.622174342008
1
Iteration 15300: Loss = -12455.621875692004
Iteration 15400: Loss = -12455.622024685676
1
Iteration 15500: Loss = -12455.62195395099
Iteration 15600: Loss = -12455.621879646975
Iteration 15700: Loss = -12455.621879441645
Iteration 15800: Loss = -12455.622065243651
1
Iteration 15900: Loss = -12455.621880119437
Iteration 16000: Loss = -12455.621873523063
Iteration 16100: Loss = -12456.083723098214
1
Iteration 16200: Loss = -12455.621857583514
Iteration 16300: Loss = -12455.621892662157
Iteration 16400: Loss = -12455.629152718335
1
Iteration 16500: Loss = -12455.621880055907
Iteration 16600: Loss = -12455.621855155981
Iteration 16700: Loss = -12455.621888875383
Iteration 16800: Loss = -12455.621961950183
Iteration 16900: Loss = -12455.621840107147
Iteration 17000: Loss = -12455.629060130344
1
Iteration 17100: Loss = -12455.621836705372
Iteration 17200: Loss = -12455.673981351998
1
Iteration 17300: Loss = -12455.62185715626
Iteration 17400: Loss = -12455.886885534754
1
Iteration 17500: Loss = -12455.621856880069
Iteration 17600: Loss = -12455.622057135111
1
Iteration 17700: Loss = -12455.622016852212
2
Iteration 17800: Loss = -12455.622306066009
3
Iteration 17900: Loss = -12455.62185417996
Iteration 18000: Loss = -12455.628466142023
1
Iteration 18100: Loss = -12455.621899152078
Iteration 18200: Loss = -12455.62221028875
1
Iteration 18300: Loss = -12455.62187746358
Iteration 18400: Loss = -12455.621842239032
Iteration 18500: Loss = -12455.622409787498
1
Iteration 18600: Loss = -12455.621818292288
Iteration 18700: Loss = -12455.625122951335
1
Iteration 18800: Loss = -12455.621867394153
Iteration 18900: Loss = -12455.627305416574
1
Iteration 19000: Loss = -12455.621920293059
Iteration 19100: Loss = -12455.62187051357
Iteration 19200: Loss = -12455.632487879118
1
Iteration 19300: Loss = -12455.621891317423
Iteration 19400: Loss = -12455.63224198528
1
Iteration 19500: Loss = -12455.622026505087
2
Iteration 19600: Loss = -12455.622023876049
3
Iteration 19700: Loss = -12455.621860149273
Iteration 19800: Loss = -12455.62227321144
1
Iteration 19900: Loss = -12455.621913639518
pi: tensor([[1.0000e+00, 3.4079e-08],
        [7.4802e-01, 2.5198e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9447, 0.0553], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2052, 0.0940],
         [0.6867, 0.5082]],

        [[0.6297, 0.0839],
         [0.6265, 0.6388]],

        [[0.6960, 0.1408],
         [0.6008, 0.5376]],

        [[0.6726, 0.2325],
         [0.6654, 0.6161]],

        [[0.5791, 0.2016],
         [0.6575, 0.6342]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.05175331019144027
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.00877236457352431
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.008886340468535413
Average Adjusted Rand Index: 0.013195849273487634
11913.81237560901
[0.036514803032027816, 0.008886340468535413] [0.9919998119331364, 0.013195849273487634] [11942.139427401964, 12455.62201392534]
-------------------------------------
This iteration is 15
True Objective function: Loss = -11918.286111040668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21592.364089715735
Iteration 100: Loss = -12408.073536144067
Iteration 200: Loss = -12407.230358917572
Iteration 300: Loss = -12406.123643690551
Iteration 400: Loss = -12403.954478061632
Iteration 500: Loss = -12403.699216614434
Iteration 600: Loss = -12403.559989346455
Iteration 700: Loss = -12403.45545204249
Iteration 800: Loss = -12403.357173827088
Iteration 900: Loss = -12403.203037962632
Iteration 1000: Loss = -12401.407262324148
Iteration 1100: Loss = -12352.336177068515
Iteration 1200: Loss = -12318.47246264076
Iteration 1300: Loss = -12318.207868753118
Iteration 1400: Loss = -12318.102608667788
Iteration 1500: Loss = -12318.035986507308
Iteration 1600: Loss = -12317.986632038052
Iteration 1700: Loss = -12317.94699862871
Iteration 1800: Loss = -12317.913914412717
Iteration 1900: Loss = -12317.885855767443
Iteration 2000: Loss = -12317.861740912647
Iteration 2100: Loss = -12317.84033275106
Iteration 2200: Loss = -12317.820787962377
Iteration 2300: Loss = -12317.80183699508
Iteration 2400: Loss = -12317.781649502722
Iteration 2500: Loss = -12317.755965845075
Iteration 2600: Loss = -12317.711319711214
Iteration 2700: Loss = -12317.596351763088
Iteration 2800: Loss = -12317.378603993648
Iteration 2900: Loss = -12317.240206509654
Iteration 3000: Loss = -12317.170262478707
Iteration 3100: Loss = -12317.133435340287
Iteration 3200: Loss = -12317.111567248372
Iteration 3300: Loss = -12317.097164149778
Iteration 3400: Loss = -12317.086922368118
Iteration 3500: Loss = -12317.079246390269
Iteration 3600: Loss = -12317.073285600149
Iteration 3700: Loss = -12317.068435846624
Iteration 3800: Loss = -12317.064414358749
Iteration 3900: Loss = -12317.061135832817
Iteration 4000: Loss = -12317.058276781068
Iteration 4100: Loss = -12317.055837784179
Iteration 4200: Loss = -12317.053722172277
Iteration 4300: Loss = -12317.051826476185
Iteration 4400: Loss = -12317.050229794639
Iteration 4500: Loss = -12317.048722940144
Iteration 4600: Loss = -12317.04743508927
Iteration 4700: Loss = -12317.04625096262
Iteration 4800: Loss = -12317.045193887436
Iteration 4900: Loss = -12317.044255955538
Iteration 5000: Loss = -12317.043333931679
Iteration 5100: Loss = -12317.042509262716
Iteration 5200: Loss = -12317.041774921383
Iteration 5300: Loss = -12317.043143738054
1
Iteration 5400: Loss = -12317.040476193637
Iteration 5500: Loss = -12317.039934026665
Iteration 5600: Loss = -12317.039379216296
Iteration 5700: Loss = -12317.038896680706
Iteration 5800: Loss = -12317.03841273592
Iteration 5900: Loss = -12317.03799895776
Iteration 6000: Loss = -12317.037627732081
Iteration 6100: Loss = -12317.0372421808
Iteration 6200: Loss = -12317.036938967034
Iteration 6300: Loss = -12317.03661057887
Iteration 6400: Loss = -12317.036266887177
Iteration 6500: Loss = -12317.035977035292
Iteration 6600: Loss = -12317.035954498127
Iteration 6700: Loss = -12317.035508498322
Iteration 6800: Loss = -12317.03526780359
Iteration 6900: Loss = -12317.035071544875
Iteration 7000: Loss = -12317.035074927317
Iteration 7100: Loss = -12317.034634456615
Iteration 7200: Loss = -12317.034447508051
Iteration 7300: Loss = -12317.034327068748
Iteration 7400: Loss = -12317.034473667512
1
Iteration 7500: Loss = -12317.0339890229
Iteration 7600: Loss = -12317.033842932917
Iteration 7700: Loss = -12317.188822876513
1
Iteration 7800: Loss = -12317.033591338755
Iteration 7900: Loss = -12317.033466128409
Iteration 8000: Loss = -12317.03340242878
Iteration 8100: Loss = -12317.033504405066
1
Iteration 8200: Loss = -12317.122832296687
2
Iteration 8300: Loss = -12317.033075261115
Iteration 8400: Loss = -12317.048033775996
1
Iteration 8500: Loss = -12317.032878450393
Iteration 8600: Loss = -12317.03279692761
Iteration 8700: Loss = -12317.034546941719
1
Iteration 8800: Loss = -12317.032642666258
Iteration 8900: Loss = -12317.03265016623
Iteration 9000: Loss = -12317.038012325083
1
Iteration 9100: Loss = -12317.032484370326
Iteration 9200: Loss = -12317.038927478228
1
Iteration 9300: Loss = -12317.04130872456
2
Iteration 9400: Loss = -12317.034296045082
3
Iteration 9500: Loss = -12317.041958952888
4
Iteration 9600: Loss = -12317.032727887437
5
Iteration 9700: Loss = -12317.032812800893
6
Iteration 9800: Loss = -12317.035312849224
7
Iteration 9900: Loss = -12317.048875968376
8
Iteration 10000: Loss = -12317.032955972245
9
Iteration 10100: Loss = -12317.03436747647
10
Iteration 10200: Loss = -12317.032205575386
Iteration 10300: Loss = -12317.035568946078
1
Iteration 10400: Loss = -12317.046554363451
2
Iteration 10500: Loss = -12317.031903291878
Iteration 10600: Loss = -12317.059121332415
1
Iteration 10700: Loss = -12317.03294770227
2
Iteration 10800: Loss = -12317.041462416713
3
Iteration 10900: Loss = -12317.041259081925
4
Iteration 11000: Loss = -12317.0318054916
Iteration 11100: Loss = -12317.031958273792
1
Iteration 11200: Loss = -12317.032290421226
2
Iteration 11300: Loss = -12317.032457776262
3
Iteration 11400: Loss = -12317.059671181156
4
Iteration 11500: Loss = -12317.106029138919
5
Iteration 11600: Loss = -12317.032121630587
6
Iteration 11700: Loss = -12317.041655313174
7
Iteration 11800: Loss = -12317.142737850283
8
Iteration 11900: Loss = -12317.073999802438
9
Iteration 12000: Loss = -12317.042475707354
10
Iteration 12100: Loss = -12317.032206186423
11
Iteration 12200: Loss = -12317.032044252406
12
Iteration 12300: Loss = -12317.131317005504
13
Iteration 12400: Loss = -12317.033973601177
14
Iteration 12500: Loss = -12317.031577823453
Iteration 12600: Loss = -12317.031962645395
1
Iteration 12700: Loss = -12317.033912937126
2
Iteration 12800: Loss = -12317.03681264591
3
Iteration 12900: Loss = -12317.051486097314
4
Iteration 13000: Loss = -12317.041812456564
5
Iteration 13100: Loss = -12317.032315898998
6
Iteration 13200: Loss = -12317.031815097596
7
Iteration 13300: Loss = -12317.031467729992
Iteration 13400: Loss = -12317.038314480224
1
Iteration 13500: Loss = -12317.042248584226
2
Iteration 13600: Loss = -12317.033060098545
3
Iteration 13700: Loss = -12317.031576624086
4
Iteration 13800: Loss = -12317.03227529724
5
Iteration 13900: Loss = -12317.049474643303
6
Iteration 14000: Loss = -12317.03490445896
7
Iteration 14100: Loss = -12317.034296626116
8
Iteration 14200: Loss = -12317.03170579351
9
Iteration 14300: Loss = -12317.031734208325
10
Iteration 14400: Loss = -12317.032600542987
11
Iteration 14500: Loss = -12317.034575763846
12
Iteration 14600: Loss = -12317.03191574534
13
Iteration 14700: Loss = -12317.036199776552
14
Iteration 14800: Loss = -12317.032197894307
15
Stopping early at iteration 14800 due to no improvement.
pi: tensor([[4.8971e-07, 1.0000e+00],
        [1.3268e-02, 9.8673e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6091, 0.3909], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3108, 0.0975],
         [0.6810, 0.2025]],

        [[0.6826, 0.3286],
         [0.7228, 0.5846]],

        [[0.6837, 0.3096],
         [0.6542, 0.6654]],

        [[0.6343, 0.2045],
         [0.5776, 0.5522]],

        [[0.5562, 0.1242],
         [0.6550, 0.6886]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: -0.007201490658206174
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.006823058974689618
Average Adjusted Rand Index: 0.19770271204511292
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21520.334232526944
Iteration 100: Loss = -12407.864263189347
Iteration 200: Loss = -12407.059457487045
Iteration 300: Loss = -12406.145447122792
Iteration 400: Loss = -12403.48451764645
Iteration 500: Loss = -12402.516018188951
Iteration 600: Loss = -12402.05726609697
Iteration 700: Loss = -12401.785412856663
Iteration 800: Loss = -12401.61949717378
Iteration 900: Loss = -12401.512238798474
Iteration 1000: Loss = -12401.452724659264
Iteration 1100: Loss = -12401.416780035632
Iteration 1200: Loss = -12401.391484368633
Iteration 1300: Loss = -12401.372272281882
Iteration 1400: Loss = -12401.356068752979
Iteration 1500: Loss = -12401.341195251543
Iteration 1600: Loss = -12401.32716436347
Iteration 1700: Loss = -12401.314071161372
Iteration 1800: Loss = -12401.302150840516
Iteration 1900: Loss = -12401.29149138777
Iteration 2000: Loss = -12401.282246471155
Iteration 2100: Loss = -12401.274169338916
Iteration 2200: Loss = -12401.267205085276
Iteration 2300: Loss = -12401.261234049476
Iteration 2400: Loss = -12401.256112936555
Iteration 2500: Loss = -12401.251753565462
Iteration 2600: Loss = -12401.247974773516
Iteration 2700: Loss = -12401.244879121095
Iteration 2800: Loss = -12401.242113322214
Iteration 2900: Loss = -12401.239830613456
Iteration 3000: Loss = -12401.237925550067
Iteration 3100: Loss = -12401.236272179114
Iteration 3200: Loss = -12401.234894520325
Iteration 3300: Loss = -12401.233745784535
Iteration 3400: Loss = -12401.232720120273
Iteration 3500: Loss = -12401.231902576837
Iteration 3600: Loss = -12401.231143875404
Iteration 3700: Loss = -12401.230538741433
Iteration 3800: Loss = -12401.230030416504
Iteration 3900: Loss = -12401.229586466345
Iteration 4000: Loss = -12401.22915524769
Iteration 4100: Loss = -12401.228852329705
Iteration 4200: Loss = -12401.228606284654
Iteration 4300: Loss = -12401.228382566076
Iteration 4400: Loss = -12401.22814333172
Iteration 4500: Loss = -12401.227963592217
Iteration 4600: Loss = -12401.227855627316
Iteration 4700: Loss = -12401.227677082801
Iteration 4800: Loss = -12401.227549805832
Iteration 4900: Loss = -12401.227475746944
Iteration 5000: Loss = -12401.227377983503
Iteration 5100: Loss = -12401.227312877552
Iteration 5200: Loss = -12401.227211293442
Iteration 5300: Loss = -12401.227117587325
Iteration 5400: Loss = -12401.22707720593
Iteration 5500: Loss = -12401.227091301833
Iteration 5600: Loss = -12401.226957225763
Iteration 5700: Loss = -12401.22694280306
Iteration 5800: Loss = -12401.226893061603
Iteration 5900: Loss = -12401.226907031552
Iteration 6000: Loss = -12401.226864758884
Iteration 6100: Loss = -12401.226807297713
Iteration 6200: Loss = -12401.226758934527
Iteration 6300: Loss = -12401.226768651346
Iteration 6400: Loss = -12401.226740085282
Iteration 6500: Loss = -12401.226730310278
Iteration 6600: Loss = -12401.226717826354
Iteration 6700: Loss = -12401.22669134691
Iteration 6800: Loss = -12401.22667333913
Iteration 6900: Loss = -12401.226690639525
Iteration 7000: Loss = -12401.226672758427
Iteration 7100: Loss = -12401.226636971493
Iteration 7200: Loss = -12401.270799563546
1
Iteration 7300: Loss = -12401.226642914546
Iteration 7400: Loss = -12401.226736444642
Iteration 7500: Loss = -12401.226677539096
Iteration 7600: Loss = -12401.226623400304
Iteration 7700: Loss = -12401.227826011149
1
Iteration 7800: Loss = -12401.226575973373
Iteration 7900: Loss = -12401.226580076594
Iteration 8000: Loss = -12401.226745171287
1
Iteration 8100: Loss = -12401.226632194746
Iteration 8200: Loss = -12401.226587555318
Iteration 8300: Loss = -12401.227109906755
1
Iteration 8400: Loss = -12401.226578151161
Iteration 8500: Loss = -12401.226605141615
Iteration 8600: Loss = -12401.23237891286
1
Iteration 8700: Loss = -12401.226573152546
Iteration 8800: Loss = -12401.226586823519
Iteration 8900: Loss = -12401.526516202746
1
Iteration 9000: Loss = -12401.226580807772
Iteration 9100: Loss = -12401.226519296033
Iteration 9200: Loss = -12401.2265612402
Iteration 9300: Loss = -12401.226576384597
Iteration 9400: Loss = -12401.226574910404
Iteration 9500: Loss = -12401.226577069454
Iteration 9600: Loss = -12401.226722691998
1
Iteration 9700: Loss = -12401.226692515931
2
Iteration 9800: Loss = -12401.226543409732
Iteration 9900: Loss = -12401.226663075087
1
Iteration 10000: Loss = -12401.226560490804
Iteration 10100: Loss = -12401.226544608775
Iteration 10200: Loss = -12401.226536722092
Iteration 10300: Loss = -12401.226540679794
Iteration 10400: Loss = -12401.227315653634
1
Iteration 10500: Loss = -12401.226741047987
2
Iteration 10600: Loss = -12401.226566759093
Iteration 10700: Loss = -12401.252196768575
1
Iteration 10800: Loss = -12401.226549153269
Iteration 10900: Loss = -12401.226548452682
Iteration 11000: Loss = -12401.27863880885
1
Iteration 11100: Loss = -12401.226514477992
Iteration 11200: Loss = -12401.226561447847
Iteration 11300: Loss = -12401.794391073405
1
Iteration 11400: Loss = -12401.226585603172
Iteration 11500: Loss = -12401.22651696147
Iteration 11600: Loss = -12401.369519856458
1
Iteration 11700: Loss = -12401.226549546953
Iteration 11800: Loss = -12401.226531558379
Iteration 11900: Loss = -12401.226603776533
Iteration 12000: Loss = -12401.22654289884
Iteration 12100: Loss = -12401.238889680391
1
Iteration 12200: Loss = -12401.226541988328
Iteration 12300: Loss = -12401.226578689117
Iteration 12400: Loss = -12401.226524392066
Iteration 12500: Loss = -12401.231617568405
1
Iteration 12600: Loss = -12401.233987229914
2
Iteration 12700: Loss = -12401.226652820784
3
Iteration 12800: Loss = -12401.226661083138
4
Iteration 12900: Loss = -12401.230478382007
5
Iteration 13000: Loss = -12401.226586199036
Iteration 13100: Loss = -12401.226567249185
Iteration 13200: Loss = -12401.228332358354
1
Iteration 13300: Loss = -12401.226543002604
Iteration 13400: Loss = -12401.226561228523
Iteration 13500: Loss = -12401.226681041977
1
Iteration 13600: Loss = -12401.226546641194
Iteration 13700: Loss = -12401.231951027581
1
Iteration 13800: Loss = -12401.226628871289
Iteration 13900: Loss = -12401.226590975475
Iteration 14000: Loss = -12401.248536618468
1
Iteration 14100: Loss = -12401.22655159117
Iteration 14200: Loss = -12401.226553861292
Iteration 14300: Loss = -12401.231328323414
1
Iteration 14400: Loss = -12401.226516883911
Iteration 14500: Loss = -12401.226547900818
Iteration 14600: Loss = -12401.240474632887
1
Iteration 14700: Loss = -12401.22656054027
Iteration 14800: Loss = -12401.226548265291
Iteration 14900: Loss = -12401.226576152101
Iteration 15000: Loss = -12401.226546148828
Iteration 15100: Loss = -12401.226537056826
Iteration 15200: Loss = -12401.231085022871
1
Iteration 15300: Loss = -12401.226562485863
Iteration 15400: Loss = -12401.226565290688
Iteration 15500: Loss = -12401.22681586058
1
Iteration 15600: Loss = -12401.22654350711
Iteration 15700: Loss = -12401.22683632896
1
Iteration 15800: Loss = -12401.226609458607
Iteration 15900: Loss = -12401.22737693465
1
Iteration 16000: Loss = -12401.227091585934
2
Iteration 16100: Loss = -12401.226533904392
Iteration 16200: Loss = -12401.226785830695
1
Iteration 16300: Loss = -12401.226577160314
Iteration 16400: Loss = -12401.226560403062
Iteration 16500: Loss = -12401.239477165327
1
Iteration 16600: Loss = -12401.226664239171
2
Iteration 16700: Loss = -12401.234560278253
3
Iteration 16800: Loss = -12401.227162402773
4
Iteration 16900: Loss = -12401.22806820566
5
Iteration 17000: Loss = -12401.228710782958
6
Iteration 17100: Loss = -12401.296564718878
7
Iteration 17200: Loss = -12401.22680699048
8
Iteration 17300: Loss = -12401.22675870728
9
Iteration 17400: Loss = -12401.22674196399
10
Iteration 17500: Loss = -12401.23609581838
11
Iteration 17600: Loss = -12401.227387741063
12
Iteration 17700: Loss = -12401.529569076885
13
Iteration 17800: Loss = -12401.226586578712
Iteration 17900: Loss = -12401.291243961217
1
Iteration 18000: Loss = -12401.226838325663
2
Iteration 18100: Loss = -12401.231373279164
3
Iteration 18200: Loss = -12401.227235351149
4
Iteration 18300: Loss = -12401.227164494681
5
Iteration 18400: Loss = -12401.226837462875
6
Iteration 18500: Loss = -12401.226552176011
Iteration 18600: Loss = -12401.598205846587
1
Iteration 18700: Loss = -12401.226555007925
Iteration 18800: Loss = -12401.226786758303
1
Iteration 18900: Loss = -12401.226595106224
Iteration 19000: Loss = -12401.226599754546
Iteration 19100: Loss = -12401.226974994663
1
Iteration 19200: Loss = -12401.239475240007
2
Iteration 19300: Loss = -12401.226570711717
Iteration 19400: Loss = -12401.450859502547
1
Iteration 19500: Loss = -12401.226547239758
Iteration 19600: Loss = -12401.226553781966
Iteration 19700: Loss = -12401.226576684858
Iteration 19800: Loss = -12401.226522616724
Iteration 19900: Loss = -12401.22853092306
1
pi: tensor([[0.4049, 0.5951],
        [0.0081, 0.9919]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0541, 0.9459], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5748, 0.2870],
         [0.5376, 0.1967]],

        [[0.6835, 0.2736],
         [0.7082, 0.6022]],

        [[0.6083, 0.2990],
         [0.5010, 0.6451]],

        [[0.6708, 0.1683],
         [0.6848, 0.5013]],

        [[0.5854, 0.2534],
         [0.6570, 0.6249]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.027769702332048285
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: -0.007201490658206174
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: -0.007726325420627255
Global Adjusted Rand Index: -0.00957338213520448
Average Adjusted Rand Index: -0.010091624505261977
11918.286111040668
[-0.006823058974689618, -0.00957338213520448] [0.19770271204511292, -0.010091624505261977] [12317.032197894307, 12401.226554840452]
-------------------------------------
This iteration is 16
True Objective function: Loss = -12035.177712603168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22432.342142164995
Iteration 100: Loss = -12561.132652161961
Iteration 200: Loss = -12560.699563769609
Iteration 300: Loss = -12560.584370818344
Iteration 400: Loss = -12560.510411748612
Iteration 500: Loss = -12560.45228062716
Iteration 600: Loss = -12560.397161490051
Iteration 700: Loss = -12560.327704516245
Iteration 800: Loss = -12560.205646355816
Iteration 900: Loss = -12560.029445104703
Iteration 1000: Loss = -12559.864679865797
Iteration 1100: Loss = -12559.692362982327
Iteration 1200: Loss = -12559.559371257172
Iteration 1300: Loss = -12559.48919298734
Iteration 1400: Loss = -12559.445979095794
Iteration 1500: Loss = -12559.413375541482
Iteration 1600: Loss = -12559.384577951052
Iteration 1700: Loss = -12559.35547195632
Iteration 1800: Loss = -12559.322935645785
Iteration 1900: Loss = -12559.284706468765
Iteration 2000: Loss = -12559.239744983071
Iteration 2100: Loss = -12559.18944058703
Iteration 2200: Loss = -12559.13816656018
Iteration 2300: Loss = -12559.091029891482
Iteration 2400: Loss = -12559.051677749196
Iteration 2500: Loss = -12559.021619331701
Iteration 2600: Loss = -12558.998856175174
Iteration 2700: Loss = -12558.982595498946
Iteration 2800: Loss = -12558.971646783524
Iteration 2900: Loss = -12558.964597062577
Iteration 3000: Loss = -12558.960407173006
Iteration 3100: Loss = -12558.957975460455
Iteration 3200: Loss = -12558.956586008362
Iteration 3300: Loss = -12558.955782777053
Iteration 3400: Loss = -12558.955274863494
Iteration 3500: Loss = -12558.95503083954
Iteration 3600: Loss = -12558.954849263795
Iteration 3700: Loss = -12558.954707469764
Iteration 3800: Loss = -12558.954585508534
Iteration 3900: Loss = -12558.954499600257
Iteration 4000: Loss = -12558.954408732938
Iteration 4100: Loss = -12558.954244247445
Iteration 4200: Loss = -12558.957069201135
1
Iteration 4300: Loss = -12558.954013666922
Iteration 4400: Loss = -12558.953810952373
Iteration 4500: Loss = -12558.954593033655
1
Iteration 4600: Loss = -12558.953264632604
Iteration 4700: Loss = -12558.952765357859
Iteration 4800: Loss = -12558.952361201407
Iteration 4900: Loss = -12558.950097265302
Iteration 5000: Loss = -12558.957934972364
1
Iteration 5100: Loss = -12558.926269104017
Iteration 5200: Loss = -12201.172922585874
Iteration 5300: Loss = -12024.024286420983
Iteration 5400: Loss = -12023.935472936002
Iteration 5500: Loss = -12023.927293005094
Iteration 5600: Loss = -12023.923269027193
Iteration 5700: Loss = -12023.920473116867
Iteration 5800: Loss = -12023.918405728917
Iteration 5900: Loss = -12023.916361285528
Iteration 6000: Loss = -12023.914468354826
Iteration 6100: Loss = -12023.913052302638
Iteration 6200: Loss = -12023.911470903568
Iteration 6300: Loss = -12023.908237162947
Iteration 6400: Loss = -12023.904043931887
Iteration 6500: Loss = -12023.901298255145
Iteration 6600: Loss = -12023.896910777743
Iteration 6700: Loss = -12023.895722797697
Iteration 6800: Loss = -12023.895631719737
Iteration 6900: Loss = -12023.894608889721
Iteration 7000: Loss = -12023.893264425908
Iteration 7100: Loss = -12023.892972202439
Iteration 7200: Loss = -12023.893324672556
1
Iteration 7300: Loss = -12023.892539612554
Iteration 7400: Loss = -12023.89396723332
1
Iteration 7500: Loss = -12023.892180651826
Iteration 7600: Loss = -12023.891998092433
Iteration 7700: Loss = -12023.891931195998
Iteration 7800: Loss = -12023.891679410328
Iteration 7900: Loss = -12023.891384794166
Iteration 8000: Loss = -12023.89068806811
Iteration 8100: Loss = -12023.872613086516
Iteration 8200: Loss = -12023.878126997683
1
Iteration 8300: Loss = -12023.871969925867
Iteration 8400: Loss = -12023.982543294602
1
Iteration 8500: Loss = -12023.87151542713
Iteration 8600: Loss = -12023.874578051213
1
Iteration 8700: Loss = -12023.875055272803
2
Iteration 8800: Loss = -12023.8722908984
3
Iteration 8900: Loss = -12023.874649090145
4
Iteration 9000: Loss = -12023.871062765476
Iteration 9100: Loss = -12023.870897435061
Iteration 9200: Loss = -12023.870171260936
Iteration 9300: Loss = -12023.869358958884
Iteration 9400: Loss = -12023.870135578321
1
Iteration 9500: Loss = -12023.874436914019
2
Iteration 9600: Loss = -12023.881810061619
3
Iteration 9700: Loss = -12023.87176332334
4
Iteration 9800: Loss = -12023.874764498789
5
Iteration 9900: Loss = -12023.878208911052
6
Iteration 10000: Loss = -12023.867668855915
Iteration 10100: Loss = -12023.872213675902
1
Iteration 10200: Loss = -12023.870707915132
2
Iteration 10300: Loss = -12023.933898938518
3
Iteration 10400: Loss = -12024.097724468984
4
Iteration 10500: Loss = -12023.888980494969
5
Iteration 10600: Loss = -12023.89878499619
6
Iteration 10700: Loss = -12023.88623955364
7
Iteration 10800: Loss = -12023.962348290868
8
Iteration 10900: Loss = -12023.860889669644
Iteration 11000: Loss = -12023.86066255211
Iteration 11100: Loss = -12023.861347439124
1
Iteration 11200: Loss = -12023.861320403965
2
Iteration 11300: Loss = -12023.861059190149
3
Iteration 11400: Loss = -12023.863083109138
4
Iteration 11500: Loss = -12024.053514511412
5
Iteration 11600: Loss = -12023.86846892141
6
Iteration 11700: Loss = -12023.860639664297
Iteration 11800: Loss = -12023.868325001753
1
Iteration 11900: Loss = -12023.873088501081
2
Iteration 12000: Loss = -12023.859656632898
Iteration 12100: Loss = -12023.859956742896
1
Iteration 12200: Loss = -12023.863610143577
2
Iteration 12300: Loss = -12023.87099259094
3
Iteration 12400: Loss = -12023.85949858611
Iteration 12500: Loss = -12023.862368110818
1
Iteration 12600: Loss = -12023.859879809428
2
Iteration 12700: Loss = -12023.867568773905
3
Iteration 12800: Loss = -12023.863897493833
4
Iteration 12900: Loss = -12023.859675078376
5
Iteration 13000: Loss = -12023.888700049494
6
Iteration 13100: Loss = -12023.863429758152
7
Iteration 13200: Loss = -12023.871571249107
8
Iteration 13300: Loss = -12023.863349170528
9
Iteration 13400: Loss = -12023.861331885591
10
Iteration 13500: Loss = -12023.863308700129
11
Iteration 13600: Loss = -12023.859891544778
12
Iteration 13700: Loss = -12023.86242904656
13
Iteration 13800: Loss = -12023.864440994479
14
Iteration 13900: Loss = -12023.85903077445
Iteration 14000: Loss = -12023.859078629794
Iteration 14100: Loss = -12023.863228940163
1
Iteration 14200: Loss = -12023.93248672061
2
Iteration 14300: Loss = -12023.878810772803
3
Iteration 14400: Loss = -12023.882688255657
4
Iteration 14500: Loss = -12023.929666583344
5
Iteration 14600: Loss = -12023.896777439255
6
Iteration 14700: Loss = -12023.892604614954
7
Iteration 14800: Loss = -12023.951756887805
8
Iteration 14900: Loss = -12023.859191475749
9
Iteration 15000: Loss = -12023.86731516713
10
Iteration 15100: Loss = -12023.900479684298
11
Iteration 15200: Loss = -12023.859959177811
12
Iteration 15300: Loss = -12023.860292314439
13
Iteration 15400: Loss = -12023.862800249392
14
Iteration 15500: Loss = -12023.91606229731
15
Stopping early at iteration 15500 due to no improvement.
pi: tensor([[0.8078, 0.1922],
        [0.2521, 0.7479]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5174, 0.4826], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3081, 0.0996],
         [0.5588, 0.3057]],

        [[0.7051, 0.1061],
         [0.5540, 0.5982]],

        [[0.6347, 0.1047],
         [0.6195, 0.6356]],

        [[0.6710, 0.1047],
         [0.6737, 0.6876]],

        [[0.6755, 0.1030],
         [0.6672, 0.7083]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840308245660749
Average Adjusted Rand Index: 0.9839992163675584
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20614.287328929862
Iteration 100: Loss = -12560.912409909513
Iteration 200: Loss = -12560.419640862925
Iteration 300: Loss = -12560.30700410375
Iteration 400: Loss = -12560.25526009525
Iteration 500: Loss = -12560.203032448902
Iteration 600: Loss = -12560.122194161682
Iteration 700: Loss = -12559.988836722803
Iteration 800: Loss = -12559.824974680829
Iteration 900: Loss = -12559.691203687215
Iteration 1000: Loss = -12559.612740098399
Iteration 1100: Loss = -12559.565741216842
Iteration 1200: Loss = -12559.531618136822
Iteration 1300: Loss = -12559.504210864954
Iteration 1400: Loss = -12559.480898792808
Iteration 1500: Loss = -12559.459834424242
Iteration 1600: Loss = -12559.439459913949
Iteration 1700: Loss = -12559.418404282793
Iteration 1800: Loss = -12559.395208434466
Iteration 1900: Loss = -12559.368381508255
Iteration 2000: Loss = -12559.336046880731
Iteration 2100: Loss = -12559.296835988878
Iteration 2200: Loss = -12559.25077345378
Iteration 2300: Loss = -12559.200638402423
Iteration 2400: Loss = -12559.151841678406
Iteration 2500: Loss = -12559.108786804834
Iteration 2600: Loss = -12559.073937081417
Iteration 2700: Loss = -12559.047165970913
Iteration 2800: Loss = -12559.026936783657
Iteration 2900: Loss = -12559.010256768142
Iteration 3000: Loss = -12558.997537287476
Iteration 3100: Loss = -12558.988264271251
Iteration 3200: Loss = -12558.980776696542
Iteration 3300: Loss = -12558.974872711855
Iteration 3400: Loss = -12558.970626991819
Iteration 3500: Loss = -12558.967315275346
Iteration 3600: Loss = -12558.965005589787
Iteration 3700: Loss = -12558.96341546644
Iteration 3800: Loss = -12558.962487648261
Iteration 3900: Loss = -12558.961794163615
Iteration 4000: Loss = -12558.96133753779
Iteration 4100: Loss = -12558.96101505724
Iteration 4200: Loss = -12558.96070036788
Iteration 4300: Loss = -12558.96044743143
Iteration 4400: Loss = -12558.960197367629
Iteration 4500: Loss = -12558.959988394607
Iteration 4600: Loss = -12558.970761239001
1
Iteration 4700: Loss = -12558.959510163922
Iteration 4800: Loss = -12558.95930553567
Iteration 4900: Loss = -12558.959039423542
Iteration 5000: Loss = -12558.958730269545
Iteration 5100: Loss = -12558.958441824847
Iteration 5200: Loss = -12558.958036240938
Iteration 5300: Loss = -12558.957674605383
Iteration 5400: Loss = -12558.957189892664
Iteration 5500: Loss = -12558.956705398115
Iteration 5600: Loss = -12558.956428650261
Iteration 5700: Loss = -12558.955993130903
Iteration 5800: Loss = -12558.955764663522
Iteration 5900: Loss = -12558.955533204253
Iteration 6000: Loss = -12558.959150785457
1
Iteration 6100: Loss = -12558.955166028101
Iteration 6200: Loss = -12558.954981768073
Iteration 6300: Loss = -12558.95495371421
Iteration 6400: Loss = -12558.954393437709
Iteration 6500: Loss = -12558.953771195327
Iteration 6600: Loss = -12558.951723465025
Iteration 6700: Loss = -12558.910898872802
Iteration 6800: Loss = -12024.035191766166
Iteration 6900: Loss = -12023.939187396707
Iteration 7000: Loss = -12023.931404646228
Iteration 7100: Loss = -12023.929722408184
Iteration 7200: Loss = -12023.928329263677
Iteration 7300: Loss = -12023.927625739669
Iteration 7400: Loss = -12023.926841764727
Iteration 7500: Loss = -12023.923819010277
Iteration 7600: Loss = -12023.92296186473
Iteration 7700: Loss = -12023.922068555152
Iteration 7800: Loss = -12023.916667228745
Iteration 7900: Loss = -12023.912223381269
Iteration 8000: Loss = -12023.911436705628
Iteration 8100: Loss = -12023.907173925376
Iteration 8200: Loss = -12023.906914552792
Iteration 8300: Loss = -12023.906327847135
Iteration 8400: Loss = -12023.905990019339
Iteration 8500: Loss = -12023.91128637054
1
Iteration 8600: Loss = -12023.908081026266
2
Iteration 8700: Loss = -12023.905119222465
Iteration 8800: Loss = -12023.905428772698
1
Iteration 8900: Loss = -12023.904151112103
Iteration 9000: Loss = -12023.89851004917
Iteration 9100: Loss = -12023.898921347118
1
Iteration 9200: Loss = -12023.978419623405
2
Iteration 9300: Loss = -12023.897717923233
Iteration 9400: Loss = -12023.894153113693
Iteration 9500: Loss = -12023.896173609068
1
Iteration 9600: Loss = -12023.878264152718
Iteration 9700: Loss = -12023.87825936024
Iteration 9800: Loss = -12023.889947975671
1
Iteration 9900: Loss = -12023.887578449423
2
Iteration 10000: Loss = -12023.87908459286
3
Iteration 10100: Loss = -12023.877984890549
Iteration 10200: Loss = -12023.880736383948
1
Iteration 10300: Loss = -12023.881168889156
2
Iteration 10400: Loss = -12023.878904109319
3
Iteration 10500: Loss = -12023.90450689697
4
Iteration 10600: Loss = -12023.88060247464
5
Iteration 10700: Loss = -12023.876605429967
Iteration 10800: Loss = -12023.882517738743
1
Iteration 10900: Loss = -12023.924749935715
2
Iteration 11000: Loss = -12023.87471817528
Iteration 11100: Loss = -12023.874102067959
Iteration 11200: Loss = -12023.874846332063
1
Iteration 11300: Loss = -12023.908337116152
2
Iteration 11400: Loss = -12023.880361097363
3
Iteration 11500: Loss = -12023.93192656149
4
Iteration 11600: Loss = -12023.890548769941
5
Iteration 11700: Loss = -12023.875592070604
6
Iteration 11800: Loss = -12023.876440192726
7
Iteration 11900: Loss = -12023.875141605133
8
Iteration 12000: Loss = -12023.88721105394
9
Iteration 12100: Loss = -12023.874178983602
Iteration 12200: Loss = -12023.87470405162
1
Iteration 12300: Loss = -12023.87795108598
2
Iteration 12400: Loss = -12023.872963646074
Iteration 12500: Loss = -12023.870926265485
Iteration 12600: Loss = -12023.870428686017
Iteration 12700: Loss = -12023.890130596845
1
Iteration 12800: Loss = -12023.863537170933
Iteration 12900: Loss = -12023.864573729721
1
Iteration 13000: Loss = -12023.866742324015
2
Iteration 13100: Loss = -12023.879010285767
3
Iteration 13200: Loss = -12023.87007859833
4
Iteration 13300: Loss = -12023.867626130233
5
Iteration 13400: Loss = -12023.860809933025
Iteration 13500: Loss = -12023.865912765561
1
Iteration 13600: Loss = -12023.866762361384
2
Iteration 13700: Loss = -12023.902040170877
3
Iteration 13800: Loss = -12023.86801411829
4
Iteration 13900: Loss = -12023.859794405893
Iteration 14000: Loss = -12023.860408056293
1
Iteration 14100: Loss = -12023.873928408308
2
Iteration 14200: Loss = -12023.877472874066
3
Iteration 14300: Loss = -12023.864975100634
4
Iteration 14400: Loss = -12023.884533498374
5
Iteration 14500: Loss = -12023.87118173254
6
Iteration 14600: Loss = -12023.861428264803
7
Iteration 14700: Loss = -12023.863010104602
8
Iteration 14800: Loss = -12023.862523822469
9
Iteration 14900: Loss = -12023.860386572722
10
Iteration 15000: Loss = -12023.869959668664
11
Iteration 15100: Loss = -12023.862686670822
12
Iteration 15200: Loss = -12023.85978623427
Iteration 15300: Loss = -12023.871543030038
1
Iteration 15400: Loss = -12023.86446253349
2
Iteration 15500: Loss = -12023.885328354048
3
Iteration 15600: Loss = -12023.93188103376
4
Iteration 15700: Loss = -12023.889257910832
5
Iteration 15800: Loss = -12023.882450351837
6
Iteration 15900: Loss = -12023.862820278184
7
Iteration 16000: Loss = -12023.860183231178
8
Iteration 16100: Loss = -12023.861307534455
9
Iteration 16200: Loss = -12023.862755701955
10
Iteration 16300: Loss = -12023.88897164285
11
Iteration 16400: Loss = -12023.860260440457
12
Iteration 16500: Loss = -12023.8602947034
13
Iteration 16600: Loss = -12023.910158272427
14
Iteration 16700: Loss = -12023.866570293963
15
Stopping early at iteration 16700 due to no improvement.
pi: tensor([[0.8091, 0.1909],
        [0.2519, 0.7481]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5191, 0.4809], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3072, 0.0997],
         [0.6735, 0.3060]],

        [[0.7136, 0.1059],
         [0.5199, 0.5934]],

        [[0.6542, 0.1048],
         [0.7295, 0.6082]],

        [[0.6449, 0.1047],
         [0.5100, 0.5926]],

        [[0.6812, 0.1030],
         [0.5272, 0.5309]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840308245660749
Average Adjusted Rand Index: 0.9839992163675584
12035.177712603168
[0.9840308245660749, 0.9840308245660749] [0.9839992163675584, 0.9839992163675584] [12023.91606229731, 12023.866570293963]
-------------------------------------
This iteration is 17
True Objective function: Loss = -11851.300780002
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21660.607680480894
Iteration 100: Loss = -12349.54737135724
Iteration 200: Loss = -12349.06267670868
Iteration 300: Loss = -12348.968477210288
Iteration 400: Loss = -12348.92090209605
Iteration 500: Loss = -12348.888088442585
Iteration 600: Loss = -12348.861529513022
Iteration 700: Loss = -12348.838711684588
Iteration 800: Loss = -12348.81872806386
Iteration 900: Loss = -12348.800713058978
Iteration 1000: Loss = -12348.78425870886
Iteration 1100: Loss = -12348.7690816287
Iteration 1200: Loss = -12348.7547748625
Iteration 1300: Loss = -12348.741257069976
Iteration 1400: Loss = -12348.728132685272
Iteration 1500: Loss = -12348.715343485599
Iteration 1600: Loss = -12348.702539040074
Iteration 1700: Loss = -12348.689574563015
Iteration 1800: Loss = -12348.676290804086
Iteration 1900: Loss = -12348.662381849128
Iteration 2000: Loss = -12348.647716003
Iteration 2100: Loss = -12348.632117721721
Iteration 2200: Loss = -12348.615741568237
Iteration 2300: Loss = -12348.598681292018
Iteration 2400: Loss = -12348.581240193416
Iteration 2500: Loss = -12348.563984132816
Iteration 2600: Loss = -12348.54731988171
Iteration 2700: Loss = -12348.531763956098
Iteration 2800: Loss = -12348.517668762666
Iteration 2900: Loss = -12348.505103916741
Iteration 3000: Loss = -12348.49420120807
Iteration 3100: Loss = -12348.484801338573
Iteration 3200: Loss = -12348.47685017803
Iteration 3300: Loss = -12348.470306830106
Iteration 3400: Loss = -12348.46491463421
Iteration 3500: Loss = -12348.460467283932
Iteration 3600: Loss = -12348.456825610885
Iteration 3700: Loss = -12348.453824544751
Iteration 3800: Loss = -12348.451354602164
Iteration 3900: Loss = -12348.449433229545
Iteration 4000: Loss = -12348.447665546175
Iteration 4100: Loss = -12348.446165137944
Iteration 4200: Loss = -12348.444980685652
Iteration 4300: Loss = -12348.44387542131
Iteration 4400: Loss = -12348.442908248682
Iteration 4500: Loss = -12348.442087762942
Iteration 4600: Loss = -12348.441234603106
Iteration 4700: Loss = -12348.441535087215
1
Iteration 4800: Loss = -12348.439918117241
Iteration 4900: Loss = -12348.439204830986
Iteration 5000: Loss = -12348.438784796997
Iteration 5100: Loss = -12348.438107204296
Iteration 5200: Loss = -12348.461798361808
1
Iteration 5300: Loss = -12348.437119043887
Iteration 5400: Loss = -12348.436501588736
Iteration 5500: Loss = -12348.43607463171
Iteration 5600: Loss = -12348.435437885557
Iteration 5700: Loss = -12348.435907126823
1
Iteration 5800: Loss = -12348.434167346939
Iteration 5900: Loss = -12348.433320227372
Iteration 6000: Loss = -12348.432676999946
Iteration 6100: Loss = -12348.431462815677
Iteration 6200: Loss = -12348.486987396183
1
Iteration 6300: Loss = -12348.427896304998
Iteration 6400: Loss = -12348.439213852662
1
Iteration 6500: Loss = -12348.418686239927
Iteration 6600: Loss = -12348.438390895182
1
Iteration 6700: Loss = -12348.370905670023
Iteration 6800: Loss = -12348.284959427981
Iteration 6900: Loss = -12348.218194667661
Iteration 7000: Loss = -12348.191479550675
Iteration 7100: Loss = -12348.168395609384
Iteration 7200: Loss = -12348.102592025272
Iteration 7300: Loss = -12348.00092859593
Iteration 7400: Loss = -12347.957576089744
Iteration 7500: Loss = -12347.905908090268
Iteration 7600: Loss = -12347.142379080227
Iteration 7700: Loss = -12346.896231765195
Iteration 7800: Loss = -12346.872635982507
Iteration 7900: Loss = -12346.994996640959
1
Iteration 8000: Loss = -12346.856945011668
Iteration 8100: Loss = -12346.853289651865
Iteration 8200: Loss = -12347.038549847355
1
Iteration 8300: Loss = -12346.849018697712
Iteration 8400: Loss = -12346.84763326621
Iteration 8500: Loss = -12346.856608063861
1
Iteration 8600: Loss = -12346.845406158052
Iteration 8700: Loss = -12346.849474922765
1
Iteration 8800: Loss = -12346.844017887732
Iteration 8900: Loss = -12346.849417828373
1
Iteration 9000: Loss = -12346.842957679175
Iteration 9100: Loss = -12346.84818378374
1
Iteration 9200: Loss = -12346.842335827485
Iteration 9300: Loss = -12346.90388063819
1
Iteration 9400: Loss = -12346.841534665544
Iteration 9500: Loss = -12346.848920715445
1
Iteration 9600: Loss = -12346.895345448651
2
Iteration 9700: Loss = -12346.840775198532
Iteration 9800: Loss = -12346.841049533787
1
Iteration 9900: Loss = -12346.84040690177
Iteration 10000: Loss = -12346.840839122198
1
Iteration 10100: Loss = -12346.840084561076
Iteration 10200: Loss = -12346.847996195285
1
Iteration 10300: Loss = -12346.839802011684
Iteration 10400: Loss = -12346.840445212067
1
Iteration 10500: Loss = -12346.839580546059
Iteration 10600: Loss = -12346.839486205401
Iteration 10700: Loss = -12347.019432747209
1
Iteration 10800: Loss = -12346.839296798786
Iteration 10900: Loss = -12346.839245558893
Iteration 11000: Loss = -12346.997328742667
1
Iteration 11100: Loss = -12346.83907274624
Iteration 11200: Loss = -12346.839027492468
Iteration 11300: Loss = -12346.8426741627
1
Iteration 11400: Loss = -12346.839305588843
2
Iteration 11500: Loss = -12346.852208950888
3
Iteration 11600: Loss = -12346.839414660446
4
Iteration 11700: Loss = -12346.863317626987
5
Iteration 11800: Loss = -12346.840200140672
6
Iteration 11900: Loss = -12346.85238914548
7
Iteration 12000: Loss = -12346.83867507331
Iteration 12100: Loss = -12346.83877119371
Iteration 12200: Loss = -12346.83852798449
Iteration 12300: Loss = -12346.838466194973
Iteration 12400: Loss = -12346.840106683927
1
Iteration 12500: Loss = -12346.838458915907
Iteration 12600: Loss = -12346.838386297617
Iteration 12700: Loss = -12346.839224998335
1
Iteration 12800: Loss = -12346.838328144897
Iteration 12900: Loss = -12346.848685888095
1
Iteration 13000: Loss = -12346.838283096356
Iteration 13100: Loss = -12346.83827837862
Iteration 13200: Loss = -12346.839912463935
1
Iteration 13300: Loss = -12346.845959549373
2
Iteration 13400: Loss = -12346.946316717824
3
Iteration 13500: Loss = -12346.83839498771
4
Iteration 13600: Loss = -12346.838436435908
5
Iteration 13700: Loss = -12346.838261223837
Iteration 13800: Loss = -12346.839298674691
1
Iteration 13900: Loss = -12346.861113754267
2
Iteration 14000: Loss = -12346.838156602551
Iteration 14100: Loss = -12346.838069939542
Iteration 14200: Loss = -12346.838238854678
1
Iteration 14300: Loss = -12346.949284474467
2
Iteration 14400: Loss = -12346.838621367722
3
Iteration 14500: Loss = -12346.88739342982
4
Iteration 14600: Loss = -12346.838093417215
Iteration 14700: Loss = -12346.857952716995
1
Iteration 14800: Loss = -12346.87554067476
2
Iteration 14900: Loss = -12346.954968766426
3
Iteration 15000: Loss = -12346.83806355674
Iteration 15100: Loss = -12346.839556824785
1
Iteration 15200: Loss = -12346.898503427026
2
Iteration 15300: Loss = -12346.839575833592
3
Iteration 15400: Loss = -12346.838073251118
Iteration 15500: Loss = -12346.925735499246
1
Iteration 15600: Loss = -12346.838516135702
2
Iteration 15700: Loss = -12346.846151287908
3
Iteration 15800: Loss = -12346.838104977658
Iteration 15900: Loss = -12346.838952575257
1
Iteration 16000: Loss = -12346.841429006306
2
Iteration 16100: Loss = -12346.838024833678
Iteration 16200: Loss = -12346.838468682216
1
Iteration 16300: Loss = -12346.852577861242
2
Iteration 16400: Loss = -12346.837943566852
Iteration 16500: Loss = -12346.838105197627
1
Iteration 16600: Loss = -12346.84494839361
2
Iteration 16700: Loss = -12346.837972105508
Iteration 16800: Loss = -12346.847965052164
1
Iteration 16900: Loss = -12346.837889755325
Iteration 17000: Loss = -12346.84044847256
1
Iteration 17100: Loss = -12346.83788128686
Iteration 17200: Loss = -12346.838708349595
1
Iteration 17300: Loss = -12346.83787651516
Iteration 17400: Loss = -12346.862759658343
1
Iteration 17500: Loss = -12346.841224050735
2
Iteration 17600: Loss = -12346.845549549846
3
Iteration 17700: Loss = -12346.838472335572
4
Iteration 17800: Loss = -12346.854788316898
5
Iteration 17900: Loss = -12346.853606695337
6
Iteration 18000: Loss = -12346.889297424403
7
Iteration 18100: Loss = -12346.839895913494
8
Iteration 18200: Loss = -12346.839322012958
9
Iteration 18300: Loss = -12346.85630556943
10
Iteration 18400: Loss = -12346.838927820401
11
Iteration 18500: Loss = -12346.838707952724
12
Iteration 18600: Loss = -12346.837938311619
Iteration 18700: Loss = -12346.837951391552
Iteration 18800: Loss = -12346.840978737575
1
Iteration 18900: Loss = -12346.842685459042
2
Iteration 19000: Loss = -12346.838058587718
3
Iteration 19100: Loss = -12346.839968825712
4
Iteration 19200: Loss = -12346.83867905279
5
Iteration 19300: Loss = -12346.83891708515
6
Iteration 19400: Loss = -12346.839621281071
7
Iteration 19500: Loss = -12346.838480877981
8
Iteration 19600: Loss = -12346.85054614931
9
Iteration 19700: Loss = -12346.843796242378
10
Iteration 19800: Loss = -12346.838509514144
11
Iteration 19900: Loss = -12346.84170943663
12
pi: tensor([[1.3521e-07, 1.0000e+00],
        [1.0000e+00, 1.3827e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0286, 0.9714], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2015, 0.2430],
         [0.6563, 0.1963]],

        [[0.7237, 0.2800],
         [0.5630, 0.5162]],

        [[0.6827, 0.1900],
         [0.6684, 0.6162]],

        [[0.6096, 0.1556],
         [0.6610, 0.6209]],

        [[0.6674, 0.1740],
         [0.6374, 0.6495]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
Global Adjusted Rand Index: -0.001801229030249589
Average Adjusted Rand Index: -0.00039960749005293867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21652.80500395285
Iteration 100: Loss = -12349.243475728874
Iteration 200: Loss = -12348.702658350572
Iteration 300: Loss = -12348.446805462441
Iteration 400: Loss = -12348.143422663075
Iteration 500: Loss = -12347.464611543273
Iteration 600: Loss = -12346.954751229343
Iteration 700: Loss = -12346.703579761179
Iteration 800: Loss = -12346.558041744956
Iteration 900: Loss = -12346.460010754989
Iteration 1000: Loss = -12346.393775491804
Iteration 1100: Loss = -12346.34813628197
Iteration 1200: Loss = -12346.313980417317
Iteration 1300: Loss = -12346.286983568441
Iteration 1400: Loss = -12346.26501486419
Iteration 1500: Loss = -12346.246944814617
Iteration 1600: Loss = -12346.231883454306
Iteration 1700: Loss = -12346.219143615199
Iteration 1800: Loss = -12346.208050759695
Iteration 1900: Loss = -12346.198329983024
Iteration 2000: Loss = -12346.189727098506
Iteration 2100: Loss = -12346.182028516247
Iteration 2200: Loss = -12346.175106247081
Iteration 2300: Loss = -12346.168621165569
Iteration 2400: Loss = -12346.162720305876
Iteration 2500: Loss = -12346.157407836401
Iteration 2600: Loss = -12346.152543947854
Iteration 2700: Loss = -12346.148122284287
Iteration 2800: Loss = -12346.144066712375
Iteration 2900: Loss = -12346.140227634918
Iteration 3000: Loss = -12346.136679131401
Iteration 3100: Loss = -12346.133451198537
Iteration 3200: Loss = -12346.130388170104
Iteration 3300: Loss = -12346.127636471947
Iteration 3400: Loss = -12346.125023037415
Iteration 3500: Loss = -12346.122642729148
Iteration 3600: Loss = -12346.120459936024
Iteration 3700: Loss = -12346.11845642786
Iteration 3800: Loss = -12346.116569414999
Iteration 3900: Loss = -12346.114796023414
Iteration 4000: Loss = -12346.11311677718
Iteration 4100: Loss = -12346.111451851775
Iteration 4200: Loss = -12346.109828492978
Iteration 4300: Loss = -12346.108101278574
Iteration 4400: Loss = -12346.106548535243
Iteration 4500: Loss = -12346.10534385633
Iteration 4600: Loss = -12346.104354905452
Iteration 4700: Loss = -12346.103452569425
Iteration 4800: Loss = -12346.102681807715
Iteration 4900: Loss = -12346.101926487607
Iteration 5000: Loss = -12346.101192752249
Iteration 5100: Loss = -12346.100558193086
Iteration 5200: Loss = -12346.099972342832
Iteration 5300: Loss = -12346.099345243216
Iteration 5400: Loss = -12346.098850523276
Iteration 5500: Loss = -12346.09834227473
Iteration 5600: Loss = -12346.097902605934
Iteration 5700: Loss = -12346.097466052692
Iteration 5800: Loss = -12346.097039133283
Iteration 5900: Loss = -12346.096680028577
Iteration 6000: Loss = -12346.096341572107
Iteration 6100: Loss = -12346.096029593902
Iteration 6200: Loss = -12346.09568850732
Iteration 6300: Loss = -12346.095428820723
Iteration 6400: Loss = -12346.095166769155
Iteration 6500: Loss = -12346.09492636149
Iteration 6600: Loss = -12346.094662758369
Iteration 6700: Loss = -12346.094466850023
Iteration 6800: Loss = -12346.094225276769
Iteration 6900: Loss = -12346.094087633066
Iteration 7000: Loss = -12346.093823995803
Iteration 7100: Loss = -12346.09364244519
Iteration 7200: Loss = -12346.093497685526
Iteration 7300: Loss = -12346.093291304524
Iteration 7400: Loss = -12346.093129460607
Iteration 7500: Loss = -12346.093026559665
Iteration 7600: Loss = -12346.116589952844
1
Iteration 7700: Loss = -12346.092718406533
Iteration 7800: Loss = -12346.092799408836
Iteration 7900: Loss = -12346.092637599791
Iteration 8000: Loss = -12346.092326646665
Iteration 8100: Loss = -12346.092203273613
Iteration 8200: Loss = -12346.093448012422
1
Iteration 8300: Loss = -12346.091982776808
Iteration 8400: Loss = -12346.09189132813
Iteration 8500: Loss = -12346.091916478023
Iteration 8600: Loss = -12346.091696802376
Iteration 8700: Loss = -12346.091639305936
Iteration 8800: Loss = -12346.091875123575
1
Iteration 8900: Loss = -12346.091488699229
Iteration 9000: Loss = -12346.091417809761
Iteration 9100: Loss = -12346.106446027756
1
Iteration 9200: Loss = -12346.091276693973
Iteration 9300: Loss = -12346.091274865692
Iteration 9400: Loss = -12346.102044408823
1
Iteration 9500: Loss = -12346.091164394958
Iteration 9600: Loss = -12346.091109831874
Iteration 9700: Loss = -12346.091068159765
Iteration 9800: Loss = -12346.091965658256
1
Iteration 9900: Loss = -12346.09097975651
Iteration 10000: Loss = -12346.090938028807
Iteration 10100: Loss = -12346.09087536551
Iteration 10200: Loss = -12346.093068636215
1
Iteration 10300: Loss = -12346.090854770096
Iteration 10400: Loss = -12346.090818247798
Iteration 10500: Loss = -12346.09127920715
1
Iteration 10600: Loss = -12346.090875085163
Iteration 10700: Loss = -12346.090742294917
Iteration 10800: Loss = -12346.090717283023
Iteration 10900: Loss = -12346.090701819421
Iteration 11000: Loss = -12346.090860750877
1
Iteration 11100: Loss = -12346.090647706202
Iteration 11200: Loss = -12346.090624740214
Iteration 11300: Loss = -12346.092701280215
1
Iteration 11400: Loss = -12346.090600413552
Iteration 11500: Loss = -12346.090583118663
Iteration 11600: Loss = -12346.09052168665
Iteration 11700: Loss = -12346.091859786144
1
Iteration 11800: Loss = -12346.090538908617
Iteration 11900: Loss = -12346.090531745742
Iteration 12000: Loss = -12346.483625250581
1
Iteration 12100: Loss = -12346.090491806268
Iteration 12200: Loss = -12346.09048882458
Iteration 12300: Loss = -12346.090466152726
Iteration 12400: Loss = -12346.090509176309
Iteration 12500: Loss = -12346.090432100355
Iteration 12600: Loss = -12346.090419240756
Iteration 12700: Loss = -12346.095841516093
1
Iteration 12800: Loss = -12346.090410960554
Iteration 12900: Loss = -12346.090378934965
Iteration 13000: Loss = -12346.1012425498
1
Iteration 13100: Loss = -12346.09041170148
Iteration 13200: Loss = -12346.090592327446
1
Iteration 13300: Loss = -12346.090351761099
Iteration 13400: Loss = -12346.090590822203
1
Iteration 13500: Loss = -12346.090302560731
Iteration 13600: Loss = -12346.09184445438
1
Iteration 13700: Loss = -12346.126364961543
2
Iteration 13800: Loss = -12346.09028339893
Iteration 13900: Loss = -12346.096286009219
1
Iteration 14000: Loss = -12346.090280213126
Iteration 14100: Loss = -12346.102393349316
1
Iteration 14200: Loss = -12346.090259514327
Iteration 14300: Loss = -12346.215134182312
1
Iteration 14400: Loss = -12346.090258008247
Iteration 14500: Loss = -12346.090256626496
Iteration 14600: Loss = -12346.09028803516
Iteration 14700: Loss = -12346.090765445173
1
Iteration 14800: Loss = -12346.090227031476
Iteration 14900: Loss = -12346.090388185978
1
Iteration 15000: Loss = -12346.090297425597
Iteration 15100: Loss = -12346.090244736348
Iteration 15200: Loss = -12346.090686545445
1
Iteration 15300: Loss = -12346.090291319833
Iteration 15400: Loss = -12346.109325319441
1
Iteration 15500: Loss = -12346.090254004868
Iteration 15600: Loss = -12346.098986625515
1
Iteration 15700: Loss = -12346.090242239023
Iteration 15800: Loss = -12346.090993700991
1
Iteration 15900: Loss = -12346.090264910865
Iteration 16000: Loss = -12346.100273352306
1
Iteration 16100: Loss = -12346.09022943056
Iteration 16200: Loss = -12346.125590508022
1
Iteration 16300: Loss = -12346.090230163156
Iteration 16400: Loss = -12346.094688453331
1
Iteration 16500: Loss = -12346.090201471587
Iteration 16600: Loss = -12346.09022674531
Iteration 16700: Loss = -12346.103976711489
1
Iteration 16800: Loss = -12346.090244003259
Iteration 16900: Loss = -12346.090193801425
Iteration 17000: Loss = -12346.091481366515
1
Iteration 17100: Loss = -12346.090196619134
Iteration 17200: Loss = -12346.090229762245
Iteration 17300: Loss = -12346.092508638767
1
Iteration 17400: Loss = -12346.090194933926
Iteration 17500: Loss = -12346.09031908803
1
Iteration 17600: Loss = -12346.090568633754
2
Iteration 17700: Loss = -12346.124023353386
3
Iteration 17800: Loss = -12346.090212382782
Iteration 17900: Loss = -12346.090792822932
1
Iteration 18000: Loss = -12346.090202026053
Iteration 18100: Loss = -12346.090355400049
1
Iteration 18200: Loss = -12346.090169319674
Iteration 18300: Loss = -12346.091064148772
1
Iteration 18400: Loss = -12346.090214163407
Iteration 18500: Loss = -12346.090251047923
Iteration 18600: Loss = -12346.090218101077
Iteration 18700: Loss = -12346.149635431737
1
Iteration 18800: Loss = -12346.090180741403
Iteration 18900: Loss = -12346.090188268894
Iteration 19000: Loss = -12346.100981898997
1
Iteration 19100: Loss = -12346.090202114865
Iteration 19200: Loss = -12346.090206844883
Iteration 19300: Loss = -12346.150694909738
1
Iteration 19400: Loss = -12346.090191216912
Iteration 19500: Loss = -12346.090205877526
Iteration 19600: Loss = -12346.389064170038
1
Iteration 19700: Loss = -12346.090192803946
Iteration 19800: Loss = -12346.0901770839
Iteration 19900: Loss = -12346.090325942509
1
pi: tensor([[1.0000e+00, 6.8006e-08],
        [1.3038e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9790, 0.0210], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1973, 0.1851],
         [0.5129, 0.2146]],

        [[0.7261, 0.2182],
         [0.6754, 0.5802]],

        [[0.5219, 0.3152],
         [0.5743, 0.6331]],

        [[0.6398, 0.2639],
         [0.6866, 0.5977]],

        [[0.5747, 0.2102],
         [0.6530, 0.5244]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -0.0001370574712950435
Average Adjusted Rand Index: -0.0004350326971138495
11851.300780002
[-0.001801229030249589, -0.0001370574712950435] [-0.00039960749005293867, -0.0004350326971138495] [12346.837859421583, 12346.090449029401]
-------------------------------------
This iteration is 18
True Objective function: Loss = -11921.762488826182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24513.86477268876
Iteration 100: Loss = -12462.55064305536
Iteration 200: Loss = -12461.337544929189
Iteration 300: Loss = -12461.039221984836
Iteration 400: Loss = -12460.903192973006
Iteration 500: Loss = -12460.819222650509
Iteration 600: Loss = -12460.758265386745
Iteration 700: Loss = -12460.71387601931
Iteration 800: Loss = -12460.68242532845
Iteration 900: Loss = -12460.657012812595
Iteration 1000: Loss = -12460.631808086235
Iteration 1100: Loss = -12460.603650990097
Iteration 1200: Loss = -12460.5702942112
Iteration 1300: Loss = -12460.529480754873
Iteration 1400: Loss = -12460.478777874268
Iteration 1500: Loss = -12460.415493524511
Iteration 1600: Loss = -12460.337829794342
Iteration 1700: Loss = -12460.246734043862
Iteration 1800: Loss = -12460.148311633204
Iteration 1900: Loss = -12460.053661633512
Iteration 2000: Loss = -12459.973077331755
Iteration 2100: Loss = -12459.910095086658
Iteration 2200: Loss = -12459.861931592615
Iteration 2300: Loss = -12459.823982610955
Iteration 2400: Loss = -12459.792566965265
Iteration 2500: Loss = -12459.76622410498
Iteration 2600: Loss = -12459.745268300567
Iteration 2700: Loss = -12459.7295432385
Iteration 2800: Loss = -12459.717596817434
Iteration 2900: Loss = -12459.707822512022
Iteration 3000: Loss = -12459.699308176334
Iteration 3100: Loss = -12459.691407436654
Iteration 3200: Loss = -12459.68372847925
Iteration 3300: Loss = -12459.675937506101
Iteration 3400: Loss = -12459.667867029399
Iteration 3500: Loss = -12459.659227572161
Iteration 3600: Loss = -12459.649862559125
Iteration 3700: Loss = -12459.639538080986
Iteration 3800: Loss = -12459.628081009932
Iteration 3900: Loss = -12459.615061658851
Iteration 4000: Loss = -12459.599621862208
Iteration 4100: Loss = -12459.580708104895
Iteration 4200: Loss = -12459.557282393127
Iteration 4300: Loss = -12459.52908393587
Iteration 4400: Loss = -12459.497705745529
Iteration 4500: Loss = -12459.466872714473
Iteration 4600: Loss = -12459.44185314283
Iteration 4700: Loss = -12459.424903802488
Iteration 4800: Loss = -12459.415101757477
Iteration 4900: Loss = -12459.40986823137
Iteration 5000: Loss = -12459.406977937
Iteration 5100: Loss = -12459.405356784962
Iteration 5200: Loss = -12459.404177185195
Iteration 5300: Loss = -12459.40334765875
Iteration 5400: Loss = -12459.402783151256
Iteration 5500: Loss = -12459.402302696722
Iteration 5600: Loss = -12459.40184883848
Iteration 5700: Loss = -12459.40153197874
Iteration 5800: Loss = -12459.40116835958
Iteration 5900: Loss = -12459.40101020306
Iteration 6000: Loss = -12459.400741315654
Iteration 6100: Loss = -12459.400574866879
Iteration 6200: Loss = -12459.400430007114
Iteration 6300: Loss = -12459.40029236639
Iteration 6400: Loss = -12459.40127997159
1
Iteration 6500: Loss = -12459.400094860543
Iteration 6600: Loss = -12459.399998234958
Iteration 6700: Loss = -12459.399964999935
Iteration 6800: Loss = -12459.399831257886
Iteration 6900: Loss = -12459.404813623005
1
Iteration 7000: Loss = -12459.399764097072
Iteration 7100: Loss = -12459.399700960477
Iteration 7200: Loss = -12459.39963382843
Iteration 7300: Loss = -12459.399625181204
Iteration 7400: Loss = -12459.399664842891
Iteration 7500: Loss = -12459.399573559696
Iteration 7600: Loss = -12459.399500711337
Iteration 7700: Loss = -12459.39954904627
Iteration 7800: Loss = -12459.399514361585
Iteration 7900: Loss = -12459.404637170848
1
Iteration 8000: Loss = -12459.399394497193
Iteration 8100: Loss = -12459.399510042056
1
Iteration 8200: Loss = -12459.399390679779
Iteration 8300: Loss = -12459.399361651549
Iteration 8400: Loss = -12459.39939379078
Iteration 8500: Loss = -12459.399545166341
1
Iteration 8600: Loss = -12459.399312650774
Iteration 8700: Loss = -12459.399631965725
1
Iteration 8800: Loss = -12459.409036710043
2
Iteration 8900: Loss = -12459.400784880636
3
Iteration 9000: Loss = -12459.399549015892
4
Iteration 9100: Loss = -12459.401247904214
5
Iteration 9200: Loss = -12459.419949806172
6
Iteration 9300: Loss = -12459.399764548862
7
Iteration 9400: Loss = -12459.40138430426
8
Iteration 9500: Loss = -12459.406948848593
9
Iteration 9600: Loss = -12459.399526304147
10
Iteration 9700: Loss = -12459.399290516594
Iteration 9800: Loss = -12459.402382818196
1
Iteration 9900: Loss = -12459.399205969978
Iteration 10000: Loss = -12459.399544565078
1
Iteration 10100: Loss = -12459.41724587365
2
Iteration 10200: Loss = -12459.406455192295
3
Iteration 10300: Loss = -12459.401820432142
4
Iteration 10400: Loss = -12459.399390761071
5
Iteration 10500: Loss = -12459.399223912802
Iteration 10600: Loss = -12459.40054100954
1
Iteration 10700: Loss = -12459.399680274973
2
Iteration 10800: Loss = -12459.453260251239
3
Iteration 10900: Loss = -12459.399194862683
Iteration 11000: Loss = -12459.407159280388
1
Iteration 11100: Loss = -12459.399176914665
Iteration 11200: Loss = -12459.400371015507
1
Iteration 11300: Loss = -12459.399698007694
2
Iteration 11400: Loss = -12459.490143953557
3
Iteration 11500: Loss = -12459.399172201927
Iteration 11600: Loss = -12459.39919440149
Iteration 11700: Loss = -12459.399600803463
1
Iteration 11800: Loss = -12459.401410292567
2
Iteration 11900: Loss = -12459.40130044989
3
Iteration 12000: Loss = -12459.399211262225
Iteration 12100: Loss = -12459.39918383066
Iteration 12200: Loss = -12459.407296738753
1
Iteration 12300: Loss = -12459.402207298657
2
Iteration 12400: Loss = -12459.399272969817
Iteration 12500: Loss = -12459.399494418009
1
Iteration 12600: Loss = -12459.48387205789
2
Iteration 12700: Loss = -12459.399135700529
Iteration 12800: Loss = -12459.400466182871
1
Iteration 12900: Loss = -12459.462612325971
2
Iteration 13000: Loss = -12459.399293340752
3
Iteration 13100: Loss = -12459.40039844926
4
Iteration 13200: Loss = -12459.399193071886
Iteration 13300: Loss = -12459.399319563841
1
Iteration 13400: Loss = -12459.399877282201
2
Iteration 13500: Loss = -12459.39969070572
3
Iteration 13600: Loss = -12459.399351611255
4
Iteration 13700: Loss = -12459.399215829899
Iteration 13800: Loss = -12459.39959638869
1
Iteration 13900: Loss = -12459.4006234669
2
Iteration 14000: Loss = -12459.399123807258
Iteration 14100: Loss = -12459.40064422088
1
Iteration 14200: Loss = -12459.400977129197
2
Iteration 14300: Loss = -12459.406961577204
3
Iteration 14400: Loss = -12459.399156862193
Iteration 14500: Loss = -12459.637949897591
1
Iteration 14600: Loss = -12459.399116351124
Iteration 14700: Loss = -12459.40721436482
1
Iteration 14800: Loss = -12459.399584907562
2
Iteration 14900: Loss = -12459.399211990416
Iteration 15000: Loss = -12459.457381588289
1
Iteration 15100: Loss = -12459.399296302723
Iteration 15200: Loss = -12459.399213761923
Iteration 15300: Loss = -12459.401420092965
1
Iteration 15400: Loss = -12459.39910929067
Iteration 15500: Loss = -12459.399153064498
Iteration 15600: Loss = -12459.417130785874
1
Iteration 15700: Loss = -12459.406578121036
2
Iteration 15800: Loss = -12459.402505811424
3
Iteration 15900: Loss = -12459.480579674651
4
Iteration 16000: Loss = -12459.399142620347
Iteration 16100: Loss = -12459.399284669022
1
Iteration 16200: Loss = -12459.416111851875
2
Iteration 16300: Loss = -12459.399197942637
Iteration 16400: Loss = -12459.436286339731
1
Iteration 16500: Loss = -12459.399436168738
2
Iteration 16600: Loss = -12459.407635666626
3
Iteration 16700: Loss = -12459.440413154673
4
Iteration 16800: Loss = -12459.39970177435
5
Iteration 16900: Loss = -12459.403948433772
6
Iteration 17000: Loss = -12459.42900395119
7
Iteration 17100: Loss = -12459.403769583201
8
Iteration 17200: Loss = -12459.39923893961
Iteration 17300: Loss = -12459.431451381726
1
Iteration 17400: Loss = -12459.399124983607
Iteration 17500: Loss = -12459.399433027602
1
Iteration 17600: Loss = -12459.406557614193
2
Iteration 17700: Loss = -12459.40140208079
3
Iteration 17800: Loss = -12459.399137440327
Iteration 17900: Loss = -12459.4009362769
1
Iteration 18000: Loss = -12459.399172001742
Iteration 18100: Loss = -12459.399366242102
1
Iteration 18200: Loss = -12459.596461824065
2
Iteration 18300: Loss = -12459.399135894131
Iteration 18400: Loss = -12459.402069789161
1
Iteration 18500: Loss = -12459.399149006853
Iteration 18600: Loss = -12459.399235333169
Iteration 18700: Loss = -12459.444030086814
1
Iteration 18800: Loss = -12459.406970217324
2
Iteration 18900: Loss = -12459.40184332297
3
Iteration 19000: Loss = -12459.407657555084
4
Iteration 19100: Loss = -12459.477155231778
5
Iteration 19200: Loss = -12459.410678836655
6
Iteration 19300: Loss = -12459.410923042295
7
Iteration 19400: Loss = -12459.399125099908
Iteration 19500: Loss = -12459.399273928108
1
Iteration 19600: Loss = -12459.40136349728
2
Iteration 19700: Loss = -12459.39965783817
3
Iteration 19800: Loss = -12459.404740036853
4
Iteration 19900: Loss = -12459.463517502798
5
pi: tensor([[0.9260, 0.0740],
        [0.9796, 0.0204]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0011, 0.9989], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2004, 0.2103],
         [0.5497, 0.2099]],

        [[0.6073, 0.1842],
         [0.6242, 0.5683]],

        [[0.6878, 0.2122],
         [0.5356, 0.6147]],

        [[0.5612, 0.1809],
         [0.6732, 0.6760]],

        [[0.6263, 0.2034],
         [0.6759, 0.6635]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015801661391253653
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20716.267068871577
Iteration 100: Loss = -12461.435243132591
Iteration 200: Loss = -12460.997021361985
Iteration 300: Loss = -12460.820691342302
Iteration 400: Loss = -12460.691699929648
Iteration 500: Loss = -12460.576551771563
Iteration 600: Loss = -12460.49293972152
Iteration 700: Loss = -12460.427326189712
Iteration 800: Loss = -12460.360013454707
Iteration 900: Loss = -12460.28609333999
Iteration 1000: Loss = -12460.205084540134
Iteration 1100: Loss = -12460.118679984807
Iteration 1200: Loss = -12460.032417885282
Iteration 1300: Loss = -12459.953029136388
Iteration 1400: Loss = -12459.88466477772
Iteration 1500: Loss = -12459.82796791337
Iteration 1600: Loss = -12459.781334548537
Iteration 1700: Loss = -12459.743221645966
Iteration 1800: Loss = -12459.712183300811
Iteration 1900: Loss = -12459.686787669516
Iteration 2000: Loss = -12459.6653045505
Iteration 2100: Loss = -12459.646454267395
Iteration 2200: Loss = -12459.629221938743
Iteration 2300: Loss = -12459.612773011942
Iteration 2400: Loss = -12459.59648784073
Iteration 2500: Loss = -12459.579811276288
Iteration 2600: Loss = -12459.562305559319
Iteration 2700: Loss = -12459.543851854314
Iteration 2800: Loss = -12459.524529011522
Iteration 2900: Loss = -12459.504750029173
Iteration 3000: Loss = -12459.485529092599
Iteration 3100: Loss = -12459.46784777761
Iteration 3200: Loss = -12459.452656002926
Iteration 3300: Loss = -12459.440487778258
Iteration 3400: Loss = -12459.43116738077
Iteration 3500: Loss = -12459.424343185414
Iteration 3600: Loss = -12459.419393838105
Iteration 3700: Loss = -12459.415741247842
Iteration 3800: Loss = -12459.413073603042
Iteration 3900: Loss = -12459.410983760694
Iteration 4000: Loss = -12459.40930615158
Iteration 4100: Loss = -12459.408010127936
Iteration 4200: Loss = -12459.406891823077
Iteration 4300: Loss = -12459.40586724545
Iteration 4400: Loss = -12459.405055297042
Iteration 4500: Loss = -12459.404370881724
Iteration 4600: Loss = -12459.403706689065
Iteration 4700: Loss = -12459.403202214588
Iteration 4800: Loss = -12459.402697881638
Iteration 4900: Loss = -12459.40231584848
Iteration 5000: Loss = -12459.416185489252
1
Iteration 5100: Loss = -12459.401618918282
Iteration 5200: Loss = -12459.401333051512
Iteration 5300: Loss = -12459.403192909931
1
Iteration 5400: Loss = -12459.400862851433
Iteration 5500: Loss = -12459.400714168789
Iteration 5600: Loss = -12459.401310261408
1
Iteration 5700: Loss = -12459.400358512246
Iteration 5800: Loss = -12459.400236636831
Iteration 5900: Loss = -12459.400169412154
Iteration 6000: Loss = -12459.400083166864
Iteration 6100: Loss = -12459.400091337984
Iteration 6200: Loss = -12459.399869958053
Iteration 6300: Loss = -12459.399830861703
Iteration 6400: Loss = -12459.400075825719
1
Iteration 6500: Loss = -12459.399686593326
Iteration 6600: Loss = -12459.39970074607
Iteration 6700: Loss = -12459.399626073
Iteration 6800: Loss = -12459.399609596729
Iteration 6900: Loss = -12459.399585852647
Iteration 7000: Loss = -12459.401489410755
1
Iteration 7100: Loss = -12459.399515455845
Iteration 7200: Loss = -12459.399469417456
Iteration 7300: Loss = -12459.399637851368
1
Iteration 7400: Loss = -12459.399534517172
Iteration 7500: Loss = -12459.419948354684
1
Iteration 7600: Loss = -12459.399403401016
Iteration 7700: Loss = -12459.545523254908
1
Iteration 7800: Loss = -12459.399340104932
Iteration 7900: Loss = -12459.39935689121
Iteration 8000: Loss = -12459.39960080708
1
Iteration 8100: Loss = -12459.399303439164
Iteration 8200: Loss = -12459.399713778554
1
Iteration 8300: Loss = -12459.399272841913
Iteration 8400: Loss = -12459.410824685638
1
Iteration 8500: Loss = -12459.518761501593
2
Iteration 8600: Loss = -12459.40002984184
3
Iteration 8700: Loss = -12459.399341429995
Iteration 8800: Loss = -12459.401044006463
1
Iteration 8900: Loss = -12459.399233145103
Iteration 9000: Loss = -12459.399303315437
Iteration 9100: Loss = -12459.399705286234
1
Iteration 9200: Loss = -12459.399545517746
2
Iteration 9300: Loss = -12459.399251882614
Iteration 9400: Loss = -12459.399474708694
1
Iteration 9500: Loss = -12459.431071629444
2
Iteration 9600: Loss = -12459.399207844092
Iteration 9700: Loss = -12459.400588410828
1
Iteration 9800: Loss = -12459.408864131392
2
Iteration 9900: Loss = -12459.399208260822
Iteration 10000: Loss = -12459.400338848258
1
Iteration 10100: Loss = -12459.401688282109
2
Iteration 10200: Loss = -12459.399261245238
Iteration 10300: Loss = -12459.399368185641
1
Iteration 10400: Loss = -12459.401631094197
2
Iteration 10500: Loss = -12459.40176240274
3
Iteration 10600: Loss = -12459.408956585035
4
Iteration 10700: Loss = -12459.52284360109
5
Iteration 10800: Loss = -12459.399147152293
Iteration 10900: Loss = -12459.570022294763
1
Iteration 11000: Loss = -12459.399230534118
Iteration 11100: Loss = -12459.40045322986
1
Iteration 11200: Loss = -12459.399495624475
2
Iteration 11300: Loss = -12459.399167997173
Iteration 11400: Loss = -12459.399495842568
1
Iteration 11500: Loss = -12459.3992009803
Iteration 11600: Loss = -12459.399182266456
Iteration 11700: Loss = -12459.40445149902
1
Iteration 11800: Loss = -12459.399691902341
2
Iteration 11900: Loss = -12459.399138691679
Iteration 12000: Loss = -12459.656701321055
1
Iteration 12100: Loss = -12459.399146348336
Iteration 12200: Loss = -12459.405424846318
1
Iteration 12300: Loss = -12459.434522089963
2
Iteration 12400: Loss = -12459.402706762172
3
Iteration 12500: Loss = -12459.399398151589
4
Iteration 12600: Loss = -12459.399503770042
5
Iteration 12700: Loss = -12459.399596006133
6
Iteration 12800: Loss = -12459.399232872973
Iteration 12900: Loss = -12459.465841674164
1
Iteration 13000: Loss = -12459.399147735294
Iteration 13100: Loss = -12459.40395608826
1
Iteration 13200: Loss = -12459.402130095179
2
Iteration 13300: Loss = -12459.448853819425
3
Iteration 13400: Loss = -12459.39917514552
Iteration 13500: Loss = -12459.399172700549
Iteration 13600: Loss = -12459.39994165696
1
Iteration 13700: Loss = -12459.427434793284
2
Iteration 13800: Loss = -12459.421049174089
3
Iteration 13900: Loss = -12459.404427992067
4
Iteration 14000: Loss = -12459.399242003909
Iteration 14100: Loss = -12459.4450825068
1
Iteration 14200: Loss = -12459.399092172036
Iteration 14300: Loss = -12459.400783881203
1
Iteration 14400: Loss = -12459.401114971397
2
Iteration 14500: Loss = -12459.399353602183
3
Iteration 14600: Loss = -12459.401306189277
4
Iteration 14700: Loss = -12459.760327310321
5
Iteration 14800: Loss = -12459.399330356588
6
Iteration 14900: Loss = -12459.440938580005
7
Iteration 15000: Loss = -12459.412150420749
8
Iteration 15100: Loss = -12459.399444565499
9
Iteration 15200: Loss = -12459.399255660523
10
Iteration 15300: Loss = -12459.40622857129
11
Iteration 15400: Loss = -12459.574012736652
12
Iteration 15500: Loss = -12459.399237093117
13
Iteration 15600: Loss = -12459.399625914495
14
Iteration 15700: Loss = -12459.39932036952
15
Stopping early at iteration 15700 due to no improvement.
pi: tensor([[0.9276, 0.0724],
        [0.9798, 0.0202]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0029, 0.9971], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2007, 0.2103],
         [0.5005, 0.2097]],

        [[0.6404, 0.1839],
         [0.5343, 0.5674]],

        [[0.6084, 0.2119],
         [0.6364, 0.6626]],

        [[0.7035, 0.1802],
         [0.6833, 0.5778]],

        [[0.7118, 0.2030],
         [0.5469, 0.5220]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015801661391253653
Average Adjusted Rand Index: 0.0
11921.762488826182
[-0.0015801661391253653, -0.0015801661391253653] [0.0, 0.0] [12459.402787161045, 12459.39932036952]
-------------------------------------
This iteration is 19
True Objective function: Loss = -12015.424557153518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20509.06974527981
Iteration 100: Loss = -12538.748726522454
Iteration 200: Loss = -12538.466693334054
Iteration 300: Loss = -12538.417882836718
Iteration 400: Loss = -12538.384838092603
Iteration 500: Loss = -12538.3531167713
Iteration 600: Loss = -12538.314696878984
Iteration 700: Loss = -12538.258843600473
Iteration 800: Loss = -12538.18367183792
Iteration 900: Loss = -12538.113456252688
Iteration 1000: Loss = -12538.063302181648
Iteration 1100: Loss = -12538.028983346312
Iteration 1200: Loss = -12538.004837738687
Iteration 1300: Loss = -12537.987119101252
Iteration 1400: Loss = -12537.973780317896
Iteration 1500: Loss = -12537.963402970474
Iteration 1600: Loss = -12537.955175591369
Iteration 1700: Loss = -12537.948497409297
Iteration 1800: Loss = -12537.942986459746
Iteration 1900: Loss = -12537.938363941932
Iteration 2000: Loss = -12537.934461201225
Iteration 2100: Loss = -12537.93105887137
Iteration 2200: Loss = -12537.9281519937
Iteration 2300: Loss = -12537.925616684968
Iteration 2400: Loss = -12537.923367795362
Iteration 2500: Loss = -12537.921326981359
Iteration 2600: Loss = -12537.919565829268
Iteration 2700: Loss = -12537.917928047336
Iteration 2800: Loss = -12537.916458041414
Iteration 2900: Loss = -12537.915097242223
Iteration 3000: Loss = -12537.9138286781
Iteration 3100: Loss = -12537.912638648748
Iteration 3200: Loss = -12537.91152816714
Iteration 3300: Loss = -12537.910466906005
Iteration 3400: Loss = -12537.909431798054
Iteration 3500: Loss = -12537.90841945333
Iteration 3600: Loss = -12537.907494177662
Iteration 3700: Loss = -12537.906554690082
Iteration 3800: Loss = -12537.905634411722
Iteration 3900: Loss = -12537.904743291841
Iteration 4000: Loss = -12537.903954520805
Iteration 4100: Loss = -12537.90309780862
Iteration 4200: Loss = -12537.902340466477
Iteration 4300: Loss = -12537.901620690574
Iteration 4400: Loss = -12537.900910732853
Iteration 4500: Loss = -12537.900270728926
Iteration 4600: Loss = -12537.899695927583
Iteration 4700: Loss = -12537.899123029869
Iteration 4800: Loss = -12537.89863295964
Iteration 4900: Loss = -12537.898137466536
Iteration 5000: Loss = -12537.897681805436
Iteration 5100: Loss = -12537.897178616888
Iteration 5200: Loss = -12537.896760982912
Iteration 5300: Loss = -12537.896313549241
Iteration 5400: Loss = -12537.895802351431
Iteration 5500: Loss = -12537.895230072047
Iteration 5600: Loss = -12537.894547135524
Iteration 5700: Loss = -12537.8937045452
Iteration 5800: Loss = -12537.892572768384
Iteration 5900: Loss = -12537.890932760145
Iteration 6000: Loss = -12537.888032229426
Iteration 6100: Loss = -12537.88177943035
Iteration 6200: Loss = -12537.85672389033
Iteration 6300: Loss = -12537.686456878057
Iteration 6400: Loss = -12534.433699151783
Iteration 6500: Loss = -12534.40691191069
Iteration 6600: Loss = -12534.3991135846
Iteration 6700: Loss = -12534.395206186355
Iteration 6800: Loss = -12534.39291990527
Iteration 6900: Loss = -12534.391157566972
Iteration 7000: Loss = -12534.389978293631
Iteration 7100: Loss = -12534.39021963376
1
Iteration 7200: Loss = -12534.388369956456
Iteration 7300: Loss = -12534.387780130588
Iteration 7400: Loss = -12534.38731944711
Iteration 7500: Loss = -12534.416216267302
1
Iteration 7600: Loss = -12534.386558219716
Iteration 7700: Loss = -12534.38622473273
Iteration 7800: Loss = -12534.385862770092
Iteration 7900: Loss = -12534.38689731544
1
Iteration 8000: Loss = -12534.384744520412
Iteration 8100: Loss = -12534.384432185057
Iteration 8200: Loss = -12534.384185347497
Iteration 8300: Loss = -12534.383855830056
Iteration 8400: Loss = -12534.383608242451
Iteration 8500: Loss = -12534.383504780648
Iteration 8600: Loss = -12535.167245561759
1
Iteration 8700: Loss = -12534.383327259915
Iteration 8800: Loss = -12534.38319861477
Iteration 8900: Loss = -12534.38308151507
Iteration 9000: Loss = -12534.472895993124
1
Iteration 9100: Loss = -12534.382948770088
Iteration 9200: Loss = -12534.3828890126
Iteration 9300: Loss = -12534.38287090993
Iteration 9400: Loss = -12534.382809259434
Iteration 9500: Loss = -12534.385255588108
1
Iteration 9600: Loss = -12534.382709431557
Iteration 9700: Loss = -12534.382634471269
Iteration 9800: Loss = -12534.382669156694
Iteration 9900: Loss = -12534.470930213913
1
Iteration 10000: Loss = -12534.38263331729
Iteration 10100: Loss = -12534.382628029496
Iteration 10200: Loss = -12534.382549397313
Iteration 10300: Loss = -12534.383031166066
1
Iteration 10400: Loss = -12534.382433435885
Iteration 10500: Loss = -12534.382431386752
Iteration 10600: Loss = -12534.382333496205
Iteration 10700: Loss = -12534.387461389228
1
Iteration 10800: Loss = -12534.381998511508
Iteration 10900: Loss = -12534.381982836418
Iteration 11000: Loss = -12534.38193998982
Iteration 11100: Loss = -12534.411063873413
1
Iteration 11200: Loss = -12534.381878585498
Iteration 11300: Loss = -12534.381879406821
Iteration 11400: Loss = -12534.381844099245
Iteration 11500: Loss = -12534.38178353442
Iteration 11600: Loss = -12534.381734116021
Iteration 11700: Loss = -12535.180208710992
1
Iteration 11800: Loss = -12534.381757432257
Iteration 11900: Loss = -12534.381729955381
Iteration 12000: Loss = -12534.381672510513
Iteration 12100: Loss = -12534.383173869752
1
Iteration 12200: Loss = -12534.381645539275
Iteration 12300: Loss = -12534.381699444988
Iteration 12400: Loss = -12534.38166791557
Iteration 12500: Loss = -12534.381961043711
1
Iteration 12600: Loss = -12534.378010364564
Iteration 12700: Loss = -12534.377855546305
Iteration 12800: Loss = -12534.68490653804
1
Iteration 12900: Loss = -12534.377568727883
Iteration 13000: Loss = -12534.377462972412
Iteration 13100: Loss = -12534.37742564654
Iteration 13200: Loss = -12534.433543336423
1
Iteration 13300: Loss = -12534.3774802362
Iteration 13400: Loss = -12534.377476191285
Iteration 13500: Loss = -12534.377465426674
Iteration 13600: Loss = -12534.39589120051
1
Iteration 13700: Loss = -12534.37741039185
Iteration 13800: Loss = -12534.377470556421
Iteration 13900: Loss = -12534.377418838463
Iteration 14000: Loss = -12534.37796598267
1
Iteration 14100: Loss = -12534.377407484555
Iteration 14200: Loss = -12534.377407802613
Iteration 14300: Loss = -12534.4534788891
1
Iteration 14400: Loss = -12534.377409323533
Iteration 14500: Loss = -12534.37739504794
Iteration 14600: Loss = -12534.377544471601
1
Iteration 14700: Loss = -12534.37744028209
Iteration 14800: Loss = -12534.377406188207
Iteration 14900: Loss = -12534.377761261308
1
Iteration 15000: Loss = -12534.377391702026
Iteration 15100: Loss = -12534.525826405517
1
Iteration 15200: Loss = -12534.377653248368
2
Iteration 15300: Loss = -12534.37739154231
Iteration 15400: Loss = -12534.377390884318
Iteration 15500: Loss = -12534.377420242192
Iteration 15600: Loss = -12534.379021866345
1
Iteration 15700: Loss = -12534.377472240267
Iteration 15800: Loss = -12534.377346273523
Iteration 15900: Loss = -12534.820866209726
1
Iteration 16000: Loss = -12534.377335339275
Iteration 16100: Loss = -12534.377293875976
Iteration 16200: Loss = -12534.377772154217
1
Iteration 16300: Loss = -12534.377361635625
Iteration 16400: Loss = -12534.667099677397
1
Iteration 16500: Loss = -12534.37742196432
Iteration 16600: Loss = -12534.377413470342
Iteration 16700: Loss = -12534.377631294727
1
Iteration 16800: Loss = -12534.377410309748
Iteration 16900: Loss = -12534.379672628726
1
Iteration 17000: Loss = -12534.377330640154
Iteration 17100: Loss = -12534.378774743886
1
Iteration 17200: Loss = -12534.377343043216
Iteration 17300: Loss = -12534.391536043753
1
Iteration 17400: Loss = -12534.377346284198
Iteration 17500: Loss = -12534.377292816705
Iteration 17600: Loss = -12534.411348604937
1
Iteration 17700: Loss = -12534.37726296944
Iteration 17800: Loss = -12534.377249157622
Iteration 17900: Loss = -12534.382311030115
1
Iteration 18000: Loss = -12534.377276854326
Iteration 18100: Loss = -12534.37723885734
Iteration 18200: Loss = -12534.377580236425
1
Iteration 18300: Loss = -12534.378202919741
2
Iteration 18400: Loss = -12534.37696146351
Iteration 18500: Loss = -12534.377101620517
1
Iteration 18600: Loss = -12534.37704620132
Iteration 18700: Loss = -12534.395059137558
1
Iteration 18800: Loss = -12534.377093018336
Iteration 18900: Loss = -12534.520714534458
1
Iteration 19000: Loss = -12534.377861959158
2
Iteration 19100: Loss = -12534.377026726399
Iteration 19200: Loss = -12534.378170936294
1
Iteration 19300: Loss = -12534.37704144226
Iteration 19400: Loss = -12534.37704351617
Iteration 19500: Loss = -12534.377599391137
1
Iteration 19600: Loss = -12534.37703413376
Iteration 19700: Loss = -12534.393174270957
1
Iteration 19800: Loss = -12534.376914346856
Iteration 19900: Loss = -12534.381122044595
1
pi: tensor([[1.0000e+00, 4.6500e-09],
        [4.8282e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9892, 0.0108], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2056, 0.2117],
         [0.7055, 0.2154]],

        [[0.5512, 0.1044],
         [0.7254, 0.5777]],

        [[0.6911, 0.1047],
         [0.6296, 0.5284]],

        [[0.6897, 0.2313],
         [0.7009, 0.6001]],

        [[0.6235, 0.1324],
         [0.7214, 0.5669]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.00015367833708429535
Average Adjusted Rand Index: 0.00047047552479642966
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22081.698027582188
Iteration 100: Loss = -12539.123716646354
Iteration 200: Loss = -12538.828617945557
Iteration 300: Loss = -12538.773945669367
Iteration 400: Loss = -12538.741773065223
Iteration 500: Loss = -12538.71972570069
Iteration 600: Loss = -12538.703274967105
Iteration 700: Loss = -12538.690128668026
Iteration 800: Loss = -12538.678696498615
Iteration 900: Loss = -12538.667817336964
Iteration 1000: Loss = -12538.656478452629
Iteration 1100: Loss = -12538.643039212102
Iteration 1200: Loss = -12538.624240094796
Iteration 1300: Loss = -12538.5921599894
Iteration 1400: Loss = -12538.526719847967
Iteration 1500: Loss = -12538.417532676645
Iteration 1600: Loss = -12538.316631992067
Iteration 1700: Loss = -12538.246864330415
Iteration 1800: Loss = -12538.185999842288
Iteration 1900: Loss = -12538.120436010726
Iteration 2000: Loss = -12538.073533580025
Iteration 2100: Loss = -12538.047519906435
Iteration 2200: Loss = -12538.032016658153
Iteration 2300: Loss = -12538.021320094173
Iteration 2400: Loss = -12538.013226541474
Iteration 2500: Loss = -12538.006701032326
Iteration 2600: Loss = -12538.001201720519
Iteration 2700: Loss = -12537.996517247117
Iteration 2800: Loss = -12537.992393750292
Iteration 2900: Loss = -12537.988874834362
Iteration 3000: Loss = -12537.985539526606
Iteration 3100: Loss = -12537.982703562375
Iteration 3200: Loss = -12537.980078611145
Iteration 3300: Loss = -12537.977671460023
Iteration 3400: Loss = -12537.975437565425
Iteration 3500: Loss = -12537.973355075508
Iteration 3600: Loss = -12537.971306420824
Iteration 3700: Loss = -12537.969280879715
Iteration 3800: Loss = -12537.96697890802
Iteration 3900: Loss = -12537.963921679177
Iteration 4000: Loss = -12537.958262342194
Iteration 4100: Loss = -12537.94672059671
Iteration 4200: Loss = -12537.93361057144
Iteration 4300: Loss = -12537.927302616687
Iteration 4400: Loss = -12537.924252702685
Iteration 4500: Loss = -12537.922067037243
Iteration 4600: Loss = -12537.92031855858
Iteration 4700: Loss = -12537.918872502736
Iteration 4800: Loss = -12537.917696004431
Iteration 4900: Loss = -12537.916684209536
Iteration 5000: Loss = -12537.915887170162
Iteration 5100: Loss = -12537.915201913323
Iteration 5200: Loss = -12537.91457498744
Iteration 5300: Loss = -12537.9140212915
Iteration 5400: Loss = -12537.913595987124
Iteration 5500: Loss = -12537.913211942621
Iteration 5600: Loss = -12537.912778588498
Iteration 5700: Loss = -12537.912488726688
Iteration 5800: Loss = -12537.912184060928
Iteration 5900: Loss = -12537.911884851135
Iteration 6000: Loss = -12537.911619793891
Iteration 6100: Loss = -12537.911572776027
Iteration 6200: Loss = -12537.911186623063
Iteration 6300: Loss = -12537.910971201152
Iteration 6400: Loss = -12537.910805117444
Iteration 6500: Loss = -12537.910618823114
Iteration 6600: Loss = -12537.910495238739
Iteration 6700: Loss = -12537.910309163959
Iteration 6800: Loss = -12537.91025138598
Iteration 6900: Loss = -12537.910015824407
Iteration 7000: Loss = -12537.909924416437
Iteration 7100: Loss = -12537.911010852478
1
Iteration 7200: Loss = -12537.912028515417
2
Iteration 7300: Loss = -12537.940605372654
3
Iteration 7400: Loss = -12537.909497521197
Iteration 7500: Loss = -12537.90960320088
1
Iteration 7600: Loss = -12537.90931381967
Iteration 7700: Loss = -12537.910436737233
1
Iteration 7800: Loss = -12537.909114294345
Iteration 7900: Loss = -12537.909356521626
1
Iteration 8000: Loss = -12537.909002055794
Iteration 8100: Loss = -12537.913874718666
1
Iteration 8200: Loss = -12537.908868039549
Iteration 8300: Loss = -12537.90881303807
Iteration 8400: Loss = -12537.90905133804
1
Iteration 8500: Loss = -12537.90867854242
Iteration 8600: Loss = -12537.908643718278
Iteration 8700: Loss = -12537.910461656053
1
Iteration 8800: Loss = -12537.908554599133
Iteration 8900: Loss = -12537.916225741734
1
Iteration 9000: Loss = -12537.908497576926
Iteration 9100: Loss = -12537.908545962324
Iteration 9200: Loss = -12537.908426484315
Iteration 9300: Loss = -12537.908347105984
Iteration 9400: Loss = -12537.90842119517
Iteration 9500: Loss = -12537.908296082725
Iteration 9600: Loss = -12537.908249486725
Iteration 9700: Loss = -12537.908498685547
1
Iteration 9800: Loss = -12537.908185399872
Iteration 9900: Loss = -12537.908159756027
Iteration 10000: Loss = -12537.9115940169
1
Iteration 10100: Loss = -12537.908103953052
Iteration 10200: Loss = -12537.90810082631
Iteration 10300: Loss = -12538.226703195094
1
Iteration 10400: Loss = -12537.908071349466
Iteration 10500: Loss = -12537.908104940703
Iteration 10600: Loss = -12537.908543185678
1
Iteration 10700: Loss = -12537.908020756406
Iteration 10800: Loss = -12538.08665725076
1
Iteration 10900: Loss = -12537.908024846638
Iteration 11000: Loss = -12537.90800889309
Iteration 11100: Loss = -12538.347689647353
1
Iteration 11200: Loss = -12537.90793956501
Iteration 11300: Loss = -12537.907920713542
Iteration 11400: Loss = -12537.919282465478
1
Iteration 11500: Loss = -12537.907938607
Iteration 11600: Loss = -12537.909726197626
1
Iteration 11700: Loss = -12537.907885046803
Iteration 11800: Loss = -12537.907868102891
Iteration 11900: Loss = -12537.907905020626
Iteration 12000: Loss = -12537.908631920318
1
Iteration 12100: Loss = -12537.907869273782
Iteration 12200: Loss = -12537.90867908123
1
Iteration 12300: Loss = -12537.907853062734
Iteration 12400: Loss = -12537.907818111498
Iteration 12500: Loss = -12538.140620748743
1
Iteration 12600: Loss = -12537.907810232153
Iteration 12700: Loss = -12537.907807018722
Iteration 12800: Loss = -12538.217142393158
1
Iteration 12900: Loss = -12537.90780931937
Iteration 13000: Loss = -12537.907787327355
Iteration 13100: Loss = -12537.911423055011
1
Iteration 13200: Loss = -12537.907927615954
2
Iteration 13300: Loss = -12537.907811885574
Iteration 13400: Loss = -12537.930706348201
1
Iteration 13500: Loss = -12537.907728168828
Iteration 13600: Loss = -12537.909874522324
1
Iteration 13700: Loss = -12537.907803844364
Iteration 13800: Loss = -12537.909153209586
1
Iteration 13900: Loss = -12537.907807818347
Iteration 14000: Loss = -12537.907920598407
1
Iteration 14100: Loss = -12537.90770230057
Iteration 14200: Loss = -12537.907943685519
1
Iteration 14300: Loss = -12537.907791402837
Iteration 14400: Loss = -12537.909031986952
1
Iteration 14500: Loss = -12537.908324482441
2
Iteration 14600: Loss = -12537.907746575298
Iteration 14700: Loss = -12537.91265888342
1
Iteration 14800: Loss = -12537.907774583018
Iteration 14900: Loss = -12538.301513550434
1
Iteration 15000: Loss = -12537.907778855188
Iteration 15100: Loss = -12537.931919330731
1
Iteration 15200: Loss = -12537.907757392713
Iteration 15300: Loss = -12537.907741573166
Iteration 15400: Loss = -12538.008147911056
1
Iteration 15500: Loss = -12537.9099308554
2
Iteration 15600: Loss = -12537.90772000448
Iteration 15700: Loss = -12537.909024193596
1
Iteration 15800: Loss = -12537.908674671116
2
Iteration 15900: Loss = -12537.907899776945
3
Iteration 16000: Loss = -12537.984628039128
4
Iteration 16100: Loss = -12537.91014640186
5
Iteration 16200: Loss = -12537.90769218518
Iteration 16300: Loss = -12537.907757350937
Iteration 16400: Loss = -12537.912236817281
1
Iteration 16500: Loss = -12537.908367529513
2
Iteration 16600: Loss = -12538.063518780898
3
Iteration 16700: Loss = -12537.907731029132
Iteration 16800: Loss = -12537.909524517601
1
Iteration 16900: Loss = -12537.907688900585
Iteration 17000: Loss = -12537.91149807942
1
Iteration 17100: Loss = -12537.934087238847
2
Iteration 17200: Loss = -12537.907674795839
Iteration 17300: Loss = -12537.908087202422
1
Iteration 17400: Loss = -12537.907650471692
Iteration 17500: Loss = -12537.908165142488
1
Iteration 17600: Loss = -12538.025377060276
2
Iteration 17700: Loss = -12537.909639598258
3
Iteration 17800: Loss = -12537.944792049324
4
Iteration 17900: Loss = -12537.907895379512
5
Iteration 18000: Loss = -12537.908535700722
6
Iteration 18100: Loss = -12537.907708187642
Iteration 18200: Loss = -12537.907674084876
Iteration 18300: Loss = -12537.907641724056
Iteration 18400: Loss = -12537.90812404982
1
Iteration 18500: Loss = -12537.907833053832
2
Iteration 18600: Loss = -12537.920459992214
3
Iteration 18700: Loss = -12537.907682878329
Iteration 18800: Loss = -12537.934442706199
1
Iteration 18900: Loss = -12537.90767613837
Iteration 19000: Loss = -12537.908224840547
1
Iteration 19100: Loss = -12537.907671260284
Iteration 19200: Loss = -12537.907759725154
Iteration 19300: Loss = -12537.910899809838
1
Iteration 19400: Loss = -12537.907707318642
Iteration 19500: Loss = -12537.907684942234
Iteration 19600: Loss = -12537.953353920651
1
Iteration 19700: Loss = -12537.907685646609
Iteration 19800: Loss = -12537.907810425926
1
Iteration 19900: Loss = -12537.912391485572
2
pi: tensor([[9.8670e-01, 1.3296e-02],
        [1.0000e+00, 7.2081e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9998e-01, 1.8674e-05], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2042, 0.2057],
         [0.7119, 0.1875]],

        [[0.5808, 0.1424],
         [0.6454, 0.6939]],

        [[0.7053, 0.2762],
         [0.6673, 0.5205]],

        [[0.5391, 0.3164],
         [0.5432, 0.6892]],

        [[0.6944, 0.1548],
         [0.6500, 0.5411]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.486306880875413e-05
Average Adjusted Rand Index: -0.0008569898232458489
12015.424557153518
[-0.00015367833708429535, -9.486306880875413e-05] [0.00047047552479642966, -0.0008569898232458489] [12534.37683439771, 12537.907694143778]
-------------------------------------
This iteration is 20
True Objective function: Loss = -11900.399351243004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20959.682753714358
Iteration 100: Loss = -12406.002519941398
Iteration 200: Loss = -12405.818502883005
Iteration 300: Loss = -12405.759543307888
Iteration 400: Loss = -12405.731495580092
Iteration 500: Loss = -12405.713817201737
Iteration 600: Loss = -12405.699572218255
Iteration 700: Loss = -12405.685981661423
Iteration 800: Loss = -12405.671689866665
Iteration 900: Loss = -12405.655761052754
Iteration 1000: Loss = -12405.637760265156
Iteration 1100: Loss = -12405.617603464807
Iteration 1200: Loss = -12405.595724038254
Iteration 1300: Loss = -12405.573045632793
Iteration 1400: Loss = -12405.550620234993
Iteration 1500: Loss = -12405.529193286688
Iteration 1600: Loss = -12405.508922473957
Iteration 1700: Loss = -12405.489660414756
Iteration 1800: Loss = -12405.471195971591
Iteration 1900: Loss = -12405.453529160215
Iteration 2000: Loss = -12405.437129659704
Iteration 2100: Loss = -12405.422426757244
Iteration 2200: Loss = -12405.409721033271
Iteration 2300: Loss = -12405.398921261001
Iteration 2400: Loss = -12405.38976138415
Iteration 2500: Loss = -12405.381953183165
Iteration 2600: Loss = -12405.37533002561
Iteration 2700: Loss = -12405.369677593295
Iteration 2800: Loss = -12405.364966318592
Iteration 2900: Loss = -12405.361203329621
Iteration 3000: Loss = -12405.35831357345
Iteration 3100: Loss = -12405.356189672813
Iteration 3200: Loss = -12405.354695644184
Iteration 3300: Loss = -12405.353633152634
Iteration 3400: Loss = -12405.352827351782
Iteration 3500: Loss = -12405.352289622831
Iteration 3600: Loss = -12405.351892241652
Iteration 3700: Loss = -12405.351609739135
Iteration 3800: Loss = -12405.35136503017
Iteration 3900: Loss = -12405.358675651763
1
Iteration 4000: Loss = -12405.351068137306
Iteration 4100: Loss = -12405.350917689533
Iteration 4200: Loss = -12405.353261854067
1
Iteration 4300: Loss = -12405.350741077014
Iteration 4400: Loss = -12405.350585515089
Iteration 4500: Loss = -12405.350480069563
Iteration 4600: Loss = -12405.350396704087
Iteration 4700: Loss = -12405.350294170632
Iteration 4800: Loss = -12405.35018261904
Iteration 4900: Loss = -12405.350147485058
Iteration 5000: Loss = -12405.350048127859
Iteration 5100: Loss = -12405.361522833833
1
Iteration 5200: Loss = -12405.349866737277
Iteration 5300: Loss = -12405.34976864169
Iteration 5400: Loss = -12405.349743836498
Iteration 5500: Loss = -12405.349629976978
Iteration 5600: Loss = -12405.350107973807
1
Iteration 5700: Loss = -12405.349481930016
Iteration 5800: Loss = -12405.349359193711
Iteration 5900: Loss = -12405.34936499358
Iteration 6000: Loss = -12405.349222858213
Iteration 6100: Loss = -12405.351444399686
1
Iteration 6200: Loss = -12405.349135349748
Iteration 6300: Loss = -12405.349065119559
Iteration 6400: Loss = -12405.349040925625
Iteration 6500: Loss = -12405.34891730097
Iteration 6600: Loss = -12405.353301109399
1
Iteration 6700: Loss = -12405.348825157262
Iteration 6800: Loss = -12405.350045904628
1
Iteration 6900: Loss = -12405.348681648571
Iteration 7000: Loss = -12405.348645597163
Iteration 7100: Loss = -12405.348560662977
Iteration 7200: Loss = -12405.34857159583
Iteration 7300: Loss = -12405.348626631181
Iteration 7400: Loss = -12405.348482030819
Iteration 7500: Loss = -12405.448805038377
1
Iteration 7600: Loss = -12405.3483903319
Iteration 7700: Loss = -12405.352304919961
1
Iteration 7800: Loss = -12405.348344544589
Iteration 7900: Loss = -12405.348418934931
Iteration 8000: Loss = -12405.352538487172
1
Iteration 8100: Loss = -12405.350631389043
2
Iteration 8200: Loss = -12405.361999223216
3
Iteration 8300: Loss = -12405.352318616806
4
Iteration 8400: Loss = -12405.348251941321
Iteration 8500: Loss = -12405.348068137795
Iteration 8600: Loss = -12405.348052238713
Iteration 8700: Loss = -12405.36549921581
1
Iteration 8800: Loss = -12405.348042456591
Iteration 8900: Loss = -12405.349471421794
1
Iteration 9000: Loss = -12405.622810362396
2
Iteration 9100: Loss = -12405.348423171927
3
Iteration 9200: Loss = -12405.36448124426
4
Iteration 9300: Loss = -12405.349430410128
5
Iteration 9400: Loss = -12405.391130159815
6
Iteration 9500: Loss = -12405.349303846935
7
Iteration 9600: Loss = -12405.357883600398
8
Iteration 9700: Loss = -12405.347795964415
Iteration 9800: Loss = -12405.347808965103
Iteration 9900: Loss = -12405.350363417041
1
Iteration 10000: Loss = -12405.349118471775
2
Iteration 10100: Loss = -12405.34777295946
Iteration 10200: Loss = -12405.4548291197
1
Iteration 10300: Loss = -12405.347668678616
Iteration 10400: Loss = -12405.350426734645
1
Iteration 10500: Loss = -12405.347614945817
Iteration 10600: Loss = -12405.366282719602
1
Iteration 10700: Loss = -12405.34757319568
Iteration 10800: Loss = -12405.347950789379
1
Iteration 10900: Loss = -12405.347670244171
Iteration 11000: Loss = -12405.34753339843
Iteration 11100: Loss = -12405.34778411998
1
Iteration 11200: Loss = -12405.347473237387
Iteration 11300: Loss = -12405.350268138487
1
Iteration 11400: Loss = -12405.347417684146
Iteration 11500: Loss = -12405.347868025807
1
Iteration 11600: Loss = -12405.347486285125
Iteration 11700: Loss = -12405.34849646796
1
Iteration 11800: Loss = -12405.347383709297
Iteration 11900: Loss = -12405.353676849625
1
Iteration 12000: Loss = -12405.348246913363
2
Iteration 12100: Loss = -12405.347690166489
3
Iteration 12200: Loss = -12405.351182217251
4
Iteration 12300: Loss = -12405.36747847199
5
Iteration 12400: Loss = -12405.34733936209
Iteration 12500: Loss = -12405.34848790529
1
Iteration 12600: Loss = -12405.347535950836
2
Iteration 12700: Loss = -12405.348483957136
3
Iteration 12800: Loss = -12405.347682722311
4
Iteration 12900: Loss = -12405.3473302295
Iteration 13000: Loss = -12405.661396551752
1
Iteration 13100: Loss = -12405.347344832375
Iteration 13200: Loss = -12405.350468384167
1
Iteration 13300: Loss = -12405.347304663825
Iteration 13400: Loss = -12405.352651397472
1
Iteration 13500: Loss = -12405.369616550825
2
Iteration 13600: Loss = -12405.347301554264
Iteration 13700: Loss = -12405.364001853304
1
Iteration 13800: Loss = -12405.496658349699
2
Iteration 13900: Loss = -12405.347299611161
Iteration 14000: Loss = -12405.351381991662
1
Iteration 14100: Loss = -12405.434789793
2
Iteration 14200: Loss = -12405.3472778611
Iteration 14300: Loss = -12405.349033016435
1
Iteration 14400: Loss = -12405.347275976697
Iteration 14500: Loss = -12405.354649792285
1
Iteration 14600: Loss = -12405.347339482381
Iteration 14700: Loss = -12405.391302364666
1
Iteration 14800: Loss = -12405.348132042696
2
Iteration 14900: Loss = -12405.348100019792
3
Iteration 15000: Loss = -12405.347512120876
4
Iteration 15100: Loss = -12405.38474300492
5
Iteration 15200: Loss = -12405.351852081787
6
Iteration 15300: Loss = -12405.347288292316
Iteration 15400: Loss = -12405.397262262262
1
Iteration 15500: Loss = -12405.431142405738
2
Iteration 15600: Loss = -12405.347349216638
Iteration 15700: Loss = -12405.34744688293
Iteration 15800: Loss = -12405.376551746072
1
Iteration 15900: Loss = -12405.347321588382
Iteration 16000: Loss = -12405.348139170097
1
Iteration 16100: Loss = -12405.348114985907
2
Iteration 16200: Loss = -12405.347417344112
Iteration 16300: Loss = -12405.35007313647
1
Iteration 16400: Loss = -12405.347383044413
Iteration 16500: Loss = -12405.348056214276
1
Iteration 16600: Loss = -12405.350008970267
2
Iteration 16700: Loss = -12405.347392747459
Iteration 16800: Loss = -12405.34825760491
1
Iteration 16900: Loss = -12405.415366344916
2
Iteration 17000: Loss = -12405.347330790668
Iteration 17100: Loss = -12405.359597170176
1
Iteration 17200: Loss = -12405.347310584424
Iteration 17300: Loss = -12405.368579638613
1
Iteration 17400: Loss = -12405.363049601567
2
Iteration 17500: Loss = -12405.45000526445
3
Iteration 17600: Loss = -12405.349840972913
4
Iteration 17700: Loss = -12405.347586084366
5
Iteration 17800: Loss = -12405.347350084434
Iteration 17900: Loss = -12405.350108353978
1
Iteration 18000: Loss = -12405.348113157645
2
Iteration 18100: Loss = -12405.351178042452
3
Iteration 18200: Loss = -12405.486337149794
4
Iteration 18300: Loss = -12405.3473757138
Iteration 18400: Loss = -12405.356374778983
1
Iteration 18500: Loss = -12405.34746692154
Iteration 18600: Loss = -12405.347560769456
Iteration 18700: Loss = -12405.347830943347
1
Iteration 18800: Loss = -12405.464702936622
2
Iteration 18900: Loss = -12405.347394162312
Iteration 19000: Loss = -12405.347602501723
1
Iteration 19100: Loss = -12405.347799673149
2
Iteration 19200: Loss = -12405.34730861695
Iteration 19300: Loss = -12405.34765710813
1
Iteration 19400: Loss = -12405.347326731533
Iteration 19500: Loss = -12405.353403084464
1
Iteration 19600: Loss = -12405.347360512797
Iteration 19700: Loss = -12405.347855991737
1
Iteration 19800: Loss = -12405.349234703157
2
Iteration 19900: Loss = -12405.347593698962
3
pi: tensor([[1.6419e-05, 9.9998e-01],
        [8.1265e-02, 9.1874e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0217, 0.9783], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1933, 0.1991],
         [0.7004, 0.2011]],

        [[0.5320, 0.2036],
         [0.5739, 0.5200]],

        [[0.6688, 0.1951],
         [0.6762, 0.6751]],

        [[0.5638, 0.1765],
         [0.5031, 0.5585]],

        [[0.5016, 0.2104],
         [0.7189, 0.7206]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22613.288260769234
Iteration 100: Loss = -12406.710277455513
Iteration 200: Loss = -12406.008212096884
Iteration 300: Loss = -12405.896962071025
Iteration 400: Loss = -12405.844498938539
Iteration 500: Loss = -12405.811895517605
Iteration 600: Loss = -12405.788782529069
Iteration 700: Loss = -12405.771027009872
Iteration 800: Loss = -12405.756425127965
Iteration 900: Loss = -12405.743839008226
Iteration 1000: Loss = -12405.732387940443
Iteration 1100: Loss = -12405.721579682988
Iteration 1200: Loss = -12405.71090094712
Iteration 1300: Loss = -12405.69995787961
Iteration 1400: Loss = -12405.688283400079
Iteration 1500: Loss = -12405.675475498923
Iteration 1600: Loss = -12405.66120795293
Iteration 1700: Loss = -12405.64516609816
Iteration 1800: Loss = -12405.627367312021
Iteration 1900: Loss = -12405.608159030884
Iteration 2000: Loss = -12405.588425306405
Iteration 2100: Loss = -12405.568889188908
Iteration 2200: Loss = -12405.54963761419
Iteration 2300: Loss = -12405.530312741177
Iteration 2400: Loss = -12405.510578376961
Iteration 2500: Loss = -12405.49059317626
Iteration 2600: Loss = -12405.470821362904
Iteration 2700: Loss = -12405.452167464506
Iteration 2800: Loss = -12405.435342362734
Iteration 2900: Loss = -12405.4209213568
Iteration 3000: Loss = -12405.408943875213
Iteration 3100: Loss = -12405.398967832973
Iteration 3200: Loss = -12405.390679021742
Iteration 3300: Loss = -12405.383690217463
Iteration 3400: Loss = -12405.37773225798
Iteration 3500: Loss = -12405.372723693843
Iteration 3600: Loss = -12405.368576059704
Iteration 3700: Loss = -12405.365240888683
Iteration 3800: Loss = -12405.362614595886
Iteration 3900: Loss = -12405.360494526785
Iteration 4000: Loss = -12405.358876454035
Iteration 4100: Loss = -12405.358561912573
Iteration 4200: Loss = -12405.356691956555
Iteration 4300: Loss = -12405.355920960536
Iteration 4400: Loss = -12405.355316239982
Iteration 4500: Loss = -12405.354787227612
Iteration 4600: Loss = -12405.356368608243
1
Iteration 4700: Loss = -12405.354017727946
Iteration 4800: Loss = -12405.35367882137
Iteration 4900: Loss = -12405.353937884891
1
Iteration 5000: Loss = -12405.353221476465
Iteration 5100: Loss = -12405.352923923554
Iteration 5200: Loss = -12405.352787900303
Iteration 5300: Loss = -12405.35252902167
Iteration 5400: Loss = -12405.358331011683
1
Iteration 5500: Loss = -12405.352165954595
Iteration 5600: Loss = -12405.35199678919
Iteration 5700: Loss = -12405.351845258727
Iteration 5800: Loss = -12405.351685850857
Iteration 5900: Loss = -12405.351491881052
Iteration 6000: Loss = -12405.355662797123
1
Iteration 6100: Loss = -12405.351171775736
Iteration 6200: Loss = -12405.351052696187
Iteration 6300: Loss = -12405.350906702713
Iteration 6400: Loss = -12405.350773715953
Iteration 6500: Loss = -12405.352172309515
1
Iteration 6600: Loss = -12405.350452479039
Iteration 6700: Loss = -12405.350343979353
Iteration 6800: Loss = -12405.350286819366
Iteration 6900: Loss = -12405.350101954384
Iteration 7000: Loss = -12405.349959272664
Iteration 7100: Loss = -12405.351874588237
1
Iteration 7200: Loss = -12405.349768849024
Iteration 7300: Loss = -12405.349674842038
Iteration 7400: Loss = -12405.349554955737
Iteration 7500: Loss = -12405.34946363397
Iteration 7600: Loss = -12405.34939161426
Iteration 7700: Loss = -12405.349245714126
Iteration 7800: Loss = -12405.522157163212
1
Iteration 7900: Loss = -12405.349129731516
Iteration 8000: Loss = -12405.349044758026
Iteration 8100: Loss = -12405.355014579563
1
Iteration 8200: Loss = -12405.348884314297
Iteration 8300: Loss = -12405.348823380222
Iteration 8400: Loss = -12405.348984854247
1
Iteration 8500: Loss = -12405.358086109562
2
Iteration 8600: Loss = -12405.348680158595
Iteration 8700: Loss = -12405.349482428957
1
Iteration 8800: Loss = -12405.348547253803
Iteration 8900: Loss = -12405.382138142451
1
Iteration 9000: Loss = -12405.348433286283
Iteration 9100: Loss = -12405.348363971507
Iteration 9200: Loss = -12405.352714996472
1
Iteration 9300: Loss = -12405.348330971647
Iteration 9400: Loss = -12405.348287563902
Iteration 9500: Loss = -12405.35656831837
1
Iteration 9600: Loss = -12405.348210381984
Iteration 9700: Loss = -12405.348153494673
Iteration 9800: Loss = -12405.35323037021
1
Iteration 9900: Loss = -12405.348061536482
Iteration 10000: Loss = -12405.350663504083
1
Iteration 10100: Loss = -12405.351374010303
2
Iteration 10200: Loss = -12405.460869994007
3
Iteration 10300: Loss = -12405.347987911533
Iteration 10400: Loss = -12405.358744789683
1
Iteration 10500: Loss = -12405.347927471217
Iteration 10600: Loss = -12405.383791120677
1
Iteration 10700: Loss = -12405.347875548976
Iteration 10800: Loss = -12405.348288582747
1
Iteration 10900: Loss = -12405.3479246125
Iteration 11000: Loss = -12405.353318760435
1
Iteration 11100: Loss = -12405.359031214555
2
Iteration 11200: Loss = -12405.467606281663
3
Iteration 11300: Loss = -12405.347729979763
Iteration 11400: Loss = -12405.36065957125
1
Iteration 11500: Loss = -12405.347679992232
Iteration 11600: Loss = -12405.372340771677
1
Iteration 11700: Loss = -12405.34764287462
Iteration 11800: Loss = -12405.347917216612
1
Iteration 11900: Loss = -12405.348155525564
2
Iteration 12000: Loss = -12405.359033414294
3
Iteration 12100: Loss = -12405.347683182537
Iteration 12200: Loss = -12405.347800859894
1
Iteration 12300: Loss = -12405.34814949587
2
Iteration 12400: Loss = -12405.348306981137
3
Iteration 12500: Loss = -12405.348017271142
4
Iteration 12600: Loss = -12405.515331484696
5
Iteration 12700: Loss = -12405.352658114463
6
Iteration 12800: Loss = -12405.347446913847
Iteration 12900: Loss = -12405.347518084662
Iteration 13000: Loss = -12405.349757648404
1
Iteration 13100: Loss = -12405.353005833367
2
Iteration 13200: Loss = -12405.347740677222
3
Iteration 13300: Loss = -12405.347557403958
Iteration 13400: Loss = -12405.347450165016
Iteration 13500: Loss = -12405.349734154574
1
Iteration 13600: Loss = -12405.356088797902
2
Iteration 13700: Loss = -12405.347648241657
3
Iteration 13800: Loss = -12405.348901883352
4
Iteration 13900: Loss = -12405.354056787673
5
Iteration 14000: Loss = -12405.347466753887
Iteration 14100: Loss = -12405.347326269222
Iteration 14200: Loss = -12405.34766560944
1
Iteration 14300: Loss = -12405.378655461254
2
Iteration 14400: Loss = -12405.34740226449
Iteration 14500: Loss = -12405.349903059257
1
Iteration 14600: Loss = -12405.347478370188
Iteration 14700: Loss = -12405.348819985833
1
Iteration 14800: Loss = -12405.350767570282
2
Iteration 14900: Loss = -12405.347561006418
Iteration 15000: Loss = -12405.34767764566
1
Iteration 15100: Loss = -12405.354180499668
2
Iteration 15200: Loss = -12405.353877144182
3
Iteration 15300: Loss = -12405.351009206832
4
Iteration 15400: Loss = -12405.34749077046
Iteration 15500: Loss = -12405.347445632136
Iteration 15600: Loss = -12405.358947524566
1
Iteration 15700: Loss = -12405.351384486676
2
Iteration 15800: Loss = -12405.3766317357
3
Iteration 15900: Loss = -12405.347452987615
Iteration 16000: Loss = -12405.35785552568
1
Iteration 16100: Loss = -12405.347354946685
Iteration 16200: Loss = -12405.349390457366
1
Iteration 16300: Loss = -12405.39247606124
2
Iteration 16400: Loss = -12405.350811521033
3
Iteration 16500: Loss = -12405.347514798577
4
Iteration 16600: Loss = -12405.34781329554
5
Iteration 16700: Loss = -12405.36839570743
6
Iteration 16800: Loss = -12405.34737021869
Iteration 16900: Loss = -12405.349471088131
1
Iteration 17000: Loss = -12405.347314896893
Iteration 17100: Loss = -12405.34849498792
1
Iteration 17200: Loss = -12405.347380215715
Iteration 17300: Loss = -12405.37412917946
1
Iteration 17400: Loss = -12405.347335273005
Iteration 17500: Loss = -12405.353037402463
1
Iteration 17600: Loss = -12405.347694924532
2
Iteration 17700: Loss = -12405.347419108157
Iteration 17800: Loss = -12405.349017176408
1
Iteration 17900: Loss = -12405.34818699031
2
Iteration 18000: Loss = -12405.347465884966
Iteration 18100: Loss = -12405.34752569077
Iteration 18200: Loss = -12405.348204813481
1
Iteration 18300: Loss = -12405.34744017507
Iteration 18400: Loss = -12405.348896410727
1
Iteration 18500: Loss = -12405.358833481516
2
Iteration 18600: Loss = -12405.475959033265
3
Iteration 18700: Loss = -12405.34736988001
Iteration 18800: Loss = -12405.34750302062
1
Iteration 18900: Loss = -12405.395298498366
2
Iteration 19000: Loss = -12405.347341172068
Iteration 19100: Loss = -12405.364897910757
1
Iteration 19200: Loss = -12405.347360359698
Iteration 19300: Loss = -12405.347581850014
1
Iteration 19400: Loss = -12405.349789384696
2
Iteration 19500: Loss = -12405.359553678894
3
Iteration 19600: Loss = -12405.34735433858
Iteration 19700: Loss = -12405.431170460957
1
Iteration 19800: Loss = -12405.347336833456
Iteration 19900: Loss = -12405.48281623729
1
pi: tensor([[3.1085e-05, 9.9997e-01],
        [8.2019e-02, 9.1798e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0211, 0.9789], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1932, 0.1990],
         [0.6165, 0.2012]],

        [[0.6710, 0.2035],
         [0.5648, 0.7170]],

        [[0.5074, 0.1950],
         [0.6417, 0.6180]],

        [[0.6663, 0.1766],
         [0.5561, 0.5967]],

        [[0.5865, 0.2103],
         [0.5053, 0.5269]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
11900.399351243004
[0.0, 0.0] [0.0, 0.0] [12405.347562198023, 12405.347368365547]
-------------------------------------
This iteration is 21
True Objective function: Loss = -11917.116107070342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20035.3365522906
Iteration 100: Loss = -12408.77422462654
Iteration 200: Loss = -12407.548112816708
Iteration 300: Loss = -12406.866860157083
Iteration 400: Loss = -12406.56774033956
Iteration 500: Loss = -12406.38698857381
Iteration 600: Loss = -12406.258258369495
Iteration 700: Loss = -12406.151697223067
Iteration 800: Loss = -12406.063809987538
Iteration 900: Loss = -12405.992866901517
Iteration 1000: Loss = -12405.934325966718
Iteration 1100: Loss = -12405.884874420211
Iteration 1200: Loss = -12405.840535716714
Iteration 1300: Loss = -12405.795069476195
Iteration 1400: Loss = -12405.737261517284
Iteration 1500: Loss = -12405.645320776834
Iteration 1600: Loss = -12405.510268312048
Iteration 1700: Loss = -12405.388806298986
Iteration 1800: Loss = -12405.289569327802
Iteration 1900: Loss = -12405.181884273909
Iteration 2000: Loss = -12405.055439290649
Iteration 2100: Loss = -12404.920487735857
Iteration 2200: Loss = -12404.804647015444
Iteration 2300: Loss = -12404.714860811557
Iteration 2400: Loss = -12404.654409469715
Iteration 2500: Loss = -12404.617598912839
Iteration 2600: Loss = -12404.595123493902
Iteration 2700: Loss = -12404.580756783638
Iteration 2800: Loss = -12404.571350701512
Iteration 2900: Loss = -12404.564893851457
Iteration 3000: Loss = -12404.560353630479
Iteration 3100: Loss = -12404.55702478135
Iteration 3200: Loss = -12404.554522221171
Iteration 3300: Loss = -12404.55256568715
Iteration 3400: Loss = -12404.55102039015
Iteration 3500: Loss = -12404.549733829897
Iteration 3600: Loss = -12404.548794442548
Iteration 3700: Loss = -12404.547881834416
Iteration 3800: Loss = -12404.547212913323
Iteration 3900: Loss = -12404.546573762009
Iteration 4000: Loss = -12404.546089620715
Iteration 4100: Loss = -12404.545608720764
Iteration 4200: Loss = -12404.545242872458
Iteration 4300: Loss = -12404.544869226833
Iteration 4400: Loss = -12404.544549767212
Iteration 4500: Loss = -12404.544301103218
Iteration 4600: Loss = -12404.544074615702
Iteration 4700: Loss = -12404.543799276373
Iteration 4800: Loss = -12404.543648456032
Iteration 4900: Loss = -12404.54342245387
Iteration 5000: Loss = -12404.543291993099
Iteration 5100: Loss = -12404.543162529653
Iteration 5200: Loss = -12404.543009550165
Iteration 5300: Loss = -12404.542888402642
Iteration 5400: Loss = -12404.54278069266
Iteration 5500: Loss = -12404.542691548519
Iteration 5600: Loss = -12404.54257359171
Iteration 5700: Loss = -12404.54247066507
Iteration 5800: Loss = -12404.542400275392
Iteration 5900: Loss = -12404.542365841635
Iteration 6000: Loss = -12404.542290087036
Iteration 6100: Loss = -12404.54224385926
Iteration 6200: Loss = -12404.543318881604
1
Iteration 6300: Loss = -12404.54215098408
Iteration 6400: Loss = -12404.542061957522
Iteration 6500: Loss = -12404.5420039649
Iteration 6600: Loss = -12404.541961028464
Iteration 6700: Loss = -12404.54192003614
Iteration 6800: Loss = -12404.541925740657
Iteration 6900: Loss = -12404.541853512668
Iteration 7000: Loss = -12404.541814277245
Iteration 7100: Loss = -12404.543423564806
1
Iteration 7200: Loss = -12404.541938090642
2
Iteration 7300: Loss = -12404.541833085303
Iteration 7400: Loss = -12404.594638868957
1
Iteration 7500: Loss = -12404.541704213763
Iteration 7600: Loss = -12404.550912452998
1
Iteration 7700: Loss = -12404.54166811117
Iteration 7800: Loss = -12404.541652919619
Iteration 7900: Loss = -12404.551202364339
1
Iteration 8000: Loss = -12404.541584359658
Iteration 8100: Loss = -12404.560192531706
1
Iteration 8200: Loss = -12404.541593232974
Iteration 8300: Loss = -12404.541550588885
Iteration 8400: Loss = -12404.568975107402
1
Iteration 8500: Loss = -12404.541522704389
Iteration 8600: Loss = -12404.55454758589
1
Iteration 8700: Loss = -12404.547846296113
2
Iteration 8800: Loss = -12404.541516267327
Iteration 8900: Loss = -12404.54251634225
1
Iteration 9000: Loss = -12404.541479667363
Iteration 9100: Loss = -12404.54345125524
1
Iteration 9200: Loss = -12404.543417050769
2
Iteration 9300: Loss = -12404.546205624662
3
Iteration 9400: Loss = -12404.541462304082
Iteration 9500: Loss = -12404.890419427487
1
Iteration 9600: Loss = -12404.541465366377
Iteration 9700: Loss = -12404.541513901582
Iteration 9800: Loss = -12404.543444993373
1
Iteration 9900: Loss = -12404.542292371252
2
Iteration 10000: Loss = -12404.551607502843
3
Iteration 10100: Loss = -12404.541476884584
Iteration 10200: Loss = -12404.541411565897
Iteration 10300: Loss = -12404.6281152645
1
Iteration 10400: Loss = -12404.541405243102
Iteration 10500: Loss = -12404.548838336936
1
Iteration 10600: Loss = -12404.541393500094
Iteration 10700: Loss = -12404.541830149163
1
Iteration 10800: Loss = -12404.603266385424
2
Iteration 10900: Loss = -12404.54145340284
Iteration 11000: Loss = -12404.541804643177
1
Iteration 11100: Loss = -12404.541591374444
2
Iteration 11200: Loss = -12404.543441808628
3
Iteration 11300: Loss = -12404.541489142113
Iteration 11400: Loss = -12404.541588921868
Iteration 11500: Loss = -12404.54395901701
1
Iteration 11600: Loss = -12404.541348439601
Iteration 11700: Loss = -12404.646350092795
1
Iteration 11800: Loss = -12404.541323543423
Iteration 11900: Loss = -12404.609107269003
1
Iteration 12000: Loss = -12404.541322504976
Iteration 12100: Loss = -12404.541308278789
Iteration 12200: Loss = -12404.541916420994
1
Iteration 12300: Loss = -12404.541340951298
Iteration 12400: Loss = -12404.550008539052
1
Iteration 12500: Loss = -12404.541328060312
Iteration 12600: Loss = -12404.557266314094
1
Iteration 12700: Loss = -12404.541340731683
Iteration 12800: Loss = -12404.54587119723
1
Iteration 12900: Loss = -12404.541437257834
Iteration 13000: Loss = -12404.541501662572
Iteration 13100: Loss = -12404.541936853113
1
Iteration 13200: Loss = -12404.542784901705
2
Iteration 13300: Loss = -12404.541320716644
Iteration 13400: Loss = -12404.541655043324
1
Iteration 13500: Loss = -12404.541304755821
Iteration 13600: Loss = -12404.541574219877
1
Iteration 13700: Loss = -12404.541727784455
2
Iteration 13800: Loss = -12404.55005581445
3
Iteration 13900: Loss = -12404.557909515435
4
Iteration 14000: Loss = -12404.5522352032
5
Iteration 14100: Loss = -12404.567413717512
6
Iteration 14200: Loss = -12404.555215110244
7
Iteration 14300: Loss = -12404.54138856379
Iteration 14400: Loss = -12404.541950856374
1
Iteration 14500: Loss = -12404.873756060559
2
Iteration 14600: Loss = -12404.541301968335
Iteration 14700: Loss = -12404.541699781168
1
Iteration 14800: Loss = -12404.541306633144
Iteration 14900: Loss = -12404.54672763273
1
Iteration 15000: Loss = -12404.547717405287
2
Iteration 15100: Loss = -12404.541450505916
3
Iteration 15200: Loss = -12404.541331383769
Iteration 15300: Loss = -12404.561310673509
1
Iteration 15400: Loss = -12404.541291347785
Iteration 15500: Loss = -12404.541850085934
1
Iteration 15600: Loss = -12404.544113974487
2
Iteration 15700: Loss = -12404.554102852206
3
Iteration 15800: Loss = -12404.541441706757
4
Iteration 15900: Loss = -12404.541416468746
5
Iteration 16000: Loss = -12404.541365154932
Iteration 16100: Loss = -12404.565975131942
1
Iteration 16200: Loss = -12404.541334691921
Iteration 16300: Loss = -12404.623624903328
1
Iteration 16400: Loss = -12404.563878097677
2
Iteration 16500: Loss = -12404.54139274027
Iteration 16600: Loss = -12404.541486778282
Iteration 16700: Loss = -12404.726637886175
1
Iteration 16800: Loss = -12404.541517826352
Iteration 16900: Loss = -12404.543877561777
1
Iteration 17000: Loss = -12404.542878341466
2
Iteration 17100: Loss = -12404.541396263574
Iteration 17200: Loss = -12404.54517607331
1
Iteration 17300: Loss = -12404.552555289616
2
Iteration 17400: Loss = -12404.541304005865
Iteration 17500: Loss = -12404.541983805775
1
Iteration 17600: Loss = -12404.56790325871
2
Iteration 17700: Loss = -12404.565229157639
3
Iteration 17800: Loss = -12404.54127337437
Iteration 17900: Loss = -12404.54256456129
1
Iteration 18000: Loss = -12404.541248873606
Iteration 18100: Loss = -12404.544369867242
1
Iteration 18200: Loss = -12404.541297634058
Iteration 18300: Loss = -12404.592824074434
1
Iteration 18400: Loss = -12404.54604571743
2
Iteration 18500: Loss = -12404.548050081738
3
Iteration 18600: Loss = -12404.54163583057
4
Iteration 18700: Loss = -12404.552340470997
5
Iteration 18800: Loss = -12404.571833862356
6
Iteration 18900: Loss = -12404.541289249744
Iteration 19000: Loss = -12404.548011270144
1
Iteration 19100: Loss = -12404.545501705617
2
Iteration 19200: Loss = -12404.541240327084
Iteration 19300: Loss = -12404.541746725992
1
Iteration 19400: Loss = -12404.587186178844
2
Iteration 19500: Loss = -12404.54122784865
Iteration 19600: Loss = -12404.562822558084
1
Iteration 19700: Loss = -12404.543472631034
2
Iteration 19800: Loss = -12404.543312814372
3
Iteration 19900: Loss = -12404.542887764157
4
pi: tensor([[0.0342, 0.9658],
        [0.0234, 0.9766]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9990e-01, 1.0405e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1952, 0.1949],
         [0.6958, 0.2027]],

        [[0.5960, 0.1222],
         [0.6229, 0.5265]],

        [[0.7056, 0.1655],
         [0.6838, 0.6158]],

        [[0.6387, 0.3255],
         [0.6777, 0.5902]],

        [[0.5429, 0.1229],
         [0.6545, 0.5057]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: -0.014617321874876162
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0010223656796501718
Average Adjusted Rand Index: -0.000984070435581293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23801.901293639457
Iteration 100: Loss = -12408.19863230939
Iteration 200: Loss = -12406.7260601011
Iteration 300: Loss = -12406.37548575656
Iteration 400: Loss = -12406.21847061768
Iteration 500: Loss = -12406.121185422639
Iteration 600: Loss = -12406.049875581803
Iteration 700: Loss = -12405.991158667215
Iteration 800: Loss = -12405.939107605649
Iteration 900: Loss = -12405.890850622302
Iteration 1000: Loss = -12405.84513885211
Iteration 1100: Loss = -12405.801853227136
Iteration 1200: Loss = -12405.761354767903
Iteration 1300: Loss = -12405.724214246164
Iteration 1400: Loss = -12405.690666414615
Iteration 1500: Loss = -12405.659811084228
Iteration 1600: Loss = -12405.629055434469
Iteration 1700: Loss = -12405.594629268839
Iteration 1800: Loss = -12405.554055386778
Iteration 1900: Loss = -12405.508818538254
Iteration 2000: Loss = -12405.462974262968
Iteration 2100: Loss = -12405.41934877935
Iteration 2200: Loss = -12405.378321599796
Iteration 2300: Loss = -12405.338434387793
Iteration 2400: Loss = -12405.29610787993
Iteration 2500: Loss = -12405.245170031973
Iteration 2600: Loss = -12405.176220329497
Iteration 2700: Loss = -12405.081854741433
Iteration 2800: Loss = -12404.958063772094
Iteration 2900: Loss = -12404.81966223762
Iteration 3000: Loss = -12404.705805447358
Iteration 3100: Loss = -12404.641113570091
Iteration 3200: Loss = -12404.607359159587
Iteration 3300: Loss = -12404.5882544958
Iteration 3400: Loss = -12404.576441715128
Iteration 3500: Loss = -12404.568651204041
Iteration 3600: Loss = -12404.563201674127
Iteration 3700: Loss = -12404.559259350835
Iteration 3800: Loss = -12404.556276508334
Iteration 3900: Loss = -12404.553996528506
Iteration 4000: Loss = -12404.55219655618
Iteration 4100: Loss = -12404.55074565263
Iteration 4200: Loss = -12404.549560095893
Iteration 4300: Loss = -12404.548594090975
Iteration 4400: Loss = -12404.54773345136
Iteration 4500: Loss = -12404.547065504014
Iteration 4600: Loss = -12404.546467113636
Iteration 4700: Loss = -12404.54594581564
Iteration 4800: Loss = -12404.545513160268
Iteration 4900: Loss = -12404.545105920368
Iteration 5000: Loss = -12404.54478269862
Iteration 5100: Loss = -12404.544465207948
Iteration 5200: Loss = -12404.544214425938
Iteration 5300: Loss = -12404.54399315519
Iteration 5400: Loss = -12404.543762844625
Iteration 5500: Loss = -12404.543539368999
Iteration 5600: Loss = -12404.543372723254
Iteration 5700: Loss = -12404.543265578453
Iteration 5800: Loss = -12404.543125569036
Iteration 5900: Loss = -12404.542996389846
Iteration 6000: Loss = -12404.542835246632
Iteration 6100: Loss = -12404.54276102026
Iteration 6200: Loss = -12404.542664888553
Iteration 6300: Loss = -12404.542562406192
Iteration 6400: Loss = -12404.542409946618
Iteration 6500: Loss = -12404.542974105338
1
Iteration 6600: Loss = -12404.542319532364
Iteration 6700: Loss = -12404.542263940228
Iteration 6800: Loss = -12404.542184143049
Iteration 6900: Loss = -12404.542183600952
Iteration 7000: Loss = -12404.542048365816
Iteration 7100: Loss = -12404.542614159853
1
Iteration 7200: Loss = -12404.54201817853
Iteration 7300: Loss = -12404.54232995181
1
Iteration 7400: Loss = -12404.542685264681
2
Iteration 7500: Loss = -12404.541929740257
Iteration 7600: Loss = -12404.542160755544
1
Iteration 7700: Loss = -12404.541969031878
Iteration 7800: Loss = -12404.541771692991
Iteration 7900: Loss = -12404.54177074714
Iteration 8000: Loss = -12404.541731959127
Iteration 8100: Loss = -12404.541726177291
Iteration 8200: Loss = -12404.541880261262
1
Iteration 8300: Loss = -12404.542543112588
2
Iteration 8400: Loss = -12404.542205228736
3
Iteration 8500: Loss = -12404.541688632698
Iteration 8600: Loss = -12404.541606145316
Iteration 8700: Loss = -12404.54258482065
1
Iteration 8800: Loss = -12404.541597242396
Iteration 8900: Loss = -12404.544640441003
1
Iteration 9000: Loss = -12404.541833053549
2
Iteration 9100: Loss = -12404.541956499705
3
Iteration 9200: Loss = -12404.545351648561
4
Iteration 9300: Loss = -12404.598165507767
5
Iteration 9400: Loss = -12404.54152386488
Iteration 9500: Loss = -12404.549295736915
1
Iteration 9600: Loss = -12404.541496918037
Iteration 9700: Loss = -12404.541657263691
1
Iteration 9800: Loss = -12404.54146907468
Iteration 9900: Loss = -12404.54285785783
1
Iteration 10000: Loss = -12404.541468297586
Iteration 10100: Loss = -12404.548070464381
1
Iteration 10200: Loss = -12404.541452600837
Iteration 10300: Loss = -12404.542642369108
1
Iteration 10400: Loss = -12404.541429210843
Iteration 10500: Loss = -12404.54354730882
1
Iteration 10600: Loss = -12404.54168812716
2
Iteration 10700: Loss = -12404.541384456717
Iteration 10800: Loss = -12404.563923967784
1
Iteration 10900: Loss = -12404.541418744231
Iteration 11000: Loss = -12404.54189980522
1
Iteration 11100: Loss = -12404.541377300844
Iteration 11200: Loss = -12404.545695069437
1
Iteration 11300: Loss = -12404.541389299437
Iteration 11400: Loss = -12404.61911306449
1
Iteration 11500: Loss = -12404.541393735713
Iteration 11600: Loss = -12404.541341498145
Iteration 11700: Loss = -12404.609888834897
1
Iteration 11800: Loss = -12404.541337178473
Iteration 11900: Loss = -12404.541322145917
Iteration 12000: Loss = -12404.555753009054
1
Iteration 12100: Loss = -12404.541315094028
Iteration 12200: Loss = -12404.541364888002
Iteration 12300: Loss = -12404.541528281523
1
Iteration 12400: Loss = -12404.541347887154
Iteration 12500: Loss = -12404.541705183343
1
Iteration 12600: Loss = -12404.54135083596
Iteration 12700: Loss = -12404.541443200595
Iteration 12800: Loss = -12404.541339094058
Iteration 12900: Loss = -12404.541495151696
1
Iteration 13000: Loss = -12404.541309840417
Iteration 13100: Loss = -12404.5540557723
1
Iteration 13200: Loss = -12404.54133174476
Iteration 13300: Loss = -12404.541324182777
Iteration 13400: Loss = -12404.542784629897
1
Iteration 13500: Loss = -12404.541494405636
2
Iteration 13600: Loss = -12404.545464938206
3
Iteration 13700: Loss = -12404.541999296142
4
Iteration 13800: Loss = -12404.541522315745
5
Iteration 13900: Loss = -12404.54165937796
6
Iteration 14000: Loss = -12404.57023820114
7
Iteration 14100: Loss = -12404.541569280416
8
Iteration 14200: Loss = -12404.63736985823
9
Iteration 14300: Loss = -12404.541295366716
Iteration 14400: Loss = -12404.54240862143
1
Iteration 14500: Loss = -12404.542143550105
2
Iteration 14600: Loss = -12404.543063066967
3
Iteration 14700: Loss = -12404.59541572752
4
Iteration 14800: Loss = -12404.54134323536
Iteration 14900: Loss = -12404.541876359717
1
Iteration 15000: Loss = -12404.542425581874
2
Iteration 15100: Loss = -12404.54136144239
Iteration 15200: Loss = -12404.541544654916
1
Iteration 15300: Loss = -12404.541425037678
Iteration 15400: Loss = -12404.541381618863
Iteration 15500: Loss = -12404.571935945762
1
Iteration 15600: Loss = -12404.541566463244
2
Iteration 15700: Loss = -12404.541834747753
3
Iteration 15800: Loss = -12404.541421067815
Iteration 15900: Loss = -12404.547309955396
1
Iteration 16000: Loss = -12404.61538948312
2
Iteration 16100: Loss = -12404.541307294927
Iteration 16200: Loss = -12404.553491547764
1
Iteration 16300: Loss = -12404.541367546384
Iteration 16400: Loss = -12404.542629371626
1
Iteration 16500: Loss = -12404.54425552097
2
Iteration 16600: Loss = -12404.541571164084
3
Iteration 16700: Loss = -12404.542217119068
4
Iteration 16800: Loss = -12404.541469968597
5
Iteration 16900: Loss = -12404.541806765052
6
Iteration 17000: Loss = -12404.544125811472
7
Iteration 17100: Loss = -12404.54437331933
8
Iteration 17200: Loss = -12404.547122460763
9
Iteration 17300: Loss = -12404.541490581598
10
Iteration 17400: Loss = -12404.541317190073
Iteration 17500: Loss = -12404.701010502084
1
Iteration 17600: Loss = -12404.541306228508
Iteration 17700: Loss = -12404.54244067625
1
Iteration 17800: Loss = -12404.54130997335
Iteration 17900: Loss = -12404.54180754501
1
Iteration 18000: Loss = -12404.589814024655
2
Iteration 18100: Loss = -12404.54123551275
Iteration 18200: Loss = -12404.542198062376
1
Iteration 18300: Loss = -12404.541333382038
Iteration 18400: Loss = -12404.54484722523
1
Iteration 18500: Loss = -12404.54147106745
2
Iteration 18600: Loss = -12404.565036366588
3
Iteration 18700: Loss = -12404.542241338586
4
Iteration 18800: Loss = -12404.573152407616
5
Iteration 18900: Loss = -12404.54401656759
6
Iteration 19000: Loss = -12404.541481646109
7
Iteration 19100: Loss = -12404.592072475843
8
Iteration 19200: Loss = -12404.559102497115
9
Iteration 19300: Loss = -12404.541612250845
10
Iteration 19400: Loss = -12404.54983289936
11
Iteration 19500: Loss = -12404.541364525312
Iteration 19600: Loss = -12404.546085112856
1
Iteration 19700: Loss = -12404.851928718303
2
Iteration 19800: Loss = -12404.541355356463
Iteration 19900: Loss = -12404.633217334513
1
pi: tensor([[0.9764, 0.0236],
        [0.9652, 0.0348]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.6352e-04, 9.9984e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2035, 0.1949],
         [0.6383, 0.1952]],

        [[0.7275, 0.1226],
         [0.6047, 0.5085]],

        [[0.7292, 0.1649],
         [0.5473, 0.5988]],

        [[0.5236, 0.3264],
         [0.7156, 0.5325]],

        [[0.5543, 0.1232],
         [0.6826, 0.6309]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: -0.014617321874876162
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0010223656796501718
Average Adjusted Rand Index: -0.000984070435581293
11917.116107070342
[0.0010223656796501718, 0.0010223656796501718] [-0.000984070435581293, -0.000984070435581293] [12404.56307211328, 12404.541225151885]
-------------------------------------
This iteration is 22
True Objective function: Loss = -11865.25270267735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21231.069260133732
Iteration 100: Loss = -12435.286735091566
Iteration 200: Loss = -12434.753062623557
Iteration 300: Loss = -12434.56367824609
Iteration 400: Loss = -12434.446680468971
Iteration 500: Loss = -12434.338554345455
Iteration 600: Loss = -12434.226714135073
Iteration 700: Loss = -12434.103632277755
Iteration 800: Loss = -12433.955300108522
Iteration 900: Loss = -12433.76834018377
Iteration 1000: Loss = -12433.547551103331
Iteration 1100: Loss = -12433.324598888199
Iteration 1200: Loss = -12433.112530645509
Iteration 1300: Loss = -12432.887059192688
Iteration 1400: Loss = -12432.60696688331
Iteration 1500: Loss = -12432.245362428805
Iteration 1600: Loss = -12431.87525342449
Iteration 1700: Loss = -12431.564430336273
Iteration 1800: Loss = -12431.338541937967
Iteration 1900: Loss = -12431.197094402096
Iteration 2000: Loss = -12431.095561392885
Iteration 2100: Loss = -12431.009636931814
Iteration 2200: Loss = -12430.923726858266
Iteration 2300: Loss = -12430.83472695355
Iteration 2400: Loss = -12430.749445335667
Iteration 2500: Loss = -12430.682536508848
Iteration 2600: Loss = -12430.639018678296
Iteration 2700: Loss = -12430.612547390465
Iteration 2800: Loss = -12430.595978050693
Iteration 2900: Loss = -12430.584472282277
Iteration 3000: Loss = -12430.57563893682
Iteration 3100: Loss = -12430.568147680191
Iteration 3200: Loss = -12430.561318218835
Iteration 3300: Loss = -12430.554690622976
Iteration 3400: Loss = -12430.548087793668
Iteration 3500: Loss = -12430.541354289508
Iteration 3600: Loss = -12430.534520045174
Iteration 3700: Loss = -12430.52770157419
Iteration 3800: Loss = -12430.521100450564
Iteration 3900: Loss = -12430.515591422876
Iteration 4000: Loss = -12430.5092137507
Iteration 4100: Loss = -12430.504180379025
Iteration 4200: Loss = -12430.500040739857
Iteration 4300: Loss = -12430.496466946306
Iteration 4400: Loss = -12430.496677418541
1
Iteration 4500: Loss = -12430.491301015993
Iteration 4600: Loss = -12430.489561900251
Iteration 4700: Loss = -12430.48841294687
Iteration 4800: Loss = -12430.487502475813
Iteration 4900: Loss = -12430.486955810406
Iteration 5000: Loss = -12430.486737783032
Iteration 5100: Loss = -12430.486517391519
Iteration 5200: Loss = -12430.489325420707
1
Iteration 5300: Loss = -12430.486349996687
Iteration 5400: Loss = -12430.486314651058
Iteration 5500: Loss = -12430.486320658661
Iteration 5600: Loss = -12430.48630705065
Iteration 5700: Loss = -12430.48633653349
Iteration 5800: Loss = -12430.486247611288
Iteration 5900: Loss = -12430.487053141778
1
Iteration 6000: Loss = -12430.486247205134
Iteration 6100: Loss = -12430.486272911092
Iteration 6200: Loss = -12430.486348484983
Iteration 6300: Loss = -12430.486206932714
Iteration 6400: Loss = -12430.49115623756
1
Iteration 6500: Loss = -12430.4866994816
2
Iteration 6600: Loss = -12430.486302369536
Iteration 6700: Loss = -12430.487467090004
1
Iteration 6800: Loss = -12430.486229695432
Iteration 6900: Loss = -12430.487399345087
1
Iteration 7000: Loss = -12430.48617372121
Iteration 7100: Loss = -12430.486157083227
Iteration 7200: Loss = -12430.486107246676
Iteration 7300: Loss = -12430.486094293232
Iteration 7400: Loss = -12430.486038881661
Iteration 7500: Loss = -12430.486395320666
1
Iteration 7600: Loss = -12430.486049526704
Iteration 7700: Loss = -12430.501037823382
1
Iteration 7800: Loss = -12430.486019487895
Iteration 7900: Loss = -12430.491452525603
1
Iteration 8000: Loss = -12430.486013487645
Iteration 8100: Loss = -12430.496061906159
1
Iteration 8200: Loss = -12430.488265807317
2
Iteration 8300: Loss = -12430.485975431578
Iteration 8400: Loss = -12430.48704659855
1
Iteration 8500: Loss = -12430.551801339054
2
Iteration 8600: Loss = -12430.510090543734
3
Iteration 8700: Loss = -12430.494042081487
4
Iteration 8800: Loss = -12430.485875244372
Iteration 8900: Loss = -12430.486725319244
1
Iteration 9000: Loss = -12430.490139575468
2
Iteration 9100: Loss = -12430.485935836063
Iteration 9200: Loss = -12430.486205602749
1
Iteration 9300: Loss = -12430.708949845159
2
Iteration 9400: Loss = -12430.48586941319
Iteration 9500: Loss = -12430.488871447771
1
Iteration 9600: Loss = -12430.489940157244
2
Iteration 9700: Loss = -12430.486401809561
3
Iteration 9800: Loss = -12430.485882648783
Iteration 9900: Loss = -12430.485920893927
Iteration 10000: Loss = -12430.487253939533
1
Iteration 10100: Loss = -12430.579326810346
2
Iteration 10200: Loss = -12430.485764343292
Iteration 10300: Loss = -12430.485859546065
Iteration 10400: Loss = -12430.53286614139
1
Iteration 10500: Loss = -12430.507205409174
2
Iteration 10600: Loss = -12430.49211986598
3
Iteration 10700: Loss = -12430.489284477662
4
Iteration 10800: Loss = -12430.485723361546
Iteration 10900: Loss = -12430.55565199396
1
Iteration 11000: Loss = -12430.492815201105
2
Iteration 11100: Loss = -12430.4996472291
3
Iteration 11200: Loss = -12430.497183372301
4
Iteration 11300: Loss = -12430.49116979692
5
Iteration 11400: Loss = -12430.520324151654
6
Iteration 11500: Loss = -12430.486109631785
7
Iteration 11600: Loss = -12430.503654489012
8
Iteration 11700: Loss = -12430.498391775693
9
Iteration 11800: Loss = -12430.487202335704
10
Iteration 11900: Loss = -12430.485871597308
11
Iteration 12000: Loss = -12430.485835212829
12
Iteration 12100: Loss = -12430.487088758338
13
Iteration 12200: Loss = -12430.4855714001
Iteration 12300: Loss = -12430.486963411286
1
Iteration 12400: Loss = -12430.495312406367
2
Iteration 12500: Loss = -12430.485577019577
Iteration 12600: Loss = -12430.495801997891
1
Iteration 12700: Loss = -12430.508488826197
2
Iteration 12800: Loss = -12430.497751824814
3
Iteration 12900: Loss = -12430.503744816298
4
Iteration 13000: Loss = -12430.640136245978
5
Iteration 13100: Loss = -12430.492121891935
6
Iteration 13200: Loss = -12430.487195410058
7
Iteration 13300: Loss = -12430.489398119878
8
Iteration 13400: Loss = -12430.485586085866
Iteration 13500: Loss = -12430.498876696965
1
Iteration 13600: Loss = -12430.495048028624
2
Iteration 13700: Loss = -12430.48567183558
Iteration 13800: Loss = -12430.48569387274
Iteration 13900: Loss = -12430.550966939401
1
Iteration 14000: Loss = -12430.499778410644
2
Iteration 14100: Loss = -12430.495251370276
3
Iteration 14200: Loss = -12430.489810949695
4
Iteration 14300: Loss = -12430.48570081379
Iteration 14400: Loss = -12430.500802943068
1
Iteration 14500: Loss = -12430.511427431218
2
Iteration 14600: Loss = -12430.488037185847
3
Iteration 14700: Loss = -12430.50390440626
4
Iteration 14800: Loss = -12430.485473012865
Iteration 14900: Loss = -12430.488532525873
1
Iteration 15000: Loss = -12430.487937158545
2
Iteration 15100: Loss = -12430.572278669699
3
Iteration 15200: Loss = -12430.489265308914
4
Iteration 15300: Loss = -12430.494912262955
5
Iteration 15400: Loss = -12430.501490315542
6
Iteration 15500: Loss = -12430.485847677879
7
Iteration 15600: Loss = -12430.509823144248
8
Iteration 15700: Loss = -12430.485487455235
Iteration 15800: Loss = -12430.4856147625
1
Iteration 15900: Loss = -12430.496633247547
2
Iteration 16000: Loss = -12430.573556394256
3
Iteration 16100: Loss = -12430.492319573314
4
Iteration 16200: Loss = -12430.485435797913
Iteration 16300: Loss = -12430.485531454058
Iteration 16400: Loss = -12430.724217463718
1
Iteration 16500: Loss = -12430.485431636867
Iteration 16600: Loss = -12430.495462900932
1
Iteration 16700: Loss = -12430.496866284077
2
Iteration 16800: Loss = -12430.486846207043
3
Iteration 16900: Loss = -12430.512088837197
4
Iteration 17000: Loss = -12430.48545631253
Iteration 17100: Loss = -12430.486676675138
1
Iteration 17200: Loss = -12430.486016748631
2
Iteration 17300: Loss = -12430.499308266411
3
Iteration 17400: Loss = -12430.4860485355
4
Iteration 17500: Loss = -12430.485852428043
5
Iteration 17600: Loss = -12430.489515813326
6
Iteration 17700: Loss = -12430.491942965393
7
Iteration 17800: Loss = -12430.485422274107
Iteration 17900: Loss = -12430.48571809509
1
Iteration 18000: Loss = -12430.498230503961
2
Iteration 18100: Loss = -12430.524286509397
3
Iteration 18200: Loss = -12430.539803119016
4
Iteration 18300: Loss = -12430.487203826708
5
Iteration 18400: Loss = -12430.485491335467
Iteration 18500: Loss = -12430.485767526638
1
Iteration 18600: Loss = -12430.625901936297
2
Iteration 18700: Loss = -12430.485411348503
Iteration 18800: Loss = -12430.485746474667
1
Iteration 18900: Loss = -12430.485442191932
Iteration 19000: Loss = -12430.48565839956
1
Iteration 19100: Loss = -12430.5107577218
2
Iteration 19200: Loss = -12430.486105966847
3
Iteration 19300: Loss = -12430.486263087718
4
Iteration 19400: Loss = -12430.487470113785
5
Iteration 19500: Loss = -12430.486197841656
6
Iteration 19600: Loss = -12430.485791235558
7
Iteration 19700: Loss = -12430.506084482937
8
Iteration 19800: Loss = -12430.511569408143
9
Iteration 19900: Loss = -12430.48747311311
10
pi: tensor([[0.1806, 0.8194],
        [0.9505, 0.0495]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.8790e-05, 9.9998e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2110, 0.1984],
         [0.6879, 0.1944]],

        [[0.6996, 0.2322],
         [0.7090, 0.7046]],

        [[0.5516, 0.2048],
         [0.6556, 0.6762]],

        [[0.6912, 0.1916],
         [0.5560, 0.6888]],

        [[0.5574, 0.1964],
         [0.7048, 0.6602]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: -0.00036662529542480103
Average Adjusted Rand Index: -5.740191886414489e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20522.72718796474
Iteration 100: Loss = -12435.381903682559
Iteration 200: Loss = -12434.802216434902
Iteration 300: Loss = -12434.578507811968
Iteration 400: Loss = -12434.371552234592
Iteration 500: Loss = -12434.206755653488
Iteration 600: Loss = -12434.070562441135
Iteration 700: Loss = -12433.913443004922
Iteration 800: Loss = -12433.693208391927
Iteration 900: Loss = -12433.393010410897
Iteration 1000: Loss = -12433.039337523203
Iteration 1100: Loss = -12432.693445352199
Iteration 1200: Loss = -12432.302124643958
Iteration 1300: Loss = -12431.856627275223
Iteration 1400: Loss = -12431.51306223274
Iteration 1500: Loss = -12431.299971746235
Iteration 1600: Loss = -12431.16767815351
Iteration 1700: Loss = -12431.069848409361
Iteration 1800: Loss = -12430.982126720506
Iteration 1900: Loss = -12430.891490865291
Iteration 2000: Loss = -12430.796988168515
Iteration 2100: Loss = -12430.712365297208
Iteration 2200: Loss = -12430.651836915345
Iteration 2300: Loss = -12430.614590447723
Iteration 2400: Loss = -12430.591934829008
Iteration 2500: Loss = -12430.576948461416
Iteration 2600: Loss = -12430.565868700027
Iteration 2700: Loss = -12430.556840211002
Iteration 2800: Loss = -12430.548745520291
Iteration 2900: Loss = -12430.541219383644
Iteration 3000: Loss = -12430.533933752697
Iteration 3100: Loss = -12430.526857129431
Iteration 3200: Loss = -12430.520040216905
Iteration 3300: Loss = -12430.513664366305
Iteration 3400: Loss = -12430.519493772457
1
Iteration 3500: Loss = -12430.50276120719
Iteration 3600: Loss = -12430.498474407164
Iteration 3700: Loss = -12430.495044994086
Iteration 3800: Loss = -12430.492391774249
Iteration 3900: Loss = -12430.50441707381
1
Iteration 4000: Loss = -12430.489034392122
Iteration 4100: Loss = -12430.488090076826
Iteration 4200: Loss = -12430.487522945172
Iteration 4300: Loss = -12430.48710863393
Iteration 4400: Loss = -12430.486849624544
Iteration 4500: Loss = -12430.486673191232
Iteration 4600: Loss = -12430.486569540486
Iteration 4700: Loss = -12430.486509129298
Iteration 4800: Loss = -12430.486444275053
Iteration 4900: Loss = -12430.486413527955
Iteration 5000: Loss = -12430.489216837128
1
Iteration 5100: Loss = -12430.486366714355
Iteration 5200: Loss = -12430.486391934832
Iteration 5300: Loss = -12430.491690709845
1
Iteration 5400: Loss = -12430.486359348795
Iteration 5500: Loss = -12430.486335382044
Iteration 5600: Loss = -12430.487078365148
1
Iteration 5700: Loss = -12430.48633322279
Iteration 5800: Loss = -12430.486301842187
Iteration 5900: Loss = -12430.486749168846
1
Iteration 6000: Loss = -12430.486412986189
2
Iteration 6100: Loss = -12430.486862150934
3
Iteration 6200: Loss = -12430.487249105685
4
Iteration 6300: Loss = -12430.486392682094
Iteration 6400: Loss = -12430.486187295179
Iteration 6500: Loss = -12430.511530369025
1
Iteration 6600: Loss = -12430.486163906595
Iteration 6700: Loss = -12430.48611888327
Iteration 6800: Loss = -12430.486159631311
Iteration 6900: Loss = -12430.48610749303
Iteration 7000: Loss = -12430.48613812013
Iteration 7100: Loss = -12430.486209412713
Iteration 7200: Loss = -12430.48604513926
Iteration 7300: Loss = -12430.48603634099
Iteration 7400: Loss = -12430.48602804354
Iteration 7500: Loss = -12430.510983705777
1
Iteration 7600: Loss = -12430.486002143722
Iteration 7700: Loss = -12430.485964344638
Iteration 7800: Loss = -12430.486390992874
1
Iteration 7900: Loss = -12430.485971266758
Iteration 8000: Loss = -12430.488730224977
1
Iteration 8100: Loss = -12430.485930637922
Iteration 8200: Loss = -12430.57515818411
1
Iteration 8300: Loss = -12430.485886711314
Iteration 8400: Loss = -12430.494128047394
1
Iteration 8500: Loss = -12430.485858956703
Iteration 8600: Loss = -12430.486543762976
1
Iteration 8700: Loss = -12430.537679877654
2
Iteration 8800: Loss = -12430.485832938331
Iteration 8900: Loss = -12430.485844612323
Iteration 9000: Loss = -12430.490071921573
1
Iteration 9100: Loss = -12430.486325702715
2
Iteration 9200: Loss = -12430.488576218033
3
Iteration 9300: Loss = -12430.490292361234
4
Iteration 9400: Loss = -12430.48597389567
5
Iteration 9500: Loss = -12430.48577827192
Iteration 9600: Loss = -12430.508040588878
1
Iteration 9700: Loss = -12430.4857093727
Iteration 9800: Loss = -12430.487001352009
1
Iteration 9900: Loss = -12430.487781745693
2
Iteration 10000: Loss = -12430.607798374529
3
Iteration 10100: Loss = -12430.504610044929
4
Iteration 10200: Loss = -12430.550504929342
5
Iteration 10300: Loss = -12430.485657364683
Iteration 10400: Loss = -12430.512482104748
1
Iteration 10500: Loss = -12430.563457403536
2
Iteration 10600: Loss = -12430.49824088346
3
Iteration 10700: Loss = -12430.486206057736
4
Iteration 10800: Loss = -12430.4934408282
5
Iteration 10900: Loss = -12430.53088057975
6
Iteration 11000: Loss = -12430.488004796205
7
Iteration 11100: Loss = -12430.48562939757
Iteration 11200: Loss = -12430.488970687511
1
Iteration 11300: Loss = -12430.522390628705
2
Iteration 11400: Loss = -12430.501923092424
3
Iteration 11500: Loss = -12430.485920995994
4
Iteration 11600: Loss = -12430.49243263006
5
Iteration 11700: Loss = -12430.487936112853
6
Iteration 11800: Loss = -12430.501888325402
7
Iteration 11900: Loss = -12430.485590399727
Iteration 12000: Loss = -12430.492036111595
1
Iteration 12100: Loss = -12430.529388380823
2
Iteration 12200: Loss = -12430.488825400631
3
Iteration 12300: Loss = -12430.485597259294
Iteration 12400: Loss = -12430.486136269416
1
Iteration 12500: Loss = -12430.701774734407
2
Iteration 12600: Loss = -12430.48550341674
Iteration 12700: Loss = -12430.49751073645
1
Iteration 12800: Loss = -12430.485491733325
Iteration 12900: Loss = -12430.488837227671
1
Iteration 13000: Loss = -12430.485881169297
2
Iteration 13100: Loss = -12430.485808563502
3
Iteration 13200: Loss = -12430.48666377301
4
Iteration 13300: Loss = -12430.606182646314
5
Iteration 13400: Loss = -12430.485508180855
Iteration 13500: Loss = -12430.486360576268
1
Iteration 13600: Loss = -12430.485791509349
2
Iteration 13700: Loss = -12430.486005587383
3
Iteration 13800: Loss = -12430.48685456338
4
Iteration 13900: Loss = -12430.489625125658
5
Iteration 14000: Loss = -12430.486034288135
6
Iteration 14100: Loss = -12430.485518799469
Iteration 14200: Loss = -12430.48568718837
1
Iteration 14300: Loss = -12430.62157966434
2
Iteration 14400: Loss = -12430.488355057747
3
Iteration 14500: Loss = -12430.541820403292
4
Iteration 14600: Loss = -12430.485474763802
Iteration 14700: Loss = -12430.485551930435
Iteration 14800: Loss = -12430.49265706806
1
Iteration 14900: Loss = -12430.495805399842
2
Iteration 15000: Loss = -12430.490837495394
3
Iteration 15100: Loss = -12430.48541545313
Iteration 15200: Loss = -12430.48579616177
1
Iteration 15300: Loss = -12430.48578588415
2
Iteration 15400: Loss = -12430.487522822867
3
Iteration 15500: Loss = -12430.705750949443
4
Iteration 15600: Loss = -12430.485406542797
Iteration 15700: Loss = -12430.48618628763
1
Iteration 15800: Loss = -12430.48996365997
2
Iteration 15900: Loss = -12430.486233519532
3
Iteration 16000: Loss = -12430.485770322646
4
Iteration 16100: Loss = -12430.485828667688
5
Iteration 16200: Loss = -12430.487394784695
6
Iteration 16300: Loss = -12430.513034495205
7
Iteration 16400: Loss = -12430.485489565344
Iteration 16500: Loss = -12430.485596031633
1
Iteration 16600: Loss = -12430.695861339664
2
Iteration 16700: Loss = -12430.486124513689
3
Iteration 16800: Loss = -12430.48577548172
4
Iteration 16900: Loss = -12430.485772521057
5
Iteration 17000: Loss = -12430.486408082343
6
Iteration 17100: Loss = -12430.485491915157
Iteration 17200: Loss = -12430.48570103638
1
Iteration 17300: Loss = -12430.505260691512
2
Iteration 17400: Loss = -12430.508234385934
3
Iteration 17500: Loss = -12430.48542597457
Iteration 17600: Loss = -12430.48634623113
1
Iteration 17700: Loss = -12430.489504388312
2
Iteration 17800: Loss = -12430.485478843126
Iteration 17900: Loss = -12430.494357742671
1
Iteration 18000: Loss = -12430.512757628603
2
Iteration 18100: Loss = -12430.5153036617
3
Iteration 18200: Loss = -12430.491565359534
4
Iteration 18300: Loss = -12430.486755044893
5
Iteration 18400: Loss = -12430.486232064182
6
Iteration 18500: Loss = -12430.485569085673
Iteration 18600: Loss = -12430.49849090904
1
Iteration 18700: Loss = -12430.488897355155
2
Iteration 18800: Loss = -12430.48638230594
3
Iteration 18900: Loss = -12430.500767517655
4
Iteration 19000: Loss = -12430.491012349854
5
Iteration 19100: Loss = -12430.494493720042
6
Iteration 19200: Loss = -12430.678506444683
7
Iteration 19300: Loss = -12430.486083698033
8
Iteration 19400: Loss = -12430.485525213264
Iteration 19500: Loss = -12430.48608018865
1
Iteration 19600: Loss = -12430.501961902999
2
Iteration 19700: Loss = -12430.562358514326
3
Iteration 19800: Loss = -12430.498891039735
4
Iteration 19900: Loss = -12430.4855271935
pi: tensor([[0.1770, 0.8230],
        [0.9509, 0.0491]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.3378e-05, 9.9999e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2118, 0.1982],
         [0.5017, 0.1934]],

        [[0.6209, 0.2319],
         [0.7049, 0.5925]],

        [[0.7130, 0.2061],
         [0.5129, 0.7077]],

        [[0.6799, 0.1911],
         [0.7119, 0.5293]],

        [[0.7307, 0.1979],
         [0.5372, 0.5814]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: -0.00036662529542480103
Average Adjusted Rand Index: -5.740191886414489e-05
11865.25270267735
[-0.00036662529542480103, -0.00036662529542480103] [-5.740191886414489e-05, -5.740191886414489e-05] [12430.581397876287, 12430.488744523316]
-------------------------------------
This iteration is 23
True Objective function: Loss = -11795.989338659829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20686.67802096654
Iteration 100: Loss = -12341.38279943787
Iteration 200: Loss = -12340.772399887806
Iteration 300: Loss = -12340.648091193316
Iteration 400: Loss = -12340.585430818064
Iteration 500: Loss = -12340.543436516697
Iteration 600: Loss = -12340.512169475389
Iteration 700: Loss = -12340.48812818331
Iteration 800: Loss = -12340.468903529574
Iteration 900: Loss = -12340.453005598069
Iteration 1000: Loss = -12340.439269671248
Iteration 1100: Loss = -12340.426948337808
Iteration 1200: Loss = -12340.415649207624
Iteration 1300: Loss = -12340.404825329992
Iteration 1400: Loss = -12340.394153761463
Iteration 1500: Loss = -12340.383332328736
Iteration 1600: Loss = -12340.372020659785
Iteration 1700: Loss = -12340.360042692992
Iteration 1800: Loss = -12340.347290448552
Iteration 1900: Loss = -12340.333601124526
Iteration 2000: Loss = -12340.31920869811
Iteration 2100: Loss = -12340.304418998503
Iteration 2200: Loss = -12340.289683284609
Iteration 2300: Loss = -12340.275311771096
Iteration 2400: Loss = -12340.261274006567
Iteration 2500: Loss = -12340.247216605578
Iteration 2600: Loss = -12340.23234721402
Iteration 2700: Loss = -12340.215039517449
Iteration 2800: Loss = -12340.192334391462
Iteration 2900: Loss = -12340.156367357338
Iteration 3000: Loss = -12340.077933557199
Iteration 3100: Loss = -12339.872492110306
Iteration 3200: Loss = -12339.720144769033
Iteration 3300: Loss = -12339.627224182572
Iteration 3400: Loss = -12339.488373167225
Iteration 3500: Loss = -12339.119868781512
Iteration 3600: Loss = -12338.685516811105
Iteration 3700: Loss = -12338.380839059539
Iteration 3800: Loss = -12314.809159312364
Iteration 3900: Loss = -12047.917994486115
Iteration 4000: Loss = -11996.943978030562
Iteration 4100: Loss = -11942.89302327822
Iteration 4200: Loss = -11930.695741284371
Iteration 4300: Loss = -11900.896270451964
Iteration 4400: Loss = -11886.695956391646
Iteration 4500: Loss = -11842.803849855294
Iteration 4600: Loss = -11842.78742831314
Iteration 4700: Loss = -11842.779804463245
Iteration 4800: Loss = -11842.768051926161
Iteration 4900: Loss = -11842.756711639066
Iteration 5000: Loss = -11842.757147578466
1
Iteration 5100: Loss = -11842.750899434575
Iteration 5200: Loss = -11842.74894383245
Iteration 5300: Loss = -11842.747535566814
Iteration 5400: Loss = -11842.745126981585
Iteration 5500: Loss = -11842.743733988587
Iteration 5600: Loss = -11842.741822149299
Iteration 5700: Loss = -11842.740625985549
Iteration 5800: Loss = -11842.73969445703
Iteration 5900: Loss = -11842.738908625834
Iteration 6000: Loss = -11842.73818919355
Iteration 6100: Loss = -11842.7517803578
1
Iteration 6200: Loss = -11842.737011556314
Iteration 6300: Loss = -11842.737027663647
Iteration 6400: Loss = -11842.73704159645
Iteration 6500: Loss = -11842.735876659544
Iteration 6600: Loss = -11839.807127139311
Iteration 6700: Loss = -11839.803886046122
Iteration 6800: Loss = -11839.803499206262
Iteration 6900: Loss = -11839.803244702
Iteration 7000: Loss = -11839.80938187585
1
Iteration 7100: Loss = -11839.802477828169
Iteration 7200: Loss = -11839.803344415295
1
Iteration 7300: Loss = -11839.801933745532
Iteration 7400: Loss = -11839.8017612697
Iteration 7500: Loss = -11839.813474622753
1
Iteration 7600: Loss = -11839.803500208729
2
Iteration 7700: Loss = -11839.80111017789
Iteration 7800: Loss = -11839.800975050966
Iteration 7900: Loss = -11839.821107547454
1
Iteration 8000: Loss = -11839.801047342275
Iteration 8100: Loss = -11839.798445776894
Iteration 8200: Loss = -11836.247428456252
Iteration 8300: Loss = -11836.244233227577
Iteration 8400: Loss = -11836.245494868002
1
Iteration 8500: Loss = -11830.849548518721
Iteration 8600: Loss = -11830.849164255415
Iteration 8700: Loss = -11830.849962718605
1
Iteration 8800: Loss = -11830.854508286124
2
Iteration 8900: Loss = -11830.883482484096
3
Iteration 9000: Loss = -11830.848478259773
Iteration 9100: Loss = -11830.848414133283
Iteration 9200: Loss = -11830.95143825291
1
Iteration 9300: Loss = -11830.848005245167
Iteration 9400: Loss = -11830.851706769758
1
Iteration 9500: Loss = -11830.847954006997
Iteration 9600: Loss = -11830.848129544067
1
Iteration 9700: Loss = -11831.048263968283
2
Iteration 9800: Loss = -11830.856843981483
3
Iteration 9900: Loss = -11830.848206007968
4
Iteration 10000: Loss = -11830.84806637963
5
Iteration 10100: Loss = -11830.882089472494
6
Iteration 10200: Loss = -11830.845505366366
Iteration 10300: Loss = -11830.855428416015
1
Iteration 10400: Loss = -11830.843324649362
Iteration 10500: Loss = -11830.846274825453
1
Iteration 10600: Loss = -11830.856150447931
2
Iteration 10700: Loss = -11830.849978377453
3
Iteration 10800: Loss = -11830.866278290607
4
Iteration 10900: Loss = -11830.855503637626
5
Iteration 11000: Loss = -11830.84490926548
6
Iteration 11100: Loss = -11830.844112703253
7
Iteration 11200: Loss = -11830.84225260952
Iteration 11300: Loss = -11830.843438060003
1
Iteration 11400: Loss = -11830.851873852938
2
Iteration 11500: Loss = -11830.84259269566
3
Iteration 11600: Loss = -11830.85205618783
4
Iteration 11700: Loss = -11830.846652396067
5
Iteration 11800: Loss = -11830.84223304408
Iteration 11900: Loss = -11830.855780190375
1
Iteration 12000: Loss = -11830.88402682058
2
Iteration 12100: Loss = -11830.84488679569
3
Iteration 12200: Loss = -11830.840826796368
Iteration 12300: Loss = -11830.849457738954
1
Iteration 12400: Loss = -11830.840335483548
Iteration 12500: Loss = -11830.842073273652
1
Iteration 12600: Loss = -11830.866789863174
2
Iteration 12700: Loss = -11830.83810349892
Iteration 12800: Loss = -11830.81814649565
Iteration 12900: Loss = -11830.81852867679
1
Iteration 13000: Loss = -11831.028004367206
2
Iteration 13100: Loss = -11830.819988995005
3
Iteration 13200: Loss = -11830.821685433715
4
Iteration 13300: Loss = -11830.815329798355
Iteration 13400: Loss = -11830.81933351606
1
Iteration 13500: Loss = -11830.828953548657
2
Iteration 13600: Loss = -11830.81718512654
3
Iteration 13700: Loss = -11830.820215085436
4
Iteration 13800: Loss = -11830.817919243975
5
Iteration 13900: Loss = -11830.828065466545
6
Iteration 14000: Loss = -11830.841146883968
7
Iteration 14100: Loss = -11830.819879058803
8
Iteration 14200: Loss = -11830.823530785323
9
Iteration 14300: Loss = -11830.816934060242
10
Iteration 14400: Loss = -11830.816107170234
11
Iteration 14500: Loss = -11830.81953933336
12
Iteration 14600: Loss = -11830.817405466674
13
Iteration 14700: Loss = -11830.814976005613
Iteration 14800: Loss = -11830.81479792436
Iteration 14900: Loss = -11830.842084839407
1
Iteration 15000: Loss = -11830.812975945631
Iteration 15100: Loss = -11830.825167270486
1
Iteration 15200: Loss = -11830.81586571395
2
Iteration 15300: Loss = -11830.814085430336
3
Iteration 15400: Loss = -11830.811962361042
Iteration 15500: Loss = -11830.813128457294
1
Iteration 15600: Loss = -11830.93026817256
2
Iteration 15700: Loss = -11830.819031460203
3
Iteration 15800: Loss = -11830.813229285315
4
Iteration 15900: Loss = -11830.816698140292
5
Iteration 16000: Loss = -11830.855236198862
6
Iteration 16100: Loss = -11830.81453708632
7
Iteration 16200: Loss = -11830.879694133648
8
Iteration 16300: Loss = -11830.812398253629
9
Iteration 16400: Loss = -11830.939900416877
10
Iteration 16500: Loss = -11830.816461374578
11
Iteration 16600: Loss = -11830.81678460229
12
Iteration 16700: Loss = -11830.814370087144
13
Iteration 16800: Loss = -11830.824082410658
14
Iteration 16900: Loss = -11830.813517317703
15
Stopping early at iteration 16900 due to no improvement.
pi: tensor([[0.6541, 0.3459],
        [0.3468, 0.6532]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6250, 0.3750], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2998, 0.0914],
         [0.5678, 0.2979]],

        [[0.6461, 0.0962],
         [0.6629, 0.6710]],

        [[0.6846, 0.1010],
         [0.5736, 0.5801]],

        [[0.6325, 0.1003],
         [0.7027, 0.6621]],

        [[0.5696, 0.0947],
         [0.6315, 0.7213]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.34443189450463313
Average Adjusted Rand Index: 0.9601372835339955
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21138.167619142965
Iteration 100: Loss = -12341.621311579696
Iteration 200: Loss = -12340.790362710333
Iteration 300: Loss = -12340.64388990498
Iteration 400: Loss = -12340.573129943332
Iteration 500: Loss = -12340.526018417262
Iteration 600: Loss = -12340.491701253326
Iteration 700: Loss = -12340.465911569898
Iteration 800: Loss = -12340.44560820482
Iteration 900: Loss = -12340.428797354334
Iteration 1000: Loss = -12340.41431819736
Iteration 1100: Loss = -12340.40133266436
Iteration 1200: Loss = -12340.38940806958
Iteration 1300: Loss = -12340.378168860912
Iteration 1400: Loss = -12340.367320375553
Iteration 1500: Loss = -12340.356730141148
Iteration 1600: Loss = -12340.346277131173
Iteration 1700: Loss = -12340.335995677287
Iteration 1800: Loss = -12340.325771247908
Iteration 1900: Loss = -12340.315733825024
Iteration 2000: Loss = -12340.305827880296
Iteration 2100: Loss = -12340.295918910655
Iteration 2200: Loss = -12340.286072190482
Iteration 2300: Loss = -12340.276118721713
Iteration 2400: Loss = -12340.265972189893
Iteration 2500: Loss = -12340.255324539976
Iteration 2600: Loss = -12340.243835228033
Iteration 2700: Loss = -12340.230668065426
Iteration 2800: Loss = -12340.21448638158
Iteration 2900: Loss = -12340.192156408788
Iteration 3000: Loss = -12340.1543429558
Iteration 3100: Loss = -12340.061142484104
Iteration 3200: Loss = -12339.780513320136
Iteration 3300: Loss = -12339.496667946634
Iteration 3400: Loss = -12339.063479483491
Iteration 3500: Loss = -12338.61675177832
Iteration 3600: Loss = -12338.34907353559
Iteration 3700: Loss = -12338.036872192637
Iteration 3800: Loss = -12083.50534208863
Iteration 3900: Loss = -12016.669638478908
Iteration 4000: Loss = -11986.958193379765
Iteration 4100: Loss = -11933.789754352541
Iteration 4200: Loss = -11914.610230686922
Iteration 4300: Loss = -11900.843458317908
Iteration 4400: Loss = -11842.886210046086
Iteration 4500: Loss = -11842.78004531113
Iteration 4600: Loss = -11842.769978448112
Iteration 4700: Loss = -11842.762513875288
Iteration 4800: Loss = -11842.766431339747
1
Iteration 4900: Loss = -11842.754126731856
Iteration 5000: Loss = -11842.750690337405
Iteration 5100: Loss = -11842.759915234165
1
Iteration 5200: Loss = -11842.746034913946
Iteration 5300: Loss = -11842.744647936965
Iteration 5400: Loss = -11842.746485316624
1
Iteration 5500: Loss = -11842.742117372201
Iteration 5600: Loss = -11842.741106254041
Iteration 5700: Loss = -11842.74153712769
1
Iteration 5800: Loss = -11842.739811510542
Iteration 5900: Loss = -11842.738582514188
Iteration 6000: Loss = -11842.738066806829
Iteration 6100: Loss = -11842.737763359868
Iteration 6200: Loss = -11842.741201810386
1
Iteration 6300: Loss = -11842.73651471038
Iteration 6400: Loss = -11842.73750918129
1
Iteration 6500: Loss = -11842.73571373368
Iteration 6600: Loss = -11842.74056477829
1
Iteration 6700: Loss = -11842.744674789199
2
Iteration 6800: Loss = -11842.734669529396
Iteration 6900: Loss = -11842.734482905133
Iteration 7000: Loss = -11842.734053833185
Iteration 7100: Loss = -11842.73409786018
Iteration 7200: Loss = -11842.733594734504
Iteration 7300: Loss = -11842.733491256697
Iteration 7400: Loss = -11842.733166423262
Iteration 7500: Loss = -11842.733077437535
Iteration 7600: Loss = -11842.735401513248
1
Iteration 7700: Loss = -11842.759980287003
2
Iteration 7800: Loss = -11842.748679402273
3
Iteration 7900: Loss = -11842.73504219217
4
Iteration 8000: Loss = -11842.732115492203
Iteration 8100: Loss = -11842.732544006984
1
Iteration 8200: Loss = -11842.731863590949
Iteration 8300: Loss = -11842.731810548654
Iteration 8400: Loss = -11842.731631604449
Iteration 8500: Loss = -11842.731528529415
Iteration 8600: Loss = -11842.731368546105
Iteration 8700: Loss = -11842.731496263768
1
Iteration 8800: Loss = -11842.926034470353
2
Iteration 8900: Loss = -11842.736709866233
3
Iteration 9000: Loss = -11842.732083655577
4
Iteration 9100: Loss = -11842.733717172976
5
Iteration 9200: Loss = -11842.750854972997
6
Iteration 9300: Loss = -11842.73088960121
Iteration 9400: Loss = -11842.733728180057
1
Iteration 9500: Loss = -11842.731151572225
2
Iteration 9600: Loss = -11842.73068821429
Iteration 9700: Loss = -11842.73374181105
1
Iteration 9800: Loss = -11842.73608759325
2
Iteration 9900: Loss = -11842.739018520722
3
Iteration 10000: Loss = -11842.73587782262
4
Iteration 10100: Loss = -11842.733875018506
5
Iteration 10200: Loss = -11842.743612055106
6
Iteration 10300: Loss = -11842.735063381446
7
Iteration 10400: Loss = -11842.730578124121
Iteration 10500: Loss = -11842.7317460546
1
Iteration 10600: Loss = -11842.73563613907
2
Iteration 10700: Loss = -11842.728995781534
Iteration 10800: Loss = -11842.727648411701
Iteration 10900: Loss = -11839.801241824034
Iteration 11000: Loss = -11839.799458626929
Iteration 11100: Loss = -11839.804937653467
1
Iteration 11200: Loss = -11839.799974347543
2
Iteration 11300: Loss = -11839.802292251978
3
Iteration 11400: Loss = -11839.798280472576
Iteration 11500: Loss = -11839.798124415156
Iteration 11600: Loss = -11839.815304131906
1
Iteration 11700: Loss = -11839.799152590513
2
Iteration 11800: Loss = -11839.797180541731
Iteration 11900: Loss = -11834.350356057792
Iteration 12000: Loss = -11834.395319060104
1
Iteration 12100: Loss = -11834.345217022443
Iteration 12200: Loss = -11834.346144305806
1
Iteration 12300: Loss = -11834.346323446069
2
Iteration 12400: Loss = -11834.42373188482
3
Iteration 12500: Loss = -11834.347156039024
4
Iteration 12600: Loss = -11834.349018726123
5
Iteration 12700: Loss = -11834.443227914233
6
Iteration 12800: Loss = -11834.35022131818
7
Iteration 12900: Loss = -11834.347241622127
8
Iteration 13000: Loss = -11834.350317340504
9
Iteration 13100: Loss = -11834.345120440095
Iteration 13200: Loss = -11834.345417302708
1
Iteration 13300: Loss = -11834.348864683217
2
Iteration 13400: Loss = -11834.351790685561
3
Iteration 13500: Loss = -11834.37814918733
4
Iteration 13600: Loss = -11834.35756260958
5
Iteration 13700: Loss = -11834.348268448342
6
Iteration 13800: Loss = -11834.346158930774
7
Iteration 13900: Loss = -11834.345349155315
8
Iteration 14000: Loss = -11834.348499475782
9
Iteration 14100: Loss = -11834.346068972132
10
Iteration 14200: Loss = -11834.34591435308
11
Iteration 14300: Loss = -11834.364498496197
12
Iteration 14400: Loss = -11834.392770849421
13
Iteration 14500: Loss = -11834.340146981502
Iteration 14600: Loss = -11834.341276904783
1
Iteration 14700: Loss = -11834.34627433904
2
Iteration 14800: Loss = -11834.349112354794
3
Iteration 14900: Loss = -11834.342467818391
4
Iteration 15000: Loss = -11834.343955571741
5
Iteration 15100: Loss = -11834.3489055927
6
Iteration 15200: Loss = -11834.37848741796
7
Iteration 15300: Loss = -11834.419846437027
8
Iteration 15400: Loss = -11834.35593086574
9
Iteration 15500: Loss = -11834.4349359178
10
Iteration 15600: Loss = -11834.355044143344
11
Iteration 15700: Loss = -11834.340929045835
12
Iteration 15800: Loss = -11834.34024684969
Iteration 15900: Loss = -11834.358904798824
1
Iteration 16000: Loss = -11834.351336911324
2
Iteration 16100: Loss = -11834.347962389053
3
Iteration 16200: Loss = -11834.342088764653
4
Iteration 16300: Loss = -11834.34393959592
5
Iteration 16400: Loss = -11834.341919969233
6
Iteration 16500: Loss = -11834.349043513608
7
Iteration 16600: Loss = -11834.342232177785
8
Iteration 16700: Loss = -11834.349857165187
9
Iteration 16800: Loss = -11834.34370047676
10
Iteration 16900: Loss = -11834.404984527393
11
Iteration 17000: Loss = -11834.344701770951
12
Iteration 17100: Loss = -11834.343194684028
13
Iteration 17200: Loss = -11834.343175879783
14
Iteration 17300: Loss = -11834.342679362771
15
Stopping early at iteration 17300 due to no improvement.
pi: tensor([[0.6576, 0.3424],
        [0.3430, 0.6570]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6164, 0.3836], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3009, 0.0934],
         [0.5405, 0.2966]],

        [[0.6912, 0.0960],
         [0.6619, 0.7278]],

        [[0.5926, 0.1010],
         [0.7182, 0.5189]],

        [[0.5225, 0.1003],
         [0.5050, 0.6118]],

        [[0.6022, 0.0946],
         [0.6988, 0.5711]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9206245835695015
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.3491615886036737
Average Adjusted Rand Index: 0.9522846424590548
11795.989338659829
[0.34443189450463313, 0.3491615886036737] [0.9601372835339955, 0.9522846424590548] [11830.813517317703, 11834.342679362771]
-------------------------------------
This iteration is 24
True Objective function: Loss = -11771.589370453867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21595.772699674944
Iteration 100: Loss = -12245.683185027667
Iteration 200: Loss = -12245.279779211007
Iteration 300: Loss = -12245.178698379073
Iteration 400: Loss = -12245.129467189508
Iteration 500: Loss = -12245.102804778084
Iteration 600: Loss = -12245.087430393369
Iteration 700: Loss = -12245.077906179762
Iteration 800: Loss = -12245.071567985893
Iteration 900: Loss = -12245.066945858467
Iteration 1000: Loss = -12245.063324789173
Iteration 1100: Loss = -12245.060306107995
Iteration 1200: Loss = -12245.057740979222
Iteration 1300: Loss = -12245.055506197048
Iteration 1400: Loss = -12245.053380488687
Iteration 1500: Loss = -12245.0514445227
Iteration 1600: Loss = -12245.049607672403
Iteration 1700: Loss = -12245.047752034174
Iteration 1800: Loss = -12245.04593300227
Iteration 1900: Loss = -12245.044001068638
Iteration 2000: Loss = -12245.041929198806
Iteration 2100: Loss = -12245.039574314002
Iteration 2200: Loss = -12245.036863068735
Iteration 2300: Loss = -12245.033586560183
Iteration 2400: Loss = -12245.029490389388
Iteration 2500: Loss = -12245.024155570258
Iteration 2600: Loss = -12245.016834192305
Iteration 2700: Loss = -12245.006456714493
Iteration 2800: Loss = -12244.991381434153
Iteration 2900: Loss = -12244.96952452413
Iteration 3000: Loss = -12244.939093879611
Iteration 3100: Loss = -12244.896774911933
Iteration 3200: Loss = -12244.838865795948
Iteration 3300: Loss = -12244.782419018877
Iteration 3400: Loss = -12244.753195760579
Iteration 3500: Loss = -12244.73805961408
Iteration 3600: Loss = -12244.729527497613
Iteration 3700: Loss = -12244.72267669739
Iteration 3800: Loss = -12244.717765196527
Iteration 3900: Loss = -12244.713845483178
Iteration 4000: Loss = -12244.709940832538
Iteration 4100: Loss = -12244.706867616489
Iteration 4200: Loss = -12244.70311637987
Iteration 4300: Loss = -12244.70019729468
Iteration 4400: Loss = -12244.69673927759
Iteration 4500: Loss = -12244.694103999856
Iteration 4600: Loss = -12244.691219270046
Iteration 4700: Loss = -12244.691261606014
Iteration 4800: Loss = -12244.687187773694
Iteration 4900: Loss = -12244.702702606013
1
Iteration 5000: Loss = -12244.684127205897
Iteration 5100: Loss = -12244.682785332754
Iteration 5200: Loss = -12244.68268159312
Iteration 5300: Loss = -12244.680941499304
Iteration 5400: Loss = -12244.689422984156
1
Iteration 5500: Loss = -12244.679522874038
Iteration 5600: Loss = -12244.701622181517
1
Iteration 5700: Loss = -12244.678419632157
Iteration 5800: Loss = -12244.677882827757
Iteration 5900: Loss = -12244.678614876604
1
Iteration 6000: Loss = -12244.677022279982
Iteration 6100: Loss = -12244.67977416953
1
Iteration 6200: Loss = -12244.676031718654
Iteration 6300: Loss = -12244.675251024019
Iteration 6400: Loss = -12244.676494417632
1
Iteration 6500: Loss = -12244.672462283648
Iteration 6600: Loss = -12244.668984923213
Iteration 6700: Loss = -12244.660998367934
Iteration 6800: Loss = -12244.643298052093
Iteration 6900: Loss = -12244.612527099167
Iteration 7000: Loss = -12244.566165645903
Iteration 7100: Loss = -12244.520048872551
Iteration 7200: Loss = -12244.454051782132
Iteration 7300: Loss = -12244.339858618117
Iteration 7400: Loss = -12244.28899920483
Iteration 7500: Loss = -12244.268778409629
Iteration 7600: Loss = -12244.378366442395
1
Iteration 7700: Loss = -12244.253646736506
Iteration 7800: Loss = -12244.25022116171
Iteration 7900: Loss = -12244.247870651798
Iteration 8000: Loss = -12244.246112275034
Iteration 8100: Loss = -12244.249478124519
1
Iteration 8200: Loss = -12244.243804676773
Iteration 8300: Loss = -12244.24302413669
Iteration 8400: Loss = -12244.249818751832
1
Iteration 8500: Loss = -12244.247707028842
2
Iteration 8600: Loss = -12244.242128825636
Iteration 8700: Loss = -12244.240943927656
Iteration 8800: Loss = -12244.285051432666
1
Iteration 8900: Loss = -12244.240476835066
Iteration 9000: Loss = -12244.241242579372
1
Iteration 9100: Loss = -12244.239952484355
Iteration 9200: Loss = -12244.239630033155
Iteration 9300: Loss = -12244.242238999223
1
Iteration 9400: Loss = -12244.244497553447
2
Iteration 9500: Loss = -12244.293034459399
3
Iteration 9600: Loss = -12244.256478115749
4
Iteration 9700: Loss = -12244.254965810673
5
Iteration 9800: Loss = -12244.23881446567
Iteration 9900: Loss = -12244.240469148623
1
Iteration 10000: Loss = -12244.238758421161
Iteration 10100: Loss = -12244.238963789552
1
Iteration 10200: Loss = -12244.238872093194
2
Iteration 10300: Loss = -12244.238392693716
Iteration 10400: Loss = -12244.248724707977
1
Iteration 10500: Loss = -12244.241125181885
2
Iteration 10600: Loss = -12244.23881474813
3
Iteration 10700: Loss = -12244.240390298919
4
Iteration 10800: Loss = -12244.241719593883
5
Iteration 10900: Loss = -12244.24265538075
6
Iteration 11000: Loss = -12244.238004225494
Iteration 11100: Loss = -12244.238324322736
1
Iteration 11200: Loss = -12244.336428812045
2
Iteration 11300: Loss = -12244.237890130069
Iteration 11400: Loss = -12244.243458572273
1
Iteration 11500: Loss = -12244.237811092657
Iteration 11600: Loss = -12244.239636634786
1
Iteration 11700: Loss = -12244.23776148761
Iteration 11800: Loss = -12244.239278080344
1
Iteration 11900: Loss = -12244.237691267992
Iteration 12000: Loss = -12244.237649294582
Iteration 12100: Loss = -12244.241692668245
1
Iteration 12200: Loss = -12244.237605945651
Iteration 12300: Loss = -12244.237798981752
1
Iteration 12400: Loss = -12244.23764457133
Iteration 12500: Loss = -12244.245381215527
1
Iteration 12600: Loss = -12244.315655406273
2
Iteration 12700: Loss = -12244.237604036858
Iteration 12800: Loss = -12244.238561171409
1
Iteration 12900: Loss = -12244.243936881983
2
Iteration 13000: Loss = -12244.237533896701
Iteration 13100: Loss = -12244.239200111526
1
Iteration 13200: Loss = -12244.25002095213
2
Iteration 13300: Loss = -12244.249256037368
3
Iteration 13400: Loss = -12244.250536742835
4
Iteration 13500: Loss = -12244.237609097292
Iteration 13600: Loss = -12244.24306253315
1
Iteration 13700: Loss = -12244.237582618667
Iteration 13800: Loss = -12244.267303572287
1
Iteration 13900: Loss = -12244.237417254168
Iteration 14000: Loss = -12244.241377179756
1
Iteration 14100: Loss = -12244.237374712558
Iteration 14200: Loss = -12244.237990408299
1
Iteration 14300: Loss = -12244.296216138751
2
Iteration 14400: Loss = -12244.238188719086
3
Iteration 14500: Loss = -12244.285633274412
4
Iteration 14600: Loss = -12244.239202777637
5
Iteration 14700: Loss = -12244.239060642532
6
Iteration 14800: Loss = -12244.238505961255
7
Iteration 14900: Loss = -12244.23777101163
8
Iteration 15000: Loss = -12244.242532962398
9
Iteration 15100: Loss = -12244.277347511157
10
Iteration 15200: Loss = -12244.248175736126
11
Iteration 15300: Loss = -12244.243578749674
12
Iteration 15400: Loss = -12244.239685289202
13
Iteration 15500: Loss = -12244.243137451305
14
Iteration 15600: Loss = -12244.251546112046
15
Stopping early at iteration 15600 due to no improvement.
pi: tensor([[1.8937e-06, 1.0000e+00],
        [1.0000e+00, 7.0614e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9632, 0.0368], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1956, 0.2080],
         [0.6763, 0.1981]],

        [[0.7136, 0.1636],
         [0.6063, 0.5346]],

        [[0.5930, 0.1651],
         [0.6291, 0.6718]],

        [[0.6363, 0.2195],
         [0.7115, 0.5276]],

        [[0.5522, 0.1561],
         [0.6042, 0.7264]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013602253155086633
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22570.90615606693
Iteration 100: Loss = -12246.273464145983
Iteration 200: Loss = -12245.453470302513
Iteration 300: Loss = -12245.298756477981
Iteration 400: Loss = -12245.226892232356
Iteration 500: Loss = -12245.184443112028
Iteration 600: Loss = -12245.156060990394
Iteration 700: Loss = -12245.135784937987
Iteration 800: Loss = -12245.12051554256
Iteration 900: Loss = -12245.108774645432
Iteration 1000: Loss = -12245.099335030092
Iteration 1100: Loss = -12245.091703344826
Iteration 1200: Loss = -12245.085313509526
Iteration 1300: Loss = -12245.080004538935
Iteration 1400: Loss = -12245.075438608195
Iteration 1500: Loss = -12245.071573577627
Iteration 1600: Loss = -12245.068210469619
Iteration 1700: Loss = -12245.065226617862
Iteration 1800: Loss = -12245.06262812583
Iteration 1900: Loss = -12245.060320666002
Iteration 2000: Loss = -12245.058242628704
Iteration 2100: Loss = -12245.056306858485
Iteration 2200: Loss = -12245.0545198283
Iteration 2300: Loss = -12245.05284842551
Iteration 2400: Loss = -12245.051287459955
Iteration 2500: Loss = -12245.049759500434
Iteration 2600: Loss = -12245.04820273525
Iteration 2700: Loss = -12245.046648393107
Iteration 2800: Loss = -12245.045055832876
Iteration 2900: Loss = -12245.043323410238
Iteration 3000: Loss = -12245.041378093716
Iteration 3100: Loss = -12245.039274225235
Iteration 3200: Loss = -12245.036733605535
Iteration 3300: Loss = -12245.03370795779
Iteration 3400: Loss = -12245.029930713732
Iteration 3500: Loss = -12245.025126325925
Iteration 3600: Loss = -12245.01878671164
Iteration 3700: Loss = -12245.010360463579
Iteration 3800: Loss = -12244.999081736141
Iteration 3900: Loss = -12244.984020418582
Iteration 4000: Loss = -12244.964296782058
Iteration 4100: Loss = -12244.937662445778
Iteration 4200: Loss = -12244.899894081927
Iteration 4300: Loss = -12244.84966013204
Iteration 4400: Loss = -12244.80107189202
Iteration 4500: Loss = -12244.768908226943
Iteration 4600: Loss = -12244.750358449046
Iteration 4700: Loss = -12244.737754240541
Iteration 4800: Loss = -12244.731149208015
Iteration 4900: Loss = -12244.723840621555
Iteration 5000: Loss = -12244.730058068066
1
Iteration 5100: Loss = -12244.715457959257
Iteration 5200: Loss = -12244.729371657178
1
Iteration 5300: Loss = -12244.708711235604
Iteration 5400: Loss = -12244.719291515576
1
Iteration 5500: Loss = -12244.702565828107
Iteration 5600: Loss = -12244.699382241786
Iteration 5700: Loss = -12244.696863187728
Iteration 5800: Loss = -12244.694018237382
Iteration 5900: Loss = -12244.693511917056
Iteration 6000: Loss = -12244.68953020614
Iteration 6100: Loss = -12244.70132063885
1
Iteration 6200: Loss = -12244.68604908209
Iteration 6300: Loss = -12244.68537815343
Iteration 6400: Loss = -12244.683468560619
Iteration 6500: Loss = -12244.682478997765
Iteration 6600: Loss = -12244.684464961365
1
Iteration 6700: Loss = -12244.680648979505
Iteration 6800: Loss = -12244.679865169946
Iteration 6900: Loss = -12244.679259961791
Iteration 7000: Loss = -12244.678478841788
Iteration 7100: Loss = -12244.678701998635
1
Iteration 7200: Loss = -12244.676976449711
Iteration 7300: Loss = -12244.676234071325
Iteration 7400: Loss = -12244.679343333617
1
Iteration 7500: Loss = -12244.673142650441
Iteration 7600: Loss = -12244.670247746879
Iteration 7700: Loss = -12244.664503002698
Iteration 7800: Loss = -12244.779273818205
1
Iteration 7900: Loss = -12244.632119536922
Iteration 8000: Loss = -12244.601180269547
Iteration 8100: Loss = -12244.554169924237
Iteration 8200: Loss = -12244.514145792959
Iteration 8300: Loss = -12244.457646925539
Iteration 8400: Loss = -12244.356814412107
Iteration 8500: Loss = -12244.297927847045
Iteration 8600: Loss = -12244.277360793049
Iteration 8700: Loss = -12244.262783060458
Iteration 8800: Loss = -12244.256367332306
Iteration 8900: Loss = -12244.252752434937
Iteration 9000: Loss = -12244.250013644494
Iteration 9100: Loss = -12244.247716026237
Iteration 9200: Loss = -12244.253785464247
1
Iteration 9300: Loss = -12244.245173432073
Iteration 9400: Loss = -12244.257061493045
1
Iteration 9500: Loss = -12244.243174047175
Iteration 9600: Loss = -12244.45456228054
1
Iteration 9700: Loss = -12244.24195041887
Iteration 9800: Loss = -12244.391846652503
1
Iteration 9900: Loss = -12244.241116640678
Iteration 10000: Loss = -12244.240803460045
Iteration 10100: Loss = -12244.240561829774
Iteration 10200: Loss = -12244.240201737948
Iteration 10300: Loss = -12244.241619294047
1
Iteration 10400: Loss = -12244.239720991063
Iteration 10500: Loss = -12244.24452111881
1
Iteration 10600: Loss = -12244.239557342686
Iteration 10700: Loss = -12244.282432903366
1
Iteration 10800: Loss = -12244.239144255225
Iteration 10900: Loss = -12244.266954372759
1
Iteration 11000: Loss = -12244.239547877627
2
Iteration 11100: Loss = -12244.238896391062
Iteration 11200: Loss = -12244.239922329358
1
Iteration 11300: Loss = -12244.238695582511
Iteration 11400: Loss = -12244.238495933687
Iteration 11500: Loss = -12244.239772989738
1
Iteration 11600: Loss = -12244.31936987926
2
Iteration 11700: Loss = -12244.238296594056
Iteration 11800: Loss = -12244.2383807599
Iteration 11900: Loss = -12244.281622220586
1
Iteration 12000: Loss = -12244.241241040103
2
Iteration 12100: Loss = -12244.246618469359
3
Iteration 12200: Loss = -12244.3009762538
4
Iteration 12300: Loss = -12244.250429157577
5
Iteration 12400: Loss = -12244.239886401314
6
Iteration 12500: Loss = -12244.298296545852
7
Iteration 12600: Loss = -12244.285928972176
8
Iteration 12700: Loss = -12244.238905390876
9
Iteration 12800: Loss = -12244.244581379713
10
Iteration 12900: Loss = -12244.251816165764
11
Iteration 13000: Loss = -12244.240847383828
12
Iteration 13100: Loss = -12244.238103881882
Iteration 13200: Loss = -12244.239034786755
1
Iteration 13300: Loss = -12244.248350606167
2
Iteration 13400: Loss = -12244.237882442305
Iteration 13500: Loss = -12244.357095071935
1
Iteration 13600: Loss = -12244.28533050496
2
Iteration 13700: Loss = -12244.238119967995
3
Iteration 13800: Loss = -12244.25790092658
4
Iteration 13900: Loss = -12244.237628865043
Iteration 14000: Loss = -12244.257840353801
1
Iteration 14100: Loss = -12244.445684826318
2
Iteration 14200: Loss = -12244.238066503494
3
Iteration 14300: Loss = -12244.237698335499
Iteration 14400: Loss = -12244.310083754133
1
Iteration 14500: Loss = -12244.238024334722
2
Iteration 14600: Loss = -12244.290720294463
3
Iteration 14700: Loss = -12244.237902365316
4
Iteration 14800: Loss = -12244.237565667368
Iteration 14900: Loss = -12244.238780291515
1
Iteration 15000: Loss = -12244.359086593186
2
Iteration 15100: Loss = -12244.237538864041
Iteration 15200: Loss = -12244.237491420321
Iteration 15300: Loss = -12244.371650121135
1
Iteration 15400: Loss = -12244.237699721081
2
Iteration 15500: Loss = -12244.238671721456
3
Iteration 15600: Loss = -12244.316155879424
4
Iteration 15700: Loss = -12244.23761492535
5
Iteration 15800: Loss = -12244.237790998073
6
Iteration 15900: Loss = -12244.238098200956
7
Iteration 16000: Loss = -12244.238014701417
8
Iteration 16100: Loss = -12244.242673904404
9
Iteration 16200: Loss = -12244.252517614697
10
Iteration 16300: Loss = -12244.241843792102
11
Iteration 16400: Loss = -12244.273557903383
12
Iteration 16500: Loss = -12244.2375531931
Iteration 16600: Loss = -12244.359906049329
1
Iteration 16700: Loss = -12244.237442089305
Iteration 16800: Loss = -12244.238919639025
1
Iteration 16900: Loss = -12244.23737925477
Iteration 17000: Loss = -12244.23870909719
1
Iteration 17100: Loss = -12244.238312355319
2
Iteration 17200: Loss = -12244.26090820212
3
Iteration 17300: Loss = -12244.265688921027
4
Iteration 17400: Loss = -12244.279076020173
5
Iteration 17500: Loss = -12244.23761490565
6
Iteration 17600: Loss = -12244.24115225785
7
Iteration 17700: Loss = -12244.247122274068
8
Iteration 17800: Loss = -12244.244263688528
9
Iteration 17900: Loss = -12244.23735294734
Iteration 18000: Loss = -12244.2378421403
1
Iteration 18100: Loss = -12244.251079212621
2
Iteration 18200: Loss = -12244.239194284342
3
Iteration 18300: Loss = -12244.237436829277
Iteration 18400: Loss = -12244.239905601731
1
Iteration 18500: Loss = -12244.237342713708
Iteration 18600: Loss = -12244.255630615882
1
Iteration 18700: Loss = -12244.23729276993
Iteration 18800: Loss = -12244.241350327995
1
Iteration 18900: Loss = -12244.23730953964
Iteration 19000: Loss = -12244.239790352336
1
Iteration 19100: Loss = -12244.240077580922
2
Iteration 19200: Loss = -12244.254879729195
3
Iteration 19300: Loss = -12244.238497379696
4
Iteration 19400: Loss = -12244.246258883877
5
Iteration 19500: Loss = -12244.237357416
Iteration 19600: Loss = -12244.237438451926
Iteration 19700: Loss = -12244.286063152707
1
Iteration 19800: Loss = -12244.237540257724
2
Iteration 19900: Loss = -12244.237745328337
3
pi: tensor([[4.1413e-07, 1.0000e+00],
        [1.0000e+00, 1.4340e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9632, 0.0368], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.2086],
         [0.5361, 0.2000]],

        [[0.6156, 0.1626],
         [0.7087, 0.5945]],

        [[0.7254, 0.1656],
         [0.5967, 0.7138]],

        [[0.5312, 0.2192],
         [0.6536, 0.5983]],

        [[0.6707, 0.1561],
         [0.6798, 0.6713]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013602253155086633
Average Adjusted Rand Index: 0.0
11771.589370453867
[-0.0013602253155086633, -0.0013602253155086633] [0.0, 0.0] [12244.251546112046, 12244.34476348374]
-------------------------------------
This iteration is 25
True Objective function: Loss = -12008.731423540668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20407.773408672783
Iteration 100: Loss = -12481.513348305258
Iteration 200: Loss = -12481.30992705403
Iteration 300: Loss = -12481.258744505336
Iteration 400: Loss = -12481.219822913657
Iteration 500: Loss = -12481.177163754272
Iteration 600: Loss = -12481.132090910469
Iteration 700: Loss = -12481.093725337065
Iteration 800: Loss = -12481.063529909778
Iteration 900: Loss = -12481.037479939318
Iteration 1000: Loss = -12481.014977447765
Iteration 1100: Loss = -12480.997491535893
Iteration 1200: Loss = -12480.985744716372
Iteration 1300: Loss = -12480.977836988443
Iteration 1400: Loss = -12480.972289256342
Iteration 1500: Loss = -12480.967946262235
Iteration 1600: Loss = -12480.96416394863
Iteration 1700: Loss = -12480.960706432916
Iteration 1800: Loss = -12480.957182928616
Iteration 1900: Loss = -12480.953342438603
Iteration 2000: Loss = -12480.948875796536
Iteration 2100: Loss = -12480.942860591116
Iteration 2200: Loss = -12480.933784203378
Iteration 2300: Loss = -12480.917498490217
Iteration 2400: Loss = -12480.897916798313
Iteration 2500: Loss = -12480.868445814076
Iteration 2600: Loss = -12480.854056222155
Iteration 2700: Loss = -12480.844459187967
Iteration 2800: Loss = -12480.851931342111
1
Iteration 2900: Loss = -12480.833276215792
Iteration 3000: Loss = -12480.830004857151
Iteration 3100: Loss = -12480.82765304263
Iteration 3200: Loss = -12480.826043152081
Iteration 3300: Loss = -12480.824830687525
Iteration 3400: Loss = -12480.82392252865
Iteration 3500: Loss = -12480.823205896973
Iteration 3600: Loss = -12480.822685611698
Iteration 3700: Loss = -12480.822243586848
Iteration 3800: Loss = -12480.821878108596
Iteration 3900: Loss = -12480.821569772315
Iteration 4000: Loss = -12480.821318212333
Iteration 4100: Loss = -12480.82109410425
Iteration 4200: Loss = -12480.820897781325
Iteration 4300: Loss = -12480.820740857642
Iteration 4400: Loss = -12480.820571382785
Iteration 4500: Loss = -12480.820438559385
Iteration 4600: Loss = -12480.820316503805
Iteration 4700: Loss = -12480.820212869781
Iteration 4800: Loss = -12480.820142037588
Iteration 4900: Loss = -12480.820058174173
Iteration 5000: Loss = -12480.819999104537
Iteration 5100: Loss = -12480.819911823071
Iteration 5200: Loss = -12480.819839357826
Iteration 5300: Loss = -12480.819818437663
Iteration 5400: Loss = -12480.819903223237
Iteration 5500: Loss = -12480.819733377606
Iteration 5600: Loss = -12480.819697420404
Iteration 5700: Loss = -12480.81966906777
Iteration 5800: Loss = -12480.819645538126
Iteration 5900: Loss = -12480.819873078517
1
Iteration 6000: Loss = -12480.819554546988
Iteration 6100: Loss = -12480.81958200884
Iteration 6200: Loss = -12480.819520346664
Iteration 6300: Loss = -12480.819475802187
Iteration 6400: Loss = -12480.821689047323
1
Iteration 6500: Loss = -12480.81946483916
Iteration 6600: Loss = -12480.819432718557
Iteration 6700: Loss = -12480.81946595046
Iteration 6800: Loss = -12480.81943234043
Iteration 6900: Loss = -12480.819386715642
Iteration 7000: Loss = -12480.819390581411
Iteration 7100: Loss = -12480.823434370173
1
Iteration 7200: Loss = -12480.819385733239
Iteration 7300: Loss = -12480.819379920633
Iteration 7400: Loss = -12480.819698798421
1
Iteration 7500: Loss = -12480.820829255239
2
Iteration 7600: Loss = -12480.820860144651
3
Iteration 7700: Loss = -12480.819301764757
Iteration 7800: Loss = -12480.821911755316
1
Iteration 7900: Loss = -12480.823461278986
2
Iteration 8000: Loss = -12480.819283908551
Iteration 8100: Loss = -12480.821619491555
1
Iteration 8200: Loss = -12480.819244416796
Iteration 8300: Loss = -12480.8219200942
1
Iteration 8400: Loss = -12480.819296303003
Iteration 8500: Loss = -12480.826442660733
1
Iteration 8600: Loss = -12480.819235991672
Iteration 8700: Loss = -12480.889110147828
1
Iteration 8800: Loss = -12480.878909705045
2
Iteration 8900: Loss = -12480.825436389585
3
Iteration 9000: Loss = -12480.819547377805
4
Iteration 9100: Loss = -12480.822283763577
5
Iteration 9200: Loss = -12480.82833816146
6
Iteration 9300: Loss = -12480.820260730467
7
Iteration 9400: Loss = -12480.819317276193
Iteration 9500: Loss = -12480.819412383487
Iteration 9600: Loss = -12480.831908288728
1
Iteration 9700: Loss = -12480.877016900766
2
Iteration 9800: Loss = -12480.849237879058
3
Iteration 9900: Loss = -12480.846833203499
4
Iteration 10000: Loss = -12480.81997767338
5
Iteration 10100: Loss = -12480.820742765425
6
Iteration 10200: Loss = -12480.824278183807
7
Iteration 10300: Loss = -12480.878663124158
8
Iteration 10400: Loss = -12480.81922886254
Iteration 10500: Loss = -12480.836021654737
1
Iteration 10600: Loss = -12480.819210045507
Iteration 10700: Loss = -12480.823100171083
1
Iteration 10800: Loss = -12480.82430791584
2
Iteration 10900: Loss = -12480.844239241607
3
Iteration 11000: Loss = -12480.819184370273
Iteration 11100: Loss = -12480.831306266207
1
Iteration 11200: Loss = -12480.819198632058
Iteration 11300: Loss = -12480.828568808864
1
Iteration 11400: Loss = -12480.819193583287
Iteration 11500: Loss = -12480.81952069535
1
Iteration 11600: Loss = -12480.819678831649
2
Iteration 11700: Loss = -12480.81930640048
3
Iteration 11800: Loss = -12480.823513926662
4
Iteration 11900: Loss = -12480.819199467422
Iteration 12000: Loss = -12480.819205747719
Iteration 12100: Loss = -12480.826730129924
1
Iteration 12200: Loss = -12480.820891859212
2
Iteration 12300: Loss = -12480.819371271042
3
Iteration 12400: Loss = -12480.853795977968
4
Iteration 12500: Loss = -12480.819331763993
5
Iteration 12600: Loss = -12480.819682836409
6
Iteration 12700: Loss = -12480.820834767612
7
Iteration 12800: Loss = -12480.819291951142
Iteration 12900: Loss = -12480.85486494387
1
Iteration 13000: Loss = -12480.820364093768
2
Iteration 13100: Loss = -12480.820902381674
3
Iteration 13200: Loss = -12480.821984666776
4
Iteration 13300: Loss = -12480.81961665639
5
Iteration 13400: Loss = -12480.819300574945
Iteration 13500: Loss = -12480.819551144326
1
Iteration 13600: Loss = -12480.823640770413
2
Iteration 13700: Loss = -12480.835106938193
3
Iteration 13800: Loss = -12480.83151271272
4
Iteration 13900: Loss = -12480.84887312204
5
Iteration 14000: Loss = -12481.009384670611
6
Iteration 14100: Loss = -12480.819225673811
Iteration 14200: Loss = -12480.819509861649
1
Iteration 14300: Loss = -12480.84384182185
2
Iteration 14400: Loss = -12480.820608661903
3
Iteration 14500: Loss = -12480.920399856303
4
Iteration 14600: Loss = -12480.8191960191
Iteration 14700: Loss = -12480.834191160244
1
Iteration 14800: Loss = -12480.861860388157
2
Iteration 14900: Loss = -12481.016895215964
3
Iteration 15000: Loss = -12480.822225064954
4
Iteration 15100: Loss = -12480.819386805902
5
Iteration 15200: Loss = -12481.004725577242
6
Iteration 15300: Loss = -12480.819175976656
Iteration 15400: Loss = -12480.82209347914
1
Iteration 15500: Loss = -12480.819319370936
2
Iteration 15600: Loss = -12480.819249762379
Iteration 15700: Loss = -12480.81921022328
Iteration 15800: Loss = -12480.819271619102
Iteration 15900: Loss = -12480.835028251902
1
Iteration 16000: Loss = -12480.82892393285
2
Iteration 16100: Loss = -12480.819622426425
3
Iteration 16200: Loss = -12480.822846706607
4
Iteration 16300: Loss = -12480.842972018856
5
Iteration 16400: Loss = -12480.827680895662
6
Iteration 16500: Loss = -12480.82704018576
7
Iteration 16600: Loss = -12480.831846579724
8
Iteration 16700: Loss = -12480.82459787055
9
Iteration 16800: Loss = -12480.8202919476
10
Iteration 16900: Loss = -12480.82025255268
11
Iteration 17000: Loss = -12480.81983287178
12
Iteration 17100: Loss = -12480.82022996099
13
Iteration 17200: Loss = -12480.819379039676
14
Iteration 17300: Loss = -12480.82951339074
15
Stopping early at iteration 17300 due to no improvement.
pi: tensor([[0.9818, 0.0182],
        [0.9722, 0.0278]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([7.4302e-04, 9.9926e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2042, 0.2006],
         [0.6702, 0.2006]],

        [[0.7149, 0.1691],
         [0.5243, 0.6726]],

        [[0.5284, 0.1313],
         [0.6934, 0.5995]],

        [[0.6555, 0.2313],
         [0.7242, 0.6401]],

        [[0.6660, 0.2439],
         [0.6462, 0.5071]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002794124748160195
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21808.748521583966
Iteration 100: Loss = -12481.86642658569
Iteration 200: Loss = -12481.542701192177
Iteration 300: Loss = -12481.474849417891
Iteration 400: Loss = -12481.437714464291
Iteration 500: Loss = -12481.411749222267
Iteration 600: Loss = -12481.391010222336
Iteration 700: Loss = -12481.372772412278
Iteration 800: Loss = -12481.354950164461
Iteration 900: Loss = -12481.335329504238
Iteration 1000: Loss = -12481.311279371388
Iteration 1100: Loss = -12481.280760479789
Iteration 1200: Loss = -12481.246685948448
Iteration 1300: Loss = -12481.215801267537
Iteration 1400: Loss = -12481.1903740365
Iteration 1500: Loss = -12481.168152062328
Iteration 1600: Loss = -12481.14729103967
Iteration 1700: Loss = -12481.127402216447
Iteration 1800: Loss = -12481.108708514605
Iteration 1900: Loss = -12481.09167422839
Iteration 2000: Loss = -12481.07668484118
Iteration 2100: Loss = -12481.06388169655
Iteration 2200: Loss = -12481.052953906432
Iteration 2300: Loss = -12481.04331847209
Iteration 2400: Loss = -12481.03416564699
Iteration 2500: Loss = -12481.024988000026
Iteration 2600: Loss = -12481.015682091005
Iteration 2700: Loss = -12481.006600757843
Iteration 2800: Loss = -12480.998368788389
Iteration 2900: Loss = -12480.9913205571
Iteration 3000: Loss = -12480.98548736417
Iteration 3100: Loss = -12480.980774178735
Iteration 3200: Loss = -12480.976847425454
Iteration 3300: Loss = -12480.973504691585
Iteration 3400: Loss = -12480.970584838517
Iteration 3500: Loss = -12480.968037397644
Iteration 3600: Loss = -12480.965748233075
Iteration 3700: Loss = -12480.963634251619
Iteration 3800: Loss = -12480.961638077342
Iteration 3900: Loss = -12480.959824211475
Iteration 4000: Loss = -12480.958037851351
Iteration 4100: Loss = -12480.956394763027
Iteration 4200: Loss = -12480.954686022535
Iteration 4300: Loss = -12480.953046710023
Iteration 4400: Loss = -12480.951318160547
Iteration 4500: Loss = -12480.949572244728
Iteration 4600: Loss = -12480.947714266904
Iteration 4700: Loss = -12480.94567683579
Iteration 4800: Loss = -12480.943372905627
Iteration 4900: Loss = -12480.940745215605
Iteration 5000: Loss = -12480.937565652164
Iteration 5100: Loss = -12480.933675821387
Iteration 5200: Loss = -12480.928590711135
Iteration 5300: Loss = -12480.921766425727
Iteration 5400: Loss = -12480.912858125823
Iteration 5500: Loss = -12480.901386751571
Iteration 5600: Loss = -12480.889018131216
Iteration 5700: Loss = -12480.876616344234
Iteration 5800: Loss = -12480.866903308295
Iteration 5900: Loss = -12480.860275996793
Iteration 6000: Loss = -12480.854095990519
Iteration 6100: Loss = -12480.850110068544
Iteration 6200: Loss = -12480.845640403873
Iteration 6300: Loss = -12480.844237881562
Iteration 6400: Loss = -12480.837549449545
Iteration 6500: Loss = -12480.834001155326
Iteration 6600: Loss = -12480.831410125536
Iteration 6700: Loss = -12480.82908855023
Iteration 6800: Loss = -12480.82755756681
Iteration 6900: Loss = -12480.826339442234
Iteration 7000: Loss = -12480.825402743045
Iteration 7100: Loss = -12480.824644701619
Iteration 7200: Loss = -12480.824027524846
Iteration 7300: Loss = -12480.823455067664
Iteration 7400: Loss = -12480.82301911666
Iteration 7500: Loss = -12480.822621595995
Iteration 7600: Loss = -12480.82236696422
Iteration 7700: Loss = -12480.8358390738
1
Iteration 7800: Loss = -12480.821852147186
Iteration 7900: Loss = -12480.82203582682
1
Iteration 8000: Loss = -12480.849638012376
2
Iteration 8100: Loss = -12480.821065984095
Iteration 8200: Loss = -12480.821747688362
1
Iteration 8300: Loss = -12480.820820180556
Iteration 8400: Loss = -12480.821001942131
1
Iteration 8500: Loss = -12480.820519486533
Iteration 8600: Loss = -12480.820449089984
Iteration 8700: Loss = -12480.820577303322
1
Iteration 8800: Loss = -12480.849843099435
2
Iteration 8900: Loss = -12480.820189334248
Iteration 9000: Loss = -12480.835324622487
1
Iteration 9100: Loss = -12480.828184257709
2
Iteration 9200: Loss = -12480.828540560677
3
Iteration 9300: Loss = -12480.84005618609
4
Iteration 9400: Loss = -12480.820588966886
5
Iteration 9500: Loss = -12480.820618425469
6
Iteration 9600: Loss = -12480.819947751295
Iteration 9700: Loss = -12480.820473073112
1
Iteration 9800: Loss = -12480.831354291913
2
Iteration 9900: Loss = -12480.820190672714
3
Iteration 10000: Loss = -12480.82873342953
4
Iteration 10100: Loss = -12480.821749464776
5
Iteration 10200: Loss = -12480.819855944012
Iteration 10300: Loss = -12480.819881303974
Iteration 10400: Loss = -12480.820010651636
1
Iteration 10500: Loss = -12480.820816600039
2
Iteration 10600: Loss = -12480.84047248466
3
Iteration 10700: Loss = -12480.82383799499
4
Iteration 10800: Loss = -12480.826010413166
5
Iteration 10900: Loss = -12480.849898608325
6
Iteration 11000: Loss = -12480.819550563134
Iteration 11100: Loss = -12480.81958755893
Iteration 11200: Loss = -12480.819532833584
Iteration 11300: Loss = -12480.823975645624
1
Iteration 11400: Loss = -12480.823640690227
2
Iteration 11500: Loss = -12480.836437088135
3
Iteration 11600: Loss = -12480.819510496392
Iteration 11700: Loss = -12480.825082276147
1
Iteration 11800: Loss = -12480.881731469975
2
Iteration 11900: Loss = -12480.819385109646
Iteration 12000: Loss = -12480.839450170291
1
Iteration 12100: Loss = -12480.819365643296
Iteration 12200: Loss = -12480.8845045307
1
Iteration 12300: Loss = -12480.819389233784
Iteration 12400: Loss = -12480.819738445096
1
Iteration 12500: Loss = -12480.822329537008
2
Iteration 12600: Loss = -12480.949738917661
3
Iteration 12700: Loss = -12480.820686366333
4
Iteration 12800: Loss = -12480.82082030508
5
Iteration 12900: Loss = -12480.826653844777
6
Iteration 13000: Loss = -12480.949216342682
7
Iteration 13100: Loss = -12480.819536039295
8
Iteration 13200: Loss = -12480.819978463203
9
Iteration 13300: Loss = -12480.859951799297
10
Iteration 13400: Loss = -12480.819318433101
Iteration 13500: Loss = -12480.821108573573
1
Iteration 13600: Loss = -12480.819850715568
2
Iteration 13700: Loss = -12480.839845884735
3
Iteration 13800: Loss = -12481.021666688852
4
Iteration 13900: Loss = -12480.820358489715
5
Iteration 14000: Loss = -12480.820896855292
6
Iteration 14100: Loss = -12480.820025897108
7
Iteration 14200: Loss = -12480.8368901312
8
Iteration 14300: Loss = -12480.820117687297
9
Iteration 14400: Loss = -12480.819473695206
10
Iteration 14500: Loss = -12480.835647314318
11
Iteration 14600: Loss = -12480.825985303889
12
Iteration 14700: Loss = -12480.819584220702
13
Iteration 14800: Loss = -12480.821386123696
14
Iteration 14900: Loss = -12480.819290993482
Iteration 15000: Loss = -12480.830360976031
1
Iteration 15100: Loss = -12480.860389148022
2
Iteration 15200: Loss = -12480.82110844661
3
Iteration 15300: Loss = -12480.819274891439
Iteration 15400: Loss = -12480.819606320092
1
Iteration 15500: Loss = -12480.922642934294
2
Iteration 15600: Loss = -12480.819184693826
Iteration 15700: Loss = -12480.82213781355
1
Iteration 15800: Loss = -12480.81920932164
Iteration 15900: Loss = -12480.835797430169
1
Iteration 16000: Loss = -12481.00066264383
2
Iteration 16100: Loss = -12480.819189252961
Iteration 16200: Loss = -12480.819931261425
1
Iteration 16300: Loss = -12480.823326467362
2
Iteration 16400: Loss = -12480.819215924523
Iteration 16500: Loss = -12480.819319739809
1
Iteration 16600: Loss = -12480.819264816162
Iteration 16700: Loss = -12481.019811429569
1
Iteration 16800: Loss = -12480.819203769892
Iteration 16900: Loss = -12480.847933404613
1
Iteration 17000: Loss = -12480.819216064814
Iteration 17100: Loss = -12480.820831875031
1
Iteration 17200: Loss = -12480.876694441604
2
Iteration 17300: Loss = -12480.819877670647
3
Iteration 17400: Loss = -12480.819691117795
4
Iteration 17500: Loss = -12480.819500981874
5
Iteration 17600: Loss = -12480.819255525514
Iteration 17700: Loss = -12480.88136928312
1
Iteration 17800: Loss = -12480.82868170241
2
Iteration 17900: Loss = -12480.81992552115
3
Iteration 18000: Loss = -12480.835438216438
4
Iteration 18100: Loss = -12480.826604988768
5
Iteration 18200: Loss = -12480.979231344643
6
Iteration 18300: Loss = -12480.819178121521
Iteration 18400: Loss = -12480.819462813164
1
Iteration 18500: Loss = -12480.822028757018
2
Iteration 18600: Loss = -12480.8191739639
Iteration 18700: Loss = -12480.821288809468
1
Iteration 18800: Loss = -12480.82308570373
2
Iteration 18900: Loss = -12480.819182549858
Iteration 19000: Loss = -12480.831087794333
1
Iteration 19100: Loss = -12480.819186051709
Iteration 19200: Loss = -12480.82751485914
1
Iteration 19300: Loss = -12480.821302543214
2
Iteration 19400: Loss = -12480.820504181727
3
Iteration 19500: Loss = -12480.819259519218
Iteration 19600: Loss = -12480.823347444555
1
Iteration 19700: Loss = -12480.819892677504
2
Iteration 19800: Loss = -12480.819200693299
Iteration 19900: Loss = -12480.83490234437
1
pi: tensor([[0.0276, 0.9724],
        [0.0181, 0.9819]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9987, 0.0013], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2004, 0.2007],
         [0.6132, 0.2040]],

        [[0.6870, 0.1693],
         [0.6707, 0.5928]],

        [[0.5993, 0.1315],
         [0.6500, 0.7013]],

        [[0.7164, 0.2316],
         [0.6291, 0.6693]],

        [[0.7295, 0.2443],
         [0.5024, 0.7209]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002794124748160195
Average Adjusted Rand Index: 0.0
12008.731423540668
[0.002794124748160195, 0.002794124748160195] [0.0, 0.0] [12480.82951339074, 12480.819229581237]
-------------------------------------
This iteration is 26
True Objective function: Loss = -11796.160926326182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22632.512339935583
Iteration 100: Loss = -12267.61222288279
Iteration 200: Loss = -12266.687395359355
Iteration 300: Loss = -12266.477306726265
Iteration 400: Loss = -12266.391793608453
Iteration 500: Loss = -12266.340811419906
Iteration 600: Loss = -12266.304242447355
Iteration 700: Loss = -12266.275535211602
Iteration 800: Loss = -12266.25182242656
Iteration 900: Loss = -12266.231341066728
Iteration 1000: Loss = -12266.213002345661
Iteration 1100: Loss = -12266.19582222465
Iteration 1200: Loss = -12266.17891881018
Iteration 1300: Loss = -12266.161309716981
Iteration 1400: Loss = -12266.1416253668
Iteration 1500: Loss = -12266.11739195403
Iteration 1600: Loss = -12266.083391311884
Iteration 1700: Loss = -12266.025179562483
Iteration 1800: Loss = -12265.889136167149
Iteration 1900: Loss = -12265.223023658576
Iteration 2000: Loss = -12259.855058003148
Iteration 2100: Loss = -12243.689752892846
Iteration 2200: Loss = -12204.645991796006
Iteration 2300: Loss = -12200.941529148635
Iteration 2400: Loss = -12142.093527548172
Iteration 2500: Loss = -11832.488106684603
Iteration 2600: Loss = -11826.176312648851
Iteration 2700: Loss = -11825.968135207015
Iteration 2800: Loss = -11825.867654315616
Iteration 2900: Loss = -11825.58318519435
Iteration 3000: Loss = -11825.532468840822
Iteration 3100: Loss = -11825.463830948429
Iteration 3200: Loss = -11825.444622070687
Iteration 3300: Loss = -11825.423805438511
Iteration 3400: Loss = -11825.381916522547
Iteration 3500: Loss = -11825.343720346065
Iteration 3600: Loss = -11825.327798830102
Iteration 3700: Loss = -11825.342131804424
1
Iteration 3800: Loss = -11825.314209639826
Iteration 3900: Loss = -11825.31062875211
Iteration 4000: Loss = -11825.304124326693
Iteration 4100: Loss = -11825.303561508183
Iteration 4200: Loss = -11825.29668801676
Iteration 4300: Loss = -11825.288356812656
Iteration 4400: Loss = -11825.260384370216
Iteration 4500: Loss = -11825.257044822461
Iteration 4600: Loss = -11825.25611887859
Iteration 4700: Loss = -11825.252143152731
Iteration 4800: Loss = -11825.25029653105
Iteration 4900: Loss = -11825.24944371516
Iteration 5000: Loss = -11825.246942150756
Iteration 5100: Loss = -11825.24260123967
Iteration 5200: Loss = -11822.835057178801
Iteration 5300: Loss = -11822.832825723608
Iteration 5400: Loss = -11822.831363986817
Iteration 5500: Loss = -11822.829875455524
Iteration 5600: Loss = -11822.83423439267
1
Iteration 5700: Loss = -11822.827066940346
Iteration 5800: Loss = -11822.82486480983
Iteration 5900: Loss = -11822.800928875098
Iteration 6000: Loss = -11822.784397818432
Iteration 6100: Loss = -11822.783243465941
Iteration 6200: Loss = -11822.781714665767
Iteration 6300: Loss = -11822.770074113818
Iteration 6400: Loss = -11822.769063138996
Iteration 6500: Loss = -11822.768189018736
Iteration 6600: Loss = -11822.767392304393
Iteration 6700: Loss = -11822.766542494252
Iteration 6800: Loss = -11822.765917262821
Iteration 6900: Loss = -11822.69435955504
Iteration 7000: Loss = -11822.685227104663
Iteration 7100: Loss = -11822.684878629927
Iteration 7200: Loss = -11822.68448976161
Iteration 7300: Loss = -11822.684156133733
Iteration 7400: Loss = -11822.683770708485
Iteration 7500: Loss = -11822.683615136302
Iteration 7600: Loss = -11822.683039602101
Iteration 7700: Loss = -11822.68201497408
Iteration 7800: Loss = -11822.681381782997
Iteration 7900: Loss = -11822.681130230461
Iteration 8000: Loss = -11822.697363731724
1
Iteration 8100: Loss = -11822.68222469895
2
Iteration 8200: Loss = -11822.685562979877
3
Iteration 8300: Loss = -11822.680389195588
Iteration 8400: Loss = -11822.679941719274
Iteration 8500: Loss = -11822.683721475176
1
Iteration 8600: Loss = -11822.678909318503
Iteration 8700: Loss = -11822.678736586711
Iteration 8800: Loss = -11822.678666716292
Iteration 8900: Loss = -11822.67883659637
1
Iteration 9000: Loss = -11822.678731082247
Iteration 9100: Loss = -11822.677901459749
Iteration 9200: Loss = -11822.677747313526
Iteration 9300: Loss = -11822.678335322234
1
Iteration 9400: Loss = -11822.677511034342
Iteration 9500: Loss = -11822.726143120213
1
Iteration 9600: Loss = -11822.679571045339
2
Iteration 9700: Loss = -11822.688943925214
3
Iteration 9800: Loss = -11822.680853970835
4
Iteration 9900: Loss = -11822.769238709723
5
Iteration 10000: Loss = -11822.681073996358
6
Iteration 10100: Loss = -11822.686908424226
7
Iteration 10200: Loss = -11822.769655085767
8
Iteration 10300: Loss = -11822.681589105538
9
Iteration 10400: Loss = -11822.679026694203
10
Iteration 10500: Loss = -11822.678814114452
11
Iteration 10600: Loss = -11822.678783486948
12
Iteration 10700: Loss = -11822.676495086538
Iteration 10800: Loss = -11822.678533313367
1
Iteration 10900: Loss = -11822.7105653836
2
Iteration 11000: Loss = -11822.67614640411
Iteration 11100: Loss = -11822.6828535805
1
Iteration 11200: Loss = -11822.866867317161
2
Iteration 11300: Loss = -11822.675079660665
Iteration 11400: Loss = -11822.689040952408
1
Iteration 11500: Loss = -11822.679307464581
2
Iteration 11600: Loss = -11822.676131588763
3
Iteration 11700: Loss = -11822.676283463434
4
Iteration 11800: Loss = -11822.67526832419
5
Iteration 11900: Loss = -11822.681589150634
6
Iteration 12000: Loss = -11822.686933481218
7
Iteration 12100: Loss = -11822.674540736982
Iteration 12200: Loss = -11822.674697642222
1
Iteration 12300: Loss = -11822.675184434378
2
Iteration 12400: Loss = -11822.676389523751
3
Iteration 12500: Loss = -11822.681838032277
4
Iteration 12600: Loss = -11822.675358888686
5
Iteration 12700: Loss = -11822.684378688122
6
Iteration 12800: Loss = -11822.724387234462
7
Iteration 12900: Loss = -11822.67049611922
Iteration 13000: Loss = -11822.673437607298
1
Iteration 13100: Loss = -11822.671472246551
2
Iteration 13200: Loss = -11822.6779558167
3
Iteration 13300: Loss = -11822.669815217063
Iteration 13400: Loss = -11822.673707470787
1
Iteration 13500: Loss = -11822.672566783209
2
Iteration 13600: Loss = -11822.669826181427
Iteration 13700: Loss = -11822.673282222351
1
Iteration 13800: Loss = -11822.685608118481
2
Iteration 13900: Loss = -11822.671428981377
3
Iteration 14000: Loss = -11822.675863076916
4
Iteration 14100: Loss = -11822.668941506256
Iteration 14200: Loss = -11822.666325075941
Iteration 14300: Loss = -11822.679473459682
1
Iteration 14400: Loss = -11822.668370721445
2
Iteration 14500: Loss = -11822.665194797508
Iteration 14600: Loss = -11822.663713463511
Iteration 14700: Loss = -11822.663600325259
Iteration 14800: Loss = -11822.663242225892
Iteration 14900: Loss = -11822.678914551956
1
Iteration 15000: Loss = -11822.663424445627
2
Iteration 15100: Loss = -11822.671335685236
3
Iteration 15200: Loss = -11822.663282922686
Iteration 15300: Loss = -11822.665367326805
1
Iteration 15400: Loss = -11822.67651344981
2
Iteration 15500: Loss = -11822.663204127062
Iteration 15600: Loss = -11822.697313491417
1
Iteration 15700: Loss = -11822.663185431478
Iteration 15800: Loss = -11822.668226636526
1
Iteration 15900: Loss = -11822.662781883055
Iteration 16000: Loss = -11822.65745713732
Iteration 16100: Loss = -11822.659390441077
1
Iteration 16200: Loss = -11822.661482843045
2
Iteration 16300: Loss = -11822.664616370332
3
Iteration 16400: Loss = -11822.657446807269
Iteration 16500: Loss = -11822.658185309458
1
Iteration 16600: Loss = -11822.741987345455
2
Iteration 16700: Loss = -11822.657951587693
3
Iteration 16800: Loss = -11822.658388197675
4
Iteration 16900: Loss = -11822.661039563389
5
Iteration 17000: Loss = -11822.808223834038
6
Iteration 17100: Loss = -11822.6565936583
Iteration 17200: Loss = -11822.657574745745
1
Iteration 17300: Loss = -11822.82200513497
2
Iteration 17400: Loss = -11822.66096047328
3
Iteration 17500: Loss = -11822.657043938436
4
Iteration 17600: Loss = -11822.667009092835
5
Iteration 17700: Loss = -11822.656642681673
Iteration 17800: Loss = -11822.680383942597
1
Iteration 17900: Loss = -11822.660657649012
2
Iteration 18000: Loss = -11822.656774370767
3
Iteration 18100: Loss = -11822.665317647901
4
Iteration 18200: Loss = -11822.658914766987
5
Iteration 18300: Loss = -11822.780005944067
6
Iteration 18400: Loss = -11822.667959452414
7
Iteration 18500: Loss = -11822.65761202166
8
Iteration 18600: Loss = -11822.666903014642
9
Iteration 18700: Loss = -11822.676598784501
10
Iteration 18800: Loss = -11822.664966566172
11
Iteration 18900: Loss = -11822.656471344428
Iteration 19000: Loss = -11822.687688174296
1
Iteration 19100: Loss = -11822.656677492043
2
Iteration 19200: Loss = -11822.65788383541
3
Iteration 19300: Loss = -11822.70199310193
4
Iteration 19400: Loss = -11822.659625186325
5
Iteration 19500: Loss = -11822.663457407965
6
Iteration 19600: Loss = -11822.719239837475
7
Iteration 19700: Loss = -11822.664866096726
8
Iteration 19800: Loss = -11822.671288645526
9
Iteration 19900: Loss = -11822.660515858726
10
pi: tensor([[0.6456, 0.3544],
        [0.3887, 0.6113]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6140, 0.3860], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2962, 0.0908],
         [0.6687, 0.2878]],

        [[0.5181, 0.1042],
         [0.6336, 0.5477]],

        [[0.5046, 0.1033],
         [0.6311, 0.6463]],

        [[0.5147, 0.0975],
         [0.7101, 0.6019]],

        [[0.7262, 0.1001],
         [0.6259, 0.6331]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.3538780951826469
Average Adjusted Rand Index: 0.9759984326972877
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22515.381273309988
Iteration 100: Loss = -12267.448481445344
Iteration 200: Loss = -12266.553363611478
Iteration 300: Loss = -12266.3492971146
Iteration 400: Loss = -12266.283002611839
Iteration 500: Loss = -12266.252646744753
Iteration 600: Loss = -12266.233384465226
Iteration 700: Loss = -12266.218764702653
Iteration 800: Loss = -12266.206681688844
Iteration 900: Loss = -12266.196195730672
Iteration 1000: Loss = -12266.186705382086
Iteration 1100: Loss = -12266.177880763855
Iteration 1200: Loss = -12266.169578275112
Iteration 1300: Loss = -12266.161487662495
Iteration 1400: Loss = -12266.153542256228
Iteration 1500: Loss = -12266.14558992798
Iteration 1600: Loss = -12266.137361681249
Iteration 1700: Loss = -12266.128911526424
Iteration 1800: Loss = -12266.11992955373
Iteration 1900: Loss = -12266.110133173579
Iteration 2000: Loss = -12266.099008471658
Iteration 2100: Loss = -12266.085633275181
Iteration 2200: Loss = -12266.067941404413
Iteration 2300: Loss = -12266.04030868994
Iteration 2400: Loss = -12265.980993528518
Iteration 2500: Loss = -12265.727445799397
Iteration 2600: Loss = -12258.439657189609
Iteration 2700: Loss = -12202.223268037618
Iteration 2800: Loss = -12201.12816725407
Iteration 2900: Loss = -11952.338136808055
Iteration 3000: Loss = -11830.491976922827
Iteration 3100: Loss = -11823.214982795369
Iteration 3200: Loss = -11823.155003050779
Iteration 3300: Loss = -11823.118955822005
Iteration 3400: Loss = -11823.037983234572
Iteration 3500: Loss = -11823.001548397962
Iteration 3600: Loss = -11822.991068879188
Iteration 3700: Loss = -11822.980762406374
Iteration 3800: Loss = -11822.973448284147
Iteration 3900: Loss = -11822.939651113044
Iteration 4000: Loss = -11822.931208808288
Iteration 4100: Loss = -11822.938166611986
1
Iteration 4200: Loss = -11822.921559187056
Iteration 4300: Loss = -11822.911404566918
Iteration 4400: Loss = -11822.905847401318
Iteration 4500: Loss = -11822.915253227507
1
Iteration 4600: Loss = -11822.90078013067
Iteration 4700: Loss = -11822.89978225232
Iteration 4800: Loss = -11822.896965888052
Iteration 4900: Loss = -11822.852299821134
Iteration 5000: Loss = -11822.849257339338
Iteration 5100: Loss = -11822.841257668857
Iteration 5200: Loss = -11822.842035396803
1
Iteration 5300: Loss = -11822.838486988512
Iteration 5400: Loss = -11822.837352251832
Iteration 5500: Loss = -11822.841772150989
1
Iteration 5600: Loss = -11822.833866865094
Iteration 5700: Loss = -11822.833048210105
Iteration 5800: Loss = -11822.833209534727
1
Iteration 5900: Loss = -11822.831813394452
Iteration 6000: Loss = -11822.831310252737
Iteration 6100: Loss = -11822.83113638554
Iteration 6200: Loss = -11822.830200909964
Iteration 6300: Loss = -11822.82979100188
Iteration 6400: Loss = -11822.831800155462
1
Iteration 6500: Loss = -11822.82900444761
Iteration 6600: Loss = -11822.828609070308
Iteration 6700: Loss = -11822.829411758428
1
Iteration 6800: Loss = -11822.828574160834
Iteration 6900: Loss = -11822.72642985341
Iteration 7000: Loss = -11822.726060460229
Iteration 7100: Loss = -11822.725855457526
Iteration 7200: Loss = -11822.727619803905
1
Iteration 7300: Loss = -11822.723631781591
Iteration 7400: Loss = -11822.717809176496
Iteration 7500: Loss = -11822.717963253828
1
Iteration 7600: Loss = -11822.715175725158
Iteration 7700: Loss = -11822.714643134184
Iteration 7800: Loss = -11822.717704878098
1
Iteration 7900: Loss = -11822.718012372496
2
Iteration 8000: Loss = -11822.716293624397
3
Iteration 8100: Loss = -11822.713893635453
Iteration 8200: Loss = -11822.71388027537
Iteration 8300: Loss = -11822.713630871609
Iteration 8400: Loss = -11822.713559058499
Iteration 8500: Loss = -11822.713332777796
Iteration 8600: Loss = -11822.713194150858
Iteration 8700: Loss = -11822.713095675557
Iteration 8800: Loss = -11822.712878827298
Iteration 8900: Loss = -11822.713535533367
1
Iteration 9000: Loss = -11822.712091804475
Iteration 9100: Loss = -11822.828755321574
1
Iteration 9200: Loss = -11822.711817701156
Iteration 9300: Loss = -11822.711744601258
Iteration 9400: Loss = -11822.724654404392
1
Iteration 9500: Loss = -11822.711338339166
Iteration 9600: Loss = -11822.71113930969
Iteration 9700: Loss = -11822.711800965393
1
Iteration 9800: Loss = -11822.898364314146
2
Iteration 9900: Loss = -11822.717407862932
3
Iteration 10000: Loss = -11822.71318883468
4
Iteration 10100: Loss = -11822.710928578692
Iteration 10200: Loss = -11822.710026868028
Iteration 10300: Loss = -11822.71985864628
1
Iteration 10400: Loss = -11822.726193428802
2
Iteration 10500: Loss = -11822.710627713752
3
Iteration 10600: Loss = -11822.710134791918
4
Iteration 10700: Loss = -11822.719703145909
5
Iteration 10800: Loss = -11822.709934088907
Iteration 10900: Loss = -11822.715728473228
1
Iteration 11000: Loss = -11822.778097961696
2
Iteration 11100: Loss = -11822.72247327348
3
Iteration 11200: Loss = -11822.71685408193
4
Iteration 11300: Loss = -11822.70951021245
Iteration 11400: Loss = -11822.713971797697
1
Iteration 11500: Loss = -11822.72223128702
2
Iteration 11600: Loss = -11822.726470242786
3
Iteration 11700: Loss = -11822.710679673113
4
Iteration 11800: Loss = -11822.71052586936
5
Iteration 11900: Loss = -11822.710723809385
6
Iteration 12000: Loss = -11822.710783906601
7
Iteration 12100: Loss = -11822.710064147492
8
Iteration 12200: Loss = -11822.711370916635
9
Iteration 12300: Loss = -11822.710048313813
10
Iteration 12400: Loss = -11822.7093799754
Iteration 12500: Loss = -11822.716160240087
1
Iteration 12600: Loss = -11822.832435809847
2
Iteration 12700: Loss = -11822.709379755843
Iteration 12800: Loss = -11822.70994993331
1
Iteration 12900: Loss = -11822.715155829632
2
Iteration 13000: Loss = -11822.756851332286
3
Iteration 13100: Loss = -11822.724968420598
4
Iteration 13200: Loss = -11822.711219184508
5
Iteration 13300: Loss = -11822.715244901845
6
Iteration 13400: Loss = -11822.70861476002
Iteration 13500: Loss = -11822.720736384688
1
Iteration 13600: Loss = -11822.70829030786
Iteration 13700: Loss = -11822.709017760391
1
Iteration 13800: Loss = -11822.712893578959
2
Iteration 13900: Loss = -11822.72871144525
3
Iteration 14000: Loss = -11822.718496590336
4
Iteration 14100: Loss = -11822.708286599829
Iteration 14200: Loss = -11822.710899388201
1
Iteration 14300: Loss = -11822.782420728789
2
Iteration 14400: Loss = -11822.708181510705
Iteration 14500: Loss = -11822.709994229606
1
Iteration 14600: Loss = -11822.708927521588
2
Iteration 14700: Loss = -11822.708609777113
3
Iteration 14800: Loss = -11822.720891811148
4
Iteration 14900: Loss = -11822.708357041935
5
Iteration 15000: Loss = -11822.710221749414
6
Iteration 15100: Loss = -11822.711041846374
7
Iteration 15200: Loss = -11822.794250941679
8
Iteration 15300: Loss = -11822.717104750367
9
Iteration 15400: Loss = -11822.7131199989
10
Iteration 15500: Loss = -11822.713114002929
11
Iteration 15600: Loss = -11822.708561881189
12
Iteration 15700: Loss = -11822.711504125153
13
Iteration 15800: Loss = -11822.70193010447
Iteration 15900: Loss = -11822.707994732056
1
Iteration 16000: Loss = -11822.815705509873
2
Iteration 16100: Loss = -11822.730680911927
3
Iteration 16200: Loss = -11822.701379258877
Iteration 16300: Loss = -11822.701618226953
1
Iteration 16400: Loss = -11822.731522484044
2
Iteration 16500: Loss = -11822.701389785278
Iteration 16600: Loss = -11822.716410646548
1
Iteration 16700: Loss = -11822.701523442845
2
Iteration 16800: Loss = -11822.701243619726
Iteration 16900: Loss = -11822.717943927119
1
Iteration 17000: Loss = -11822.760460267302
2
Iteration 17100: Loss = -11822.700804798875
Iteration 17200: Loss = -11822.701222416299
1
Iteration 17300: Loss = -11822.702074379777
2
Iteration 17400: Loss = -11822.726323056053
3
Iteration 17500: Loss = -11822.706322354868
4
Iteration 17600: Loss = -11822.701472065395
5
Iteration 17700: Loss = -11822.745033968764
6
Iteration 17800: Loss = -11822.703135932554
7
Iteration 17900: Loss = -11822.700253012701
Iteration 18000: Loss = -11822.700011424353
Iteration 18100: Loss = -11822.706856198065
1
Iteration 18200: Loss = -11822.732635761753
2
Iteration 18300: Loss = -11822.69988907847
Iteration 18400: Loss = -11822.700798333544
1
Iteration 18500: Loss = -11822.7012831013
2
Iteration 18600: Loss = -11822.699909124702
Iteration 18700: Loss = -11822.702556198805
1
Iteration 18800: Loss = -11822.682146588999
Iteration 18900: Loss = -11822.736805177303
1
Iteration 19000: Loss = -11822.675918947562
Iteration 19100: Loss = -11822.670298486164
Iteration 19200: Loss = -11822.677709784122
1
Iteration 19300: Loss = -11822.766743764267
2
Iteration 19400: Loss = -11822.736935016866
3
Iteration 19500: Loss = -11822.66701306644
Iteration 19600: Loss = -11822.668687926749
1
Iteration 19700: Loss = -11822.683858752447
2
Iteration 19800: Loss = -11822.666703344796
Iteration 19900: Loss = -11822.66892659667
1
pi: tensor([[0.6075, 0.3925],
        [0.3502, 0.6498]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3832, 0.6168], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2889, 0.0899],
         [0.5211, 0.2949]],

        [[0.6087, 0.1040],
         [0.7067, 0.6041]],

        [[0.6755, 0.1030],
         [0.7044, 0.7032]],

        [[0.5622, 0.0973],
         [0.5998, 0.5746]],

        [[0.7156, 0.1001],
         [0.6738, 0.5264]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.3538780951826469
Average Adjusted Rand Index: 0.9759984326972877
11796.160926326182
[0.3538780951826469, 0.3538780951826469] [0.9759984326972877, 0.9759984326972877] [11822.710041822718, 11822.672444973055]
-------------------------------------
This iteration is 27
True Objective function: Loss = -11814.608479284829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21699.500056106797
Iteration 100: Loss = -12278.438391952457
Iteration 200: Loss = -12277.881213689869
Iteration 300: Loss = -12277.718424851339
Iteration 400: Loss = -12277.623469853272
Iteration 500: Loss = -12277.554981819418
Iteration 600: Loss = -12277.498789578212
Iteration 700: Loss = -12277.44956627295
Iteration 800: Loss = -12277.404945201415
Iteration 900: Loss = -12277.363538056345
Iteration 1000: Loss = -12277.323709986436
Iteration 1100: Loss = -12277.283530956443
Iteration 1200: Loss = -12277.240584013587
Iteration 1300: Loss = -12277.192843654739
Iteration 1400: Loss = -12277.139991450887
Iteration 1500: Loss = -12277.084689878475
Iteration 1600: Loss = -12277.030256966684
Iteration 1700: Loss = -12276.977001203979
Iteration 1800: Loss = -12276.9218555837
Iteration 1900: Loss = -12276.860206068908
Iteration 2000: Loss = -12276.787072348705
Iteration 2100: Loss = -12276.69956941187
Iteration 2200: Loss = -12276.601942846293
Iteration 2300: Loss = -12276.50703402493
Iteration 2400: Loss = -12276.429820691006
Iteration 2500: Loss = -12276.374839865171
Iteration 2600: Loss = -12276.338453536231
Iteration 2700: Loss = -12276.313945639962
Iteration 2800: Loss = -12276.297427834179
Iteration 2900: Loss = -12276.286460328956
Iteration 3000: Loss = -12276.27865196186
Iteration 3100: Loss = -12276.272635830743
Iteration 3200: Loss = -12276.26766372744
Iteration 3300: Loss = -12276.263433964788
Iteration 3400: Loss = -12276.259776065672
Iteration 3500: Loss = -12276.256505700598
Iteration 3600: Loss = -12276.25362941749
Iteration 3700: Loss = -12276.250990117423
Iteration 3800: Loss = -12276.248613944144
Iteration 3900: Loss = -12276.246455613807
Iteration 4000: Loss = -12276.244451126468
Iteration 4100: Loss = -12276.242652518253
Iteration 4200: Loss = -12276.240995541923
Iteration 4300: Loss = -12276.239414030424
Iteration 4400: Loss = -12276.238034076168
Iteration 4500: Loss = -12276.236686590388
Iteration 4600: Loss = -12276.2354959201
Iteration 4700: Loss = -12276.234382826486
Iteration 4800: Loss = -12276.23334111387
Iteration 4900: Loss = -12276.232338489805
Iteration 5000: Loss = -12276.231443846
Iteration 5100: Loss = -12276.230581624843
Iteration 5200: Loss = -12276.229816285377
Iteration 5300: Loss = -12276.229081945636
Iteration 5400: Loss = -12276.228425083827
Iteration 5500: Loss = -12276.22774023914
Iteration 5600: Loss = -12276.227165192606
Iteration 5700: Loss = -12276.226563111211
Iteration 5800: Loss = -12276.226037042818
Iteration 5900: Loss = -12276.225569900686
Iteration 6000: Loss = -12276.225130972523
Iteration 6100: Loss = -12276.224690293171
Iteration 6200: Loss = -12276.224994901973
1
Iteration 6300: Loss = -12276.22387567421
Iteration 6400: Loss = -12276.223533499335
Iteration 6500: Loss = -12276.223149161682
Iteration 6600: Loss = -12276.226205263269
1
Iteration 6700: Loss = -12276.222551823388
Iteration 6800: Loss = -12276.222261642484
Iteration 6900: Loss = -12276.221943881412
Iteration 7000: Loss = -12276.221756118885
Iteration 7100: Loss = -12276.227825336786
1
Iteration 7200: Loss = -12276.238289491987
2
Iteration 7300: Loss = -12276.2239295938
3
Iteration 7400: Loss = -12276.225446106333
4
Iteration 7500: Loss = -12276.220637996958
Iteration 7600: Loss = -12276.220554481217
Iteration 7700: Loss = -12276.221088816217
1
Iteration 7800: Loss = -12276.220165021292
Iteration 7900: Loss = -12276.21998995584
Iteration 8000: Loss = -12276.220100614739
1
Iteration 8100: Loss = -12276.219717957756
Iteration 8200: Loss = -12276.224921521698
1
Iteration 8300: Loss = -12276.219469406327
Iteration 8400: Loss = -12276.21935516283
Iteration 8500: Loss = -12276.219561994758
1
Iteration 8600: Loss = -12276.219118273682
Iteration 8700: Loss = -12276.219006814988
Iteration 8800: Loss = -12276.218961554609
Iteration 8900: Loss = -12276.218807169076
Iteration 9000: Loss = -12276.218865024755
Iteration 9100: Loss = -12276.21870050795
Iteration 9200: Loss = -12276.218621530412
Iteration 9300: Loss = -12276.38389565845
1
Iteration 9400: Loss = -12276.218481709155
Iteration 9500: Loss = -12276.218389282805
Iteration 9600: Loss = -12276.634790732835
1
Iteration 9700: Loss = -12276.218284196879
Iteration 9800: Loss = -12276.21823312843
Iteration 9900: Loss = -12276.226078523616
1
Iteration 10000: Loss = -12276.218106136736
Iteration 10100: Loss = -12276.218102073746
Iteration 10200: Loss = -12276.242685287205
1
Iteration 10300: Loss = -12276.21799761336
Iteration 10400: Loss = -12276.217937229692
Iteration 10500: Loss = -12276.219067626971
1
Iteration 10600: Loss = -12276.217841425136
Iteration 10700: Loss = -12276.217826539612
Iteration 10800: Loss = -12276.218719968443
1
Iteration 10900: Loss = -12276.217728344509
Iteration 11000: Loss = -12276.221199137151
1
Iteration 11100: Loss = -12276.217980655623
2
Iteration 11200: Loss = -12276.217709358212
Iteration 11300: Loss = -12276.220029206108
1
Iteration 11400: Loss = -12276.218790870607
2
Iteration 11500: Loss = -12276.217631666772
Iteration 11600: Loss = -12276.21789116873
1
Iteration 11700: Loss = -12276.217576120467
Iteration 11800: Loss = -12276.223748528082
1
Iteration 11900: Loss = -12276.217499403327
Iteration 12000: Loss = -12276.240591831856
1
Iteration 12100: Loss = -12276.219788712704
2
Iteration 12200: Loss = -12276.217558435028
Iteration 12300: Loss = -12276.224067390754
1
Iteration 12400: Loss = -12276.219345030479
2
Iteration 12500: Loss = -12276.224800752301
3
Iteration 12600: Loss = -12276.217897194485
4
Iteration 12700: Loss = -12276.250817169628
5
Iteration 12800: Loss = -12276.217507250185
Iteration 12900: Loss = -12276.326437461577
1
Iteration 13000: Loss = -12276.256911909999
2
Iteration 13100: Loss = -12276.217429935783
Iteration 13200: Loss = -12276.218659780405
1
Iteration 13300: Loss = -12276.21754610207
2
Iteration 13400: Loss = -12276.217344741004
Iteration 13500: Loss = -12276.21872458618
1
Iteration 13600: Loss = -12276.217354690752
Iteration 13700: Loss = -12276.217657063333
1
Iteration 13800: Loss = -12276.217432042125
Iteration 13900: Loss = -12276.220649860546
1
Iteration 14000: Loss = -12276.217331047617
Iteration 14100: Loss = -12276.229468915011
1
Iteration 14200: Loss = -12276.247703092882
2
Iteration 14300: Loss = -12276.217244099129
Iteration 14400: Loss = -12276.25203920934
1
Iteration 14500: Loss = -12276.217287645984
Iteration 14600: Loss = -12276.22135567245
1
Iteration 14700: Loss = -12276.229238865752
2
Iteration 14800: Loss = -12276.217233346006
Iteration 14900: Loss = -12276.217359150543
1
Iteration 15000: Loss = -12276.217354244938
2
Iteration 15100: Loss = -12276.328122397394
3
Iteration 15200: Loss = -12276.21823508624
4
Iteration 15300: Loss = -12276.21741155911
5
Iteration 15400: Loss = -12276.22983712033
6
Iteration 15500: Loss = -12276.217227296445
Iteration 15600: Loss = -12276.217358702663
1
Iteration 15700: Loss = -12276.2193957088
2
Iteration 15800: Loss = -12276.21786779497
3
Iteration 15900: Loss = -12276.235378662654
4
Iteration 16000: Loss = -12276.245923787921
5
Iteration 16100: Loss = -12276.41599387657
6
Iteration 16200: Loss = -12276.220080865543
7
Iteration 16300: Loss = -12276.232237238446
8
Iteration 16400: Loss = -12276.217580116872
9
Iteration 16500: Loss = -12276.524505815558
10
Iteration 16600: Loss = -12276.217187773427
Iteration 16700: Loss = -12276.432731093655
1
Iteration 16800: Loss = -12276.217198367904
Iteration 16900: Loss = -12276.592642191552
1
Iteration 17000: Loss = -12276.217169740808
Iteration 17100: Loss = -12276.217164834376
Iteration 17200: Loss = -12276.217238923244
Iteration 17300: Loss = -12276.217154682492
Iteration 17400: Loss = -12276.55332369253
1
Iteration 17500: Loss = -12276.217159403037
Iteration 17600: Loss = -12276.217180690226
Iteration 17700: Loss = -12276.218463034631
1
Iteration 17800: Loss = -12276.217323966688
2
Iteration 17900: Loss = -12276.21722433544
Iteration 18000: Loss = -12276.217242688914
Iteration 18100: Loss = -12276.217349624298
1
Iteration 18200: Loss = -12276.218611030516
2
Iteration 18300: Loss = -12276.217181477296
Iteration 18400: Loss = -12276.221448395574
1
Iteration 18500: Loss = -12276.21718408551
Iteration 18600: Loss = -12276.225746787813
1
Iteration 18700: Loss = -12276.217165561278
Iteration 18800: Loss = -12276.238543273268
1
Iteration 18900: Loss = -12276.217177504475
Iteration 19000: Loss = -12276.226978140809
1
Iteration 19100: Loss = -12276.217201052837
Iteration 19200: Loss = -12276.241346820541
1
Iteration 19300: Loss = -12276.21717407217
Iteration 19400: Loss = -12276.21857831418
1
Iteration 19500: Loss = -12276.217165262366
Iteration 19600: Loss = -12276.217180219093
Iteration 19700: Loss = -12276.218801161696
1
Iteration 19800: Loss = -12276.217363645954
2
Iteration 19900: Loss = -12276.336794167662
3
pi: tensor([[6.5221e-01, 3.4779e-01],
        [8.3281e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1136, 0.8864], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2536, 0.2124],
         [0.5694, 0.1943]],

        [[0.5365, 0.2408],
         [0.5160, 0.5612]],

        [[0.5032, 0.2298],
         [0.5319, 0.6241]],

        [[0.6159, 0.1844],
         [0.7143, 0.6789]],

        [[0.6806, 0.2054],
         [0.5718, 0.6876]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005221098356234914
Average Adjusted Rand Index: -0.0017427542642344796
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25104.014405822443
Iteration 100: Loss = -12277.806493117621
Iteration 200: Loss = -12277.499074243073
Iteration 300: Loss = -12277.433277202492
Iteration 400: Loss = -12277.399650486903
Iteration 500: Loss = -12277.37311150377
Iteration 600: Loss = -12277.343729253653
Iteration 700: Loss = -12277.299382255269
Iteration 800: Loss = -12277.202978256506
Iteration 900: Loss = -12276.90125531639
Iteration 1000: Loss = -12276.615929382673
Iteration 1100: Loss = -12276.454435634983
Iteration 1200: Loss = -12276.348501982855
Iteration 1300: Loss = -12276.286316707194
Iteration 1400: Loss = -12276.256483932068
Iteration 1500: Loss = -12276.244879548467
Iteration 1600: Loss = -12276.239888722366
Iteration 1700: Loss = -12276.237173297204
Iteration 1800: Loss = -12276.235279527888
Iteration 1900: Loss = -12276.233794378131
Iteration 2000: Loss = -12276.23239673796
Iteration 2100: Loss = -12276.231371377626
Iteration 2200: Loss = -12276.230465194678
Iteration 2300: Loss = -12276.229645228284
Iteration 2400: Loss = -12276.228888863037
Iteration 2500: Loss = -12276.228186402528
Iteration 2600: Loss = -12276.227510828176
Iteration 2700: Loss = -12276.226891481472
Iteration 2800: Loss = -12276.226326030506
Iteration 2900: Loss = -12276.2258174772
Iteration 3000: Loss = -12276.225308349713
Iteration 3100: Loss = -12276.224797862096
Iteration 3200: Loss = -12276.224390551992
Iteration 3300: Loss = -12276.223964234905
Iteration 3400: Loss = -12276.22355218978
Iteration 3500: Loss = -12276.223181820114
Iteration 3600: Loss = -12276.222832976859
Iteration 3700: Loss = -12276.22258725409
Iteration 3800: Loss = -12276.22220879013
Iteration 3900: Loss = -12276.222084838693
Iteration 4000: Loss = -12276.221696320094
Iteration 4100: Loss = -12276.225466969387
1
Iteration 4200: Loss = -12276.221208530784
Iteration 4300: Loss = -12276.221011609734
Iteration 4400: Loss = -12276.220770189251
Iteration 4500: Loss = -12276.22060001155
Iteration 4600: Loss = -12276.220358562257
Iteration 4700: Loss = -12276.220272351684
Iteration 4800: Loss = -12276.22003981402
Iteration 4900: Loss = -12276.219971534982
Iteration 5000: Loss = -12276.219780136349
Iteration 5100: Loss = -12276.219815092847
Iteration 5200: Loss = -12276.219476907152
Iteration 5300: Loss = -12276.219366153651
Iteration 5400: Loss = -12276.219246612322
Iteration 5500: Loss = -12276.219137392813
Iteration 5600: Loss = -12276.219064546256
Iteration 5700: Loss = -12276.218921690772
Iteration 5800: Loss = -12276.218880598206
Iteration 5900: Loss = -12276.218786409658
Iteration 6000: Loss = -12276.219009830362
1
Iteration 6100: Loss = -12276.21861830844
Iteration 6200: Loss = -12276.228249151807
1
Iteration 6300: Loss = -12276.21846265149
Iteration 6400: Loss = -12276.224367164195
1
Iteration 6500: Loss = -12276.218352315234
Iteration 6600: Loss = -12276.219896500903
1
Iteration 6700: Loss = -12276.218235760036
Iteration 6800: Loss = -12276.21876076797
1
Iteration 6900: Loss = -12276.218122795777
Iteration 7000: Loss = -12276.21809809496
Iteration 7100: Loss = -12276.21802558473
Iteration 7200: Loss = -12276.217971364222
Iteration 7300: Loss = -12276.217990022578
Iteration 7400: Loss = -12276.21789528258
Iteration 7500: Loss = -12276.218059914378
1
Iteration 7600: Loss = -12276.217890994858
Iteration 7700: Loss = -12276.217820804675
Iteration 7800: Loss = -12276.21778421712
Iteration 7900: Loss = -12276.217823397616
Iteration 8000: Loss = -12276.21799827269
1
Iteration 8100: Loss = -12276.271508440519
2
Iteration 8200: Loss = -12276.22347247625
3
Iteration 8300: Loss = -12276.217935864475
4
Iteration 8400: Loss = -12276.2176225992
Iteration 8500: Loss = -12276.224340861003
1
Iteration 8600: Loss = -12276.21759088986
Iteration 8700: Loss = -12276.21784767033
1
Iteration 8800: Loss = -12276.21751442224
Iteration 8900: Loss = -12276.23343658939
1
Iteration 9000: Loss = -12276.217462827066
Iteration 9100: Loss = -12276.21748770712
Iteration 9200: Loss = -12276.217518318505
Iteration 9300: Loss = -12276.217437866875
Iteration 9400: Loss = -12276.282496013273
1
Iteration 9500: Loss = -12276.217440592047
Iteration 9600: Loss = -12276.217556478427
1
Iteration 9700: Loss = -12276.217419056697
Iteration 9800: Loss = -12276.217376313038
Iteration 9900: Loss = -12276.217339496035
Iteration 10000: Loss = -12276.253697505575
1
Iteration 10100: Loss = -12276.217352980915
Iteration 10200: Loss = -12276.217332613867
Iteration 10300: Loss = -12276.226201936384
1
Iteration 10400: Loss = -12276.217330234233
Iteration 10500: Loss = -12276.217727084919
1
Iteration 10600: Loss = -12276.246581017595
2
Iteration 10700: Loss = -12276.217311518089
Iteration 10800: Loss = -12276.218393006622
1
Iteration 10900: Loss = -12276.218253270628
2
Iteration 11000: Loss = -12276.217342366639
Iteration 11100: Loss = -12276.217737314122
1
Iteration 11200: Loss = -12276.279999211776
2
Iteration 11300: Loss = -12276.222442876582
3
Iteration 11400: Loss = -12276.282145537343
4
Iteration 11500: Loss = -12276.23457375038
5
Iteration 11600: Loss = -12276.33642878136
6
Iteration 11700: Loss = -12276.221260350214
7
Iteration 11800: Loss = -12276.220152004225
8
Iteration 11900: Loss = -12276.218331135537
9
Iteration 12000: Loss = -12276.219354643017
10
Iteration 12100: Loss = -12276.217407099937
Iteration 12200: Loss = -12276.21747372782
Iteration 12300: Loss = -12276.217219180879
Iteration 12400: Loss = -12276.25948366849
1
Iteration 12500: Loss = -12276.217251137807
Iteration 12600: Loss = -12276.226970763468
1
Iteration 12700: Loss = -12276.226323238254
2
Iteration 12800: Loss = -12276.219778818717
3
Iteration 12900: Loss = -12276.223286642573
4
Iteration 13000: Loss = -12276.221641671495
5
Iteration 13100: Loss = -12276.219030611646
6
Iteration 13200: Loss = -12276.218798498352
7
Iteration 13300: Loss = -12276.217602028746
8
Iteration 13400: Loss = -12276.217320444923
Iteration 13500: Loss = -12276.226059929968
1
Iteration 13600: Loss = -12276.225400124205
2
Iteration 13700: Loss = -12276.22079900117
3
Iteration 13800: Loss = -12276.388660858007
4
Iteration 13900: Loss = -12276.232048019001
5
Iteration 14000: Loss = -12276.217176395685
Iteration 14100: Loss = -12276.217196664411
Iteration 14200: Loss = -12276.217146529869
Iteration 14300: Loss = -12276.217572656251
1
Iteration 14400: Loss = -12276.23737725588
2
Iteration 14500: Loss = -12276.217630689733
3
Iteration 14600: Loss = -12276.405752618946
4
Iteration 14700: Loss = -12276.21719703989
Iteration 14800: Loss = -12276.217530744294
1
Iteration 14900: Loss = -12276.217190236592
Iteration 15000: Loss = -12276.217530708047
1
Iteration 15100: Loss = -12276.283144641593
2
Iteration 15200: Loss = -12276.217228797792
Iteration 15300: Loss = -12276.286815812138
1
Iteration 15400: Loss = -12276.217162018267
Iteration 15500: Loss = -12276.219830107571
1
Iteration 15600: Loss = -12276.217146848467
Iteration 15700: Loss = -12276.249393476219
1
Iteration 15800: Loss = -12276.217150320648
Iteration 15900: Loss = -12276.218829812944
1
Iteration 16000: Loss = -12276.221771791588
2
Iteration 16100: Loss = -12276.217503055468
3
Iteration 16200: Loss = -12276.224381952043
4
Iteration 16300: Loss = -12276.2172496436
Iteration 16400: Loss = -12276.266857834979
1
Iteration 16500: Loss = -12276.217185006608
Iteration 16600: Loss = -12276.218036535485
1
Iteration 16700: Loss = -12276.260189258644
2
Iteration 16800: Loss = -12276.2171842162
Iteration 16900: Loss = -12276.220699933045
1
Iteration 17000: Loss = -12276.217178196026
Iteration 17100: Loss = -12276.21716478893
Iteration 17200: Loss = -12276.247342370472
1
Iteration 17300: Loss = -12276.226504721462
2
Iteration 17400: Loss = -12276.227825267682
3
Iteration 17500: Loss = -12276.599874467254
4
Iteration 17600: Loss = -12276.217161872879
Iteration 17700: Loss = -12276.217410644611
1
Iteration 17800: Loss = -12276.217617900333
2
Iteration 17900: Loss = -12276.222945052275
3
Iteration 18000: Loss = -12276.218367425756
4
Iteration 18100: Loss = -12276.290931874295
5
Iteration 18200: Loss = -12276.247919584022
6
Iteration 18300: Loss = -12276.217254422172
Iteration 18400: Loss = -12276.217513982527
1
Iteration 18500: Loss = -12276.246382203632
2
Iteration 18600: Loss = -12276.220452378915
3
Iteration 18700: Loss = -12276.21751481934
4
Iteration 18800: Loss = -12276.22266519378
5
Iteration 18900: Loss = -12276.230010084142
6
Iteration 19000: Loss = -12276.227748523535
7
Iteration 19100: Loss = -12276.2271697242
8
Iteration 19200: Loss = -12276.218797764175
9
Iteration 19300: Loss = -12276.219211325824
10
Iteration 19400: Loss = -12276.217171785076
Iteration 19500: Loss = -12276.241326866431
1
Iteration 19600: Loss = -12276.259515035399
2
Iteration 19700: Loss = -12276.223498131689
3
Iteration 19800: Loss = -12276.237814776361
4
Iteration 19900: Loss = -12276.21723283933
pi: tensor([[1.0000e+00, 1.7859e-07],
        [3.4695e-01, 6.5305e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8860, 0.1140], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1938, 0.2128],
         [0.6452, 0.2542]],

        [[0.5995, 0.2409],
         [0.6747, 0.6234]],

        [[0.5693, 0.2297],
         [0.5178, 0.5824]],

        [[0.6977, 0.1849],
         [0.6539, 0.5782]],

        [[0.5104, 0.2059],
         [0.6467, 0.6272]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005221098356234914
Average Adjusted Rand Index: -0.0017427542642344796
11814.608479284829
[-0.0005221098356234914, -0.0005221098356234914] [-0.0017427542642344796, -0.0017427542642344796] [12276.227874827653, 12276.217924457604]
-------------------------------------
This iteration is 28
True Objective function: Loss = -11911.515233799195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20746.97760455761
Iteration 100: Loss = -12376.92307405927
Iteration 200: Loss = -12376.66985309203
Iteration 300: Loss = -12376.618728382717
Iteration 400: Loss = -12376.572176691028
Iteration 500: Loss = -12376.488377224812
Iteration 600: Loss = -12376.220296328218
Iteration 700: Loss = -12375.996489577752
Iteration 800: Loss = -12375.870570459261
Iteration 900: Loss = -12375.730012727412
Iteration 1000: Loss = -12375.57887644216
Iteration 1100: Loss = -12375.447554614799
Iteration 1200: Loss = -12375.374932593031
Iteration 1300: Loss = -12375.350286477194
Iteration 1400: Loss = -12375.34268513579
Iteration 1500: Loss = -12375.339795855818
Iteration 1600: Loss = -12375.338514984975
Iteration 1700: Loss = -12375.33785417409
Iteration 1800: Loss = -12375.337425187781
Iteration 1900: Loss = -12375.33719833136
Iteration 2000: Loss = -12375.336985649841
Iteration 2100: Loss = -12375.33685411862
Iteration 2200: Loss = -12375.33670975446
Iteration 2300: Loss = -12375.336607961795
Iteration 2400: Loss = -12375.336459725137
Iteration 2500: Loss = -12375.336347430435
Iteration 2600: Loss = -12375.336251271647
Iteration 2700: Loss = -12375.336120811891
Iteration 2800: Loss = -12375.33598766794
Iteration 2900: Loss = -12375.335896505137
Iteration 3000: Loss = -12375.33577754268
Iteration 3100: Loss = -12375.335654159648
Iteration 3200: Loss = -12375.335562412049
Iteration 3300: Loss = -12375.335484128202
Iteration 3400: Loss = -12375.335378754678
Iteration 3500: Loss = -12375.335248527155
Iteration 3600: Loss = -12375.335172882698
Iteration 3700: Loss = -12375.335056903281
Iteration 3800: Loss = -12375.33492918522
Iteration 3900: Loss = -12375.33486294173
Iteration 4000: Loss = -12375.334750731146
Iteration 4100: Loss = -12375.334698998662
Iteration 4200: Loss = -12375.334547434446
Iteration 4300: Loss = -12375.334468457168
Iteration 4400: Loss = -12375.334364435928
Iteration 4500: Loss = -12375.334276791618
Iteration 4600: Loss = -12375.334206656207
Iteration 4700: Loss = -12375.334129012745
Iteration 4800: Loss = -12375.333993505234
Iteration 4900: Loss = -12375.333926232492
Iteration 5000: Loss = -12375.333831635797
Iteration 5100: Loss = -12375.333740236189
Iteration 5200: Loss = -12375.333685319314
Iteration 5300: Loss = -12375.333555283914
Iteration 5400: Loss = -12375.33353598087
Iteration 5500: Loss = -12375.333446182141
Iteration 5600: Loss = -12375.333377477273
Iteration 5700: Loss = -12375.33331888034
Iteration 5800: Loss = -12375.333253827099
Iteration 5900: Loss = -12375.333186454256
Iteration 6000: Loss = -12375.3331066947
Iteration 6100: Loss = -12375.333042660064
Iteration 6200: Loss = -12375.332983272589
Iteration 6300: Loss = -12375.332897060312
Iteration 6400: Loss = -12375.334710596499
1
Iteration 6500: Loss = -12375.332796312376
Iteration 6600: Loss = -12375.332946841898
1
Iteration 6700: Loss = -12375.34506540122
2
Iteration 6800: Loss = -12375.359012977946
3
Iteration 6900: Loss = -12375.357265168084
4
Iteration 7000: Loss = -12375.332580217977
Iteration 7100: Loss = -12375.335773659812
1
Iteration 7200: Loss = -12375.332502007224
Iteration 7300: Loss = -12375.333750547023
1
Iteration 7400: Loss = -12375.33240058409
Iteration 7500: Loss = -12375.363399450034
1
Iteration 7600: Loss = -12375.332286440778
Iteration 7700: Loss = -12375.332256940743
Iteration 7800: Loss = -12375.332503092795
1
Iteration 7900: Loss = -12375.332233318357
Iteration 8000: Loss = -12375.33217594908
Iteration 8100: Loss = -12375.334678366102
1
Iteration 8200: Loss = -12375.33213708762
Iteration 8300: Loss = -12375.332084491565
Iteration 8400: Loss = -12375.334758073766
1
Iteration 8500: Loss = -12375.33201354841
Iteration 8600: Loss = -12375.331984141823
Iteration 8700: Loss = -12375.34186673232
1
Iteration 8800: Loss = -12375.331923799855
Iteration 8900: Loss = -12375.331937410107
Iteration 9000: Loss = -12375.344511810586
1
Iteration 9100: Loss = -12375.331873215471
Iteration 9200: Loss = -12375.331891016298
Iteration 9300: Loss = -12375.343232765683
1
Iteration 9400: Loss = -12375.331827461214
Iteration 9500: Loss = -12375.33177414673
Iteration 9600: Loss = -12375.896578727747
1
Iteration 9700: Loss = -12375.331803161478
Iteration 9800: Loss = -12375.331763652806
Iteration 9900: Loss = -12375.453490909931
1
Iteration 10000: Loss = -12375.33174910828
Iteration 10100: Loss = -12375.331717791447
Iteration 10200: Loss = -12375.331706311033
Iteration 10300: Loss = -12375.331818789096
1
Iteration 10400: Loss = -12375.331699939557
Iteration 10500: Loss = -12375.331733875626
Iteration 10600: Loss = -12375.331772541213
Iteration 10700: Loss = -12375.33163384473
Iteration 10800: Loss = -12375.331820943142
1
Iteration 10900: Loss = -12375.331581111119
Iteration 11000: Loss = -12375.331633605922
Iteration 11100: Loss = -12375.331948688765
1
Iteration 11200: Loss = -12375.331587088103
Iteration 11300: Loss = -12375.331608710461
Iteration 11400: Loss = -12375.33157941853
Iteration 11500: Loss = -12375.33734269982
1
Iteration 11600: Loss = -12375.33155510333
Iteration 11700: Loss = -12375.335204547548
1
Iteration 11800: Loss = -12375.331560343078
Iteration 11900: Loss = -12375.332421905088
1
Iteration 12000: Loss = -12375.331567993895
Iteration 12100: Loss = -12375.352109894699
1
Iteration 12200: Loss = -12375.331511586906
Iteration 12300: Loss = -12375.336972598729
1
Iteration 12400: Loss = -12375.331520033189
Iteration 12500: Loss = -12375.498895960445
1
Iteration 12600: Loss = -12375.331547015743
Iteration 12700: Loss = -12375.398892995128
1
Iteration 12800: Loss = -12375.339553142727
2
Iteration 12900: Loss = -12375.331539475095
Iteration 13000: Loss = -12375.331628194293
Iteration 13100: Loss = -12375.365163260762
1
Iteration 13200: Loss = -12375.331570462788
Iteration 13300: Loss = -12375.336403534859
1
Iteration 13400: Loss = -12375.331442688866
Iteration 13500: Loss = -12375.335016118544
1
Iteration 13600: Loss = -12375.331557972506
2
Iteration 13700: Loss = -12375.331504458212
Iteration 13800: Loss = -12375.33369495762
1
Iteration 13900: Loss = -12375.48251654688
2
Iteration 14000: Loss = -12375.33146286868
Iteration 14100: Loss = -12375.33257776455
1
Iteration 14200: Loss = -12375.336802449034
2
Iteration 14300: Loss = -12375.333808803389
3
Iteration 14400: Loss = -12375.337334212938
4
Iteration 14500: Loss = -12375.418436580347
5
Iteration 14600: Loss = -12375.331481922707
Iteration 14700: Loss = -12375.33466386684
1
Iteration 14800: Loss = -12375.35007224606
2
Iteration 14900: Loss = -12375.331435491264
Iteration 15000: Loss = -12375.331700398016
1
Iteration 15100: Loss = -12375.33210881284
2
Iteration 15200: Loss = -12375.347121248666
3
Iteration 15300: Loss = -12375.344614888134
4
Iteration 15400: Loss = -12375.331719925909
5
Iteration 15500: Loss = -12375.345425673904
6
Iteration 15600: Loss = -12375.341738102215
7
Iteration 15700: Loss = -12375.34195558054
8
Iteration 15800: Loss = -12375.332087031566
9
Iteration 15900: Loss = -12375.332075807088
10
Iteration 16000: Loss = -12375.487918980136
11
Iteration 16100: Loss = -12375.335157350708
12
Iteration 16200: Loss = -12375.386929627519
13
Iteration 16300: Loss = -12375.331526568298
Iteration 16400: Loss = -12375.348939832003
1
Iteration 16500: Loss = -12375.332524277997
2
Iteration 16600: Loss = -12375.332076084866
3
Iteration 16700: Loss = -12375.337399654794
4
Iteration 16800: Loss = -12375.331526139407
Iteration 16900: Loss = -12375.331660411957
1
Iteration 17000: Loss = -12375.331699732706
2
Iteration 17100: Loss = -12375.331760012583
3
Iteration 17200: Loss = -12375.395637710097
4
Iteration 17300: Loss = -12375.356145994583
5
Iteration 17400: Loss = -12375.359285081426
6
Iteration 17500: Loss = -12375.336370386854
7
Iteration 17600: Loss = -12375.353041887663
8
Iteration 17700: Loss = -12375.333306184713
9
Iteration 17800: Loss = -12375.334384341299
10
Iteration 17900: Loss = -12375.331496435343
Iteration 18000: Loss = -12375.331883294652
1
Iteration 18100: Loss = -12375.35372594712
2
Iteration 18200: Loss = -12375.331430406051
Iteration 18300: Loss = -12375.337293110459
1
Iteration 18400: Loss = -12375.331644850317
2
Iteration 18500: Loss = -12375.338477594807
3
Iteration 18600: Loss = -12375.345228564787
4
Iteration 18700: Loss = -12375.511638205568
5
Iteration 18800: Loss = -12375.331606378595
6
Iteration 18900: Loss = -12375.331996640312
7
Iteration 19000: Loss = -12375.331430724786
Iteration 19100: Loss = -12375.440425491353
1
Iteration 19200: Loss = -12375.33849972008
2
Iteration 19300: Loss = -12375.341410451092
3
Iteration 19400: Loss = -12375.331464173596
Iteration 19500: Loss = -12375.331653364443
1
Iteration 19600: Loss = -12375.331582214749
2
Iteration 19700: Loss = -12375.336692394772
3
Iteration 19800: Loss = -12375.331561211058
Iteration 19900: Loss = -12375.331567782694
pi: tensor([[9.8307e-01, 1.6928e-02],
        [9.9996e-01, 3.6605e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9171, 0.0829], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.2472],
         [0.7200, 0.3488]],

        [[0.6448, 0.2414],
         [0.5611, 0.5656]],

        [[0.6977, 0.1949],
         [0.5353, 0.6113]],

        [[0.5954, 0.2436],
         [0.6361, 0.5786]],

        [[0.6653, 0.1710],
         [0.7309, 0.6516]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006422660312947975
Average Adjusted Rand Index: -0.001292929292929293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22587.80652937923
Iteration 100: Loss = -12388.125922963412
Iteration 200: Loss = -12331.037026303004
Iteration 300: Loss = -12080.52516390211
Iteration 400: Loss = -12079.888792383093
Iteration 500: Loss = -12077.382317955953
Iteration 600: Loss = -12077.223324156097
Iteration 700: Loss = -12077.095086659137
Iteration 800: Loss = -12077.058496876121
Iteration 900: Loss = -12077.039596309414
Iteration 1000: Loss = -12077.01332852288
Iteration 1100: Loss = -12076.997450084771
Iteration 1200: Loss = -12077.021487772301
1
Iteration 1300: Loss = -12076.951756714916
Iteration 1400: Loss = -12076.87182847638
Iteration 1500: Loss = -12072.647349029441
Iteration 1600: Loss = -12053.9172624236
Iteration 1700: Loss = -12047.798758359106
Iteration 1800: Loss = -12037.655684186037
Iteration 1900: Loss = -12020.096643672308
Iteration 2000: Loss = -11988.50949376872
Iteration 2100: Loss = -11985.014930350755
Iteration 2200: Loss = -11972.788820622694
Iteration 2300: Loss = -11960.746075693309
Iteration 2400: Loss = -11955.162445665355
Iteration 2500: Loss = -11955.163215283701
1
Iteration 2600: Loss = -11952.129804033735
Iteration 2700: Loss = -11952.036571832796
Iteration 2800: Loss = -11951.41883844613
Iteration 2900: Loss = -11931.303835224773
Iteration 3000: Loss = -11928.699260498632
Iteration 3100: Loss = -11928.679201092724
Iteration 3200: Loss = -11917.786332113265
Iteration 3300: Loss = -11915.845563817647
Iteration 3400: Loss = -11906.621652984724
Iteration 3500: Loss = -11906.586163167578
Iteration 3600: Loss = -11906.574369046248
Iteration 3700: Loss = -11906.57086606604
Iteration 3800: Loss = -11906.568964832637
Iteration 3900: Loss = -11906.570931826307
1
Iteration 4000: Loss = -11900.71699353411
Iteration 4100: Loss = -11900.669554990174
Iteration 4200: Loss = -11900.668939441632
Iteration 4300: Loss = -11900.67545321452
1
Iteration 4400: Loss = -11900.668088082024
Iteration 4500: Loss = -11900.667814575487
Iteration 4600: Loss = -11900.672274996154
1
Iteration 4700: Loss = -11900.667647801072
Iteration 4800: Loss = -11900.66709730656
Iteration 4900: Loss = -11900.666912745839
Iteration 5000: Loss = -11900.666890844252
Iteration 5100: Loss = -11900.671433160061
1
Iteration 5200: Loss = -11900.66632603122
Iteration 5300: Loss = -11900.666187532332
Iteration 5400: Loss = -11900.666460855398
1
Iteration 5500: Loss = -11900.667136716691
2
Iteration 5600: Loss = -11900.665730573488
Iteration 5700: Loss = -11900.665577789578
Iteration 5800: Loss = -11900.66581414826
1
Iteration 5900: Loss = -11900.664957126426
Iteration 6000: Loss = -11900.664111285027
Iteration 6100: Loss = -11900.663819589194
Iteration 6200: Loss = -11900.663494927412
Iteration 6300: Loss = -11900.660702118525
Iteration 6400: Loss = -11900.658781339576
Iteration 6500: Loss = -11900.660888656863
1
Iteration 6600: Loss = -11900.658070206944
Iteration 6700: Loss = -11900.656782084301
Iteration 6800: Loss = -11900.656739403486
Iteration 6900: Loss = -11900.656638802871
Iteration 7000: Loss = -11900.657713387136
1
Iteration 7100: Loss = -11900.65704693045
2
Iteration 7200: Loss = -11900.656520629094
Iteration 7300: Loss = -11900.662077783452
1
Iteration 7400: Loss = -11900.656395677768
Iteration 7500: Loss = -11900.65638888834
Iteration 7600: Loss = -11900.656608809175
1
Iteration 7700: Loss = -11900.656412013828
Iteration 7800: Loss = -11900.656329669602
Iteration 7900: Loss = -11900.655947266952
Iteration 8000: Loss = -11900.649682479325
Iteration 8100: Loss = -11900.651977898562
1
Iteration 8200: Loss = -11900.652941483124
2
Iteration 8300: Loss = -11900.650121100623
3
Iteration 8400: Loss = -11900.651185149081
4
Iteration 8500: Loss = -11900.659937826786
5
Iteration 8600: Loss = -11900.649729962604
Iteration 8700: Loss = -11900.649146010848
Iteration 8800: Loss = -11900.648866344038
Iteration 8900: Loss = -11900.650093634213
1
Iteration 9000: Loss = -11900.669748320157
2
Iteration 9100: Loss = -11900.64856448133
Iteration 9200: Loss = -11900.710799574266
1
Iteration 9300: Loss = -11900.648391960538
Iteration 9400: Loss = -11900.714348867787
1
Iteration 9500: Loss = -11900.648340304164
Iteration 9600: Loss = -11900.800558649711
1
Iteration 9700: Loss = -11900.64829772233
Iteration 9800: Loss = -11900.648932168535
1
Iteration 9900: Loss = -11900.650117882538
2
Iteration 10000: Loss = -11900.540631148633
Iteration 10100: Loss = -11900.538621828497
Iteration 10200: Loss = -11900.543297898035
1
Iteration 10300: Loss = -11900.535655885027
Iteration 10400: Loss = -11900.544629479089
1
Iteration 10500: Loss = -11900.54913903451
2
Iteration 10600: Loss = -11900.558619181906
3
Iteration 10700: Loss = -11900.619261960108
4
Iteration 10800: Loss = -11900.544485585304
5
Iteration 10900: Loss = -11900.539024082635
6
Iteration 11000: Loss = -11900.535647948745
Iteration 11100: Loss = -11900.623850210619
1
Iteration 11200: Loss = -11900.533121887182
Iteration 11300: Loss = -11900.532578079483
Iteration 11400: Loss = -11900.532688547371
1
Iteration 11500: Loss = -11900.554629690676
2
Iteration 11600: Loss = -11900.653763788197
3
Iteration 11700: Loss = -11900.533627962672
4
Iteration 11800: Loss = -11900.532465783155
Iteration 11900: Loss = -11900.539548984312
1
Iteration 12000: Loss = -11900.539015305854
2
Iteration 12100: Loss = -11900.540683468242
3
Iteration 12200: Loss = -11900.579088266226
4
Iteration 12300: Loss = -11900.53252229431
Iteration 12400: Loss = -11900.533586586127
1
Iteration 12500: Loss = -11900.556208318356
2
Iteration 12600: Loss = -11900.579886752092
3
Iteration 12700: Loss = -11900.581087582543
4
Iteration 12800: Loss = -11900.550250741879
5
Iteration 12900: Loss = -11900.561459776807
6
Iteration 13000: Loss = -11900.586625578882
7
Iteration 13100: Loss = -11900.53256159141
Iteration 13200: Loss = -11900.534165533812
1
Iteration 13300: Loss = -11900.675457488049
2
Iteration 13400: Loss = -11900.532610785795
Iteration 13500: Loss = -11900.532288669621
Iteration 13600: Loss = -11900.53249432922
1
Iteration 13700: Loss = -11900.533441503652
2
Iteration 13800: Loss = -11900.533407736992
3
Iteration 13900: Loss = -11900.532679902859
4
Iteration 14000: Loss = -11900.53462474232
5
Iteration 14100: Loss = -11900.551317483023
6
Iteration 14200: Loss = -11900.53334315214
7
Iteration 14300: Loss = -11900.573015553215
8
Iteration 14400: Loss = -11900.540194760675
9
Iteration 14500: Loss = -11900.589766626425
10
Iteration 14600: Loss = -11900.532362575705
Iteration 14700: Loss = -11900.66220605007
1
Iteration 14800: Loss = -11900.543093939023
2
Iteration 14900: Loss = -11900.532829157393
3
Iteration 15000: Loss = -11900.593639786593
4
Iteration 15100: Loss = -11900.541049396545
5
Iteration 15200: Loss = -11900.54045669069
6
Iteration 15300: Loss = -11900.532486941798
7
Iteration 15400: Loss = -11900.53287468249
8
Iteration 15500: Loss = -11900.534550038634
9
Iteration 15600: Loss = -11900.534967867452
10
Iteration 15700: Loss = -11900.579618880394
11
Iteration 15800: Loss = -11900.532555805366
12
Iteration 15900: Loss = -11900.532326283788
Iteration 16000: Loss = -11900.533116324981
1
Iteration 16100: Loss = -11900.536147568466
2
Iteration 16200: Loss = -11900.54372086004
3
Iteration 16300: Loss = -11900.53212675059
Iteration 16400: Loss = -11900.539244085307
1
Iteration 16500: Loss = -11900.542215645664
2
Iteration 16600: Loss = -11900.54373202732
3
Iteration 16700: Loss = -11900.538604169584
4
Iteration 16800: Loss = -11900.531964788006
Iteration 16900: Loss = -11900.55382047893
1
Iteration 17000: Loss = -11900.531791485584
Iteration 17100: Loss = -11900.532051008633
1
Iteration 17200: Loss = -11900.54444959338
2
Iteration 17300: Loss = -11900.531488828561
Iteration 17400: Loss = -11900.532276456779
1
Iteration 17500: Loss = -11900.531541411547
Iteration 17600: Loss = -11900.531544817573
Iteration 17700: Loss = -11900.564750670967
1
Iteration 17800: Loss = -11900.54979095596
2
Iteration 17900: Loss = -11900.53150637172
Iteration 18000: Loss = -11900.53599006878
1
Iteration 18100: Loss = -11900.532834409916
2
Iteration 18200: Loss = -11900.531891363345
3
Iteration 18300: Loss = -11900.542199285737
4
Iteration 18400: Loss = -11900.53202365113
5
Iteration 18500: Loss = -11900.54099896603
6
Iteration 18600: Loss = -11900.540448953841
7
Iteration 18700: Loss = -11900.53157896218
Iteration 18800: Loss = -11900.538243218132
1
Iteration 18900: Loss = -11900.558236500108
2
Iteration 19000: Loss = -11900.54325703138
3
Iteration 19100: Loss = -11900.531457795789
Iteration 19200: Loss = -11900.53356520339
1
Iteration 19300: Loss = -11900.593922026646
2
Iteration 19400: Loss = -11900.70675809154
3
Iteration 19500: Loss = -11900.53646178352
4
Iteration 19600: Loss = -11900.533078997974
5
Iteration 19700: Loss = -11900.53229691769
6
Iteration 19800: Loss = -11900.531138547405
Iteration 19900: Loss = -11900.531321622188
1
pi: tensor([[0.7244, 0.2756],
        [0.2589, 0.7411]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4497, 0.5503], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2922, 0.0997],
         [0.6033, 0.3026]],

        [[0.6049, 0.1068],
         [0.6045, 0.6628]],

        [[0.6116, 0.1068],
         [0.7137, 0.5950]],

        [[0.5176, 0.1089],
         [0.6278, 0.5129]],

        [[0.7167, 0.0885],
         [0.6243, 0.7008]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320640625895
Average Adjusted Rand Index: 0.9839996238662728
11911.515233799195
[-0.0006422660312947975, 0.9840320640625895] [-0.001292929292929293, 0.9839996238662728] [12375.331976772253, 11900.53444998239]
-------------------------------------
This iteration is 29
True Objective function: Loss = -11879.0029284395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22505.22408541885
Iteration 100: Loss = -12440.265746734915
Iteration 200: Loss = -12439.250087424965
Iteration 300: Loss = -12439.026839609995
Iteration 400: Loss = -12438.920762155032
Iteration 500: Loss = -12438.842501181229
Iteration 600: Loss = -12438.767510834348
Iteration 700: Loss = -12438.68542676795
Iteration 800: Loss = -12438.584816885212
Iteration 900: Loss = -12438.441571915966
Iteration 1000: Loss = -12438.155275688185
Iteration 1100: Loss = -12437.714610498486
Iteration 1200: Loss = -12437.455847721625
Iteration 1300: Loss = -12437.30223217019
Iteration 1400: Loss = -12437.195522276048
Iteration 1500: Loss = -12437.113563296462
Iteration 1600: Loss = -12437.047392993101
Iteration 1700: Loss = -12436.994451415749
Iteration 1800: Loss = -12436.95466358463
Iteration 1900: Loss = -12436.924819842952
Iteration 2000: Loss = -12436.901539271395
Iteration 2100: Loss = -12436.882841721375
Iteration 2200: Loss = -12436.867665087126
Iteration 2300: Loss = -12436.855106745717
Iteration 2400: Loss = -12436.844788293194
Iteration 2500: Loss = -12436.83608503583
Iteration 2600: Loss = -12436.828813010883
Iteration 2700: Loss = -12436.822758865857
Iteration 2800: Loss = -12436.817628598103
Iteration 2900: Loss = -12436.81326227476
Iteration 3000: Loss = -12436.80957967663
Iteration 3100: Loss = -12436.806464892003
Iteration 3200: Loss = -12436.803823275939
Iteration 3300: Loss = -12436.801560837815
Iteration 3400: Loss = -12436.799573286127
Iteration 3500: Loss = -12436.797851655776
Iteration 3600: Loss = -12436.796383227578
Iteration 3700: Loss = -12436.79507342493
Iteration 3800: Loss = -12436.793856614171
Iteration 3900: Loss = -12436.792862464745
Iteration 4000: Loss = -12436.791935288426
Iteration 4100: Loss = -12436.791081446514
Iteration 4200: Loss = -12436.790352155065
Iteration 4300: Loss = -12436.789635100178
Iteration 4400: Loss = -12436.789055696956
Iteration 4500: Loss = -12436.788532284794
Iteration 4600: Loss = -12436.787990108573
Iteration 4700: Loss = -12436.787548376604
Iteration 4800: Loss = -12436.787083403186
Iteration 4900: Loss = -12436.786691844181
Iteration 5000: Loss = -12436.7863345352
Iteration 5100: Loss = -12436.786005395152
Iteration 5200: Loss = -12436.78571877255
Iteration 5300: Loss = -12436.785466253037
Iteration 5400: Loss = -12436.785230657017
Iteration 5500: Loss = -12436.785024021648
Iteration 5600: Loss = -12436.784815839155
Iteration 5700: Loss = -12436.784661251759
Iteration 5800: Loss = -12436.78451377637
Iteration 5900: Loss = -12436.78438981739
Iteration 6000: Loss = -12436.784240880103
Iteration 6100: Loss = -12436.784139642736
Iteration 6200: Loss = -12436.784045568758
Iteration 6300: Loss = -12436.78393306017
Iteration 6400: Loss = -12436.783864190753
Iteration 6500: Loss = -12436.783763887794
Iteration 6600: Loss = -12436.801736390395
1
Iteration 6700: Loss = -12436.783707295437
Iteration 6800: Loss = -12436.783671130137
Iteration 6900: Loss = -12436.783618196501
Iteration 7000: Loss = -12436.785464225377
1
Iteration 7100: Loss = -12436.783569738904
Iteration 7200: Loss = -12436.783488374575
Iteration 7300: Loss = -12437.424087494252
1
Iteration 7400: Loss = -12436.783467542862
Iteration 7500: Loss = -12436.783460490917
Iteration 7600: Loss = -12436.783461468838
Iteration 7700: Loss = -12436.783639396352
1
Iteration 7800: Loss = -12436.78336758345
Iteration 7900: Loss = -12436.783333008672
Iteration 8000: Loss = -12436.78464745526
1
Iteration 8100: Loss = -12436.783337926747
Iteration 8200: Loss = -12436.783332302351
Iteration 8300: Loss = -12436.788120158688
1
Iteration 8400: Loss = -12436.783323905249
Iteration 8500: Loss = -12436.783342141816
Iteration 8600: Loss = -12436.783360903935
Iteration 8700: Loss = -12436.78326299916
Iteration 8800: Loss = -12437.134680246054
1
Iteration 8900: Loss = -12436.783227457823
Iteration 9000: Loss = -12436.783262827226
Iteration 9100: Loss = -12436.783216631875
Iteration 9200: Loss = -12436.785390085312
1
Iteration 9300: Loss = -12436.783224957808
Iteration 9400: Loss = -12436.783194411104
Iteration 9500: Loss = -12436.860092793326
1
Iteration 9600: Loss = -12436.783229465376
Iteration 9700: Loss = -12436.783187993424
Iteration 9800: Loss = -12436.783193691776
Iteration 9900: Loss = -12436.79232028711
1
Iteration 10000: Loss = -12436.783192204693
Iteration 10100: Loss = -12436.78318011112
Iteration 10200: Loss = -12436.982572740502
1
Iteration 10300: Loss = -12436.783198459563
Iteration 10400: Loss = -12436.783168622036
Iteration 10500: Loss = -12436.801888548149
1
Iteration 10600: Loss = -12436.783157896332
Iteration 10700: Loss = -12436.783139083669
Iteration 10800: Loss = -12436.79364508341
1
Iteration 10900: Loss = -12436.783129134232
Iteration 11000: Loss = -12436.783167120877
Iteration 11100: Loss = -12436.783255311659
Iteration 11200: Loss = -12436.783173919159
Iteration 11300: Loss = -12436.783162380194
Iteration 11400: Loss = -12436.783126829376
Iteration 11500: Loss = -12436.783224899578
Iteration 11600: Loss = -12436.783178263811
Iteration 11700: Loss = -12436.783142446908
Iteration 11800: Loss = -12436.783651613872
1
Iteration 11900: Loss = -12436.783143893463
Iteration 12000: Loss = -12436.783185816772
Iteration 12100: Loss = -12436.784920323042
1
Iteration 12200: Loss = -12436.783202623137
Iteration 12300: Loss = -12436.783184481552
Iteration 12400: Loss = -12436.783260682254
Iteration 12500: Loss = -12436.78318041796
Iteration 12600: Loss = -12436.8061654824
1
Iteration 12700: Loss = -12436.783112998604
Iteration 12800: Loss = -12436.78312528331
Iteration 12900: Loss = -12436.790795916206
1
Iteration 13000: Loss = -12436.78315481216
Iteration 13100: Loss = -12436.783140355994
Iteration 13200: Loss = -12437.250872965342
1
Iteration 13300: Loss = -12436.783187558713
Iteration 13400: Loss = -12436.78311722938
Iteration 13500: Loss = -12436.78322758195
1
Iteration 13600: Loss = -12436.783186776898
Iteration 13700: Loss = -12436.783131400542
Iteration 13800: Loss = -12436.783229842904
Iteration 13900: Loss = -12436.784565251719
1
Iteration 14000: Loss = -12436.783133597793
Iteration 14100: Loss = -12436.784053268195
1
Iteration 14200: Loss = -12436.78311227256
Iteration 14300: Loss = -12436.783110218314
Iteration 14400: Loss = -12436.783686005205
1
Iteration 14500: Loss = -12436.783130514223
Iteration 14600: Loss = -12436.783324185719
1
Iteration 14700: Loss = -12436.783249848044
2
Iteration 14800: Loss = -12436.783132120254
Iteration 14900: Loss = -12436.821652366292
1
Iteration 15000: Loss = -12436.783103234653
Iteration 15100: Loss = -12436.783153102366
Iteration 15200: Loss = -12436.783432628354
1
Iteration 15300: Loss = -12436.783071094678
Iteration 15400: Loss = -12436.783239802844
1
Iteration 15500: Loss = -12436.783120137094
Iteration 15600: Loss = -12436.913434971488
1
Iteration 15700: Loss = -12436.783130404156
Iteration 15800: Loss = -12436.783164935603
Iteration 15900: Loss = -12436.788094349115
1
Iteration 16000: Loss = -12436.78313042555
Iteration 16100: Loss = -12436.783411703043
1
Iteration 16200: Loss = -12436.783131125003
Iteration 16300: Loss = -12436.949088542553
1
Iteration 16400: Loss = -12436.783080790792
Iteration 16500: Loss = -12436.787035061261
1
Iteration 16600: Loss = -12436.783293046883
2
Iteration 16700: Loss = -12436.78329218506
3
Iteration 16800: Loss = -12436.783208462912
4
Iteration 16900: Loss = -12436.783087565596
Iteration 17000: Loss = -12436.789241163797
1
Iteration 17100: Loss = -12436.783249601971
2
Iteration 17200: Loss = -12436.783694068066
3
Iteration 17300: Loss = -12436.783138475192
Iteration 17400: Loss = -12436.802526811945
1
Iteration 17500: Loss = -12436.784384948698
2
Iteration 17600: Loss = -12436.783775036536
3
Iteration 17700: Loss = -12436.97905539706
4
Iteration 17800: Loss = -12436.783161020521
Iteration 17900: Loss = -12436.785055208502
1
Iteration 18000: Loss = -12436.783165040095
Iteration 18100: Loss = -12436.783220701114
Iteration 18200: Loss = -12436.783140823065
Iteration 18300: Loss = -12436.783133861569
Iteration 18400: Loss = -12436.783969533464
1
Iteration 18500: Loss = -12436.783240734278
2
Iteration 18600: Loss = -12436.783761409417
3
Iteration 18700: Loss = -12436.783143240991
Iteration 18800: Loss = -12436.785821318294
1
Iteration 18900: Loss = -12436.78317410417
Iteration 19000: Loss = -12436.783147068132
Iteration 19100: Loss = -12436.783191668115
Iteration 19200: Loss = -12436.783102855792
Iteration 19300: Loss = -12436.795091804614
1
Iteration 19400: Loss = -12436.783126113005
Iteration 19500: Loss = -12436.78339406227
1
Iteration 19600: Loss = -12436.783354972333
2
Iteration 19700: Loss = -12437.047283500371
3
Iteration 19800: Loss = -12436.783112076713
Iteration 19900: Loss = -12436.786507174273
1
pi: tensor([[0.4363, 0.5637],
        [0.0083, 0.9917]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0030, 0.9970], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4838, 0.1880],
         [0.5667, 0.2010]],

        [[0.6023, 0.2253],
         [0.6393, 0.5128]],

        [[0.6111, 0.3030],
         [0.6013, 0.6525]],

        [[0.6613, 0.1035],
         [0.6460, 0.6727]],

        [[0.6458, 0.2843],
         [0.6782, 0.6693]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.009987515605493134
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0001704756602228382
Average Adjusted Rand Index: 0.0014048246330729869
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21767.640699210388
Iteration 100: Loss = -12439.892640332117
Iteration 200: Loss = -12439.184210710131
Iteration 300: Loss = -12438.984730815268
Iteration 400: Loss = -12438.872814875223
Iteration 500: Loss = -12438.792589386023
Iteration 600: Loss = -12438.72863681642
Iteration 700: Loss = -12438.675374299106
Iteration 800: Loss = -12438.631528135536
Iteration 900: Loss = -12438.594944633858
Iteration 1000: Loss = -12438.563482629172
Iteration 1100: Loss = -12438.5358529279
Iteration 1200: Loss = -12438.511210613324
Iteration 1300: Loss = -12438.489383794104
Iteration 1400: Loss = -12438.470498640081
Iteration 1500: Loss = -12438.454819515307
Iteration 1600: Loss = -12438.442601831483
Iteration 1700: Loss = -12438.433388908195
Iteration 1800: Loss = -12438.426521304076
Iteration 1900: Loss = -12438.421430298771
Iteration 2000: Loss = -12438.417502937187
Iteration 2100: Loss = -12438.414460306933
Iteration 2200: Loss = -12438.411811958014
Iteration 2300: Loss = -12438.409574263342
Iteration 2400: Loss = -12438.407507428603
Iteration 2500: Loss = -12438.40546905218
Iteration 2600: Loss = -12438.403449942924
Iteration 2700: Loss = -12438.401388268805
Iteration 2800: Loss = -12438.399196515084
Iteration 2900: Loss = -12438.39679988756
Iteration 3000: Loss = -12438.39418474623
Iteration 3100: Loss = -12438.391370243926
Iteration 3200: Loss = -12438.388147691769
Iteration 3300: Loss = -12438.384505562177
Iteration 3400: Loss = -12438.380402952022
Iteration 3500: Loss = -12438.37564538701
Iteration 3600: Loss = -12438.370221351945
Iteration 3700: Loss = -12438.363949452967
Iteration 3800: Loss = -12438.356793692637
Iteration 3900: Loss = -12438.348516751832
Iteration 4000: Loss = -12438.339122469415
Iteration 4100: Loss = -12438.328559136742
Iteration 4200: Loss = -12438.316808550346
Iteration 4300: Loss = -12438.304061276021
Iteration 4400: Loss = -12438.290399134896
Iteration 4500: Loss = -12438.27599324536
Iteration 4600: Loss = -12438.261051035812
Iteration 4700: Loss = -12438.24569239067
Iteration 4800: Loss = -12438.22997563902
Iteration 4900: Loss = -12438.21364381366
Iteration 5000: Loss = -12438.196411204503
Iteration 5100: Loss = -12438.17705645572
Iteration 5200: Loss = -12438.150915696351
Iteration 5300: Loss = -12438.051567395903
Iteration 5400: Loss = -12417.152109116683
Iteration 5500: Loss = -11886.99290069459
Iteration 5600: Loss = -11866.778676105063
Iteration 5700: Loss = -11866.010739352208
Iteration 5800: Loss = -11865.959933716716
Iteration 5900: Loss = -11865.947285140468
Iteration 6000: Loss = -11865.937955527856
Iteration 6100: Loss = -11865.931876607523
Iteration 6200: Loss = -11865.927389992308
Iteration 6300: Loss = -11865.92400269828
Iteration 6400: Loss = -11865.921180482917
Iteration 6500: Loss = -11865.918967141137
Iteration 6600: Loss = -11865.917158186836
Iteration 6700: Loss = -11865.917860204487
1
Iteration 6800: Loss = -11865.914959737802
Iteration 6900: Loss = -11865.914546150187
Iteration 7000: Loss = -11865.912300560216
Iteration 7100: Loss = -11865.91125989959
Iteration 7200: Loss = -11865.910524124587
Iteration 7300: Loss = -11865.911283226167
1
Iteration 7400: Loss = -11865.913893242776
2
Iteration 7500: Loss = -11865.909097616111
Iteration 7600: Loss = -11865.907947958392
Iteration 7700: Loss = -11865.907422778098
Iteration 7800: Loss = -11865.909248426267
1
Iteration 7900: Loss = -11865.910760656758
2
Iteration 8000: Loss = -11865.91646509737
3
Iteration 8100: Loss = -11865.911671157895
4
Iteration 8200: Loss = -11865.953239856239
5
Iteration 8300: Loss = -11865.906033043959
Iteration 8400: Loss = -11865.9053189995
Iteration 8500: Loss = -11865.905221713585
Iteration 8600: Loss = -11865.907154590028
1
Iteration 8700: Loss = -11865.91062393802
2
Iteration 8800: Loss = -11865.905487263019
3
Iteration 8900: Loss = -11865.902941394063
Iteration 9000: Loss = -11865.902924046237
Iteration 9100: Loss = -11865.903045348132
1
Iteration 9200: Loss = -11865.90395440609
2
Iteration 9300: Loss = -11865.906612443077
3
Iteration 9400: Loss = -11865.905848255476
4
Iteration 9500: Loss = -11865.903329501007
5
Iteration 9600: Loss = -11865.902990731644
Iteration 9700: Loss = -11865.99291704974
1
Iteration 9800: Loss = -11865.901751238296
Iteration 9900: Loss = -11865.902783100282
1
Iteration 10000: Loss = -11865.903669918258
2
Iteration 10100: Loss = -11865.904340465246
3
Iteration 10200: Loss = -11865.994087359646
4
Iteration 10300: Loss = -11865.900967059264
Iteration 10400: Loss = -11865.902813032997
1
Iteration 10500: Loss = -11865.92026416094
2
Iteration 10600: Loss = -11865.908521855106
3
Iteration 10700: Loss = -11865.909589271065
4
Iteration 10800: Loss = -11865.906284770004
5
Iteration 10900: Loss = -11865.912507425779
6
Iteration 11000: Loss = -11865.91944862002
7
Iteration 11100: Loss = -11865.900048556749
Iteration 11200: Loss = -11866.034388843298
1
Iteration 11300: Loss = -11865.89755773793
Iteration 11400: Loss = -11865.900825458692
1
Iteration 11500: Loss = -11865.906051461236
2
Iteration 11600: Loss = -11866.038465886357
3
Iteration 11700: Loss = -11865.88948077239
Iteration 11800: Loss = -11865.890755436585
1
Iteration 11900: Loss = -11865.891268889083
2
Iteration 12000: Loss = -11865.895877465167
3
Iteration 12100: Loss = -11865.891289857638
4
Iteration 12200: Loss = -11865.917123954447
5
Iteration 12300: Loss = -11865.885760976504
Iteration 12400: Loss = -11865.836473432531
Iteration 12500: Loss = -11865.837028288961
1
Iteration 12600: Loss = -11865.847006690205
2
Iteration 12700: Loss = -11865.834376983534
Iteration 12800: Loss = -11865.835177095585
1
Iteration 12900: Loss = -11865.843612719758
2
Iteration 13000: Loss = -11865.834508981257
3
Iteration 13100: Loss = -11865.845609508453
4
Iteration 13200: Loss = -11865.845450634677
5
Iteration 13300: Loss = -11865.852690754988
6
Iteration 13400: Loss = -11865.834177436185
Iteration 13500: Loss = -11865.836188978328
1
Iteration 13600: Loss = -11865.869681682594
2
Iteration 13700: Loss = -11865.925798662422
3
Iteration 13800: Loss = -11865.837564830426
4
Iteration 13900: Loss = -11865.835282906812
5
Iteration 14000: Loss = -11865.836124819476
6
Iteration 14100: Loss = -11865.836322470092
7
Iteration 14200: Loss = -11865.856464165932
8
Iteration 14300: Loss = -11865.842615579133
9
Iteration 14400: Loss = -11865.867310354184
10
Iteration 14500: Loss = -11865.852849344286
11
Iteration 14600: Loss = -11865.84146988145
12
Iteration 14700: Loss = -11865.860692470827
13
Iteration 14800: Loss = -11865.841397940785
14
Iteration 14900: Loss = -11865.845283132374
15
Stopping early at iteration 14900 due to no improvement.
pi: tensor([[0.7433, 0.2567],
        [0.1715, 0.8285]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4960, 0.5040], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3097, 0.1054],
         [0.5614, 0.2984]],

        [[0.5233, 0.0986],
         [0.7207, 0.5167]],

        [[0.6672, 0.0991],
         [0.5738, 0.6921]],

        [[0.6870, 0.0913],
         [0.6640, 0.5395]],

        [[0.5675, 0.0959],
         [0.6164, 0.5573]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.984030281556705
Average Adjusted Rand Index: 0.9839898021422545
11879.0029284395
[0.0001704756602228382, 0.984030281556705] [0.0014048246330729869, 0.9839898021422545] [12436.783154173037, 11865.845283132374]
-------------------------------------
This iteration is 30
True Objective function: Loss = -11842.7333971895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21130.70101906788
Iteration 100: Loss = -12404.537307281023
Iteration 200: Loss = -12403.898740841136
Iteration 300: Loss = -12403.633074357438
Iteration 400: Loss = -12403.44148325983
Iteration 500: Loss = -12403.3190135887
Iteration 600: Loss = -12403.237452898373
Iteration 700: Loss = -12403.181967912064
Iteration 800: Loss = -12403.144123067439
Iteration 900: Loss = -12403.117563477523
Iteration 1000: Loss = -12403.098254647843
Iteration 1100: Loss = -12403.083824721505
Iteration 1200: Loss = -12403.072592264141
Iteration 1300: Loss = -12403.063647293598
Iteration 1400: Loss = -12403.056364143094
Iteration 1500: Loss = -12403.05015882538
Iteration 1600: Loss = -12403.04441089275
Iteration 1700: Loss = -12403.038730120508
Iteration 1800: Loss = -12403.032649391147
Iteration 1900: Loss = -12403.025576360247
Iteration 2000: Loss = -12403.016554774767
Iteration 2100: Loss = -12403.003765609252
Iteration 2200: Loss = -12402.983601842394
Iteration 2300: Loss = -12402.95889882997
Iteration 2400: Loss = -12402.942497157652
Iteration 2500: Loss = -12402.932596762656
Iteration 2600: Loss = -12402.924761638371
Iteration 2700: Loss = -12402.917358704468
Iteration 2800: Loss = -12402.908979325455
Iteration 2900: Loss = -12402.897579414974
Iteration 3000: Loss = -12402.878892278699
Iteration 3100: Loss = -12402.847672002195
Iteration 3200: Loss = -12402.808056304606
Iteration 3300: Loss = -12402.740916881978
Iteration 3400: Loss = -12402.666632641292
Iteration 3500: Loss = -12402.62474714776
Iteration 3600: Loss = -12402.609077709243
Iteration 3700: Loss = -12402.603441598743
Iteration 3800: Loss = -12402.601018548003
Iteration 3900: Loss = -12402.59963045475
Iteration 4000: Loss = -12402.598672498767
Iteration 4100: Loss = -12402.597846349912
Iteration 4200: Loss = -12402.59717955079
Iteration 4300: Loss = -12402.596641798982
Iteration 4400: Loss = -12402.596236404965
Iteration 4500: Loss = -12402.595861456177
Iteration 4600: Loss = -12402.595608094784
Iteration 4700: Loss = -12402.595431143991
Iteration 4800: Loss = -12402.595271347393
Iteration 4900: Loss = -12402.595139020666
Iteration 5000: Loss = -12402.595076418582
Iteration 5100: Loss = -12402.595019144332
Iteration 5200: Loss = -12402.594946289499
Iteration 5300: Loss = -12402.594902916942
Iteration 5400: Loss = -12402.594824710732
Iteration 5500: Loss = -12402.594887977006
Iteration 5600: Loss = -12402.595075411906
1
Iteration 5700: Loss = -12402.594842839953
Iteration 5800: Loss = -12402.596639807243
1
Iteration 5900: Loss = -12402.594795867999
Iteration 6000: Loss = -12402.594860680307
Iteration 6100: Loss = -12402.5957129475
1
Iteration 6200: Loss = -12402.594803088237
Iteration 6300: Loss = -12402.59584004755
1
Iteration 6400: Loss = -12402.594799746472
Iteration 6500: Loss = -12402.594813694937
Iteration 6600: Loss = -12402.594812678388
Iteration 6700: Loss = -12402.594792372822
Iteration 6800: Loss = -12402.59480225093
Iteration 6900: Loss = -12402.594828144422
Iteration 7000: Loss = -12402.594802948292
Iteration 7100: Loss = -12402.597954924368
1
Iteration 7200: Loss = -12402.594815398177
Iteration 7300: Loss = -12402.595401682365
1
Iteration 7400: Loss = -12402.594794336985
Iteration 7500: Loss = -12402.594790586327
Iteration 7600: Loss = -12402.594794481824
Iteration 7700: Loss = -12402.594774895606
Iteration 7800: Loss = -12402.594871974141
Iteration 7900: Loss = -12402.597336889065
1
Iteration 8000: Loss = -12402.596252190071
2
Iteration 8100: Loss = -12402.601762630356
3
Iteration 8200: Loss = -12402.594836347631
Iteration 8300: Loss = -12402.61084709015
1
Iteration 8400: Loss = -12402.594804009044
Iteration 8500: Loss = -12402.657537324247
1
Iteration 8600: Loss = -12402.594823923795
Iteration 8700: Loss = -12402.599514833913
1
Iteration 8800: Loss = -12402.594749014585
Iteration 8900: Loss = -12402.595620685932
1
Iteration 9000: Loss = -12402.594792338305
Iteration 9100: Loss = -12402.595470028034
1
Iteration 9200: Loss = -12402.594770877462
Iteration 9300: Loss = -12402.628888220906
1
Iteration 9400: Loss = -12402.596994242436
2
Iteration 9500: Loss = -12402.594812253194
Iteration 9600: Loss = -12402.595307331714
1
Iteration 9700: Loss = -12402.594796430873
Iteration 9800: Loss = -12402.719574027757
1
Iteration 9900: Loss = -12402.594796812306
Iteration 10000: Loss = -12402.61211468843
1
Iteration 10100: Loss = -12402.594815560822
Iteration 10200: Loss = -12402.594918188435
1
Iteration 10300: Loss = -12402.612897503375
2
Iteration 10400: Loss = -12402.594840686897
Iteration 10500: Loss = -12402.882371986414
1
Iteration 10600: Loss = -12402.594775214542
Iteration 10700: Loss = -12402.609802204968
1
Iteration 10800: Loss = -12402.594806878544
Iteration 10900: Loss = -12402.594836278666
Iteration 11000: Loss = -12402.608060942297
1
Iteration 11100: Loss = -12402.59478154159
Iteration 11200: Loss = -12402.59783151586
1
Iteration 11300: Loss = -12402.594894488653
2
Iteration 11400: Loss = -12402.595189723617
3
Iteration 11500: Loss = -12402.607384729947
4
Iteration 11600: Loss = -12402.594883443893
5
Iteration 11700: Loss = -12402.606320074878
6
Iteration 11800: Loss = -12402.59482326911
Iteration 11900: Loss = -12402.597491529617
1
Iteration 12000: Loss = -12402.594804799328
Iteration 12100: Loss = -12402.596212591221
1
Iteration 12200: Loss = -12402.61566908069
2
Iteration 12300: Loss = -12402.595900192897
3
Iteration 12400: Loss = -12402.596662293328
4
Iteration 12500: Loss = -12402.596222936965
5
Iteration 12600: Loss = -12402.596834814463
6
Iteration 12700: Loss = -12402.594923036646
7
Iteration 12800: Loss = -12402.606133408392
8
Iteration 12900: Loss = -12402.594927182216
9
Iteration 13000: Loss = -12402.594956750203
10
Iteration 13100: Loss = -12402.664889324906
11
Iteration 13200: Loss = -12402.631410029362
12
Iteration 13300: Loss = -12402.603841121869
13
Iteration 13400: Loss = -12402.594836118986
Iteration 13500: Loss = -12402.59493881931
1
Iteration 13600: Loss = -12402.59740401902
2
Iteration 13700: Loss = -12402.595950749152
3
Iteration 13800: Loss = -12402.600286680154
4
Iteration 13900: Loss = -12402.60622489027
5
Iteration 14000: Loss = -12402.598391929158
6
Iteration 14100: Loss = -12402.598881664479
7
Iteration 14200: Loss = -12402.640019077853
8
Iteration 14300: Loss = -12402.59482659578
Iteration 14400: Loss = -12402.595276933356
1
Iteration 14500: Loss = -12402.6350693487
2
Iteration 14600: Loss = -12402.599410324892
3
Iteration 14700: Loss = -12402.596327414054
4
Iteration 14800: Loss = -12402.59648518622
5
Iteration 14900: Loss = -12402.594853372364
Iteration 15000: Loss = -12402.597417084284
1
Iteration 15100: Loss = -12402.598846515803
2
Iteration 15200: Loss = -12402.603497962684
3
Iteration 15300: Loss = -12402.595190871116
4
Iteration 15400: Loss = -12402.595034163493
5
Iteration 15500: Loss = -12402.594909946349
Iteration 15600: Loss = -12402.59802798785
1
Iteration 15700: Loss = -12402.76552861308
2
Iteration 15800: Loss = -12402.594815157465
Iteration 15900: Loss = -12402.595537294903
1
Iteration 16000: Loss = -12402.59492968621
2
Iteration 16100: Loss = -12402.596213256897
3
Iteration 16200: Loss = -12402.595883877455
4
Iteration 16300: Loss = -12402.91320499614
5
Iteration 16400: Loss = -12402.594788439266
Iteration 16500: Loss = -12402.602663398251
1
Iteration 16600: Loss = -12402.659776629629
2
Iteration 16700: Loss = -12402.594834330052
Iteration 16800: Loss = -12402.594807519657
Iteration 16900: Loss = -12402.597481274923
1
Iteration 17000: Loss = -12402.6647768064
2
Iteration 17100: Loss = -12402.594943701217
3
Iteration 17200: Loss = -12402.59505444456
4
Iteration 17300: Loss = -12402.595675992698
5
Iteration 17400: Loss = -12402.654992421505
6
Iteration 17500: Loss = -12402.601807780751
7
Iteration 17600: Loss = -12402.595995187075
8
Iteration 17700: Loss = -12402.597626008182
9
Iteration 17800: Loss = -12402.599984862438
10
Iteration 17900: Loss = -12402.599451467793
11
Iteration 18000: Loss = -12402.619586445337
12
Iteration 18100: Loss = -12402.665655084329
13
Iteration 18200: Loss = -12402.597701354265
14
Iteration 18300: Loss = -12402.595271938711
15
Stopping early at iteration 18300 due to no improvement.
pi: tensor([[0.9409, 0.0591],
        [0.9879, 0.0121]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0432, 0.9568], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1995, 0.2058],
         [0.6945, 0.2036]],

        [[0.7252, 0.0961],
         [0.7150, 0.6757]],

        [[0.6343, 0.1799],
         [0.5148, 0.5017]],

        [[0.6455, 0.2401],
         [0.5347, 0.6694]],

        [[0.6688, 0.2194],
         [0.5634, 0.6843]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011794127935722436
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21947.942659559118
Iteration 100: Loss = -12404.983825929132
Iteration 200: Loss = -12404.435101358627
Iteration 300: Loss = -12404.306584882417
Iteration 400: Loss = -12404.230915794933
Iteration 500: Loss = -12404.158567355133
Iteration 600: Loss = -12404.070230603113
Iteration 700: Loss = -12403.95259233679
Iteration 800: Loss = -12403.796963972212
Iteration 900: Loss = -12403.611678115281
Iteration 1000: Loss = -12403.406670019192
Iteration 1100: Loss = -12403.040372995056
Iteration 1200: Loss = -12402.421035626287
Iteration 1300: Loss = -12401.932512725974
Iteration 1400: Loss = -12401.61760388824
Iteration 1500: Loss = -12401.406701656968
Iteration 1600: Loss = -12401.252045710247
Iteration 1700: Loss = -12401.130505178122
Iteration 1800: Loss = -12401.03309115763
Iteration 1900: Loss = -12400.955996664348
Iteration 2000: Loss = -12400.89640361524
Iteration 2100: Loss = -12400.850355632174
Iteration 2200: Loss = -12400.814359421494
Iteration 2300: Loss = -12400.785618201995
Iteration 2400: Loss = -12400.762176405127
Iteration 2500: Loss = -12400.742604590405
Iteration 2600: Loss = -12400.726035274012
Iteration 2700: Loss = -12400.711903002977
Iteration 2800: Loss = -12400.699642829753
Iteration 2900: Loss = -12400.68905075892
Iteration 3000: Loss = -12400.679904118993
Iteration 3100: Loss = -12400.671885394293
Iteration 3200: Loss = -12400.664815689493
Iteration 3300: Loss = -12400.658548722751
Iteration 3400: Loss = -12400.653004834663
Iteration 3500: Loss = -12400.648013731305
Iteration 3600: Loss = -12400.643550693521
Iteration 3700: Loss = -12400.63948185981
Iteration 3800: Loss = -12400.63579339445
Iteration 3900: Loss = -12400.632426817454
Iteration 4000: Loss = -12400.629277675052
Iteration 4100: Loss = -12400.62642369679
Iteration 4200: Loss = -12400.62372418144
Iteration 4300: Loss = -12400.62122956544
Iteration 4400: Loss = -12400.618907147318
Iteration 4500: Loss = -12400.616639481344
Iteration 4600: Loss = -12400.614535824801
Iteration 4700: Loss = -12400.612587221027
Iteration 4800: Loss = -12400.610626520831
Iteration 4900: Loss = -12400.608845093246
Iteration 5000: Loss = -12400.607091335918
Iteration 5100: Loss = -12400.605359905372
Iteration 5200: Loss = -12400.603662512538
Iteration 5300: Loss = -12400.602100673665
Iteration 5400: Loss = -12400.600465233238
Iteration 5500: Loss = -12400.598834085731
Iteration 5600: Loss = -12400.59721255033
Iteration 5700: Loss = -12400.595550081005
Iteration 5800: Loss = -12400.59383469565
Iteration 5900: Loss = -12400.592006734365
Iteration 6000: Loss = -12400.590138229241
Iteration 6100: Loss = -12400.588123631796
Iteration 6200: Loss = -12400.585992262386
Iteration 6300: Loss = -12400.583624923232
Iteration 6400: Loss = -12400.581039854553
Iteration 6500: Loss = -12400.578120894057
Iteration 6600: Loss = -12400.574824707375
Iteration 6700: Loss = -12400.571066156617
Iteration 6800: Loss = -12400.566638625136
Iteration 6900: Loss = -12400.56117030494
Iteration 7000: Loss = -12400.553721472963
Iteration 7100: Loss = -12400.224079644086
Iteration 7200: Loss = -12399.243650133008
Iteration 7300: Loss = -12399.094947295343
Iteration 7400: Loss = -12399.039451414634
Iteration 7500: Loss = -12399.00947625944
Iteration 7600: Loss = -12398.990878375067
Iteration 7700: Loss = -12398.980746141842
Iteration 7800: Loss = -12398.968649715938
Iteration 7900: Loss = -12398.961388868069
Iteration 8000: Loss = -12399.249343407244
1
Iteration 8100: Loss = -12398.949836345175
Iteration 8200: Loss = -12398.115359691827
Iteration 8300: Loss = -12398.10420268079
Iteration 8400: Loss = -12398.279111610147
1
Iteration 8500: Loss = -12398.090686227331
Iteration 8600: Loss = -12398.086204162131
Iteration 8700: Loss = -12398.0825544203
Iteration 8800: Loss = -12398.081615441435
Iteration 8900: Loss = -12398.07704358059
Iteration 9000: Loss = -12398.074968632745
Iteration 9100: Loss = -12398.238994149877
1
Iteration 9200: Loss = -12398.071577073883
Iteration 9300: Loss = -12398.070226371356
Iteration 9400: Loss = -12398.06901365417
Iteration 9500: Loss = -12398.068669074728
Iteration 9600: Loss = -12398.066927763868
Iteration 9700: Loss = -12398.066098869214
Iteration 9800: Loss = -12398.3626053178
1
Iteration 9900: Loss = -12398.064658703524
Iteration 10000: Loss = -12398.064002003734
Iteration 10100: Loss = -12398.063403333921
Iteration 10200: Loss = -12398.0634794647
Iteration 10300: Loss = -12398.062404486094
Iteration 10400: Loss = -12398.061975977602
Iteration 10500: Loss = -12398.072335346338
1
Iteration 10600: Loss = -12398.061172948164
Iteration 10700: Loss = -12398.060866817692
Iteration 10800: Loss = -12398.062142092958
1
Iteration 10900: Loss = -12398.060263952
Iteration 11000: Loss = -12398.05996498712
Iteration 11100: Loss = -12398.059745737019
Iteration 11200: Loss = -12398.066387184517
1
Iteration 11300: Loss = -12398.059237265003
Iteration 11400: Loss = -12398.05903261182
Iteration 11500: Loss = -12398.0650750555
1
Iteration 11600: Loss = -12398.058671264862
Iteration 11700: Loss = -12398.058480674594
Iteration 11800: Loss = -12398.058313467965
Iteration 11900: Loss = -12398.058301596624
Iteration 12000: Loss = -12398.05802016666
Iteration 12100: Loss = -12398.057890661976
Iteration 12200: Loss = -12398.057813766958
Iteration 12300: Loss = -12398.057625034427
Iteration 12400: Loss = -12398.203245943321
1
Iteration 12500: Loss = -12398.057426507898
Iteration 12600: Loss = -12398.058201619859
1
Iteration 12700: Loss = -12398.05728256772
Iteration 12800: Loss = -12398.05750350851
1
Iteration 12900: Loss = -12398.098499549267
2
Iteration 13000: Loss = -12398.057055431067
Iteration 13100: Loss = -12398.391367411834
1
Iteration 13200: Loss = -12398.05686839065
Iteration 13300: Loss = -12398.240792557028
1
Iteration 13400: Loss = -12398.056954656173
Iteration 13500: Loss = -12398.057574643284
1
Iteration 13600: Loss = -12398.056821752518
Iteration 13700: Loss = -12398.056829147434
Iteration 13800: Loss = -12398.056616282265
Iteration 13900: Loss = -12398.056883247946
1
Iteration 14000: Loss = -12398.056608716111
Iteration 14100: Loss = -12398.057477947506
1
Iteration 14200: Loss = -12398.057168076142
2
Iteration 14300: Loss = -12398.056886288836
3
Iteration 14400: Loss = -12398.057867921843
4
Iteration 14500: Loss = -12398.056405888838
Iteration 14600: Loss = -12398.063060651968
1
Iteration 14700: Loss = -12398.05646442707
Iteration 14800: Loss = -12398.056305163404
Iteration 14900: Loss = -12398.05755812133
1
Iteration 15000: Loss = -12398.056275002618
Iteration 15100: Loss = -12398.056864622313
1
Iteration 15200: Loss = -12398.056398320956
2
Iteration 15300: Loss = -12398.05620160926
Iteration 15400: Loss = -12398.056121313019
Iteration 15500: Loss = -12398.056276293824
1
Iteration 15600: Loss = -12398.05972619936
2
Iteration 15700: Loss = -12398.057664350363
3
Iteration 15800: Loss = -12398.061814729417
4
Iteration 15900: Loss = -12398.057012018091
5
Iteration 16000: Loss = -12398.057016848921
6
Iteration 16100: Loss = -12398.056113664967
Iteration 16200: Loss = -12398.056120508767
Iteration 16300: Loss = -12398.0594381278
1
Iteration 16400: Loss = -12398.05600680864
Iteration 16500: Loss = -12398.063106145679
1
Iteration 16600: Loss = -12398.056142723186
2
Iteration 16700: Loss = -12398.056506989038
3
Iteration 16800: Loss = -12398.068563222341
4
Iteration 16900: Loss = -12398.055992607235
Iteration 17000: Loss = -12398.197139068461
1
Iteration 17100: Loss = -12398.055942445642
Iteration 17200: Loss = -12398.061641662309
1
Iteration 17300: Loss = -12398.055919552247
Iteration 17400: Loss = -12398.056383745263
1
Iteration 17500: Loss = -12398.05596640313
Iteration 17600: Loss = -12398.056191757152
1
Iteration 17700: Loss = -12398.058494472763
2
Iteration 17800: Loss = -12398.05585353939
Iteration 17900: Loss = -12398.090414260529
1
Iteration 18000: Loss = -12398.056088227873
2
Iteration 18100: Loss = -12398.056056697573
3
Iteration 18200: Loss = -12398.152532793887
4
Iteration 18300: Loss = -12398.058389950482
5
Iteration 18400: Loss = -12398.057189724446
6
Iteration 18500: Loss = -12398.057045813242
7
Iteration 18600: Loss = -12398.055846796447
Iteration 18700: Loss = -12398.056705683752
1
Iteration 18800: Loss = -12398.056798486752
2
Iteration 18900: Loss = -12398.098363857769
3
Iteration 19000: Loss = -12398.056135420642
4
Iteration 19100: Loss = -12398.055832121845
Iteration 19200: Loss = -12398.097831960677
1
Iteration 19300: Loss = -12398.056128685093
2
Iteration 19400: Loss = -12398.067803645432
3
Iteration 19500: Loss = -12398.055719706086
Iteration 19600: Loss = -12398.057527711408
1
Iteration 19700: Loss = -12398.055757952217
Iteration 19800: Loss = -12398.05674919661
1
Iteration 19900: Loss = -12398.055852510734
pi: tensor([[1.0000e+00, 4.8690e-09],
        [1.4791e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9628, 0.0372], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.9900e-01, 2.1194e-01],
         [5.0213e-01, 1.4071e-06]],

        [[6.9412e-01, 1.3648e-01],
         [7.2314e-01, 6.6963e-01]],

        [[5.7980e-01, 2.3814e-01],
         [5.1826e-01, 7.2907e-01]],

        [[6.1662e-01, 2.5725e-01],
         [5.7141e-01, 5.5624e-01]],

        [[5.5803e-01, 2.7336e-01],
         [6.1403e-01, 6.8287e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015539260913902783
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.008260141115681973
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0006921612735767433
Global Adjusted Rand Index: -0.0006306600706604942
Average Adjusted Rand Index: -0.002774553765487786
11842.7333971895
[-0.0011794127935722436, -0.0006306600706604942] [-0.000982071485668608, -0.002774553765487786] [12402.595271938711, 12398.055746017699]
-------------------------------------
This iteration is 31
True Objective function: Loss = -11770.017227971859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21842.947631453517
Iteration 100: Loss = -12310.394037071634
Iteration 200: Loss = -12309.974930772873
Iteration 300: Loss = -12309.841092153734
Iteration 400: Loss = -12309.787781183852
Iteration 500: Loss = -12309.762652981115
Iteration 600: Loss = -12309.746880488603
Iteration 700: Loss = -12309.734225720526
Iteration 800: Loss = -12309.722280046386
Iteration 900: Loss = -12309.709816135233
Iteration 1000: Loss = -12309.695929021958
Iteration 1100: Loss = -12309.679729955962
Iteration 1200: Loss = -12309.660330767183
Iteration 1300: Loss = -12309.63714768953
Iteration 1400: Loss = -12309.610011650404
Iteration 1500: Loss = -12309.578889061406
Iteration 1600: Loss = -12309.542614123526
Iteration 1700: Loss = -12309.499519733667
Iteration 1800: Loss = -12309.45108453199
Iteration 1900: Loss = -12309.406183565865
Iteration 2000: Loss = -12309.372655238158
Iteration 2100: Loss = -12309.349392699483
Iteration 2200: Loss = -12309.333240520686
Iteration 2300: Loss = -12309.321810925783
Iteration 2400: Loss = -12309.313878418789
Iteration 2500: Loss = -12309.308239657244
Iteration 2600: Loss = -12309.304117961014
Iteration 2700: Loss = -12309.300940623449
Iteration 2800: Loss = -12309.298271426882
Iteration 2900: Loss = -12309.295754389352
Iteration 3000: Loss = -12309.293306573336
Iteration 3100: Loss = -12309.290545351038
Iteration 3200: Loss = -12309.287406933941
Iteration 3300: Loss = -12309.283600358676
Iteration 3400: Loss = -12309.278876020804
Iteration 3500: Loss = -12309.272812019377
Iteration 3600: Loss = -12309.264832939085
Iteration 3700: Loss = -12309.254051899237
Iteration 3800: Loss = -12309.239117364572
Iteration 3900: Loss = -12309.217369456986
Iteration 4000: Loss = -12309.180279871745
Iteration 4100: Loss = -12309.059019042252
Iteration 4200: Loss = -12306.979622033285
Iteration 4300: Loss = -12304.796422282294
Iteration 4400: Loss = -12304.19301085108
Iteration 4500: Loss = -12303.931418153603
Iteration 4600: Loss = -12303.788595274422
Iteration 4700: Loss = -12303.69876160883
Iteration 4800: Loss = -12303.638306404342
Iteration 4900: Loss = -12303.594906611823
Iteration 5000: Loss = -12303.56245901064
Iteration 5100: Loss = -12303.53738449654
Iteration 5200: Loss = -12303.517523792998
Iteration 5300: Loss = -12303.50140519259
Iteration 5400: Loss = -12303.48820683333
Iteration 5500: Loss = -12303.477190440875
Iteration 5600: Loss = -12303.467881412795
Iteration 5700: Loss = -12303.460676812696
Iteration 5800: Loss = -12303.453187585063
Iteration 5900: Loss = -12303.447262425676
Iteration 6000: Loss = -12303.44257321271
Iteration 6100: Loss = -12303.437675658866
Iteration 6200: Loss = -12303.433637700526
Iteration 6300: Loss = -12303.430334292534
Iteration 6400: Loss = -12303.427058583358
Iteration 6500: Loss = -12303.424310638606
Iteration 6600: Loss = -12303.4218797089
Iteration 6700: Loss = -12303.419563172247
Iteration 6800: Loss = -12303.417603727243
Iteration 6900: Loss = -12303.415792492571
Iteration 7000: Loss = -12303.414179441095
Iteration 7100: Loss = -12303.412688952054
Iteration 7200: Loss = -12303.472778527042
1
Iteration 7300: Loss = -12303.410191998231
Iteration 7400: Loss = -12303.414881789538
1
Iteration 7500: Loss = -12303.408079982275
Iteration 7600: Loss = -12303.42940346775
1
Iteration 7700: Loss = -12303.406426162026
Iteration 7800: Loss = -12303.405650326182
Iteration 7900: Loss = -12303.405378009402
Iteration 8000: Loss = -12303.404353175645
Iteration 8100: Loss = -12303.403793490794
Iteration 8200: Loss = -12303.403377720875
Iteration 8300: Loss = -12303.402830264986
Iteration 8400: Loss = -12303.402430279273
Iteration 8500: Loss = -12303.402134430149
Iteration 8600: Loss = -12303.401713627274
Iteration 8700: Loss = -12303.401363362704
Iteration 8800: Loss = -12303.40110885669
Iteration 8900: Loss = -12303.402855151418
1
Iteration 9000: Loss = -12303.400618866326
Iteration 9100: Loss = -12303.400371535134
Iteration 9200: Loss = -12303.400255476108
Iteration 9300: Loss = -12303.400020867497
Iteration 9400: Loss = -12303.399831496374
Iteration 9500: Loss = -12303.3997790579
Iteration 9600: Loss = -12303.39966639906
Iteration 9700: Loss = -12303.399453976781
Iteration 9800: Loss = -12303.399341067498
Iteration 9900: Loss = -12303.399337213592
Iteration 10000: Loss = -12303.399135082074
Iteration 10100: Loss = -12303.399076683518
Iteration 10200: Loss = -12303.399018150734
Iteration 10300: Loss = -12303.39905354069
Iteration 10400: Loss = -12303.398877830025
Iteration 10500: Loss = -12303.398805038534
Iteration 10600: Loss = -12303.399261089844
1
Iteration 10700: Loss = -12303.398724622482
Iteration 10800: Loss = -12303.398677225496
Iteration 10900: Loss = -12303.399793000543
1
Iteration 11000: Loss = -12303.398615653623
Iteration 11100: Loss = -12303.398569299496
Iteration 11200: Loss = -12303.45350082075
1
Iteration 11300: Loss = -12303.398510746392
Iteration 11400: Loss = -12303.398520784658
Iteration 11500: Loss = -12303.398500127067
Iteration 11600: Loss = -12303.40151920046
1
Iteration 11700: Loss = -12303.398416484843
Iteration 11800: Loss = -12303.398359744613
Iteration 11900: Loss = -12303.507562722581
1
Iteration 12000: Loss = -12303.398349604877
Iteration 12100: Loss = -12303.39831891727
Iteration 12200: Loss = -12303.670184485783
1
Iteration 12300: Loss = -12303.398277287546
Iteration 12400: Loss = -12303.400825205628
1
Iteration 12500: Loss = -12303.398335786287
Iteration 12600: Loss = -12303.399854592131
1
Iteration 12700: Loss = -12303.398425305511
Iteration 12800: Loss = -12303.502935048264
1
Iteration 12900: Loss = -12303.398217373247
Iteration 13000: Loss = -12303.419971195784
1
Iteration 13100: Loss = -12303.398231867131
Iteration 13200: Loss = -12303.399260352311
1
Iteration 13300: Loss = -12303.403259702052
2
Iteration 13400: Loss = -12303.39830580788
Iteration 13500: Loss = -12303.39849173706
1
Iteration 13600: Loss = -12303.548416510479
2
Iteration 13700: Loss = -12303.39847976678
3
Iteration 13800: Loss = -12303.483467950238
4
Iteration 13900: Loss = -12303.400601152502
5
Iteration 14000: Loss = -12303.400496843364
6
Iteration 14100: Loss = -12303.39862338691
7
Iteration 14200: Loss = -12303.398916867787
8
Iteration 14300: Loss = -12303.402118884798
9
Iteration 14400: Loss = -12303.398186902388
Iteration 14500: Loss = -12303.398302674963
1
Iteration 14600: Loss = -12303.398235148901
Iteration 14700: Loss = -12303.414407607257
1
Iteration 14800: Loss = -12303.399194846816
2
Iteration 14900: Loss = -12303.409593122655
3
Iteration 15000: Loss = -12303.399937839478
4
Iteration 15100: Loss = -12303.400451036418
5
Iteration 15200: Loss = -12303.398171973053
Iteration 15300: Loss = -12303.399749069487
1
Iteration 15400: Loss = -12303.517649055275
2
Iteration 15500: Loss = -12303.398440174577
3
Iteration 15600: Loss = -12303.399472621973
4
Iteration 15700: Loss = -12303.398155174413
Iteration 15800: Loss = -12303.39877480844
1
Iteration 15900: Loss = -12303.398758494399
2
Iteration 16000: Loss = -12303.420083508341
3
Iteration 16100: Loss = -12303.398548747573
4
Iteration 16200: Loss = -12303.665028208956
5
Iteration 16300: Loss = -12303.399438440914
6
Iteration 16400: Loss = -12303.412231631992
7
Iteration 16500: Loss = -12303.399665621135
8
Iteration 16600: Loss = -12303.399221651229
9
Iteration 16700: Loss = -12303.399564467847
10
Iteration 16800: Loss = -12303.398715782647
11
Iteration 16900: Loss = -12303.40505338898
12
Iteration 17000: Loss = -12303.398804365514
13
Iteration 17100: Loss = -12303.422328738776
14
Iteration 17200: Loss = -12303.399203517545
15
Stopping early at iteration 17200 due to no improvement.
pi: tensor([[1.0000e+00, 1.1706e-07],
        [5.2021e-02, 9.4798e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9532, 0.0468], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.9591e-01, 2.2666e-01],
         [7.0330e-01, 2.9791e-04]],

        [[5.1989e-01, 2.7697e-01],
         [5.1316e-01, 7.1932e-01]],

        [[5.3443e-01, 2.2837e-01],
         [6.9471e-01, 7.0285e-01]],

        [[7.2527e-01, 1.3188e-01],
         [6.0964e-01, 7.2831e-01]],

        [[7.1036e-01, 2.4024e-01],
         [6.0308e-01, 6.6015e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0015169269193791859
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0005624593535232805
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0006921612735767433
Global Adjusted Rand Index: -0.0003054512639248836
Average Adjusted Rand Index: 0.002611476421421078
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20445.523755569502
Iteration 100: Loss = -12310.091962317849
Iteration 200: Loss = -12309.863983777193
Iteration 300: Loss = -12309.812687623691
Iteration 400: Loss = -12309.789178976816
Iteration 500: Loss = -12309.776154209274
Iteration 600: Loss = -12309.767468108652
Iteration 700: Loss = -12309.760904376257
Iteration 800: Loss = -12309.755633379125
Iteration 900: Loss = -12309.75110683016
Iteration 1000: Loss = -12309.74697244801
Iteration 1100: Loss = -12309.743049628078
Iteration 1200: Loss = -12309.73904421491
Iteration 1300: Loss = -12309.734637326743
Iteration 1400: Loss = -12309.729393342057
Iteration 1500: Loss = -12309.722548558386
Iteration 1600: Loss = -12309.712759946762
Iteration 1700: Loss = -12309.697319101764
Iteration 1800: Loss = -12309.670536722055
Iteration 1900: Loss = -12309.624441705546
Iteration 2000: Loss = -12309.561358719382
Iteration 2100: Loss = -12309.480534158782
Iteration 2200: Loss = -12309.386591090664
Iteration 2300: Loss = -12309.33692987894
Iteration 2400: Loss = -12309.31817546175
Iteration 2500: Loss = -12309.309281456579
Iteration 2600: Loss = -12309.304369943733
Iteration 2700: Loss = -12309.301135216538
Iteration 2800: Loss = -12309.298162691366
Iteration 2900: Loss = -12309.294862275601
Iteration 3000: Loss = -12309.290486903896
Iteration 3100: Loss = -12309.284273708283
Iteration 3200: Loss = -12309.274934611518
Iteration 3300: Loss = -12309.260278604344
Iteration 3400: Loss = -12309.236882735871
Iteration 3500: Loss = -12309.197725669677
Iteration 3600: Loss = -12309.076123111141
Iteration 3700: Loss = -12306.63319172788
Iteration 3800: Loss = -12304.698897866956
Iteration 3900: Loss = -12304.156338633456
Iteration 4000: Loss = -12303.913592690866
Iteration 4100: Loss = -12303.778039561335
Iteration 4200: Loss = -12303.692394409467
Iteration 4300: Loss = -12303.633866847247
Iteration 4400: Loss = -12303.591683555574
Iteration 4500: Loss = -12303.559949146496
Iteration 4600: Loss = -12303.53537095022
Iteration 4700: Loss = -12303.516697172958
Iteration 4800: Loss = -12303.499973090029
Iteration 4900: Loss = -12303.486911384758
Iteration 5000: Loss = -12303.476054878132
Iteration 5100: Loss = -12303.466817068838
Iteration 5200: Loss = -12303.458958339474
Iteration 5300: Loss = -12303.452253568985
Iteration 5400: Loss = -12303.446388651042
Iteration 5500: Loss = -12303.441292584685
Iteration 5600: Loss = -12303.43686711365
Iteration 5700: Loss = -12303.432883407833
Iteration 5800: Loss = -12303.431201796002
Iteration 5900: Loss = -12303.42629495151
Iteration 6000: Loss = -12303.42356586979
Iteration 6100: Loss = -12303.421384248597
Iteration 6200: Loss = -12303.41884374997
Iteration 6300: Loss = -12303.41691285541
Iteration 6400: Loss = -12303.415164283897
Iteration 6500: Loss = -12303.413523373769
Iteration 6600: Loss = -12303.413880918699
1
Iteration 6700: Loss = -12303.410749503593
Iteration 6800: Loss = -12303.409563490703
Iteration 6900: Loss = -12303.408588976083
Iteration 7000: Loss = -12303.40749829355
Iteration 7100: Loss = -12303.406605106355
Iteration 7200: Loss = -12303.412022447415
1
Iteration 7300: Loss = -12303.405117490787
Iteration 7400: Loss = -12303.406400037402
1
Iteration 7500: Loss = -12303.40381959492
Iteration 7600: Loss = -12303.404105806147
1
Iteration 7700: Loss = -12303.402774362037
Iteration 7800: Loss = -12303.730999307689
1
Iteration 7900: Loss = -12303.401926486695
Iteration 8000: Loss = -12303.401552077234
Iteration 8100: Loss = -12303.403181620624
1
Iteration 8200: Loss = -12303.400905327399
Iteration 8300: Loss = -12303.400648495728
Iteration 8400: Loss = -12303.40934720277
1
Iteration 8500: Loss = -12303.400153707666
Iteration 8600: Loss = -12303.399949401883
Iteration 8700: Loss = -12303.399767296229
Iteration 8800: Loss = -12303.40266170921
1
Iteration 8900: Loss = -12303.399467307438
Iteration 9000: Loss = -12303.399361646112
Iteration 9100: Loss = -12303.399229189437
Iteration 9200: Loss = -12303.399191625525
Iteration 9300: Loss = -12303.398992719267
Iteration 9400: Loss = -12303.398902125344
Iteration 9500: Loss = -12303.675158335858
1
Iteration 9600: Loss = -12303.398739594772
Iteration 9700: Loss = -12303.398654941668
Iteration 9800: Loss = -12303.398638887309
Iteration 9900: Loss = -12303.39925478173
1
Iteration 10000: Loss = -12303.398581216949
Iteration 10100: Loss = -12303.398499001232
Iteration 10200: Loss = -12303.400970372568
1
Iteration 10300: Loss = -12303.398441764071
Iteration 10400: Loss = -12303.398411829226
Iteration 10500: Loss = -12303.398376588571
Iteration 10600: Loss = -12303.488858756898
1
Iteration 10700: Loss = -12303.398363693928
Iteration 10800: Loss = -12303.399337638992
1
Iteration 10900: Loss = -12303.39830502276
Iteration 11000: Loss = -12303.398773780465
1
Iteration 11100: Loss = -12303.398291067124
Iteration 11200: Loss = -12303.608345690485
1
Iteration 11300: Loss = -12303.39833661723
Iteration 11400: Loss = -12303.610502570793
1
Iteration 11500: Loss = -12303.40057281436
2
Iteration 11600: Loss = -12303.398492103812
3
Iteration 11700: Loss = -12303.404903120412
4
Iteration 11800: Loss = -12303.39855466485
5
Iteration 11900: Loss = -12303.39963881974
6
Iteration 12000: Loss = -12303.398248937072
Iteration 12100: Loss = -12303.510744552083
1
Iteration 12200: Loss = -12303.398209806304
Iteration 12300: Loss = -12303.465349282233
1
Iteration 12400: Loss = -12303.398637801129
2
Iteration 12500: Loss = -12303.398378478845
3
Iteration 12600: Loss = -12303.53644319683
4
Iteration 12700: Loss = -12303.399005704583
5
Iteration 12800: Loss = -12303.39829144493
Iteration 12900: Loss = -12303.398490013218
1
Iteration 13000: Loss = -12303.403617941041
2
Iteration 13100: Loss = -12303.398646922824
3
Iteration 13200: Loss = -12303.413330003157
4
Iteration 13300: Loss = -12303.398369883862
Iteration 13400: Loss = -12303.398723173867
1
Iteration 13500: Loss = -12303.398274010373
Iteration 13600: Loss = -12303.40192700119
1
Iteration 13700: Loss = -12303.398742122014
2
Iteration 13800: Loss = -12303.398487644366
3
Iteration 13900: Loss = -12303.401357602752
4
Iteration 14000: Loss = -12303.398195593878
Iteration 14100: Loss = -12303.447633150821
1
Iteration 14200: Loss = -12303.399441781912
2
Iteration 14300: Loss = -12303.403414023263
3
Iteration 14400: Loss = -12303.398593405353
4
Iteration 14500: Loss = -12303.419754357836
5
Iteration 14600: Loss = -12303.401079933328
6
Iteration 14700: Loss = -12303.398366796671
7
Iteration 14800: Loss = -12303.398393952388
8
Iteration 14900: Loss = -12303.398177642874
Iteration 15000: Loss = -12303.398567120985
1
Iteration 15100: Loss = -12303.445565728334
2
Iteration 15200: Loss = -12303.43671918463
3
Iteration 15300: Loss = -12303.399281682796
4
Iteration 15400: Loss = -12303.398311275248
5
Iteration 15500: Loss = -12303.401100287625
6
Iteration 15600: Loss = -12303.402352473082
7
Iteration 15700: Loss = -12303.398226030487
Iteration 15800: Loss = -12303.413305526994
1
Iteration 15900: Loss = -12303.39848572323
2
Iteration 16000: Loss = -12303.398578850003
3
Iteration 16100: Loss = -12303.413683565359
4
Iteration 16200: Loss = -12303.398441671035
5
Iteration 16300: Loss = -12303.406365016135
6
Iteration 16400: Loss = -12303.398705439551
7
Iteration 16500: Loss = -12303.404819388432
8
Iteration 16600: Loss = -12303.400978679892
9
Iteration 16700: Loss = -12303.39968546456
10
Iteration 16800: Loss = -12303.399599226661
11
Iteration 16900: Loss = -12303.39942936025
12
Iteration 17000: Loss = -12303.402193989961
13
Iteration 17100: Loss = -12303.398324555626
Iteration 17200: Loss = -12303.400062091208
1
Iteration 17300: Loss = -12303.39856000473
2
Iteration 17400: Loss = -12303.398155017981
Iteration 17500: Loss = -12303.399679282487
1
Iteration 17600: Loss = -12303.398186041799
Iteration 17700: Loss = -12303.425416378002
1
Iteration 17800: Loss = -12303.39940181519
2
Iteration 17900: Loss = -12303.476427064425
3
Iteration 18000: Loss = -12303.400622624597
4
Iteration 18100: Loss = -12303.402825742021
5
Iteration 18200: Loss = -12303.400327589692
6
Iteration 18300: Loss = -12303.398941300557
7
Iteration 18400: Loss = -12303.398321819715
8
Iteration 18500: Loss = -12303.402953917586
9
Iteration 18600: Loss = -12303.402336616546
10
Iteration 18700: Loss = -12303.408766138495
11
Iteration 18800: Loss = -12303.398727362082
12
Iteration 18900: Loss = -12303.398314578304
13
Iteration 19000: Loss = -12303.399391043555
14
Iteration 19100: Loss = -12303.411722852394
15
Stopping early at iteration 19100 due to no improvement.
pi: tensor([[1.0000e+00, 2.2375e-08],
        [5.1623e-02, 9.4838e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9534, 0.0466], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.9624e-01, 2.2578e-01],
         [5.5923e-01, 2.9819e-04]],

        [[5.8344e-01, 2.7749e-01],
         [6.8361e-01, 6.6533e-01]],

        [[7.1844e-01, 2.2841e-01],
         [6.0614e-01, 5.3394e-01]],

        [[5.7278e-01, 1.3265e-01],
         [5.7767e-01, 7.2395e-01]],

        [[7.2074e-01, 2.4003e-01],
         [5.3984e-01, 6.8691e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0015169269193791859
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0005624593535232805
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0006921612735767433
Global Adjusted Rand Index: -0.0003054512639248836
Average Adjusted Rand Index: 0.002611476421421078
11770.017227971859
[-0.0003054512639248836, -0.0003054512639248836] [0.002611476421421078, 0.002611476421421078] [12303.399203517545, 12303.411722852394]
-------------------------------------
This iteration is 32
True Objective function: Loss = -11871.965429111695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20176.266929391848
Iteration 100: Loss = -12440.608387489374
Iteration 200: Loss = -12440.244485757587
Iteration 300: Loss = -12440.165876897352
Iteration 400: Loss = -12440.132412662248
Iteration 500: Loss = -12440.109536791606
Iteration 600: Loss = -12440.089534177712
Iteration 700: Loss = -12440.0720297193
Iteration 800: Loss = -12440.05726388284
Iteration 900: Loss = -12440.043837295409
Iteration 1000: Loss = -12440.029897866338
Iteration 1100: Loss = -12440.014369680433
Iteration 1200: Loss = -12439.996434237959
Iteration 1300: Loss = -12439.975842405654
Iteration 1400: Loss = -12439.951928748254
Iteration 1500: Loss = -12439.92408363406
Iteration 1600: Loss = -12439.890992245322
Iteration 1700: Loss = -12439.851458969111
Iteration 1800: Loss = -12439.805274860073
Iteration 1900: Loss = -12439.754085513014
Iteration 2000: Loss = -12439.70176393509
Iteration 2100: Loss = -12439.653072266216
Iteration 2200: Loss = -12439.61228900402
Iteration 2300: Loss = -12439.579576850101
Iteration 2400: Loss = -12439.560277589057
Iteration 2500: Loss = -12439.535912374056
Iteration 2600: Loss = -12439.52153174262
Iteration 2700: Loss = -12439.510257879465
Iteration 2800: Loss = -12439.50116522355
Iteration 2900: Loss = -12439.493537578142
Iteration 3000: Loss = -12439.486948910158
Iteration 3100: Loss = -12439.48118586023
Iteration 3200: Loss = -12439.475936179044
Iteration 3300: Loss = -12439.471190210514
Iteration 3400: Loss = -12439.466751423139
Iteration 3500: Loss = -12439.462622309024
Iteration 3600: Loss = -12439.458836342385
Iteration 3700: Loss = -12439.455291429305
Iteration 3800: Loss = -12439.45197803678
Iteration 3900: Loss = -12439.448727528918
Iteration 4000: Loss = -12439.445458000724
Iteration 4100: Loss = -12439.441684860383
Iteration 4200: Loss = -12439.43666627116
Iteration 4300: Loss = -12439.43012185575
Iteration 4400: Loss = -12439.423479949286
Iteration 4500: Loss = -12439.416736385503
Iteration 4600: Loss = -12439.411066983555
Iteration 4700: Loss = -12439.406455257209
Iteration 4800: Loss = -12439.402069233527
Iteration 4900: Loss = -12439.398818068577
Iteration 5000: Loss = -12439.395633130689
Iteration 5100: Loss = -12439.394339168486
Iteration 5200: Loss = -12439.391057607278
Iteration 5300: Loss = -12439.388933543343
Iteration 5400: Loss = -12439.38730562797
Iteration 5500: Loss = -12439.385760864332
Iteration 5600: Loss = -12439.3839316767
Iteration 5700: Loss = -12439.382431849046
Iteration 5800: Loss = -12439.381252205736
Iteration 5900: Loss = -12439.379329819294
Iteration 6000: Loss = -12439.37732507102
Iteration 6100: Loss = -12439.375163948815
Iteration 6200: Loss = -12439.372214117628
Iteration 6300: Loss = -12439.373458250151
1
Iteration 6400: Loss = -12439.35943342005
Iteration 6500: Loss = -12439.339541657306
Iteration 6600: Loss = -12439.247413977426
Iteration 6700: Loss = -12438.922314276699
Iteration 6800: Loss = -12438.850945170025
Iteration 6900: Loss = -12438.767625903338
Iteration 7000: Loss = -12438.294143719264
Iteration 7100: Loss = -12438.199422678717
Iteration 7200: Loss = -12438.184176695297
Iteration 7300: Loss = -12438.173885247057
Iteration 7400: Loss = -12438.169821462496
Iteration 7500: Loss = -12438.16710171837
Iteration 7600: Loss = -12438.165156796333
Iteration 7700: Loss = -12438.163648328165
Iteration 7800: Loss = -12438.164388660087
1
Iteration 7900: Loss = -12438.16162025051
Iteration 8000: Loss = -12438.160857095885
Iteration 8100: Loss = -12438.160194266049
Iteration 8200: Loss = -12438.162392156091
1
Iteration 8300: Loss = -12438.159195456008
Iteration 8400: Loss = -12438.158775898883
Iteration 8500: Loss = -12438.481272122912
1
Iteration 8600: Loss = -12438.158147366192
Iteration 8700: Loss = -12438.157885813678
Iteration 8800: Loss = -12438.157657372944
Iteration 8900: Loss = -12438.159673048889
1
Iteration 9000: Loss = -12438.157216704007
Iteration 9100: Loss = -12438.157068569271
Iteration 9200: Loss = -12438.467714560416
1
Iteration 9300: Loss = -12438.156790728588
Iteration 9400: Loss = -12438.156628809242
Iteration 9500: Loss = -12438.156507866295
Iteration 9600: Loss = -12438.157691441716
1
Iteration 9700: Loss = -12438.156251339227
Iteration 9800: Loss = -12438.156115288299
Iteration 9900: Loss = -12438.156068022847
Iteration 10000: Loss = -12438.15600022792
Iteration 10100: Loss = -12438.15582906458
Iteration 10200: Loss = -12438.155809534073
Iteration 10300: Loss = -12438.155684049836
Iteration 10400: Loss = -12438.15586724457
1
Iteration 10500: Loss = -12438.155598055293
Iteration 10600: Loss = -12438.155549236973
Iteration 10700: Loss = -12438.159361597196
1
Iteration 10800: Loss = -12438.15546278596
Iteration 10900: Loss = -12438.155414239473
Iteration 11000: Loss = -12438.155379623273
Iteration 11100: Loss = -12438.155330572003
Iteration 11200: Loss = -12438.155257566808
Iteration 11300: Loss = -12438.155252953862
Iteration 11400: Loss = -12438.155201822212
Iteration 11500: Loss = -12438.155184848496
Iteration 11600: Loss = -12438.155154374423
Iteration 11700: Loss = -12438.155153949718
Iteration 11800: Loss = -12438.155104506308
Iteration 11900: Loss = -12438.155079256583
Iteration 12000: Loss = -12438.155173515453
Iteration 12100: Loss = -12438.155040142503
Iteration 12200: Loss = -12438.154902562317
Iteration 12300: Loss = -12438.191066939069
1
Iteration 12400: Loss = -12438.154682062346
Iteration 12500: Loss = -12438.154730153134
Iteration 12600: Loss = -12438.16302746387
1
Iteration 12700: Loss = -12438.15467051254
Iteration 12800: Loss = -12438.154668216146
Iteration 12900: Loss = -12438.154665238204
Iteration 13000: Loss = -12438.155827753983
1
Iteration 13100: Loss = -12438.15460715459
Iteration 13200: Loss = -12438.154568523929
Iteration 13300: Loss = -12438.158775775943
1
Iteration 13400: Loss = -12438.15456837722
Iteration 13500: Loss = -12438.154562680746
Iteration 13600: Loss = -12438.230215484416
1
Iteration 13700: Loss = -12438.154556872063
Iteration 13800: Loss = -12438.154536783974
Iteration 13900: Loss = -12438.154545016288
Iteration 14000: Loss = -12438.165904817743
1
Iteration 14100: Loss = -12438.154667552752
2
Iteration 14200: Loss = -12438.154541621156
Iteration 14300: Loss = -12438.273353500941
1
Iteration 14400: Loss = -12438.15449842205
Iteration 14500: Loss = -12438.15479315852
1
Iteration 14600: Loss = -12438.154630239074
2
Iteration 14700: Loss = -12438.154566629348
Iteration 14800: Loss = -12438.195270089787
1
Iteration 14900: Loss = -12438.154577810788
Iteration 15000: Loss = -12438.154587978008
Iteration 15100: Loss = -12438.156677926963
1
Iteration 15200: Loss = -12438.154541683178
Iteration 15300: Loss = -12438.154533624705
Iteration 15400: Loss = -12438.465560558572
1
Iteration 15500: Loss = -12438.154536498503
Iteration 15600: Loss = -12438.154529305877
Iteration 15700: Loss = -12438.154504501152
Iteration 15800: Loss = -12438.169162495189
1
Iteration 15900: Loss = -12438.15447141574
Iteration 16000: Loss = -12438.154463086988
Iteration 16100: Loss = -12438.154465324911
Iteration 16200: Loss = -12438.155613551955
1
Iteration 16300: Loss = -12438.154444176915
Iteration 16400: Loss = -12438.154451635226
Iteration 16500: Loss = -12438.261967593868
1
Iteration 16600: Loss = -12438.154477707045
Iteration 16700: Loss = -12438.154488987842
Iteration 16800: Loss = -12438.15448854875
Iteration 16900: Loss = -12438.15458213455
Iteration 17000: Loss = -12438.154473787754
Iteration 17100: Loss = -12438.15447747902
Iteration 17200: Loss = -12438.267776800243
1
Iteration 17300: Loss = -12438.154520634389
Iteration 17400: Loss = -12438.154494359653
Iteration 17500: Loss = -12438.154498060965
Iteration 17600: Loss = -12438.208538688794
1
Iteration 17700: Loss = -12438.154488954955
Iteration 17800: Loss = -12438.15447292243
Iteration 17900: Loss = -12438.161183028995
1
Iteration 18000: Loss = -12438.154468100864
Iteration 18100: Loss = -12438.154467605644
Iteration 18200: Loss = -12438.154475106525
Iteration 18300: Loss = -12438.154754986053
1
Iteration 18400: Loss = -12438.154471776279
Iteration 18500: Loss = -12438.156802940961
1
Iteration 18600: Loss = -12438.15446601883
Iteration 18700: Loss = -12438.180300504344
1
Iteration 18800: Loss = -12438.154498819891
Iteration 18900: Loss = -12438.154480594134
Iteration 19000: Loss = -12438.178566730527
1
Iteration 19100: Loss = -12438.15449610452
Iteration 19200: Loss = -12438.154485624025
Iteration 19300: Loss = -12438.189941773862
1
Iteration 19400: Loss = -12438.154531103515
Iteration 19500: Loss = -12438.154478436225
Iteration 19600: Loss = -12438.154485106104
Iteration 19700: Loss = -12438.154542521614
Iteration 19800: Loss = -12438.154481294778
Iteration 19900: Loss = -12438.154459861496
pi: tensor([[1.0000e+00, 6.9239e-09],
        [6.4274e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9821, 0.0179], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.2571],
         [0.5282, 0.7648]],

        [[0.7091, 0.1881],
         [0.6315, 0.6105]],

        [[0.7016, 0.2197],
         [0.6789, 0.5366]],

        [[0.5763, 0.3030],
         [0.6588, 0.6421]],

        [[0.7078, 0.2084],
         [0.5254, 0.6350]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
Global Adjusted Rand Index: -0.00027388137537946147
Average Adjusted Rand Index: 0.0009323724959880217
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24648.125176147547
Iteration 100: Loss = -12440.603660844265
Iteration 200: Loss = -12440.28140859414
Iteration 300: Loss = -12440.216981266929
Iteration 400: Loss = -12440.172243051478
Iteration 500: Loss = -12440.12424439947
Iteration 600: Loss = -12440.070900081726
Iteration 700: Loss = -12440.041910179889
Iteration 800: Loss = -12440.024003972354
Iteration 900: Loss = -12440.006774202688
Iteration 1000: Loss = -12439.986610968861
Iteration 1100: Loss = -12439.96093827749
Iteration 1200: Loss = -12439.926920766151
Iteration 1300: Loss = -12439.87980788958
Iteration 1400: Loss = -12439.813222220484
Iteration 1500: Loss = -12439.725038489056
Iteration 1600: Loss = -12439.639341225251
Iteration 1700: Loss = -12439.58754837008
Iteration 1800: Loss = -12439.56039148562
Iteration 1900: Loss = -12439.54409694998
Iteration 2000: Loss = -12439.532211466194
Iteration 2100: Loss = -12439.521114515968
Iteration 2200: Loss = -12439.50850205788
Iteration 2300: Loss = -12439.493769041479
Iteration 2400: Loss = -12439.482253857585
Iteration 2500: Loss = -12439.4749130114
Iteration 2600: Loss = -12439.467504127946
Iteration 2700: Loss = -12439.459517898289
Iteration 2800: Loss = -12439.452021116465
Iteration 2900: Loss = -12439.446025360212
Iteration 3000: Loss = -12439.44163102037
Iteration 3100: Loss = -12439.437756173282
Iteration 3200: Loss = -12439.43141629011
Iteration 3300: Loss = -12439.416938040436
Iteration 3400: Loss = -12439.410398726985
Iteration 3500: Loss = -12439.405044358062
Iteration 3600: Loss = -12439.392763024087
Iteration 3700: Loss = -12439.389521779956
Iteration 3800: Loss = -12439.3871500471
Iteration 3900: Loss = -12439.385669937365
Iteration 4000: Loss = -12439.3836701949
Iteration 4100: Loss = -12439.382491974793
Iteration 4200: Loss = -12439.380771328257
Iteration 4300: Loss = -12439.379234735597
Iteration 4400: Loss = -12439.377400272555
Iteration 4500: Loss = -12439.375315784111
Iteration 4600: Loss = -12439.37266120776
Iteration 4700: Loss = -12439.367717867108
Iteration 4800: Loss = -12439.35602644179
Iteration 4900: Loss = -12439.310102120848
Iteration 5000: Loss = -12438.98658410265
Iteration 5100: Loss = -12438.855779376912
Iteration 5200: Loss = -12438.838289406993
Iteration 5300: Loss = -12438.824061281375
Iteration 5400: Loss = -12438.806000396624
Iteration 5500: Loss = -12438.76508037449
Iteration 5600: Loss = -12438.517604457094
Iteration 5700: Loss = -12438.298196958127
Iteration 5800: Loss = -12438.216459680323
Iteration 5900: Loss = -12438.183581021554
Iteration 6000: Loss = -12438.16928002419
Iteration 6100: Loss = -12438.162898557091
Iteration 6200: Loss = -12438.15994521454
Iteration 6300: Loss = -12438.158481661374
Iteration 6400: Loss = -12438.157652728933
Iteration 6500: Loss = -12438.157092179346
Iteration 6600: Loss = -12438.156721898951
Iteration 6700: Loss = -12438.156395262695
Iteration 6800: Loss = -12438.156171716222
Iteration 6900: Loss = -12438.155977107097
Iteration 7000: Loss = -12438.155787115546
Iteration 7100: Loss = -12438.15564094272
Iteration 7200: Loss = -12438.15554766857
Iteration 7300: Loss = -12438.155403601073
Iteration 7400: Loss = -12438.15528529449
Iteration 7500: Loss = -12438.155165958042
Iteration 7600: Loss = -12438.155122716402
Iteration 7700: Loss = -12438.155018060761
Iteration 7800: Loss = -12438.15496178303
Iteration 7900: Loss = -12438.210381061435
1
Iteration 8000: Loss = -12438.158707858001
2
Iteration 8100: Loss = -12438.155766947428
3
Iteration 8200: Loss = -12438.188209236681
4
Iteration 8300: Loss = -12438.154725561844
Iteration 8400: Loss = -12438.155005643988
1
Iteration 8500: Loss = -12438.154658178779
Iteration 8600: Loss = -12438.154757328428
Iteration 8700: Loss = -12438.154593571197
Iteration 8800: Loss = -12438.156052111875
1
Iteration 8900: Loss = -12438.154510225275
Iteration 9000: Loss = -12438.167000325306
1
Iteration 9100: Loss = -12438.15448221394
Iteration 9200: Loss = -12438.154458069219
Iteration 9300: Loss = -12438.366012385572
1
Iteration 9400: Loss = -12438.154415452025
Iteration 9500: Loss = -12438.154371175004
Iteration 9600: Loss = -12438.155517903344
1
Iteration 9700: Loss = -12438.154358863021
Iteration 9800: Loss = -12438.154296958033
Iteration 9900: Loss = -12438.154328556957
Iteration 10000: Loss = -12438.162888769863
1
Iteration 10100: Loss = -12438.154318506506
Iteration 10200: Loss = -12438.154297932258
Iteration 10300: Loss = -12438.154295963319
Iteration 10400: Loss = -12438.157137096052
1
Iteration 10500: Loss = -12438.15426581576
Iteration 10600: Loss = -12438.154260850199
Iteration 10700: Loss = -12438.154239238325
Iteration 10800: Loss = -12438.155859258237
1
Iteration 10900: Loss = -12438.154226665129
Iteration 11000: Loss = -12438.154279258848
Iteration 11100: Loss = -12438.154300834032
Iteration 11200: Loss = -12438.154254570714
Iteration 11300: Loss = -12438.15426401472
Iteration 11400: Loss = -12438.154261161157
Iteration 11500: Loss = -12438.183083643265
1
Iteration 11600: Loss = -12438.154209319804
Iteration 11700: Loss = -12438.154234057185
Iteration 11800: Loss = -12438.154217835017
Iteration 11900: Loss = -12438.15435536054
1
Iteration 12000: Loss = -12438.154193969734
Iteration 12100: Loss = -12438.154202378331
Iteration 12200: Loss = -12438.155349789049
1
Iteration 12300: Loss = -12438.154209793121
Iteration 12400: Loss = -12438.154183844106
Iteration 12500: Loss = -12438.190594510656
1
Iteration 12600: Loss = -12438.154164381604
Iteration 12700: Loss = -12438.154143951022
Iteration 12800: Loss = -12438.154194774741
Iteration 12900: Loss = -12438.154371662624
1
Iteration 13000: Loss = -12438.154168015652
Iteration 13100: Loss = -12438.154190889878
Iteration 13200: Loss = -12438.157831989502
1
Iteration 13300: Loss = -12438.154199079649
Iteration 13400: Loss = -12438.154199928582
Iteration 13500: Loss = -12438.154199876093
Iteration 13600: Loss = -12438.230634410129
1
Iteration 13700: Loss = -12438.15421299333
Iteration 13800: Loss = -12438.154195577246
Iteration 13900: Loss = -12438.154188397733
Iteration 14000: Loss = -12438.158988172081
1
Iteration 14100: Loss = -12438.154196069321
Iteration 14200: Loss = -12438.154173994035
Iteration 14300: Loss = -12438.158133842922
1
Iteration 14400: Loss = -12438.154189176188
Iteration 14500: Loss = -12438.154165652231
Iteration 14600: Loss = -12438.163337340196
1
Iteration 14700: Loss = -12438.154158990083
Iteration 14800: Loss = -12438.154176107619
Iteration 14900: Loss = -12438.154370681741
1
Iteration 15000: Loss = -12438.453904765904
2
Iteration 15100: Loss = -12438.15415972497
Iteration 15200: Loss = -12438.378218690794
1
Iteration 15300: Loss = -12438.154154553189
Iteration 15400: Loss = -12438.1541699024
Iteration 15500: Loss = -12438.15447509646
1
Iteration 15600: Loss = -12438.154192374135
Iteration 15700: Loss = -12438.154990543813
1
Iteration 15800: Loss = -12438.154186368814
Iteration 15900: Loss = -12438.15414578368
Iteration 16000: Loss = -12438.158340057442
1
Iteration 16100: Loss = -12438.154112973853
Iteration 16200: Loss = -12438.154108916699
Iteration 16300: Loss = -12438.154109053285
Iteration 16400: Loss = -12438.154190834906
Iteration 16500: Loss = -12438.154102151835
Iteration 16600: Loss = -12438.15409859181
Iteration 16700: Loss = -12438.154911676329
1
Iteration 16800: Loss = -12438.154124988816
Iteration 16900: Loss = -12438.154108568062
Iteration 17000: Loss = -12438.155116401891
1
Iteration 17100: Loss = -12438.154131951778
Iteration 17200: Loss = -12438.154122022775
Iteration 17300: Loss = -12438.185620179243
1
Iteration 17400: Loss = -12438.154151124136
Iteration 17500: Loss = -12438.154120227426
Iteration 17600: Loss = -12438.15834716575
1
Iteration 17700: Loss = -12438.15415993187
Iteration 17800: Loss = -12438.154122559652
Iteration 17900: Loss = -12438.154134801582
Iteration 18000: Loss = -12438.154219438758
Iteration 18100: Loss = -12438.154128016511
Iteration 18200: Loss = -12438.154105780695
Iteration 18300: Loss = -12438.155751022367
1
Iteration 18400: Loss = -12438.154130329736
Iteration 18500: Loss = -12438.155458582889
1
Iteration 18600: Loss = -12438.154130818975
Iteration 18700: Loss = -12438.154193128377
Iteration 18800: Loss = -12438.154137510952
Iteration 18900: Loss = -12438.158141902853
1
Iteration 19000: Loss = -12438.15414064405
Iteration 19100: Loss = -12438.29305025655
1
Iteration 19200: Loss = -12438.154121364016
Iteration 19300: Loss = -12438.154097184566
Iteration 19400: Loss = -12438.170924998054
1
Iteration 19500: Loss = -12438.154112372602
Iteration 19600: Loss = -12438.154122961
Iteration 19700: Loss = -12438.23137891947
1
Iteration 19800: Loss = -12438.154147988233
Iteration 19900: Loss = -12438.154124875091
pi: tensor([[1.0000e+00, 1.6263e-07],
        [1.8708e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0182, 0.9818], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.7650, 0.2550],
         [0.6364, 0.1993]],

        [[0.5062, 0.1859],
         [0.7172, 0.6990]],

        [[0.6855, 0.2171],
         [0.5862, 0.7310]],

        [[0.6189, 0.3015],
         [0.6534, 0.6393]],

        [[0.5370, 0.2060],
         [0.7254, 0.5751]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
Global Adjusted Rand Index: -0.00027388137537946147
Average Adjusted Rand Index: 0.0009323724959880217
11871.965429111695
[-0.00027388137537946147, -0.00027388137537946147] [0.0009323724959880217, 0.0009323724959880217] [12438.154561810223, 12438.516092313701]
-------------------------------------
This iteration is 33
True Objective function: Loss = -12019.236934010527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21494.541117650788
Iteration 100: Loss = -12576.977100353357
Iteration 200: Loss = -12576.503107853483
Iteration 300: Loss = -12576.322314372888
Iteration 400: Loss = -12576.09751483665
Iteration 500: Loss = -12575.8601477587
Iteration 600: Loss = -12575.660315951522
Iteration 700: Loss = -12575.381807870592
Iteration 800: Loss = -12575.079002486229
Iteration 900: Loss = -12574.929774689725
Iteration 1000: Loss = -12574.855436784035
Iteration 1100: Loss = -12574.810182622474
Iteration 1200: Loss = -12574.778429644135
Iteration 1300: Loss = -12574.754358578766
Iteration 1400: Loss = -12574.735404684468
Iteration 1500: Loss = -12574.720093869186
Iteration 1600: Loss = -12574.70749317503
Iteration 1700: Loss = -12574.696986548204
Iteration 1800: Loss = -12574.688095249949
Iteration 1900: Loss = -12574.68053949268
Iteration 2000: Loss = -12574.67402679192
Iteration 2100: Loss = -12574.668381541147
Iteration 2200: Loss = -12574.663479181685
Iteration 2300: Loss = -12574.659140825997
Iteration 2400: Loss = -12574.65527910804
Iteration 2500: Loss = -12574.65180465317
Iteration 2600: Loss = -12574.64885641801
Iteration 2700: Loss = -12574.64606473177
Iteration 2800: Loss = -12574.643679380833
Iteration 2900: Loss = -12574.641416165548
Iteration 3000: Loss = -12574.639456388733
Iteration 3100: Loss = -12574.637652853964
Iteration 3200: Loss = -12574.636047374841
Iteration 3300: Loss = -12574.634516815388
Iteration 3400: Loss = -12574.63318375345
Iteration 3500: Loss = -12574.631893335523
Iteration 3600: Loss = -12574.630811323519
Iteration 3700: Loss = -12574.629749103633
Iteration 3800: Loss = -12574.628849906974
Iteration 3900: Loss = -12574.627912554368
Iteration 4000: Loss = -12574.62712449738
Iteration 4100: Loss = -12574.626411774601
Iteration 4200: Loss = -12574.625709934238
Iteration 4300: Loss = -12574.625068637464
Iteration 4400: Loss = -12574.624496140954
Iteration 4500: Loss = -12574.623955017283
Iteration 4600: Loss = -12574.623450074114
Iteration 4700: Loss = -12574.623007289534
Iteration 4800: Loss = -12574.622539729236
Iteration 4900: Loss = -12574.622160848678
Iteration 5000: Loss = -12574.621760513395
Iteration 5100: Loss = -12574.621758775877
Iteration 5200: Loss = -12574.621050984058
Iteration 5300: Loss = -12574.620803474318
Iteration 5400: Loss = -12574.620499359846
Iteration 5500: Loss = -12574.620215778365
Iteration 5600: Loss = -12574.619971508322
Iteration 5700: Loss = -12574.619824371377
Iteration 5800: Loss = -12574.61948396239
Iteration 5900: Loss = -12574.622274803292
1
Iteration 6000: Loss = -12574.619081432053
Iteration 6100: Loss = -12574.618919475683
Iteration 6200: Loss = -12574.618788666905
Iteration 6300: Loss = -12574.618550971123
Iteration 6400: Loss = -12574.619391718057
1
Iteration 6500: Loss = -12574.618286792249
Iteration 6600: Loss = -12574.619340679288
1
Iteration 6700: Loss = -12574.617970205109
Iteration 6800: Loss = -12574.617857077583
Iteration 6900: Loss = -12574.617811326483
Iteration 7000: Loss = -12574.617641458804
Iteration 7100: Loss = -12574.617671730884
Iteration 7200: Loss = -12574.63351379397
1
Iteration 7300: Loss = -12574.629888093785
2
Iteration 7400: Loss = -12574.617274969964
Iteration 7500: Loss = -12574.621357022295
1
Iteration 7600: Loss = -12574.617043990087
Iteration 7700: Loss = -12574.88462627221
1
Iteration 7800: Loss = -12574.616911886453
Iteration 7900: Loss = -12574.61682261941
Iteration 8000: Loss = -12574.617035083376
1
Iteration 8100: Loss = -12574.616775805946
Iteration 8200: Loss = -12574.616682072781
Iteration 8300: Loss = -12574.650245960976
1
Iteration 8400: Loss = -12574.616572416491
Iteration 8500: Loss = -12574.616473662129
Iteration 8600: Loss = -12574.616460352949
Iteration 8700: Loss = -12574.617184523611
1
Iteration 8800: Loss = -12574.616380035324
Iteration 8900: Loss = -12574.616333081873
Iteration 9000: Loss = -12574.617514327003
1
Iteration 9100: Loss = -12574.61623197492
Iteration 9200: Loss = -12574.616203011834
Iteration 9300: Loss = -12574.641198854531
1
Iteration 9400: Loss = -12574.616133429525
Iteration 9500: Loss = -12574.616126462799
Iteration 9600: Loss = -12574.616460597545
1
Iteration 9700: Loss = -12574.616045183133
Iteration 9800: Loss = -12574.616051900524
Iteration 9900: Loss = -12574.627529620724
1
Iteration 10000: Loss = -12574.615991761959
Iteration 10100: Loss = -12574.615936354508
Iteration 10200: Loss = -12574.616001737171
Iteration 10300: Loss = -12574.615907564068
Iteration 10400: Loss = -12574.615884285655
Iteration 10500: Loss = -12574.615903623628
Iteration 10600: Loss = -12574.62568780273
1
Iteration 10700: Loss = -12574.61585500299
Iteration 10800: Loss = -12574.615796391416
Iteration 10900: Loss = -12574.62172784937
1
Iteration 11000: Loss = -12574.615858709563
Iteration 11100: Loss = -12574.61581068799
Iteration 11200: Loss = -12574.615808851668
Iteration 11300: Loss = -12574.616471575337
1
Iteration 11400: Loss = -12574.6157744379
Iteration 11500: Loss = -12574.615912043268
1
Iteration 11600: Loss = -12574.616121633011
2
Iteration 11700: Loss = -12574.615731731506
Iteration 11800: Loss = -12574.616054330192
1
Iteration 11900: Loss = -12574.616457675556
2
Iteration 12000: Loss = -12574.738075683621
3
Iteration 12100: Loss = -12574.617345492741
4
Iteration 12200: Loss = -12574.622205730351
5
Iteration 12300: Loss = -12574.615735650345
Iteration 12400: Loss = -12574.616370505979
1
Iteration 12500: Loss = -12574.615848099202
2
Iteration 12600: Loss = -12574.639024858121
3
Iteration 12700: Loss = -12574.615706947565
Iteration 12800: Loss = -12574.61568035231
Iteration 12900: Loss = -12574.620598915253
1
Iteration 13000: Loss = -12574.622100598323
2
Iteration 13100: Loss = -12574.625569378546
3
Iteration 13200: Loss = -12574.615686139288
Iteration 13300: Loss = -12574.616305005655
1
Iteration 13400: Loss = -12574.615593078508
Iteration 13500: Loss = -12574.617614803487
1
Iteration 13600: Loss = -12574.615634317504
Iteration 13700: Loss = -12574.616004203941
1
Iteration 13800: Loss = -12574.615690977229
Iteration 13900: Loss = -12574.61579235552
1
Iteration 14000: Loss = -12574.615746221627
Iteration 14100: Loss = -12574.616378053397
1
Iteration 14200: Loss = -12574.615600268891
Iteration 14300: Loss = -12574.933869400675
1
Iteration 14400: Loss = -12574.615573557881
Iteration 14500: Loss = -12574.666361880605
1
Iteration 14600: Loss = -12574.615605722092
Iteration 14700: Loss = -12574.622474135993
1
Iteration 14800: Loss = -12574.615616439414
Iteration 14900: Loss = -12574.617022680819
1
Iteration 15000: Loss = -12574.61556265397
Iteration 15100: Loss = -12574.616678907572
1
Iteration 15200: Loss = -12574.6155967794
Iteration 15300: Loss = -12574.615884924599
1
Iteration 15400: Loss = -12574.61568622541
Iteration 15500: Loss = -12574.622728062026
1
Iteration 15600: Loss = -12574.616503394096
2
Iteration 15700: Loss = -12574.619861593146
3
Iteration 15800: Loss = -12574.615610392444
Iteration 15900: Loss = -12575.00832454951
1
Iteration 16000: Loss = -12574.615821609033
2
Iteration 16100: Loss = -12574.618854847824
3
Iteration 16200: Loss = -12574.616028374983
4
Iteration 16300: Loss = -12574.615792407587
5
Iteration 16400: Loss = -12574.61607071269
6
Iteration 16500: Loss = -12574.69471023036
7
Iteration 16600: Loss = -12574.615687880387
Iteration 16700: Loss = -12574.615945083679
1
Iteration 16800: Loss = -12574.70426606596
2
Iteration 16900: Loss = -12574.616048859914
3
Iteration 17000: Loss = -12574.648018079626
4
Iteration 17100: Loss = -12574.615542501473
Iteration 17200: Loss = -12574.636091958595
1
Iteration 17300: Loss = -12574.615587952421
Iteration 17400: Loss = -12574.616020850726
1
Iteration 17500: Loss = -12574.621431552465
2
Iteration 17600: Loss = -12574.615567096236
Iteration 17700: Loss = -12574.621221368367
1
Iteration 17800: Loss = -12574.615552128871
Iteration 17900: Loss = -12574.63108245658
1
Iteration 18000: Loss = -12574.615596470923
Iteration 18100: Loss = -12574.615921848039
1
Iteration 18200: Loss = -12574.934409961415
2
Iteration 18300: Loss = -12574.615564616757
Iteration 18400: Loss = -12574.884594148101
1
Iteration 18500: Loss = -12574.616758365637
2
Iteration 18600: Loss = -12574.624688004204
3
Iteration 18700: Loss = -12574.615708971363
4
Iteration 18800: Loss = -12574.624748421678
5
Iteration 18900: Loss = -12574.615821559799
6
Iteration 19000: Loss = -12574.615782238756
7
Iteration 19100: Loss = -12574.615540092587
Iteration 19200: Loss = -12574.616147610515
1
Iteration 19300: Loss = -12574.615499122328
Iteration 19400: Loss = -12574.616051632102
1
Iteration 19500: Loss = -12574.615578224213
Iteration 19600: Loss = -12574.615667278104
Iteration 19700: Loss = -12574.615783341727
1
Iteration 19800: Loss = -12574.615899262168
2
Iteration 19900: Loss = -12574.61555327323
pi: tensor([[9.5921e-01, 4.0789e-02],
        [1.0000e+00, 9.0541e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9949, 0.0051], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.1724],
         [0.6315, 0.2883]],

        [[0.6880, 0.2522],
         [0.5252, 0.6063]],

        [[0.5616, 0.2916],
         [0.5206, 0.6044]],

        [[0.5082, 0.1974],
         [0.5784, 0.7182]],

        [[0.6329, 0.2681],
         [0.5341, 0.5207]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -6.397451132144595e-06
Average Adjusted Rand Index: -0.0005235442726599145
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19560.493841842133
Iteration 100: Loss = -12576.907363678509
Iteration 200: Loss = -12576.393294289443
Iteration 300: Loss = -12576.15400419503
Iteration 400: Loss = -12575.745427773662
Iteration 500: Loss = -12575.243854934226
Iteration 600: Loss = -12574.959457189721
Iteration 700: Loss = -12574.857304728983
Iteration 800: Loss = -12574.8084398249
Iteration 900: Loss = -12574.779619612238
Iteration 1000: Loss = -12574.7592532133
Iteration 1100: Loss = -12574.743158691806
Iteration 1200: Loss = -12574.729618276859
Iteration 1300: Loss = -12574.717896571725
Iteration 1400: Loss = -12574.7075664715
Iteration 1500: Loss = -12574.698489330667
Iteration 1600: Loss = -12574.69045387771
Iteration 1700: Loss = -12574.683298066513
Iteration 1800: Loss = -12574.676938909575
Iteration 1900: Loss = -12574.671267921847
Iteration 2000: Loss = -12574.666235269322
Iteration 2100: Loss = -12574.661743981736
Iteration 2200: Loss = -12574.657757022444
Iteration 2300: Loss = -12574.65412636695
Iteration 2400: Loss = -12574.65095024685
Iteration 2500: Loss = -12574.648039087653
Iteration 2600: Loss = -12574.645388866847
Iteration 2700: Loss = -12574.643066321905
Iteration 2800: Loss = -12574.640972519037
Iteration 2900: Loss = -12574.639020256991
Iteration 3000: Loss = -12574.637273827086
Iteration 3100: Loss = -12574.635699079166
Iteration 3200: Loss = -12574.634203766185
Iteration 3300: Loss = -12574.632926414979
Iteration 3400: Loss = -12574.631699162324
Iteration 3500: Loss = -12574.63064576949
Iteration 3600: Loss = -12574.62959604466
Iteration 3700: Loss = -12574.628702207405
Iteration 3800: Loss = -12574.62780003257
Iteration 3900: Loss = -12574.627016607006
Iteration 4000: Loss = -12574.62630432487
Iteration 4100: Loss = -12574.625664525052
Iteration 4200: Loss = -12574.624999661059
Iteration 4300: Loss = -12574.624451247033
Iteration 4400: Loss = -12574.623884155493
Iteration 4500: Loss = -12574.623397378475
Iteration 4600: Loss = -12574.622932896144
Iteration 4700: Loss = -12574.62250126603
Iteration 4800: Loss = -12574.62213711723
Iteration 4900: Loss = -12574.621744228514
Iteration 5000: Loss = -12574.621376377936
Iteration 5100: Loss = -12574.621075586427
Iteration 5200: Loss = -12574.620790074314
Iteration 5300: Loss = -12574.620494434439
Iteration 5400: Loss = -12574.62020520349
Iteration 5500: Loss = -12574.6211124539
1
Iteration 5600: Loss = -12574.61975183267
Iteration 5700: Loss = -12574.619503002321
Iteration 5800: Loss = -12574.619310044045
Iteration 5900: Loss = -12574.61907596117
Iteration 6000: Loss = -12574.618927690582
Iteration 6100: Loss = -12574.618721767882
Iteration 6200: Loss = -12574.618642672927
Iteration 6300: Loss = -12574.618395392024
Iteration 6400: Loss = -12574.618305836026
Iteration 6500: Loss = -12574.63639789121
1
Iteration 6600: Loss = -12574.626685206573
2
Iteration 6700: Loss = -12574.69875086222
3
Iteration 6800: Loss = -12574.617766756077
Iteration 6900: Loss = -12574.617645438906
Iteration 7000: Loss = -12574.735258374685
1
Iteration 7100: Loss = -12574.617414364253
Iteration 7200: Loss = -12574.69775097615
1
Iteration 7300: Loss = -12574.61722208202
Iteration 7400: Loss = -12574.617789617252
1
Iteration 7500: Loss = -12574.617077147976
Iteration 7600: Loss = -12574.617022655944
Iteration 7700: Loss = -12574.622706772152
1
Iteration 7800: Loss = -12574.616854999244
Iteration 7900: Loss = -12574.616745701942
Iteration 8000: Loss = -12574.62432548029
1
Iteration 8100: Loss = -12574.616673996923
Iteration 8200: Loss = -12574.616604780445
Iteration 8300: Loss = -12574.725702067395
1
Iteration 8400: Loss = -12574.616492110881
Iteration 8500: Loss = -12574.616454853716
Iteration 8600: Loss = -12574.616411225565
Iteration 8700: Loss = -12574.616377249255
Iteration 8800: Loss = -12574.616332192269
Iteration 8900: Loss = -12574.616276330727
Iteration 9000: Loss = -12574.955254206476
1
Iteration 9100: Loss = -12574.616229564223
Iteration 9200: Loss = -12574.616165089155
Iteration 9300: Loss = -12574.680456389902
1
Iteration 9400: Loss = -12574.616135908453
Iteration 9500: Loss = -12574.616082581639
Iteration 9600: Loss = -12574.631053365149
1
Iteration 9700: Loss = -12574.61606889217
Iteration 9800: Loss = -12574.61602220284
Iteration 9900: Loss = -12574.619369885977
1
Iteration 10000: Loss = -12574.615954805728
Iteration 10100: Loss = -12574.615935647273
Iteration 10200: Loss = -12575.017711517934
1
Iteration 10300: Loss = -12574.61591262941
Iteration 10400: Loss = -12574.615879127512
Iteration 10500: Loss = -12574.901589075947
1
Iteration 10600: Loss = -12574.615867020573
Iteration 10700: Loss = -12574.615855221464
Iteration 10800: Loss = -12574.668088385768
1
Iteration 10900: Loss = -12574.615840041413
Iteration 11000: Loss = -12574.615776720206
Iteration 11100: Loss = -12574.615843091226
Iteration 11200: Loss = -12574.615789377887
Iteration 11300: Loss = -12574.615807697457
Iteration 11400: Loss = -12574.615947506862
1
Iteration 11500: Loss = -12574.616113266156
2
Iteration 11600: Loss = -12574.615727549906
Iteration 11700: Loss = -12574.618975291258
1
Iteration 11800: Loss = -12574.617523944866
2
Iteration 11900: Loss = -12574.65385818185
3
Iteration 12000: Loss = -12574.616246086955
4
Iteration 12100: Loss = -12574.61648488332
5
Iteration 12200: Loss = -12574.62553361486
6
Iteration 12300: Loss = -12574.615974756678
7
Iteration 12400: Loss = -12574.619350556462
8
Iteration 12500: Loss = -12574.615732627544
Iteration 12600: Loss = -12574.704835997849
1
Iteration 12700: Loss = -12574.615641831468
Iteration 12800: Loss = -12574.615910885423
1
Iteration 12900: Loss = -12574.658287566077
2
Iteration 13000: Loss = -12574.61561945296
Iteration 13100: Loss = -12574.625151292683
1
Iteration 13200: Loss = -12574.615619800736
Iteration 13300: Loss = -12574.658890619017
1
Iteration 13400: Loss = -12574.615704172738
Iteration 13500: Loss = -12574.6279810175
1
Iteration 13600: Loss = -12574.616639355538
2
Iteration 13700: Loss = -12574.61775580377
3
Iteration 13800: Loss = -12574.615597903758
Iteration 13900: Loss = -12574.617104645304
1
Iteration 14000: Loss = -12574.615608073018
Iteration 14100: Loss = -12574.616437641032
1
Iteration 14200: Loss = -12574.615596792637
Iteration 14300: Loss = -12574.617047014593
1
Iteration 14400: Loss = -12574.615624913637
Iteration 14500: Loss = -12574.615651745107
Iteration 14600: Loss = -12574.615780887492
1
Iteration 14700: Loss = -12574.615636319284
Iteration 14800: Loss = -12574.61682927261
1
Iteration 14900: Loss = -12574.617009186037
2
Iteration 15000: Loss = -12574.615600453524
Iteration 15100: Loss = -12574.77060001201
1
Iteration 15200: Loss = -12574.615653910772
Iteration 15300: Loss = -12574.645238603389
1
Iteration 15400: Loss = -12574.61564726835
Iteration 15500: Loss = -12574.624453694993
1
Iteration 15600: Loss = -12574.61690124701
2
Iteration 15700: Loss = -12574.62245140092
3
Iteration 15800: Loss = -12574.615557948931
Iteration 15900: Loss = -12574.618605527467
1
Iteration 16000: Loss = -12574.615555115832
Iteration 16100: Loss = -12574.63641159673
1
Iteration 16200: Loss = -12574.615554607653
Iteration 16300: Loss = -12574.620084876058
1
Iteration 16400: Loss = -12574.615548819977
Iteration 16500: Loss = -12574.61614780585
1
Iteration 16600: Loss = -12574.61609637732
2
Iteration 16700: Loss = -12574.842213357992
3
Iteration 16800: Loss = -12574.615551205407
Iteration 16900: Loss = -12574.616933148132
1
Iteration 17000: Loss = -12574.615516847416
Iteration 17100: Loss = -12574.615799839958
1
Iteration 17200: Loss = -12574.6242633232
2
Iteration 17300: Loss = -12574.615576786142
Iteration 17400: Loss = -12574.616874750456
1
Iteration 17500: Loss = -12574.615620370163
Iteration 17600: Loss = -12574.615563786694
Iteration 17700: Loss = -12574.615879093026
1
Iteration 17800: Loss = -12574.615504832813
Iteration 17900: Loss = -12574.694443929751
1
Iteration 18000: Loss = -12574.615568671603
Iteration 18100: Loss = -12574.65855293387
1
Iteration 18200: Loss = -12574.61605771918
2
Iteration 18300: Loss = -12574.625976908628
3
Iteration 18400: Loss = -12574.615796586377
4
Iteration 18500: Loss = -12574.61560088893
Iteration 18600: Loss = -12574.61867398654
1
Iteration 18700: Loss = -12574.615587312815
Iteration 18800: Loss = -12574.621624953315
1
Iteration 18900: Loss = -12574.63006997089
2
Iteration 19000: Loss = -12574.67279338539
3
Iteration 19100: Loss = -12574.615581606826
Iteration 19200: Loss = -12574.619111273054
1
Iteration 19300: Loss = -12574.615507740666
Iteration 19400: Loss = -12574.616024415258
1
Iteration 19500: Loss = -12574.615546256255
Iteration 19600: Loss = -12574.62111802615
1
Iteration 19700: Loss = -12574.615585810367
Iteration 19800: Loss = -12574.61900920966
1
Iteration 19900: Loss = -12574.615970507939
2
pi: tensor([[8.6826e-07, 1.0000e+00],
        [4.0305e-02, 9.5969e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0051, 0.9949], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2881, 0.1719],
         [0.6381, 0.2027]],

        [[0.6595, 0.2522],
         [0.6498, 0.5967]],

        [[0.6978, 0.2924],
         [0.5590, 0.7002]],

        [[0.5764, 0.1969],
         [0.5894, 0.5631]],

        [[0.6723, 0.2702],
         [0.5126, 0.6215]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -6.397451132144595e-06
Average Adjusted Rand Index: -0.0005235442726599145
12019.236934010527
[-6.397451132144595e-06, -6.397451132144595e-06] [-0.0005235442726599145, -0.0005235442726599145] [12574.625465145531, 12574.620984236215]
-------------------------------------
This iteration is 34
True Objective function: Loss = -11855.192237897678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22303.664361257284
Iteration 100: Loss = -12444.804560905177
Iteration 200: Loss = -12444.237240579192
Iteration 300: Loss = -12443.967508047848
Iteration 400: Loss = -12442.993281549323
Iteration 500: Loss = -12441.583308397408
Iteration 600: Loss = -12440.921451484992
Iteration 700: Loss = -12440.585851168387
Iteration 800: Loss = -12440.402096997204
Iteration 900: Loss = -12440.280980197449
Iteration 1000: Loss = -12440.187589587735
Iteration 1100: Loss = -12440.108704673012
Iteration 1200: Loss = -12440.040236426248
Iteration 1300: Loss = -12439.981911771663
Iteration 1400: Loss = -12439.934283272594
Iteration 1500: Loss = -12439.897102545405
Iteration 1600: Loss = -12439.869028917174
Iteration 1700: Loss = -12439.848315801735
Iteration 1800: Loss = -12439.832976923435
Iteration 1900: Loss = -12439.8215555205
Iteration 2000: Loss = -12439.812863668474
Iteration 2100: Loss = -12439.806062245898
Iteration 2200: Loss = -12439.80059115422
Iteration 2300: Loss = -12439.79618258003
Iteration 2400: Loss = -12439.792388360587
Iteration 2500: Loss = -12439.789230120134
Iteration 2600: Loss = -12439.786487328482
Iteration 2700: Loss = -12439.78406996884
Iteration 2800: Loss = -12439.782001801497
Iteration 2900: Loss = -12439.78016815585
Iteration 3000: Loss = -12439.77851551451
Iteration 3100: Loss = -12439.777007211114
Iteration 3200: Loss = -12439.775696781207
Iteration 3300: Loss = -12439.774535159311
Iteration 3400: Loss = -12439.773432015176
Iteration 3500: Loss = -12439.772419875288
Iteration 3600: Loss = -12439.771503438978
Iteration 3700: Loss = -12439.770676181568
Iteration 3800: Loss = -12439.769910201901
Iteration 3900: Loss = -12439.76921091243
Iteration 4000: Loss = -12439.768542021995
Iteration 4100: Loss = -12439.767980475379
Iteration 4200: Loss = -12439.76746870001
Iteration 4300: Loss = -12439.766943094468
Iteration 4400: Loss = -12439.76648269766
Iteration 4500: Loss = -12439.766037449264
Iteration 4600: Loss = -12439.765615777014
Iteration 4700: Loss = -12439.765270531287
Iteration 4800: Loss = -12439.764930921681
Iteration 4900: Loss = -12439.764610388776
Iteration 5000: Loss = -12439.764261136985
Iteration 5100: Loss = -12439.764024948563
Iteration 5200: Loss = -12439.76374705179
Iteration 5300: Loss = -12439.763493068505
Iteration 5400: Loss = -12439.763241088038
Iteration 5500: Loss = -12439.763024589767
Iteration 5600: Loss = -12439.762819630541
Iteration 5700: Loss = -12439.762618585613
Iteration 5800: Loss = -12439.762439537473
Iteration 5900: Loss = -12439.762275881878
Iteration 6000: Loss = -12439.762124348097
Iteration 6100: Loss = -12439.76195290186
Iteration 6200: Loss = -12439.761777131662
Iteration 6300: Loss = -12439.761687138387
Iteration 6400: Loss = -12439.761584247874
Iteration 6500: Loss = -12439.76141776761
Iteration 6600: Loss = -12439.761284595805
Iteration 6700: Loss = -12439.761168528637
Iteration 6800: Loss = -12439.76111323527
Iteration 6900: Loss = -12439.760969403382
Iteration 7000: Loss = -12439.760892029619
Iteration 7100: Loss = -12439.760782261483
Iteration 7200: Loss = -12439.76179204284
1
Iteration 7300: Loss = -12439.760687331649
Iteration 7400: Loss = -12439.760701818625
Iteration 7500: Loss = -12439.760539992503
Iteration 7600: Loss = -12439.763409949182
1
Iteration 7700: Loss = -12439.760397648506
Iteration 7800: Loss = -12439.78896189176
1
Iteration 7900: Loss = -12439.760233474162
Iteration 8000: Loss = -12439.760176556258
Iteration 8100: Loss = -12439.760265650593
Iteration 8200: Loss = -12439.760097148534
Iteration 8300: Loss = -12439.764415815458
1
Iteration 8400: Loss = -12439.760067874902
Iteration 8500: Loss = -12439.759958254477
Iteration 8600: Loss = -12439.759911367113
Iteration 8700: Loss = -12439.760160484975
1
Iteration 8800: Loss = -12439.759819207893
Iteration 8900: Loss = -12439.759820050645
Iteration 9000: Loss = -12439.775278427498
1
Iteration 9100: Loss = -12439.75973475264
Iteration 9200: Loss = -12439.759704105363
Iteration 9300: Loss = -12439.759704671025
Iteration 9400: Loss = -12439.759941947805
1
Iteration 9500: Loss = -12439.759637656678
Iteration 9600: Loss = -12439.759642580759
Iteration 9700: Loss = -12439.759611293652
Iteration 9800: Loss = -12439.770347056377
1
Iteration 9900: Loss = -12439.759576024147
Iteration 10000: Loss = -12439.759514328633
Iteration 10100: Loss = -12439.760010026519
1
Iteration 10200: Loss = -12439.759552707623
Iteration 10300: Loss = -12439.759443865036
Iteration 10400: Loss = -12439.75942857325
Iteration 10500: Loss = -12439.759665644806
1
Iteration 10600: Loss = -12439.759388570814
Iteration 10700: Loss = -12439.75936346126
Iteration 10800: Loss = -12439.809574612718
1
Iteration 10900: Loss = -12439.759333887778
Iteration 11000: Loss = -12439.759349903834
Iteration 11100: Loss = -12439.759310539219
Iteration 11200: Loss = -12439.759372271858
Iteration 11300: Loss = -12439.759318809543
Iteration 11400: Loss = -12439.75931751997
Iteration 11500: Loss = -12439.759290764954
Iteration 11600: Loss = -12439.83950637799
1
Iteration 11700: Loss = -12439.75928523892
Iteration 11800: Loss = -12439.759260354456
Iteration 11900: Loss = -12439.759276981044
Iteration 12000: Loss = -12439.841469734458
1
Iteration 12100: Loss = -12439.759222976352
Iteration 12200: Loss = -12439.759210001124
Iteration 12300: Loss = -12439.759214489397
Iteration 12400: Loss = -12439.75921894654
Iteration 12500: Loss = -12439.759372272982
1
Iteration 12600: Loss = -12439.75916434626
Iteration 12700: Loss = -12439.759161623677
Iteration 12800: Loss = -12439.759974772609
1
Iteration 12900: Loss = -12439.759164572304
Iteration 13000: Loss = -12439.759162858161
Iteration 13100: Loss = -12439.842372752868
1
Iteration 13200: Loss = -12439.759151229682
Iteration 13300: Loss = -12439.759141108856
Iteration 13400: Loss = -12439.759141235347
Iteration 13500: Loss = -12439.792838823758
1
Iteration 13600: Loss = -12439.759138607358
Iteration 13700: Loss = -12439.759128265368
Iteration 13800: Loss = -12439.759142804089
Iteration 13900: Loss = -12439.759194873797
Iteration 14000: Loss = -12439.759121760588
Iteration 14100: Loss = -12439.75910618292
Iteration 14200: Loss = -12439.759771901961
1
Iteration 14300: Loss = -12439.759079250682
Iteration 14400: Loss = -12439.75910481501
Iteration 14500: Loss = -12439.798781256779
1
Iteration 14600: Loss = -12439.759116531179
Iteration 14700: Loss = -12439.759101645934
Iteration 14800: Loss = -12439.759101103495
Iteration 14900: Loss = -12439.801238377331
1
Iteration 15000: Loss = -12439.759057652225
Iteration 15100: Loss = -12439.759099263863
Iteration 15200: Loss = -12439.759084406784
Iteration 15300: Loss = -12439.759512943481
1
Iteration 15400: Loss = -12439.759096932092
Iteration 15500: Loss = -12439.75907494549
Iteration 15600: Loss = -12439.773774445863
1
Iteration 15700: Loss = -12439.759113715823
Iteration 15800: Loss = -12439.759094575064
Iteration 15900: Loss = -12439.759037482912
Iteration 16000: Loss = -12439.76227542315
1
Iteration 16100: Loss = -12439.759087418503
Iteration 16200: Loss = -12439.758982973342
Iteration 16300: Loss = -12439.760066380204
1
Iteration 16400: Loss = -12439.759056907225
Iteration 16500: Loss = -12439.758977187927
Iteration 16600: Loss = -12439.758957534075
Iteration 16700: Loss = -12439.76190569246
1
Iteration 16800: Loss = -12439.758955462421
Iteration 16900: Loss = -12439.758932228759
Iteration 17000: Loss = -12439.758962500373
Iteration 17100: Loss = -12439.764502691194
1
Iteration 17200: Loss = -12439.75896957703
Iteration 17300: Loss = -12439.758957929713
Iteration 17400: Loss = -12439.758972016733
Iteration 17500: Loss = -12439.774498568537
1
Iteration 17600: Loss = -12439.758953609042
Iteration 17700: Loss = -12439.758995318527
Iteration 17800: Loss = -12440.43646178871
1
Iteration 17900: Loss = -12439.758975264662
Iteration 18000: Loss = -12439.758962767102
Iteration 18100: Loss = -12439.758952506816
Iteration 18200: Loss = -12439.759667084463
1
Iteration 18300: Loss = -12439.758983858226
Iteration 18400: Loss = -12439.758966478616
Iteration 18500: Loss = -12439.793401559451
1
Iteration 18600: Loss = -12439.75898012917
Iteration 18700: Loss = -12439.758994308633
Iteration 18800: Loss = -12439.758989006814
Iteration 18900: Loss = -12439.759298907304
1
Iteration 19000: Loss = -12439.75899855648
Iteration 19100: Loss = -12439.759010394388
Iteration 19200: Loss = -12439.807123030396
1
Iteration 19300: Loss = -12439.759016070871
Iteration 19400: Loss = -12439.758998154884
Iteration 19500: Loss = -12439.759017036795
Iteration 19600: Loss = -12439.759195086359
1
Iteration 19700: Loss = -12439.759010812348
Iteration 19800: Loss = -12439.75901967647
Iteration 19900: Loss = -12439.767365652235
1
pi: tensor([[1.0000e+00, 2.3434e-07],
        [1.5335e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0315, 0.9685], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3028, 0.1794],
         [0.6158, 0.2034]],

        [[0.6745, 0.1101],
         [0.5015, 0.6803]],

        [[0.7079, 0.2513],
         [0.7239, 0.6790]],

        [[0.7260, 0.1982],
         [0.5599, 0.6582]],

        [[0.5328, 0.1408],
         [0.6999, 0.5214]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: -0.004267232452421997
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.012864505300896736
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.021933873838361234
Global Adjusted Rand Index: 0.008741313685109638
Average Adjusted Rand Index: 0.00723135893042734
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20218.237902688492
Iteration 100: Loss = -12448.052231445898
Iteration 200: Loss = -12443.23217495779
Iteration 300: Loss = -12442.850436765943
Iteration 400: Loss = -12442.69500060325
Iteration 500: Loss = -12442.591515221226
Iteration 600: Loss = -12442.509702759911
Iteration 700: Loss = -12442.442399216183
Iteration 800: Loss = -12442.390345669139
Iteration 900: Loss = -12442.351938214166
Iteration 1000: Loss = -12442.323340053184
Iteration 1100: Loss = -12442.301051368922
Iteration 1200: Loss = -12442.282546023896
Iteration 1300: Loss = -12442.265396294593
Iteration 1400: Loss = -12442.245164294623
Iteration 1500: Loss = -12442.20563054282
Iteration 1600: Loss = -12442.071817078226
Iteration 1700: Loss = -12442.038796681585
Iteration 1800: Loss = -12442.032543587142
Iteration 1900: Loss = -12442.02790791421
Iteration 2000: Loss = -12442.02424757052
Iteration 2100: Loss = -12442.022102675437
Iteration 2200: Loss = -12442.018601326588
Iteration 2300: Loss = -12442.01634975206
Iteration 2400: Loss = -12442.014404989024
Iteration 2500: Loss = -12442.012725187311
Iteration 2600: Loss = -12442.011425721474
Iteration 2700: Loss = -12442.009886185257
Iteration 2800: Loss = -12442.008767249274
Iteration 2900: Loss = -12442.008667910324
Iteration 3000: Loss = -12442.006748208883
Iteration 3100: Loss = -12442.005987483666
Iteration 3200: Loss = -12442.00523863833
Iteration 3300: Loss = -12442.004532429217
Iteration 3400: Loss = -12442.00434065026
Iteration 3500: Loss = -12442.00334957821
Iteration 3600: Loss = -12442.002805763159
Iteration 3700: Loss = -12442.002362582782
Iteration 3800: Loss = -12442.001896700802
Iteration 3900: Loss = -12442.001535791269
Iteration 4000: Loss = -12442.001148466145
Iteration 4100: Loss = -12442.000843489852
Iteration 4200: Loss = -12442.000482703299
Iteration 4300: Loss = -12442.000179968692
Iteration 4400: Loss = -12441.999959605953
Iteration 4500: Loss = -12441.999706007171
Iteration 4600: Loss = -12442.003072472626
1
Iteration 4700: Loss = -12441.99927110612
Iteration 4800: Loss = -12441.999113932818
Iteration 4900: Loss = -12441.998853629144
Iteration 5000: Loss = -12441.998672739654
Iteration 5100: Loss = -12441.998637107576
Iteration 5200: Loss = -12441.998380756993
Iteration 5300: Loss = -12441.998543540902
1
Iteration 5400: Loss = -12441.99810272833
Iteration 5500: Loss = -12441.997930323885
Iteration 5600: Loss = -12441.997897571326
Iteration 5700: Loss = -12441.997789260426
Iteration 5800: Loss = -12442.002764344163
1
Iteration 5900: Loss = -12441.997512931623
Iteration 6000: Loss = -12441.997502817314
Iteration 6100: Loss = -12441.997368051098
Iteration 6200: Loss = -12441.997267721947
Iteration 6300: Loss = -12441.998269012982
1
Iteration 6400: Loss = -12441.997156657513
Iteration 6500: Loss = -12442.002216088376
1
Iteration 6600: Loss = -12442.02903311574
2
Iteration 6700: Loss = -12441.996984301166
Iteration 6800: Loss = -12441.99871363374
1
Iteration 6900: Loss = -12441.996826824281
Iteration 7000: Loss = -12441.996996788856
1
Iteration 7100: Loss = -12441.9967247923
Iteration 7200: Loss = -12441.997261646433
1
Iteration 7300: Loss = -12441.996666047979
Iteration 7400: Loss = -12441.998388547063
1
Iteration 7500: Loss = -12441.996580408879
Iteration 7600: Loss = -12442.31162906457
1
Iteration 7700: Loss = -12441.996554111454
Iteration 7800: Loss = -12441.996479588497
Iteration 7900: Loss = -12441.997152838823
1
Iteration 8000: Loss = -12441.996447356338
Iteration 8100: Loss = -12441.996900269281
1
Iteration 8200: Loss = -12441.996382578514
Iteration 8300: Loss = -12441.996339664016
Iteration 8400: Loss = -12441.996503534014
1
Iteration 8500: Loss = -12441.99627458673
Iteration 8600: Loss = -12442.050596022867
1
Iteration 8700: Loss = -12441.996229062237
Iteration 8800: Loss = -12442.017526362937
1
Iteration 8900: Loss = -12441.99624375862
Iteration 9000: Loss = -12442.004142516173
1
Iteration 9100: Loss = -12441.996151209081
Iteration 9200: Loss = -12441.996182158044
Iteration 9300: Loss = -12441.996918046685
1
Iteration 9400: Loss = -12441.996223205653
Iteration 9500: Loss = -12441.998533552727
1
Iteration 9600: Loss = -12441.996134213578
Iteration 9700: Loss = -12441.996296579337
1
Iteration 9800: Loss = -12441.996140340581
Iteration 9900: Loss = -12441.99620280787
Iteration 10000: Loss = -12441.99606623293
Iteration 10100: Loss = -12441.996084372171
Iteration 10200: Loss = -12441.996038698157
Iteration 10300: Loss = -12442.008970930803
1
Iteration 10400: Loss = -12441.996039038953
Iteration 10500: Loss = -12442.081988134689
1
Iteration 10600: Loss = -12442.005263096647
2
Iteration 10700: Loss = -12442.007400049037
3
Iteration 10800: Loss = -12441.996076336449
Iteration 10900: Loss = -12441.996341250897
1
Iteration 11000: Loss = -12441.998308809867
2
Iteration 11100: Loss = -12441.9959908796
Iteration 11200: Loss = -12441.996010378734
Iteration 11300: Loss = -12441.996248104228
1
Iteration 11400: Loss = -12441.995959852908
Iteration 11500: Loss = -12441.997416478554
1
Iteration 11600: Loss = -12441.995966412149
Iteration 11700: Loss = -12442.334282037718
1
Iteration 11800: Loss = -12441.99598032914
Iteration 11900: Loss = -12442.327054080459
1
Iteration 12000: Loss = -12441.995962458834
Iteration 12100: Loss = -12441.997479346646
1
Iteration 12200: Loss = -12441.996059795085
Iteration 12300: Loss = -12442.022698238652
1
Iteration 12400: Loss = -12441.995949697657
Iteration 12500: Loss = -12442.000445164775
1
Iteration 12600: Loss = -12442.075505276674
2
Iteration 12700: Loss = -12441.995916415475
Iteration 12800: Loss = -12442.00130549849
1
Iteration 12900: Loss = -12441.995953921161
Iteration 13000: Loss = -12442.222266577592
1
Iteration 13100: Loss = -12441.995949602282
Iteration 13200: Loss = -12442.041289195411
1
Iteration 13300: Loss = -12441.996287792841
2
Iteration 13400: Loss = -12441.996045166488
Iteration 13500: Loss = -12442.008185516353
1
Iteration 13600: Loss = -12441.99628385108
2
Iteration 13700: Loss = -12442.086974109685
3
Iteration 13800: Loss = -12442.177709121615
4
Iteration 13900: Loss = -12441.99679263197
5
Iteration 14000: Loss = -12441.995977744813
Iteration 14100: Loss = -12441.99614816069
1
Iteration 14200: Loss = -12442.023941146319
2
Iteration 14300: Loss = -12441.99597219288
Iteration 14400: Loss = -12441.998927971312
1
Iteration 14500: Loss = -12441.995949760943
Iteration 14600: Loss = -12441.99701029845
1
Iteration 14700: Loss = -12441.99594784424
Iteration 14800: Loss = -12441.996114291309
1
Iteration 14900: Loss = -12441.996743983016
2
Iteration 15000: Loss = -12441.996243623122
3
Iteration 15100: Loss = -12441.998073257899
4
Iteration 15200: Loss = -12442.025790747775
5
Iteration 15300: Loss = -12441.995910353906
Iteration 15400: Loss = -12442.034163727409
1
Iteration 15500: Loss = -12441.996051643702
2
Iteration 15600: Loss = -12442.026221248123
3
Iteration 15700: Loss = -12441.995937951684
Iteration 15800: Loss = -12441.996603992615
1
Iteration 15900: Loss = -12441.995983658044
Iteration 16000: Loss = -12442.033044279971
1
Iteration 16100: Loss = -12441.997026374469
2
Iteration 16200: Loss = -12441.996143995782
3
Iteration 16300: Loss = -12442.199343933878
4
Iteration 16400: Loss = -12441.995898489973
Iteration 16500: Loss = -12441.996291421006
1
Iteration 16600: Loss = -12442.020984873738
2
Iteration 16700: Loss = -12441.995974262587
Iteration 16800: Loss = -12441.995901650842
Iteration 16900: Loss = -12441.99747526833
1
Iteration 17000: Loss = -12441.995898448036
Iteration 17100: Loss = -12441.99606058064
1
Iteration 17200: Loss = -12441.995964406187
Iteration 17300: Loss = -12441.996953981467
1
Iteration 17400: Loss = -12441.997876766174
2
Iteration 17500: Loss = -12441.995936031955
Iteration 17600: Loss = -12442.024164109289
1
Iteration 17700: Loss = -12441.995885091152
Iteration 17800: Loss = -12441.99604894498
1
Iteration 17900: Loss = -12441.99589503475
Iteration 18000: Loss = -12441.996074245004
1
Iteration 18100: Loss = -12441.995880135046
Iteration 18200: Loss = -12441.996535162896
1
Iteration 18300: Loss = -12441.995890503735
Iteration 18400: Loss = -12442.027811092606
1
Iteration 18500: Loss = -12441.995903567353
Iteration 18600: Loss = -12442.023307998505
1
Iteration 18700: Loss = -12441.998191801244
2
Iteration 18800: Loss = -12441.997351127904
3
Iteration 18900: Loss = -12441.999773127885
4
Iteration 19000: Loss = -12442.209420302908
5
Iteration 19100: Loss = -12441.996820146915
6
Iteration 19200: Loss = -12441.998626997181
7
Iteration 19300: Loss = -12441.998221640466
8
Iteration 19400: Loss = -12441.995936894938
Iteration 19500: Loss = -12442.029810334243
1
Iteration 19600: Loss = -12441.995874693852
Iteration 19700: Loss = -12441.996161987798
1
Iteration 19800: Loss = -12441.995872233403
Iteration 19900: Loss = -12442.294937005892
1
pi: tensor([[3.2643e-01, 6.7357e-01],
        [2.5886e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9838, 0.0162], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1935, 0.2702],
         [0.5234, 0.2015]],

        [[0.5243, 0.2032],
         [0.6091, 0.5035]],

        [[0.5937, 0.2099],
         [0.6897, 0.5013]],

        [[0.6415, 0.2317],
         [0.6851, 0.7276]],

        [[0.6894, 0.3344],
         [0.5627, 0.5369]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
Global Adjusted Rand Index: -0.0022695733275203043
Average Adjusted Rand Index: 0.002693321705669444
11855.192237897678
[0.008741313685109638, -0.0022695733275203043] [0.00723135893042734, 0.002693321705669444] [12439.75901179634, 12441.995876908972]
-------------------------------------
This iteration is 35
True Objective function: Loss = -11770.404964332021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21366.75068589207
Iteration 100: Loss = -12234.620836604323
Iteration 200: Loss = -12233.993404100296
Iteration 300: Loss = -12233.880526420726
Iteration 400: Loss = -12233.834679711363
Iteration 500: Loss = -12233.807181596367
Iteration 600: Loss = -12233.786730826241
Iteration 700: Loss = -12233.769874195037
Iteration 800: Loss = -12233.75522511424
Iteration 900: Loss = -12233.741954376928
Iteration 1000: Loss = -12233.729628271158
Iteration 1100: Loss = -12233.717683241843
Iteration 1200: Loss = -12233.705743077368
Iteration 1300: Loss = -12233.693229848886
Iteration 1400: Loss = -12233.679509908057
Iteration 1500: Loss = -12233.663537408213
Iteration 1600: Loss = -12233.643732563161
Iteration 1700: Loss = -12233.618061823197
Iteration 1800: Loss = -12233.584987888546
Iteration 1900: Loss = -12233.550064127006
Iteration 2000: Loss = -12233.524124715874
Iteration 2100: Loss = -12233.507231522972
Iteration 2200: Loss = -12233.494598224248
Iteration 2300: Loss = -12233.48371977175
Iteration 2400: Loss = -12233.473146151027
Iteration 2500: Loss = -12233.462196951532
Iteration 2600: Loss = -12233.45064482893
Iteration 2700: Loss = -12233.43873253628
Iteration 2800: Loss = -12233.42697791226
Iteration 2900: Loss = -12233.416152020505
Iteration 3000: Loss = -12233.40667475033
Iteration 3100: Loss = -12233.398500153631
Iteration 3200: Loss = -12233.391548906038
Iteration 3300: Loss = -12233.38539650723
Iteration 3400: Loss = -12233.379733505468
Iteration 3500: Loss = -12233.37430411858
Iteration 3600: Loss = -12233.368785518242
Iteration 3700: Loss = -12233.363017554415
Iteration 3800: Loss = -12233.356765543063
Iteration 3900: Loss = -12233.349877022929
Iteration 4000: Loss = -12233.342065829442
Iteration 4100: Loss = -12233.33332389103
Iteration 4200: Loss = -12233.323505473161
Iteration 4300: Loss = -12233.313006752664
Iteration 4400: Loss = -12233.30238147001
Iteration 4500: Loss = -12233.292698811942
Iteration 4600: Loss = -12233.28510236424
Iteration 4700: Loss = -12233.279847140455
Iteration 4800: Loss = -12233.275882797778
Iteration 4900: Loss = -12233.27266969914
Iteration 5000: Loss = -12233.269724909735
Iteration 5100: Loss = -12233.267030194113
Iteration 5200: Loss = -12233.264443137909
Iteration 5300: Loss = -12233.261880112099
Iteration 5400: Loss = -12233.259388200524
Iteration 5500: Loss = -12233.25681799868
Iteration 5600: Loss = -12233.254148930415
Iteration 5700: Loss = -12233.25115751583
Iteration 5800: Loss = -12233.247551829507
Iteration 5900: Loss = -12233.242634574219
Iteration 6000: Loss = -12233.23497029168
Iteration 6100: Loss = -12233.221801717025
Iteration 6200: Loss = -12233.20084081295
Iteration 6300: Loss = -12233.175434516834
Iteration 6400: Loss = -12233.152777270552
Iteration 6500: Loss = -12233.135619840908
Iteration 6600: Loss = -12233.122773610952
Iteration 6700: Loss = -12233.112817007168
Iteration 6800: Loss = -12233.104518040553
Iteration 6900: Loss = -12233.097337607243
Iteration 7000: Loss = -12233.090878526365
Iteration 7100: Loss = -12233.085188450023
Iteration 7200: Loss = -12233.079958190761
Iteration 7300: Loss = -12233.07549117646
Iteration 7400: Loss = -12233.071709111144
Iteration 7500: Loss = -12233.068482942628
Iteration 7600: Loss = -12233.06581244818
Iteration 7700: Loss = -12233.06372512047
Iteration 7800: Loss = -12233.061820484434
Iteration 7900: Loss = -12233.060265430036
Iteration 8000: Loss = -12233.05901988238
Iteration 8100: Loss = -12233.059373447817
1
Iteration 8200: Loss = -12233.056931404222
Iteration 8300: Loss = -12233.056125334693
Iteration 8400: Loss = -12233.05543099213
Iteration 8500: Loss = -12233.055259922603
Iteration 8600: Loss = -12233.05433197605
Iteration 8700: Loss = -12233.053819261193
Iteration 8800: Loss = -12233.180058777754
1
Iteration 8900: Loss = -12233.0530291084
Iteration 9000: Loss = -12233.052679233391
Iteration 9100: Loss = -12233.052332847023
Iteration 9200: Loss = -12233.053648219819
1
Iteration 9300: Loss = -12233.05179632455
Iteration 9400: Loss = -12233.051536381243
Iteration 9500: Loss = -12233.31071284624
1
Iteration 9600: Loss = -12233.051114661841
Iteration 9700: Loss = -12233.05091898599
Iteration 9800: Loss = -12233.050688066007
Iteration 9900: Loss = -12233.06705966742
1
Iteration 10000: Loss = -12233.050376045374
Iteration 10100: Loss = -12233.050176761522
Iteration 10200: Loss = -12233.050034014672
Iteration 10300: Loss = -12233.054435989465
1
Iteration 10400: Loss = -12233.049797471971
Iteration 10500: Loss = -12233.049687429488
Iteration 10600: Loss = -12233.049784392162
Iteration 10700: Loss = -12233.049549267696
Iteration 10800: Loss = -12233.049395564636
Iteration 10900: Loss = -12233.049328951338
Iteration 11000: Loss = -12233.18824537674
1
Iteration 11100: Loss = -12233.049191894957
Iteration 11200: Loss = -12233.049142487684
Iteration 11300: Loss = -12233.049061946414
Iteration 11400: Loss = -12233.161646660648
1
Iteration 11500: Loss = -12233.0489948672
Iteration 11600: Loss = -12233.04889539819
Iteration 11700: Loss = -12233.048834923655
Iteration 11800: Loss = -12233.049235507784
1
Iteration 11900: Loss = -12233.048769858658
Iteration 12000: Loss = -12233.048720158953
Iteration 12100: Loss = -12233.048731498004
Iteration 12200: Loss = -12233.048592082609
Iteration 12300: Loss = -12233.048512067055
Iteration 12400: Loss = -12233.048467339102
Iteration 12500: Loss = -12233.04843096677
Iteration 12600: Loss = -12233.053402502563
1
Iteration 12700: Loss = -12233.048341796686
Iteration 12800: Loss = -12233.048327776809
Iteration 12900: Loss = -12233.370657533858
1
Iteration 13000: Loss = -12233.048286049985
Iteration 13100: Loss = -12233.048211136103
Iteration 13200: Loss = -12233.048192545786
Iteration 13300: Loss = -12233.04906645536
1
Iteration 13400: Loss = -12233.048140394149
Iteration 13500: Loss = -12233.048116589896
Iteration 13600: Loss = -12233.099660793883
1
Iteration 13700: Loss = -12233.04810009876
Iteration 13800: Loss = -12233.048374027077
1
Iteration 13900: Loss = -12233.05546393227
2
Iteration 14000: Loss = -12233.048052252363
Iteration 14100: Loss = -12233.048097792444
Iteration 14200: Loss = -12233.050595415818
1
Iteration 14300: Loss = -12233.048027963132
Iteration 14400: Loss = -12233.221644137806
1
Iteration 14500: Loss = -12233.048026488204
Iteration 14600: Loss = -12233.047959685102
Iteration 14700: Loss = -12233.048100961909
1
Iteration 14800: Loss = -12233.047943603837
Iteration 14900: Loss = -12233.052188486456
1
Iteration 15000: Loss = -12233.0479270509
Iteration 15100: Loss = -12233.048583001884
1
Iteration 15200: Loss = -12233.047879944763
Iteration 15300: Loss = -12233.67881156328
1
Iteration 15400: Loss = -12233.047868785276
Iteration 15500: Loss = -12233.047845431585
Iteration 15600: Loss = -12233.056169771748
1
Iteration 15700: Loss = -12233.04785811738
Iteration 15800: Loss = -12233.194394190805
1
Iteration 15900: Loss = -12233.047851264046
Iteration 16000: Loss = -12233.058688623098
1
Iteration 16100: Loss = -12233.047842643888
Iteration 16200: Loss = -12233.154714018208
1
Iteration 16300: Loss = -12233.0478647921
Iteration 16400: Loss = -12233.047838001945
Iteration 16500: Loss = -12233.047932511869
Iteration 16600: Loss = -12233.047812751107
Iteration 16700: Loss = -12233.10664285344
1
Iteration 16800: Loss = -12233.048006711348
2
Iteration 16900: Loss = -12233.047841603386
Iteration 17000: Loss = -12233.047856803973
Iteration 17100: Loss = -12233.047779146076
Iteration 17200: Loss = -12233.048943242131
1
Iteration 17300: Loss = -12233.047836877438
Iteration 17400: Loss = -12233.047872519912
Iteration 17500: Loss = -12233.049037986493
1
Iteration 17600: Loss = -12233.04853116951
2
Iteration 17700: Loss = -12233.234584893704
3
Iteration 17800: Loss = -12233.047860425444
Iteration 17900: Loss = -12233.048872261612
1
Iteration 18000: Loss = -12233.04876294413
2
Iteration 18100: Loss = -12233.047826621289
Iteration 18200: Loss = -12233.04778796988
Iteration 18300: Loss = -12233.047778938506
Iteration 18400: Loss = -12233.048982092034
1
Iteration 18500: Loss = -12233.04778180122
Iteration 18600: Loss = -12233.047777504766
Iteration 18700: Loss = -12233.047901333994
1
Iteration 18800: Loss = -12233.047768744105
Iteration 18900: Loss = -12233.0522524759
1
Iteration 19000: Loss = -12233.047788730522
Iteration 19100: Loss = -12233.047699633355
Iteration 19200: Loss = -12233.048599014715
1
Iteration 19300: Loss = -12233.047637432108
Iteration 19400: Loss = -12233.04760982329
Iteration 19500: Loss = -12233.047870366712
1
Iteration 19600: Loss = -12233.047599169395
Iteration 19700: Loss = -12233.062137441348
1
Iteration 19800: Loss = -12233.04763865419
Iteration 19900: Loss = -12233.047614654612
pi: tensor([[9.9999e-01, 9.1577e-06],
        [8.3954e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0095, 0.9905], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4163, 0.1822],
         [0.7002, 0.1961]],

        [[0.5858, 0.2023],
         [0.5122, 0.6492]],

        [[0.6625, 0.3174],
         [0.6204, 0.5110]],

        [[0.7120, 0.1540],
         [0.6314, 0.6938]],

        [[0.6931, 0.1999],
         [0.6557, 0.7014]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0004611683854498105
Average Adjusted Rand Index: -0.00020900598007777922
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23266.194524161037
Iteration 100: Loss = -12234.795550602148
Iteration 200: Loss = -12234.035147990953
Iteration 300: Loss = -12233.89041088653
Iteration 400: Loss = -12233.820724542156
Iteration 500: Loss = -12233.772623173407
Iteration 600: Loss = -12233.732268744834
Iteration 700: Loss = -12233.694042407164
Iteration 800: Loss = -12233.656402645396
Iteration 900: Loss = -12233.621577973205
Iteration 1000: Loss = -12233.592233636615
Iteration 1100: Loss = -12233.568629475187
Iteration 1200: Loss = -12233.54924800551
Iteration 1300: Loss = -12233.532766929162
Iteration 1400: Loss = -12233.518476969166
Iteration 1500: Loss = -12233.50580480949
Iteration 1600: Loss = -12233.494609831097
Iteration 1700: Loss = -12233.484636983187
Iteration 1800: Loss = -12233.475771485122
Iteration 1900: Loss = -12233.467853464746
Iteration 2000: Loss = -12233.460707827151
Iteration 2100: Loss = -12233.454173136412
Iteration 2200: Loss = -12233.448134331527
Iteration 2300: Loss = -12233.442345850339
Iteration 2400: Loss = -12233.436686819714
Iteration 2500: Loss = -12233.431084028469
Iteration 2600: Loss = -12233.425366050244
Iteration 2700: Loss = -12233.419488378218
Iteration 2800: Loss = -12233.41347565772
Iteration 2900: Loss = -12233.407319713431
Iteration 3000: Loss = -12233.40128997364
Iteration 3100: Loss = -12233.395231122533
Iteration 3200: Loss = -12233.389228292728
Iteration 3300: Loss = -12233.382903909871
Iteration 3400: Loss = -12233.375866340702
Iteration 3500: Loss = -12233.367419490933
Iteration 3600: Loss = -12233.356471591473
Iteration 3700: Loss = -12233.34127662606
Iteration 3800: Loss = -12233.32024830049
Iteration 3900: Loss = -12233.293699823485
Iteration 4000: Loss = -12233.26531364666
Iteration 4100: Loss = -12233.239459246497
Iteration 4200: Loss = -12233.217263897348
Iteration 4300: Loss = -12233.198458149289
Iteration 4400: Loss = -12233.182775270952
Iteration 4500: Loss = -12233.169586261292
Iteration 4600: Loss = -12233.158419787464
Iteration 4700: Loss = -12233.148860058012
Iteration 4800: Loss = -12233.140763521378
Iteration 4900: Loss = -12233.133649843534
Iteration 5000: Loss = -12233.127355994324
Iteration 5100: Loss = -12233.121777295208
Iteration 5200: Loss = -12233.116752785349
Iteration 5300: Loss = -12233.112192221204
Iteration 5400: Loss = -12233.107973748887
Iteration 5500: Loss = -12233.104016320161
Iteration 5600: Loss = -12233.100260175015
Iteration 5700: Loss = -12233.096679181532
Iteration 5800: Loss = -12233.09320329041
Iteration 5900: Loss = -12233.089915888415
Iteration 6000: Loss = -12233.086615276392
Iteration 6100: Loss = -12233.083483035629
Iteration 6200: Loss = -12233.08044502734
Iteration 6300: Loss = -12233.077534739434
Iteration 6400: Loss = -12233.07475000074
Iteration 6500: Loss = -12233.072186956564
Iteration 6600: Loss = -12233.06978251072
Iteration 6700: Loss = -12233.067590422774
Iteration 6800: Loss = -12233.065628545915
Iteration 6900: Loss = -12233.06386650996
Iteration 7000: Loss = -12233.062288533896
Iteration 7100: Loss = -12233.060950701585
Iteration 7200: Loss = -12233.059766741068
Iteration 7300: Loss = -12233.058734640423
Iteration 7400: Loss = -12233.057822381417
Iteration 7500: Loss = -12233.057013076592
Iteration 7600: Loss = -12233.05647132489
Iteration 7700: Loss = -12233.055700986062
Iteration 7800: Loss = -12233.055098114639
Iteration 7900: Loss = -12233.057410825462
1
Iteration 8000: Loss = -12233.054118239608
Iteration 8100: Loss = -12233.053664363415
Iteration 8200: Loss = -12233.18536521256
1
Iteration 8300: Loss = -12233.052898137936
Iteration 8400: Loss = -12233.052579535426
Iteration 8500: Loss = -12233.052256505767
Iteration 8600: Loss = -12233.052826405854
1
Iteration 8700: Loss = -12233.051734749244
Iteration 8800: Loss = -12233.051467584668
Iteration 8900: Loss = -12233.051844216465
1
Iteration 9000: Loss = -12233.051037129106
Iteration 9100: Loss = -12233.05085403173
Iteration 9200: Loss = -12233.505438852664
1
Iteration 9300: Loss = -12233.05052569866
Iteration 9400: Loss = -12233.050374306753
Iteration 9500: Loss = -12233.050224713106
Iteration 9600: Loss = -12233.051006666128
1
Iteration 9700: Loss = -12233.049997934904
Iteration 9800: Loss = -12233.04989467717
Iteration 9900: Loss = -12233.04991769259
Iteration 10000: Loss = -12233.049669010383
Iteration 10100: Loss = -12233.049590375062
Iteration 10200: Loss = -12233.594496878175
1
Iteration 10300: Loss = -12233.049434237899
Iteration 10400: Loss = -12233.049322645762
Iteration 10500: Loss = -12233.049223877546
Iteration 10600: Loss = -12233.0497503367
1
Iteration 10700: Loss = -12233.049108520554
Iteration 10800: Loss = -12233.049035220865
Iteration 10900: Loss = -12233.324884479141
1
Iteration 11000: Loss = -12233.0488870711
Iteration 11100: Loss = -12233.04880367833
Iteration 11200: Loss = -12233.048797893282
Iteration 11300: Loss = -12233.048745190004
Iteration 11400: Loss = -12233.04864659019
Iteration 11500: Loss = -12233.048616206926
Iteration 11600: Loss = -12233.085093502128
1
Iteration 11700: Loss = -12233.048495951061
Iteration 11800: Loss = -12233.048493749087
Iteration 11900: Loss = -12233.06811349491
1
Iteration 12000: Loss = -12233.048374271395
Iteration 12100: Loss = -12233.048349172073
Iteration 12200: Loss = -12233.048324389898
Iteration 12300: Loss = -12233.049457450812
1
Iteration 12400: Loss = -12233.048194275318
Iteration 12500: Loss = -12233.04818146677
Iteration 12600: Loss = -12233.048173252628
Iteration 12700: Loss = -12233.048271161748
Iteration 12800: Loss = -12233.048100793834
Iteration 12900: Loss = -12233.048102265691
Iteration 13000: Loss = -12233.048083110636
Iteration 13100: Loss = -12233.048056203288
Iteration 13200: Loss = -12233.047980757412
Iteration 13300: Loss = -12233.04802015488
Iteration 13400: Loss = -12233.055394122295
1
Iteration 13500: Loss = -12233.047969751722
Iteration 13600: Loss = -12233.047972209239
Iteration 13700: Loss = -12233.047918497983
Iteration 13800: Loss = -12233.047939627686
Iteration 13900: Loss = -12233.047918620889
Iteration 14000: Loss = -12233.047848649143
Iteration 14100: Loss = -12233.04788032815
Iteration 14200: Loss = -12233.060400606706
1
Iteration 14300: Loss = -12233.047833645209
Iteration 14400: Loss = -12233.047846937949
Iteration 14500: Loss = -12233.047824073697
Iteration 14600: Loss = -12233.047917860758
Iteration 14700: Loss = -12233.04779693881
Iteration 14800: Loss = -12233.047805160188
Iteration 14900: Loss = -12233.048028358926
1
Iteration 15000: Loss = -12233.04764548975
Iteration 15100: Loss = -12233.047909377108
1
Iteration 15200: Loss = -12233.047639698134
Iteration 15300: Loss = -12233.052493119516
1
Iteration 15400: Loss = -12233.047642146086
Iteration 15500: Loss = -12233.053110932722
1
Iteration 15600: Loss = -12233.047695477784
Iteration 15700: Loss = -12233.047615470416
Iteration 15800: Loss = -12233.094835761787
1
Iteration 15900: Loss = -12233.047621199674
Iteration 16000: Loss = -12233.049407636614
1
Iteration 16100: Loss = -12233.047693497088
Iteration 16200: Loss = -12233.04768265912
Iteration 16300: Loss = -12233.048327915192
1
Iteration 16400: Loss = -12233.04761326014
Iteration 16500: Loss = -12233.047830639913
1
Iteration 16600: Loss = -12233.047555795225
Iteration 16700: Loss = -12233.050510088186
1
Iteration 16800: Loss = -12233.04757126638
Iteration 16900: Loss = -12233.04962024625
1
Iteration 17000: Loss = -12233.047578325493
Iteration 17100: Loss = -12233.124291030663
1
Iteration 17200: Loss = -12233.04755689906
Iteration 17300: Loss = -12233.047543496723
Iteration 17400: Loss = -12233.30695653033
1
Iteration 17500: Loss = -12233.047543344246
Iteration 17600: Loss = -12233.047521722552
Iteration 17700: Loss = -12233.191134584396
1
Iteration 17800: Loss = -12233.048918048684
2
Iteration 17900: Loss = -12233.048971693795
3
Iteration 18000: Loss = -12233.047487027667
Iteration 18100: Loss = -12233.047751292763
1
Iteration 18200: Loss = -12233.047529591733
Iteration 18300: Loss = -12233.147492204851
1
Iteration 18400: Loss = -12233.047497461914
Iteration 18500: Loss = -12233.04753548854
Iteration 18600: Loss = -12233.047551505828
Iteration 18700: Loss = -12233.047730937049
1
Iteration 18800: Loss = -12233.139310457345
2
Iteration 18900: Loss = -12233.047546787659
Iteration 19000: Loss = -12233.047475767655
Iteration 19100: Loss = -12233.047516408746
Iteration 19200: Loss = -12233.048317093811
1
Iteration 19300: Loss = -12233.275450086765
2
Iteration 19400: Loss = -12233.047471140171
Iteration 19500: Loss = -12233.047436394983
Iteration 19600: Loss = -12233.051017816664
1
Iteration 19700: Loss = -12233.047477939594
Iteration 19800: Loss = -12233.048301594548
1
Iteration 19900: Loss = -12233.047480967374
pi: tensor([[1.0000e+00, 6.0779e-08],
        [8.9475e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9904, 0.0096], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1953, 0.1816],
         [0.5939, 0.4159]],

        [[0.6339, 0.2016],
         [0.6964, 0.5594]],

        [[0.5031, 0.3176],
         [0.5217, 0.6870]],

        [[0.6185, 0.1547],
         [0.6699, 0.5154]],

        [[0.5970, 0.2005],
         [0.5870, 0.5563]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0004611683854498105
Average Adjusted Rand Index: -0.00020900598007777922
11770.404964332021
[0.0004611683854498105, 0.0004611683854498105] [-0.00020900598007777922, -0.00020900598007777922] [12233.06430597913, 12233.047550505325]
-------------------------------------
This iteration is 36
True Objective function: Loss = -11819.877723201182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21023.41814206714
Iteration 100: Loss = -12337.065664666077
Iteration 200: Loss = -12336.491425413184
Iteration 300: Loss = -12336.361619995025
Iteration 400: Loss = -12336.29149995926
Iteration 500: Loss = -12336.24578739567
Iteration 600: Loss = -12336.212383522692
Iteration 700: Loss = -12336.184607121673
Iteration 800: Loss = -12336.157975360966
Iteration 900: Loss = -12336.129178440691
Iteration 1000: Loss = -12336.09502066735
Iteration 1100: Loss = -12336.051240884668
Iteration 1200: Loss = -12335.991715148186
Iteration 1300: Loss = -12335.906974143858
Iteration 1400: Loss = -12335.7885038428
Iteration 1500: Loss = -12335.644111455898
Iteration 1600: Loss = -12335.498604113993
Iteration 1700: Loss = -12335.36683512353
Iteration 1800: Loss = -12335.251958126113
Iteration 1900: Loss = -12335.15259872621
Iteration 2000: Loss = -12335.068039673288
Iteration 2100: Loss = -12334.99703304913
Iteration 2200: Loss = -12334.93876106827
Iteration 2300: Loss = -12334.888561521962
Iteration 2400: Loss = -12334.847232086997
Iteration 2500: Loss = -12334.812782279207
Iteration 2600: Loss = -12334.78400651875
Iteration 2700: Loss = -12334.759352534038
Iteration 2800: Loss = -12334.738113306366
Iteration 2900: Loss = -12334.720287649896
Iteration 3000: Loss = -12334.705404513783
Iteration 3100: Loss = -12334.693464445205
Iteration 3200: Loss = -12334.683973740703
Iteration 3300: Loss = -12334.676540150196
Iteration 3400: Loss = -12334.67071384141
Iteration 3500: Loss = -12334.666132671942
Iteration 3600: Loss = -12334.662423182153
Iteration 3700: Loss = -12334.6594941654
Iteration 3800: Loss = -12334.65709796577
Iteration 3900: Loss = -12334.655125613765
Iteration 4000: Loss = -12334.653491496578
Iteration 4100: Loss = -12334.652105102577
Iteration 4200: Loss = -12334.650911413122
Iteration 4300: Loss = -12334.649965818884
Iteration 4400: Loss = -12334.649084606799
Iteration 4500: Loss = -12334.64836668841
Iteration 4600: Loss = -12334.647712751288
Iteration 4700: Loss = -12334.647133021152
Iteration 4800: Loss = -12334.646639839275
Iteration 4900: Loss = -12334.646218283413
Iteration 5000: Loss = -12334.64580747442
Iteration 5100: Loss = -12334.645421998093
Iteration 5200: Loss = -12334.6451244411
Iteration 5300: Loss = -12334.644840994277
Iteration 5400: Loss = -12334.644592463163
Iteration 5500: Loss = -12334.644364968983
Iteration 5600: Loss = -12334.644158535466
Iteration 5700: Loss = -12334.643964060735
Iteration 5800: Loss = -12334.643778420288
Iteration 5900: Loss = -12334.643656660674
Iteration 6000: Loss = -12334.643497332228
Iteration 6100: Loss = -12334.643330274454
Iteration 6200: Loss = -12334.643243277524
Iteration 6300: Loss = -12334.643088465064
Iteration 6400: Loss = -12334.64302576232
Iteration 6500: Loss = -12334.642918541524
Iteration 6600: Loss = -12334.642798143988
Iteration 6700: Loss = -12334.642765257273
Iteration 6800: Loss = -12334.64271221175
Iteration 6900: Loss = -12334.642653018333
Iteration 7000: Loss = -12334.6425961434
Iteration 7100: Loss = -12334.642516295336
Iteration 7200: Loss = -12334.642410293904
Iteration 7300: Loss = -12334.642373367791
Iteration 7400: Loss = -12334.642327349795
Iteration 7500: Loss = -12334.643781401208
1
Iteration 7600: Loss = -12334.64391308484
2
Iteration 7700: Loss = -12334.642267579478
Iteration 7800: Loss = -12334.64221944142
Iteration 7900: Loss = -12334.642201777153
Iteration 8000: Loss = -12334.642195212822
Iteration 8100: Loss = -12334.642064963951
Iteration 8200: Loss = -12334.650904173377
1
Iteration 8300: Loss = -12334.64208953829
Iteration 8400: Loss = -12334.642129135436
Iteration 8500: Loss = -12334.652960912286
1
Iteration 8600: Loss = -12334.641980189015
Iteration 8700: Loss = -12334.642818907101
1
Iteration 8800: Loss = -12334.767847832629
2
Iteration 8900: Loss = -12334.641961851294
Iteration 9000: Loss = -12334.689246153905
1
Iteration 9100: Loss = -12334.641928467467
Iteration 9200: Loss = -12334.703293857152
1
Iteration 9300: Loss = -12334.641828771471
Iteration 9400: Loss = -12334.646369348686
1
Iteration 9500: Loss = -12334.641833471665
Iteration 9600: Loss = -12334.650561987324
1
Iteration 9700: Loss = -12334.641829020538
Iteration 9800: Loss = -12334.641810862822
Iteration 9900: Loss = -12334.683536198658
1
Iteration 10000: Loss = -12334.641780394832
Iteration 10100: Loss = -12334.641852560342
Iteration 10200: Loss = -12334.641877658769
Iteration 10300: Loss = -12334.64177637925
Iteration 10400: Loss = -12334.642478922462
1
Iteration 10500: Loss = -12334.641758432557
Iteration 10600: Loss = -12334.644695140985
1
Iteration 10700: Loss = -12334.641783527999
Iteration 10800: Loss = -12334.648503392738
1
Iteration 10900: Loss = -12334.673309688673
2
Iteration 11000: Loss = -12334.641773686824
Iteration 11100: Loss = -12334.641877017391
1
Iteration 11200: Loss = -12334.675985234186
2
Iteration 11300: Loss = -12334.644301276594
3
Iteration 11400: Loss = -12334.642294273415
4
Iteration 11500: Loss = -12334.894449679747
5
Iteration 11600: Loss = -12334.641705712173
Iteration 11700: Loss = -12334.674839213802
1
Iteration 11800: Loss = -12334.641692295732
Iteration 11900: Loss = -12334.643662269982
1
Iteration 12000: Loss = -12334.641935848336
2
Iteration 12100: Loss = -12334.645554569604
3
Iteration 12200: Loss = -12334.678795516087
4
Iteration 12300: Loss = -12334.64166908543
Iteration 12400: Loss = -12334.682132942587
1
Iteration 12500: Loss = -12334.641643032648
Iteration 12600: Loss = -12334.712890536954
1
Iteration 12700: Loss = -12334.645802673154
2
Iteration 12800: Loss = -12334.641698765867
Iteration 12900: Loss = -12334.64211382115
1
Iteration 13000: Loss = -12334.642462679249
2
Iteration 13100: Loss = -12334.642376244741
3
Iteration 13200: Loss = -12334.650580224306
4
Iteration 13300: Loss = -12334.641654590372
Iteration 13400: Loss = -12334.646690353497
1
Iteration 13500: Loss = -12334.641676642228
Iteration 13600: Loss = -12334.641802571065
1
Iteration 13700: Loss = -12334.642095476116
2
Iteration 13800: Loss = -12334.641790917167
3
Iteration 13900: Loss = -12334.64284629716
4
Iteration 14000: Loss = -12334.642120913708
5
Iteration 14100: Loss = -12334.659575758165
6
Iteration 14200: Loss = -12334.643458366483
7
Iteration 14300: Loss = -12334.641695123299
Iteration 14400: Loss = -12334.641827629112
1
Iteration 14500: Loss = -12334.64184432547
2
Iteration 14600: Loss = -12334.641964755307
3
Iteration 14700: Loss = -12334.67502448873
4
Iteration 14800: Loss = -12334.641848939063
5
Iteration 14900: Loss = -12334.64189377639
6
Iteration 15000: Loss = -12334.642016860926
7
Iteration 15100: Loss = -12334.654931819316
8
Iteration 15200: Loss = -12334.64251386413
9
Iteration 15300: Loss = -12334.64837197663
10
Iteration 15400: Loss = -12334.642229172374
11
Iteration 15500: Loss = -12334.64166262637
Iteration 15600: Loss = -12334.719394556552
1
Iteration 15700: Loss = -12334.64163652983
Iteration 15800: Loss = -12334.644619713144
1
Iteration 15900: Loss = -12334.641637612538
Iteration 16000: Loss = -12334.641990174732
1
Iteration 16100: Loss = -12334.641700532022
Iteration 16200: Loss = -12334.642769504002
1
Iteration 16300: Loss = -12334.641811719359
2
Iteration 16400: Loss = -12334.64167247733
Iteration 16500: Loss = -12334.705796638342
1
Iteration 16600: Loss = -12334.641631915234
Iteration 16700: Loss = -12334.646431460014
1
Iteration 16800: Loss = -12334.641663349948
Iteration 16900: Loss = -12334.644146734654
1
Iteration 17000: Loss = -12334.641627180989
Iteration 17100: Loss = -12334.658131742946
1
Iteration 17200: Loss = -12334.718077760475
2
Iteration 17300: Loss = -12334.642814757104
3
Iteration 17400: Loss = -12334.641690980177
Iteration 17500: Loss = -12334.641869209905
1
Iteration 17600: Loss = -12334.641908512971
2
Iteration 17700: Loss = -12334.775804429526
3
Iteration 17800: Loss = -12334.64185841468
4
Iteration 17900: Loss = -12334.641805215484
5
Iteration 18000: Loss = -12334.658040283239
6
Iteration 18100: Loss = -12334.641649653322
Iteration 18200: Loss = -12334.641722217526
Iteration 18300: Loss = -12334.641715854099
Iteration 18400: Loss = -12334.647905408694
1
Iteration 18500: Loss = -12334.691440891746
2
Iteration 18600: Loss = -12334.641621649964
Iteration 18700: Loss = -12334.643805339574
1
Iteration 18800: Loss = -12334.685542555668
2
Iteration 18900: Loss = -12334.64186258324
3
Iteration 19000: Loss = -12334.648374868064
4
Iteration 19100: Loss = -12334.652422219628
5
Iteration 19200: Loss = -12334.651968380715
6
Iteration 19300: Loss = -12334.667729159173
7
Iteration 19400: Loss = -12334.644898322584
8
Iteration 19500: Loss = -12334.642175821138
9
Iteration 19600: Loss = -12334.64195002539
10
Iteration 19700: Loss = -12334.642341975334
11
Iteration 19800: Loss = -12334.737430294274
12
Iteration 19900: Loss = -12334.641647299655
pi: tensor([[0.9872, 0.0128],
        [0.9735, 0.0265]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([6.1948e-04, 9.9938e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1988, 0.2059],
         [0.5902, 0.2063]],

        [[0.5659, 0.1293],
         [0.5309, 0.6621]],

        [[0.5961, 0.1146],
         [0.5803, 0.6956]],

        [[0.6006, 0.2045],
         [0.6403, 0.5330]],

        [[0.6261, 0.2155],
         [0.5004, 0.6584]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008933065927818041
Average Adjusted Rand Index: 0.0005107695272972583
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21395.99755305159
Iteration 100: Loss = -12336.8906375755
Iteration 200: Loss = -12336.457103245952
Iteration 300: Loss = -12336.328154403393
Iteration 400: Loss = -12336.268165676474
Iteration 500: Loss = -12336.231222781511
Iteration 600: Loss = -12336.20469958606
Iteration 700: Loss = -12336.182724559494
Iteration 800: Loss = -12336.161177754786
Iteration 900: Loss = -12336.136984575845
Iteration 1000: Loss = -12336.106905204133
Iteration 1100: Loss = -12336.066894449159
Iteration 1200: Loss = -12336.0102672579
Iteration 1300: Loss = -12335.925972090501
Iteration 1400: Loss = -12335.796232944138
Iteration 1500: Loss = -12335.59131590519
Iteration 1600: Loss = -12335.294093634273
Iteration 1700: Loss = -12335.118406823904
Iteration 1800: Loss = -12335.040449481308
Iteration 1900: Loss = -12334.991966231702
Iteration 2000: Loss = -12334.957427530182
Iteration 2100: Loss = -12334.931443044054
Iteration 2200: Loss = -12334.911244779565
Iteration 2300: Loss = -12334.89493104404
Iteration 2400: Loss = -12334.881145437697
Iteration 2500: Loss = -12334.868668184783
Iteration 2600: Loss = -12334.856195640257
Iteration 2700: Loss = -12334.841878003701
Iteration 2800: Loss = -12334.822010967126
Iteration 2900: Loss = -12334.788749406816
Iteration 3000: Loss = -12334.73937278702
Iteration 3100: Loss = -12334.704790849299
Iteration 3200: Loss = -12334.68678866617
Iteration 3300: Loss = -12334.67478341188
Iteration 3400: Loss = -12334.666561811704
Iteration 3500: Loss = -12334.660876483546
Iteration 3600: Loss = -12334.656954957052
Iteration 3700: Loss = -12334.654126243431
Iteration 3800: Loss = -12334.652090557938
Iteration 3900: Loss = -12334.650500636728
Iteration 4000: Loss = -12334.649304113746
Iteration 4100: Loss = -12334.64834280596
Iteration 4200: Loss = -12334.6475722052
Iteration 4300: Loss = -12334.646898055325
Iteration 4400: Loss = -12334.646367301291
Iteration 4500: Loss = -12334.645855464605
Iteration 4600: Loss = -12334.645461721988
Iteration 4700: Loss = -12334.645108610894
Iteration 4800: Loss = -12334.644736381857
Iteration 4900: Loss = -12334.6445027583
Iteration 5000: Loss = -12334.644265926961
Iteration 5100: Loss = -12334.644011651955
Iteration 5200: Loss = -12334.643803638419
Iteration 5300: Loss = -12334.643657567156
Iteration 5400: Loss = -12334.643534280694
Iteration 5500: Loss = -12334.64335395644
Iteration 5600: Loss = -12334.643259477445
Iteration 5700: Loss = -12334.643137759098
Iteration 5800: Loss = -12334.643027024933
Iteration 5900: Loss = -12334.642930054963
Iteration 6000: Loss = -12334.64284935199
Iteration 6100: Loss = -12334.642746592173
Iteration 6200: Loss = -12334.64266637232
Iteration 6300: Loss = -12334.642597108497
Iteration 6400: Loss = -12334.642539212173
Iteration 6500: Loss = -12334.642492462315
Iteration 6600: Loss = -12334.642419553922
Iteration 6700: Loss = -12334.642367919578
Iteration 6800: Loss = -12334.64230923551
Iteration 6900: Loss = -12334.642266157214
Iteration 7000: Loss = -12334.642439614907
1
Iteration 7100: Loss = -12334.642209493202
Iteration 7200: Loss = -12334.642161520524
Iteration 7300: Loss = -12334.64219798228
Iteration 7400: Loss = -12334.642108317541
Iteration 7500: Loss = -12334.642941942155
1
Iteration 7600: Loss = -12334.67791906064
2
Iteration 7700: Loss = -12334.650404708826
3
Iteration 7800: Loss = -12334.654696489064
4
Iteration 7900: Loss = -12334.747968452002
5
Iteration 8000: Loss = -12334.643682558568
6
Iteration 8100: Loss = -12334.64579319469
7
Iteration 8200: Loss = -12334.641926779223
Iteration 8300: Loss = -12334.64378725058
1
Iteration 8400: Loss = -12334.645132502159
2
Iteration 8500: Loss = -12334.68640558648
3
Iteration 8600: Loss = -12334.642509150563
4
Iteration 8700: Loss = -12334.641934680742
Iteration 8800: Loss = -12334.643481928817
1
Iteration 8900: Loss = -12334.643411097282
2
Iteration 9000: Loss = -12334.643265454128
3
Iteration 9100: Loss = -12334.642773005053
4
Iteration 9200: Loss = -12334.649072240207
5
Iteration 9300: Loss = -12334.641812516198
Iteration 9400: Loss = -12334.675245262226
1
Iteration 9500: Loss = -12334.736560270592
2
Iteration 9600: Loss = -12334.643706256993
3
Iteration 9700: Loss = -12334.649174143478
4
Iteration 9800: Loss = -12334.651002309496
5
Iteration 9900: Loss = -12334.642146248216
6
Iteration 10000: Loss = -12334.648577733275
7
Iteration 10100: Loss = -12334.641841612924
Iteration 10200: Loss = -12334.643882212138
1
Iteration 10300: Loss = -12334.641723605419
Iteration 10400: Loss = -12334.647346489555
1
Iteration 10500: Loss = -12334.641703533935
Iteration 10600: Loss = -12334.641918495037
1
Iteration 10700: Loss = -12334.645659406968
2
Iteration 10800: Loss = -12334.802804907475
3
Iteration 10900: Loss = -12334.643800799666
4
Iteration 11000: Loss = -12334.642234657525
5
Iteration 11100: Loss = -12334.641880670062
6
Iteration 11200: Loss = -12334.65519438344
7
Iteration 11300: Loss = -12334.641706097727
Iteration 11400: Loss = -12334.647915256912
1
Iteration 11500: Loss = -12334.642535583716
2
Iteration 11600: Loss = -12334.643978194255
3
Iteration 11700: Loss = -12334.64178785205
Iteration 11800: Loss = -12334.641829557375
Iteration 11900: Loss = -12334.66443669113
1
Iteration 12000: Loss = -12334.641703672143
Iteration 12100: Loss = -12334.644756468848
1
Iteration 12200: Loss = -12334.65837435833
2
Iteration 12300: Loss = -12334.641743037717
Iteration 12400: Loss = -12334.64240676372
1
Iteration 12500: Loss = -12334.641664177325
Iteration 12600: Loss = -12334.648263339219
1
Iteration 12700: Loss = -12334.643150600617
2
Iteration 12800: Loss = -12334.6663465711
3
Iteration 12900: Loss = -12334.649158978616
4
Iteration 13000: Loss = -12334.641986091061
5
Iteration 13100: Loss = -12334.643871740192
6
Iteration 13200: Loss = -12334.655878551079
7
Iteration 13300: Loss = -12334.660379170096
8
Iteration 13400: Loss = -12334.641851106408
9
Iteration 13500: Loss = -12334.642371866494
10
Iteration 13600: Loss = -12334.762939577771
11
Iteration 13700: Loss = -12334.641609512983
Iteration 13800: Loss = -12334.743750513928
1
Iteration 13900: Loss = -12334.641634245738
Iteration 14000: Loss = -12334.658401485985
1
Iteration 14100: Loss = -12334.641948447486
2
Iteration 14200: Loss = -12334.64163991753
Iteration 14300: Loss = -12334.641686794204
Iteration 14400: Loss = -12334.714672347161
1
Iteration 14500: Loss = -12334.641642972101
Iteration 14600: Loss = -12334.643714377584
1
Iteration 14700: Loss = -12334.641657369757
Iteration 14800: Loss = -12334.64180330849
1
Iteration 14900: Loss = -12334.705121680261
2
Iteration 15000: Loss = -12334.641716762531
Iteration 15100: Loss = -12334.645121958256
1
Iteration 15200: Loss = -12334.69975631254
2
Iteration 15300: Loss = -12334.659279796067
3
Iteration 15400: Loss = -12334.642399468017
4
Iteration 15500: Loss = -12334.641849608563
5
Iteration 15600: Loss = -12334.647326212607
6
Iteration 15700: Loss = -12334.705192015712
7
Iteration 15800: Loss = -12334.722294286703
8
Iteration 15900: Loss = -12334.642132182324
9
Iteration 16000: Loss = -12334.641841966712
10
Iteration 16100: Loss = -12334.646539791653
11
Iteration 16200: Loss = -12334.64796352904
12
Iteration 16300: Loss = -12334.642914540293
13
Iteration 16400: Loss = -12334.652129109218
14
Iteration 16500: Loss = -12334.661951004638
15
Stopping early at iteration 16500 due to no improvement.
pi: tensor([[0.9872, 0.0128],
        [0.9737, 0.0263]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0015, 0.9985], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1981, 0.2064],
         [0.5949, 0.2042]],

        [[0.5971, 0.1291],
         [0.6735, 0.6344]],

        [[0.6178, 0.1153],
         [0.7066, 0.6582]],

        [[0.6565, 0.2054],
         [0.6009, 0.5063]],

        [[0.5904, 0.2164],
         [0.7246, 0.6949]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008933065927818041
Average Adjusted Rand Index: 0.0005107695272972583
11819.877723201182
[-0.0008933065927818041, -0.0008933065927818041] [0.0005107695272972583, 0.0005107695272972583] [12334.689734668891, 12334.661951004638]
-------------------------------------
This iteration is 37
True Objective function: Loss = -11916.584939165668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23248.718542583927
Iteration 100: Loss = -12427.123724894043
Iteration 200: Loss = -12426.278775728159
Iteration 300: Loss = -12425.542634250427
Iteration 400: Loss = -12424.79299646324
Iteration 500: Loss = -12424.515945929967
Iteration 600: Loss = -12424.328385722154
Iteration 700: Loss = -12424.178592700622
Iteration 800: Loss = -12424.06692609881
Iteration 900: Loss = -12423.974893092853
Iteration 1000: Loss = -12423.895085146494
Iteration 1100: Loss = -12423.82676987632
Iteration 1200: Loss = -12423.768662584798
Iteration 1300: Loss = -12423.719808573902
Iteration 1400: Loss = -12423.67875051712
Iteration 1500: Loss = -12423.643730262129
Iteration 1600: Loss = -12423.612899905898
Iteration 1700: Loss = -12423.58466869039
Iteration 1800: Loss = -12423.55807399189
Iteration 1900: Loss = -12423.532608618014
Iteration 2000: Loss = -12423.508855768627
Iteration 2100: Loss = -12423.487981057362
Iteration 2200: Loss = -12423.47197789893
Iteration 2300: Loss = -12423.46185650022
Iteration 2400: Loss = -12423.456841181993
Iteration 2500: Loss = -12423.454675900362
Iteration 2600: Loss = -12423.45356525695
Iteration 2700: Loss = -12423.452791457763
Iteration 2800: Loss = -12423.452136931732
Iteration 2900: Loss = -12423.451543573568
Iteration 3000: Loss = -12423.45094831131
Iteration 3100: Loss = -12423.450423306545
Iteration 3200: Loss = -12423.449917661492
Iteration 3300: Loss = -12423.449477481154
Iteration 3400: Loss = -12423.449044709949
Iteration 3500: Loss = -12423.448655434046
Iteration 3600: Loss = -12423.44820614533
Iteration 3700: Loss = -12423.44782768606
Iteration 3800: Loss = -12423.447479762548
Iteration 3900: Loss = -12423.447143944783
Iteration 4000: Loss = -12423.446808562534
Iteration 4100: Loss = -12423.446508073346
Iteration 4200: Loss = -12423.446174960038
Iteration 4300: Loss = -12423.445914418977
Iteration 4400: Loss = -12423.445693578089
Iteration 4500: Loss = -12423.445408695601
Iteration 4600: Loss = -12423.445220375641
Iteration 4700: Loss = -12423.444964682787
Iteration 4800: Loss = -12423.444804201004
Iteration 4900: Loss = -12423.444572572102
Iteration 5000: Loss = -12423.444398593538
Iteration 5100: Loss = -12423.444187580935
Iteration 5200: Loss = -12423.443999328716
Iteration 5300: Loss = -12423.44388328394
Iteration 5400: Loss = -12423.443706455793
Iteration 5500: Loss = -12423.443564556266
Iteration 5600: Loss = -12423.443442485155
Iteration 5700: Loss = -12423.44329610134
Iteration 5800: Loss = -12423.44316211345
Iteration 5900: Loss = -12423.443072231967
Iteration 6000: Loss = -12423.44295165645
Iteration 6100: Loss = -12423.442849844665
Iteration 6200: Loss = -12423.44272126096
Iteration 6300: Loss = -12423.442642254255
Iteration 6400: Loss = -12423.442642324328
Iteration 6500: Loss = -12423.442462437071
Iteration 6600: Loss = -12423.442370457828
Iteration 6700: Loss = -12423.442328570842
Iteration 6800: Loss = -12423.44237741441
Iteration 6900: Loss = -12423.442157092308
Iteration 7000: Loss = -12423.442103074583
Iteration 7100: Loss = -12423.442525112478
1
Iteration 7200: Loss = -12423.441960461681
Iteration 7300: Loss = -12423.444604234604
1
Iteration 7400: Loss = -12423.44572689735
2
Iteration 7500: Loss = -12423.441825641788
Iteration 7600: Loss = -12423.442961004097
1
Iteration 7700: Loss = -12423.488460539676
2
Iteration 7800: Loss = -12423.441916816164
Iteration 7900: Loss = -12423.441670638285
Iteration 8000: Loss = -12423.668136514969
1
Iteration 8100: Loss = -12423.441550104042
Iteration 8200: Loss = -12423.441723852678
1
Iteration 8300: Loss = -12423.441542971446
Iteration 8400: Loss = -12423.441470093381
Iteration 8500: Loss = -12423.767744535206
1
Iteration 8600: Loss = -12423.441405303647
Iteration 8700: Loss = -12423.441344654935
Iteration 8800: Loss = -12423.441286905574
Iteration 8900: Loss = -12423.447443420422
1
Iteration 9000: Loss = -12423.441281710433
Iteration 9100: Loss = -12423.44122338483
Iteration 9200: Loss = -12423.441538550678
1
Iteration 9300: Loss = -12423.441171148203
Iteration 9400: Loss = -12423.441191829696
Iteration 9500: Loss = -12423.441283027792
Iteration 9600: Loss = -12423.441139710825
Iteration 9700: Loss = -12423.442848476185
1
Iteration 9800: Loss = -12423.441113367753
Iteration 9900: Loss = -12423.482731895712
1
Iteration 10000: Loss = -12423.441064232291
Iteration 10100: Loss = -12423.44105989556
Iteration 10200: Loss = -12423.494464125362
1
Iteration 10300: Loss = -12423.441031074915
Iteration 10400: Loss = -12423.44103015396
Iteration 10500: Loss = -12423.442185588288
1
Iteration 10600: Loss = -12423.44100471453
Iteration 10700: Loss = -12423.440964827316
Iteration 10800: Loss = -12423.498034656577
1
Iteration 10900: Loss = -12423.440954617261
Iteration 11000: Loss = -12423.440957388731
Iteration 11100: Loss = -12423.440972015147
Iteration 11200: Loss = -12423.441521334551
1
Iteration 11300: Loss = -12423.440951292434
Iteration 11400: Loss = -12423.445664865574
1
Iteration 11500: Loss = -12423.441037186996
Iteration 11600: Loss = -12423.441656664418
1
Iteration 11700: Loss = -12423.440918130913
Iteration 11800: Loss = -12423.444258386595
1
Iteration 11900: Loss = -12423.440906897798
Iteration 12000: Loss = -12423.440888131887
Iteration 12100: Loss = -12423.441424195404
1
Iteration 12200: Loss = -12423.44090594686
Iteration 12300: Loss = -12423.444396262465
1
Iteration 12400: Loss = -12423.440916011314
Iteration 12500: Loss = -12423.440889695683
Iteration 12600: Loss = -12423.441419568095
1
Iteration 12700: Loss = -12423.44223504553
2
Iteration 12800: Loss = -12423.441398901803
3
Iteration 12900: Loss = -12423.543985065637
4
Iteration 13000: Loss = -12423.44098051545
Iteration 13100: Loss = -12423.451619213558
1
Iteration 13200: Loss = -12423.447098736953
2
Iteration 13300: Loss = -12423.441977054394
3
Iteration 13400: Loss = -12423.440820544298
Iteration 13500: Loss = -12423.441041478312
1
Iteration 13600: Loss = -12423.459589854225
2
Iteration 13700: Loss = -12423.44178560489
3
Iteration 13800: Loss = -12423.440841887435
Iteration 13900: Loss = -12423.44114847872
1
Iteration 14000: Loss = -12423.4442694014
2
Iteration 14100: Loss = -12423.4698401003
3
Iteration 14200: Loss = -12423.440940730681
Iteration 14300: Loss = -12423.440834124021
Iteration 14400: Loss = -12423.444780573138
1
Iteration 14500: Loss = -12423.465136446039
2
Iteration 14600: Loss = -12423.440897743681
Iteration 14700: Loss = -12423.572192946893
1
Iteration 14800: Loss = -12423.440801248324
Iteration 14900: Loss = -12423.441193347202
1
Iteration 15000: Loss = -12423.458356554298
2
Iteration 15100: Loss = -12423.440827271845
Iteration 15200: Loss = -12423.444812602767
1
Iteration 15300: Loss = -12423.489424939462
2
Iteration 15400: Loss = -12423.44503747246
3
Iteration 15500: Loss = -12423.440848229322
Iteration 15600: Loss = -12423.44199541847
1
Iteration 15700: Loss = -12423.536931466886
2
Iteration 15800: Loss = -12423.440819319936
Iteration 15900: Loss = -12423.441086583758
1
Iteration 16000: Loss = -12423.441211150852
2
Iteration 16100: Loss = -12423.440864564056
Iteration 16200: Loss = -12423.458616196907
1
Iteration 16300: Loss = -12423.44079950698
Iteration 16400: Loss = -12423.440856559324
Iteration 16500: Loss = -12423.444186529163
1
Iteration 16600: Loss = -12423.455963888528
2
Iteration 16700: Loss = -12423.484684944848
3
Iteration 16800: Loss = -12423.440799256376
Iteration 16900: Loss = -12423.441661003631
1
Iteration 17000: Loss = -12423.443741848301
2
Iteration 17100: Loss = -12423.440771225416
Iteration 17200: Loss = -12423.463707119423
1
Iteration 17300: Loss = -12423.442415465433
2
Iteration 17400: Loss = -12423.451214466071
3
Iteration 17500: Loss = -12423.442705667494
4
Iteration 17600: Loss = -12423.441092856589
5
Iteration 17700: Loss = -12423.440775410532
Iteration 17800: Loss = -12423.441012138885
1
Iteration 17900: Loss = -12423.44097236148
2
Iteration 18000: Loss = -12423.441267089092
3
Iteration 18100: Loss = -12423.634418646707
4
Iteration 18200: Loss = -12423.440817886452
Iteration 18300: Loss = -12423.44266852034
1
Iteration 18400: Loss = -12423.440815370432
Iteration 18500: Loss = -12423.440868607553
Iteration 18600: Loss = -12423.441059512097
1
Iteration 18700: Loss = -12423.44081143591
Iteration 18800: Loss = -12423.446141741748
1
Iteration 18900: Loss = -12423.440776655132
Iteration 19000: Loss = -12423.78222673504
1
Iteration 19100: Loss = -12423.440806869403
Iteration 19200: Loss = -12423.451090905306
1
Iteration 19300: Loss = -12423.44082601755
Iteration 19400: Loss = -12423.440800102448
Iteration 19500: Loss = -12423.472926390723
1
Iteration 19600: Loss = -12423.440806103188
Iteration 19700: Loss = -12423.440957774152
1
Iteration 19800: Loss = -12423.441961682096
2
Iteration 19900: Loss = -12423.440933702137
3
pi: tensor([[4.1134e-06, 1.0000e+00],
        [2.6828e-02, 9.7317e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0726, 0.9274], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1405, 0.1604],
         [0.5847, 0.2020]],

        [[0.5146, 0.1700],
         [0.6285, 0.5136]],

        [[0.6583, 0.2352],
         [0.5094, 0.6359]],

        [[0.6919, 0.1505],
         [0.5854, 0.6352]],

        [[0.6077, 0.3253],
         [0.5982, 0.6289]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
Global Adjusted Rand Index: -0.0005239263250504326
Average Adjusted Rand Index: -0.0007444228734768356
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24253.5669938135
Iteration 100: Loss = -12426.460294629966
Iteration 200: Loss = -12425.31365683798
Iteration 300: Loss = -12424.9668170864
Iteration 400: Loss = -12424.731299349283
Iteration 500: Loss = -12424.569229756393
Iteration 600: Loss = -12424.437199937429
Iteration 700: Loss = -12424.32929792688
Iteration 800: Loss = -12424.23935783409
Iteration 900: Loss = -12424.160773234787
Iteration 1000: Loss = -12424.086781079195
Iteration 1100: Loss = -12424.011134556855
Iteration 1200: Loss = -12423.92982402993
Iteration 1300: Loss = -12423.849484805203
Iteration 1400: Loss = -12423.783788996017
Iteration 1500: Loss = -12423.731522582715
Iteration 1600: Loss = -12423.688920055489
Iteration 1700: Loss = -12423.653539413064
Iteration 1800: Loss = -12423.622973228239
Iteration 1900: Loss = -12423.595666821187
Iteration 2000: Loss = -12423.570195929433
Iteration 2100: Loss = -12423.546004377074
Iteration 2200: Loss = -12423.52321804852
Iteration 2300: Loss = -12423.502553285402
Iteration 2400: Loss = -12423.485514845835
Iteration 2500: Loss = -12423.473304185862
Iteration 2600: Loss = -12423.466060421213
Iteration 2700: Loss = -12423.462317959373
Iteration 2800: Loss = -12423.460310987004
Iteration 2900: Loss = -12423.458952226232
Iteration 3000: Loss = -12423.457857140489
Iteration 3100: Loss = -12423.45689556094
Iteration 3200: Loss = -12423.455967696042
Iteration 3300: Loss = -12423.45509816132
Iteration 3400: Loss = -12423.454304567204
Iteration 3500: Loss = -12423.453546426488
Iteration 3600: Loss = -12423.452851920805
Iteration 3700: Loss = -12423.452204391882
Iteration 3800: Loss = -12423.451595700977
Iteration 3900: Loss = -12423.450999657978
Iteration 4000: Loss = -12423.450438708755
Iteration 4100: Loss = -12423.4499512091
Iteration 4200: Loss = -12423.449454878499
Iteration 4300: Loss = -12423.448972979468
Iteration 4400: Loss = -12423.448538874865
Iteration 4500: Loss = -12423.4481029226
Iteration 4600: Loss = -12423.447750720901
Iteration 4700: Loss = -12423.447360279742
Iteration 4800: Loss = -12423.447030514373
Iteration 4900: Loss = -12423.446713778196
Iteration 5000: Loss = -12423.44636497833
Iteration 5100: Loss = -12423.446117996944
Iteration 5200: Loss = -12423.445821347255
Iteration 5300: Loss = -12423.445552280169
Iteration 5400: Loss = -12423.445266540937
Iteration 5500: Loss = -12423.445054510645
Iteration 5600: Loss = -12423.444848738922
Iteration 5700: Loss = -12423.44466203184
Iteration 5800: Loss = -12423.444423176841
Iteration 5900: Loss = -12423.444226442927
Iteration 6000: Loss = -12423.444241090374
Iteration 6100: Loss = -12423.443917144334
Iteration 6200: Loss = -12423.443767354176
Iteration 6300: Loss = -12423.443606982075
Iteration 6400: Loss = -12423.4434620586
Iteration 6500: Loss = -12423.443454684882
Iteration 6600: Loss = -12423.443255005726
Iteration 6700: Loss = -12423.44309109867
Iteration 6800: Loss = -12423.442998775798
Iteration 6900: Loss = -12423.44285408069
Iteration 7000: Loss = -12423.442745031267
Iteration 7100: Loss = -12423.443505685598
1
Iteration 7200: Loss = -12423.443499034183
2
Iteration 7300: Loss = -12423.442477690272
Iteration 7400: Loss = -12423.442384921183
Iteration 7500: Loss = -12423.443653830513
1
Iteration 7600: Loss = -12423.442256020935
Iteration 7700: Loss = -12423.442616313252
1
Iteration 7800: Loss = -12423.442180445654
Iteration 7900: Loss = -12423.526839615524
1
Iteration 8000: Loss = -12423.44918614159
2
Iteration 8100: Loss = -12423.55472057376
3
Iteration 8200: Loss = -12423.441849449062
Iteration 8300: Loss = -12423.44777851667
1
Iteration 8400: Loss = -12423.441734944705
Iteration 8500: Loss = -12423.443447218682
1
Iteration 8600: Loss = -12423.441665733739
Iteration 8700: Loss = -12423.441640389716
Iteration 8800: Loss = -12423.442700629535
1
Iteration 8900: Loss = -12423.441565721634
Iteration 9000: Loss = -12423.441502295627
Iteration 9100: Loss = -12423.442124787212
1
Iteration 9200: Loss = -12423.441441726927
Iteration 9300: Loss = -12423.441392853681
Iteration 9400: Loss = -12423.441892662697
1
Iteration 9500: Loss = -12423.441341246316
Iteration 9600: Loss = -12423.441331985568
Iteration 9700: Loss = -12423.442349258923
1
Iteration 9800: Loss = -12423.441265755737
Iteration 9900: Loss = -12423.441231990042
Iteration 10000: Loss = -12423.441589621876
1
Iteration 10100: Loss = -12423.441212658232
Iteration 10200: Loss = -12423.441209036273
Iteration 10300: Loss = -12423.441914313704
1
Iteration 10400: Loss = -12423.441115549776
Iteration 10500: Loss = -12423.441108650022
Iteration 10600: Loss = -12423.442691350307
1
Iteration 10700: Loss = -12423.441062331909
Iteration 10800: Loss = -12423.44109717694
Iteration 10900: Loss = -12423.44745281893
1
Iteration 11000: Loss = -12423.441020556278
Iteration 11100: Loss = -12423.441052869357
Iteration 11200: Loss = -12423.441990365356
1
Iteration 11300: Loss = -12423.440993244327
Iteration 11400: Loss = -12423.555287517689
1
Iteration 11500: Loss = -12423.440983426068
Iteration 11600: Loss = -12423.440964394256
Iteration 11700: Loss = -12423.450867638938
1
Iteration 11800: Loss = -12423.440975370972
Iteration 11900: Loss = -12423.444241222158
1
Iteration 12000: Loss = -12423.441142334043
2
Iteration 12100: Loss = -12423.440990434246
Iteration 12200: Loss = -12423.440925219527
Iteration 12300: Loss = -12423.44210252748
1
Iteration 12400: Loss = -12423.441076659632
2
Iteration 12500: Loss = -12423.441162842684
3
Iteration 12600: Loss = -12423.474780104621
4
Iteration 12700: Loss = -12423.440903458959
Iteration 12800: Loss = -12423.444642067487
1
Iteration 12900: Loss = -12423.440908849503
Iteration 13000: Loss = -12423.441360401932
1
Iteration 13100: Loss = -12423.441049835827
2
Iteration 13200: Loss = -12423.44091286854
Iteration 13300: Loss = -12423.482896347536
1
Iteration 13400: Loss = -12423.440875033317
Iteration 13500: Loss = -12423.458689033185
1
Iteration 13600: Loss = -12423.440854650327
Iteration 13700: Loss = -12423.440854209079
Iteration 13800: Loss = -12423.441411928477
1
Iteration 13900: Loss = -12423.440839992634
Iteration 14000: Loss = -12423.455349405698
1
Iteration 14100: Loss = -12423.44087204475
Iteration 14200: Loss = -12423.44084669646
Iteration 14300: Loss = -12423.441114670966
1
Iteration 14400: Loss = -12423.440870635926
Iteration 14500: Loss = -12423.440876825807
Iteration 14600: Loss = -12423.440826992883
Iteration 14700: Loss = -12423.441617114177
1
Iteration 14800: Loss = -12423.440836950698
Iteration 14900: Loss = -12423.443051372933
1
Iteration 15000: Loss = -12423.440807275227
Iteration 15100: Loss = -12423.440827090692
Iteration 15200: Loss = -12423.440945410477
1
Iteration 15300: Loss = -12423.442814407623
2
Iteration 15400: Loss = -12423.44082952874
Iteration 15500: Loss = -12423.481522880555
1
Iteration 15600: Loss = -12423.453142891258
2
Iteration 15700: Loss = -12423.44205066686
3
Iteration 15800: Loss = -12423.454154772926
4
Iteration 15900: Loss = -12423.447963299028
5
Iteration 16000: Loss = -12423.44086082367
Iteration 16100: Loss = -12423.44090723999
Iteration 16200: Loss = -12423.441746852593
1
Iteration 16300: Loss = -12423.44346612222
2
Iteration 16400: Loss = -12423.443530344653
3
Iteration 16500: Loss = -12423.440904049865
Iteration 16600: Loss = -12423.441689403418
1
Iteration 16700: Loss = -12423.4411608592
2
Iteration 16800: Loss = -12423.440882261308
Iteration 16900: Loss = -12423.457944801436
1
Iteration 17000: Loss = -12423.440834985868
Iteration 17100: Loss = -12423.44253080287
1
Iteration 17200: Loss = -12423.443021705325
2
Iteration 17300: Loss = -12423.440835642317
Iteration 17400: Loss = -12423.441054870285
1
Iteration 17500: Loss = -12423.639604545006
2
Iteration 17600: Loss = -12423.44078977832
Iteration 17700: Loss = -12423.440884423297
Iteration 17800: Loss = -12423.467837921806
1
Iteration 17900: Loss = -12423.440801300767
Iteration 18000: Loss = -12423.442975631093
1
Iteration 18100: Loss = -12423.440860581975
Iteration 18200: Loss = -12423.768203759226
1
Iteration 18300: Loss = -12423.440832568866
Iteration 18400: Loss = -12423.443360043906
1
Iteration 18500: Loss = -12423.442486338477
2
Iteration 18600: Loss = -12423.440963160403
3
Iteration 18700: Loss = -12423.451460439675
4
Iteration 18800: Loss = -12423.440786263436
Iteration 18900: Loss = -12423.442374284165
1
Iteration 19000: Loss = -12423.440784419534
Iteration 19100: Loss = -12423.445912774972
1
Iteration 19200: Loss = -12423.44453123426
2
Iteration 19300: Loss = -12423.488682045963
3
Iteration 19400: Loss = -12423.441075094861
4
Iteration 19500: Loss = -12423.440819065583
Iteration 19600: Loss = -12423.449403000302
1
Iteration 19700: Loss = -12423.452219240386
2
Iteration 19800: Loss = -12423.469808174888
3
Iteration 19900: Loss = -12423.521589083437
4
pi: tensor([[6.1033e-06, 9.9999e-01],
        [2.6841e-02, 9.7316e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0727, 0.9273], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1402, 0.1597],
         [0.6538, 0.2024]],

        [[0.5658, 0.1697],
         [0.5125, 0.5042]],

        [[0.7089, 0.2348],
         [0.6604, 0.5888]],

        [[0.7290, 0.1503],
         [0.6218, 0.6655]],

        [[0.5424, 0.3256],
         [0.5532, 0.6256]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
Global Adjusted Rand Index: -0.0005239263250504326
Average Adjusted Rand Index: -0.0007444228734768356
11916.584939165668
[-0.0005239263250504326, -0.0005239263250504326] [-0.0007444228734768356, -0.0007444228734768356] [12423.440883187639, 12423.452428848157]
-------------------------------------
This iteration is 38
True Objective function: Loss = -11877.572974129685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20820.091757953433
Iteration 100: Loss = -12374.724557183648
Iteration 200: Loss = -12373.964593070357
Iteration 300: Loss = -12373.375436997589
Iteration 400: Loss = -12371.597285017791
Iteration 500: Loss = -12370.597889693352
Iteration 600: Loss = -12370.124448907105
Iteration 700: Loss = -12369.705931529867
Iteration 800: Loss = -12369.264443174285
Iteration 900: Loss = -12368.923670538956
Iteration 1000: Loss = -12368.544475409408
Iteration 1100: Loss = -12368.27136652682
Iteration 1200: Loss = -12368.125073250745
Iteration 1300: Loss = -12368.012340542684
Iteration 1400: Loss = -12367.921672998076
Iteration 1500: Loss = -12367.851851454276
Iteration 1600: Loss = -12367.801100398077
Iteration 1700: Loss = -12367.762153349107
Iteration 1800: Loss = -12367.730058841467
Iteration 1900: Loss = -12367.703044080767
Iteration 2000: Loss = -12367.68123450023
Iteration 2100: Loss = -12367.663850806575
Iteration 2200: Loss = -12367.649896117868
Iteration 2300: Loss = -12367.638472399805
Iteration 2400: Loss = -12367.628618295108
Iteration 2500: Loss = -12367.62007252809
Iteration 2600: Loss = -12367.612775791378
Iteration 2700: Loss = -12367.606342483612
Iteration 2800: Loss = -12367.60037707432
Iteration 2900: Loss = -12367.594782403008
Iteration 3000: Loss = -12367.589724442118
Iteration 3100: Loss = -12367.585012627864
Iteration 3200: Loss = -12367.580109177561
Iteration 3300: Loss = -12367.573261311685
Iteration 3400: Loss = -12367.559452854464
Iteration 3500: Loss = -12367.547823638237
Iteration 3600: Loss = -12367.544467267366
Iteration 3700: Loss = -12367.542261484083
Iteration 3800: Loss = -12367.54053249961
Iteration 3900: Loss = -12367.539132036494
Iteration 4000: Loss = -12367.537875631038
Iteration 4100: Loss = -12367.536809313951
Iteration 4200: Loss = -12367.535875582458
Iteration 4300: Loss = -12367.535036351357
Iteration 4400: Loss = -12367.534298446619
Iteration 4500: Loss = -12367.533576836535
Iteration 4600: Loss = -12367.532943454717
Iteration 4700: Loss = -12367.532371671876
Iteration 4800: Loss = -12367.531861167283
Iteration 4900: Loss = -12367.531361667207
Iteration 5000: Loss = -12367.53091420452
Iteration 5100: Loss = -12367.530528301728
Iteration 5200: Loss = -12367.530154832968
Iteration 5300: Loss = -12367.529819550311
Iteration 5400: Loss = -12367.52949537105
Iteration 5500: Loss = -12367.529212425276
Iteration 5600: Loss = -12367.52896465581
Iteration 5700: Loss = -12367.528751278203
Iteration 5800: Loss = -12367.528542530375
Iteration 5900: Loss = -12367.528325910938
Iteration 6000: Loss = -12367.528115693134
Iteration 6100: Loss = -12367.527950972964
Iteration 6200: Loss = -12367.527778196016
Iteration 6300: Loss = -12367.527676199332
Iteration 6400: Loss = -12367.527505119504
Iteration 6500: Loss = -12367.527353344094
Iteration 6600: Loss = -12367.52725498657
Iteration 6700: Loss = -12367.527155447291
Iteration 6800: Loss = -12367.527007889905
Iteration 6900: Loss = -12367.526897392327
Iteration 7000: Loss = -12367.526808658164
Iteration 7100: Loss = -12367.526739850864
Iteration 7200: Loss = -12367.526650155545
Iteration 7300: Loss = -12367.526576653221
Iteration 7400: Loss = -12367.526516959573
Iteration 7500: Loss = -12367.526440271007
Iteration 7600: Loss = -12367.526664195331
1
Iteration 7700: Loss = -12367.526681255813
2
Iteration 7800: Loss = -12367.542751061414
3
Iteration 7900: Loss = -12367.5261421615
Iteration 8000: Loss = -12367.52638115841
1
Iteration 8100: Loss = -12367.526016550784
Iteration 8200: Loss = -12367.526025266234
Iteration 8300: Loss = -12367.525956880423
Iteration 8400: Loss = -12367.807423303655
1
Iteration 8500: Loss = -12367.525937319826
Iteration 8600: Loss = -12367.525859270208
Iteration 8700: Loss = -12367.543845804443
1
Iteration 8800: Loss = -12367.52576551331
Iteration 8900: Loss = -12367.52574616721
Iteration 9000: Loss = -12367.546969310853
1
Iteration 9100: Loss = -12367.525709763377
Iteration 9200: Loss = -12367.52567233271
Iteration 9300: Loss = -12367.525645615333
Iteration 9400: Loss = -12367.525759948361
1
Iteration 9500: Loss = -12367.525624418848
Iteration 9600: Loss = -12367.52587411142
1
Iteration 9700: Loss = -12367.525649785253
Iteration 9800: Loss = -12367.525577293905
Iteration 9900: Loss = -12367.525718512208
1
Iteration 10000: Loss = -12367.525526059277
Iteration 10100: Loss = -12367.52550747888
Iteration 10200: Loss = -12367.627737662915
1
Iteration 10300: Loss = -12367.52547456026
Iteration 10400: Loss = -12367.525422946788
Iteration 10500: Loss = -12367.568009435605
1
Iteration 10600: Loss = -12367.5254638937
Iteration 10700: Loss = -12367.525430981308
Iteration 10800: Loss = -12367.98912873819
1
Iteration 10900: Loss = -12367.525386727759
Iteration 11000: Loss = -12367.525432400475
Iteration 11100: Loss = -12367.547851520285
1
Iteration 11200: Loss = -12367.525390562152
Iteration 11300: Loss = -12367.52535334843
Iteration 11400: Loss = -12367.528183260714
1
Iteration 11500: Loss = -12367.525341322615
Iteration 11600: Loss = -12367.525350361819
Iteration 11700: Loss = -12367.525919745833
1
Iteration 11800: Loss = -12367.52532374806
Iteration 11900: Loss = -12367.525345689814
Iteration 12000: Loss = -12367.525330872817
Iteration 12100: Loss = -12367.525369764755
Iteration 12200: Loss = -12367.568952143914
1
Iteration 12300: Loss = -12367.525287278231
Iteration 12400: Loss = -12367.525264587162
Iteration 12500: Loss = -12367.525311407533
Iteration 12600: Loss = -12367.526010189424
1
Iteration 12700: Loss = -12367.525297295013
Iteration 12800: Loss = -12367.845884919414
1
Iteration 12900: Loss = -12367.525266325129
Iteration 13000: Loss = -12367.577392904966
1
Iteration 13100: Loss = -12367.525285498547
Iteration 13200: Loss = -12367.531445849241
1
Iteration 13300: Loss = -12367.525263690719
Iteration 13400: Loss = -12367.529048291657
1
Iteration 13500: Loss = -12367.534640574942
2
Iteration 13600: Loss = -12367.525247143905
Iteration 13700: Loss = -12367.525283008363
Iteration 13800: Loss = -12367.532573667462
1
Iteration 13900: Loss = -12367.52524017501
Iteration 14000: Loss = -12367.526052640062
1
Iteration 14100: Loss = -12367.525507200175
2
Iteration 14200: Loss = -12367.525379732831
3
Iteration 14300: Loss = -12367.5252818441
Iteration 14400: Loss = -12367.525390195238
1
Iteration 14500: Loss = -12367.52534228445
Iteration 14600: Loss = -12367.525316464551
Iteration 14700: Loss = -12367.554337881513
1
Iteration 14800: Loss = -12367.525250491964
Iteration 14900: Loss = -12367.529608157154
1
Iteration 15000: Loss = -12367.525255560111
Iteration 15100: Loss = -12367.525516120208
1
Iteration 15200: Loss = -12367.526569488093
2
Iteration 15300: Loss = -12367.526165468309
3
Iteration 15400: Loss = -12367.530782881644
4
Iteration 15500: Loss = -12367.525309525903
Iteration 15600: Loss = -12367.525441554575
1
Iteration 15700: Loss = -12367.52528015101
Iteration 15800: Loss = -12367.52528887549
Iteration 15900: Loss = -12367.525212912149
Iteration 16000: Loss = -12367.52567598799
1
Iteration 16100: Loss = -12367.525251221885
Iteration 16200: Loss = -12367.54990567715
1
Iteration 16300: Loss = -12367.525267920388
Iteration 16400: Loss = -12367.601481269121
1
Iteration 16500: Loss = -12367.525214455254
Iteration 16600: Loss = -12367.525620954524
1
Iteration 16700: Loss = -12367.525270973616
Iteration 16800: Loss = -12367.52963124724
1
Iteration 16900: Loss = -12367.525318620985
Iteration 17000: Loss = -12367.525298691076
Iteration 17100: Loss = -12367.534655752268
1
Iteration 17200: Loss = -12367.526361064383
2
Iteration 17300: Loss = -12367.769287840105
3
Iteration 17400: Loss = -12367.525307098402
Iteration 17500: Loss = -12367.55208803633
1
Iteration 17600: Loss = -12367.525200684786
Iteration 17700: Loss = -12367.526633065383
1
Iteration 17800: Loss = -12367.525195596303
Iteration 17900: Loss = -12367.525486540726
1
Iteration 18000: Loss = -12367.52522188857
Iteration 18100: Loss = -12367.525398771584
1
Iteration 18200: Loss = -12367.525821993382
2
Iteration 18300: Loss = -12367.525727393388
3
Iteration 18400: Loss = -12367.525482939078
4
Iteration 18500: Loss = -12367.571893657014
5
Iteration 18600: Loss = -12367.525247786303
Iteration 18700: Loss = -12367.565554907436
1
Iteration 18800: Loss = -12367.525225018742
Iteration 18900: Loss = -12367.525292578166
Iteration 19000: Loss = -12367.528149631804
1
Iteration 19100: Loss = -12367.536437110351
2
Iteration 19200: Loss = -12367.53115533013
3
Iteration 19300: Loss = -12367.525481004599
4
Iteration 19400: Loss = -12367.525263483501
Iteration 19500: Loss = -12367.525232703623
Iteration 19600: Loss = -12367.526666233107
1
Iteration 19700: Loss = -12367.526300887897
2
Iteration 19800: Loss = -12367.525368302064
3
Iteration 19900: Loss = -12367.52566221525
4
pi: tensor([[1.0000e+00, 5.0103e-07],
        [7.2386e-03, 9.9276e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0539, 0.9461], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2674, 0.2704],
         [0.6781, 0.1954]],

        [[0.5451, 0.1731],
         [0.6101, 0.5320]],

        [[0.5779, 0.1946],
         [0.5665, 0.5331]],

        [[0.5681, 0.2290],
         [0.6865, 0.5247]],

        [[0.6039, 0.2617],
         [0.7283, 0.5723]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.021712907117008445
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.003243945514707375
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.0140971543785986
Global Adjusted Rand Index: -0.009632693382530698
Average Adjusted Rand Index: -0.010074072344572981
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21086.051744479442
Iteration 100: Loss = -12207.330045336972
Iteration 200: Loss = -11872.420770317243
Iteration 300: Loss = -11871.996372531206
Iteration 400: Loss = -11871.838530792047
Iteration 500: Loss = -11871.742135196999
Iteration 600: Loss = -11871.697161321292
Iteration 700: Loss = -11871.668181514491
Iteration 800: Loss = -11871.648023433894
Iteration 900: Loss = -11871.63345071114
Iteration 1000: Loss = -11871.622488787623
Iteration 1100: Loss = -11871.613993520463
Iteration 1200: Loss = -11871.607364404552
Iteration 1300: Loss = -11871.60200740799
Iteration 1400: Loss = -11871.59760561224
Iteration 1500: Loss = -11871.593968845194
Iteration 1600: Loss = -11871.590925924373
Iteration 1700: Loss = -11871.588306983731
Iteration 1800: Loss = -11871.586114361775
Iteration 1900: Loss = -11871.58415224128
Iteration 2000: Loss = -11871.582463569408
Iteration 2100: Loss = -11871.581059321328
Iteration 2200: Loss = -11871.579760569279
Iteration 2300: Loss = -11871.578585414247
Iteration 2400: Loss = -11871.577557644123
Iteration 2500: Loss = -11871.576668030819
Iteration 2600: Loss = -11871.575851169391
Iteration 2700: Loss = -11871.575156949893
Iteration 2800: Loss = -11871.57454950956
Iteration 2900: Loss = -11871.574399932064
Iteration 3000: Loss = -11871.57341293714
Iteration 3100: Loss = -11871.573189803796
Iteration 3200: Loss = -11871.573168309307
Iteration 3300: Loss = -11871.572172837785
Iteration 3400: Loss = -11871.57177271393
Iteration 3500: Loss = -11871.57173720234
Iteration 3600: Loss = -11871.571353204336
Iteration 3700: Loss = -11871.570897152837
Iteration 3800: Loss = -11871.570914298627
Iteration 3900: Loss = -11871.57038216122
Iteration 4000: Loss = -11871.570151472468
Iteration 4100: Loss = -11871.56998912648
Iteration 4200: Loss = -11871.569942080887
Iteration 4300: Loss = -11871.569626334172
Iteration 4400: Loss = -11871.570068352434
1
Iteration 4500: Loss = -11871.571095864703
2
Iteration 4600: Loss = -11871.578183632102
3
Iteration 4700: Loss = -11871.56902428813
Iteration 4800: Loss = -11871.57158518369
1
Iteration 4900: Loss = -11871.56908374182
Iteration 5000: Loss = -11871.571832259438
1
Iteration 5100: Loss = -11871.57829849005
2
Iteration 5200: Loss = -11871.56853516061
Iteration 5300: Loss = -11871.568436641468
Iteration 5400: Loss = -11871.584373707987
1
Iteration 5500: Loss = -11871.56825576562
Iteration 5600: Loss = -11871.580926084784
1
Iteration 5700: Loss = -11871.568081623813
Iteration 5800: Loss = -11871.572813606943
1
Iteration 5900: Loss = -11871.568911969021
2
Iteration 6000: Loss = -11871.567924747209
Iteration 6100: Loss = -11871.567857655493
Iteration 6200: Loss = -11871.581251487667
1
Iteration 6300: Loss = -11871.567742502142
Iteration 6400: Loss = -11871.56771150688
Iteration 6500: Loss = -11871.56768118025
Iteration 6600: Loss = -11871.567633896699
Iteration 6700: Loss = -11871.567853677143
1
Iteration 6800: Loss = -11871.567546224638
Iteration 6900: Loss = -11871.56747742629
Iteration 7000: Loss = -11871.567462904284
Iteration 7100: Loss = -11871.567861605967
1
Iteration 7200: Loss = -11871.56742133959
Iteration 7300: Loss = -11871.567411195596
Iteration 7400: Loss = -11871.568351223512
1
Iteration 7500: Loss = -11871.567343330109
Iteration 7600: Loss = -11871.567333465686
Iteration 7700: Loss = -11871.56731609039
Iteration 7800: Loss = -11871.567319554242
Iteration 7900: Loss = -11871.567266061344
Iteration 8000: Loss = -11871.568225934121
1
Iteration 8100: Loss = -11871.567230626802
Iteration 8200: Loss = -11871.567406388196
1
Iteration 8300: Loss = -11871.567227033938
Iteration 8400: Loss = -11871.56810376763
1
Iteration 8500: Loss = -11871.56720691948
Iteration 8600: Loss = -11871.580866325297
1
Iteration 8700: Loss = -11871.569599934954
2
Iteration 8800: Loss = -11871.579266543771
3
Iteration 8900: Loss = -11871.568012657983
4
Iteration 9000: Loss = -11871.567261490703
Iteration 9100: Loss = -11871.569085170793
1
Iteration 9200: Loss = -11871.56836189056
2
Iteration 9300: Loss = -11871.567252220575
Iteration 9400: Loss = -11871.585819816692
1
Iteration 9500: Loss = -11871.567283575165
Iteration 9600: Loss = -11871.578870634456
1
Iteration 9700: Loss = -11871.57380689548
2
Iteration 9800: Loss = -11871.610999646535
3
Iteration 9900: Loss = -11871.56804091169
4
Iteration 10000: Loss = -11871.573187132768
5
Iteration 10100: Loss = -11871.571365097077
6
Iteration 10200: Loss = -11871.56815046683
7
Iteration 10300: Loss = -11871.56873770266
8
Iteration 10400: Loss = -11871.570198618627
9
Iteration 10500: Loss = -11871.578441847736
10
Iteration 10600: Loss = -11871.569260848082
11
Iteration 10700: Loss = -11871.568234412562
12
Iteration 10800: Loss = -11871.575442440786
13
Iteration 10900: Loss = -11871.567730417757
14
Iteration 11000: Loss = -11871.575731306872
15
Stopping early at iteration 11000 due to no improvement.
pi: tensor([[0.7678, 0.2322],
        [0.2810, 0.7190]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6054, 0.3946], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2981, 0.0978],
         [0.6515, 0.2973]],

        [[0.6541, 0.0978],
         [0.5206, 0.7109]],

        [[0.5749, 0.0948],
         [0.6329, 0.5058]],

        [[0.6474, 0.1071],
         [0.6143, 0.6888]],

        [[0.6354, 0.1016],
         [0.5621, 0.5702]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919986675553487
Average Adjusted Rand Index: 0.9919905858125253
11877.572974129685
[-0.009632693382530698, 0.9919986675553487] [-0.010074072344572981, 0.9919905858125253] [12367.525292407898, 11871.575731306872]
-------------------------------------
This iteration is 39
True Objective function: Loss = -11740.030858799195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19585.30358299948
Iteration 100: Loss = -12287.805696677387
Iteration 200: Loss = -12287.259912403308
Iteration 300: Loss = -12287.050680050366
Iteration 400: Loss = -12286.726547505663
Iteration 500: Loss = -12286.538268863722
Iteration 600: Loss = -12286.382655654232
Iteration 700: Loss = -12286.226208581398
Iteration 800: Loss = -12286.067952995963
Iteration 900: Loss = -12285.893773715798
Iteration 1000: Loss = -12285.723163665643
Iteration 1100: Loss = -12285.554249873661
Iteration 1200: Loss = -12285.28472731141
Iteration 1300: Loss = -12284.964024992918
Iteration 1400: Loss = -12284.836728836755
Iteration 1500: Loss = -12284.767778618072
Iteration 1600: Loss = -12284.715646247198
Iteration 1700: Loss = -12284.665755772112
Iteration 1800: Loss = -12284.609129933133
Iteration 1900: Loss = -12284.538512141984
Iteration 2000: Loss = -12284.455090715617
Iteration 2100: Loss = -12284.384533631317
Iteration 2200: Loss = -12284.348139102907
Iteration 2300: Loss = -12284.332402137294
Iteration 2400: Loss = -12284.32349443498
Iteration 2500: Loss = -12284.317231695311
Iteration 2600: Loss = -12284.31237801838
Iteration 2700: Loss = -12284.30850380489
Iteration 2800: Loss = -12284.305247589979
Iteration 2900: Loss = -12284.302579243355
Iteration 3000: Loss = -12284.300227121888
Iteration 3100: Loss = -12284.298239837563
Iteration 3200: Loss = -12284.29646421748
Iteration 3300: Loss = -12284.294891901582
Iteration 3400: Loss = -12284.293527369187
Iteration 3500: Loss = -12284.292281568094
Iteration 3600: Loss = -12284.291145186566
Iteration 3700: Loss = -12284.290132264863
Iteration 3800: Loss = -12284.289245228994
Iteration 3900: Loss = -12284.288401425982
Iteration 4000: Loss = -12284.287697180687
Iteration 4100: Loss = -12284.287030893489
Iteration 4200: Loss = -12284.2863370268
Iteration 4300: Loss = -12284.285798389667
Iteration 4400: Loss = -12284.285246563917
Iteration 4500: Loss = -12284.28474883485
Iteration 4600: Loss = -12284.284273690091
Iteration 4700: Loss = -12284.283890643203
Iteration 4800: Loss = -12284.283533377426
Iteration 4900: Loss = -12284.283132521497
Iteration 5000: Loss = -12284.282841976821
Iteration 5100: Loss = -12284.282480213966
Iteration 5200: Loss = -12284.282213698658
Iteration 5300: Loss = -12284.282001554302
Iteration 5400: Loss = -12284.281660129915
Iteration 5500: Loss = -12284.281427087793
Iteration 5600: Loss = -12284.281208177334
Iteration 5700: Loss = -12284.281014607604
Iteration 5800: Loss = -12284.280998686108
Iteration 5900: Loss = -12284.280623288982
Iteration 6000: Loss = -12284.281593330097
1
Iteration 6100: Loss = -12284.280296175539
Iteration 6200: Loss = -12284.280123557995
Iteration 6300: Loss = -12284.280034046109
Iteration 6400: Loss = -12284.279865379785
Iteration 6500: Loss = -12284.2798850321
Iteration 6600: Loss = -12284.279620654157
Iteration 6700: Loss = -12284.279540361444
Iteration 6800: Loss = -12284.279453177614
Iteration 6900: Loss = -12284.279320975236
Iteration 7000: Loss = -12284.281262995351
1
Iteration 7100: Loss = -12284.279140568182
Iteration 7200: Loss = -12284.288289230184
1
Iteration 7300: Loss = -12284.279083088975
Iteration 7400: Loss = -12284.278942776979
Iteration 7500: Loss = -12284.28481460786
1
Iteration 7600: Loss = -12284.278783168573
Iteration 7700: Loss = -12284.278882001388
Iteration 7800: Loss = -12284.278676360162
Iteration 7900: Loss = -12284.27864735411
Iteration 8000: Loss = -12284.278593226427
Iteration 8100: Loss = -12284.27851344387
Iteration 8200: Loss = -12284.278588884044
Iteration 8300: Loss = -12284.27851107612
Iteration 8400: Loss = -12284.350886915296
1
Iteration 8500: Loss = -12284.278392419814
Iteration 8600: Loss = -12284.308743581098
1
Iteration 8700: Loss = -12284.278310349891
Iteration 8800: Loss = -12284.278237699451
Iteration 8900: Loss = -12284.278384535519
1
Iteration 9000: Loss = -12284.27816933319
Iteration 9100: Loss = -12284.350026121976
1
Iteration 9200: Loss = -12284.278156755321
Iteration 9300: Loss = -12284.278153850199
Iteration 9400: Loss = -12284.28053760363
1
Iteration 9500: Loss = -12284.278071550854
Iteration 9600: Loss = -12284.278023594501
Iteration 9700: Loss = -12284.278084585369
Iteration 9800: Loss = -12284.278009538122
Iteration 9900: Loss = -12284.277992547404
Iteration 10000: Loss = -12284.27808802872
Iteration 10100: Loss = -12284.27804966963
Iteration 10200: Loss = -12284.277922698107
Iteration 10300: Loss = -12284.278229246072
1
Iteration 10400: Loss = -12284.27793695516
Iteration 10500: Loss = -12284.289184175272
1
Iteration 10600: Loss = -12284.277879698804
Iteration 10700: Loss = -12284.426503930026
1
Iteration 10800: Loss = -12284.277866344157
Iteration 10900: Loss = -12284.2795171265
1
Iteration 11000: Loss = -12284.281402693368
2
Iteration 11100: Loss = -12284.27801038517
3
Iteration 11200: Loss = -12284.278417848265
4
Iteration 11300: Loss = -12284.290243740334
5
Iteration 11400: Loss = -12284.27780815702
Iteration 11500: Loss = -12284.277977486065
1
Iteration 11600: Loss = -12284.286160519145
2
Iteration 11700: Loss = -12284.277811025233
Iteration 11800: Loss = -12284.279359119253
1
Iteration 11900: Loss = -12284.278263023438
2
Iteration 12000: Loss = -12284.279506435354
3
Iteration 12100: Loss = -12284.277903903652
Iteration 12200: Loss = -12284.277969422395
Iteration 12300: Loss = -12284.312329257484
1
Iteration 12400: Loss = -12284.513329358608
2
Iteration 12500: Loss = -12284.279712055772
3
Iteration 12600: Loss = -12284.279625343235
4
Iteration 12700: Loss = -12284.283093634238
5
Iteration 12800: Loss = -12284.27787275595
Iteration 12900: Loss = -12284.279659966809
1
Iteration 13000: Loss = -12284.346145309277
2
Iteration 13100: Loss = -12284.277729985566
Iteration 13200: Loss = -12284.278057251491
1
Iteration 13300: Loss = -12284.28332648985
2
Iteration 13400: Loss = -12284.288077102497
3
Iteration 13500: Loss = -12284.277739264164
Iteration 13600: Loss = -12284.27915749998
1
Iteration 13700: Loss = -12284.28600751325
2
Iteration 13800: Loss = -12284.277706025296
Iteration 13900: Loss = -12284.282902101872
1
Iteration 14000: Loss = -12284.277852626532
2
Iteration 14100: Loss = -12284.279226025239
3
Iteration 14200: Loss = -12284.279723824735
4
Iteration 14300: Loss = -12284.277973450455
5
Iteration 14400: Loss = -12284.278351895731
6
Iteration 14500: Loss = -12284.278564777736
7
Iteration 14600: Loss = -12284.292315458373
8
Iteration 14700: Loss = -12284.278777353296
9
Iteration 14800: Loss = -12284.2780268233
10
Iteration 14900: Loss = -12284.281440164612
11
Iteration 15000: Loss = -12284.277724168676
Iteration 15100: Loss = -12284.2783396004
1
Iteration 15200: Loss = -12284.278995997727
2
Iteration 15300: Loss = -12284.293548320265
3
Iteration 15400: Loss = -12284.280713635
4
Iteration 15500: Loss = -12284.277995478957
5
Iteration 15600: Loss = -12284.284189326183
6
Iteration 15700: Loss = -12284.277999368725
7
Iteration 15800: Loss = -12284.283058301426
8
Iteration 15900: Loss = -12284.300395304612
9
Iteration 16000: Loss = -12284.277706847284
Iteration 16100: Loss = -12284.277830489476
1
Iteration 16200: Loss = -12284.32828022749
2
Iteration 16300: Loss = -12284.291602183474
3
Iteration 16400: Loss = -12284.278355162754
4
Iteration 16500: Loss = -12284.278076399129
5
Iteration 16600: Loss = -12284.277862704046
6
Iteration 16700: Loss = -12284.2806017451
7
Iteration 16800: Loss = -12284.280668984868
8
Iteration 16900: Loss = -12284.27772026941
Iteration 17000: Loss = -12284.283057798892
1
Iteration 17100: Loss = -12284.278371742488
2
Iteration 17200: Loss = -12284.279104370704
3
Iteration 17300: Loss = -12284.285204339654
4
Iteration 17400: Loss = -12284.277949518673
5
Iteration 17500: Loss = -12284.3065720871
6
Iteration 17600: Loss = -12284.277882545348
7
Iteration 17700: Loss = -12284.277799443562
Iteration 17800: Loss = -12284.277668223123
Iteration 17900: Loss = -12284.279536378828
1
Iteration 18000: Loss = -12284.29336670637
2
Iteration 18100: Loss = -12284.281445113711
3
Iteration 18200: Loss = -12284.277695872184
Iteration 18300: Loss = -12284.27812066673
1
Iteration 18400: Loss = -12284.282330581198
2
Iteration 18500: Loss = -12284.475760257983
3
Iteration 18600: Loss = -12284.277662823568
Iteration 18700: Loss = -12284.278677378441
1
Iteration 18800: Loss = -12284.27823647085
2
Iteration 18900: Loss = -12284.27779791675
3
Iteration 19000: Loss = -12284.278198694708
4
Iteration 19100: Loss = -12284.278068680394
5
Iteration 19200: Loss = -12284.278462183782
6
Iteration 19300: Loss = -12284.278239974317
7
Iteration 19400: Loss = -12284.277748538141
Iteration 19500: Loss = -12284.277651070817
Iteration 19600: Loss = -12284.290155103241
1
Iteration 19700: Loss = -12284.27996091338
2
Iteration 19800: Loss = -12284.277681598898
Iteration 19900: Loss = -12284.289692203702
1
pi: tensor([[2.6659e-06, 1.0000e+00],
        [6.0962e-02, 9.3904e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9962, 0.0038], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2072, 0.2048],
         [0.6023, 0.1915]],

        [[0.5114, 0.1944],
         [0.7232, 0.5909]],

        [[0.6990, 0.2657],
         [0.5684, 0.5610]],

        [[0.5532, 0.2252],
         [0.6795, 0.5056]],

        [[0.5182, 0.1962],
         [0.6849, 0.6866]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00042552395964998385
Average Adjusted Rand Index: -0.0006490834186426951
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20532.249475865592
Iteration 100: Loss = -12288.505605706634
Iteration 200: Loss = -12287.658898663976
Iteration 300: Loss = -12287.381917655888
Iteration 400: Loss = -12287.182350377014
Iteration 500: Loss = -12286.962574522633
Iteration 600: Loss = -12286.77861522495
Iteration 700: Loss = -12286.684721594273
Iteration 800: Loss = -12286.61118644062
Iteration 900: Loss = -12286.536529835153
Iteration 1000: Loss = -12286.452993792176
Iteration 1100: Loss = -12286.354845955504
Iteration 1200: Loss = -12286.23629564708
Iteration 1300: Loss = -12286.08860147878
Iteration 1400: Loss = -12285.890685619474
Iteration 1500: Loss = -12285.618311164939
Iteration 1600: Loss = -12285.328366058338
Iteration 1700: Loss = -12285.067915100146
Iteration 1800: Loss = -12284.856558869053
Iteration 1900: Loss = -12284.697260120236
Iteration 2000: Loss = -12284.576365531815
Iteration 2100: Loss = -12284.489437091366
Iteration 2200: Loss = -12284.434750343467
Iteration 2300: Loss = -12284.401886105516
Iteration 2400: Loss = -12284.380517832502
Iteration 2500: Loss = -12284.36520916521
Iteration 2600: Loss = -12284.353488482948
Iteration 2700: Loss = -12284.344195120593
Iteration 2800: Loss = -12284.33661353924
Iteration 2900: Loss = -12284.33031057772
Iteration 3000: Loss = -12284.325031801462
Iteration 3100: Loss = -12284.320466333624
Iteration 3200: Loss = -12284.31653634896
Iteration 3300: Loss = -12284.313081347449
Iteration 3400: Loss = -12284.31009814013
Iteration 3500: Loss = -12284.307426477964
Iteration 3600: Loss = -12284.30503263696
Iteration 3700: Loss = -12284.302954909512
Iteration 3800: Loss = -12284.300991652199
Iteration 3900: Loss = -12284.299271083037
Iteration 4000: Loss = -12284.297744287533
Iteration 4100: Loss = -12284.296342070413
Iteration 4200: Loss = -12284.295082915416
Iteration 4300: Loss = -12284.293841275425
Iteration 4400: Loss = -12284.292836272716
Iteration 4500: Loss = -12284.291777499182
Iteration 4600: Loss = -12284.290884885297
Iteration 4700: Loss = -12284.290030363409
Iteration 4800: Loss = -12284.289237340894
Iteration 4900: Loss = -12284.288486192692
Iteration 5000: Loss = -12284.289532310462
1
Iteration 5100: Loss = -12284.287216165381
Iteration 5200: Loss = -12284.286670671572
Iteration 5300: Loss = -12284.286107897755
Iteration 5400: Loss = -12284.285577015022
Iteration 5500: Loss = -12284.28515705863
Iteration 5600: Loss = -12284.289872443667
1
Iteration 5700: Loss = -12284.284264871292
Iteration 5800: Loss = -12284.283911446526
Iteration 5900: Loss = -12284.283734925599
Iteration 6000: Loss = -12284.283201130927
Iteration 6100: Loss = -12284.282906291708
Iteration 6200: Loss = -12284.282785259107
Iteration 6300: Loss = -12284.282350000272
Iteration 6400: Loss = -12284.290672094477
1
Iteration 6500: Loss = -12284.28180344637
Iteration 6600: Loss = -12284.281577684102
Iteration 6700: Loss = -12284.281375611234
Iteration 6800: Loss = -12284.281150682491
Iteration 6900: Loss = -12284.281889680771
1
Iteration 7000: Loss = -12284.280792437463
Iteration 7100: Loss = -12284.304506620676
1
Iteration 7200: Loss = -12284.280448146868
Iteration 7300: Loss = -12284.283658732405
1
Iteration 7400: Loss = -12284.28173085098
2
Iteration 7500: Loss = -12284.280028266752
Iteration 7600: Loss = -12284.292065343658
1
Iteration 7700: Loss = -12284.279782113455
Iteration 7800: Loss = -12284.314324496589
1
Iteration 7900: Loss = -12284.27957394714
Iteration 8000: Loss = -12284.279431137627
Iteration 8100: Loss = -12284.279836199628
1
Iteration 8200: Loss = -12284.27925540373
Iteration 8300: Loss = -12284.341846546478
1
Iteration 8400: Loss = -12284.279136379191
Iteration 8500: Loss = -12284.279658294046
1
Iteration 8600: Loss = -12284.284895921799
2
Iteration 8700: Loss = -12284.278945593294
Iteration 8800: Loss = -12284.278890232401
Iteration 8900: Loss = -12284.452078348486
1
Iteration 9000: Loss = -12284.278715031027
Iteration 9100: Loss = -12284.588860747783
1
Iteration 9200: Loss = -12284.278610312367
Iteration 9300: Loss = -12284.42680892192
1
Iteration 9400: Loss = -12284.27981718348
2
Iteration 9500: Loss = -12284.278567357595
Iteration 9600: Loss = -12284.281934975334
1
Iteration 9700: Loss = -12284.278393496423
Iteration 9800: Loss = -12284.468084968144
1
Iteration 9900: Loss = -12284.278333669265
Iteration 10000: Loss = -12284.279730658378
1
Iteration 10100: Loss = -12284.278263554577
Iteration 10200: Loss = -12284.278892471892
1
Iteration 10300: Loss = -12284.281248390504
2
Iteration 10400: Loss = -12284.296825973657
3
Iteration 10500: Loss = -12284.2781302051
Iteration 10600: Loss = -12284.279161630075
1
Iteration 10700: Loss = -12284.279311048867
2
Iteration 10800: Loss = -12284.280301036486
3
Iteration 10900: Loss = -12284.30197266367
4
Iteration 11000: Loss = -12284.285260483603
5
Iteration 11100: Loss = -12284.285261877812
6
Iteration 11200: Loss = -12284.278485067052
7
Iteration 11300: Loss = -12284.278023018747
Iteration 11400: Loss = -12284.278502900248
1
Iteration 11500: Loss = -12284.278423518303
2
Iteration 11600: Loss = -12284.295106721205
3
Iteration 11700: Loss = -12284.278004032705
Iteration 11800: Loss = -12284.278030663854
Iteration 11900: Loss = -12284.280851380998
1
Iteration 12000: Loss = -12284.328484581918
2
Iteration 12100: Loss = -12284.279745772825
3
Iteration 12200: Loss = -12284.280444944414
4
Iteration 12300: Loss = -12284.287359634709
5
Iteration 12400: Loss = -12284.27792341748
Iteration 12500: Loss = -12284.278581926612
1
Iteration 12600: Loss = -12284.299471287804
2
Iteration 12700: Loss = -12284.279481373851
3
Iteration 12800: Loss = -12284.278043467217
4
Iteration 12900: Loss = -12284.278247611337
5
Iteration 13000: Loss = -12284.29751161501
6
Iteration 13100: Loss = -12284.277771570998
Iteration 13200: Loss = -12284.313627540361
1
Iteration 13300: Loss = -12284.277809163306
Iteration 13400: Loss = -12284.277885750269
Iteration 13500: Loss = -12284.534277790233
1
Iteration 13600: Loss = -12284.278613734205
2
Iteration 13700: Loss = -12284.287248435674
3
Iteration 13800: Loss = -12284.278164669915
4
Iteration 13900: Loss = -12284.278158353662
5
Iteration 14000: Loss = -12284.28046653339
6
Iteration 14100: Loss = -12284.278725793587
7
Iteration 14200: Loss = -12284.294569024007
8
Iteration 14300: Loss = -12284.279830637981
9
Iteration 14400: Loss = -12284.278984866483
10
Iteration 14500: Loss = -12284.421201010213
11
Iteration 14600: Loss = -12284.277728312827
Iteration 14700: Loss = -12284.283475422118
1
Iteration 14800: Loss = -12284.279610511434
2
Iteration 14900: Loss = -12284.277778950101
Iteration 15000: Loss = -12284.306510680472
1
Iteration 15100: Loss = -12284.277706519475
Iteration 15200: Loss = -12284.278340398863
1
Iteration 15300: Loss = -12284.284039413864
2
Iteration 15400: Loss = -12284.280684416915
3
Iteration 15500: Loss = -12284.340603330364
4
Iteration 15600: Loss = -12284.277993531332
5
Iteration 15700: Loss = -12284.306088867328
6
Iteration 15800: Loss = -12284.277814784984
7
Iteration 15900: Loss = -12284.281487058584
8
Iteration 16000: Loss = -12284.277915545197
9
Iteration 16100: Loss = -12284.279785691228
10
Iteration 16200: Loss = -12284.375160382455
11
Iteration 16300: Loss = -12284.277734440791
Iteration 16400: Loss = -12284.27823301905
1
Iteration 16500: Loss = -12284.385910652361
2
Iteration 16600: Loss = -12284.277692868085
Iteration 16700: Loss = -12284.278468471886
1
Iteration 16800: Loss = -12284.277708405109
Iteration 16900: Loss = -12284.277882161212
1
Iteration 17000: Loss = -12284.277745770736
Iteration 17100: Loss = -12284.278615271987
1
Iteration 17200: Loss = -12284.29546650465
2
Iteration 17300: Loss = -12284.322670797867
3
Iteration 17400: Loss = -12284.277685467994
Iteration 17500: Loss = -12284.283909164009
1
Iteration 17600: Loss = -12284.282384307571
2
Iteration 17700: Loss = -12284.300944697901
3
Iteration 17800: Loss = -12284.279975856045
4
Iteration 17900: Loss = -12284.277679132023
Iteration 18000: Loss = -12284.281592455503
1
Iteration 18100: Loss = -12284.278239647942
2
Iteration 18200: Loss = -12284.278171323123
3
Iteration 18300: Loss = -12284.277754520268
Iteration 18400: Loss = -12284.278065771448
1
Iteration 18500: Loss = -12284.344233526712
2
Iteration 18600: Loss = -12284.309447736665
3
Iteration 18700: Loss = -12284.277737659075
Iteration 18800: Loss = -12284.277936095257
1
Iteration 18900: Loss = -12284.364462014242
2
Iteration 19000: Loss = -12284.27849362428
3
Iteration 19100: Loss = -12284.30755372172
4
Iteration 19200: Loss = -12284.277727253444
Iteration 19300: Loss = -12284.2798518822
1
Iteration 19400: Loss = -12284.33522413952
2
Iteration 19500: Loss = -12284.277714895175
Iteration 19600: Loss = -12284.277714000878
Iteration 19700: Loss = -12284.278619711255
1
Iteration 19800: Loss = -12284.278150117372
2
Iteration 19900: Loss = -12284.27797793445
3
pi: tensor([[9.3923e-01, 6.0765e-02],
        [9.9999e-01, 5.2275e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0040, 0.9960], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1915, 0.2048],
         [0.5576, 0.2072]],

        [[0.5502, 0.1944],
         [0.6859, 0.5602]],

        [[0.5596, 0.2663],
         [0.6400, 0.6249]],

        [[0.5944, 0.2254],
         [0.7029, 0.5890]],

        [[0.6780, 0.1963],
         [0.5862, 0.5412]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00042552395964998385
Average Adjusted Rand Index: -0.0006490834186426951
11740.030858799195
[-0.00042552395964998385, -0.00042552395964998385] [-0.0006490834186426951, -0.0006490834186426951] [12284.279526722723, 12284.278412489713]
-------------------------------------
This iteration is 40
True Objective function: Loss = -11902.251808210178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22198.52806561717
Iteration 100: Loss = -12364.202656191046
Iteration 200: Loss = -12363.394980053828
Iteration 300: Loss = -12363.100574034972
Iteration 400: Loss = -12362.95444689763
Iteration 500: Loss = -12362.864361079794
Iteration 600: Loss = -12362.801161452999
Iteration 700: Loss = -12362.751913436123
Iteration 800: Loss = -12362.709984183733
Iteration 900: Loss = -12362.671547945667
Iteration 1000: Loss = -12362.634503513234
Iteration 1100: Loss = -12362.597682545855
Iteration 1200: Loss = -12362.560052279667
Iteration 1300: Loss = -12362.519698118187
Iteration 1400: Loss = -12362.473790689448
Iteration 1500: Loss = -12362.419673208156
Iteration 1600: Loss = -12362.356496200402
Iteration 1700: Loss = -12362.284956156507
Iteration 1800: Loss = -12362.205464955798
Iteration 1900: Loss = -12362.118541046471
Iteration 2000: Loss = -12362.028157064176
Iteration 2100: Loss = -12361.941714957162
Iteration 2200: Loss = -12361.864386685209
Iteration 2300: Loss = -12361.795334978107
Iteration 2400: Loss = -12361.732641198698
Iteration 2500: Loss = -12361.67456700426
Iteration 2600: Loss = -12361.61830507377
Iteration 2700: Loss = -12361.560792437802
Iteration 2800: Loss = -12361.500174382076
Iteration 2900: Loss = -12361.446510525564
Iteration 3000: Loss = -12361.378848850383
Iteration 3100: Loss = -12361.325600674994
Iteration 3200: Loss = -12361.282860274421
Iteration 3300: Loss = -12361.249214216392
Iteration 3400: Loss = -12361.227782760367
Iteration 3500: Loss = -12361.205508923
Iteration 3600: Loss = -12361.191103076533
Iteration 3700: Loss = -12361.180408138049
Iteration 3800: Loss = -12361.17263705112
Iteration 3900: Loss = -12361.167230230945
Iteration 4000: Loss = -12361.160903636235
Iteration 4100: Loss = -12361.15698543048
Iteration 4200: Loss = -12361.162226777591
1
Iteration 4300: Loss = -12361.150811617978
Iteration 4400: Loss = -12361.147556179125
Iteration 4500: Loss = -12361.14326599546
Iteration 4600: Loss = -12361.134310078603
Iteration 4700: Loss = -12361.113658268276
Iteration 4800: Loss = -12361.053122629557
Iteration 4900: Loss = -12360.916981067377
Iteration 5000: Loss = -12360.806117246437
Iteration 5100: Loss = -12360.76035458225
Iteration 5200: Loss = -12360.739823026624
Iteration 5300: Loss = -12360.727686575003
Iteration 5400: Loss = -12360.718922991544
Iteration 5500: Loss = -12360.711545623637
Iteration 5600: Loss = -12360.71049874049
Iteration 5700: Loss = -12360.6937446837
Iteration 5800: Loss = -12360.673874930826
Iteration 5900: Loss = -12360.642523476317
Iteration 6000: Loss = -12360.6258759463
Iteration 6100: Loss = -12360.617050585446
Iteration 6200: Loss = -12360.610594672944
Iteration 6300: Loss = -12360.606822063628
Iteration 6400: Loss = -12360.601795743385
Iteration 6500: Loss = -12360.598616867144
Iteration 6600: Loss = -12360.59604749225
Iteration 6700: Loss = -12360.593894442318
Iteration 6800: Loss = -12360.592184821979
Iteration 6900: Loss = -12360.590418197675
Iteration 7000: Loss = -12360.589049572536
Iteration 7100: Loss = -12360.587857588194
Iteration 7200: Loss = -12360.586762802672
Iteration 7300: Loss = -12360.585881269108
Iteration 7400: Loss = -12360.584982441853
Iteration 7500: Loss = -12360.586698086736
1
Iteration 7600: Loss = -12360.583518672087
Iteration 7700: Loss = -12360.583590927752
Iteration 7800: Loss = -12360.592540564787
1
Iteration 7900: Loss = -12360.582150280441
Iteration 8000: Loss = -12360.58142383352
Iteration 8100: Loss = -12360.582177963111
1
Iteration 8200: Loss = -12360.580547605772
Iteration 8300: Loss = -12360.580489071848
Iteration 8400: Loss = -12360.579879199177
Iteration 8500: Loss = -12360.581825230069
1
Iteration 8600: Loss = -12360.579302041166
Iteration 8700: Loss = -12360.585638595845
1
Iteration 8800: Loss = -12360.578815683832
Iteration 8900: Loss = -12360.656841299025
1
Iteration 9000: Loss = -12360.578353804927
Iteration 9100: Loss = -12360.593449152371
1
Iteration 9200: Loss = -12360.579272228093
2
Iteration 9300: Loss = -12360.577893211437
Iteration 9400: Loss = -12360.57793084837
Iteration 9500: Loss = -12360.577566340935
Iteration 9600: Loss = -12360.57792296069
1
Iteration 9700: Loss = -12360.70724322664
2
Iteration 9800: Loss = -12360.59269298838
3
Iteration 9900: Loss = -12360.581269542181
4
Iteration 10000: Loss = -12360.584340956633
5
Iteration 10100: Loss = -12360.739018605971
6
Iteration 10200: Loss = -12360.668494214404
7
Iteration 10300: Loss = -12360.76076034679
8
Iteration 10400: Loss = -12360.576495703202
Iteration 10500: Loss = -12360.578363957935
1
Iteration 10600: Loss = -12360.583803774933
2
Iteration 10700: Loss = -12360.698068460144
3
Iteration 10800: Loss = -12360.576322552159
Iteration 10900: Loss = -12360.578083906144
1
Iteration 11000: Loss = -12360.578865480515
2
Iteration 11100: Loss = -12360.576108306986
Iteration 11200: Loss = -12360.588793969027
1
Iteration 11300: Loss = -12360.58774870526
2
Iteration 11400: Loss = -12360.580004661557
3
Iteration 11500: Loss = -12360.57624386227
4
Iteration 11600: Loss = -12360.599274066362
5
Iteration 11700: Loss = -12360.621228853412
6
Iteration 11800: Loss = -12360.578645617505
7
Iteration 11900: Loss = -12360.576136104484
Iteration 12000: Loss = -12360.619933268626
1
Iteration 12100: Loss = -12360.581702815123
2
Iteration 12200: Loss = -12360.575605074164
Iteration 12300: Loss = -12360.575706653439
1
Iteration 12400: Loss = -12360.623809637904
2
Iteration 12500: Loss = -12360.57546339789
Iteration 12600: Loss = -12360.578335852615
1
Iteration 12700: Loss = -12360.616540625842
2
Iteration 12800: Loss = -12360.579466248784
3
Iteration 12900: Loss = -12360.576459306285
4
Iteration 13000: Loss = -12360.582960086755
5
Iteration 13100: Loss = -12360.61568932418
6
Iteration 13200: Loss = -12360.764699317759
7
Iteration 13300: Loss = -12360.575925866506
8
Iteration 13400: Loss = -12360.575291461402
Iteration 13500: Loss = -12360.575690582014
1
Iteration 13600: Loss = -12360.580865838296
2
Iteration 13700: Loss = -12360.575978131914
3
Iteration 13800: Loss = -12360.575225883329
Iteration 13900: Loss = -12360.578942259714
1
Iteration 14000: Loss = -12360.576272871383
2
Iteration 14100: Loss = -12360.575292243126
Iteration 14200: Loss = -12360.575777721653
1
Iteration 14300: Loss = -12360.577164961254
2
Iteration 14400: Loss = -12360.57891339246
3
Iteration 14500: Loss = -12360.585611387402
4
Iteration 14600: Loss = -12360.6134012586
5
Iteration 14700: Loss = -12360.575103836327
Iteration 14800: Loss = -12360.582029271201
1
Iteration 14900: Loss = -12360.575085842558
Iteration 15000: Loss = -12360.577415065392
1
Iteration 15100: Loss = -12360.575070370987
Iteration 15200: Loss = -12360.589644885835
1
Iteration 15300: Loss = -12360.575121346348
Iteration 15400: Loss = -12360.575112954715
Iteration 15500: Loss = -12360.575322907795
1
Iteration 15600: Loss = -12360.575121009359
Iteration 15700: Loss = -12360.575232577556
1
Iteration 15800: Loss = -12360.5856119443
2
Iteration 15900: Loss = -12360.574995614359
Iteration 16000: Loss = -12360.575349013474
1
Iteration 16100: Loss = -12360.575531922497
2
Iteration 16200: Loss = -12360.631483271261
3
Iteration 16300: Loss = -12360.575265349155
4
Iteration 16400: Loss = -12360.575062790407
Iteration 16500: Loss = -12360.577219599469
1
Iteration 16600: Loss = -12360.575009792068
Iteration 16700: Loss = -12360.575268964009
1
Iteration 16800: Loss = -12360.575040432135
Iteration 16900: Loss = -12360.575120963536
Iteration 17000: Loss = -12360.575036972508
Iteration 17100: Loss = -12360.575129135583
Iteration 17200: Loss = -12360.715643326894
1
Iteration 17300: Loss = -12360.575000257406
Iteration 17400: Loss = -12360.575176731538
1
Iteration 17500: Loss = -12360.664400455593
2
Iteration 17600: Loss = -12360.574983417413
Iteration 17700: Loss = -12360.57758855061
1
Iteration 17800: Loss = -12360.575023756783
Iteration 17900: Loss = -12360.577937169224
1
Iteration 18000: Loss = -12360.584121422607
2
Iteration 18100: Loss = -12360.623896140898
3
Iteration 18200: Loss = -12360.574987406497
Iteration 18300: Loss = -12360.575373123069
1
Iteration 18400: Loss = -12360.57495620704
Iteration 18500: Loss = -12360.575425637628
1
Iteration 18600: Loss = -12360.575231371628
2
Iteration 18700: Loss = -12360.57505639997
3
Iteration 18800: Loss = -12360.574971865914
Iteration 18900: Loss = -12360.576309225378
1
Iteration 19000: Loss = -12360.574976543892
Iteration 19100: Loss = -12360.575118783916
1
Iteration 19200: Loss = -12360.658259467524
2
Iteration 19300: Loss = -12360.583715197474
3
Iteration 19400: Loss = -12360.737191297523
4
Iteration 19500: Loss = -12360.575158093763
5
Iteration 19600: Loss = -12360.575007757443
Iteration 19700: Loss = -12360.584194983132
1
Iteration 19800: Loss = -12360.610462795923
2
Iteration 19900: Loss = -12360.574968558529
pi: tensor([[4.7029e-06, 1.0000e+00],
        [1.0000e+00, 3.7250e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8148, 0.1852], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2040, 0.1979],
         [0.6979, 0.1922]],

        [[0.6907, 0.2093],
         [0.6259, 0.6396]],

        [[0.6515, 0.1977],
         [0.5563, 0.6958]],

        [[0.6634, 0.1869],
         [0.5402, 0.6912]],

        [[0.5792, 0.2055],
         [0.5534, 0.5716]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011475446544096347
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22337.4708511679
Iteration 100: Loss = -12363.710596131801
Iteration 200: Loss = -12363.08480968371
Iteration 300: Loss = -12362.901934944393
Iteration 400: Loss = -12362.818263971454
Iteration 500: Loss = -12362.771356707815
Iteration 600: Loss = -12362.740412887002
Iteration 700: Loss = -12362.716833024875
Iteration 800: Loss = -12362.696370637863
Iteration 900: Loss = -12362.676797517159
Iteration 1000: Loss = -12362.656570759982
Iteration 1100: Loss = -12362.63480263096
Iteration 1200: Loss = -12362.61058751676
Iteration 1300: Loss = -12362.582384072477
Iteration 1400: Loss = -12362.547251875483
Iteration 1500: Loss = -12362.500051852112
Iteration 1600: Loss = -12362.433399541465
Iteration 1700: Loss = -12362.342886738616
Iteration 1800: Loss = -12362.2362818529
Iteration 1900: Loss = -12362.121917845305
Iteration 2000: Loss = -12362.002991840518
Iteration 2100: Loss = -12361.887791649693
Iteration 2200: Loss = -12361.78657827751
Iteration 2300: Loss = -12361.69931217464
Iteration 2400: Loss = -12361.617002098812
Iteration 2500: Loss = -12361.537891068696
Iteration 2600: Loss = -12361.459791336772
Iteration 2700: Loss = -12361.39731293545
Iteration 2800: Loss = -12361.328231269472
Iteration 2900: Loss = -12361.280591490864
Iteration 3000: Loss = -12361.24446187473
Iteration 3100: Loss = -12361.21453497162
Iteration 3200: Loss = -12361.186338801985
Iteration 3300: Loss = -12361.152265114944
Iteration 3400: Loss = -12361.095646432119
Iteration 3500: Loss = -12360.98575291399
Iteration 3600: Loss = -12360.853651768575
Iteration 3700: Loss = -12360.779430035138
Iteration 3800: Loss = -12360.746254854452
Iteration 3900: Loss = -12360.72866474832
Iteration 4000: Loss = -12360.716573618518
Iteration 4100: Loss = -12360.705753907749
Iteration 4200: Loss = -12360.693120121126
Iteration 4300: Loss = -12360.675399311589
Iteration 4400: Loss = -12360.653601417913
Iteration 4500: Loss = -12360.637784678993
Iteration 4600: Loss = -12360.64324867189
1
Iteration 4700: Loss = -12360.620618195564
Iteration 4800: Loss = -12360.614808208604
Iteration 4900: Loss = -12360.611611133512
Iteration 5000: Loss = -12360.606187649919
Iteration 5100: Loss = -12360.602930636862
Iteration 5200: Loss = -12360.60026380375
Iteration 5300: Loss = -12360.597834128079
Iteration 5400: Loss = -12360.595725657908
Iteration 5500: Loss = -12360.593949564107
Iteration 5600: Loss = -12360.592379500353
Iteration 5700: Loss = -12360.59112731589
Iteration 5800: Loss = -12360.589728985233
Iteration 5900: Loss = -12360.588634357951
Iteration 6000: Loss = -12360.587599585404
Iteration 6100: Loss = -12360.586671650075
Iteration 6200: Loss = -12360.585909055008
Iteration 6300: Loss = -12360.585112105893
Iteration 6400: Loss = -12360.584478658218
Iteration 6500: Loss = -12360.583759836432
Iteration 6600: Loss = -12360.583305142238
Iteration 6700: Loss = -12360.58269385779
Iteration 6800: Loss = -12360.583339726667
1
Iteration 6900: Loss = -12360.581773574677
Iteration 7000: Loss = -12360.584676703975
1
Iteration 7100: Loss = -12360.580950140044
Iteration 7200: Loss = -12360.586922751423
1
Iteration 7300: Loss = -12360.580237559874
Iteration 7400: Loss = -12360.602584205022
1
Iteration 7500: Loss = -12360.58293843576
2
Iteration 7600: Loss = -12360.579414488551
Iteration 7700: Loss = -12360.57914358115
Iteration 7800: Loss = -12360.57897439059
Iteration 7900: Loss = -12360.57866289353
Iteration 8000: Loss = -12360.578475944421
Iteration 8100: Loss = -12360.578290427668
Iteration 8200: Loss = -12360.582860491459
1
Iteration 8300: Loss = -12360.58021814668
2
Iteration 8400: Loss = -12360.57778223634
Iteration 8500: Loss = -12360.577847594746
Iteration 8600: Loss = -12360.577464032483
Iteration 8700: Loss = -12360.577371484207
Iteration 8800: Loss = -12360.57719593079
Iteration 8900: Loss = -12360.57716746212
Iteration 9000: Loss = -12360.788094908738
1
Iteration 9100: Loss = -12360.576882234376
Iteration 9200: Loss = -12360.591398135
1
Iteration 9300: Loss = -12360.576673077978
Iteration 9400: Loss = -12360.599841775866
1
Iteration 9500: Loss = -12360.576495003228
Iteration 9600: Loss = -12360.576909934734
1
Iteration 9700: Loss = -12360.576630524802
2
Iteration 9800: Loss = -12360.582688355673
3
Iteration 9900: Loss = -12360.577934123543
4
Iteration 10000: Loss = -12360.57619325411
Iteration 10100: Loss = -12360.57669147653
1
Iteration 10200: Loss = -12360.582596569258
2
Iteration 10300: Loss = -12360.704201325414
3
Iteration 10400: Loss = -12360.57593523546
Iteration 10500: Loss = -12360.57602005421
Iteration 10600: Loss = -12360.644475356557
1
Iteration 10700: Loss = -12360.575777890026
Iteration 10800: Loss = -12360.576020794542
1
Iteration 10900: Loss = -12360.576076755988
2
Iteration 11000: Loss = -12360.576007107353
3
Iteration 11100: Loss = -12360.57564291768
Iteration 11200: Loss = -12360.578008785105
1
Iteration 11300: Loss = -12360.575543330924
Iteration 11400: Loss = -12360.575693668728
1
Iteration 11500: Loss = -12360.576641756674
2
Iteration 11600: Loss = -12360.579240358775
3
Iteration 11700: Loss = -12360.575484478439
Iteration 11800: Loss = -12360.575665422259
1
Iteration 11900: Loss = -12360.577999354507
2
Iteration 12000: Loss = -12360.575339443429
Iteration 12100: Loss = -12360.576401271028
1
Iteration 12200: Loss = -12360.584915239879
2
Iteration 12300: Loss = -12360.57567946327
3
Iteration 12400: Loss = -12360.575351215568
Iteration 12500: Loss = -12360.58012317087
1
Iteration 12600: Loss = -12360.575414300438
Iteration 12700: Loss = -12360.57650955637
1
Iteration 12800: Loss = -12360.80442083713
2
Iteration 12900: Loss = -12360.575231177752
Iteration 13000: Loss = -12360.57713331936
1
Iteration 13100: Loss = -12360.57536633041
2
Iteration 13200: Loss = -12360.575275157142
Iteration 13300: Loss = -12360.575860026393
1
Iteration 13400: Loss = -12360.601670811118
2
Iteration 13500: Loss = -12360.663856888756
3
Iteration 13600: Loss = -12360.57775654501
4
Iteration 13700: Loss = -12360.575126153164
Iteration 13800: Loss = -12360.576267043665
1
Iteration 13900: Loss = -12360.619569730248
2
Iteration 14000: Loss = -12360.575087040743
Iteration 14100: Loss = -12360.577748977392
1
Iteration 14200: Loss = -12360.716854034721
2
Iteration 14300: Loss = -12360.575133572633
Iteration 14400: Loss = -12360.5753523133
1
Iteration 14500: Loss = -12360.576393246363
2
Iteration 14600: Loss = -12360.581278077685
3
Iteration 14700: Loss = -12360.614743630405
4
Iteration 14800: Loss = -12360.608537911028
5
Iteration 14900: Loss = -12360.575072779566
Iteration 15000: Loss = -12360.576330854747
1
Iteration 15100: Loss = -12360.614485187169
2
Iteration 15200: Loss = -12360.575959557013
3
Iteration 15300: Loss = -12360.596370970014
4
Iteration 15400: Loss = -12360.577745387332
5
Iteration 15500: Loss = -12360.576723712324
6
Iteration 15600: Loss = -12360.76270382348
7
Iteration 15700: Loss = -12360.575094668808
Iteration 15800: Loss = -12360.575202104314
1
Iteration 15900: Loss = -12360.575136150374
Iteration 16000: Loss = -12360.575390311526
1
Iteration 16100: Loss = -12360.575413740567
2
Iteration 16200: Loss = -12360.768909117223
3
Iteration 16300: Loss = -12360.574967854744
Iteration 16400: Loss = -12360.6116706215
1
Iteration 16500: Loss = -12360.575061352556
Iteration 16600: Loss = -12360.575086472374
Iteration 16700: Loss = -12360.668906306208
1
Iteration 16800: Loss = -12360.574976917764
Iteration 16900: Loss = -12360.575210013301
1
Iteration 17000: Loss = -12360.575559519708
2
Iteration 17100: Loss = -12360.586763032077
3
Iteration 17200: Loss = -12360.621654941839
4
Iteration 17300: Loss = -12360.615176985362
5
Iteration 17400: Loss = -12360.614011413672
6
Iteration 17500: Loss = -12360.581968098451
7
Iteration 17600: Loss = -12360.576172307212
8
Iteration 17700: Loss = -12360.576342177037
9
Iteration 17800: Loss = -12360.575604925383
10
Iteration 17900: Loss = -12360.575053534654
Iteration 18000: Loss = -12360.575521080791
1
Iteration 18100: Loss = -12360.58793995484
2
Iteration 18200: Loss = -12360.57706715999
3
Iteration 18300: Loss = -12360.57580004616
4
Iteration 18400: Loss = -12360.580690217828
5
Iteration 18500: Loss = -12360.577816426081
6
Iteration 18600: Loss = -12360.671362139368
7
Iteration 18700: Loss = -12360.577398631098
8
Iteration 18800: Loss = -12360.58092106363
9
Iteration 18900: Loss = -12360.576434220427
10
Iteration 19000: Loss = -12360.575018112342
Iteration 19100: Loss = -12360.582505850509
1
Iteration 19200: Loss = -12360.584201937181
2
Iteration 19300: Loss = -12360.574982232194
Iteration 19400: Loss = -12360.576325137943
1
Iteration 19500: Loss = -12360.59939966863
2
Iteration 19600: Loss = -12360.575056753003
Iteration 19700: Loss = -12360.575045568701
Iteration 19800: Loss = -12360.6619026791
1
Iteration 19900: Loss = -12360.57510546825
pi: tensor([[2.3594e-06, 1.0000e+00],
        [1.0000e+00, 3.1664e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1868, 0.8132], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1923, 0.1977],
         [0.6106, 0.2044]],

        [[0.5052, 0.2088],
         [0.6200, 0.5284]],

        [[0.6928, 0.1972],
         [0.6012, 0.5399]],

        [[0.6671, 0.1868],
         [0.5130, 0.5678]],

        [[0.7093, 0.2053],
         [0.5047, 0.7066]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011475446544096347
Average Adjusted Rand Index: 0.0
11902.251808210178
[-0.0011475446544096347, -0.0011475446544096347] [0.0, 0.0] [12360.586113093013, 12360.575698558117]
-------------------------------------
This iteration is 41
True Objective function: Loss = -11931.664360528048
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22444.792732548372
Iteration 100: Loss = -12454.10868116762
Iteration 200: Loss = -12452.584782997146
Iteration 300: Loss = -12451.792119851072
Iteration 400: Loss = -12451.384207155766
Iteration 500: Loss = -12451.15285511325
Iteration 600: Loss = -12451.021992647184
Iteration 700: Loss = -12450.941016455017
Iteration 800: Loss = -12450.885421862982
Iteration 900: Loss = -12450.844566822225
Iteration 1000: Loss = -12450.813846713227
Iteration 1100: Loss = -12450.790853626071
Iteration 1200: Loss = -12450.774106876746
Iteration 1300: Loss = -12450.761997129433
Iteration 1400: Loss = -12450.753362251582
Iteration 1500: Loss = -12450.747041523471
Iteration 1600: Loss = -12450.742205449906
Iteration 1700: Loss = -12450.738327082347
Iteration 1800: Loss = -12450.735033263283
Iteration 1900: Loss = -12450.73201379132
Iteration 2000: Loss = -12450.729348708692
Iteration 2100: Loss = -12450.726779160068
Iteration 2200: Loss = -12450.724391556121
Iteration 2300: Loss = -12450.722025768322
Iteration 2400: Loss = -12450.719829647896
Iteration 2500: Loss = -12450.717659617723
Iteration 2600: Loss = -12450.715618899347
Iteration 2700: Loss = -12450.713668979824
Iteration 2800: Loss = -12450.711767179786
Iteration 2900: Loss = -12450.709991402242
Iteration 3000: Loss = -12450.708267612981
Iteration 3100: Loss = -12450.706674788558
Iteration 3200: Loss = -12450.705178133267
Iteration 3300: Loss = -12450.703697148754
Iteration 3400: Loss = -12450.702347518716
Iteration 3500: Loss = -12450.701044423262
Iteration 3600: Loss = -12450.699834419374
Iteration 3700: Loss = -12450.698629341166
Iteration 3800: Loss = -12450.697616932508
Iteration 3900: Loss = -12450.696616840709
Iteration 4000: Loss = -12450.695683959902
Iteration 4100: Loss = -12450.694810092364
Iteration 4200: Loss = -12450.69395046434
Iteration 4300: Loss = -12450.693151255946
Iteration 4400: Loss = -12450.692435912304
Iteration 4500: Loss = -12450.691732123265
Iteration 4600: Loss = -12450.691078329752
Iteration 4700: Loss = -12450.690510674249
Iteration 4800: Loss = -12450.689983676262
Iteration 4900: Loss = -12450.6894192311
Iteration 5000: Loss = -12450.688904069813
Iteration 5100: Loss = -12450.688458649249
Iteration 5200: Loss = -12450.688053012176
Iteration 5300: Loss = -12450.687597062382
Iteration 5400: Loss = -12450.687235196341
Iteration 5500: Loss = -12450.686877814807
Iteration 5600: Loss = -12450.686524607268
Iteration 5700: Loss = -12450.686323791937
Iteration 5800: Loss = -12450.685922353825
Iteration 5900: Loss = -12450.68562650593
Iteration 6000: Loss = -12450.685350955173
Iteration 6100: Loss = -12450.685110929873
Iteration 6200: Loss = -12450.685324348344
1
Iteration 6300: Loss = -12450.684681877949
Iteration 6400: Loss = -12450.684439041523
Iteration 6500: Loss = -12450.684273928879
Iteration 6600: Loss = -12450.684044355337
Iteration 6700: Loss = -12450.684287092494
1
Iteration 6800: Loss = -12450.683717516273
Iteration 6900: Loss = -12450.683579466982
Iteration 7000: Loss = -12450.683436031557
Iteration 7100: Loss = -12450.683285531128
Iteration 7200: Loss = -12450.683152826667
Iteration 7300: Loss = -12450.785711602377
1
Iteration 7400: Loss = -12450.719475793565
2
Iteration 7500: Loss = -12450.682812010682
Iteration 7600: Loss = -12450.68295891643
1
Iteration 7700: Loss = -12450.68756992358
2
Iteration 7800: Loss = -12450.682524051597
Iteration 7900: Loss = -12450.699125539066
1
Iteration 8000: Loss = -12450.682352100079
Iteration 8100: Loss = -12450.682277889706
Iteration 8200: Loss = -12450.683015293189
1
Iteration 8300: Loss = -12450.682136521878
Iteration 8400: Loss = -12450.682047015964
Iteration 8500: Loss = -12450.68259339078
1
Iteration 8600: Loss = -12450.681944806862
Iteration 8700: Loss = -12450.681882941843
Iteration 8800: Loss = -12450.681839477662
Iteration 8900: Loss = -12450.681803048275
Iteration 9000: Loss = -12450.681740425602
Iteration 9100: Loss = -12450.681844667277
1
Iteration 9200: Loss = -12450.681662469357
Iteration 9300: Loss = -12450.681626269747
Iteration 9400: Loss = -12451.331062720757
1
Iteration 9500: Loss = -12450.6815397611
Iteration 9600: Loss = -12450.681499243941
Iteration 9700: Loss = -12450.681491433452
Iteration 9800: Loss = -12450.681454481155
Iteration 9900: Loss = -12450.681399448406
Iteration 10000: Loss = -12450.68138904471
Iteration 10100: Loss = -12450.681790096214
1
Iteration 10200: Loss = -12450.681315496235
Iteration 10300: Loss = -12450.681311679564
Iteration 10400: Loss = -12450.68948535122
1
Iteration 10500: Loss = -12450.681299561444
Iteration 10600: Loss = -12450.681261440639
Iteration 10700: Loss = -12450.710702469838
1
Iteration 10800: Loss = -12450.681206304414
Iteration 10900: Loss = -12450.681247052124
Iteration 11000: Loss = -12450.685403796333
1
Iteration 11100: Loss = -12450.681169653022
Iteration 11200: Loss = -12450.681141945479
Iteration 11300: Loss = -12450.681164924836
Iteration 11400: Loss = -12450.68141039725
1
Iteration 11500: Loss = -12450.681120891935
Iteration 11600: Loss = -12450.681134742013
Iteration 11700: Loss = -12450.683377841358
1
Iteration 11800: Loss = -12450.68111230643
Iteration 11900: Loss = -12450.68110784953
Iteration 12000: Loss = -12450.687741093867
1
Iteration 12100: Loss = -12450.681059894876
Iteration 12200: Loss = -12450.681035000665
Iteration 12300: Loss = -12450.730269308342
1
Iteration 12400: Loss = -12450.68100955761
Iteration 12500: Loss = -12450.680997082945
Iteration 12600: Loss = -12451.099050039376
1
Iteration 12700: Loss = -12450.681042901753
Iteration 12800: Loss = -12450.681001378118
Iteration 12900: Loss = -12450.68095202033
Iteration 13000: Loss = -12450.681104338742
1
Iteration 13100: Loss = -12450.680957490518
Iteration 13200: Loss = -12450.680951517945
Iteration 13300: Loss = -12450.682159433438
1
Iteration 13400: Loss = -12450.680969727764
Iteration 13500: Loss = -12450.687144656702
1
Iteration 13600: Loss = -12450.681555710535
2
Iteration 13700: Loss = -12450.693130719204
3
Iteration 13800: Loss = -12450.681261751226
4
Iteration 13900: Loss = -12450.681303535554
5
Iteration 14000: Loss = -12450.686645815253
6
Iteration 14100: Loss = -12450.681767065429
7
Iteration 14200: Loss = -12450.681119037508
8
Iteration 14300: Loss = -12450.681240407303
9
Iteration 14400: Loss = -12450.681028654944
Iteration 14500: Loss = -12450.681155470693
1
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 42%|████▏     | 42/100 [13:52:18<20:29:23, 1271.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 43%|████▎     | 43/100 [14:13:41<20:11:31, 1275.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 44%|████▍     | 44/100 [14:27:04<17:38:06, 1133.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 45%|████▌     | 45/100 [14:48:23<17:59:11, 1177.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 46%|████▌     | 46/100 [15:08:32<17:48:04, 1186.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 47%|████▋     | 47/100 [15:29:50<17:52:25, 1214.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 48%|████▊     | 48/100 [15:50:14<17:34:47, 1217.06s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 49%|████▉     | 49/100 [16:11:12<17:24:52, 1229.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 50%|█████     | 50/100 [16:29:13<16:27:14, 1184.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 51%|█████     | 51/100 [16:45:30<15:16:37, 1122.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 52%|█████▏    | 52/100 [17:06:16<15:27:40, 1159.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 53%|█████▎    | 53/100 [17:25:18<15:04:13, 1154.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 54%|█████▍    | 54/100 [17:46:33<15:12:49, 1190.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 55%|█████▌    | 55/100 [18:02:44<14:03:33, 1124.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 56%|█████▌    | 56/100 [18:24:37<14:26:04, 1181.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 57%|█████▋    | 57/100 [18:45:37<14:23:26, 1204.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 58%|█████▊    | 58/100 [19:07:47<14:29:35, 1242.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 59%|█████▉    | 59/100 [19:25:54<13:37:10, 1195.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 60%|██████    | 60/100 [19:44:20<12:59:15, 1168.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 61%|██████    | 61/100 [20:05:35<13:00:25, 1200.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 62%|██████▏   | 62/100 [20:22:06<12:00:35, 1137.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 63%|██████▎   | 63/100 [20:43:23<12:07:19, 1179.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 64%|██████▍   | 64/100 [21:04:41<12:05:29, 1209.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 65%|██████▌   | 65/100 [21:19:18<10:47:14, 1109.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 66%|██████▌   | 66/100 [21:40:37<10:57:34, 1160.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 67%|██████▋   | 67/100 [22:02:33<11:03:54, 1207.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 68%|██████▊   | 68/100 [22:16:26<9:43:51, 1094.74s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 69%|██████▉   | 69/100 [22:38:15<9:58:48, 1158.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 70%|███████   | 70/100 [22:49:49<8:29:42, 1019.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 71%|███████   | 71/100 [23:11:48<8:56:13, 1109.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 72%|███████▏  | 72/100 [23:33:01<9:00:35, 1158.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 73%|███████▎  | 73/100 [23:50:08<8:23:38, 1119.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 74%|███████▍  | 74/100 [24:07:21<7:53:40, 1093.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 75%|███████▌  | 75/100 [24:25:04<7:31:44, 1084.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 76%|███████▌  | 76/100 [24:44:42<7:24:56, 1112.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 77%|███████▋  | 77/100 [25:06:42<7:30:13, 1174.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 78%|███████▊  | 78/100 [25:24:45<7:00:39, 1147.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 79%|███████▉  | 79/100 [25:45:29<6:51:40, 1176.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 80%|████████  | 80/100 [26:05:15<6:33:03, 1179.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 81%|████████  | 81/100 [26:20:34<5:48:40, 1101.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 82%|████████▏ | 82/100 [26:41:54<5:46:22, 1154.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 83%|████████▎ | 83/100 [27:00:23<5:23:17, 1141.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
Iteration 14600: Loss = -12450.683658622504
2
Iteration 14700: Loss = -12450.680920217168
Iteration 14800: Loss = -12450.689623041304
1
Iteration 14900: Loss = -12450.680898309476
Iteration 15000: Loss = -12450.681526265409
1
Iteration 15100: Loss = -12450.680905003042
Iteration 15200: Loss = -12450.68528817479
1
Iteration 15300: Loss = -12450.680920763192
Iteration 15400: Loss = -12450.698451729777
1
Iteration 15500: Loss = -12450.680917519298
Iteration 15600: Loss = -12450.683144234028
1
Iteration 15700: Loss = -12450.682021063942
2
Iteration 15800: Loss = -12450.680886643046
Iteration 15900: Loss = -12450.680900370915
Iteration 16000: Loss = -12450.681039625926
1
Iteration 16100: Loss = -12450.680890372858
Iteration 16200: Loss = -12450.68305663176
1
Iteration 16300: Loss = -12450.680939553598
Iteration 16400: Loss = -12450.680887211765
Iteration 16500: Loss = -12450.93256885368
1
Iteration 16600: Loss = -12450.680877861843
Iteration 16700: Loss = -12450.84219337299
1
Iteration 16800: Loss = -12450.680863734433
Iteration 16900: Loss = -12450.83369807306
1
Iteration 17000: Loss = -12450.68150362955
2
Iteration 17100: Loss = -12450.68087767745
Iteration 17200: Loss = -12450.683574697185
1
Iteration 17300: Loss = -12450.680870496544
Iteration 17400: Loss = -12450.681303245872
1
Iteration 17500: Loss = -12450.680896674068
Iteration 17600: Loss = -12450.682567422324
1
Iteration 17700: Loss = -12450.680931618992
Iteration 17800: Loss = -12450.68195640291
1
Iteration 17900: Loss = -12450.68085462788
Iteration 18000: Loss = -12450.684727007272
1
Iteration 18100: Loss = -12450.680869811005
Iteration 18200: Loss = -12450.836460595867
1
Iteration 18300: Loss = -12450.680889892543
Iteration 18400: Loss = -12450.680861179546
Iteration 18500: Loss = -12450.681150562177
1
Iteration 18600: Loss = -12451.002441840372
2
Iteration 18700: Loss = -12450.680904302682
Iteration 18800: Loss = -12451.02419425629
1
Iteration 18900: Loss = -12450.680871482306
Iteration 19000: Loss = -12450.681425029123
1
Iteration 19100: Loss = -12450.680902279591
Iteration 19200: Loss = -12450.680831004747
Iteration 19300: Loss = -12450.681089736112
1
Iteration 19400: Loss = -12450.715277287227
2
Iteration 19500: Loss = -12450.680841076357
Iteration 19600: Loss = -12450.68118635644
1
Iteration 19700: Loss = -12450.68305997076
2
Iteration 19800: Loss = -12450.680896713267
Iteration 19900: Loss = -12450.681552678943
1
pi: tensor([[2.4751e-06, 1.0000e+00],
        [2.8223e-02, 9.7178e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0016, 0.9984], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2908, 0.1953],
         [0.5198, 0.2003]],

        [[0.5731, 0.3154],
         [0.6230, 0.5897]],

        [[0.5199, 0.1550],
         [0.6278, 0.6643]],

        [[0.6024, 0.2265],
         [0.6474, 0.6953]],

        [[0.5403, 0.2583],
         [0.5229, 0.6981]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.011374456256342739
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008735738497905159
Average Adjusted Rand Index: -0.002274891251268548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22500.862811720184
Iteration 100: Loss = -12454.454461011555
Iteration 200: Loss = -12452.7381668602
Iteration 300: Loss = -12452.008405422224
Iteration 400: Loss = -12451.57296501334
Iteration 500: Loss = -12451.280092270814
Iteration 600: Loss = -12451.118660254708
Iteration 700: Loss = -12451.02529130586
Iteration 800: Loss = -12450.96239000234
Iteration 900: Loss = -12450.915962386243
Iteration 1000: Loss = -12450.88003272107
Iteration 1100: Loss = -12450.85162595671
Iteration 1200: Loss = -12450.82907114672
Iteration 1300: Loss = -12450.811307907767
Iteration 1400: Loss = -12450.797310657532
Iteration 1500: Loss = -12450.786411331663
Iteration 1600: Loss = -12450.777885637764
Iteration 1700: Loss = -12450.771174878111
Iteration 1800: Loss = -12450.76573486582
Iteration 1900: Loss = -12450.761218882884
Iteration 2000: Loss = -12450.757307484457
Iteration 2100: Loss = -12450.753970256996
Iteration 2200: Loss = -12450.750830426769
Iteration 2300: Loss = -12450.747958243328
Iteration 2400: Loss = -12450.74516601793
Iteration 2500: Loss = -12450.742478975404
Iteration 2600: Loss = -12450.739853511084
Iteration 2700: Loss = -12450.737289443
Iteration 2800: Loss = -12450.734782742198
Iteration 2900: Loss = -12450.732323758
Iteration 3000: Loss = -12450.729897200248
Iteration 3100: Loss = -12450.727532109331
Iteration 3200: Loss = -12450.725249109191
Iteration 3300: Loss = -12450.722987959844
Iteration 3400: Loss = -12450.720793194738
Iteration 3500: Loss = -12450.718658092188
Iteration 3600: Loss = -12450.716637188938
Iteration 3700: Loss = -12450.714668814937
Iteration 3800: Loss = -12450.71279342899
Iteration 3900: Loss = -12450.710972509274
Iteration 4000: Loss = -12450.709256792274
Iteration 4100: Loss = -12450.707618807166
Iteration 4200: Loss = -12450.70605844785
Iteration 4300: Loss = -12450.704625496088
Iteration 4400: Loss = -12450.703208335803
Iteration 4500: Loss = -12450.701897043104
Iteration 4600: Loss = -12450.700672845225
Iteration 4700: Loss = -12450.699490201085
Iteration 4800: Loss = -12450.69839679679
Iteration 4900: Loss = -12450.697337869738
Iteration 5000: Loss = -12450.696386630067
Iteration 5100: Loss = -12450.695472256188
Iteration 5200: Loss = -12450.694631484002
Iteration 5300: Loss = -12450.695016872234
1
Iteration 5400: Loss = -12450.693039223239
Iteration 5500: Loss = -12450.69239264892
Iteration 5600: Loss = -12450.69167086719
Iteration 5700: Loss = -12450.691093305088
Iteration 5800: Loss = -12450.690449890571
Iteration 5900: Loss = -12450.689947657642
Iteration 6000: Loss = -12450.689449532096
Iteration 6100: Loss = -12450.689102163433
Iteration 6200: Loss = -12450.688490723933
Iteration 6300: Loss = -12450.688067697769
Iteration 6400: Loss = -12450.687694230264
Iteration 6500: Loss = -12450.687287940362
Iteration 6600: Loss = -12450.686943993367
Iteration 6700: Loss = -12450.68658517738
Iteration 6800: Loss = -12450.695224627552
1
Iteration 6900: Loss = -12450.725700067169
2
Iteration 7000: Loss = -12450.685668830387
Iteration 7100: Loss = -12450.688410403702
1
Iteration 7200: Loss = -12450.68517798856
Iteration 7300: Loss = -12450.68558544897
1
Iteration 7400: Loss = -12450.684797347638
Iteration 7500: Loss = -12450.684523897433
Iteration 7600: Loss = -12450.684320643773
Iteration 7700: Loss = -12450.685802327747
1
Iteration 7800: Loss = -12450.684029600974
Iteration 7900: Loss = -12450.683841057204
Iteration 8000: Loss = -12450.720352474305
1
Iteration 8100: Loss = -12450.683522487221
Iteration 8200: Loss = -12450.683426602767
Iteration 8300: Loss = -12450.74143659091
1
Iteration 8400: Loss = -12450.683175800645
Iteration 8500: Loss = -12450.682994616143
Iteration 8600: Loss = -12450.6828670752
Iteration 8700: Loss = -12450.682927546248
Iteration 8800: Loss = -12450.682692298835
Iteration 8900: Loss = -12450.682590198181
Iteration 9000: Loss = -12450.683386269458
1
Iteration 9100: Loss = -12450.682438399977
Iteration 9200: Loss = -12450.682353568718
Iteration 9300: Loss = -12450.685563871062
1
Iteration 9400: Loss = -12450.682226144227
Iteration 9500: Loss = -12450.682154533819
Iteration 9600: Loss = -12450.682089874806
Iteration 9700: Loss = -12450.682026039933
Iteration 9800: Loss = -12450.681936314724
Iteration 9900: Loss = -12450.681906880905
Iteration 10000: Loss = -12450.685915466078
1
Iteration 10100: Loss = -12450.681809184813
Iteration 10200: Loss = -12450.681765837156
Iteration 10300: Loss = -12450.692712423202
1
Iteration 10400: Loss = -12450.681698448121
Iteration 10500: Loss = -12450.68166283256
Iteration 10600: Loss = -12450.844931675803
1
Iteration 10700: Loss = -12450.68153449497
Iteration 10800: Loss = -12450.681542009388
Iteration 10900: Loss = -12450.68148794025
Iteration 11000: Loss = -12450.777631942434
1
Iteration 11100: Loss = -12450.681431511242
Iteration 11200: Loss = -12450.68144829899
Iteration 11300: Loss = -12450.711895947003
1
Iteration 11400: Loss = -12450.68135836036
Iteration 11500: Loss = -12450.681336634525
Iteration 11600: Loss = -12450.681316516886
Iteration 11700: Loss = -12450.681422922575
1
Iteration 11800: Loss = -12450.68134305209
Iteration 11900: Loss = -12450.684753777436
1
Iteration 12000: Loss = -12450.681263984612
Iteration 12100: Loss = -12450.681256711794
Iteration 12200: Loss = -12450.730943196413
1
Iteration 12300: Loss = -12450.681221278153
Iteration 12400: Loss = -12450.68118672796
Iteration 12500: Loss = -12450.683511385761
1
Iteration 12600: Loss = -12450.691848582968
2
Iteration 12700: Loss = -12450.687349891079
3
Iteration 12800: Loss = -12450.681164056106
Iteration 12900: Loss = -12450.713903651182
1
Iteration 13000: Loss = -12450.681093338422
Iteration 13100: Loss = -12450.725038323648
1
Iteration 13200: Loss = -12450.681092994884
Iteration 13300: Loss = -12450.682811865518
1
Iteration 13400: Loss = -12450.681091336144
Iteration 13500: Loss = -12450.681054334142
Iteration 13600: Loss = -12450.702535274597
1
Iteration 13700: Loss = -12450.681001191766
Iteration 13800: Loss = -12450.681046667602
Iteration 13900: Loss = -12450.685699257216
1
Iteration 14000: Loss = -12450.680994323839
Iteration 14100: Loss = -12450.681032822791
Iteration 14200: Loss = -12450.681928540362
1
Iteration 14300: Loss = -12450.680994028113
Iteration 14400: Loss = -12450.680977522687
Iteration 14500: Loss = -12450.681050128704
Iteration 14600: Loss = -12450.680958774232
Iteration 14700: Loss = -12450.680948916877
Iteration 14800: Loss = -12450.692853241975
1
Iteration 14900: Loss = -12450.68094294643
Iteration 15000: Loss = -12450.680926607132
Iteration 15100: Loss = -12450.735530451238
1
Iteration 15200: Loss = -12450.680937539575
Iteration 15300: Loss = -12450.680928450382
Iteration 15400: Loss = -12450.685636140057
1
Iteration 15500: Loss = -12450.680919781675
Iteration 15600: Loss = -12450.680994193992
Iteration 15700: Loss = -12450.680964575668
Iteration 15800: Loss = -12450.6809038995
Iteration 15900: Loss = -12451.251629041095
1
Iteration 16000: Loss = -12450.68089294445
Iteration 16100: Loss = -12450.680938290065
Iteration 16200: Loss = -12450.886252767483
1
Iteration 16300: Loss = -12450.680930495353
Iteration 16400: Loss = -12450.680888940835
Iteration 16500: Loss = -12450.680923018894
Iteration 16600: Loss = -12450.680971051133
Iteration 16700: Loss = -12450.680890522231
Iteration 16800: Loss = -12450.681085851085
1
Iteration 16900: Loss = -12450.680946525677
Iteration 17000: Loss = -12450.680899989007
Iteration 17100: Loss = -12450.73542887749
1
Iteration 17200: Loss = -12450.680925230916
Iteration 17300: Loss = -12450.680870384629
Iteration 17400: Loss = -12450.705436041957
1
Iteration 17500: Loss = -12450.680875363563
Iteration 17600: Loss = -12450.680885694896
Iteration 17700: Loss = -12450.962008414406
1
Iteration 17800: Loss = -12450.680895371937
Iteration 17900: Loss = -12450.680907518961
Iteration 18000: Loss = -12450.68250957123
1
Iteration 18100: Loss = -12450.680922164052
Iteration 18200: Loss = -12450.680866993123
Iteration 18300: Loss = -12450.680881876187
Iteration 18400: Loss = -12450.6811919395
1
Iteration 18500: Loss = -12450.680875751104
Iteration 18600: Loss = -12450.742801235992
1
Iteration 18700: Loss = -12450.680897475711
Iteration 18800: Loss = -12450.680853678778
Iteration 18900: Loss = -12450.693324219068
1
Iteration 19000: Loss = -12450.680882468661
Iteration 19100: Loss = -12450.68089404992
Iteration 19200: Loss = -12450.680889075402
Iteration 19300: Loss = -12450.681560666755
1
Iteration 19400: Loss = -12450.680919697234
Iteration 19500: Loss = -12450.735299812646
1
Iteration 19600: Loss = -12450.680858477323
Iteration 19700: Loss = -12450.69791635403
1
Iteration 19800: Loss = -12450.680840105493
Iteration 19900: Loss = -12450.6809359267
pi: tensor([[9.7177e-01, 2.8232e-02],
        [1.0000e+00, 4.2796e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9984, 0.0016], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2003, 0.1953],
         [0.6677, 0.2907]],

        [[0.6022, 0.3155],
         [0.6033, 0.6995]],

        [[0.7054, 0.1550],
         [0.6560, 0.6740]],

        [[0.5091, 0.2264],
         [0.6949, 0.5554]],

        [[0.5427, 0.2582],
         [0.7124, 0.6677]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008735738497905159
Average Adjusted Rand Index: -0.002274891251268548
11931.664360528048
[-0.0008735738497905159, -0.0008735738497905159] [-0.002274891251268548, -0.002274891251268548] [12450.680848917991, 12450.681139587936]
-------------------------------------
This iteration is 42
True Objective function: Loss = -11977.524330719172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22535.936813109565
Iteration 100: Loss = -12441.51002682539
Iteration 200: Loss = -12440.761997989897
Iteration 300: Loss = -12440.543516171063
Iteration 400: Loss = -12440.432659214026
Iteration 500: Loss = -12440.366527870887
Iteration 600: Loss = -12440.323199909239
Iteration 700: Loss = -12440.292730325513
Iteration 800: Loss = -12440.270025463973
Iteration 900: Loss = -12440.252231439179
Iteration 1000: Loss = -12440.237705247824
Iteration 1100: Loss = -12440.225316683754
Iteration 1200: Loss = -12440.214529075865
Iteration 1300: Loss = -12440.204856305807
Iteration 1400: Loss = -12440.196056642639
Iteration 1500: Loss = -12440.187831694651
Iteration 1600: Loss = -12440.180178667333
Iteration 1700: Loss = -12440.172952803421
Iteration 1800: Loss = -12440.165963877922
Iteration 1900: Loss = -12440.159168532782
Iteration 2000: Loss = -12440.152406581117
Iteration 2100: Loss = -12440.145438238564
Iteration 2200: Loss = -12440.13813906048
Iteration 2300: Loss = -12440.130076361414
Iteration 2400: Loss = -12440.120982430904
Iteration 2500: Loss = -12440.110162290985
Iteration 2600: Loss = -12440.09683830944
Iteration 2700: Loss = -12440.079666428717
Iteration 2800: Loss = -12440.056728180547
Iteration 2900: Loss = -12440.025505674581
Iteration 3000: Loss = -12439.984704599008
Iteration 3100: Loss = -12439.93606948066
Iteration 3200: Loss = -12439.884550984336
Iteration 3300: Loss = -12439.83941235059
Iteration 3400: Loss = -12439.810439405588
Iteration 3500: Loss = -12439.79612630815
Iteration 3600: Loss = -12439.788669867641
Iteration 3700: Loss = -12439.78385404938
Iteration 3800: Loss = -12439.780526470948
Iteration 3900: Loss = -12439.778196989619
Iteration 4000: Loss = -12439.776549035763
Iteration 4100: Loss = -12439.77531411664
Iteration 4200: Loss = -12439.774327840756
Iteration 4300: Loss = -12439.77354294598
Iteration 4400: Loss = -12439.772855517806
Iteration 4500: Loss = -12439.77224573453
Iteration 4600: Loss = -12439.771854621695
Iteration 4700: Loss = -12439.771170483029
Iteration 4800: Loss = -12439.770677764416
Iteration 4900: Loss = -12439.770232186615
Iteration 5000: Loss = -12439.769735402037
Iteration 5100: Loss = -12439.769329256435
Iteration 5200: Loss = -12439.769638936046
1
Iteration 5300: Loss = -12439.768473264723
Iteration 5400: Loss = -12439.76808794598
Iteration 5500: Loss = -12439.767788313924
Iteration 5600: Loss = -12439.76733840042
Iteration 5700: Loss = -12439.766902565703
Iteration 5800: Loss = -12439.76662539475
Iteration 5900: Loss = -12439.766230392008
Iteration 6000: Loss = -12439.765935606176
Iteration 6100: Loss = -12439.76566411805
Iteration 6200: Loss = -12439.765325241742
Iteration 6300: Loss = -12439.767989953738
1
Iteration 6400: Loss = -12439.764797443671
Iteration 6500: Loss = -12439.764545343533
Iteration 6600: Loss = -12439.764302618774
Iteration 6700: Loss = -12439.764064832914
Iteration 6800: Loss = -12439.767389049839
1
Iteration 6900: Loss = -12439.763627865716
Iteration 7000: Loss = -12439.763474402376
Iteration 7100: Loss = -12439.763240257607
Iteration 7200: Loss = -12439.76305742962
Iteration 7300: Loss = -12439.763859397292
1
Iteration 7400: Loss = -12439.810580580284
2
Iteration 7500: Loss = -12439.762623060242
Iteration 7600: Loss = -12439.762451839017
Iteration 7700: Loss = -12439.76229643704
Iteration 7800: Loss = -12439.762205175639
Iteration 7900: Loss = -12439.762055238903
Iteration 8000: Loss = -12439.77073752452
1
Iteration 8100: Loss = -12439.76174570583
Iteration 8200: Loss = -12439.761681914199
Iteration 8300: Loss = -12439.770200372415
1
Iteration 8400: Loss = -12439.761495958217
Iteration 8500: Loss = -12439.76141830407
Iteration 8600: Loss = -12439.806432579748
1
Iteration 8700: Loss = -12439.76125024052
Iteration 8800: Loss = -12439.76115331011
Iteration 8900: Loss = -12439.76123329675
Iteration 9000: Loss = -12439.76112294224
Iteration 9100: Loss = -12439.760945982973
Iteration 9200: Loss = -12439.760895696156
Iteration 9300: Loss = -12439.760828346623
Iteration 9400: Loss = -12439.760765374573
Iteration 9500: Loss = -12439.76178924504
1
Iteration 9600: Loss = -12439.760689594
Iteration 9700: Loss = -12439.760659942001
Iteration 9800: Loss = -12439.760590350179
Iteration 9900: Loss = -12439.76053995862
Iteration 10000: Loss = -12439.760473123115
Iteration 10100: Loss = -12439.760465926192
Iteration 10200: Loss = -12439.761666986686
1
Iteration 10300: Loss = -12439.76036578134
Iteration 10400: Loss = -12439.760357381383
Iteration 10500: Loss = -12439.760418668517
Iteration 10600: Loss = -12439.760264236678
Iteration 10700: Loss = -12439.815770199642
1
Iteration 10800: Loss = -12439.760800613487
2
Iteration 10900: Loss = -12439.760278008427
Iteration 11000: Loss = -12439.765003409744
1
Iteration 11100: Loss = -12439.760245977426
Iteration 11200: Loss = -12439.760946479244
1
Iteration 11300: Loss = -12439.76009337215
Iteration 11400: Loss = -12439.764775973073
1
Iteration 11500: Loss = -12439.84990556403
2
Iteration 11600: Loss = -12439.760055950486
Iteration 11700: Loss = -12439.76071749984
1
Iteration 11800: Loss = -12439.760092410843
Iteration 11900: Loss = -12439.75999477497
Iteration 12000: Loss = -12439.759971617059
Iteration 12100: Loss = -12439.760137563595
1
Iteration 12200: Loss = -12439.773100254393
2
Iteration 12300: Loss = -12439.759928338937
Iteration 12400: Loss = -12439.76186998081
1
Iteration 12500: Loss = -12439.759938737594
Iteration 12600: Loss = -12439.76006460186
1
Iteration 12700: Loss = -12439.759927453119
Iteration 12800: Loss = -12439.76068246983
1
Iteration 12900: Loss = -12439.801831571203
2
Iteration 13000: Loss = -12439.759875317046
Iteration 13100: Loss = -12439.761338902417
1
Iteration 13200: Loss = -12439.759852047942
Iteration 13300: Loss = -12439.759996962996
1
Iteration 13400: Loss = -12439.759802833127
Iteration 13500: Loss = -12439.760112374364
1
Iteration 13600: Loss = -12439.768867758676
2
Iteration 13700: Loss = -12439.760085820557
3
Iteration 13800: Loss = -12439.759918078611
4
Iteration 13900: Loss = -12439.844033517442
5
Iteration 14000: Loss = -12439.759839381826
Iteration 14100: Loss = -12439.767682319773
1
Iteration 14200: Loss = -12439.759805473099
Iteration 14300: Loss = -12439.75983207375
Iteration 14400: Loss = -12439.7599657067
1
Iteration 14500: Loss = -12439.75979235982
Iteration 14600: Loss = -12439.76664401516
1
Iteration 14700: Loss = -12439.876838448845
2
Iteration 14800: Loss = -12439.759777535111
Iteration 14900: Loss = -12439.759961019641
1
Iteration 15000: Loss = -12439.759976607811
2
Iteration 15100: Loss = -12439.759952201053
3
Iteration 15200: Loss = -12439.760003656884
4
Iteration 15300: Loss = -12439.759888446832
5
Iteration 15400: Loss = -12439.759757666576
Iteration 15500: Loss = -12439.760585607524
1
Iteration 15600: Loss = -12439.765640152656
2
Iteration 15700: Loss = -12439.773674384243
3
Iteration 15800: Loss = -12439.759779024798
Iteration 15900: Loss = -12439.763118924957
1
Iteration 16000: Loss = -12439.759741058264
Iteration 16100: Loss = -12439.76199739647
1
Iteration 16200: Loss = -12439.759731239992
Iteration 16300: Loss = -12439.762835188643
1
Iteration 16400: Loss = -12439.759758848933
Iteration 16500: Loss = -12439.759769893017
Iteration 16600: Loss = -12439.759703983591
Iteration 16700: Loss = -12439.760208157184
1
Iteration 16800: Loss = -12439.759755943356
Iteration 16900: Loss = -12439.766520227864
1
Iteration 17000: Loss = -12439.759709966036
Iteration 17100: Loss = -12439.759975361369
1
Iteration 17200: Loss = -12439.759793213432
Iteration 17300: Loss = -12439.763085073258
1
Iteration 17400: Loss = -12439.75973048037
Iteration 17500: Loss = -12439.77996799236
1
Iteration 17600: Loss = -12439.759818735649
Iteration 17700: Loss = -12439.75966223634
Iteration 17800: Loss = -12439.769220422137
1
Iteration 17900: Loss = -12439.759718050771
Iteration 18000: Loss = -12439.75974657799
Iteration 18100: Loss = -12439.761303495181
1
Iteration 18200: Loss = -12439.769308947642
2
Iteration 18300: Loss = -12439.759734880696
Iteration 18400: Loss = -12439.76047766476
1
Iteration 18500: Loss = -12439.759706527213
Iteration 18600: Loss = -12439.83344574361
1
Iteration 18700: Loss = -12439.759761222032
Iteration 18800: Loss = -12439.85117713082
1
Iteration 18900: Loss = -12439.759730890879
Iteration 19000: Loss = -12439.759741718533
Iteration 19100: Loss = -12439.76009813986
1
Iteration 19200: Loss = -12439.760646024395
2
Iteration 19300: Loss = -12439.75979897698
Iteration 19400: Loss = -12439.759739602501
Iteration 19500: Loss = -12439.76083534639
1
Iteration 19600: Loss = -12439.765154026278
2
Iteration 19700: Loss = -12439.759801942619
Iteration 19800: Loss = -12439.761122402359
1
Iteration 19900: Loss = -12439.759756076843
pi: tensor([[9.2972e-01, 7.0279e-02],
        [9.9996e-01, 3.9271e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9988, 0.0012], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2034, 0.2083],
         [0.5074, 0.1634]],

        [[0.5444, 0.1726],
         [0.5554, 0.5538]],

        [[0.6595, 0.1693],
         [0.5409, 0.5045]],

        [[0.6103, 0.1990],
         [0.6744, 0.6317]],

        [[0.6099, 0.1852],
         [0.7270, 0.6788]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22528.799190891958
Iteration 100: Loss = -12441.050437310303
Iteration 200: Loss = -12440.43979463975
Iteration 300: Loss = -12440.303575274995
Iteration 400: Loss = -12440.242630240811
Iteration 500: Loss = -12440.209767417142
Iteration 600: Loss = -12440.188877718545
Iteration 700: Loss = -12440.173492600657
Iteration 800: Loss = -12440.160936490573
Iteration 900: Loss = -12440.149868293936
Iteration 1000: Loss = -12440.139468139163
Iteration 1100: Loss = -12440.129214250896
Iteration 1200: Loss = -12440.118561255636
Iteration 1300: Loss = -12440.106774875532
Iteration 1400: Loss = -12440.093057013242
Iteration 1500: Loss = -12440.076053724832
Iteration 1600: Loss = -12440.054374402729
Iteration 1700: Loss = -12440.025614148755
Iteration 1800: Loss = -12439.987478744612
Iteration 1900: Loss = -12439.940577174666
Iteration 2000: Loss = -12439.890436459094
Iteration 2100: Loss = -12439.845803679777
Iteration 2200: Loss = -12439.81439738211
Iteration 2300: Loss = -12439.797213349122
Iteration 2400: Loss = -12439.788348880222
Iteration 2500: Loss = -12439.782998510675
Iteration 2600: Loss = -12439.779196741054
Iteration 2700: Loss = -12439.776495001355
Iteration 2800: Loss = -12439.774331075789
Iteration 2900: Loss = -12439.772762151908
Iteration 3000: Loss = -12439.77151820114
Iteration 3100: Loss = -12439.770489684483
Iteration 3200: Loss = -12439.76969342023
Iteration 3300: Loss = -12439.768963913119
Iteration 3400: Loss = -12439.768392572632
Iteration 3500: Loss = -12439.76785587769
Iteration 3600: Loss = -12439.7673784536
Iteration 3700: Loss = -12439.766919605376
Iteration 3800: Loss = -12439.766568605535
Iteration 3900: Loss = -12439.766195921948
Iteration 4000: Loss = -12439.76584592335
Iteration 4100: Loss = -12439.76546717718
Iteration 4200: Loss = -12439.7651553285
Iteration 4300: Loss = -12439.764856956637
Iteration 4400: Loss = -12439.765171649009
1
Iteration 4500: Loss = -12439.764350800593
Iteration 4600: Loss = -12439.76407748086
Iteration 4700: Loss = -12439.763851452315
Iteration 4800: Loss = -12439.763610781505
Iteration 4900: Loss = -12439.767384163213
1
Iteration 5000: Loss = -12439.763258830451
Iteration 5100: Loss = -12439.763045882104
Iteration 5200: Loss = -12439.762958254054
Iteration 5300: Loss = -12439.762697365268
Iteration 5400: Loss = -12439.762515967881
Iteration 5500: Loss = -12439.76239114156
Iteration 5600: Loss = -12439.762255943037
Iteration 5700: Loss = -12439.762113271932
Iteration 5800: Loss = -12439.762013825115
Iteration 5900: Loss = -12439.761896749536
Iteration 6000: Loss = -12439.762728829463
1
Iteration 6100: Loss = -12439.7616407876
Iteration 6200: Loss = -12439.761562270896
Iteration 6300: Loss = -12439.761473605548
Iteration 6400: Loss = -12439.761318208708
Iteration 6500: Loss = -12439.76705171185
1
Iteration 6600: Loss = -12439.761201089897
Iteration 6700: Loss = -12439.761110192001
Iteration 6800: Loss = -12439.761074819124
Iteration 6900: Loss = -12439.760975308216
Iteration 7000: Loss = -12439.760970633763
Iteration 7100: Loss = -12439.76082979733
Iteration 7200: Loss = -12439.769631524696
1
Iteration 7300: Loss = -12439.760726179187
Iteration 7400: Loss = -12439.760667077297
Iteration 7500: Loss = -12439.760620848601
Iteration 7600: Loss = -12439.760556677673
Iteration 7700: Loss = -12439.760662324547
1
Iteration 7800: Loss = -12439.788750963471
2
Iteration 7900: Loss = -12439.76060078597
Iteration 8000: Loss = -12439.76052802241
Iteration 8100: Loss = -12439.761219520402
1
Iteration 8200: Loss = -12439.760317656324
Iteration 8300: Loss = -12439.765511495243
1
Iteration 8400: Loss = -12439.760305188709
Iteration 8500: Loss = -12439.76360964334
1
Iteration 8600: Loss = -12439.760277375291
Iteration 8700: Loss = -12439.760157351757
Iteration 8800: Loss = -12439.781586802925
1
Iteration 8900: Loss = -12439.760154627169
Iteration 9000: Loss = -12439.760133163469
Iteration 9100: Loss = -12439.76161374376
1
Iteration 9200: Loss = -12439.760103490542
Iteration 9300: Loss = -12439.760090587019
Iteration 9400: Loss = -12439.760179301904
Iteration 9500: Loss = -12439.760030150233
Iteration 9600: Loss = -12439.760030785588
Iteration 9700: Loss = -12439.760111936388
Iteration 9800: Loss = -12439.759987383444
Iteration 9900: Loss = -12439.759931855418
Iteration 10000: Loss = -12439.76079093497
1
Iteration 10100: Loss = -12439.75994399482
Iteration 10200: Loss = -12439.760001454024
Iteration 10300: Loss = -12439.796828244973
1
Iteration 10400: Loss = -12439.76527320203
2
Iteration 10500: Loss = -12439.75994802039
Iteration 10600: Loss = -12439.76607535283
1
Iteration 10700: Loss = -12439.759927424888
Iteration 10800: Loss = -12439.76384279506
1
Iteration 10900: Loss = -12439.759845750912
Iteration 11000: Loss = -12439.771810747921
1
Iteration 11100: Loss = -12439.821867489858
2
Iteration 11200: Loss = -12439.75989013394
Iteration 11300: Loss = -12439.759993306952
1
Iteration 11400: Loss = -12439.82564848798
2
Iteration 11500: Loss = -12439.75981121201
Iteration 11600: Loss = -12439.835049343033
1
Iteration 11700: Loss = -12439.782712664535
2
Iteration 11800: Loss = -12439.854635528838
3
Iteration 11900: Loss = -12439.759812615044
Iteration 12000: Loss = -12439.761289596645
1
Iteration 12100: Loss = -12439.770950727503
2
Iteration 12200: Loss = -12439.75982243301
Iteration 12300: Loss = -12439.766227989023
1
Iteration 12400: Loss = -12439.759802904488
Iteration 12500: Loss = -12439.761426685833
1
Iteration 12600: Loss = -12439.759779691283
Iteration 12700: Loss = -12439.761983550372
1
Iteration 12800: Loss = -12439.759800150701
Iteration 12900: Loss = -12439.75985749146
Iteration 13000: Loss = -12439.759774404489
Iteration 13100: Loss = -12439.763666671035
1
Iteration 13200: Loss = -12439.759791710047
Iteration 13300: Loss = -12439.761052048181
1
Iteration 13400: Loss = -12439.759776912673
Iteration 13500: Loss = -12439.787510503707
1
Iteration 13600: Loss = -12439.759908497486
2
Iteration 13700: Loss = -12439.759821114394
Iteration 13800: Loss = -12439.759772736958
Iteration 13900: Loss = -12439.759841466554
Iteration 14000: Loss = -12439.78023752645
1
Iteration 14100: Loss = -12439.759723736928
Iteration 14200: Loss = -12439.847268667761
1
Iteration 14300: Loss = -12439.759758584127
Iteration 14400: Loss = -12439.762970378672
1
Iteration 14500: Loss = -12439.759755049
Iteration 14600: Loss = -12439.770835262745
1
Iteration 14700: Loss = -12439.759821353879
Iteration 14800: Loss = -12439.771029535785
1
Iteration 14900: Loss = -12439.759725543223
Iteration 15000: Loss = -12439.76180264827
1
Iteration 15100: Loss = -12439.759795853259
Iteration 15200: Loss = -12439.807583848471
1
Iteration 15300: Loss = -12439.759778585423
Iteration 15400: Loss = -12439.75971722336
Iteration 15500: Loss = -12439.760597192137
1
Iteration 15600: Loss = -12439.760420700473
2
Iteration 15700: Loss = -12439.759811415586
Iteration 15800: Loss = -12439.759728247662
Iteration 15900: Loss = -12439.764635826245
1
Iteration 16000: Loss = -12440.090077914154
2
Iteration 16100: Loss = -12439.759774723634
Iteration 16200: Loss = -12440.145637911533
1
Iteration 16300: Loss = -12439.759776588886
Iteration 16400: Loss = -12439.851191026974
1
Iteration 16500: Loss = -12439.759735060341
Iteration 16600: Loss = -12439.759739523191
Iteration 16700: Loss = -12439.75981581408
Iteration 16800: Loss = -12439.830524431773
1
Iteration 16900: Loss = -12439.759719447877
Iteration 17000: Loss = -12439.792791902903
1
Iteration 17100: Loss = -12439.759738513372
Iteration 17200: Loss = -12439.781746772955
1
Iteration 17300: Loss = -12439.75976201589
Iteration 17400: Loss = -12439.759709890568
Iteration 17500: Loss = -12439.764061845794
1
Iteration 17600: Loss = -12439.759713261325
Iteration 17700: Loss = -12439.759854258607
1
Iteration 17800: Loss = -12439.759743795483
Iteration 17900: Loss = -12439.75986466422
1
Iteration 18000: Loss = -12439.759812427807
Iteration 18100: Loss = -12439.759823408082
Iteration 18200: Loss = -12439.759741654829
Iteration 18300: Loss = -12439.761220805858
1
Iteration 18400: Loss = -12439.777777817537
2
Iteration 18500: Loss = -12439.759753333097
Iteration 18600: Loss = -12439.759882955283
1
Iteration 18700: Loss = -12439.763365956478
2
Iteration 18800: Loss = -12439.759833536262
Iteration 18900: Loss = -12439.759700001732
Iteration 19000: Loss = -12439.770659891728
1
Iteration 19100: Loss = -12439.759779443673
Iteration 19200: Loss = -12439.760446847713
1
Iteration 19300: Loss = -12439.759820308202
Iteration 19400: Loss = -12439.759737094755
Iteration 19500: Loss = -12439.762375985923
1
Iteration 19600: Loss = -12439.759766460156
Iteration 19700: Loss = -12439.759729406507
Iteration 19800: Loss = -12439.759671041016
Iteration 19900: Loss = -12439.759917001113
1
pi: tensor([[1.4260e-05, 9.9999e-01],
        [7.3863e-02, 9.2614e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0013, 0.9987], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1631, 0.2071],
         [0.7217, 0.2046]],

        [[0.6214, 0.1731],
         [0.6628, 0.5341]],

        [[0.5196, 0.1694],
         [0.6613, 0.5175]],

        [[0.6367, 0.1979],
         [0.5165, 0.7059]],

        [[0.6096, 0.1844],
         [0.6835, 0.5177]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
11977.524330719172
[0.0, 0.0] [0.0, 0.0] [12439.877164227102, 12439.770384845704]
-------------------------------------
This iteration is 43
True Objective function: Loss = -11722.525122567185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23024.432907723876
Iteration 100: Loss = -12258.68743428645
Iteration 200: Loss = -12258.0697036811
Iteration 300: Loss = -12257.921993634325
Iteration 400: Loss = -12257.853651709689
Iteration 500: Loss = -12257.81414081115
Iteration 600: Loss = -12257.786853012054
Iteration 700: Loss = -12257.765732979056
Iteration 800: Loss = -12257.74809236469
Iteration 900: Loss = -12257.73289171779
Iteration 1000: Loss = -12257.719787383628
Iteration 1100: Loss = -12257.708602107617
Iteration 1200: Loss = -12257.699043209479
Iteration 1300: Loss = -12257.690669090698
Iteration 1400: Loss = -12257.682948041449
Iteration 1500: Loss = -12257.675400308117
Iteration 1600: Loss = -12257.667484447642
Iteration 1700: Loss = -12257.658599844503
Iteration 1800: Loss = -12257.648043324496
Iteration 1900: Loss = -12257.634847523226
Iteration 2000: Loss = -12257.61769151116
Iteration 2100: Loss = -12257.595040931068
Iteration 2200: Loss = -12257.565485291248
Iteration 2300: Loss = -12257.529839417088
Iteration 2400: Loss = -12257.49246077881
Iteration 2500: Loss = -12257.453028010334
Iteration 2600: Loss = -12257.40529325042
Iteration 2700: Loss = -12257.341025204336
Iteration 2800: Loss = -12257.25241910478
Iteration 2900: Loss = -12257.14253706378
Iteration 3000: Loss = -12257.050664575187
Iteration 3100: Loss = -12257.002236592909
Iteration 3200: Loss = -12256.979731299814
Iteration 3300: Loss = -12256.96421722588
Iteration 3400: Loss = -12257.02613642953
1
Iteration 3500: Loss = -12256.949380461165
Iteration 3600: Loss = -12256.944848229941
Iteration 3700: Loss = -12256.94211360858
Iteration 3800: Loss = -12256.939117504773
Iteration 3900: Loss = -12256.936983189169
Iteration 4000: Loss = -12256.935407001647
Iteration 4100: Loss = -12256.93380273246
Iteration 4200: Loss = -12256.936825217945
1
Iteration 4300: Loss = -12256.931393585934
Iteration 4400: Loss = -12256.930478015249
Iteration 4500: Loss = -12256.931051512467
1
Iteration 4600: Loss = -12256.929195450855
Iteration 4700: Loss = -12256.928058459167
Iteration 4800: Loss = -12256.927920791402
Iteration 4900: Loss = -12256.92700332446
Iteration 5000: Loss = -12256.926195564653
Iteration 5100: Loss = -12256.925893335574
Iteration 5200: Loss = -12256.92536458508
Iteration 5300: Loss = -12256.93953741349
1
Iteration 5400: Loss = -12256.924139729443
Iteration 5500: Loss = -12256.984216051662
1
Iteration 5600: Loss = -12256.92331736159
Iteration 5700: Loss = -12256.932388674913
1
Iteration 5800: Loss = -12256.923352203346
Iteration 5900: Loss = -12256.926746364405
1
Iteration 6000: Loss = -12256.922193602315
Iteration 6100: Loss = -12256.952269591036
1
Iteration 6200: Loss = -12256.921608164917
Iteration 6300: Loss = -12256.923981065607
1
Iteration 6400: Loss = -12256.921248324086
Iteration 6500: Loss = -12256.973575305286
1
Iteration 6600: Loss = -12256.92101503585
Iteration 6700: Loss = -12256.920955702677
Iteration 6800: Loss = -12256.923209711693
1
Iteration 6900: Loss = -12256.920820844069
Iteration 7000: Loss = -12256.920761212417
Iteration 7100: Loss = -12256.920956791904
1
Iteration 7200: Loss = -12256.920711477605
Iteration 7300: Loss = -12256.936250638368
1
Iteration 7400: Loss = -12256.920700433606
Iteration 7500: Loss = -12256.920661856097
Iteration 7600: Loss = -12256.9207085187
Iteration 7700: Loss = -12256.920959114119
1
Iteration 7800: Loss = -12256.920881605007
2
Iteration 7900: Loss = -12257.0600300743
3
Iteration 8000: Loss = -12256.929713220887
4
Iteration 8100: Loss = -12256.920676942162
Iteration 8200: Loss = -12256.920892351607
1
Iteration 8300: Loss = -12256.920949138377
2
Iteration 8400: Loss = -12256.921066917545
3
Iteration 8500: Loss = -12256.920714455466
Iteration 8600: Loss = -12256.923010407812
1
Iteration 8700: Loss = -12256.929084222937
2
Iteration 8800: Loss = -12256.922093964606
3
Iteration 8900: Loss = -12256.938454681827
4
Iteration 9000: Loss = -12256.92077725114
Iteration 9100: Loss = -12257.115032775173
1
Iteration 9200: Loss = -12256.92440587507
2
Iteration 9300: Loss = -12257.1002776926
3
Iteration 9400: Loss = -12256.936418027259
4
Iteration 9500: Loss = -12256.937410299353
5
Iteration 9600: Loss = -12256.921754144432
6
Iteration 9700: Loss = -12256.92425901538
7
Iteration 9800: Loss = -12256.929438637324
8
Iteration 9900: Loss = -12256.93864004137
9
Iteration 10000: Loss = -12256.932049749505
10
Iteration 10100: Loss = -12256.92842772727
11
Iteration 10200: Loss = -12256.92482532941
12
Iteration 10300: Loss = -12256.931224767246
13
Iteration 10400: Loss = -12256.920862662719
Iteration 10500: Loss = -12256.921047876082
1
Iteration 10600: Loss = -12256.923371732115
2
Iteration 10700: Loss = -12257.031337041859
3
Iteration 10800: Loss = -12256.933707884093
4
Iteration 10900: Loss = -12256.921338401544
5
Iteration 11000: Loss = -12256.92197166277
6
Iteration 11100: Loss = -12256.921363133311
7
Iteration 11200: Loss = -12256.94469355714
8
Iteration 11300: Loss = -12256.92165763016
9
Iteration 11400: Loss = -12256.925146089881
10
Iteration 11500: Loss = -12256.949615946422
11
Iteration 11600: Loss = -12256.924147289339
12
Iteration 11700: Loss = -12257.008036190393
13
Iteration 11800: Loss = -12256.920801199254
Iteration 11900: Loss = -12256.922309623045
1
Iteration 12000: Loss = -12256.923009784792
2
Iteration 12100: Loss = -12257.165816141058
3
Iteration 12200: Loss = -12256.947600196117
4
Iteration 12300: Loss = -12256.9254096763
5
Iteration 12400: Loss = -12256.92230145978
6
Iteration 12500: Loss = -12256.950875218472
7
Iteration 12600: Loss = -12256.948219888503
8
Iteration 12700: Loss = -12256.95086588124
9
Iteration 12800: Loss = -12256.92072108059
Iteration 12900: Loss = -12256.922313991337
1
Iteration 13000: Loss = -12256.923744332666
2
Iteration 13100: Loss = -12256.921201234843
3
Iteration 13200: Loss = -12256.924245793682
4
Iteration 13300: Loss = -12256.925239815944
5
Iteration 13400: Loss = -12256.930801236635
6
Iteration 13500: Loss = -12256.92317747857
7
Iteration 13600: Loss = -12256.980150440102
8
Iteration 13700: Loss = -12256.92852075783
9
Iteration 13800: Loss = -12256.964115751347
10
Iteration 13900: Loss = -12256.925416158956
11
Iteration 14000: Loss = -12256.920954336907
12
Iteration 14100: Loss = -12256.927306015274
13
Iteration 14200: Loss = -12257.025791609442
14
Iteration 14300: Loss = -12256.930488177886
15
Stopping early at iteration 14300 due to no improvement.
pi: tensor([[0.4177, 0.5823],
        [0.5808, 0.4192]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9934, 0.0066], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1969, 0.2316],
         [0.6151, 0.1953]],

        [[0.5126, 0.1943],
         [0.5021, 0.6158]],

        [[0.6830, 0.1903],
         [0.6203, 0.6524]],

        [[0.7004, 0.1959],
         [0.5139, 0.6800]],

        [[0.6710, 0.2038],
         [0.5090, 0.5091]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.019259861732411548
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.009483393045217982
Global Adjusted Rand Index: -0.0003982303088445522
Average Adjusted Rand Index: 0.0019552937374387133
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22760.186393223707
Iteration 100: Loss = -12258.154082272775
Iteration 200: Loss = -12257.904159097228
Iteration 300: Loss = -12257.833583309883
Iteration 400: Loss = -12257.79306257481
Iteration 500: Loss = -12257.76457013085
Iteration 600: Loss = -12257.740763928374
Iteration 700: Loss = -12257.718869394062
Iteration 800: Loss = -12257.698190697798
Iteration 900: Loss = -12257.678056036406
Iteration 1000: Loss = -12257.658285789754
Iteration 1100: Loss = -12257.638906878232
Iteration 1200: Loss = -12257.619990054101
Iteration 1300: Loss = -12257.601656963629
Iteration 1400: Loss = -12257.583998314793
Iteration 1500: Loss = -12257.56677288399
Iteration 1600: Loss = -12257.549443493592
Iteration 1700: Loss = -12257.531475457388
Iteration 1800: Loss = -12257.512388906545
Iteration 1900: Loss = -12257.491720536154
Iteration 2000: Loss = -12257.469375613851
Iteration 2100: Loss = -12257.444872068258
Iteration 2200: Loss = -12257.417643977102
Iteration 2300: Loss = -12257.386914067112
Iteration 2400: Loss = -12257.351509331367
Iteration 2500: Loss = -12257.310786677004
Iteration 2600: Loss = -12257.264375519255
Iteration 2700: Loss = -12257.213483227717
Iteration 2800: Loss = -12257.161514787946
Iteration 2900: Loss = -12257.113944339982
Iteration 3000: Loss = -12257.074636170913
Iteration 3100: Loss = -12257.043839328331
Iteration 3200: Loss = -12257.020218248212
Iteration 3300: Loss = -12257.008226159065
Iteration 3400: Loss = -12256.989081779144
Iteration 3500: Loss = -12256.978573751912
Iteration 3600: Loss = -12256.970717548616
Iteration 3700: Loss = -12256.964147458883
Iteration 3800: Loss = -12256.962357785405
Iteration 3900: Loss = -12256.954883707487
Iteration 4000: Loss = -12256.951270439404
Iteration 4100: Loss = -12256.952014739903
1
Iteration 4200: Loss = -12256.945831999781
Iteration 4300: Loss = -12256.943599359696
Iteration 4400: Loss = -12256.941734485079
Iteration 4500: Loss = -12256.940038862347
Iteration 4600: Loss = -12256.938549470053
Iteration 4700: Loss = -12256.937285545684
Iteration 4800: Loss = -12256.936037523972
Iteration 4900: Loss = -12256.949319206838
1
Iteration 5000: Loss = -12256.934012369591
Iteration 5100: Loss = -12256.934478425032
1
Iteration 5200: Loss = -12256.932999788089
Iteration 5300: Loss = -12256.931620851936
Iteration 5400: Loss = -12256.93092940046
Iteration 5500: Loss = -12256.930288587751
Iteration 5600: Loss = -12256.930849364358
1
Iteration 5700: Loss = -12256.932458251327
2
Iteration 5800: Loss = -12256.928566391576
Iteration 5900: Loss = -12256.928126777171
Iteration 6000: Loss = -12256.9276560638
Iteration 6100: Loss = -12256.928114999
1
Iteration 6200: Loss = -12256.949783356535
2
Iteration 6300: Loss = -12256.9262757085
Iteration 6400: Loss = -12256.925915569065
Iteration 6500: Loss = -12256.925723923201
Iteration 6600: Loss = -12256.925190094184
Iteration 6700: Loss = -12256.927320287181
1
Iteration 6800: Loss = -12256.92444426801
Iteration 6900: Loss = -12256.92466326793
1
Iteration 7000: Loss = -12256.923976961658
Iteration 7100: Loss = -12256.923691401344
Iteration 7200: Loss = -12256.930443152587
1
Iteration 7300: Loss = -12256.922791651845
Iteration 7400: Loss = -12256.923219718683
1
Iteration 7500: Loss = -12256.927631074243
2
Iteration 7600: Loss = -12256.921991335958
Iteration 7700: Loss = -12256.922336083553
1
Iteration 7800: Loss = -12256.921598540572
Iteration 7900: Loss = -12256.92151589666
Iteration 8000: Loss = -12256.921304316538
Iteration 8100: Loss = -12256.921340538604
Iteration 8200: Loss = -12256.921582623507
1
Iteration 8300: Loss = -12256.92145111013
2
Iteration 8400: Loss = -12257.019726918074
3
Iteration 8500: Loss = -12256.945478653939
4
Iteration 8600: Loss = -12256.920816201426
Iteration 8700: Loss = -12256.924397011135
1
Iteration 8800: Loss = -12256.940608638473
2
Iteration 8900: Loss = -12256.929314480129
3
Iteration 9000: Loss = -12256.944040344852
4
Iteration 9100: Loss = -12256.921381227576
5
Iteration 9200: Loss = -12256.927174520182
6
Iteration 9300: Loss = -12256.92067183067
Iteration 9400: Loss = -12256.92877987818
1
Iteration 9500: Loss = -12256.977060840538
2
Iteration 9600: Loss = -12256.924232304438
3
Iteration 9700: Loss = -12256.920956600688
4
Iteration 9800: Loss = -12256.926121580284
5
Iteration 9900: Loss = -12257.017550641815
6
Iteration 10000: Loss = -12256.921681537884
7
Iteration 10100: Loss = -12256.92160951277
8
Iteration 10200: Loss = -12256.93605344965
9
Iteration 10300: Loss = -12257.001776860272
10
Iteration 10400: Loss = -12256.92151789264
11
Iteration 10500: Loss = -12256.92084734012
12
Iteration 10600: Loss = -12256.925417094944
13
Iteration 10700: Loss = -12256.92184416083
14
Iteration 10800: Loss = -12256.92498038165
15
Stopping early at iteration 10800 due to no improvement.
pi: tensor([[0.4274, 0.5726],
        [0.5813, 0.4187]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9933, 0.0067], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1967, 0.2317],
         [0.5818, 0.1954]],

        [[0.5157, 0.1943],
         [0.6086, 0.6203]],

        [[0.7026, 0.1903],
         [0.6646, 0.6369]],

        [[0.5085, 0.1952],
         [0.5540, 0.7013]],

        [[0.6036, 0.2038],
         [0.6909, 0.7194]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.008873483535528597
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.017390386003203494
Global Adjusted Rand Index: -0.0019966909686451088
Average Adjusted Rand Index: 0.0017033804935349794
11722.525122567185
[-0.0003982303088445522, -0.0019966909686451088] [0.0019552937374387133, 0.0017033804935349794] [12256.930488177886, 12256.92498038165]
-------------------------------------
This iteration is 44
True Objective function: Loss = -11844.185237769521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21429.70731339529
Iteration 100: Loss = -12318.76470526152
Iteration 200: Loss = -12318.073845218527
Iteration 300: Loss = -12317.795475171848
Iteration 400: Loss = -12317.607859098243
Iteration 500: Loss = -12317.463501463972
Iteration 600: Loss = -12317.352144893042
Iteration 700: Loss = -12317.256430634745
Iteration 800: Loss = -12317.16079600123
Iteration 900: Loss = -12317.04270465631
Iteration 1000: Loss = -12316.854888184851
Iteration 1100: Loss = -12316.679403441987
Iteration 1200: Loss = -12316.5886081602
Iteration 1300: Loss = -12316.532532989542
Iteration 1400: Loss = -12316.491523294671
Iteration 1500: Loss = -12316.45711792521
Iteration 1600: Loss = -12316.425202086992
Iteration 1700: Loss = -12316.39327854232
Iteration 1800: Loss = -12316.359938004458
Iteration 1900: Loss = -12316.325758250332
Iteration 2000: Loss = -12316.29342829796
Iteration 2100: Loss = -12316.265949027304
Iteration 2200: Loss = -12316.244474154662
Iteration 2300: Loss = -12316.228612579194
Iteration 2400: Loss = -12316.217362374413
Iteration 2500: Loss = -12316.211759507021
Iteration 2600: Loss = -12316.204090645431
Iteration 2700: Loss = -12316.200245994234
Iteration 2800: Loss = -12316.198290155553
Iteration 2900: Loss = -12316.19535360401
Iteration 3000: Loss = -12316.1937598034
Iteration 3100: Loss = -12316.192734869479
Iteration 3200: Loss = -12316.191489953737
Iteration 3300: Loss = -12316.19057360371
Iteration 3400: Loss = -12316.18990322548
Iteration 3500: Loss = -12316.189095760628
Iteration 3600: Loss = -12316.18850721134
Iteration 3700: Loss = -12316.18788783339
Iteration 3800: Loss = -12316.18731047566
Iteration 3900: Loss = -12316.186766004836
Iteration 4000: Loss = -12316.186232476006
Iteration 4100: Loss = -12316.185725502519
Iteration 4200: Loss = -12316.185208857572
Iteration 4300: Loss = -12316.18476574923
Iteration 4400: Loss = -12316.184256969122
Iteration 4500: Loss = -12316.183902129018
Iteration 4600: Loss = -12316.183475674094
Iteration 4700: Loss = -12316.183066223066
Iteration 4800: Loss = -12316.183746828903
1
Iteration 4900: Loss = -12316.182306939036
Iteration 5000: Loss = -12316.181960199456
Iteration 5100: Loss = -12316.182004005377
Iteration 5200: Loss = -12316.181369650667
Iteration 5300: Loss = -12316.181051661943
Iteration 5400: Loss = -12316.180786854638
Iteration 5500: Loss = -12316.18049557996
Iteration 5600: Loss = -12316.180255428426
Iteration 5700: Loss = -12316.180000675064
Iteration 5800: Loss = -12316.179753892677
Iteration 5900: Loss = -12316.179536355226
Iteration 6000: Loss = -12316.179357134044
Iteration 6100: Loss = -12316.179141387527
Iteration 6200: Loss = -12316.180756425636
1
Iteration 6300: Loss = -12316.178775048918
Iteration 6400: Loss = -12316.180118867565
1
Iteration 6500: Loss = -12316.178441080845
Iteration 6600: Loss = -12316.1800311295
1
Iteration 6700: Loss = -12316.178144048428
Iteration 6800: Loss = -12316.177984213207
Iteration 6900: Loss = -12316.178290479229
1
Iteration 7000: Loss = -12316.177884626235
Iteration 7100: Loss = -12316.177744395283
Iteration 7200: Loss = -12316.177542604635
Iteration 7300: Loss = -12316.177642538905
Iteration 7400: Loss = -12316.178358376777
1
Iteration 7500: Loss = -12316.19554732605
2
Iteration 7600: Loss = -12316.354231203546
3
Iteration 7700: Loss = -12316.176998317316
Iteration 7800: Loss = -12316.177344315523
1
Iteration 7900: Loss = -12316.176851179129
Iteration 8000: Loss = -12316.176784362426
Iteration 8100: Loss = -12316.176666654485
Iteration 8200: Loss = -12316.236086946545
1
Iteration 8300: Loss = -12316.17655211786
Iteration 8400: Loss = -12316.176499492409
Iteration 8500: Loss = -12316.176745614608
1
Iteration 8600: Loss = -12316.176370830926
Iteration 8700: Loss = -12316.177991142962
1
Iteration 8800: Loss = -12316.176303363744
Iteration 8900: Loss = -12316.178912660553
1
Iteration 9000: Loss = -12316.179450801746
2
Iteration 9100: Loss = -12316.244535281154
3
Iteration 9200: Loss = -12316.176135085916
Iteration 9300: Loss = -12316.176364455894
1
Iteration 9400: Loss = -12316.182856757869
2
Iteration 9500: Loss = -12316.175978509602
Iteration 9600: Loss = -12316.176271227116
1
Iteration 9700: Loss = -12316.199731566438
2
Iteration 9800: Loss = -12316.180039711655
3
Iteration 9900: Loss = -12316.201479039191
4
Iteration 10000: Loss = -12316.180992395512
5
Iteration 10100: Loss = -12316.175989184907
Iteration 10200: Loss = -12316.175826630613
Iteration 10300: Loss = -12316.176620280681
1
Iteration 10400: Loss = -12316.176136169923
2
Iteration 10500: Loss = -12316.194894128574
3
Iteration 10600: Loss = -12316.284070214926
4
Iteration 10700: Loss = -12316.183456512532
5
Iteration 10800: Loss = -12316.175681599248
Iteration 10900: Loss = -12316.17731401527
1
Iteration 11000: Loss = -12316.17600532351
2
Iteration 11100: Loss = -12316.175617038538
Iteration 11200: Loss = -12316.231281697817
1
Iteration 11300: Loss = -12316.17559881514
Iteration 11400: Loss = -12316.180318622275
1
Iteration 11500: Loss = -12316.17556010815
Iteration 11600: Loss = -12316.189390912767
1
Iteration 11700: Loss = -12316.175535148594
Iteration 11800: Loss = -12316.176070554457
1
Iteration 11900: Loss = -12316.175581555752
Iteration 12000: Loss = -12316.175473645228
Iteration 12100: Loss = -12316.177623300351
1
Iteration 12200: Loss = -12316.175481036207
Iteration 12300: Loss = -12316.177366504846
1
Iteration 12400: Loss = -12316.175517659629
Iteration 12500: Loss = -12316.175468847041
Iteration 12600: Loss = -12316.176009081986
1
Iteration 12700: Loss = -12316.175428371727
Iteration 12800: Loss = -12316.304622378195
1
Iteration 12900: Loss = -12316.175427958855
Iteration 13000: Loss = -12316.17553768052
1
Iteration 13100: Loss = -12316.176161746977
2
Iteration 13200: Loss = -12316.175492958448
Iteration 13300: Loss = -12316.17546056069
Iteration 13400: Loss = -12316.176159956574
1
Iteration 13500: Loss = -12316.175436031515
Iteration 13600: Loss = -12316.176101385872
1
Iteration 13700: Loss = -12316.175437060345
Iteration 13800: Loss = -12316.17544433884
Iteration 13900: Loss = -12316.175569514546
1
Iteration 14000: Loss = -12316.17536184922
Iteration 14100: Loss = -12316.17544863539
Iteration 14200: Loss = -12316.175359414945
Iteration 14300: Loss = -12316.186605717063
1
Iteration 14400: Loss = -12316.175388123893
Iteration 14500: Loss = -12316.17768830128
1
Iteration 14600: Loss = -12316.253631917636
2
Iteration 14700: Loss = -12316.175379469178
Iteration 14800: Loss = -12316.177217821465
1
Iteration 14900: Loss = -12316.17581755408
2
Iteration 15000: Loss = -12316.177038897955
3
Iteration 15100: Loss = -12316.175716672891
4
Iteration 15200: Loss = -12316.175805210323
5
Iteration 15300: Loss = -12316.223179093975
6
Iteration 15400: Loss = -12316.175659526529
7
Iteration 15500: Loss = -12316.181339591893
8
Iteration 15600: Loss = -12316.175379282053
Iteration 15700: Loss = -12316.175471097378
Iteration 15800: Loss = -12316.180663529198
1
Iteration 15900: Loss = -12316.175436096119
Iteration 16000: Loss = -12316.175983096255
1
Iteration 16100: Loss = -12316.182151746192
2
Iteration 16200: Loss = -12316.175316675131
Iteration 16300: Loss = -12316.180246660748
1
Iteration 16400: Loss = -12316.179168319883
2
Iteration 16500: Loss = -12316.175329345433
Iteration 16600: Loss = -12316.277263743863
1
Iteration 16700: Loss = -12316.175333448648
Iteration 16800: Loss = -12316.175853218227
1
Iteration 16900: Loss = -12316.176161054405
2
Iteration 17000: Loss = -12316.1787163027
3
Iteration 17100: Loss = -12316.178035605637
4
Iteration 17200: Loss = -12316.23949693338
5
Iteration 17300: Loss = -12316.176865786478
6
Iteration 17400: Loss = -12316.175418620307
Iteration 17500: Loss = -12316.175335443048
Iteration 17600: Loss = -12316.17595152458
1
Iteration 17700: Loss = -12316.176128591233
2
Iteration 17800: Loss = -12316.175992383958
3
Iteration 17900: Loss = -12316.17574310254
4
Iteration 18000: Loss = -12316.18308977087
5
Iteration 18100: Loss = -12316.187673436136
6
Iteration 18200: Loss = -12316.17647228853
7
Iteration 18300: Loss = -12316.175521497376
8
Iteration 18400: Loss = -12316.1933646257
9
Iteration 18500: Loss = -12316.187585934376
10
Iteration 18600: Loss = -12316.17540714672
Iteration 18700: Loss = -12316.183076046593
1
Iteration 18800: Loss = -12316.175327556046
Iteration 18900: Loss = -12316.176670917255
1
Iteration 19000: Loss = -12316.175341247268
Iteration 19100: Loss = -12316.267912736934
1
Iteration 19200: Loss = -12316.175333697496
Iteration 19300: Loss = -12316.175284616394
Iteration 19400: Loss = -12316.19951657897
1
Iteration 19500: Loss = -12316.175309927872
Iteration 19600: Loss = -12316.176667612015
1
Iteration 19700: Loss = -12316.175319344196
Iteration 19800: Loss = -12316.17550987227
1
Iteration 19900: Loss = -12316.464998169346
2
pi: tensor([[3.3210e-06, 1.0000e+00],
        [2.8668e-02, 9.7133e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2139, 0.7861], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2208, 0.2074],
         [0.6249, 0.1952]],

        [[0.5009, 0.1591],
         [0.5903, 0.5252]],

        [[0.6886, 0.2738],
         [0.5029, 0.7184]],

        [[0.7053, 0.2963],
         [0.6068, 0.5836]],

        [[0.6105, 0.2152],
         [0.5733, 0.6882]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 4.865438858070249e-06
Average Adjusted Rand Index: -0.0005639446780821619
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22019.38805240901
Iteration 100: Loss = -12318.551392320102
Iteration 200: Loss = -12318.014608762209
Iteration 300: Loss = -12317.792388053685
Iteration 400: Loss = -12317.62667318888
Iteration 500: Loss = -12317.455971083482
Iteration 600: Loss = -12317.231436421665
Iteration 700: Loss = -12316.971728600794
Iteration 800: Loss = -12316.80118970362
Iteration 900: Loss = -12316.70067010925
Iteration 1000: Loss = -12316.630368975091
Iteration 1100: Loss = -12316.577077188827
Iteration 1200: Loss = -12316.53539909838
Iteration 1300: Loss = -12316.502221955592
Iteration 1400: Loss = -12316.47517780549
Iteration 1500: Loss = -12316.45242372527
Iteration 1600: Loss = -12316.432385937076
Iteration 1700: Loss = -12316.4135008555
Iteration 1800: Loss = -12316.394782659385
Iteration 1900: Loss = -12316.375211792556
Iteration 2000: Loss = -12316.354390791614
Iteration 2100: Loss = -12316.332293473886
Iteration 2200: Loss = -12316.309599765216
Iteration 2300: Loss = -12316.287401144255
Iteration 2400: Loss = -12316.266988269635
Iteration 2500: Loss = -12316.249337439205
Iteration 2600: Loss = -12316.234828580007
Iteration 2700: Loss = -12316.223357472134
Iteration 2800: Loss = -12316.21464938573
Iteration 2900: Loss = -12316.20818779536
Iteration 3000: Loss = -12316.203288707085
Iteration 3100: Loss = -12316.199689187042
Iteration 3200: Loss = -12316.196913553465
Iteration 3300: Loss = -12316.194886611007
Iteration 3400: Loss = -12316.194881862522
Iteration 3500: Loss = -12316.191984868947
Iteration 3600: Loss = -12316.190975711806
Iteration 3700: Loss = -12316.190171346074
Iteration 3800: Loss = -12316.189366269678
Iteration 3900: Loss = -12316.188698689817
Iteration 4000: Loss = -12316.18830850121
Iteration 4100: Loss = -12316.187608040696
Iteration 4200: Loss = -12316.187082317563
Iteration 4300: Loss = -12316.187907189407
1
Iteration 4400: Loss = -12316.186104789747
Iteration 4500: Loss = -12316.185698732234
Iteration 4600: Loss = -12316.185287490189
Iteration 4700: Loss = -12316.184792792541
Iteration 4800: Loss = -12316.184432444425
Iteration 4900: Loss = -12316.184073020182
Iteration 5000: Loss = -12316.183650271907
Iteration 5100: Loss = -12316.183314526288
Iteration 5200: Loss = -12316.18297673798
Iteration 5300: Loss = -12316.182640815205
Iteration 5400: Loss = -12316.18234280394
Iteration 5500: Loss = -12316.181983049586
Iteration 5600: Loss = -12316.181701929114
Iteration 5700: Loss = -12316.181418402233
Iteration 5800: Loss = -12316.181175106369
Iteration 5900: Loss = -12316.180878042462
Iteration 6000: Loss = -12316.180776971376
Iteration 6100: Loss = -12316.180364058675
Iteration 6200: Loss = -12316.180187526137
Iteration 6300: Loss = -12316.180302283796
1
Iteration 6400: Loss = -12316.179677128477
Iteration 6500: Loss = -12316.181217901538
1
Iteration 6600: Loss = -12316.179327193506
Iteration 6700: Loss = -12316.179173000375
Iteration 6800: Loss = -12316.178970550938
Iteration 6900: Loss = -12316.178781937188
Iteration 7000: Loss = -12316.179027826322
1
Iteration 7100: Loss = -12316.178444618947
Iteration 7200: Loss = -12316.17836887871
Iteration 7300: Loss = -12316.181269071794
1
Iteration 7400: Loss = -12316.178209267786
Iteration 7500: Loss = -12316.177961941625
Iteration 7600: Loss = -12316.177813014461
Iteration 7700: Loss = -12316.177700725379
Iteration 7800: Loss = -12316.177525286508
Iteration 7900: Loss = -12316.191415623627
1
Iteration 8000: Loss = -12316.177369661478
Iteration 8100: Loss = -12316.177233084047
Iteration 8200: Loss = -12316.177188775331
Iteration 8300: Loss = -12316.177082969518
Iteration 8400: Loss = -12316.177953342616
1
Iteration 8500: Loss = -12316.176899377268
Iteration 8600: Loss = -12316.176804122173
Iteration 8700: Loss = -12316.177915861299
1
Iteration 8800: Loss = -12316.176761490296
Iteration 8900: Loss = -12316.176615628248
Iteration 9000: Loss = -12316.181374266222
1
Iteration 9100: Loss = -12316.176519208
Iteration 9200: Loss = -12316.176409242145
Iteration 9300: Loss = -12316.176389841301
Iteration 9400: Loss = -12316.176414062702
Iteration 9500: Loss = -12316.176235133227
Iteration 9600: Loss = -12316.176219975656
Iteration 9700: Loss = -12316.176396425448
1
Iteration 9800: Loss = -12316.176149208424
Iteration 9900: Loss = -12316.177126437095
1
Iteration 10000: Loss = -12316.176175827653
Iteration 10100: Loss = -12316.39114539657
1
Iteration 10200: Loss = -12316.176000496785
Iteration 10300: Loss = -12316.197857993884
1
Iteration 10400: Loss = -12316.232220570486
2
Iteration 10500: Loss = -12316.175864959758
Iteration 10600: Loss = -12316.176793478322
1
Iteration 10700: Loss = -12316.184687882958
2
Iteration 10800: Loss = -12316.17591404614
Iteration 10900: Loss = -12316.175806497546
Iteration 11000: Loss = -12316.189355563232
1
Iteration 11100: Loss = -12316.176123528989
2
Iteration 11200: Loss = -12316.179896329519
3
Iteration 11300: Loss = -12316.1920169318
4
Iteration 11400: Loss = -12316.175696894967
Iteration 11500: Loss = -12316.176127378752
1
Iteration 11600: Loss = -12316.17782452795
2
Iteration 11700: Loss = -12316.217093847697
3
Iteration 11800: Loss = -12316.200025680766
4
Iteration 11900: Loss = -12316.18896411798
5
Iteration 12000: Loss = -12316.17573967949
Iteration 12100: Loss = -12316.228286968439
1
Iteration 12200: Loss = -12316.175577643216
Iteration 12300: Loss = -12316.18190799405
1
Iteration 12400: Loss = -12316.17571491287
2
Iteration 12500: Loss = -12316.22017539807
3
Iteration 12600: Loss = -12316.2592318821
4
Iteration 12700: Loss = -12316.175511608986
Iteration 12800: Loss = -12316.175923014935
1
Iteration 12900: Loss = -12316.305946604894
2
Iteration 13000: Loss = -12316.175463452226
Iteration 13100: Loss = -12316.18056189866
1
Iteration 13200: Loss = -12316.176897083153
2
Iteration 13300: Loss = -12316.180236648095
3
Iteration 13400: Loss = -12316.184415644573
4
Iteration 13500: Loss = -12316.175880631976
5
Iteration 13600: Loss = -12316.175937928874
6
Iteration 13700: Loss = -12316.175708137689
7
Iteration 13800: Loss = -12316.175695724616
8
Iteration 13900: Loss = -12316.177919807245
9
Iteration 14000: Loss = -12316.244987963999
10
Iteration 14100: Loss = -12316.177637690074
11
Iteration 14200: Loss = -12316.175393178175
Iteration 14300: Loss = -12316.386882009005
1
Iteration 14400: Loss = -12316.175381760735
Iteration 14500: Loss = -12316.317789727353
1
Iteration 14600: Loss = -12316.175408550776
Iteration 14700: Loss = -12316.205019396182
1
Iteration 14800: Loss = -12316.17534003927
Iteration 14900: Loss = -12316.17557386522
1
Iteration 15000: Loss = -12316.175424351446
Iteration 15100: Loss = -12316.181200527977
1
Iteration 15200: Loss = -12316.175764625468
2
Iteration 15300: Loss = -12316.175502971522
Iteration 15400: Loss = -12316.176313580178
1
Iteration 15500: Loss = -12316.202452595873
2
Iteration 15600: Loss = -12316.176669743889
3
Iteration 15700: Loss = -12316.176930852356
4
Iteration 15800: Loss = -12316.17535826101
Iteration 15900: Loss = -12316.175393117974
Iteration 16000: Loss = -12316.175400421678
Iteration 16100: Loss = -12316.178330090166
1
Iteration 16200: Loss = -12316.179742099781
2
Iteration 16300: Loss = -12316.175669927768
3
Iteration 16400: Loss = -12316.175376023457
Iteration 16500: Loss = -12316.177443615004
1
Iteration 16600: Loss = -12316.175613906415
2
Iteration 16700: Loss = -12316.177443738059
3
Iteration 16800: Loss = -12316.175409602816
Iteration 16900: Loss = -12316.176334483174
1
Iteration 17000: Loss = -12316.175352965036
Iteration 17100: Loss = -12316.175311833738
Iteration 17200: Loss = -12316.175519049184
1
Iteration 17300: Loss = -12316.175322474348
Iteration 17400: Loss = -12316.177760133645
1
Iteration 17500: Loss = -12316.17534917002
Iteration 17600: Loss = -12316.175314725044
Iteration 17700: Loss = -12316.17622479314
1
Iteration 17800: Loss = -12316.175316388095
Iteration 17900: Loss = -12316.175388777428
Iteration 18000: Loss = -12316.175453902155
Iteration 18100: Loss = -12316.175703359411
1
Iteration 18200: Loss = -12316.175441219919
Iteration 18300: Loss = -12316.17563286204
1
Iteration 18400: Loss = -12316.196344505992
2
Iteration 18500: Loss = -12316.175316892002
Iteration 18600: Loss = -12316.176085458683
1
Iteration 18700: Loss = -12316.176120878996
2
Iteration 18800: Loss = -12316.175482070927
3
Iteration 18900: Loss = -12316.177687505093
4
Iteration 19000: Loss = -12316.176785132759
5
Iteration 19100: Loss = -12316.175390309107
Iteration 19200: Loss = -12316.176934472829
1
Iteration 19300: Loss = -12316.267666426465
2
Iteration 19400: Loss = -12316.175313867097
Iteration 19500: Loss = -12316.175549016776
1
Iteration 19600: Loss = -12316.250135250843
2
Iteration 19700: Loss = -12316.175334203906
Iteration 19800: Loss = -12316.175330855514
Iteration 19900: Loss = -12316.175642243956
1
pi: tensor([[4.7268e-06, 1.0000e+00],
        [2.8697e-02, 9.7130e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2128, 0.7872], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2210, 0.2076],
         [0.5341, 0.1950]],

        [[0.6250, 0.1593],
         [0.6053, 0.7278]],

        [[0.7084, 0.2741],
         [0.5345, 0.5029]],

        [[0.5800, 0.2960],
         [0.5495, 0.6109]],

        [[0.5138, 0.2154],
         [0.6395, 0.5990]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 4.865438858070249e-06
Average Adjusted Rand Index: -0.0005639446780821619
11844.185237769521
[4.865438858070249e-06, 4.865438858070249e-06] [-0.0005639446780821619, -0.0005639446780821619] [12316.17529919322, 12316.179340770612]
-------------------------------------
This iteration is 45
True Objective function: Loss = -11814.377764248846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22965.30010535107
Iteration 100: Loss = -12290.4887454723
Iteration 200: Loss = -12289.672853477152
Iteration 300: Loss = -12289.328403376872
Iteration 400: Loss = -12288.992723634077
Iteration 500: Loss = -12288.661635177226
Iteration 600: Loss = -12288.38123054574
Iteration 700: Loss = -12288.17209968479
Iteration 800: Loss = -12288.022128410657
Iteration 900: Loss = -12287.965719165946
Iteration 1000: Loss = -12287.94188924933
Iteration 1100: Loss = -12287.927277976727
Iteration 1200: Loss = -12287.916042094366
Iteration 1300: Loss = -12287.907769399262
Iteration 1400: Loss = -12287.90132959739
Iteration 1500: Loss = -12287.8955523825
Iteration 1600: Loss = -12287.890649619585
Iteration 1700: Loss = -12287.885542440436
Iteration 1800: Loss = -12287.88013428514
Iteration 1900: Loss = -12287.875238998278
Iteration 2000: Loss = -12287.871915701326
Iteration 2100: Loss = -12287.869153942916
Iteration 2200: Loss = -12287.86678402593
Iteration 2300: Loss = -12287.864815551
Iteration 2400: Loss = -12287.863012893564
Iteration 2500: Loss = -12287.861219198705
Iteration 2600: Loss = -12287.859784266853
Iteration 2700: Loss = -12287.858695828676
Iteration 2800: Loss = -12287.857755027988
Iteration 2900: Loss = -12287.856852468494
Iteration 3000: Loss = -12287.855816819869
Iteration 3100: Loss = -12287.854498466257
Iteration 3200: Loss = -12287.853054101313
Iteration 3300: Loss = -12287.851899496254
Iteration 3400: Loss = -12287.850559964656
Iteration 3500: Loss = -12287.848942268656
Iteration 3600: Loss = -12287.847362954877
Iteration 3700: Loss = -12287.845074729428
Iteration 3800: Loss = -12287.837253879949
Iteration 3900: Loss = -12287.75351646413
Iteration 4000: Loss = -12287.489981163217
Iteration 4100: Loss = -12287.234985705607
Iteration 4200: Loss = -12287.194196734683
Iteration 4300: Loss = -12287.15563516818
Iteration 4400: Loss = -12287.14872249165
Iteration 4500: Loss = -12287.109378788715
Iteration 4600: Loss = -12287.103828571557
Iteration 4700: Loss = -12287.101788466236
Iteration 4800: Loss = -12287.100489524026
Iteration 4900: Loss = -12287.099457930633
Iteration 5000: Loss = -12287.098711704442
Iteration 5100: Loss = -12287.097930592705
Iteration 5200: Loss = -12287.099253415674
1
Iteration 5300: Loss = -12287.096888225267
Iteration 5400: Loss = -12287.096432565035
Iteration 5500: Loss = -12287.09611846863
Iteration 5600: Loss = -12287.095770968403
Iteration 5700: Loss = -12287.095609055083
Iteration 5800: Loss = -12287.09605941185
1
Iteration 5900: Loss = -12287.095047447048
Iteration 6000: Loss = -12287.095480967322
1
Iteration 6100: Loss = -12287.094598589787
Iteration 6200: Loss = -12287.0944962864
Iteration 6300: Loss = -12287.094876218136
1
Iteration 6400: Loss = -12287.094362105265
Iteration 6500: Loss = -12287.094101742752
Iteration 6600: Loss = -12287.093990080279
Iteration 6700: Loss = -12287.095737006055
1
Iteration 6800: Loss = -12287.09370410108
Iteration 6900: Loss = -12287.093664733324
Iteration 7000: Loss = -12287.094221754602
1
Iteration 7100: Loss = -12287.093499749693
Iteration 7200: Loss = -12287.093667645455
1
Iteration 7300: Loss = -12287.093317121393
Iteration 7400: Loss = -12287.094008017437
1
Iteration 7500: Loss = -12287.093232250067
Iteration 7600: Loss = -12287.094425624966
1
Iteration 7700: Loss = -12287.093121085505
Iteration 7800: Loss = -12287.093071447838
Iteration 7900: Loss = -12287.14597026473
1
Iteration 8000: Loss = -12287.092997274756
Iteration 8100: Loss = -12287.126137766083
1
Iteration 8200: Loss = -12287.092886859133
Iteration 8300: Loss = -12287.158360016905
1
Iteration 8400: Loss = -12287.092846861213
Iteration 8500: Loss = -12287.138935350225
1
Iteration 8600: Loss = -12287.0928118171
Iteration 8700: Loss = -12287.092942553129
1
Iteration 8800: Loss = -12287.092863868795
Iteration 8900: Loss = -12287.317763554149
1
Iteration 9000: Loss = -12287.0927027432
Iteration 9100: Loss = -12287.09331068389
1
Iteration 9200: Loss = -12287.194573951838
2
Iteration 9300: Loss = -12287.25755213974
3
Iteration 9400: Loss = -12287.094630710644
4
Iteration 9500: Loss = -12287.092662921925
Iteration 9600: Loss = -12287.101544804947
1
Iteration 9700: Loss = -12287.093668363454
2
Iteration 9800: Loss = -12287.09275744202
Iteration 9900: Loss = -12287.092898839233
1
Iteration 10000: Loss = -12287.094965457769
2
Iteration 10100: Loss = -12287.092552811357
Iteration 10200: Loss = -12287.09444131243
1
Iteration 10300: Loss = -12287.110682598437
2
Iteration 10400: Loss = -12287.09249242631
Iteration 10500: Loss = -12287.09357964696
1
Iteration 10600: Loss = -12287.102125447202
2
Iteration 10700: Loss = -12287.163247403618
3
Iteration 10800: Loss = -12287.092565204532
Iteration 10900: Loss = -12287.092607669361
Iteration 11000: Loss = -12287.10559134327
1
Iteration 11100: Loss = -12287.154194919809
2
Iteration 11200: Loss = -12287.09242662107
Iteration 11300: Loss = -12287.094542213941
1
Iteration 11400: Loss = -12287.095055870668
2
Iteration 11500: Loss = -12287.099837958314
3
Iteration 11600: Loss = -12287.097841338244
4
Iteration 11700: Loss = -12287.09256522491
5
Iteration 11800: Loss = -12287.09245407106
Iteration 11900: Loss = -12287.113682231946
1
Iteration 12000: Loss = -12287.092413235538
Iteration 12100: Loss = -12287.09888826681
1
Iteration 12200: Loss = -12287.094149197941
2
Iteration 12300: Loss = -12287.095716500158
3
Iteration 12400: Loss = -12287.092620619931
4
Iteration 12500: Loss = -12287.097438665041
5
Iteration 12600: Loss = -12287.09235437013
Iteration 12700: Loss = -12287.093006515732
1
Iteration 12800: Loss = -12287.092399029492
Iteration 12900: Loss = -12287.093749001893
1
Iteration 13000: Loss = -12287.142196234077
2
Iteration 13100: Loss = -12287.09240811753
Iteration 13200: Loss = -12287.092480463967
Iteration 13300: Loss = -12287.287924594353
1
Iteration 13400: Loss = -12287.092370007837
Iteration 13500: Loss = -12287.100517252618
1
Iteration 13600: Loss = -12287.092371247458
Iteration 13700: Loss = -12287.097472641666
1
Iteration 13800: Loss = -12287.139539421854
2
Iteration 13900: Loss = -12287.193986512857
3
Iteration 14000: Loss = -12287.094371753547
4
Iteration 14100: Loss = -12287.092410673162
Iteration 14200: Loss = -12287.09310648206
1
Iteration 14300: Loss = -12287.121789868846
2
Iteration 14400: Loss = -12287.092475111187
Iteration 14500: Loss = -12287.092402842092
Iteration 14600: Loss = -12287.442127607816
1
Iteration 14700: Loss = -12287.092335696732
Iteration 14800: Loss = -12287.443178199961
1
Iteration 14900: Loss = -12287.092342089787
Iteration 15000: Loss = -12287.092935937648
1
Iteration 15100: Loss = -12287.092388364483
Iteration 15200: Loss = -12287.093270783766
1
Iteration 15300: Loss = -12287.096964831653
2
Iteration 15400: Loss = -12287.092489479272
3
Iteration 15500: Loss = -12287.095807035816
4
Iteration 15600: Loss = -12287.09809826734
5
Iteration 15700: Loss = -12287.093481321148
6
Iteration 15800: Loss = -12287.107717557863
7
Iteration 15900: Loss = -12287.092326995258
Iteration 16000: Loss = -12287.092810712023
1
Iteration 16100: Loss = -12287.092486623285
2
Iteration 16200: Loss = -12287.092354159548
Iteration 16300: Loss = -12287.141402095229
1
Iteration 16400: Loss = -12287.092324511197
Iteration 16500: Loss = -12287.093011324412
1
Iteration 16600: Loss = -12287.092577007024
2
Iteration 16700: Loss = -12287.09254675445
3
Iteration 16800: Loss = -12287.093732243766
4
Iteration 16900: Loss = -12287.124322018228
5
Iteration 17000: Loss = -12287.092412829827
Iteration 17100: Loss = -12287.09766945003
1
Iteration 17200: Loss = -12287.093501164783
2
Iteration 17300: Loss = -12287.109420148492
3
Iteration 17400: Loss = -12287.095557007111
4
Iteration 17500: Loss = -12287.092337824623
Iteration 17600: Loss = -12287.16757467692
1
Iteration 17700: Loss = -12287.092373472826
Iteration 17800: Loss = -12287.092516743569
1
Iteration 17900: Loss = -12287.092379775884
Iteration 18000: Loss = -12287.09243634195
Iteration 18100: Loss = -12287.110557267733
1
Iteration 18200: Loss = -12287.092353411592
Iteration 18300: Loss = -12287.093432753485
1
Iteration 18400: Loss = -12287.095236497347
2
Iteration 18500: Loss = -12287.092501260186
3
Iteration 18600: Loss = -12287.092371324434
Iteration 18700: Loss = -12287.092899584492
1
Iteration 18800: Loss = -12287.095405562817
2
Iteration 18900: Loss = -12287.117295994649
3
Iteration 19000: Loss = -12287.092323888333
Iteration 19100: Loss = -12287.149964272714
1
Iteration 19200: Loss = -12287.092334881825
Iteration 19300: Loss = -12287.113020776651
1
Iteration 19400: Loss = -12287.09232474167
Iteration 19500: Loss = -12287.092390412648
Iteration 19600: Loss = -12287.093403263616
1
Iteration 19700: Loss = -12287.092369025662
Iteration 19800: Loss = -12287.092672749544
1
Iteration 19900: Loss = -12287.105799333935
2
pi: tensor([[6.1063e-07, 1.0000e+00],
        [3.8562e-02, 9.6144e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2787, 0.7213], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1608, 0.1814],
         [0.5014, 0.2020]],

        [[0.5606, 0.1944],
         [0.5925, 0.5974]],

        [[0.6458, 0.1803],
         [0.5784, 0.5899]],

        [[0.7183, 0.1116],
         [0.7123, 0.7128]],

        [[0.5874, 0.1224],
         [0.6765, 0.6825]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
Global Adjusted Rand Index: 0.0006798640611026187
Average Adjusted Rand Index: 0.005046274562741854
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21007.763804706025
Iteration 100: Loss = -12290.956826106716
Iteration 200: Loss = -12290.358576856112
Iteration 300: Loss = -12290.240063029863
Iteration 400: Loss = -12290.160726116605
Iteration 500: Loss = -12290.095694435833
Iteration 600: Loss = -12290.034839226526
Iteration 700: Loss = -12289.968393627832
Iteration 800: Loss = -12289.879634503866
Iteration 900: Loss = -12289.74822334384
Iteration 1000: Loss = -12289.604732811062
Iteration 1100: Loss = -12289.497435377192
Iteration 1200: Loss = -12289.415703596673
Iteration 1300: Loss = -12289.344366434067
Iteration 1400: Loss = -12289.279714032713
Iteration 1500: Loss = -12289.222971391026
Iteration 1600: Loss = -12289.175111893766
Iteration 1700: Loss = -12289.135519886222
Iteration 1800: Loss = -12289.10257442965
Iteration 1900: Loss = -12289.074824825198
Iteration 2000: Loss = -12289.050862630109
Iteration 2100: Loss = -12289.029768639179
Iteration 2200: Loss = -12289.010910165594
Iteration 2300: Loss = -12288.993624202883
Iteration 2400: Loss = -12288.97763437994
Iteration 2500: Loss = -12288.962645902528
Iteration 2600: Loss = -12288.948353863167
Iteration 2700: Loss = -12288.934855337016
Iteration 2800: Loss = -12288.922146695686
Iteration 2900: Loss = -12288.910483567803
Iteration 3000: Loss = -12288.900162926559
Iteration 3100: Loss = -12288.891336680508
Iteration 3200: Loss = -12288.88410097944
Iteration 3300: Loss = -12288.87839922045
Iteration 3400: Loss = -12288.873950948187
Iteration 3500: Loss = -12288.870490215546
Iteration 3600: Loss = -12288.86782685803
Iteration 3700: Loss = -12288.865680085515
Iteration 3800: Loss = -12288.863958010143
Iteration 3900: Loss = -12288.86233496543
Iteration 4000: Loss = -12288.860905370808
Iteration 4100: Loss = -12288.859313837813
Iteration 4200: Loss = -12288.85730651379
Iteration 4300: Loss = -12288.854051584802
Iteration 4400: Loss = -12288.844548241172
Iteration 4500: Loss = -12287.77962602649
Iteration 4600: Loss = -12287.072637584559
Iteration 4700: Loss = -12286.882388918268
Iteration 4800: Loss = -12286.792169121793
Iteration 4900: Loss = -12286.750456639293
Iteration 5000: Loss = -12286.726684217067
Iteration 5100: Loss = -12286.710838295628
Iteration 5200: Loss = -12286.699616986853
Iteration 5300: Loss = -12286.691351859292
Iteration 5400: Loss = -12286.684951481377
Iteration 5500: Loss = -12286.679818689354
Iteration 5600: Loss = -12286.67559545293
Iteration 5700: Loss = -12286.671971407975
Iteration 5800: Loss = -12286.668837842877
Iteration 5900: Loss = -12286.666006153408
Iteration 6000: Loss = -12286.663630482424
Iteration 6100: Loss = -12286.661654629523
Iteration 6200: Loss = -12286.65999507847
Iteration 6300: Loss = -12286.65855033435
Iteration 6400: Loss = -12286.65727147799
Iteration 6500: Loss = -12286.656069850666
Iteration 6600: Loss = -12286.655026046701
Iteration 6700: Loss = -12286.654078934338
Iteration 6800: Loss = -12286.653226316646
Iteration 6900: Loss = -12286.660625621118
1
Iteration 7000: Loss = -12286.651590691952
Iteration 7100: Loss = -12286.650911287123
Iteration 7200: Loss = -12286.650305390403
Iteration 7300: Loss = -12286.650189053451
Iteration 7400: Loss = -12286.649268789297
Iteration 7500: Loss = -12286.648822387766
Iteration 7600: Loss = -12286.71390409075
1
Iteration 7700: Loss = -12286.648028603578
Iteration 7800: Loss = -12286.647637448012
Iteration 7900: Loss = -12286.647388409454
Iteration 8000: Loss = -12286.647355734187
Iteration 8100: Loss = -12286.64671904977
Iteration 8200: Loss = -12286.64647088631
Iteration 8300: Loss = -12286.646189711264
Iteration 8400: Loss = -12286.646118507619
Iteration 8500: Loss = -12286.645705519139
Iteration 8600: Loss = -12286.645472733475
Iteration 8700: Loss = -12286.703243653637
1
Iteration 8800: Loss = -12286.64505286232
Iteration 8900: Loss = -12286.64484500419
Iteration 9000: Loss = -12286.915831603998
1
Iteration 9100: Loss = -12286.644058623318
Iteration 9200: Loss = -12286.667790114403
1
Iteration 9300: Loss = -12286.643353443478
Iteration 9400: Loss = -12286.643233656383
Iteration 9500: Loss = -12286.642983798178
Iteration 9600: Loss = -12286.642876234102
Iteration 9700: Loss = -12286.66348549368
1
Iteration 9800: Loss = -12286.642579260011
Iteration 9900: Loss = -12286.64249913543
Iteration 10000: Loss = -12286.66410701217
1
Iteration 10100: Loss = -12286.642267097945
Iteration 10200: Loss = -12286.642141506272
Iteration 10300: Loss = -12286.657736517016
1
Iteration 10400: Loss = -12286.64182016725
Iteration 10500: Loss = -12286.639882239939
Iteration 10600: Loss = -12286.639379054193
Iteration 10700: Loss = -12286.639571698528
1
Iteration 10800: Loss = -12286.639190669051
Iteration 10900: Loss = -12286.649459087013
1
Iteration 11000: Loss = -12286.63910608118
Iteration 11100: Loss = -12286.638993751329
Iteration 11200: Loss = -12286.645955829552
1
Iteration 11300: Loss = -12286.63883066968
Iteration 11400: Loss = -12286.638785662451
Iteration 11500: Loss = -12286.650750814804
1
Iteration 11600: Loss = -12286.63864711236
Iteration 11700: Loss = -12286.638713527476
Iteration 11800: Loss = -12286.638511320094
Iteration 11900: Loss = -12286.63869004675
1
Iteration 12000: Loss = -12286.63846336712
Iteration 12100: Loss = -12286.638440377923
Iteration 12200: Loss = -12286.638853633956
1
Iteration 12300: Loss = -12286.642990856308
2
Iteration 12400: Loss = -12286.639092194731
3
Iteration 12500: Loss = -12286.638471253951
Iteration 12600: Loss = -12286.639429879102
1
Iteration 12700: Loss = -12286.639588221467
2
Iteration 12800: Loss = -12286.640679076196
3
Iteration 12900: Loss = -12286.641525744024
4
Iteration 13000: Loss = -12286.638335876085
Iteration 13100: Loss = -12286.82693541014
1
Iteration 13200: Loss = -12286.63812817204
Iteration 13300: Loss = -12286.645347441867
1
Iteration 13400: Loss = -12286.638003638162
Iteration 13500: Loss = -12286.638166869434
1
Iteration 13600: Loss = -12286.63790280876
Iteration 13700: Loss = -12286.638761785209
1
Iteration 13800: Loss = -12286.826456669987
2
Iteration 13900: Loss = -12286.643887691975
3
Iteration 14000: Loss = -12286.638269438257
4
Iteration 14100: Loss = -12286.638079023156
5
Iteration 14200: Loss = -12286.638055877514
6
Iteration 14300: Loss = -12286.637846316342
Iteration 14400: Loss = -12286.638816267221
1
Iteration 14500: Loss = -12286.63824599173
2
Iteration 14600: Loss = -12286.638001873693
3
Iteration 14700: Loss = -12286.63981523856
4
Iteration 14800: Loss = -12286.637772785858
Iteration 14900: Loss = -12286.762595431166
1
Iteration 15000: Loss = -12286.63771340174
Iteration 15100: Loss = -12286.643529563378
1
Iteration 15200: Loss = -12286.644776137322
2
Iteration 15300: Loss = -12286.637696974456
Iteration 15400: Loss = -12286.637832630418
1
Iteration 15500: Loss = -12286.64549249287
2
Iteration 15600: Loss = -12286.637811878098
3
Iteration 15700: Loss = -12286.637685428832
Iteration 15800: Loss = -12286.64377391647
1
Iteration 15900: Loss = -12286.642429138044
2
Iteration 16000: Loss = -12286.637982714594
3
Iteration 16100: Loss = -12286.63792614696
4
Iteration 16200: Loss = -12286.934057860339
5
Iteration 16300: Loss = -12286.637619573212
Iteration 16400: Loss = -12286.6399079364
1
Iteration 16500: Loss = -12286.639551784614
2
Iteration 16600: Loss = -12286.638967144896
3
Iteration 16700: Loss = -12286.63773110239
4
Iteration 16800: Loss = -12286.638110162146
5
Iteration 16900: Loss = -12286.804971369875
6
Iteration 17000: Loss = -12286.638777221655
7
Iteration 17100: Loss = -12286.638039949916
8
Iteration 17200: Loss = -12286.654841750591
9
Iteration 17300: Loss = -12286.699569854947
10
Iteration 17400: Loss = -12286.640853018476
11
Iteration 17500: Loss = -12286.638166673365
12
Iteration 17600: Loss = -12286.657602642365
13
Iteration 17700: Loss = -12286.639647896274
14
Iteration 17800: Loss = -12286.638266885686
15
Stopping early at iteration 17800 due to no improvement.
pi: tensor([[1.0000e+00, 1.6898e-07],
        [2.4367e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9611, 0.0389], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.1186],
         [0.5289, 0.2684]],

        [[0.5253, 0.2053],
         [0.5271, 0.6658]],

        [[0.5330, 0.2126],
         [0.7210, 0.6453]],

        [[0.7298, 0.1343],
         [0.5890, 0.5183]],

        [[0.6893, 0.2387],
         [0.6539, 0.6752]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.01575757575757576
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.007262881945936654
Global Adjusted Rand Index: -0.00035123166339314003
Average Adjusted Rand Index: 0.0043837759262587124
11814.377764248846
[0.0006798640611026187, -0.00035123166339314003] [0.005046274562741854, 0.0043837759262587124] [12287.145559799621, 12286.638266885686]
-------------------------------------
This iteration is 46
True Objective function: Loss = -11931.95885998401
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20188.935708877714
Iteration 100: Loss = -12489.791259109019
Iteration 200: Loss = -12489.462298600725
Iteration 300: Loss = -12489.399714653491
Iteration 400: Loss = -12489.326603069145
Iteration 500: Loss = -12489.24052951134
Iteration 600: Loss = -12489.168229413172
Iteration 700: Loss = -12489.098675624899
Iteration 800: Loss = -12489.013287055628
Iteration 900: Loss = -12488.918979405425
Iteration 1000: Loss = -12488.832203450189
Iteration 1100: Loss = -12488.75306117085
Iteration 1200: Loss = -12488.662886309521
Iteration 1300: Loss = -12488.525176623014
Iteration 1400: Loss = -12487.59466284633
Iteration 1500: Loss = -12486.760346714562
Iteration 1600: Loss = -12486.619451477094
Iteration 1700: Loss = -12486.552869380854
Iteration 1800: Loss = -12486.50923824418
Iteration 1900: Loss = -12486.47525302284
Iteration 2000: Loss = -12486.44794606807
Iteration 2100: Loss = -12486.426427255601
Iteration 2200: Loss = -12486.410111186444
Iteration 2300: Loss = -12486.398120077773
Iteration 2400: Loss = -12486.389605668772
Iteration 2500: Loss = -12486.38369730069
Iteration 2600: Loss = -12486.379519317654
Iteration 2700: Loss = -12486.376599699077
Iteration 2800: Loss = -12486.374476857292
Iteration 2900: Loss = -12486.372975713424
Iteration 3000: Loss = -12486.37184280498
Iteration 3100: Loss = -12486.371047083872
Iteration 3200: Loss = -12486.370320884796
Iteration 3300: Loss = -12486.36979952175
Iteration 3400: Loss = -12486.36932486633
Iteration 3500: Loss = -12486.368986083278
Iteration 3600: Loss = -12486.368647431447
Iteration 3700: Loss = -12486.368322513279
Iteration 3800: Loss = -12486.368044539044
Iteration 3900: Loss = -12486.367823384773
Iteration 4000: Loss = -12486.367612903241
Iteration 4100: Loss = -12486.36734974546
Iteration 4200: Loss = -12486.367134786367
Iteration 4300: Loss = -12486.366981426025
Iteration 4400: Loss = -12486.366782455178
Iteration 4500: Loss = -12486.366638941146
Iteration 4600: Loss = -12486.366483764577
Iteration 4700: Loss = -12486.366301760663
Iteration 4800: Loss = -12486.366178687227
Iteration 4900: Loss = -12486.366020019686
Iteration 5000: Loss = -12486.365923883794
Iteration 5100: Loss = -12486.365815479905
Iteration 5200: Loss = -12486.365691790857
Iteration 5300: Loss = -12486.369086937095
1
Iteration 5400: Loss = -12486.365493554418
Iteration 5500: Loss = -12486.36540644823
Iteration 5600: Loss = -12486.365275832562
Iteration 5700: Loss = -12486.365231386459
Iteration 5800: Loss = -12486.365658607521
1
Iteration 5900: Loss = -12486.365040501356
Iteration 6000: Loss = -12486.364981613973
Iteration 6100: Loss = -12486.364922405299
Iteration 6200: Loss = -12486.364834717033
Iteration 6300: Loss = -12486.365533063548
1
Iteration 6400: Loss = -12486.364779706528
Iteration 6500: Loss = -12486.364685092865
Iteration 6600: Loss = -12486.364628459485
Iteration 6700: Loss = -12486.364560081694
Iteration 6800: Loss = -12486.364853736182
1
Iteration 6900: Loss = -12486.36454991912
Iteration 7000: Loss = -12486.367928575051
1
Iteration 7100: Loss = -12486.36440834989
Iteration 7200: Loss = -12486.364789141864
1
Iteration 7300: Loss = -12486.364350337617
Iteration 7400: Loss = -12486.36756321263
1
Iteration 7500: Loss = -12486.364366822678
Iteration 7600: Loss = -12486.364409251384
Iteration 7700: Loss = -12486.552989755804
1
Iteration 7800: Loss = -12486.364214289917
Iteration 7900: Loss = -12486.374925898928
1
Iteration 8000: Loss = -12486.364183224117
Iteration 8100: Loss = -12486.366919206444
1
Iteration 8200: Loss = -12486.364303390257
2
Iteration 8300: Loss = -12486.365036992702
3
Iteration 8400: Loss = -12486.431250102549
4
Iteration 8500: Loss = -12486.468398747975
5
Iteration 8600: Loss = -12486.377175076244
6
Iteration 8700: Loss = -12486.49737657605
7
Iteration 8800: Loss = -12486.372513035765
8
Iteration 8900: Loss = -12486.375785550834
9
Iteration 9000: Loss = -12486.364045387176
Iteration 9100: Loss = -12486.364059437508
Iteration 9200: Loss = -12486.364224614306
1
Iteration 9300: Loss = -12486.36407486682
Iteration 9400: Loss = -12486.366349403621
1
Iteration 9500: Loss = -12486.364009901848
Iteration 9600: Loss = -12486.36493851024
1
Iteration 9700: Loss = -12486.367360079945
2
Iteration 9800: Loss = -12486.367462978294
3
Iteration 9900: Loss = -12486.397544687718
4
Iteration 10000: Loss = -12486.364016105179
Iteration 10100: Loss = -12486.364376389058
1
Iteration 10200: Loss = -12486.36431212848
2
Iteration 10300: Loss = -12486.364041931432
Iteration 10400: Loss = -12486.371508162336
1
Iteration 10500: Loss = -12486.365607027474
2
Iteration 10600: Loss = -12486.363964935079
Iteration 10700: Loss = -12486.364768695632
1
Iteration 10800: Loss = -12486.64569265805
2
Iteration 10900: Loss = -12486.364000238422
Iteration 11000: Loss = -12486.366885610558
1
Iteration 11100: Loss = -12486.363926853122
Iteration 11200: Loss = -12486.36393114759
Iteration 11300: Loss = -12486.363953224567
Iteration 11400: Loss = -12486.368298455882
1
Iteration 11500: Loss = -12486.364136222292
2
Iteration 11600: Loss = -12486.364016814126
Iteration 11700: Loss = -12486.456332361768
1
Iteration 11800: Loss = -12486.363894760489
Iteration 11900: Loss = -12486.364735927697
1
Iteration 12000: Loss = -12486.364066525171
2
Iteration 12100: Loss = -12486.399899013202
3
Iteration 12200: Loss = -12486.363907860983
Iteration 12300: Loss = -12486.370725828105
1
Iteration 12400: Loss = -12486.364160850113
2
Iteration 12500: Loss = -12486.36392017413
Iteration 12600: Loss = -12486.370083560836
1
Iteration 12700: Loss = -12486.365527960723
2
Iteration 12800: Loss = -12486.367105326059
3
Iteration 12900: Loss = -12486.364584842988
4
Iteration 13000: Loss = -12486.363930888874
Iteration 13100: Loss = -12486.367664926065
1
Iteration 13200: Loss = -12486.417263984906
2
Iteration 13300: Loss = -12486.36409607564
3
Iteration 13400: Loss = -12486.366670385902
4
Iteration 13500: Loss = -12486.364570934284
5
Iteration 13600: Loss = -12486.446575923
6
Iteration 13700: Loss = -12486.365129065045
7
Iteration 13800: Loss = -12486.368735689206
8
Iteration 13900: Loss = -12486.36462281587
9
Iteration 14000: Loss = -12486.367541691045
10
Iteration 14100: Loss = -12486.365386423697
11
Iteration 14200: Loss = -12486.397374914313
12
Iteration 14300: Loss = -12486.363915023285
Iteration 14400: Loss = -12486.364185888311
1
Iteration 14500: Loss = -12486.363899172786
Iteration 14600: Loss = -12486.364715944024
1
Iteration 14700: Loss = -12486.46822370074
2
Iteration 14800: Loss = -12486.364010242596
3
Iteration 14900: Loss = -12486.366654153157
4
Iteration 15000: Loss = -12486.553155925294
5
Iteration 15100: Loss = -12486.363851815857
Iteration 15200: Loss = -12486.370463552228
1
Iteration 15300: Loss = -12486.363857832728
Iteration 15400: Loss = -12486.369132479402
1
Iteration 15500: Loss = -12486.364450833304
2
Iteration 15600: Loss = -12486.368480331364
3
Iteration 15700: Loss = -12486.36409413433
4
Iteration 15800: Loss = -12486.36816600278
5
Iteration 15900: Loss = -12486.36880680995
6
Iteration 16000: Loss = -12486.375639045453
7
Iteration 16100: Loss = -12486.380583228802
8
Iteration 16200: Loss = -12486.363863166005
Iteration 16300: Loss = -12486.439615850193
1
Iteration 16400: Loss = -12486.363885363176
Iteration 16500: Loss = -12486.408204820093
1
Iteration 16600: Loss = -12486.40056039715
2
Iteration 16700: Loss = -12486.371492589571
3
Iteration 16800: Loss = -12486.366573552708
4
Iteration 16900: Loss = -12486.36425893047
5
Iteration 17000: Loss = -12486.37668211068
6
Iteration 17100: Loss = -12486.365075105008
7
Iteration 17200: Loss = -12486.383059491205
8
Iteration 17300: Loss = -12486.371751899702
9
Iteration 17400: Loss = -12486.369433195303
10
Iteration 17500: Loss = -12486.363963447011
Iteration 17600: Loss = -12486.416081534584
1
Iteration 17700: Loss = -12486.36386072165
Iteration 17800: Loss = -12486.36882353982
1
Iteration 17900: Loss = -12486.369009372418
2
Iteration 18000: Loss = -12486.367041534546
3
Iteration 18100: Loss = -12486.363906734006
Iteration 18200: Loss = -12486.38379841992
1
Iteration 18300: Loss = -12486.363881029636
Iteration 18400: Loss = -12486.50323727914
1
Iteration 18500: Loss = -12486.363835127218
Iteration 18600: Loss = -12486.365669360188
1
Iteration 18700: Loss = -12486.364045063148
2
Iteration 18800: Loss = -12486.363910271848
Iteration 18900: Loss = -12486.41733523
1
Iteration 19000: Loss = -12486.374119038666
2
Iteration 19100: Loss = -12486.41010934709
3
Iteration 19200: Loss = -12486.36452839136
4
Iteration 19300: Loss = -12486.366742663797
5
Iteration 19400: Loss = -12486.366939814008
6
Iteration 19500: Loss = -12486.363893968623
Iteration 19600: Loss = -12486.364776563309
1
Iteration 19700: Loss = -12486.364334287458
2
Iteration 19800: Loss = -12486.369907380878
3
Iteration 19900: Loss = -12486.36691962073
4
pi: tensor([[0.0119, 0.9881],
        [0.0343, 0.9657]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9966e-01, 3.4113e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1956, 0.1967],
         [0.6492, 0.2067]],

        [[0.6248, 0.2818],
         [0.6526, 0.6649]],

        [[0.6231, 0.2028],
         [0.6124, 0.5334]],

        [[0.5239, 0.1809],
         [0.6006, 0.7083]],

        [[0.5976, 0.1067],
         [0.5816, 0.5176]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
Global Adjusted Rand Index: -0.0019655407700242883
Average Adjusted Rand Index: 0.0011834947531591965
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22043.84583625235
Iteration 100: Loss = -12489.861869883951
Iteration 200: Loss = -12489.133066857266
Iteration 300: Loss = -12488.332274237175
Iteration 400: Loss = -12487.733054524031
Iteration 500: Loss = -12487.506694672491
Iteration 600: Loss = -12487.377503105776
Iteration 700: Loss = -12487.287267344715
Iteration 800: Loss = -12487.216865319248
Iteration 900: Loss = -12487.160531980273
Iteration 1000: Loss = -12487.104073635615
Iteration 1100: Loss = -12487.034117980524
Iteration 1200: Loss = -12486.931247303422
Iteration 1300: Loss = -12486.763919858915
Iteration 1400: Loss = -12486.632835274242
Iteration 1500: Loss = -12486.56810498296
Iteration 1600: Loss = -12486.519132515434
Iteration 1700: Loss = -12486.479067318578
Iteration 1800: Loss = -12486.446276119468
Iteration 1900: Loss = -12486.420687737544
Iteration 2000: Loss = -12486.402436255938
Iteration 2100: Loss = -12486.39039939657
Iteration 2200: Loss = -12486.382854879756
Iteration 2300: Loss = -12486.37806567985
Iteration 2400: Loss = -12486.375079575426
Iteration 2500: Loss = -12486.373163338614
Iteration 2600: Loss = -12486.371811062503
Iteration 2700: Loss = -12486.370935080777
Iteration 2800: Loss = -12486.370191760849
Iteration 2900: Loss = -12486.369639205815
Iteration 3000: Loss = -12486.369162801775
Iteration 3100: Loss = -12486.368783187292
Iteration 3200: Loss = -12486.368428605798
Iteration 3300: Loss = -12486.368068612972
Iteration 3400: Loss = -12486.367812448309
Iteration 3500: Loss = -12486.367559284889
Iteration 3600: Loss = -12486.367313474651
Iteration 3700: Loss = -12486.367061978555
Iteration 3800: Loss = -12486.366903631291
Iteration 3900: Loss = -12486.366678301234
Iteration 4000: Loss = -12486.366494549206
Iteration 4100: Loss = -12486.366343435127
Iteration 4200: Loss = -12486.36621658282
Iteration 4300: Loss = -12486.366005698052
Iteration 4400: Loss = -12486.36585910253
Iteration 4500: Loss = -12486.365792061393
Iteration 4600: Loss = -12486.365658089397
Iteration 4700: Loss = -12486.365529028955
Iteration 4800: Loss = -12486.365377949873
Iteration 4900: Loss = -12486.365316477757
Iteration 5000: Loss = -12486.365237055918
Iteration 5100: Loss = -12486.365126650195
Iteration 5200: Loss = -12486.365046839248
Iteration 5300: Loss = -12486.364990031818
Iteration 5400: Loss = -12486.36491519755
Iteration 5500: Loss = -12486.364807289312
Iteration 5600: Loss = -12486.364761741657
Iteration 5700: Loss = -12486.364733162542
Iteration 5800: Loss = -12486.364667249558
Iteration 5900: Loss = -12486.364680235514
Iteration 6000: Loss = -12486.36452590764
Iteration 6100: Loss = -12486.3645124391
Iteration 6200: Loss = -12486.3644735799
Iteration 6300: Loss = -12486.364469029486
Iteration 6400: Loss = -12486.364385911213
Iteration 6500: Loss = -12486.364395130728
Iteration 6600: Loss = -12486.364322670766
Iteration 6700: Loss = -12486.364411085095
Iteration 6800: Loss = -12486.364322232566
Iteration 6900: Loss = -12486.364291921558
Iteration 7000: Loss = -12486.364237129272
Iteration 7100: Loss = -12486.364224680865
Iteration 7200: Loss = -12486.372973901907
1
Iteration 7300: Loss = -12486.364198130275
Iteration 7400: Loss = -12486.364139047464
Iteration 7500: Loss = -12486.364173140164
Iteration 7600: Loss = -12486.364095669942
Iteration 7700: Loss = -12486.365113488484
1
Iteration 7800: Loss = -12486.364138981637
Iteration 7900: Loss = -12486.364117029507
Iteration 8000: Loss = -12486.364385565003
1
Iteration 8100: Loss = -12486.602513952395
2
Iteration 8200: Loss = -12486.364038851741
Iteration 8300: Loss = -12486.650722887794
1
Iteration 8400: Loss = -12486.364032590323
Iteration 8500: Loss = -12486.364038119138
Iteration 8600: Loss = -12486.378516000474
1
Iteration 8700: Loss = -12486.36419544556
2
Iteration 8800: Loss = -12486.368948726693
3
Iteration 8900: Loss = -12486.43766361503
4
Iteration 9000: Loss = -12486.364611962257
5
Iteration 9100: Loss = -12486.401238334918
6
Iteration 9200: Loss = -12486.363973599378
Iteration 9300: Loss = -12486.364022215297
Iteration 9400: Loss = -12486.364514944404
1
Iteration 9500: Loss = -12486.364084865661
Iteration 9600: Loss = -12486.381497098902
1
Iteration 9700: Loss = -12486.374929403286
2
Iteration 9800: Loss = -12486.370423617698
3
Iteration 9900: Loss = -12486.399055279038
4
Iteration 10000: Loss = -12486.365445725738
5
Iteration 10100: Loss = -12486.370500711762
6
Iteration 10200: Loss = -12486.365444344085
7
Iteration 10300: Loss = -12486.365428640343
8
Iteration 10400: Loss = -12486.398434732293
9
Iteration 10500: Loss = -12486.370039782902
10
Iteration 10600: Loss = -12486.364765001437
11
Iteration 10700: Loss = -12486.364066195965
Iteration 10800: Loss = -12486.364299723318
1
Iteration 10900: Loss = -12486.36464288222
2
Iteration 11000: Loss = -12486.37580546349
3
Iteration 11100: Loss = -12486.363927566046
Iteration 11200: Loss = -12486.444176890398
1
Iteration 11300: Loss = -12486.363931483811
Iteration 11400: Loss = -12486.363899478541
Iteration 11500: Loss = -12486.363964512198
Iteration 11600: Loss = -12486.36388549961
Iteration 11700: Loss = -12486.365819756902
1
Iteration 11800: Loss = -12486.366170944448
2
Iteration 11900: Loss = -12486.36694204533
3
Iteration 12000: Loss = -12486.395237543327
4
Iteration 12100: Loss = -12486.363895649702
Iteration 12200: Loss = -12486.387388137542
1
Iteration 12300: Loss = -12486.363905060678
Iteration 12400: Loss = -12486.548410607274
1
Iteration 12500: Loss = -12486.36390337369
Iteration 12600: Loss = -12486.490939213312
1
Iteration 12700: Loss = -12486.364261532415
2
Iteration 12800: Loss = -12486.648141507734
3
Iteration 12900: Loss = -12486.363892379146
Iteration 13000: Loss = -12486.678075017238
1
Iteration 13100: Loss = -12486.363885287243
Iteration 13200: Loss = -12486.36388950037
Iteration 13300: Loss = -12486.364073853898
1
Iteration 13400: Loss = -12486.363906222638
Iteration 13500: Loss = -12486.366801721833
1
Iteration 13600: Loss = -12486.364049525286
2
Iteration 13700: Loss = -12486.368522300765
3
Iteration 13800: Loss = -12486.414038356419
4
Iteration 13900: Loss = -12486.365516016709
5
Iteration 14000: Loss = -12486.368128905893
6
Iteration 14100: Loss = -12486.364636790593
7
Iteration 14200: Loss = -12486.365008394316
8
Iteration 14300: Loss = -12486.36795417965
9
Iteration 14400: Loss = -12486.365159316345
10
Iteration 14500: Loss = -12486.364074869656
11
Iteration 14600: Loss = -12486.3747374247
12
Iteration 14700: Loss = -12486.363907038096
Iteration 14800: Loss = -12486.408364623163
1
Iteration 14900: Loss = -12486.363902637058
Iteration 15000: Loss = -12486.363899593083
Iteration 15100: Loss = -12486.36589707955
1
Iteration 15200: Loss = -12486.363845056821
Iteration 15300: Loss = -12486.363857012282
Iteration 15400: Loss = -12486.36495282196
1
Iteration 15500: Loss = -12486.363918706214
Iteration 15600: Loss = -12486.369202266926
1
Iteration 15700: Loss = -12486.363898845291
Iteration 15800: Loss = -12486.456435249043
1
Iteration 15900: Loss = -12486.363894249453
Iteration 16000: Loss = -12486.365121781573
1
Iteration 16100: Loss = -12486.36386121468
Iteration 16200: Loss = -12486.364307990976
1
Iteration 16300: Loss = -12486.363974882119
2
Iteration 16400: Loss = -12486.37018337712
3
Iteration 16500: Loss = -12486.363850765943
Iteration 16600: Loss = -12486.364833777783
1
Iteration 16700: Loss = -12486.363871221434
Iteration 16800: Loss = -12486.364998051795
1
Iteration 16900: Loss = -12486.460028242596
2
Iteration 17000: Loss = -12486.363843240988
Iteration 17100: Loss = -12486.364205181992
1
Iteration 17200: Loss = -12486.3772540245
2
Iteration 17300: Loss = -12486.364252370644
3
Iteration 17400: Loss = -12486.36405156682
4
Iteration 17500: Loss = -12486.364099903454
5
Iteration 17600: Loss = -12486.378828107205
6
Iteration 17700: Loss = -12486.36390936685
Iteration 17800: Loss = -12486.366662142698
1
Iteration 17900: Loss = -12486.364083943316
2
Iteration 18000: Loss = -12486.36416405006
3
Iteration 18100: Loss = -12486.381341136186
4
Iteration 18200: Loss = -12486.42614951742
5
Iteration 18300: Loss = -12486.363859443942
Iteration 18400: Loss = -12486.366057717158
1
Iteration 18500: Loss = -12486.363936389442
Iteration 18600: Loss = -12486.364465555826
1
Iteration 18700: Loss = -12486.36684066579
2
Iteration 18800: Loss = -12486.3667643273
3
Iteration 18900: Loss = -12486.364260407268
4
Iteration 19000: Loss = -12486.364212925053
5
Iteration 19100: Loss = -12486.363981385644
Iteration 19200: Loss = -12486.364920062822
1
Iteration 19300: Loss = -12486.366362995583
2
Iteration 19400: Loss = -12486.36574083658
3
Iteration 19500: Loss = -12486.366673694987
4
Iteration 19600: Loss = -12486.369884383375
5
Iteration 19700: Loss = -12486.379456338784
6
Iteration 19800: Loss = -12486.460431984266
7
Iteration 19900: Loss = -12486.363999054598
pi: tensor([[0.0119, 0.9881],
        [0.0343, 0.9657]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9975e-01, 2.5434e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1960, 0.1966],
         [0.6321, 0.2066]],

        [[0.6089, 0.2820],
         [0.7183, 0.6341]],

        [[0.6023, 0.2030],
         [0.6621, 0.5964]],

        [[0.5362, 0.1811],
         [0.6967, 0.7258]],

        [[0.6480, 0.1066],
         [0.7285, 0.6459]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
Global Adjusted Rand Index: -0.0019655407700242883
Average Adjusted Rand Index: 0.0011834947531591965
11931.95885998401
[-0.0019655407700242883, -0.0019655407700242883] [0.0011834947531591965, 0.0011834947531591965] [12486.367631414598, 12486.364006612575]
-------------------------------------
This iteration is 47
True Objective function: Loss = -11846.4033190645
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22387.98096970812
Iteration 100: Loss = -12360.825468338693
Iteration 200: Loss = -12360.112213112001
Iteration 300: Loss = -12359.937446919836
Iteration 400: Loss = -12359.841486321564
Iteration 500: Loss = -12359.776053862197
Iteration 600: Loss = -12359.719287684766
Iteration 700: Loss = -12359.665684027643
Iteration 800: Loss = -12359.616430495544
Iteration 900: Loss = -12359.570588748089
Iteration 1000: Loss = -12359.52686581333
Iteration 1100: Loss = -12359.484578703847
Iteration 1200: Loss = -12359.443400775552
Iteration 1300: Loss = -12359.402869058446
Iteration 1400: Loss = -12359.362301073861
Iteration 1500: Loss = -12359.321454358162
Iteration 1600: Loss = -12359.28021352248
Iteration 1700: Loss = -12359.23898474265
Iteration 1800: Loss = -12359.197955502668
Iteration 1900: Loss = -12359.157269485895
Iteration 2000: Loss = -12359.116344803648
Iteration 2100: Loss = -12359.074221901556
Iteration 2200: Loss = -12359.029672199655
Iteration 2300: Loss = -12358.981729408362
Iteration 2400: Loss = -12358.929549596322
Iteration 2500: Loss = -12358.872493021721
Iteration 2600: Loss = -12358.808967774632
Iteration 2700: Loss = -12358.739434678193
Iteration 2800: Loss = -12358.669197677387
Iteration 2900: Loss = -12358.601914104225
Iteration 3000: Loss = -12358.541944393164
Iteration 3100: Loss = -12358.49369801614
Iteration 3200: Loss = -12358.451314343198
Iteration 3300: Loss = -12358.418911513638
Iteration 3400: Loss = -12358.393971982463
Iteration 3500: Loss = -12358.37417768209
Iteration 3600: Loss = -12358.36868860784
Iteration 3700: Loss = -12358.34858066236
Iteration 3800: Loss = -12358.33698578398
Iteration 3900: Loss = -12358.329321050282
Iteration 4000: Loss = -12358.322836292149
Iteration 4100: Loss = -12358.317374818052
Iteration 4200: Loss = -12358.312979176559
Iteration 4300: Loss = -12358.310863717388
Iteration 4400: Loss = -12358.308002764574
Iteration 4500: Loss = -12358.301289527046
Iteration 4600: Loss = -12358.297137908097
Iteration 4700: Loss = -12358.291340710764
Iteration 4800: Loss = -12358.283358282799
Iteration 4900: Loss = -12358.279982252825
Iteration 5000: Loss = -12358.253810605434
Iteration 5100: Loss = -12358.232073692872
Iteration 5200: Loss = -12358.210565453232
Iteration 5300: Loss = -12358.193133389072
Iteration 5400: Loss = -12358.180270533952
Iteration 5500: Loss = -12358.171302019262
Iteration 5600: Loss = -12358.164574235925
Iteration 5700: Loss = -12358.15973036696
Iteration 5800: Loss = -12358.156248012288
Iteration 5900: Loss = -12358.15325145937
Iteration 6000: Loss = -12358.152670159136
Iteration 6100: Loss = -12358.149291073181
Iteration 6200: Loss = -12358.147780507144
Iteration 6300: Loss = -12358.14674211258
Iteration 6400: Loss = -12358.145612500473
Iteration 6500: Loss = -12358.144784190188
Iteration 6600: Loss = -12358.144071892262
Iteration 6700: Loss = -12358.143386434791
Iteration 6800: Loss = -12358.142856965917
Iteration 6900: Loss = -12358.142352352897
Iteration 7000: Loss = -12358.142855647124
1
Iteration 7100: Loss = -12358.141495822943
Iteration 7200: Loss = -12358.141139292493
Iteration 7300: Loss = -12358.140767995194
Iteration 7400: Loss = -12358.14075959183
Iteration 7500: Loss = -12358.140454708257
Iteration 7600: Loss = -12358.139982698729
Iteration 7700: Loss = -12358.141234642442
1
Iteration 7800: Loss = -12358.139696146729
Iteration 7900: Loss = -12358.139520496221
Iteration 8000: Loss = -12358.140464679504
1
Iteration 8100: Loss = -12358.140383481536
2
Iteration 8200: Loss = -12358.159034314685
3
Iteration 8300: Loss = -12358.148177324307
4
Iteration 8400: Loss = -12358.142012022046
5
Iteration 8500: Loss = -12358.140144247349
6
Iteration 8600: Loss = -12358.138619195122
Iteration 8700: Loss = -12358.139114615502
1
Iteration 8800: Loss = -12358.334258857578
2
Iteration 8900: Loss = -12358.138236296925
Iteration 9000: Loss = -12358.138797006955
1
Iteration 9100: Loss = -12358.21117155945
2
Iteration 9200: Loss = -12358.140430216397
3
Iteration 9300: Loss = -12358.139185250404
4
Iteration 9400: Loss = -12358.149543515136
5
Iteration 9500: Loss = -12358.137571678599
Iteration 9600: Loss = -12358.137595364806
Iteration 9700: Loss = -12358.531184579528
1
Iteration 9800: Loss = -12358.139865740592
2
Iteration 9900: Loss = -12358.182936546065
3
Iteration 10000: Loss = -12358.177254695262
4
Iteration 10100: Loss = -12358.137194118797
Iteration 10200: Loss = -12358.13850335887
1
Iteration 10300: Loss = -12358.370609540332
2
Iteration 10400: Loss = -12358.1395050748
3
Iteration 10500: Loss = -12358.139003383034
4
Iteration 10600: Loss = -12358.18586247287
5
Iteration 10700: Loss = -12358.230446486219
6
Iteration 10800: Loss = -12358.147745885119
7
Iteration 10900: Loss = -12358.360474955174
8
Iteration 11000: Loss = -12358.136865996521
Iteration 11100: Loss = -12358.431108891007
1
Iteration 11200: Loss = -12358.137285680776
2
Iteration 11300: Loss = -12358.136838372
Iteration 11400: Loss = -12358.13906599324
1
Iteration 11500: Loss = -12358.137715898363
2
Iteration 11600: Loss = -12358.136719926848
Iteration 11700: Loss = -12358.138355545028
1
Iteration 11800: Loss = -12358.137398436787
2
Iteration 11900: Loss = -12358.160148291216
3
Iteration 12000: Loss = -12358.144898687457
4
Iteration 12100: Loss = -12358.186673307198
5
Iteration 12200: Loss = -12358.136612723556
Iteration 12300: Loss = -12358.138614006322
1
Iteration 12400: Loss = -12358.140414831278
2
Iteration 12500: Loss = -12358.165583488286
3
Iteration 12600: Loss = -12358.136609325047
Iteration 12700: Loss = -12358.136594064175
Iteration 12800: Loss = -12358.136786427147
1
Iteration 12900: Loss = -12358.13696141383
2
Iteration 13000: Loss = -12358.13829109981
3
Iteration 13100: Loss = -12358.136471329783
Iteration 13200: Loss = -12358.139664657881
1
Iteration 13300: Loss = -12358.174135506693
2
Iteration 13400: Loss = -12358.13694685111
3
Iteration 13500: Loss = -12358.138186684635
4
Iteration 13600: Loss = -12358.163360521356
5
Iteration 13700: Loss = -12358.179108170993
6
Iteration 13800: Loss = -12358.13684462163
7
Iteration 13900: Loss = -12358.151693607038
8
Iteration 14000: Loss = -12358.143474708175
9
Iteration 14100: Loss = -12358.137677359724
10
Iteration 14200: Loss = -12358.13642245798
Iteration 14300: Loss = -12358.136723592037
1
Iteration 14400: Loss = -12358.136391716234
Iteration 14500: Loss = -12358.136531470152
1
Iteration 14600: Loss = -12358.175585776817
2
Iteration 14700: Loss = -12358.249094101027
3
Iteration 14800: Loss = -12358.144340287741
4
Iteration 14900: Loss = -12358.137659028114
5
Iteration 15000: Loss = -12358.137315734119
6
Iteration 15100: Loss = -12358.136369251677
Iteration 15200: Loss = -12358.136841140022
1
Iteration 15300: Loss = -12358.268550447749
2
Iteration 15400: Loss = -12358.165758115902
3
Iteration 15500: Loss = -12358.166903682208
4
Iteration 15600: Loss = -12358.136431832498
Iteration 15700: Loss = -12358.145795193848
1
Iteration 15800: Loss = -12358.13632544201
Iteration 15900: Loss = -12358.136860359293
1
Iteration 16000: Loss = -12358.1690944788
2
Iteration 16100: Loss = -12358.136391052061
Iteration 16200: Loss = -12358.174875140898
1
Iteration 16300: Loss = -12358.144220190608
2
Iteration 16400: Loss = -12358.136846239939
3
Iteration 16500: Loss = -12358.136560005507
4
Iteration 16600: Loss = -12358.168915780736
5
Iteration 16700: Loss = -12358.136273993132
Iteration 16800: Loss = -12358.136581455079
1
Iteration 16900: Loss = -12358.309792825421
2
Iteration 17000: Loss = -12358.137180727292
3
Iteration 17100: Loss = -12358.140394938715
4
Iteration 17200: Loss = -12358.13744457613
5
Iteration 17300: Loss = -12358.142497481404
6
Iteration 17400: Loss = -12358.136436394236
7
Iteration 17500: Loss = -12358.139149453598
8
Iteration 17600: Loss = -12358.138272877299
9
Iteration 17700: Loss = -12358.18029067115
10
Iteration 17800: Loss = -12358.157514600607
11
Iteration 17900: Loss = -12358.139304167902
12
Iteration 18000: Loss = -12358.137105476744
13
Iteration 18100: Loss = -12358.155438606362
14
Iteration 18200: Loss = -12358.237880730632
15
Stopping early at iteration 18200 due to no improvement.
pi: tensor([[7.2995e-01, 2.7005e-01],
        [1.2132e-04, 9.9988e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9736, 0.0264], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1955, 0.2347],
         [0.6690, 0.2035]],

        [[0.5260, 0.1935],
         [0.7194, 0.6539]],

        [[0.5939, 0.1951],
         [0.5214, 0.6407]],

        [[0.6991, 0.2107],
         [0.6825, 0.5136]],

        [[0.5534, 0.1957],
         [0.6678, 0.6678]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.008773023868670224
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001593950016376342
Average Adjusted Rand Index: -0.0017546047737340448
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22236.38674362831
Iteration 100: Loss = -12361.346727962
Iteration 200: Loss = -12360.44283225388
Iteration 300: Loss = -12360.177403331438
Iteration 400: Loss = -12360.051064289577
Iteration 500: Loss = -12359.971570439211
Iteration 600: Loss = -12359.908046391078
Iteration 700: Loss = -12359.853015199202
Iteration 800: Loss = -12359.80334896723
Iteration 900: Loss = -12359.755532345334
Iteration 1000: Loss = -12359.70611509229
Iteration 1100: Loss = -12359.652465615745
Iteration 1200: Loss = -12359.593844809488
Iteration 1300: Loss = -12359.533275593143
Iteration 1400: Loss = -12359.477288388316
Iteration 1500: Loss = -12359.430919352875
Iteration 1600: Loss = -12359.392490636032
Iteration 1700: Loss = -12359.359933559515
Iteration 1800: Loss = -12359.334083398697
Iteration 1900: Loss = -12359.315266743171
Iteration 2000: Loss = -12359.301952306192
Iteration 2100: Loss = -12359.292317035399
Iteration 2200: Loss = -12359.285070552613
Iteration 2300: Loss = -12359.279485540656
Iteration 2400: Loss = -12359.275035015442
Iteration 2500: Loss = -12359.271361564248
Iteration 2600: Loss = -12359.268154140784
Iteration 2700: Loss = -12359.265320170049
Iteration 2800: Loss = -12359.262736143533
Iteration 2900: Loss = -12359.260252297143
Iteration 3000: Loss = -12359.258005846272
Iteration 3100: Loss = -12359.255800365823
Iteration 3200: Loss = -12359.253759483232
Iteration 3300: Loss = -12359.251742321525
Iteration 3400: Loss = -12359.249916901988
Iteration 3500: Loss = -12359.248191266046
Iteration 3600: Loss = -12359.24653179437
Iteration 3700: Loss = -12359.245003700398
Iteration 3800: Loss = -12359.24357045282
Iteration 3900: Loss = -12359.242202795514
Iteration 4000: Loss = -12359.240951166687
Iteration 4100: Loss = -12359.239806969646
Iteration 4200: Loss = -12359.238730750305
Iteration 4300: Loss = -12359.237715686268
Iteration 4400: Loss = -12359.236805858529
Iteration 4500: Loss = -12359.235970914337
Iteration 4600: Loss = -12359.235157015784
Iteration 4700: Loss = -12359.234368920239
Iteration 4800: Loss = -12359.233662476521
Iteration 4900: Loss = -12359.232978419888
Iteration 5000: Loss = -12359.232367728933
Iteration 5100: Loss = -12359.231722349467
Iteration 5200: Loss = -12359.231101858428
Iteration 5300: Loss = -12359.231171135147
Iteration 5400: Loss = -12359.230088857572
Iteration 5500: Loss = -12359.229542542478
Iteration 5600: Loss = -12359.22915796226
Iteration 5700: Loss = -12359.228635670628
Iteration 5800: Loss = -12359.228193094861
Iteration 5900: Loss = -12359.227894717416
Iteration 6000: Loss = -12359.227389885911
Iteration 6100: Loss = -12359.227042755161
Iteration 6200: Loss = -12359.226717533256
Iteration 6300: Loss = -12359.226369968092
Iteration 6400: Loss = -12359.230051938033
1
Iteration 6500: Loss = -12359.22573523783
Iteration 6600: Loss = -12359.225540345755
Iteration 6700: Loss = -12359.226266271233
1
Iteration 6800: Loss = -12359.225365712948
Iteration 6900: Loss = -12359.22468746643
Iteration 7000: Loss = -12359.225090125494
1
Iteration 7100: Loss = -12359.224231750735
Iteration 7200: Loss = -12359.224066663612
Iteration 7300: Loss = -12359.223816838557
Iteration 7400: Loss = -12359.2356571169
1
Iteration 7500: Loss = -12359.223428967985
Iteration 7600: Loss = -12359.223239456716
Iteration 7700: Loss = -12359.224011906635
1
Iteration 7800: Loss = -12359.222900101566
Iteration 7900: Loss = -12359.222753529873
Iteration 8000: Loss = -12359.224801148053
1
Iteration 8100: Loss = -12359.222513796354
Iteration 8200: Loss = -12359.22237436673
Iteration 8300: Loss = -12359.235385282416
1
Iteration 8400: Loss = -12359.222109344864
Iteration 8500: Loss = -12359.221995771491
Iteration 8600: Loss = -12359.224249375764
1
Iteration 8700: Loss = -12359.221780077954
Iteration 8800: Loss = -12359.221678812833
Iteration 8900: Loss = -12359.221616656583
Iteration 9000: Loss = -12359.221692320143
Iteration 9100: Loss = -12359.22146190191
Iteration 9200: Loss = -12359.221395823923
Iteration 9300: Loss = -12359.225867827308
1
Iteration 9400: Loss = -12359.221233488739
Iteration 9500: Loss = -12359.221119491498
Iteration 9600: Loss = -12359.222041205207
1
Iteration 9700: Loss = -12359.220992957024
Iteration 9800: Loss = -12359.220909488718
Iteration 9900: Loss = -12359.232130946983
1
Iteration 10000: Loss = -12359.22082452344
Iteration 10100: Loss = -12359.220771448287
Iteration 10200: Loss = -12359.22519737464
1
Iteration 10300: Loss = -12359.220701478469
Iteration 10400: Loss = -12359.220650938663
Iteration 10500: Loss = -12359.31484323779
1
Iteration 10600: Loss = -12359.220567768143
Iteration 10700: Loss = -12359.220541791856
Iteration 10800: Loss = -12359.784005483372
1
Iteration 10900: Loss = -12359.220452802492
Iteration 11000: Loss = -12359.220423133414
Iteration 11100: Loss = -12359.350613503004
1
Iteration 11200: Loss = -12359.220396664703
Iteration 11300: Loss = -12359.22034882295
Iteration 11400: Loss = -12359.220322053137
Iteration 11500: Loss = -12359.220327980218
Iteration 11600: Loss = -12359.220271838105
Iteration 11700: Loss = -12359.221024810637
1
Iteration 11800: Loss = -12359.33727800855
2
Iteration 11900: Loss = -12359.220221000305
Iteration 12000: Loss = -12359.229046787194
1
Iteration 12100: Loss = -12359.220141859721
Iteration 12200: Loss = -12359.221933474624
1
Iteration 12300: Loss = -12359.220325900758
2
Iteration 12400: Loss = -12359.263521635423
3
Iteration 12500: Loss = -12359.220092322426
Iteration 12600: Loss = -12359.225621890457
1
Iteration 12700: Loss = -12359.235920720863
2
Iteration 12800: Loss = -12359.220034320582
Iteration 12900: Loss = -12359.22552545773
1
Iteration 13000: Loss = -12359.220061866568
Iteration 13100: Loss = -12359.245432288979
1
Iteration 13200: Loss = -12359.2200444276
Iteration 13300: Loss = -12359.220014248309
Iteration 13400: Loss = -12359.5913473117
1
Iteration 13500: Loss = -12359.220022951196
Iteration 13600: Loss = -12359.220000261657
Iteration 13700: Loss = -12359.228998858702
1
Iteration 13800: Loss = -12359.458552438906
2
Iteration 13900: Loss = -12359.220069848447
Iteration 14000: Loss = -12359.220291805257
1
Iteration 14100: Loss = -12359.2538520899
2
Iteration 14200: Loss = -12359.219958629326
Iteration 14300: Loss = -12359.220026364106
Iteration 14400: Loss = -12359.222099481101
1
Iteration 14500: Loss = -12359.226638242555
2
Iteration 14600: Loss = -12359.223434531434
3
Iteration 14700: Loss = -12359.219967595547
Iteration 14800: Loss = -12359.41089702203
1
Iteration 14900: Loss = -12359.219930373738
Iteration 15000: Loss = -12359.220436343321
1
Iteration 15100: Loss = -12359.231597886048
2
Iteration 15200: Loss = -12359.219973838699
Iteration 15300: Loss = -12359.220135538835
1
Iteration 15400: Loss = -12359.22927722276
2
Iteration 15500: Loss = -12359.226854604114
3
Iteration 15600: Loss = -12359.219894843285
Iteration 15700: Loss = -12359.241235804397
1
Iteration 15800: Loss = -12359.222950551655
2
Iteration 15900: Loss = -12359.220583353397
3
Iteration 16000: Loss = -12359.269901017913
4
Iteration 16100: Loss = -12359.225991817559
5
Iteration 16200: Loss = -12359.219843267776
Iteration 16300: Loss = -12359.221875112478
1
Iteration 16400: Loss = -12359.428936133714
2
Iteration 16500: Loss = -12359.21984595994
Iteration 16600: Loss = -12359.220139122961
1
Iteration 16700: Loss = -12359.219892428458
Iteration 16800: Loss = -12359.219901692062
Iteration 16900: Loss = -12359.220262350535
1
Iteration 17000: Loss = -12359.221807521324
2
Iteration 17100: Loss = -12359.227460788397
3
Iteration 17200: Loss = -12359.220007869662
4
Iteration 17300: Loss = -12359.22057956514
5
Iteration 17400: Loss = -12359.222327801215
6
Iteration 17500: Loss = -12359.219820646385
Iteration 17600: Loss = -12359.220506305042
1
Iteration 17700: Loss = -12359.219900251175
Iteration 17800: Loss = -12359.313006609917
1
Iteration 17900: Loss = -12359.338799604113
2
Iteration 18000: Loss = -12359.21980640793
Iteration 18100: Loss = -12359.221086040143
1
Iteration 18200: Loss = -12359.219826169081
Iteration 18300: Loss = -12359.220447738346
1
Iteration 18400: Loss = -12359.223286795577
2
Iteration 18500: Loss = -12359.22944939682
3
Iteration 18600: Loss = -12359.220010896464
4
Iteration 18700: Loss = -12359.220057655471
5
Iteration 18800: Loss = -12359.422688646699
6
Iteration 18900: Loss = -12359.220361219375
7
Iteration 19000: Loss = -12359.267300536578
8
Iteration 19100: Loss = -12359.219805849865
Iteration 19200: Loss = -12359.243576105977
1
Iteration 19300: Loss = -12359.219812638039
Iteration 19400: Loss = -12359.221741084879
1
Iteration 19500: Loss = -12359.21979117812
Iteration 19600: Loss = -12359.221320898763
1
Iteration 19700: Loss = -12359.219846080186
Iteration 19800: Loss = -12359.220822310825
1
Iteration 19900: Loss = -12359.287197946775
2
pi: tensor([[1.0664e-05, 9.9999e-01],
        [3.1855e-02, 9.6814e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0490, 0.9510], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2582, 0.2265],
         [0.7135, 0.1978]],

        [[0.6129, 0.1853],
         [0.5334, 0.5787]],

        [[0.5869, 0.2752],
         [0.5629, 0.6767]],

        [[0.5887, 0.2530],
         [0.6759, 0.5620]],

        [[0.5348, 0.2018],
         [0.5604, 0.5090]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0001967235538417382
Average Adjusted Rand Index: 0.0005107695272972583
11846.4033190645
[-0.001593950016376342, 0.0001967235538417382] [-0.0017546047737340448, 0.0005107695272972583] [12358.237880730632, 12359.259992554364]
-------------------------------------
This iteration is 48
True Objective function: Loss = -11817.709651832021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21997.554524474643
Iteration 100: Loss = -12367.764706877688
Iteration 200: Loss = -12366.739384742272
Iteration 300: Loss = -12366.385165538308
Iteration 400: Loss = -12366.160081373244
Iteration 500: Loss = -12365.958629872244
Iteration 600: Loss = -12365.750638132899
Iteration 700: Loss = -12365.533933519153
Iteration 800: Loss = -12365.28259250317
Iteration 900: Loss = -12364.578831414163
Iteration 1000: Loss = -12363.676625097047
Iteration 1100: Loss = -12362.953306448291
Iteration 1200: Loss = -12362.492766917607
Iteration 1300: Loss = -12362.285330989347
Iteration 1400: Loss = -12362.16377094608
Iteration 1500: Loss = -12362.08258167726
Iteration 1600: Loss = -12362.031206401676
Iteration 1700: Loss = -12361.993367348234
Iteration 1800: Loss = -12361.961914127674
Iteration 1900: Loss = -12361.91775565766
Iteration 2000: Loss = -12361.796166484555
Iteration 2100: Loss = -12361.219952906042
Iteration 2200: Loss = -12359.874755010786
Iteration 2300: Loss = -12359.517652108456
Iteration 2400: Loss = -12359.348200557066
Iteration 2500: Loss = -12359.244335996684
Iteration 2600: Loss = -12359.177687904794
Iteration 2700: Loss = -12359.132414054737
Iteration 2800: Loss = -12359.09996226124
Iteration 2900: Loss = -12359.075706131895
Iteration 3000: Loss = -12359.056998502741
Iteration 3100: Loss = -12359.042195315285
Iteration 3200: Loss = -12359.030223901818
Iteration 3300: Loss = -12359.020296114395
Iteration 3400: Loss = -12359.012013805253
Iteration 3500: Loss = -12359.00502906472
Iteration 3600: Loss = -12358.998987396188
Iteration 3700: Loss = -12358.993773084247
Iteration 3800: Loss = -12358.989274361518
Iteration 3900: Loss = -12358.985263699526
Iteration 4000: Loss = -12358.981758362339
Iteration 4100: Loss = -12358.978632367172
Iteration 4200: Loss = -12358.97580302542
Iteration 4300: Loss = -12358.973342208119
Iteration 4400: Loss = -12358.971049400709
Iteration 4500: Loss = -12358.969009090579
Iteration 4600: Loss = -12358.967146463976
Iteration 4700: Loss = -12358.965459759052
Iteration 4800: Loss = -12358.963908632031
Iteration 4900: Loss = -12358.962475723501
Iteration 5000: Loss = -12358.96341131744
1
Iteration 5100: Loss = -12358.959935599038
Iteration 5200: Loss = -12358.958872633297
Iteration 5300: Loss = -12358.957850981902
Iteration 5400: Loss = -12358.956889071105
Iteration 5500: Loss = -12358.956128543621
Iteration 5600: Loss = -12358.955206206887
Iteration 5700: Loss = -12358.954479856193
Iteration 5800: Loss = -12358.953767419014
Iteration 5900: Loss = -12358.953159907891
Iteration 6000: Loss = -12358.97044983319
1
Iteration 6100: Loss = -12358.951926562546
Iteration 6200: Loss = -12358.951404739695
Iteration 6300: Loss = -12358.950956209685
Iteration 6400: Loss = -12358.950454959191
Iteration 6500: Loss = -12358.949993201566
Iteration 6600: Loss = -12358.949626433472
Iteration 6700: Loss = -12358.949238651387
Iteration 6800: Loss = -12358.95182478659
1
Iteration 6900: Loss = -12358.948525970534
Iteration 7000: Loss = -12358.948237598297
Iteration 7100: Loss = -12358.947916589448
Iteration 7200: Loss = -12358.947595416814
Iteration 7300: Loss = -12358.947351827319
Iteration 7400: Loss = -12358.947119973935
Iteration 7500: Loss = -12358.94805322138
1
Iteration 7600: Loss = -12358.946732624
Iteration 7700: Loss = -12358.946470871246
Iteration 7800: Loss = -12359.023702074639
1
Iteration 7900: Loss = -12358.946036632306
Iteration 8000: Loss = -12358.946329549088
1
Iteration 8100: Loss = -12358.947660016855
2
Iteration 8200: Loss = -12358.967936082736
3
Iteration 8300: Loss = -12358.968915354113
4
Iteration 8400: Loss = -12358.94740397147
5
Iteration 8500: Loss = -12358.994907834378
6
Iteration 8600: Loss = -12358.951892345945
7
Iteration 8700: Loss = -12358.956767381937
8
Iteration 8800: Loss = -12358.944797279946
Iteration 8900: Loss = -12358.94467290507
Iteration 9000: Loss = -12358.94457211506
Iteration 9100: Loss = -12358.944535372348
Iteration 9200: Loss = -12358.944355608515
Iteration 9300: Loss = -12358.94490123978
1
Iteration 9400: Loss = -12358.944199293024
Iteration 9500: Loss = -12358.95486758809
1
Iteration 9600: Loss = -12358.944674873426
2
Iteration 9700: Loss = -12358.944442728269
3
Iteration 9800: Loss = -12358.943995007316
Iteration 9900: Loss = -12358.944241334486
1
Iteration 10000: Loss = -12359.003553086624
2
Iteration 10100: Loss = -12358.943708146004
Iteration 10200: Loss = -12358.944180674484
1
Iteration 10300: Loss = -12358.95846191399
2
Iteration 10400: Loss = -12358.945579766605
3
Iteration 10500: Loss = -12358.94355083446
Iteration 10600: Loss = -12358.944215834152
1
Iteration 10700: Loss = -12359.152999489499
2
Iteration 10800: Loss = -12358.943839390948
3
Iteration 10900: Loss = -12358.945189170536
4
Iteration 11000: Loss = -12358.947906038913
5
Iteration 11100: Loss = -12359.05051352087
6
Iteration 11200: Loss = -12359.084569347324
7
Iteration 11300: Loss = -12358.94334759968
Iteration 11400: Loss = -12358.943305757986
Iteration 11500: Loss = -12359.104584486084
1
Iteration 11600: Loss = -12358.94312568855
Iteration 11700: Loss = -12359.224748803219
1
Iteration 11800: Loss = -12358.94316832795
Iteration 11900: Loss = -12358.984787741469
1
Iteration 12000: Loss = -12358.967771953248
2
Iteration 12100: Loss = -12358.946203276366
3
Iteration 12200: Loss = -12358.94321462319
Iteration 12300: Loss = -12358.945069956271
1
Iteration 12400: Loss = -12358.944808336992
2
Iteration 12500: Loss = -12358.971439738698
3
Iteration 12600: Loss = -12358.945708816587
4
Iteration 12700: Loss = -12358.942928897792
Iteration 12800: Loss = -12358.94671261533
1
Iteration 12900: Loss = -12358.942864229464
Iteration 13000: Loss = -12358.948871847428
1
Iteration 13100: Loss = -12358.943396539586
2
Iteration 13200: Loss = -12358.942878635773
Iteration 13300: Loss = -12359.184631691307
1
Iteration 13400: Loss = -12358.944167203246
2
Iteration 13500: Loss = -12358.954206817512
3
Iteration 13600: Loss = -12358.94280572684
Iteration 13700: Loss = -12358.943578072005
1
Iteration 13800: Loss = -12358.951265835442
2
Iteration 13900: Loss = -12358.943672425848
3
Iteration 14000: Loss = -12358.942839772822
Iteration 14100: Loss = -12358.942942929483
1
Iteration 14200: Loss = -12358.94954996834
2
Iteration 14300: Loss = -12358.942786204
Iteration 14400: Loss = -12358.942749528516
Iteration 14500: Loss = -12358.963430630321
1
Iteration 14600: Loss = -12358.942908375815
2
Iteration 14700: Loss = -12358.955884005274
3
Iteration 14800: Loss = -12358.945156302208
4
Iteration 14900: Loss = -12358.945693891386
5
Iteration 15000: Loss = -12358.946297906148
6
Iteration 15100: Loss = -12358.943508033813
7
Iteration 15200: Loss = -12358.943066160848
8
Iteration 15300: Loss = -12359.054051076355
9
Iteration 15400: Loss = -12358.957040131962
10
Iteration 15500: Loss = -12358.943161703426
11
Iteration 15600: Loss = -12358.944271531114
12
Iteration 15700: Loss = -12358.942706316993
Iteration 15800: Loss = -12358.953435274994
1
Iteration 15900: Loss = -12358.952747543693
2
Iteration 16000: Loss = -12358.974824022705
3
Iteration 16100: Loss = -12358.94282815919
4
Iteration 16200: Loss = -12358.949083851525
5
Iteration 16300: Loss = -12358.942879417995
6
Iteration 16400: Loss = -12358.942683147812
Iteration 16500: Loss = -12358.988349994619
1
Iteration 16600: Loss = -12358.942690329426
Iteration 16700: Loss = -12358.942754586142
Iteration 16800: Loss = -12359.025321290415
1
Iteration 16900: Loss = -12358.966151000162
2
Iteration 17000: Loss = -12358.988184079675
3
Iteration 17100: Loss = -12359.08943601892
4
Iteration 17200: Loss = -12358.942799330896
Iteration 17300: Loss = -12358.942707132543
Iteration 17400: Loss = -12358.95605085399
1
Iteration 17500: Loss = -12358.942705233263
Iteration 17600: Loss = -12358.957156415081
1
Iteration 17700: Loss = -12358.948064472233
2
Iteration 17800: Loss = -12358.954210338155
3
Iteration 17900: Loss = -12358.94583835375
4
Iteration 18000: Loss = -12358.957074688442
5
Iteration 18100: Loss = -12358.944112772424
6
Iteration 18200: Loss = -12358.943497868773
7
Iteration 18300: Loss = -12358.945085698242
8
Iteration 18400: Loss = -12358.947318521828
9
Iteration 18500: Loss = -12358.947608134122
10
Iteration 18600: Loss = -12359.047010052245
11
Iteration 18700: Loss = -12358.958791261302
12
Iteration 18800: Loss = -12359.025108736249
13
Iteration 18900: Loss = -12358.946760227129
14
Iteration 19000: Loss = -12358.9435022462
15
Stopping early at iteration 19000 due to no improvement.
pi: tensor([[3.1797e-01, 6.8203e-01],
        [7.9295e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2748, 0.7252], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1772, 0.1934],
         [0.5010, 0.2025]],

        [[0.5717, 0.1313],
         [0.5800, 0.5331]],

        [[0.7258, 0.3032],
         [0.6423, 0.6343]],

        [[0.5780, 0.0820],
         [0.5304, 0.6822]],

        [[0.7307, 0.1933],
         [0.6707, 0.5272]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.005131431169739117
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002821183194503557
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0003545984953076914
Average Adjusted Rand Index: -0.0024475126960943837
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21453.604507786982
Iteration 100: Loss = -12367.125348934056
Iteration 200: Loss = -12366.449778976252
Iteration 300: Loss = -12366.038433160986
Iteration 400: Loss = -12365.740307359138
Iteration 500: Loss = -12365.391147379472
Iteration 600: Loss = -12364.614699640475
Iteration 700: Loss = -12364.014732784783
Iteration 800: Loss = -12363.665532416671
Iteration 900: Loss = -12363.286663764875
Iteration 1000: Loss = -12362.812903218062
Iteration 1100: Loss = -12362.455562203579
Iteration 1200: Loss = -12362.237081783676
Iteration 1300: Loss = -12362.114220393369
Iteration 1400: Loss = -12362.031028205758
Iteration 1500: Loss = -12361.9607670758
Iteration 1600: Loss = -12361.890281941889
Iteration 1700: Loss = -12361.781897194509
Iteration 1800: Loss = -12361.469661737623
Iteration 1900: Loss = -12360.863593501974
Iteration 2000: Loss = -12359.836288795028
Iteration 2100: Loss = -12359.288794272292
Iteration 2200: Loss = -12359.18215351067
Iteration 2300: Loss = -12359.13444350729
Iteration 2400: Loss = -12359.102665778324
Iteration 2500: Loss = -12359.078983954481
Iteration 2600: Loss = -12359.060597653086
Iteration 2700: Loss = -12359.04584039742
Iteration 2800: Loss = -12359.03381707244
Iteration 2900: Loss = -12359.023787061096
Iteration 3000: Loss = -12359.015341084014
Iteration 3100: Loss = -12359.008186792833
Iteration 3200: Loss = -12359.001957580618
Iteration 3300: Loss = -12358.996578786664
Iteration 3400: Loss = -12358.991854635591
Iteration 3500: Loss = -12358.987716109936
Iteration 3600: Loss = -12358.9840178904
Iteration 3700: Loss = -12358.980776207609
Iteration 3800: Loss = -12358.97784814823
Iteration 3900: Loss = -12358.975231103972
Iteration 4000: Loss = -12358.972809271845
Iteration 4100: Loss = -12358.970667663465
Iteration 4200: Loss = -12358.96870344869
Iteration 4300: Loss = -12358.966882922168
Iteration 4400: Loss = -12358.965265052559
Iteration 4500: Loss = -12358.963716373702
Iteration 4600: Loss = -12358.962360116278
Iteration 4700: Loss = -12358.961055990696
Iteration 4800: Loss = -12358.959929294637
Iteration 4900: Loss = -12358.959253578978
Iteration 5000: Loss = -12358.957827864637
Iteration 5100: Loss = -12358.956908160071
Iteration 5200: Loss = -12358.958119185498
1
Iteration 5300: Loss = -12358.955238506675
Iteration 5400: Loss = -12358.954593244785
Iteration 5500: Loss = -12358.953811200729
Iteration 5600: Loss = -12358.953212861981
Iteration 5700: Loss = -12358.952567960567
Iteration 5800: Loss = -12358.95199803087
Iteration 5900: Loss = -12358.951460176284
Iteration 6000: Loss = -12358.969978726225
1
Iteration 6100: Loss = -12358.950507204823
Iteration 6200: Loss = -12358.950066918149
Iteration 6300: Loss = -12358.95035185877
1
Iteration 6400: Loss = -12358.949267080568
Iteration 6500: Loss = -12358.948886378881
Iteration 6600: Loss = -12358.948562552336
Iteration 6700: Loss = -12358.948245961124
Iteration 6800: Loss = -12358.954964487788
1
Iteration 6900: Loss = -12358.947648009454
Iteration 7000: Loss = -12358.947410610643
Iteration 7100: Loss = -12358.947134929087
Iteration 7200: Loss = -12358.946926546621
Iteration 7300: Loss = -12358.946891715472
Iteration 7400: Loss = -12358.946477276146
Iteration 7500: Loss = -12358.951802865418
1
Iteration 7600: Loss = -12358.94603641931
Iteration 7700: Loss = -12358.94603574621
Iteration 7800: Loss = -12358.94569751351
Iteration 7900: Loss = -12358.990906725377
1
Iteration 8000: Loss = -12358.945427299563
Iteration 8100: Loss = -12358.945244782646
Iteration 8200: Loss = -12358.969965207196
1
Iteration 8300: Loss = -12358.944981162607
Iteration 8400: Loss = -12358.944867877997
Iteration 8500: Loss = -12358.997504457095
1
Iteration 8600: Loss = -12358.944650524792
Iteration 8700: Loss = -12358.945176635974
1
Iteration 8800: Loss = -12358.944451303863
Iteration 8900: Loss = -12358.946021487009
1
Iteration 9000: Loss = -12358.944289459463
Iteration 9100: Loss = -12358.945738521594
1
Iteration 9200: Loss = -12358.94414127012
Iteration 9300: Loss = -12358.944457170488
1
Iteration 9400: Loss = -12358.94437444737
2
Iteration 9500: Loss = -12358.95422189653
3
Iteration 9600: Loss = -12358.944799723413
4
Iteration 9700: Loss = -12358.985219095242
5
Iteration 9800: Loss = -12358.943733859001
Iteration 9900: Loss = -12358.988237325757
1
Iteration 10000: Loss = -12358.943613323261
Iteration 10100: Loss = -12358.946469651411
1
Iteration 10200: Loss = -12358.943496878615
Iteration 10300: Loss = -12358.943707314516
1
Iteration 10400: Loss = -12359.073723459076
2
Iteration 10500: Loss = -12358.943367331363
Iteration 10600: Loss = -12358.943557078685
1
Iteration 10700: Loss = -12358.943309548165
Iteration 10800: Loss = -12358.943259436506
Iteration 10900: Loss = -12358.947525728721
1
Iteration 11000: Loss = -12358.943361711903
2
Iteration 11100: Loss = -12358.943310274128
Iteration 11200: Loss = -12358.953056641352
1
Iteration 11300: Loss = -12358.943079572355
Iteration 11400: Loss = -12358.943998585582
1
Iteration 11500: Loss = -12358.94325776773
2
Iteration 11600: Loss = -12358.943106379635
Iteration 11700: Loss = -12358.943511226043
1
Iteration 11800: Loss = -12358.943522014017
2
Iteration 11900: Loss = -12359.034226170175
3
Iteration 12000: Loss = -12358.943765598473
4
Iteration 12100: Loss = -12358.942954769838
Iteration 12200: Loss = -12358.943373263388
1
Iteration 12300: Loss = -12358.957599903038
2
Iteration 12400: Loss = -12358.942959747566
Iteration 12500: Loss = -12358.942882252812
Iteration 12600: Loss = -12358.942971539233
Iteration 12700: Loss = -12358.942892924833
Iteration 12800: Loss = -12358.943224068316
1
Iteration 12900: Loss = -12358.942861260559
Iteration 13000: Loss = -12358.94746176168
1
Iteration 13100: Loss = -12358.942792977927
Iteration 13200: Loss = -12359.02460338919
1
Iteration 13300: Loss = -12358.942938455335
2
Iteration 13400: Loss = -12358.94547066439
3
Iteration 13500: Loss = -12358.943965864255
4
Iteration 13600: Loss = -12358.95554891423
5
Iteration 13700: Loss = -12358.947505749846
6
Iteration 13800: Loss = -12358.943192188099
7
Iteration 13900: Loss = -12358.944470438484
8
Iteration 14000: Loss = -12358.947071968312
9
Iteration 14100: Loss = -12358.943539988712
10
Iteration 14200: Loss = -12358.949008894368
11
Iteration 14300: Loss = -12358.950928802285
12
Iteration 14400: Loss = -12359.088883072114
13
Iteration 14500: Loss = -12358.942712432736
Iteration 14600: Loss = -12358.943401698078
1
Iteration 14700: Loss = -12359.046294848788
2
Iteration 14800: Loss = -12358.943807877404
3
Iteration 14900: Loss = -12358.945448126968
4
Iteration 15000: Loss = -12358.943385052025
5
Iteration 15100: Loss = -12358.950441014193
6
Iteration 15200: Loss = -12358.94383893644
7
Iteration 15300: Loss = -12358.943480757358
8
Iteration 15400: Loss = -12358.94288608907
9
Iteration 15500: Loss = -12358.974064575086
10
Iteration 15600: Loss = -12358.942690137777
Iteration 15700: Loss = -12358.942757314677
Iteration 15800: Loss = -12358.942651447793
Iteration 15900: Loss = -12358.943040935732
1
Iteration 16000: Loss = -12358.942671226148
Iteration 16100: Loss = -12358.944315221961
1
Iteration 16200: Loss = -12358.942655004123
Iteration 16300: Loss = -12358.944438862885
1
Iteration 16400: Loss = -12358.94270364908
Iteration 16500: Loss = -12358.944995401333
1
Iteration 16600: Loss = -12358.942731393376
Iteration 16700: Loss = -12358.942955547294
1
Iteration 16800: Loss = -12359.031403727984
2
Iteration 16900: Loss = -12358.942634518547
Iteration 17000: Loss = -12358.943386545048
1
Iteration 17100: Loss = -12359.029916901893
2
Iteration 17200: Loss = -12358.942650245315
Iteration 17300: Loss = -12358.943421181348
1
Iteration 17400: Loss = -12358.991850020917
2
Iteration 17500: Loss = -12359.035737316537
3
Iteration 17600: Loss = -12358.957510496
4
Iteration 17700: Loss = -12358.968876137873
5
Iteration 17800: Loss = -12358.942956122282
6
Iteration 17900: Loss = -12358.942733728218
Iteration 18000: Loss = -12358.95708719855
1
Iteration 18100: Loss = -12358.944099185677
2
Iteration 18200: Loss = -12358.94381732388
3
Iteration 18300: Loss = -12358.95456115507
4
Iteration 18400: Loss = -12358.942649846229
Iteration 18500: Loss = -12358.943007530439
1
Iteration 18600: Loss = -12358.944064956378
2
Iteration 18700: Loss = -12358.942754828824
3
Iteration 18800: Loss = -12358.949256112752
4
Iteration 18900: Loss = -12358.950675535641
5
Iteration 19000: Loss = -12358.942740028882
Iteration 19100: Loss = -12358.94564380982
1
Iteration 19200: Loss = -12358.946396431495
2
Iteration 19300: Loss = -12358.944874590203
3
Iteration 19400: Loss = -12358.947135736154
4
Iteration 19500: Loss = -12358.951069255776
5
Iteration 19600: Loss = -12358.951575971818
6
Iteration 19700: Loss = -12358.942656489291
Iteration 19800: Loss = -12358.946260732071
1
Iteration 19900: Loss = -12358.946566325525
2
pi: tensor([[3.1948e-01, 6.8052e-01],
        [4.0514e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2728, 0.7272], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1769, 0.1930],
         [0.6649, 0.2026]],

        [[0.6436, 0.1307],
         [0.6633, 0.5681]],

        [[0.6035, 0.3034],
         [0.5086, 0.6363]],

        [[0.5963, 0.0819],
         [0.5720, 0.6756]],

        [[0.5928, 0.1919],
         [0.6388, 0.6624]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.005131431169739117
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002821183194503557
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0003545984953076914
Average Adjusted Rand Index: -0.0024475126960943837
11817.709651832021
[-0.0003545984953076914, -0.0003545984953076914] [-0.0024475126960943837, -0.0024475126960943837] [12358.9435022462, 12358.945098654614]
-------------------------------------
This iteration is 49
True Objective function: Loss = -11927.06910892735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20816.837618466136
Iteration 100: Loss = -12436.64366236164
Iteration 200: Loss = -12436.248059671525
Iteration 300: Loss = -12436.095484686028
Iteration 400: Loss = -12436.019441024713
Iteration 500: Loss = -12435.963376635636
Iteration 600: Loss = -12435.902243149963
Iteration 700: Loss = -12435.82891328601
Iteration 800: Loss = -12435.749211921915
Iteration 900: Loss = -12435.667996112272
Iteration 1000: Loss = -12435.580613126283
Iteration 1100: Loss = -12435.477468273411
Iteration 1200: Loss = -12435.359007426981
Iteration 1300: Loss = -12435.245677928373
Iteration 1400: Loss = -12435.156389560409
Iteration 1500: Loss = -12435.095000875997
Iteration 1600: Loss = -12435.056539277917
Iteration 1700: Loss = -12435.03257552608
Iteration 1800: Loss = -12435.016929683796
Iteration 1900: Loss = -12435.006374572553
Iteration 2000: Loss = -12434.999153821027
Iteration 2100: Loss = -12434.993865585217
Iteration 2200: Loss = -12434.989429356378
Iteration 2300: Loss = -12434.985227560965
Iteration 2400: Loss = -12434.980903592119
Iteration 2500: Loss = -12434.975775376963
Iteration 2600: Loss = -12434.968904855825
Iteration 2700: Loss = -12434.95837421042
Iteration 2800: Loss = -12434.939043621775
Iteration 2900: Loss = -12434.894889593525
Iteration 3000: Loss = -12434.777226606835
Iteration 3100: Loss = -12434.53436938275
Iteration 3200: Loss = -12434.301552137125
Iteration 3300: Loss = -12434.174162110172
Iteration 3400: Loss = -12434.1068373111
Iteration 3500: Loss = -12434.066680082973
Iteration 3600: Loss = -12434.040079911385
Iteration 3700: Loss = -12434.02131659481
Iteration 3800: Loss = -12434.007326749808
Iteration 3900: Loss = -12433.996492830629
Iteration 4000: Loss = -12433.987921038004
Iteration 4100: Loss = -12433.98094754942
Iteration 4200: Loss = -12433.975277326495
Iteration 4300: Loss = -12433.970445226985
Iteration 4400: Loss = -12433.966368300156
Iteration 4500: Loss = -12433.962816060017
Iteration 4600: Loss = -12433.959822348643
Iteration 4700: Loss = -12433.957196065818
Iteration 4800: Loss = -12433.954801881768
Iteration 4900: Loss = -12433.952730367924
Iteration 5000: Loss = -12433.950897457928
Iteration 5100: Loss = -12433.949258566574
Iteration 5200: Loss = -12433.9477479829
Iteration 5300: Loss = -12433.946394489067
Iteration 5400: Loss = -12433.945179452478
Iteration 5500: Loss = -12433.944048860621
Iteration 5600: Loss = -12433.943036715229
Iteration 5700: Loss = -12433.942107099869
Iteration 5800: Loss = -12433.94123382632
Iteration 5900: Loss = -12433.940430365292
Iteration 6000: Loss = -12433.939659830912
Iteration 6100: Loss = -12433.939011568067
Iteration 6200: Loss = -12433.938419676953
Iteration 6300: Loss = -12433.937818385184
Iteration 6400: Loss = -12433.937254013848
Iteration 6500: Loss = -12433.936759087843
Iteration 6600: Loss = -12433.936514872827
Iteration 6700: Loss = -12433.935850487691
Iteration 6800: Loss = -12434.008365866568
1
Iteration 6900: Loss = -12433.935041136825
Iteration 7000: Loss = -12433.93633109675
1
Iteration 7100: Loss = -12433.934420228923
Iteration 7200: Loss = -12433.934046461767
Iteration 7300: Loss = -12434.024039587091
1
Iteration 7400: Loss = -12433.933462223951
Iteration 7500: Loss = -12433.933197855344
Iteration 7600: Loss = -12433.932964140038
Iteration 7700: Loss = -12433.932756691063
Iteration 7800: Loss = -12433.932524666952
Iteration 7900: Loss = -12433.932293465532
Iteration 8000: Loss = -12433.933424465169
1
Iteration 8100: Loss = -12433.93204215909
Iteration 8200: Loss = -12433.931751132628
Iteration 8300: Loss = -12433.931619086885
Iteration 8400: Loss = -12433.931515235672
Iteration 8500: Loss = -12433.931254362758
Iteration 8600: Loss = -12433.93123825336
Iteration 8700: Loss = -12433.9311029278
Iteration 8800: Loss = -12433.930853773873
Iteration 8900: Loss = -12433.930634599741
Iteration 9000: Loss = -12433.935261030309
1
Iteration 9100: Loss = -12433.930238603909
Iteration 9200: Loss = -12433.93009369659
Iteration 9300: Loss = -12433.961875715064
1
Iteration 9400: Loss = -12433.929888856794
Iteration 9500: Loss = -12433.92985726474
Iteration 9600: Loss = -12434.01718996809
1
Iteration 9700: Loss = -12433.929649236654
Iteration 9800: Loss = -12433.929591371081
Iteration 9900: Loss = -12433.932707460255
1
Iteration 10000: Loss = -12433.929499272923
Iteration 10100: Loss = -12433.929393875713
Iteration 10200: Loss = -12433.929360825676
Iteration 10300: Loss = -12433.929395114987
Iteration 10400: Loss = -12433.929289568347
Iteration 10500: Loss = -12433.929238216622
Iteration 10600: Loss = -12433.942252817795
1
Iteration 10700: Loss = -12433.929083022924
Iteration 10800: Loss = -12433.92906742907
Iteration 10900: Loss = -12434.100428615455
1
Iteration 11000: Loss = -12433.928978289994
Iteration 11100: Loss = -12433.928933813932
Iteration 11200: Loss = -12433.928885297102
Iteration 11300: Loss = -12433.929057130645
1
Iteration 11400: Loss = -12433.928796291835
Iteration 11500: Loss = -12433.928767903268
Iteration 11600: Loss = -12433.928822761696
Iteration 11700: Loss = -12433.928718678515
Iteration 11800: Loss = -12433.92867936322
Iteration 11900: Loss = -12433.931638241791
1
Iteration 12000: Loss = -12433.928608071443
Iteration 12100: Loss = -12433.928545080367
Iteration 12200: Loss = -12433.929202719897
1
Iteration 12300: Loss = -12433.928525524689
Iteration 12400: Loss = -12433.929845858609
1
Iteration 12500: Loss = -12433.928496684046
Iteration 12600: Loss = -12433.97933895749
1
Iteration 12700: Loss = -12433.928490909233
Iteration 12800: Loss = -12433.928960124895
1
Iteration 12900: Loss = -12433.931214352964
2
Iteration 13000: Loss = -12433.928719177213
3
Iteration 13100: Loss = -12433.928688804297
4
Iteration 13200: Loss = -12433.94084398591
5
Iteration 13300: Loss = -12433.928391256488
Iteration 13400: Loss = -12433.929946913115
1
Iteration 13500: Loss = -12433.957311897464
2
Iteration 13600: Loss = -12433.928383343897
Iteration 13700: Loss = -12433.936781195218
1
Iteration 13800: Loss = -12433.928369894305
Iteration 13900: Loss = -12433.939242181388
1
Iteration 14000: Loss = -12433.928314187418
Iteration 14100: Loss = -12433.928969913624
1
Iteration 14200: Loss = -12433.928279972612
Iteration 14300: Loss = -12433.928298103066
Iteration 14400: Loss = -12433.928175186731
Iteration 14500: Loss = -12433.928189468217
Iteration 14600: Loss = -12433.944624101505
1
Iteration 14700: Loss = -12433.928187603178
Iteration 14800: Loss = -12433.937079090032
1
Iteration 14900: Loss = -12433.928167230946
Iteration 15000: Loss = -12433.969760321235
1
Iteration 15100: Loss = -12433.928147125307
Iteration 15200: Loss = -12433.92810526581
Iteration 15300: Loss = -12433.928497136525
1
Iteration 15400: Loss = -12433.928065135815
Iteration 15500: Loss = -12433.950231273027
1
Iteration 15600: Loss = -12433.928072227305
Iteration 15700: Loss = -12433.92811984666
Iteration 15800: Loss = -12433.92933986091
1
Iteration 15900: Loss = -12433.928047104575
Iteration 16000: Loss = -12433.928317695345
1
Iteration 16100: Loss = -12433.928034822437
Iteration 16200: Loss = -12433.928192261526
1
Iteration 16300: Loss = -12433.927986659291
Iteration 16400: Loss = -12433.9326334215
1
Iteration 16500: Loss = -12433.927986728078
Iteration 16600: Loss = -12433.928021848378
Iteration 16700: Loss = -12433.927982956458
Iteration 16800: Loss = -12433.93075747164
1
Iteration 16900: Loss = -12433.928056036466
Iteration 17000: Loss = -12433.929447870458
1
Iteration 17100: Loss = -12433.927964002212
Iteration 17200: Loss = -12433.92821342793
1
Iteration 17300: Loss = -12433.928533693668
2
Iteration 17400: Loss = -12433.928102155778
3
Iteration 17500: Loss = -12433.928217045332
4
Iteration 17600: Loss = -12433.92802881547
Iteration 17700: Loss = -12433.938527152957
1
Iteration 17800: Loss = -12433.927959570914
Iteration 17900: Loss = -12433.928900532665
1
Iteration 18000: Loss = -12433.928754836827
2
Iteration 18100: Loss = -12433.928555771658
3
Iteration 18200: Loss = -12433.949672918974
4
Iteration 18300: Loss = -12433.927980393652
Iteration 18400: Loss = -12433.927991419296
Iteration 18500: Loss = -12433.92790010634
Iteration 18600: Loss = -12433.928069474892
1
Iteration 18700: Loss = -12433.927895215736
Iteration 18800: Loss = -12433.952877464453
1
Iteration 18900: Loss = -12433.927899148472
Iteration 19000: Loss = -12433.933379245284
1
Iteration 19100: Loss = -12433.92789103912
Iteration 19200: Loss = -12433.962775344084
1
Iteration 19300: Loss = -12433.927934022106
Iteration 19400: Loss = -12433.929038032076
1
Iteration 19500: Loss = -12433.927965317127
Iteration 19600: Loss = -12433.92787545405
Iteration 19700: Loss = -12433.934612682442
1
Iteration 19800: Loss = -12433.928132020545
2
Iteration 19900: Loss = -12433.928350098617
3
pi: tensor([[1.0000e+00, 3.6016e-08],
        [1.5552e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9665, 0.0335], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2000, 0.2317],
         [0.5462, 0.1797]],

        [[0.5513, 0.2940],
         [0.5567, 0.5952]],

        [[0.7010, 0.1658],
         [0.5176, 0.5305]],

        [[0.6565, 0.1955],
         [0.5676, 0.5200]],

        [[0.5076, 0.2326],
         [0.5142, 0.7024]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: -0.000344133339997668
Average Adjusted Rand Index: -0.0004976719855557115
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21214.00109140098
Iteration 100: Loss = -12436.99578490096
Iteration 200: Loss = -12436.354830705364
Iteration 300: Loss = -12436.20232973343
Iteration 400: Loss = -12436.12547413856
Iteration 500: Loss = -12436.071028417899
Iteration 600: Loss = -12436.023396177445
Iteration 700: Loss = -12435.972906876797
Iteration 800: Loss = -12435.905558019705
Iteration 900: Loss = -12435.788419263963
Iteration 1000: Loss = -12435.571294396987
Iteration 1100: Loss = -12435.309793959668
Iteration 1200: Loss = -12435.089287133591
Iteration 1300: Loss = -12434.932731603525
Iteration 1400: Loss = -12434.843157512965
Iteration 1500: Loss = -12434.796170863028
Iteration 1600: Loss = -12434.76665855452
Iteration 1700: Loss = -12434.744714378767
Iteration 1800: Loss = -12434.727023279576
Iteration 1900: Loss = -12434.712551332872
Iteration 2000: Loss = -12434.701038823232
Iteration 2100: Loss = -12434.69209155074
Iteration 2200: Loss = -12434.685110599856
Iteration 2300: Loss = -12434.679780105153
Iteration 2400: Loss = -12434.675744472868
Iteration 2500: Loss = -12434.672638822783
Iteration 2600: Loss = -12434.670283674812
Iteration 2700: Loss = -12434.668400132523
Iteration 2800: Loss = -12434.666951105788
Iteration 2900: Loss = -12434.665772779059
Iteration 3000: Loss = -12434.664731352788
Iteration 3100: Loss = -12434.663805419916
Iteration 3200: Loss = -12434.663024787473
Iteration 3300: Loss = -12434.662242963504
Iteration 3400: Loss = -12434.661563320517
Iteration 3500: Loss = -12434.66090681446
Iteration 3600: Loss = -12434.660283166231
Iteration 3700: Loss = -12434.65968763212
Iteration 3800: Loss = -12434.659111973502
Iteration 3900: Loss = -12434.658540551778
Iteration 4000: Loss = -12434.657909642643
Iteration 4100: Loss = -12434.657548966266
Iteration 4200: Loss = -12434.656777528135
Iteration 4300: Loss = -12434.656142625598
Iteration 4400: Loss = -12434.655593139905
Iteration 4500: Loss = -12434.65483209369
Iteration 4600: Loss = -12434.660753257718
1
Iteration 4700: Loss = -12434.653232156998
Iteration 4800: Loss = -12434.65237255881
Iteration 4900: Loss = -12434.651383685501
Iteration 5000: Loss = -12434.65017687105
Iteration 5100: Loss = -12434.654348426613
1
Iteration 5200: Loss = -12434.647170155913
Iteration 5300: Loss = -12434.645184774776
Iteration 5400: Loss = -12434.642845572405
Iteration 5500: Loss = -12434.639887863264
Iteration 5600: Loss = -12434.63637488991
Iteration 5700: Loss = -12434.631917330737
Iteration 5800: Loss = -12434.62661229887
Iteration 5900: Loss = -12434.620052191218
Iteration 6000: Loss = -12434.61267245158
Iteration 6100: Loss = -12434.604488959312
Iteration 6200: Loss = -12434.596338326532
Iteration 6300: Loss = -12434.58926849211
Iteration 6400: Loss = -12434.583794988455
Iteration 6500: Loss = -12434.580045005909
Iteration 6600: Loss = -12434.576913093708
Iteration 6700: Loss = -12434.574833013994
Iteration 6800: Loss = -12434.573189751101
Iteration 6900: Loss = -12434.571944516614
Iteration 7000: Loss = -12434.570970887948
Iteration 7100: Loss = -12434.570178803497
Iteration 7200: Loss = -12434.569526939995
Iteration 7300: Loss = -12434.569443497798
Iteration 7400: Loss = -12434.568604024884
Iteration 7500: Loss = -12434.568225062421
Iteration 7600: Loss = -12434.603678797368
1
Iteration 7700: Loss = -12434.567535020815
Iteration 7800: Loss = -12434.567326065573
Iteration 7900: Loss = -12434.567066132622
Iteration 8000: Loss = -12434.566936404299
Iteration 8100: Loss = -12434.570714382427
1
Iteration 8200: Loss = -12434.56660649527
Iteration 8300: Loss = -12434.566504791066
Iteration 8400: Loss = -12435.02250004751
1
Iteration 8500: Loss = -12434.56629583182
Iteration 8600: Loss = -12434.566261545455
Iteration 8700: Loss = -12434.68128225201
1
Iteration 8800: Loss = -12434.566098597055
Iteration 8900: Loss = -12434.566050256082
Iteration 9000: Loss = -12434.56747330098
1
Iteration 9100: Loss = -12434.633055983755
2
Iteration 9200: Loss = -12434.565884949525
Iteration 9300: Loss = -12434.565845635456
Iteration 9400: Loss = -12434.566154772656
1
Iteration 9500: Loss = -12434.565842622726
Iteration 9600: Loss = -12434.565733077701
Iteration 9700: Loss = -12434.56582934964
Iteration 9800: Loss = -12434.565663857955
Iteration 9900: Loss = -12434.566750977436
1
Iteration 10000: Loss = -12434.565625772899
Iteration 10100: Loss = -12434.657527567371
1
Iteration 10200: Loss = -12434.56555178597
Iteration 10300: Loss = -12434.584833183526
1
Iteration 10400: Loss = -12434.566828951796
2
Iteration 10500: Loss = -12434.64997310203
3
Iteration 10600: Loss = -12434.565785168212
4
Iteration 10700: Loss = -12434.576999266
5
Iteration 10800: Loss = -12434.584446980076
6
Iteration 10900: Loss = -12434.577361371938
7
Iteration 11000: Loss = -12434.58795726639
8
Iteration 11100: Loss = -12434.57025504711
9
Iteration 11200: Loss = -12434.565657313538
10
Iteration 11300: Loss = -12434.565461123413
Iteration 11400: Loss = -12434.894932569834
1
Iteration 11500: Loss = -12434.56539586765
Iteration 11600: Loss = -12434.594461880213
1
Iteration 11700: Loss = -12434.612938048294
2
Iteration 11800: Loss = -12434.56538431646
Iteration 11900: Loss = -12434.565460970805
Iteration 12000: Loss = -12434.620816432991
1
Iteration 12100: Loss = -12434.565384474668
Iteration 12200: Loss = -12434.573134086128
1
Iteration 12300: Loss = -12434.565338286902
Iteration 12400: Loss = -12434.56633473443
1
Iteration 12500: Loss = -12434.566131948706
2
Iteration 12600: Loss = -12434.565483900566
3
Iteration 12700: Loss = -12434.569534865199
4
Iteration 12800: Loss = -12434.56550871226
5
Iteration 12900: Loss = -12434.566814575783
6
Iteration 13000: Loss = -12434.589764682576
7
Iteration 13100: Loss = -12434.566281884041
8
Iteration 13200: Loss = -12434.567597572968
9
Iteration 13300: Loss = -12434.570409587897
10
Iteration 13400: Loss = -12434.634523756951
11
Iteration 13500: Loss = -12434.574160954455
12
Iteration 13600: Loss = -12434.571085521915
13
Iteration 13700: Loss = -12434.569630155418
14
Iteration 13800: Loss = -12434.568525748567
15
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[0.9670, 0.0330],
        [0.9752, 0.0248]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0088, 0.9912], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2005, 0.2058],
         [0.5946, 0.2059]],

        [[0.7188, 0.2831],
         [0.6357, 0.7183]],

        [[0.6707, 0.1985],
         [0.6170, 0.5721]],

        [[0.7004, 0.2157],
         [0.5259, 0.7000]],

        [[0.6419, 0.1245],
         [0.6658, 0.6477]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -0.0011960189223633472
Average Adjusted Rand Index: -0.0001522251028708703
11927.06910892735
[-0.000344133339997668, -0.0011960189223633472] [-0.0004976719855557115, -0.0001522251028708703] [12433.927840718867, 12434.568525748567]
-------------------------------------
This iteration is 50
True Objective function: Loss = -11932.899084433191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20316.758272562245
Iteration 100: Loss = -12429.580907049047
Iteration 200: Loss = -12429.258016708936
Iteration 300: Loss = -12429.198630221506
Iteration 400: Loss = -12429.169079777059
Iteration 500: Loss = -12429.14527874
Iteration 600: Loss = -12429.116272584897
Iteration 700: Loss = -12429.068359920799
Iteration 800: Loss = -12428.960153152058
Iteration 900: Loss = -12428.617060993944
Iteration 1000: Loss = -12428.195713921597
Iteration 1100: Loss = -12428.040253685629
Iteration 1200: Loss = -12427.945025674695
Iteration 1300: Loss = -12427.864869959485
Iteration 1400: Loss = -12427.799121948101
Iteration 1500: Loss = -12427.74601858323
Iteration 1600: Loss = -12427.702401458186
Iteration 1700: Loss = -12427.665689562195
Iteration 1800: Loss = -12427.633129245647
Iteration 1900: Loss = -12427.603130177227
Iteration 2000: Loss = -12427.572433910591
Iteration 2100: Loss = -12427.512835225758
Iteration 2200: Loss = -12426.715738821253
Iteration 2300: Loss = -12426.521331203114
Iteration 2400: Loss = -12426.44145401137
Iteration 2500: Loss = -12426.39403715467
Iteration 2600: Loss = -12426.363730747005
Iteration 2700: Loss = -12426.20808923291
Iteration 2800: Loss = -12426.170399020322
Iteration 2900: Loss = -12426.138040869437
Iteration 3000: Loss = -12426.105785191
Iteration 3100: Loss = -12426.07207363528
Iteration 3200: Loss = -12426.035948813133
Iteration 3300: Loss = -12425.99529409831
Iteration 3400: Loss = -12425.956246590938
Iteration 3500: Loss = -12425.918529443472
Iteration 3600: Loss = -12425.883045196837
Iteration 3700: Loss = -12425.85059915181
Iteration 3800: Loss = -12425.821616976367
Iteration 3900: Loss = -12425.795967578499
Iteration 4000: Loss = -12425.773547327011
Iteration 4100: Loss = -12425.753980701651
Iteration 4200: Loss = -12425.73699303684
Iteration 4300: Loss = -12425.722110059858
Iteration 4400: Loss = -12425.709176343807
Iteration 4500: Loss = -12425.697745252779
Iteration 4600: Loss = -12425.687671809235
Iteration 4700: Loss = -12425.678830255583
Iteration 4800: Loss = -12425.670970808655
Iteration 4900: Loss = -12425.663887429564
Iteration 5000: Loss = -12425.65760520012
Iteration 5100: Loss = -12425.651990074155
Iteration 5200: Loss = -12425.646882811558
Iteration 5300: Loss = -12425.642306039905
Iteration 5400: Loss = -12425.638123518556
Iteration 5500: Loss = -12425.634366827548
Iteration 5600: Loss = -12425.630925643087
Iteration 5700: Loss = -12425.627819807334
Iteration 5800: Loss = -12425.624932286713
Iteration 5900: Loss = -12425.622271155979
Iteration 6000: Loss = -12425.619914670966
Iteration 6100: Loss = -12425.617651514964
Iteration 6200: Loss = -12425.615589532348
Iteration 6300: Loss = -12425.613746257204
Iteration 6400: Loss = -12425.611985183852
Iteration 6500: Loss = -12425.61038123439
Iteration 6600: Loss = -12425.608844886288
Iteration 6700: Loss = -12425.60748570508
Iteration 6800: Loss = -12425.606212033646
Iteration 6900: Loss = -12425.60499105552
Iteration 7000: Loss = -12425.603896392726
Iteration 7100: Loss = -12425.607848411979
1
Iteration 7200: Loss = -12425.602127659751
Iteration 7300: Loss = -12425.60094752539
Iteration 7400: Loss = -12425.624750673449
1
Iteration 7500: Loss = -12425.599260310428
Iteration 7600: Loss = -12425.982896458463
1
Iteration 7700: Loss = -12425.59787660921
Iteration 7800: Loss = -12425.5972278081
Iteration 7900: Loss = -12425.59873586786
1
Iteration 8000: Loss = -12425.596084087274
Iteration 8100: Loss = -12425.595556829769
Iteration 8200: Loss = -12425.59514138078
Iteration 8300: Loss = -12425.594640318208
Iteration 8400: Loss = -12425.594229863307
Iteration 8500: Loss = -12425.593787471618
Iteration 8600: Loss = -12425.593389200369
Iteration 8700: Loss = -12425.593008692162
Iteration 8800: Loss = -12425.592742082054
Iteration 8900: Loss = -12425.592347926751
Iteration 9000: Loss = -12425.592078915162
Iteration 9100: Loss = -12425.598897067714
1
Iteration 9200: Loss = -12425.59155836747
Iteration 9300: Loss = -12425.591330324994
Iteration 9400: Loss = -12425.697235912932
1
Iteration 9500: Loss = -12425.590862173121
Iteration 9600: Loss = -12425.590636964233
Iteration 9700: Loss = -12425.59042340091
Iteration 9800: Loss = -12425.590697070613
1
Iteration 9900: Loss = -12425.590080674852
Iteration 10000: Loss = -12425.58994480907
Iteration 10100: Loss = -12425.765423326851
1
Iteration 10200: Loss = -12425.589631637338
Iteration 10300: Loss = -12425.589435926797
Iteration 10400: Loss = -12425.588839041691
Iteration 10500: Loss = -12425.586942540102
Iteration 10600: Loss = -12425.586619690012
Iteration 10700: Loss = -12425.586490391115
Iteration 10800: Loss = -12425.590717070718
1
Iteration 10900: Loss = -12425.580543015467
Iteration 11000: Loss = -12425.58041840751
Iteration 11100: Loss = -12425.583124002436
1
Iteration 11200: Loss = -12425.55522167105
Iteration 11300: Loss = -12425.555121183968
Iteration 11400: Loss = -12426.060971336357
1
Iteration 11500: Loss = -12425.554962547361
Iteration 11600: Loss = -12425.554914498007
Iteration 11700: Loss = -12425.554863934545
Iteration 11800: Loss = -12425.556039747662
1
Iteration 11900: Loss = -12425.554762465386
Iteration 12000: Loss = -12425.55472918987
Iteration 12100: Loss = -12425.562961046398
1
Iteration 12200: Loss = -12425.554629540025
Iteration 12300: Loss = -12425.554595961268
Iteration 12400: Loss = -12425.554535237385
Iteration 12500: Loss = -12425.599667244553
1
Iteration 12600: Loss = -12425.55450071673
Iteration 12700: Loss = -12425.554463147626
Iteration 12800: Loss = -12425.554452790177
Iteration 12900: Loss = -12425.556897424092
1
Iteration 13000: Loss = -12425.554403746695
Iteration 13100: Loss = -12425.55435352606
Iteration 13200: Loss = -12425.55436687419
Iteration 13300: Loss = -12425.55588093643
1
Iteration 13400: Loss = -12425.55432920698
Iteration 13500: Loss = -12425.554302909048
Iteration 13600: Loss = -12426.103398957377
1
Iteration 13700: Loss = -12425.554305115278
Iteration 13800: Loss = -12425.55456106729
1
Iteration 13900: Loss = -12425.599398771661
2
Iteration 14000: Loss = -12425.554243824125
Iteration 14100: Loss = -12425.55438354506
1
Iteration 14200: Loss = -12425.554379841084
2
Iteration 14300: Loss = -12425.55584100655
3
Iteration 14400: Loss = -12425.554454499084
4
Iteration 14500: Loss = -12425.554179977067
Iteration 14600: Loss = -12425.56034319746
1
Iteration 14700: Loss = -12425.554201193445
Iteration 14800: Loss = -12425.559889997854
1
Iteration 14900: Loss = -12425.554192662083
Iteration 15000: Loss = -12425.554186289433
Iteration 15100: Loss = -12425.55492188941
1
Iteration 15200: Loss = -12425.553794956142
Iteration 15300: Loss = -12425.553798302604
Iteration 15400: Loss = -12425.554397028502
1
Iteration 15500: Loss = -12425.553809091443
Iteration 15600: Loss = -12425.553805042926
Iteration 15700: Loss = -12425.563941317649
1
Iteration 15800: Loss = -12425.553799834317
Iteration 15900: Loss = -12425.555266801017
1
Iteration 16000: Loss = -12425.553760678966
Iteration 16100: Loss = -12425.5560684505
1
Iteration 16200: Loss = -12425.553753909784
Iteration 16300: Loss = -12425.591909109751
1
Iteration 16400: Loss = -12425.553794151834
Iteration 16500: Loss = -12425.580941622551
1
Iteration 16600: Loss = -12425.553778780613
Iteration 16700: Loss = -12425.5802616848
1
Iteration 16800: Loss = -12425.553802988483
Iteration 16900: Loss = -12425.813652948502
1
Iteration 17000: Loss = -12425.553804285582
Iteration 17100: Loss = -12425.646026154602
1
Iteration 17200: Loss = -12425.553841940025
Iteration 17300: Loss = -12425.55379901859
Iteration 17400: Loss = -12425.5545682927
1
Iteration 17500: Loss = -12425.553780919929
Iteration 17600: Loss = -12425.55411158563
1
Iteration 17700: Loss = -12425.55378768214
Iteration 17800: Loss = -12425.555838587965
1
Iteration 17900: Loss = -12425.553770259732
Iteration 18000: Loss = -12425.559440069004
1
Iteration 18100: Loss = -12425.553754776078
Iteration 18200: Loss = -12425.554673350069
1
Iteration 18300: Loss = -12425.553832878964
Iteration 18400: Loss = -12425.554840797395
1
Iteration 18500: Loss = -12425.55469038219
2
Iteration 18600: Loss = -12425.553770657434
Iteration 18700: Loss = -12425.554115962226
1
Iteration 18800: Loss = -12425.553792729277
Iteration 18900: Loss = -12425.942681781706
1
Iteration 19000: Loss = -12425.553756311563
Iteration 19100: Loss = -12425.553764096996
Iteration 19200: Loss = -12425.580667206741
1
Iteration 19300: Loss = -12425.553754897259
Iteration 19400: Loss = -12425.555103844705
1
Iteration 19500: Loss = -12425.561689914459
2
Iteration 19600: Loss = -12425.553749577994
Iteration 19700: Loss = -12425.645742638351
1
Iteration 19800: Loss = -12425.55377179642
Iteration 19900: Loss = -12425.565142760306
1
pi: tensor([[1.0000e+00, 1.7622e-07],
        [7.0177e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0171, 0.9829], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[4.2723e-04, 1.9307e-01],
         [6.8363e-01, 2.0173e-01]],

        [[6.5898e-01, 1.5872e-01],
         [5.2546e-01, 5.1617e-01]],

        [[5.8473e-01, 9.9794e-02],
         [5.5700e-01, 5.6861e-01]],

        [[5.3497e-01, 2.8228e-01],
         [6.9494e-01, 5.3076e-01]],

        [[5.3730e-01, 2.1380e-01],
         [5.8705e-01, 6.2570e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
Global Adjusted Rand Index: -0.000798483401738101
Average Adjusted Rand Index: 0.001919281554566649
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20747.56251656855
Iteration 100: Loss = -12429.524863866767
Iteration 200: Loss = -12429.171789088281
Iteration 300: Loss = -12429.112639250978
Iteration 400: Loss = -12429.061764839113
Iteration 500: Loss = -12428.989260794722
Iteration 600: Loss = -12428.869159153113
Iteration 700: Loss = -12428.632667983295
Iteration 800: Loss = -12427.174823738218
Iteration 900: Loss = -12425.907793710054
Iteration 1000: Loss = -12425.035662996746
Iteration 1100: Loss = -12421.398414173462
Iteration 1200: Loss = -12332.269322554323
Iteration 1300: Loss = -12291.427259653363
Iteration 1400: Loss = -12174.994184865942
Iteration 1500: Loss = -12126.381234960134
Iteration 1600: Loss = -12090.037772395488
Iteration 1700: Loss = -12081.665409959036
Iteration 1800: Loss = -12081.10915942185
Iteration 1900: Loss = -12080.766010329387
Iteration 2000: Loss = -12080.611308825724
Iteration 2100: Loss = -12080.485286111525
Iteration 2200: Loss = -12079.28858529499
Iteration 2300: Loss = -12073.07314695945
Iteration 2400: Loss = -12071.069888478927
Iteration 2500: Loss = -12065.707069907363
Iteration 2600: Loss = -12063.226607687151
Iteration 2700: Loss = -12057.346719781697
Iteration 2800: Loss = -12049.418300713465
Iteration 2900: Loss = -12035.8261118028
Iteration 3000: Loss = -12030.320729909772
Iteration 3100: Loss = -12018.147758337565
Iteration 3200: Loss = -12009.029200748213
Iteration 3300: Loss = -12009.020652247858
Iteration 3400: Loss = -12008.895119562618
Iteration 3500: Loss = -11977.549142994
Iteration 3600: Loss = -11942.347073240218
Iteration 3700: Loss = -11942.323785246255
Iteration 3800: Loss = -11942.315782256353
Iteration 3900: Loss = -11942.297793477594
Iteration 4000: Loss = -11931.798770220363
Iteration 4100: Loss = -11931.79581392849
Iteration 4200: Loss = -11931.794818743218
Iteration 4300: Loss = -11931.796343386417
1
Iteration 4400: Loss = -11931.789742318482
Iteration 4500: Loss = -11931.780586174953
Iteration 4600: Loss = -11931.778874656608
Iteration 4700: Loss = -11931.7809647792
1
Iteration 4800: Loss = -11931.777767091517
Iteration 4900: Loss = -11931.781105624974
1
Iteration 5000: Loss = -11931.77704323242
Iteration 5100: Loss = -11931.778809885594
1
Iteration 5200: Loss = -11931.775337853906
Iteration 5300: Loss = -11931.774757100096
Iteration 5400: Loss = -11931.774528921285
Iteration 5500: Loss = -11931.773936611662
Iteration 5600: Loss = -11931.774277242084
1
Iteration 5700: Loss = -11931.773399753221
Iteration 5800: Loss = -11931.773147014184
Iteration 5900: Loss = -11931.772968808324
Iteration 6000: Loss = -11931.777509843727
1
Iteration 6100: Loss = -11931.78018311226
2
Iteration 6200: Loss = -11931.773131242408
3
Iteration 6300: Loss = -11931.776069895595
4
Iteration 6400: Loss = -11931.777609467832
5
Iteration 6500: Loss = -11931.776628105556
6
Iteration 6600: Loss = -11931.771975832447
Iteration 6700: Loss = -11931.771584077977
Iteration 6800: Loss = -11931.781384166912
1
Iteration 6900: Loss = -11931.771343532671
Iteration 7000: Loss = -11931.826342088676
1
Iteration 7100: Loss = -11931.77104742872
Iteration 7200: Loss = -11931.778233426898
1
Iteration 7300: Loss = -11931.770837183263
Iteration 7400: Loss = -11931.775763714319
1
Iteration 7500: Loss = -11931.770660041506
Iteration 7600: Loss = -11931.771511955734
1
Iteration 7700: Loss = -11931.783837019255
2
Iteration 7800: Loss = -11931.779825177562
3
Iteration 7900: Loss = -11931.770310034892
Iteration 8000: Loss = -11931.770571325484
1
Iteration 8100: Loss = -11930.359314856896
Iteration 8200: Loss = -11930.352389728712
Iteration 8300: Loss = -11930.160703807362
Iteration 8400: Loss = -11930.155299800164
Iteration 8500: Loss = -11930.155242482322
Iteration 8600: Loss = -11930.16100065589
1
Iteration 8700: Loss = -11930.154558075721
Iteration 8800: Loss = -11930.17144149412
1
Iteration 8900: Loss = -11930.154440610071
Iteration 9000: Loss = -11930.169945960592
1
Iteration 9100: Loss = -11930.154523616737
Iteration 9200: Loss = -11930.160453646142
1
Iteration 9300: Loss = -11930.157302496013
2
Iteration 9400: Loss = -11930.157349176221
3
Iteration 9500: Loss = -11930.16387433778
4
Iteration 9600: Loss = -11930.155883537851
5
Iteration 9700: Loss = -11930.201815296652
6
Iteration 9800: Loss = -11930.155191950249
7
Iteration 9900: Loss = -11930.15514048584
8
Iteration 10000: Loss = -11930.169992065817
9
Iteration 10100: Loss = -11930.16016052795
10
Iteration 10200: Loss = -11930.158035932893
11
Iteration 10300: Loss = -11930.15594413542
12
Iteration 10400: Loss = -11930.155234007087
13
Iteration 10500: Loss = -11930.155124168487
14
Iteration 10600: Loss = -11930.170452720748
15
Stopping early at iteration 10600 due to no improvement.
pi: tensor([[0.7864, 0.2136],
        [0.3037, 0.6963]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4386, 0.5614], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3045, 0.1015],
         [0.6915, 0.2943]],

        [[0.5782, 0.0972],
         [0.6986, 0.6034]],

        [[0.5656, 0.1036],
         [0.5177, 0.6023]],

        [[0.6198, 0.1008],
         [0.5365, 0.6169]],

        [[0.5259, 0.1030],
         [0.5895, 0.6926]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.992
11932.899084433191
[-0.000798483401738101, 0.9919999730634713] [0.001919281554566649, 0.992] [12425.553761238947, 11930.170452720748]
-------------------------------------
This iteration is 51
True Objective function: Loss = -11840.565264248846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20252.17049884618
Iteration 100: Loss = -12347.60333077002
Iteration 200: Loss = -12346.622601262816
Iteration 300: Loss = -12346.358571203433
Iteration 400: Loss = -12346.235036454415
Iteration 500: Loss = -12346.158114300955
Iteration 600: Loss = -12346.10214763504
Iteration 700: Loss = -12346.057902175713
Iteration 800: Loss = -12346.022442867752
Iteration 900: Loss = -12345.995420790774
Iteration 1000: Loss = -12345.975662237584
Iteration 1100: Loss = -12345.960433594719
Iteration 1200: Loss = -12345.947528342667
Iteration 1300: Loss = -12345.935749946846
Iteration 1400: Loss = -12345.924569848074
Iteration 1500: Loss = -12345.913881875449
Iteration 1600: Loss = -12345.903419333232
Iteration 1700: Loss = -12345.893124208333
Iteration 1800: Loss = -12345.882989694443
Iteration 1900: Loss = -12345.872837316074
Iteration 2000: Loss = -12345.862738979713
Iteration 2100: Loss = -12345.852706062664
Iteration 2200: Loss = -12345.842801401603
Iteration 2300: Loss = -12345.833120005023
Iteration 2400: Loss = -12345.823675610927
Iteration 2500: Loss = -12345.81449819882
Iteration 2600: Loss = -12345.812446481643
Iteration 2700: Loss = -12345.7971336548
Iteration 2800: Loss = -12345.7886716852
Iteration 2900: Loss = -12345.780505875364
Iteration 3000: Loss = -12345.77215027505
Iteration 3100: Loss = -12345.76404978045
Iteration 3200: Loss = -12345.755708322627
Iteration 3300: Loss = -12345.74734357749
Iteration 3400: Loss = -12345.738551652898
Iteration 3500: Loss = -12345.72945612776
Iteration 3600: Loss = -12345.719352708358
Iteration 3700: Loss = -12345.70853683229
Iteration 3800: Loss = -12345.695927760637
Iteration 3900: Loss = -12345.68172938171
Iteration 4000: Loss = -12345.66513757922
Iteration 4100: Loss = -12345.648022691645
Iteration 4200: Loss = -12345.623404601762
Iteration 4300: Loss = -12345.598423558522
Iteration 4400: Loss = -12345.572161459993
Iteration 4500: Loss = -12345.546328494662
Iteration 4600: Loss = -12345.541813124652
Iteration 4700: Loss = -12345.503512372818
Iteration 4800: Loss = -12345.495501622163
Iteration 4900: Loss = -12345.472416251445
Iteration 5000: Loss = -12345.458837183476
Iteration 5100: Loss = -12345.444066776685
Iteration 5200: Loss = -12345.425810725841
Iteration 5300: Loss = -12345.402424752981
Iteration 5400: Loss = -12345.383882764188
Iteration 5500: Loss = -12345.354698539102
Iteration 5600: Loss = -12345.340163695288
Iteration 5700: Loss = -12345.330328581831
Iteration 5800: Loss = -12345.323107123433
Iteration 5900: Loss = -12345.350256202879
1
Iteration 6000: Loss = -12345.313388683277
Iteration 6100: Loss = -12345.309993371586
Iteration 6200: Loss = -12345.307409277377
Iteration 6300: Loss = -12345.30489083323
Iteration 6400: Loss = -12345.317942769178
1
Iteration 6500: Loss = -12345.301389891025
Iteration 6600: Loss = -12345.328057920455
1
Iteration 6700: Loss = -12345.298813574807
Iteration 6800: Loss = -12345.298859743547
Iteration 6900: Loss = -12345.296881828555
Iteration 7000: Loss = -12345.298832587507
1
Iteration 7100: Loss = -12345.295348672256
Iteration 7200: Loss = -12345.541125878073
1
Iteration 7300: Loss = -12345.294238061866
Iteration 7400: Loss = -12345.295895635447
1
Iteration 7500: Loss = -12345.293355968213
Iteration 7600: Loss = -12345.30007011304
1
Iteration 7700: Loss = -12345.294558134237
2
Iteration 7800: Loss = -12345.329603711307
3
Iteration 7900: Loss = -12345.306423209884
4
Iteration 8000: Loss = -12345.293528107068
5
Iteration 8100: Loss = -12345.291781833826
Iteration 8200: Loss = -12345.291632430044
Iteration 8300: Loss = -12345.291253816338
Iteration 8400: Loss = -12345.290821441962
Iteration 8500: Loss = -12345.291617857305
1
Iteration 8600: Loss = -12345.291540275663
2
Iteration 8700: Loss = -12345.328121717614
3
Iteration 8800: Loss = -12345.29413231436
4
Iteration 8900: Loss = -12345.296639706685
5
Iteration 9000: Loss = -12345.36795095173
6
Iteration 9100: Loss = -12345.294398129688
7
Iteration 9200: Loss = -12345.290145998222
Iteration 9300: Loss = -12345.291640861931
1
Iteration 9400: Loss = -12345.289876196039
Iteration 9500: Loss = -12345.290310969078
1
Iteration 9600: Loss = -12345.45198869787
2
Iteration 9700: Loss = -12345.290951498482
3
Iteration 9800: Loss = -12345.292714908963
4
Iteration 9900: Loss = -12345.289201649906
Iteration 10000: Loss = -12345.293458274924
1
Iteration 10100: Loss = -12345.337778029254
2
Iteration 10200: Loss = -12345.348500191174
3
Iteration 10300: Loss = -12345.294006308457
4
Iteration 10400: Loss = -12345.401225282034
5
Iteration 10500: Loss = -12345.288962081739
Iteration 10600: Loss = -12345.300569216168
1
Iteration 10700: Loss = -12345.288669068186
Iteration 10800: Loss = -12345.29214985491
1
Iteration 10900: Loss = -12345.29035285216
2
Iteration 11000: Loss = -12345.289226515133
3
Iteration 11100: Loss = -12345.292571379687
4
Iteration 11200: Loss = -12345.28966258593
5
Iteration 11300: Loss = -12345.28868085589
Iteration 11400: Loss = -12345.288968282717
1
Iteration 11500: Loss = -12345.297971812826
2
Iteration 11600: Loss = -12345.288759217405
Iteration 11700: Loss = -12345.31538872084
1
Iteration 11800: Loss = -12345.298940046101
2
Iteration 11900: Loss = -12345.290611177772
3
Iteration 12000: Loss = -12345.292366884949
4
Iteration 12100: Loss = -12345.2970186694
5
Iteration 12200: Loss = -12345.289268182536
6
Iteration 12300: Loss = -12345.295076601922
7
Iteration 12400: Loss = -12345.288290859002
Iteration 12500: Loss = -12345.290030351587
1
Iteration 12600: Loss = -12345.291337316521
2
Iteration 12700: Loss = -12345.288666485434
3
Iteration 12800: Loss = -12345.288313018184
Iteration 12900: Loss = -12345.290805126433
1
Iteration 13000: Loss = -12345.327786873004
2
Iteration 13100: Loss = -12345.288214337108
Iteration 13200: Loss = -12345.288479835317
1
Iteration 13300: Loss = -12345.508856709726
2
Iteration 13400: Loss = -12345.289385564252
3
Iteration 13500: Loss = -12345.348036405958
4
Iteration 13600: Loss = -12345.288576085091
5
Iteration 13700: Loss = -12345.293008096867
6
Iteration 13800: Loss = -12345.334453113353
7
Iteration 13900: Loss = -12345.301391712846
8
Iteration 14000: Loss = -12345.291408266023
9
Iteration 14100: Loss = -12345.288224880866
Iteration 14200: Loss = -12345.288159169866
Iteration 14300: Loss = -12345.288140077026
Iteration 14400: Loss = -12345.290102205718
1
Iteration 14500: Loss = -12345.291623011452
2
Iteration 14600: Loss = -12345.373784890817
3
Iteration 14700: Loss = -12345.288697598038
4
Iteration 14800: Loss = -12345.289015236
5
Iteration 14900: Loss = -12345.424520368644
6
Iteration 15000: Loss = -12345.354487983426
7
Iteration 15100: Loss = -12345.305170479678
8
Iteration 15200: Loss = -12345.28809479663
Iteration 15300: Loss = -12345.288076487708
Iteration 15400: Loss = -12345.289674819101
1
Iteration 15500: Loss = -12345.317894235133
2
Iteration 15600: Loss = -12345.29768410758
3
Iteration 15700: Loss = -12345.28813909089
Iteration 15800: Loss = -12345.288339483875
1
Iteration 15900: Loss = -12345.288212394278
Iteration 16000: Loss = -12345.28818970054
Iteration 16100: Loss = -12345.318424930769
1
Iteration 16200: Loss = -12345.288082413874
Iteration 16300: Loss = -12345.289472909888
1
Iteration 16400: Loss = -12345.288585744072
2
Iteration 16500: Loss = -12345.288374451771
3
Iteration 16600: Loss = -12345.28801622879
Iteration 16700: Loss = -12345.28991138619
1
Iteration 16800: Loss = -12345.294877199754
2
Iteration 16900: Loss = -12345.288065407027
Iteration 17000: Loss = -12345.288542822933
1
Iteration 17100: Loss = -12345.288422603233
2
Iteration 17200: Loss = -12345.288773434138
3
Iteration 17300: Loss = -12345.288194020728
4
Iteration 17400: Loss = -12345.292770000497
5
Iteration 17500: Loss = -12345.293737635035
6
Iteration 17600: Loss = -12345.293996204528
7
Iteration 17700: Loss = -12345.291348110415
8
Iteration 17800: Loss = -12345.288065239105
Iteration 17900: Loss = -12345.297617561457
1
Iteration 18000: Loss = -12345.323827284687
2
Iteration 18100: Loss = -12345.288776017545
3
Iteration 18200: Loss = -12345.288036535158
Iteration 18300: Loss = -12345.299906744149
1
Iteration 18400: Loss = -12345.289504355767
2
Iteration 18500: Loss = -12345.409396214089
3
Iteration 18600: Loss = -12345.34228637958
4
Iteration 18700: Loss = -12345.350323070452
5
Iteration 18800: Loss = -12345.288671653809
6
Iteration 18900: Loss = -12345.354372275424
7
Iteration 19000: Loss = -12345.290801529442
8
Iteration 19100: Loss = -12345.532592468506
9
Iteration 19200: Loss = -12345.33300917433
10
Iteration 19300: Loss = -12345.573305195549
11
Iteration 19400: Loss = -12345.288642371823
12
Iteration 19500: Loss = -12345.289234564316
13
Iteration 19600: Loss = -12345.288057309526
Iteration 19700: Loss = -12345.357813097338
1
Iteration 19800: Loss = -12345.353081592284
2
Iteration 19900: Loss = -12345.288960056885
3
pi: tensor([[7.0859e-01, 2.9141e-01],
        [1.4469e-05, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9943, 0.0057], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.2137],
         [0.5741, 0.1948]],

        [[0.5920, 0.1983],
         [0.5005, 0.6965]],

        [[0.5444, 0.2023],
         [0.5251, 0.5713]],

        [[0.6003, 0.1987],
         [0.6254, 0.6407]],

        [[0.5705, 0.1926],
         [0.6509, 0.6274]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: -8.228408875514459e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0018628352600175316
Average Adjusted Rand Index: -1.645681775102892e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21978.391286386413
Iteration 100: Loss = -12347.259931800701
Iteration 200: Loss = -12346.504389941774
Iteration 300: Loss = -12346.3057556916
Iteration 400: Loss = -12346.21314163355
Iteration 500: Loss = -12346.154671163782
Iteration 600: Loss = -12346.112615089856
Iteration 700: Loss = -12346.080111502777
Iteration 800: Loss = -12346.053769239506
Iteration 900: Loss = -12346.031667819534
Iteration 1000: Loss = -12346.01275165744
Iteration 1100: Loss = -12345.996657921221
Iteration 1200: Loss = -12345.98312486683
Iteration 1300: Loss = -12345.971879642697
Iteration 1400: Loss = -12345.962579085908
Iteration 1500: Loss = -12345.95429110334
Iteration 1600: Loss = -12345.946896099884
Iteration 1700: Loss = -12345.939759980018
Iteration 1800: Loss = -12345.932685355285
Iteration 1900: Loss = -12345.925506261654
Iteration 2000: Loss = -12345.918155445128
Iteration 2100: Loss = -12345.910521781645
Iteration 2200: Loss = -12345.902443419318
Iteration 2300: Loss = -12345.893973391378
Iteration 2400: Loss = -12345.884939835416
Iteration 2500: Loss = -12345.875454196463
Iteration 2600: Loss = -12345.865380074087
Iteration 2700: Loss = -12345.854818825941
Iteration 2800: Loss = -12345.843829267267
Iteration 2900: Loss = -12345.832618888486
Iteration 3000: Loss = -12345.821322835256
Iteration 3100: Loss = -12345.810219343599
Iteration 3200: Loss = -12345.799057845581
Iteration 3300: Loss = -12345.78931712444
Iteration 3400: Loss = -12345.77734520252
Iteration 3500: Loss = -12345.76826005132
Iteration 3600: Loss = -12345.755798358849
Iteration 3700: Loss = -12345.745013299005
Iteration 3800: Loss = -12345.733500029897
Iteration 3900: Loss = -12345.721696853074
Iteration 4000: Loss = -12345.708654804053
Iteration 4100: Loss = -12345.694568549336
Iteration 4200: Loss = -12345.67858546732
Iteration 4300: Loss = -12345.660669519499
Iteration 4400: Loss = -12345.640329120328
Iteration 4500: Loss = -12345.618013622632
Iteration 4600: Loss = -12345.594367437336
Iteration 4700: Loss = -12345.57723705171
Iteration 4800: Loss = -12345.548945624283
Iteration 4900: Loss = -12345.533353708626
Iteration 5000: Loss = -12345.513386325441
Iteration 5100: Loss = -12345.501918657657
Iteration 5200: Loss = -12345.489405534934
Iteration 5300: Loss = -12345.481001061618
Iteration 5400: Loss = -12345.473948514487
Iteration 5500: Loss = -12345.469453341366
Iteration 5600: Loss = -12345.463920953911
Iteration 5700: Loss = -12345.461644204324
Iteration 5800: Loss = -12345.45716214725
Iteration 5900: Loss = -12345.45518062469
Iteration 6000: Loss = -12345.45237700874
Iteration 6100: Loss = -12345.450261127859
Iteration 6200: Loss = -12345.448573752465
Iteration 6300: Loss = -12345.446487639054
Iteration 6400: Loss = -12345.444729830588
Iteration 6500: Loss = -12345.441872325742
Iteration 6600: Loss = -12345.438545981942
Iteration 6700: Loss = -12345.431736643555
Iteration 6800: Loss = -12345.420098371924
Iteration 6900: Loss = -12345.44694996267
1
Iteration 7000: Loss = -12345.362641142054
Iteration 7100: Loss = -12345.338389136297
Iteration 7200: Loss = -12345.348208026404
1
Iteration 7300: Loss = -12345.316926039224
Iteration 7400: Loss = -12345.311134469035
Iteration 7500: Loss = -12345.30797756125
Iteration 7600: Loss = -12345.303986600387
Iteration 7700: Loss = -12345.310954050445
1
Iteration 7800: Loss = -12345.300192237111
Iteration 7900: Loss = -12345.309995500578
1
Iteration 8000: Loss = -12345.297005841383
Iteration 8100: Loss = -12345.296291920477
Iteration 8200: Loss = -12345.295963342453
Iteration 8300: Loss = -12345.29662398679
1
Iteration 8400: Loss = -12345.296199590779
2
Iteration 8500: Loss = -12345.295606571199
Iteration 8600: Loss = -12345.31669711464
1
Iteration 8700: Loss = -12345.292561073566
Iteration 8800: Loss = -12345.298375381244
1
Iteration 8900: Loss = -12345.3060623687
2
Iteration 9000: Loss = -12345.291494677358
Iteration 9100: Loss = -12345.292675956049
1
Iteration 9200: Loss = -12345.503794175815
2
Iteration 9300: Loss = -12345.291118897381
Iteration 9400: Loss = -12345.290570307736
Iteration 9500: Loss = -12345.295699899549
1
Iteration 9600: Loss = -12345.293620319555
2
Iteration 9700: Loss = -12345.311357883409
3
Iteration 9800: Loss = -12345.298491697247
4
Iteration 9900: Loss = -12345.289988821465
Iteration 10000: Loss = -12345.32279733306
1
Iteration 10100: Loss = -12345.53495821292
2
Iteration 10200: Loss = -12345.291111225904
3
Iteration 10300: Loss = -12345.289616962107
Iteration 10400: Loss = -12345.291856231182
1
Iteration 10500: Loss = -12345.290319889302
2
Iteration 10600: Loss = -12345.658455461404
3
Iteration 10700: Loss = -12345.289867594664
4
Iteration 10800: Loss = -12345.28951101801
Iteration 10900: Loss = -12345.290351878848
1
Iteration 11000: Loss = -12345.289350825204
Iteration 11100: Loss = -12345.288987053673
Iteration 11200: Loss = -12345.28959075652
1
Iteration 11300: Loss = -12345.289330090332
2
Iteration 11400: Loss = -12345.288683490158
Iteration 11500: Loss = -12345.363797390188
1
Iteration 11600: Loss = -12345.30230499425
2
Iteration 11700: Loss = -12345.29319077451
3
Iteration 11800: Loss = -12345.298436305482
4
Iteration 11900: Loss = -12345.288965799795
5
Iteration 12000: Loss = -12345.289071998981
6
Iteration 12100: Loss = -12345.291779181109
7
Iteration 12200: Loss = -12345.289810142636
8
Iteration 12300: Loss = -12345.440613259452
9
Iteration 12400: Loss = -12345.288422119203
Iteration 12500: Loss = -12345.288498032236
Iteration 12600: Loss = -12345.303612514397
1
Iteration 12700: Loss = -12345.313145435392
2
Iteration 12800: Loss = -12345.290118147743
3
Iteration 12900: Loss = -12345.294574745356
4
Iteration 13000: Loss = -12345.292093290298
5
Iteration 13100: Loss = -12345.366531851412
6
Iteration 13200: Loss = -12345.297449827724
7
Iteration 13300: Loss = -12345.289929269164
8
Iteration 13400: Loss = -12345.302646166512
9
Iteration 13500: Loss = -12345.3138595458
10
Iteration 13600: Loss = -12345.288223367648
Iteration 13700: Loss = -12345.293990319658
1
Iteration 13800: Loss = -12345.288212243266
Iteration 13900: Loss = -12345.288256551654
Iteration 14000: Loss = -12345.337535079769
1
Iteration 14100: Loss = -12345.289417704402
2
Iteration 14200: Loss = -12345.33991608408
3
Iteration 14300: Loss = -12345.2904493744
4
Iteration 14400: Loss = -12345.288970836467
5
Iteration 14500: Loss = -12345.29138678622
6
Iteration 14600: Loss = -12345.290403826162
7
Iteration 14700: Loss = -12345.288353885962
Iteration 14800: Loss = -12345.289068872282
1
Iteration 14900: Loss = -12345.313799285532
2
Iteration 15000: Loss = -12345.291713332068
3
Iteration 15100: Loss = -12345.463364120873
4
Iteration 15200: Loss = -12345.30002753925
5
Iteration 15300: Loss = -12345.296688455317
6
Iteration 15400: Loss = -12345.332409070628
7
Iteration 15500: Loss = -12345.296091876247
8
Iteration 15600: Loss = -12345.288064626018
Iteration 15700: Loss = -12345.567141641093
1
Iteration 15800: Loss = -12345.28977046352
2
Iteration 15900: Loss = -12345.299175543847
3
Iteration 16000: Loss = -12345.29140722329
4
Iteration 16100: Loss = -12345.290523081356
5
Iteration 16200: Loss = -12345.289446520143
6
Iteration 16300: Loss = -12345.290027113457
7
Iteration 16400: Loss = -12345.289496234831
8
Iteration 16500: Loss = -12345.288146466537
Iteration 16600: Loss = -12345.288323609073
1
Iteration 16700: Loss = -12345.568785130092
2
Iteration 16800: Loss = -12345.338658961291
3
Iteration 16900: Loss = -12345.293420970624
4
Iteration 17000: Loss = -12345.288355124208
5
Iteration 17100: Loss = -12345.290116946555
6
Iteration 17200: Loss = -12345.288080674774
Iteration 17300: Loss = -12345.28814044493
Iteration 17400: Loss = -12345.406138864699
1
Iteration 17500: Loss = -12345.360249063064
2
Iteration 17600: Loss = -12345.288057912336
Iteration 17700: Loss = -12345.288566612324
1
Iteration 17800: Loss = -12345.438072526735
2
Iteration 17900: Loss = -12345.293945933197
3
Iteration 18000: Loss = -12345.31054632724
4
Iteration 18100: Loss = -12345.288358800964
5
Iteration 18200: Loss = -12345.288911888738
6
Iteration 18300: Loss = -12345.288320678435
7
Iteration 18400: Loss = -12345.297342633658
8
Iteration 18500: Loss = -12345.289094578095
9
Iteration 18600: Loss = -12345.290942227235
10
Iteration 18700: Loss = -12345.288208840397
11
Iteration 18800: Loss = -12345.289198217799
12
Iteration 18900: Loss = -12345.291802833353
13
Iteration 19000: Loss = -12345.296899908515
14
Iteration 19100: Loss = -12345.288346177527
15
Stopping early at iteration 19100 due to no improvement.
pi: tensor([[7.0946e-01, 2.9054e-01],
        [2.9780e-05, 9.9997e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9943, 0.0057], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.2138],
         [0.5678, 0.1947]],

        [[0.7151, 0.1983],
         [0.6680, 0.7075]],

        [[0.5201, 0.2020],
         [0.5582, 0.6252]],

        [[0.5648, 0.1987],
         [0.7256, 0.6861]],

        [[0.6648, 0.1926],
         [0.6683, 0.6231]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.006500137378581577
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001990783124446519
Average Adjusted Rand Index: -0.0013000274757163153
11840.565264248846
[-0.0018628352600175316, -0.001990783124446519] [-1.645681775102892e-05, -0.0013000274757163153] [12345.288708828433, 12345.288346177527]
-------------------------------------
This iteration is 52
True Objective function: Loss = -11927.240027888682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19934.3033167689
Iteration 100: Loss = -12473.385484666433
Iteration 200: Loss = -12473.043396837018
Iteration 300: Loss = -12472.808803514909
Iteration 400: Loss = -12472.27331648916
Iteration 500: Loss = -12471.749498267089
Iteration 600: Loss = -12471.432438550197
Iteration 700: Loss = -12471.211834890852
Iteration 800: Loss = -12470.934713376211
Iteration 900: Loss = -12470.767145348545
Iteration 1000: Loss = -12470.689684995008
Iteration 1100: Loss = -12470.648395321572
Iteration 1200: Loss = -12470.621628272307
Iteration 1300: Loss = -12470.602017565143
Iteration 1400: Loss = -12470.586153355342
Iteration 1500: Loss = -12470.571479066943
Iteration 1600: Loss = -12470.556561730971
Iteration 1700: Loss = -12470.53802205229
Iteration 1800: Loss = -12470.505085842566
Iteration 1900: Loss = -12470.429191088951
Iteration 2000: Loss = -12470.267748193613
Iteration 2100: Loss = -12470.06563609907
Iteration 2200: Loss = -12469.915713805987
Iteration 2300: Loss = -12469.814591337163
Iteration 2400: Loss = -12469.738085035127
Iteration 2500: Loss = -12469.677845708027
Iteration 2600: Loss = -12469.629095197562
Iteration 2700: Loss = -12469.58644579685
Iteration 2800: Loss = -12396.435934492773
Iteration 2900: Loss = -11931.568188219413
Iteration 3000: Loss = -11931.242388481673
Iteration 3100: Loss = -11930.729583328211
Iteration 3200: Loss = -11930.695026364589
Iteration 3300: Loss = -11930.639555231082
Iteration 3400: Loss = -11930.625458193139
Iteration 3500: Loss = -11930.61790928439
Iteration 3600: Loss = -11930.612684719199
Iteration 3700: Loss = -11930.608511693106
Iteration 3800: Loss = -11930.604982602144
Iteration 3900: Loss = -11930.601863581589
Iteration 4000: Loss = -11930.600387538816
Iteration 4100: Loss = -11930.597697986166
Iteration 4200: Loss = -11930.594530909095
Iteration 4300: Loss = -11930.592935059856
Iteration 4400: Loss = -11930.591139301947
Iteration 4500: Loss = -11930.595716062337
1
Iteration 4600: Loss = -11930.352680399912
Iteration 4700: Loss = -11930.347544028393
Iteration 4800: Loss = -11930.346904708087
Iteration 4900: Loss = -11930.347559274052
1
Iteration 5000: Loss = -11930.345226967378
Iteration 5100: Loss = -11930.345023577418
Iteration 5200: Loss = -11930.34395367075
Iteration 5300: Loss = -11930.343242038358
Iteration 5400: Loss = -11930.345406725179
1
Iteration 5500: Loss = -11930.34670833173
2
Iteration 5600: Loss = -11930.339731440228
Iteration 5700: Loss = -11930.339839967146
1
Iteration 5800: Loss = -11923.933596132181
Iteration 5900: Loss = -11923.92842222348
Iteration 6000: Loss = -11923.927496857072
Iteration 6100: Loss = -11923.927124297315
Iteration 6200: Loss = -11923.928981819003
1
Iteration 6300: Loss = -11923.926808599725
Iteration 6400: Loss = -11923.925593869906
Iteration 6500: Loss = -11923.924706916287
Iteration 6600: Loss = -11923.931150093158
1
Iteration 6700: Loss = -11923.927633977446
2
Iteration 6800: Loss = -11923.941127093362
3
Iteration 6900: Loss = -11923.954966161631
4
Iteration 7000: Loss = -11923.929334449022
5
Iteration 7100: Loss = -11923.923900665022
Iteration 7200: Loss = -11923.922514488308
Iteration 7300: Loss = -11919.428596848491
Iteration 7400: Loss = -11919.407853115685
Iteration 7500: Loss = -11919.407911294062
Iteration 7600: Loss = -11919.409036873085
1
Iteration 7700: Loss = -11919.40821045127
2
Iteration 7800: Loss = -11919.404879795735
Iteration 7900: Loss = -11919.41355550604
1
Iteration 8000: Loss = -11919.401033317863
Iteration 8100: Loss = -11919.400905200091
Iteration 8200: Loss = -11919.412104474459
1
Iteration 8300: Loss = -11919.480242794078
2
Iteration 8400: Loss = -11919.394465706444
Iteration 8500: Loss = -11919.394386950693
Iteration 8600: Loss = -11919.44377342568
1
Iteration 8700: Loss = -11919.394185248531
Iteration 8800: Loss = -11919.412926758092
1
Iteration 8900: Loss = -11919.394081313554
Iteration 9000: Loss = -11919.393973731172
Iteration 9100: Loss = -11919.394193874614
1
Iteration 9200: Loss = -11919.394602461647
2
Iteration 9300: Loss = -11919.399565031952
3
Iteration 9400: Loss = -11919.393888681567
Iteration 9500: Loss = -11919.393988596248
Iteration 9600: Loss = -11919.395342089761
1
Iteration 9700: Loss = -11919.420100895195
2
Iteration 9800: Loss = -11919.396867546111
3
Iteration 9900: Loss = -11919.392736304091
Iteration 10000: Loss = -11919.391629740034
Iteration 10100: Loss = -11919.396046870224
1
Iteration 10200: Loss = -11919.413982963017
2
Iteration 10300: Loss = -11919.405062506998
3
Iteration 10400: Loss = -11919.390402566378
Iteration 10500: Loss = -11919.412657036739
1
Iteration 10600: Loss = -11919.389983200324
Iteration 10700: Loss = -11919.390367347107
1
Iteration 10800: Loss = -11919.391395407556
2
Iteration 10900: Loss = -11919.392976024168
3
Iteration 11000: Loss = -11919.398772773928
4
Iteration 11100: Loss = -11919.42305335639
5
Iteration 11200: Loss = -11919.38937858445
Iteration 11300: Loss = -11919.390062460352
1
Iteration 11400: Loss = -11919.394170251537
2
Iteration 11500: Loss = -11919.394994059008
3
Iteration 11600: Loss = -11919.389277663791
Iteration 11700: Loss = -11919.390480925467
1
Iteration 11800: Loss = -11919.433948624828
2
Iteration 11900: Loss = -11919.394700546514
3
Iteration 12000: Loss = -11919.398543882135
4
Iteration 12100: Loss = -11919.389262594443
Iteration 12200: Loss = -11919.390951494559
1
Iteration 12300: Loss = -11919.4073092609
2
Iteration 12400: Loss = -11919.400826302483
3
Iteration 12500: Loss = -11919.393029724544
4
Iteration 12600: Loss = -11919.401974425073
5
Iteration 12700: Loss = -11919.42356332084
6
Iteration 12800: Loss = -11919.412194418455
7
Iteration 12900: Loss = -11919.392071244898
8
Iteration 13000: Loss = -11919.388911310616
Iteration 13100: Loss = -11919.388151712223
Iteration 13200: Loss = -11919.389801937214
1
Iteration 13300: Loss = -11919.401729109943
2
Iteration 13400: Loss = -11919.426828011075
3
Iteration 13500: Loss = -11919.383385977268
Iteration 13600: Loss = -11919.383893996948
1
Iteration 13700: Loss = -11919.383550234807
2
Iteration 13800: Loss = -11919.4148382576
3
Iteration 13900: Loss = -11919.390929865824
4
Iteration 14000: Loss = -11919.383365900163
Iteration 14100: Loss = -11919.42333380304
1
Iteration 14200: Loss = -11919.399634843632
2
Iteration 14300: Loss = -11919.392467882724
3
Iteration 14400: Loss = -11919.38693379741
4
Iteration 14500: Loss = -11919.383206163158
Iteration 14600: Loss = -11919.395634843373
1
Iteration 14700: Loss = -11919.381909759764
Iteration 14800: Loss = -11919.355118269845
Iteration 14900: Loss = -11919.355393346621
1
Iteration 15000: Loss = -11919.355776054541
2
Iteration 15100: Loss = -11919.353070736002
Iteration 15200: Loss = -11919.39705005908
1
Iteration 15300: Loss = -11919.355303421562
2
Iteration 15400: Loss = -11919.357974372937
3
Iteration 15500: Loss = -11919.356216471517
4
Iteration 15600: Loss = -11919.3733106075
5
Iteration 15700: Loss = -11919.353131943375
Iteration 15800: Loss = -11919.396512311283
1
Iteration 15900: Loss = -11919.358440397651
2
Iteration 16000: Loss = -11919.354248222646
3
Iteration 16100: Loss = -11919.353860165236
4
Iteration 16200: Loss = -11919.353977224802
5
Iteration 16300: Loss = -11919.41385705684
6
Iteration 16400: Loss = -11919.366423991054
7
Iteration 16500: Loss = -11919.353406956434
8
Iteration 16600: Loss = -11919.353456265022
9
Iteration 16700: Loss = -11919.375986459905
10
Iteration 16800: Loss = -11919.35785759632
11
Iteration 16900: Loss = -11919.353075061741
Iteration 17000: Loss = -11919.379896277143
1
Iteration 17100: Loss = -11919.35467390167
2
Iteration 17200: Loss = -11919.356107195561
3
Iteration 17300: Loss = -11919.356746145231
4
Iteration 17400: Loss = -11919.36956219561
5
Iteration 17500: Loss = -11919.353737692982
6
Iteration 17600: Loss = -11919.353675073406
7
Iteration 17700: Loss = -11919.410717736346
8
Iteration 17800: Loss = -11919.356027046337
9
Iteration 17900: Loss = -11919.360237926978
10
Iteration 18000: Loss = -11919.353588546419
11
Iteration 18100: Loss = -11919.353735361336
12
Iteration 18200: Loss = -11919.353503642102
13
Iteration 18300: Loss = -11919.359059462282
14
Iteration 18400: Loss = -11919.353068804658
Iteration 18500: Loss = -11919.35355563205
1
Iteration 18600: Loss = -11919.354454494416
2
Iteration 18700: Loss = -11919.353934310306
3
Iteration 18800: Loss = -11919.403453939647
4
Iteration 18900: Loss = -11919.356880685324
5
Iteration 19000: Loss = -11919.370640785426
6
Iteration 19100: Loss = -11919.357195400778
7
Iteration 19200: Loss = -11919.357498045327
8
Iteration 19300: Loss = -11919.476542278286
9
Iteration 19400: Loss = -11919.355203034953
10
Iteration 19500: Loss = -11919.353147125708
Iteration 19600: Loss = -11919.371778216375
1
Iteration 19700: Loss = -11919.352823398038
Iteration 19800: Loss = -11919.382936420401
1
Iteration 19900: Loss = -11919.352856709202
pi: tensor([[0.7369, 0.2631],
        [0.2259, 0.7741]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4487, 0.5513], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3098, 0.0916],
         [0.6959, 0.3009]],

        [[0.5282, 0.1020],
         [0.5951, 0.6856]],

        [[0.6444, 0.1008],
         [0.5657, 0.5533]],

        [[0.5737, 0.1014],
         [0.7306, 0.5191]],

        [[0.5224, 0.1007],
         [0.5838, 0.5657]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9840312292878678
Average Adjusted Rand Index: 0.9839995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23581.979277234026
Iteration 100: Loss = -12472.119839921104
Iteration 200: Loss = -12471.153136863553
Iteration 300: Loss = -12470.793387557616
Iteration 400: Loss = -12470.586489707372
Iteration 500: Loss = -12470.534605677292
Iteration 600: Loss = -12470.522373070606
Iteration 700: Loss = -12470.513069099301
Iteration 800: Loss = -12470.49983167295
Iteration 900: Loss = -12470.472647931734
Iteration 1000: Loss = -12470.412085418215
Iteration 1100: Loss = -12470.351308640084
Iteration 1200: Loss = -12470.288531283757
Iteration 1300: Loss = -12470.112950846255
Iteration 1400: Loss = -12469.507548233656
Iteration 1500: Loss = -12469.02865324673
Iteration 1600: Loss = -12468.763136446001
Iteration 1700: Loss = -12468.55722225215
Iteration 1800: Loss = -12303.169722828043
Iteration 1900: Loss = -11991.338524106388
Iteration 2000: Loss = -11957.17234485466
Iteration 2100: Loss = -11956.974274665301
Iteration 2200: Loss = -11956.953707014634
Iteration 2300: Loss = -11956.944531247143
Iteration 2400: Loss = -11956.93747145239
Iteration 2500: Loss = -11956.933164294802
Iteration 2600: Loss = -11956.930052595044
Iteration 2700: Loss = -11956.927456078287
Iteration 2800: Loss = -11956.927513254694
Iteration 2900: Loss = -11956.921055311563
Iteration 3000: Loss = -11956.920212300987
Iteration 3100: Loss = -11956.91858071775
Iteration 3200: Loss = -11956.920489718206
1
Iteration 3300: Loss = -11956.91669588108
Iteration 3400: Loss = -11956.916845250125
1
Iteration 3500: Loss = -11956.915044829862
Iteration 3600: Loss = -11956.914182361887
Iteration 3700: Loss = -11956.912798554125
Iteration 3800: Loss = -11956.908686316448
Iteration 3900: Loss = -11956.908039241576
Iteration 4000: Loss = -11956.907570229616
Iteration 4100: Loss = -11956.907016225725
Iteration 4200: Loss = -11956.906303490381
Iteration 4300: Loss = -11956.913376532228
1
Iteration 4400: Loss = -11956.9057112693
Iteration 4500: Loss = -11956.905514427903
Iteration 4600: Loss = -11956.90620863398
1
Iteration 4700: Loss = -11956.905227938576
Iteration 4800: Loss = -11956.910687650396
1
Iteration 4900: Loss = -11956.904730506749
Iteration 5000: Loss = -11956.904334820865
Iteration 5100: Loss = -11956.911087002101
1
Iteration 5200: Loss = -11956.903958754054
Iteration 5300: Loss = -11956.903818947429
Iteration 5400: Loss = -11956.907910460775
1
Iteration 5500: Loss = -11956.903931704524
2
Iteration 5600: Loss = -11956.905294302476
3
Iteration 5700: Loss = -11956.903339965716
Iteration 5800: Loss = -11956.910538317448
1
Iteration 5900: Loss = -11956.90323696115
Iteration 6000: Loss = -11956.902806510536
Iteration 6100: Loss = -11956.902495087812
Iteration 6200: Loss = -11956.904496198156
1
Iteration 6300: Loss = -11956.90740344769
2
Iteration 6400: Loss = -11956.90291849472
3
Iteration 6500: Loss = -11956.902838992592
4
Iteration 6600: Loss = -11956.902231774937
Iteration 6700: Loss = -11956.902139279948
Iteration 6800: Loss = -11956.901681693938
Iteration 6900: Loss = -11956.902933089692
1
Iteration 7000: Loss = -11956.901800678002
2
Iteration 7100: Loss = -11956.90172132546
Iteration 7200: Loss = -11956.90141251555
Iteration 7300: Loss = -11956.901226771475
Iteration 7400: Loss = -11956.90146896497
1
Iteration 7500: Loss = -11956.945206432063
2
Iteration 7600: Loss = -11956.898673373207
Iteration 7700: Loss = -11956.898583866401
Iteration 7800: Loss = -11956.898379119792
Iteration 7900: Loss = -11956.898288133883
Iteration 8000: Loss = -11956.899357511473
1
Iteration 8100: Loss = -11956.897984885503
Iteration 8200: Loss = -11956.897824802458
Iteration 8300: Loss = -11956.900914264057
1
Iteration 8400: Loss = -11956.89980568175
2
Iteration 8500: Loss = -11956.923661900173
3
Iteration 8600: Loss = -11956.897424220506
Iteration 8700: Loss = -11956.897135685282
Iteration 8800: Loss = -11956.900496824666
1
Iteration 8900: Loss = -11956.898849033514
2
Iteration 9000: Loss = -11957.07320415257
3
Iteration 9100: Loss = -11956.896796037938
Iteration 9200: Loss = -11956.848341389177
Iteration 9300: Loss = -11956.78730663376
Iteration 9400: Loss = -11956.785803738876
Iteration 9500: Loss = -11956.78594390821
1
Iteration 9600: Loss = -11956.791230928096
2
Iteration 9700: Loss = -11956.785634423124
Iteration 9800: Loss = -11956.785947548044
1
Iteration 9900: Loss = -11956.79936614322
2
Iteration 10000: Loss = -11956.788094024538
3
Iteration 10100: Loss = -11956.78717190642
4
Iteration 10200: Loss = -11956.842711480587
5
Iteration 10300: Loss = -11956.871468635893
6
Iteration 10400: Loss = -11956.80071068993
7
Iteration 10500: Loss = -11956.79434549931
8
Iteration 10600: Loss = -11956.797126387884
9
Iteration 10700: Loss = -11956.80605800228
10
Iteration 10800: Loss = -11956.785599343311
Iteration 10900: Loss = -11956.813474693672
1
Iteration 11000: Loss = -11956.785546427127
Iteration 11100: Loss = -11956.799015710694
1
Iteration 11200: Loss = -11956.786570754759
2
Iteration 11300: Loss = -11956.78619438898
3
Iteration 11400: Loss = -11956.819709308405
4
Iteration 11500: Loss = -11956.785598939085
Iteration 11600: Loss = -11956.795998836575
1
Iteration 11700: Loss = -11956.796811776645
2
Iteration 11800: Loss = -11956.789186581998
3
Iteration 11900: Loss = -11956.794996116238
4
Iteration 12000: Loss = -11956.78754496977
5
Iteration 12100: Loss = -11956.787410587152
6
Iteration 12200: Loss = -11956.790621816439
7
Iteration 12300: Loss = -11956.786090877955
8
Iteration 12400: Loss = -11956.785722781808
9
Iteration 12500: Loss = -11956.859137308596
10
Iteration 12600: Loss = -11956.801050782893
11
Iteration 12700: Loss = -11956.78553986264
Iteration 12800: Loss = -11956.788102082266
1
Iteration 12900: Loss = -11956.79950690366
2
Iteration 13000: Loss = -11956.788633976277
3
Iteration 13100: Loss = -11956.788607062697
4
Iteration 13200: Loss = -11956.786708786896
5
Iteration 13300: Loss = -11956.789389163074
6
Iteration 13400: Loss = -11956.795531686059
7
Iteration 13500: Loss = -11956.839640202168
8
Iteration 13600: Loss = -11956.79470022994
9
Iteration 13700: Loss = -11956.786464720195
10
Iteration 13800: Loss = -11956.790305183104
11
Iteration 13900: Loss = -11956.82416666505
12
Iteration 14000: Loss = -11956.88867444336
13
Iteration 14100: Loss = -11956.785716030003
14
Iteration 14200: Loss = -11956.792255808949
15
Stopping early at iteration 14200 due to no improvement.
pi: tensor([[0.8041, 0.1959],
        [0.2451, 0.7549]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5539, 0.4461], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2926, 0.0920],
         [0.5459, 0.3141]],

        [[0.6682, 0.1051],
         [0.5970, 0.5497]],

        [[0.7039, 0.1058],
         [0.7215, 0.6151]],

        [[0.6323, 0.1076],
         [0.6057, 0.5551]],

        [[0.6862, 0.1006],
         [0.6450, 0.5602]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9368921261444934
Average Adjusted Rand Index: 0.936623094906383
11927.240027888682
[0.9840312292878678, 0.9368921261444934] [0.9839995611635631, 0.936623094906383] [11919.397118288514, 11956.792255808949]
-------------------------------------
This iteration is 53
True Objective function: Loss = -11863.412817879685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21586.889179819937
Iteration 100: Loss = -12308.92103360882
Iteration 200: Loss = -12308.518514544467
Iteration 300: Loss = -12308.400019040653
Iteration 400: Loss = -12308.320474000848
Iteration 500: Loss = -12308.254052587134
Iteration 600: Loss = -12308.182505226088
Iteration 700: Loss = -12308.077229667182
Iteration 800: Loss = -12307.90313701148
Iteration 900: Loss = -12307.64504244239
Iteration 1000: Loss = -12307.30207001686
Iteration 1100: Loss = -12307.01807480643
Iteration 1200: Loss = -12306.808220266525
Iteration 1300: Loss = -12306.641625291519
Iteration 1400: Loss = -12306.524119704878
Iteration 1500: Loss = -12306.452497694081
Iteration 1600: Loss = -12306.40999770447
Iteration 1700: Loss = -12306.379565288498
Iteration 1800: Loss = -12306.356571396445
Iteration 1900: Loss = -12306.33768203893
Iteration 2000: Loss = -12306.320834208036
Iteration 2100: Loss = -12306.304144034686
Iteration 2200: Loss = -12306.288041975771
Iteration 2300: Loss = -12306.273486742544
Iteration 2400: Loss = -12306.261033393717
Iteration 2500: Loss = -12306.250793888672
Iteration 2600: Loss = -12306.242608014469
Iteration 2700: Loss = -12306.235989819304
Iteration 2800: Loss = -12306.230775034293
Iteration 2900: Loss = -12306.226537057599
Iteration 3000: Loss = -12306.223156586579
Iteration 3100: Loss = -12306.220395887347
Iteration 3200: Loss = -12306.21817631556
Iteration 3300: Loss = -12306.21628980294
Iteration 3400: Loss = -12306.214759598945
Iteration 3500: Loss = -12306.213405382949
Iteration 3600: Loss = -12306.21229971622
Iteration 3700: Loss = -12306.211357161195
Iteration 3800: Loss = -12306.21055249556
Iteration 3900: Loss = -12306.209831691029
Iteration 4000: Loss = -12306.209183074687
Iteration 4100: Loss = -12306.208578141232
Iteration 4200: Loss = -12306.20801833346
Iteration 4300: Loss = -12306.207545243957
Iteration 4400: Loss = -12306.207119428118
Iteration 4500: Loss = -12306.206666306307
Iteration 4600: Loss = -12306.206263635502
Iteration 4700: Loss = -12306.205849300075
Iteration 4800: Loss = -12306.205526501484
Iteration 4900: Loss = -12306.205658823526
1
Iteration 5000: Loss = -12306.204850674922
Iteration 5100: Loss = -12306.204561670345
Iteration 5200: Loss = -12306.204295095518
Iteration 5300: Loss = -12306.203977279863
Iteration 5400: Loss = -12306.203754446824
Iteration 5500: Loss = -12306.203489435351
Iteration 5600: Loss = -12306.203222193573
Iteration 5700: Loss = -12306.202987927632
Iteration 5800: Loss = -12306.202765731823
Iteration 5900: Loss = -12306.20432749236
1
Iteration 6000: Loss = -12306.20237031281
Iteration 6100: Loss = -12306.202379929611
Iteration 6200: Loss = -12306.204120957345
1
Iteration 6300: Loss = -12306.201849306111
Iteration 6400: Loss = -12306.201737702024
Iteration 6500: Loss = -12306.202225212748
1
Iteration 6600: Loss = -12306.201346948756
Iteration 6700: Loss = -12306.201321264827
Iteration 6800: Loss = -12306.201097844294
Iteration 6900: Loss = -12306.201034556476
Iteration 7000: Loss = -12306.200990099822
Iteration 7100: Loss = -12306.201646015581
1
Iteration 7200: Loss = -12306.202005935078
2
Iteration 7300: Loss = -12306.226840948406
3
Iteration 7400: Loss = -12306.200473088184
Iteration 7500: Loss = -12306.200483620829
Iteration 7600: Loss = -12306.200273138777
Iteration 7700: Loss = -12306.200265436659
Iteration 7800: Loss = -12306.200105798098
Iteration 7900: Loss = -12306.20805530489
1
Iteration 8000: Loss = -12306.199974752031
Iteration 8100: Loss = -12306.210910052552
1
Iteration 8200: Loss = -12306.199792662523
Iteration 8300: Loss = -12306.200317611423
1
Iteration 8400: Loss = -12306.291940451418
2
Iteration 8500: Loss = -12306.199648321073
Iteration 8600: Loss = -12306.20917247065
1
Iteration 8700: Loss = -12306.199591776243
Iteration 8800: Loss = -12306.206216074126
1
Iteration 8900: Loss = -12306.199493422479
Iteration 9000: Loss = -12306.200014027732
1
Iteration 9100: Loss = -12306.199409499677
Iteration 9200: Loss = -12306.19941981401
Iteration 9300: Loss = -12306.199887630413
1
Iteration 9400: Loss = -12306.203706123893
2
Iteration 9500: Loss = -12306.199216123108
Iteration 9600: Loss = -12306.204322558131
1
Iteration 9700: Loss = -12306.20213554974
2
Iteration 9800: Loss = -12306.202044730913
3
Iteration 9900: Loss = -12306.222689265545
4
Iteration 10000: Loss = -12306.199157995814
Iteration 10100: Loss = -12306.237775075238
1
Iteration 10200: Loss = -12306.199039504714
Iteration 10300: Loss = -12306.212663647959
1
Iteration 10400: Loss = -12306.238225374656
2
Iteration 10500: Loss = -12306.198959727917
Iteration 10600: Loss = -12306.20646605396
1
Iteration 10700: Loss = -12306.203563260493
2
Iteration 10800: Loss = -12306.198951423883
Iteration 10900: Loss = -12306.210512126787
1
Iteration 11000: Loss = -12306.19890729439
Iteration 11100: Loss = -12306.199002178628
Iteration 11200: Loss = -12306.200097949664
1
Iteration 11300: Loss = -12306.200991668351
2
Iteration 11400: Loss = -12306.207213009373
3
Iteration 11500: Loss = -12306.208640184235
4
Iteration 11600: Loss = -12306.19883614558
Iteration 11700: Loss = -12306.199107116161
1
Iteration 11800: Loss = -12306.4701159226
2
Iteration 11900: Loss = -12306.199144573957
3
Iteration 12000: Loss = -12306.204330855739
4
Iteration 12100: Loss = -12306.2006610408
5
Iteration 12200: Loss = -12306.204656942004
6
Iteration 12300: Loss = -12306.198747596725
Iteration 12400: Loss = -12306.20353932223
1
Iteration 12500: Loss = -12306.455084370957
2
Iteration 12600: Loss = -12306.204982574489
3
Iteration 12700: Loss = -12306.199710589617
4
Iteration 12800: Loss = -12306.203295122577
5
Iteration 12900: Loss = -12306.198701296289
Iteration 13000: Loss = -12306.199085676
1
Iteration 13100: Loss = -12306.199411027925
2
Iteration 13200: Loss = -12306.199331519352
3
Iteration 13300: Loss = -12306.392696328061
4
Iteration 13400: Loss = -12306.206110561092
5
Iteration 13500: Loss = -12306.227517718939
6
Iteration 13600: Loss = -12306.198651381206
Iteration 13700: Loss = -12306.208042700693
1
Iteration 13800: Loss = -12306.198649007467
Iteration 13900: Loss = -12306.1987708955
1
Iteration 14000: Loss = -12306.19863888493
Iteration 14100: Loss = -12306.198709143158
Iteration 14200: Loss = -12306.198631474903
Iteration 14300: Loss = -12306.205874537814
1
Iteration 14400: Loss = -12306.198795754559
2
Iteration 14500: Loss = -12306.244943924012
3
Iteration 14600: Loss = -12306.201431108828
4
Iteration 14700: Loss = -12306.198695181309
Iteration 14800: Loss = -12306.198895731823
1
Iteration 14900: Loss = -12306.562778829155
2
Iteration 15000: Loss = -12306.19862568059
Iteration 15100: Loss = -12306.222735556115
1
Iteration 15200: Loss = -12306.19865224306
Iteration 15300: Loss = -12306.200259720203
1
Iteration 15400: Loss = -12306.223133261596
2
Iteration 15500: Loss = -12306.199740056309
3
Iteration 15600: Loss = -12306.198971997466
4
Iteration 15700: Loss = -12306.200399300515
5
Iteration 15800: Loss = -12306.199104971456
6
Iteration 15900: Loss = -12306.215214454081
7
Iteration 16000: Loss = -12306.2107953157
8
Iteration 16100: Loss = -12306.206057802383
9
Iteration 16200: Loss = -12306.1987426963
Iteration 16300: Loss = -12306.199051561878
1
Iteration 16400: Loss = -12306.198974374027
2
Iteration 16500: Loss = -12306.198804702313
Iteration 16600: Loss = -12306.20221619488
1
Iteration 16700: Loss = -12306.208379889404
2
Iteration 16800: Loss = -12306.210043655105
3
Iteration 16900: Loss = -12306.198634839868
Iteration 17000: Loss = -12306.198571394294
Iteration 17100: Loss = -12306.198734820588
1
Iteration 17200: Loss = -12306.204067666678
2
Iteration 17300: Loss = -12306.220019521561
3
Iteration 17400: Loss = -12306.228531975052
4
Iteration 17500: Loss = -12306.19854634002
Iteration 17600: Loss = -12306.198621912547
Iteration 17700: Loss = -12306.210186242046
1
Iteration 17800: Loss = -12306.19888756899
2
Iteration 17900: Loss = -12306.198716760244
Iteration 18000: Loss = -12306.20007099248
1
Iteration 18100: Loss = -12306.19978330347
2
Iteration 18200: Loss = -12306.427863153805
3
Iteration 18300: Loss = -12306.198575404138
Iteration 18400: Loss = -12306.200400531257
1
Iteration 18500: Loss = -12306.198563374204
Iteration 18600: Loss = -12306.20914285013
1
Iteration 18700: Loss = -12306.237451353769
2
Iteration 18800: Loss = -12306.198567960708
Iteration 18900: Loss = -12306.200976724647
1
Iteration 19000: Loss = -12306.228929479767
2
Iteration 19100: Loss = -12306.198555822517
Iteration 19200: Loss = -12306.218844904131
1
Iteration 19300: Loss = -12306.200702334829
2
Iteration 19400: Loss = -12306.199139947828
3
Iteration 19500: Loss = -12306.20163515191
4
Iteration 19600: Loss = -12306.288937839938
5
Iteration 19700: Loss = -12306.198569414062
Iteration 19800: Loss = -12306.202538845271
1
Iteration 19900: Loss = -12306.200451635135
2
pi: tensor([[2.1454e-06, 1.0000e+00],
        [4.3825e-02, 9.5617e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3829, 0.6171], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2074, 0.2020],
         [0.6198, 0.1963]],

        [[0.5534, 0.1825],
         [0.6158, 0.5914]],

        [[0.5020, 0.1313],
         [0.5807, 0.6797]],

        [[0.6462, 0.2675],
         [0.7081, 0.5871]],

        [[0.6243, 0.2204],
         [0.6397, 0.5990]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00036641132057026226
Average Adjusted Rand Index: -0.0005926784880256398
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21255.011045522628
Iteration 100: Loss = -12309.346588625953
Iteration 200: Loss = -12308.76666313443
Iteration 300: Loss = -12308.549018137268
Iteration 400: Loss = -12308.427550446226
Iteration 500: Loss = -12308.34130202075
Iteration 600: Loss = -12308.265949391698
Iteration 700: Loss = -12308.18416929578
Iteration 800: Loss = -12308.086640119722
Iteration 900: Loss = -12307.978748896789
Iteration 1000: Loss = -12307.851772847185
Iteration 1100: Loss = -12307.673972364713
Iteration 1200: Loss = -12307.419495515713
Iteration 1300: Loss = -12307.11917034789
Iteration 1400: Loss = -12306.868671591352
Iteration 1500: Loss = -12306.692495474881
Iteration 1600: Loss = -12306.576704996158
Iteration 1700: Loss = -12306.510544642932
Iteration 1800: Loss = -12306.476303522395
Iteration 1900: Loss = -12306.455987429319
Iteration 2000: Loss = -12306.440143489297
Iteration 2100: Loss = -12306.424998136026
Iteration 2200: Loss = -12306.409265149356
Iteration 2300: Loss = -12306.392335896435
Iteration 2400: Loss = -12306.374163407001
Iteration 2500: Loss = -12306.354991723469
Iteration 2600: Loss = -12306.335484096642
Iteration 2700: Loss = -12306.31676029985
Iteration 2800: Loss = -12306.29964119134
Iteration 2900: Loss = -12306.284586703576
Iteration 3000: Loss = -12306.27179943864
Iteration 3100: Loss = -12306.261206228086
Iteration 3200: Loss = -12306.252334435927
Iteration 3300: Loss = -12306.245073437565
Iteration 3400: Loss = -12306.23907832524
Iteration 3500: Loss = -12306.23407358024
Iteration 3600: Loss = -12306.229964019327
Iteration 3700: Loss = -12306.226496344394
Iteration 3800: Loss = -12306.223657979288
Iteration 3900: Loss = -12306.221234467526
Iteration 4000: Loss = -12306.219169176444
Iteration 4100: Loss = -12306.21742824522
Iteration 4200: Loss = -12306.215932281435
Iteration 4300: Loss = -12306.214562045388
Iteration 4400: Loss = -12306.213442471653
Iteration 4500: Loss = -12306.212455764506
Iteration 4600: Loss = -12306.211546069078
Iteration 4700: Loss = -12306.210725206784
Iteration 4800: Loss = -12306.210027910753
Iteration 4900: Loss = -12306.20938203082
Iteration 5000: Loss = -12306.209229720966
Iteration 5100: Loss = -12306.208238206538
Iteration 5200: Loss = -12306.207743692363
Iteration 5300: Loss = -12306.207261327936
Iteration 5400: Loss = -12306.206819359802
Iteration 5500: Loss = -12306.206416414816
Iteration 5600: Loss = -12306.206006243507
Iteration 5700: Loss = -12306.205652685989
Iteration 5800: Loss = -12306.205302112296
Iteration 5900: Loss = -12306.20498900234
Iteration 6000: Loss = -12306.204647457043
Iteration 6100: Loss = -12306.206036279837
1
Iteration 6200: Loss = -12306.204060709895
Iteration 6300: Loss = -12306.203772564413
Iteration 6400: Loss = -12306.203530059856
Iteration 6500: Loss = -12306.203242630752
Iteration 6600: Loss = -12306.203087110818
Iteration 6700: Loss = -12306.202939395238
Iteration 6800: Loss = -12306.211781889684
1
Iteration 6900: Loss = -12306.20781016619
2
Iteration 7000: Loss = -12306.202227317806
Iteration 7100: Loss = -12306.202275195474
Iteration 7200: Loss = -12306.433774122797
1
Iteration 7300: Loss = -12306.201680561291
Iteration 7400: Loss = -12306.423548816125
1
Iteration 7500: Loss = -12306.201445524082
Iteration 7600: Loss = -12306.201233519005
Iteration 7700: Loss = -12306.201444763294
1
Iteration 7800: Loss = -12306.201001831105
Iteration 7900: Loss = -12306.397891886121
1
Iteration 8000: Loss = -12306.20076922336
Iteration 8100: Loss = -12306.20068815911
Iteration 8200: Loss = -12306.203872373766
1
Iteration 8300: Loss = -12306.200446605337
Iteration 8400: Loss = -12306.200385776538
Iteration 8500: Loss = -12306.200372175614
Iteration 8600: Loss = -12306.200216248319
Iteration 8700: Loss = -12306.200212058933
Iteration 8800: Loss = -12306.200083656304
Iteration 8900: Loss = -12306.202067549868
1
Iteration 9000: Loss = -12306.199909596664
Iteration 9100: Loss = -12306.286401995667
1
Iteration 9200: Loss = -12306.199789283799
Iteration 9300: Loss = -12306.199759729316
Iteration 9400: Loss = -12306.200651752368
1
Iteration 9500: Loss = -12306.201084471692
2
Iteration 9600: Loss = -12306.199682162052
Iteration 9700: Loss = -12306.200850858042
1
Iteration 9800: Loss = -12306.199480141766
Iteration 9900: Loss = -12306.199543107406
Iteration 10000: Loss = -12306.202573477542
1
Iteration 10100: Loss = -12306.200843043358
2
Iteration 10200: Loss = -12306.21450185937
3
Iteration 10300: Loss = -12306.36249315193
4
Iteration 10400: Loss = -12306.199275837713
Iteration 10500: Loss = -12306.199754775376
1
Iteration 10600: Loss = -12306.327440498653
2
Iteration 10700: Loss = -12306.311455581807
3
Iteration 10800: Loss = -12306.199155670047
Iteration 10900: Loss = -12306.199160537008
Iteration 11000: Loss = -12306.204513858384
1
Iteration 11100: Loss = -12306.199546878719
2
Iteration 11200: Loss = -12306.2005106923
3
Iteration 11300: Loss = -12306.198917237627
Iteration 11400: Loss = -12306.213521074147
1
Iteration 11500: Loss = -12306.202743984237
2
Iteration 11600: Loss = -12306.4023498597
3
Iteration 11700: Loss = -12306.198950347401
Iteration 11800: Loss = -12306.201093129654
1
Iteration 11900: Loss = -12306.360316048153
2
Iteration 12000: Loss = -12306.200005119075
3
Iteration 12100: Loss = -12306.199584705493
4
Iteration 12200: Loss = -12306.200627238582
5
Iteration 12300: Loss = -12306.200670495642
6
Iteration 12400: Loss = -12306.19886622395
Iteration 12500: Loss = -12306.199561177307
1
Iteration 12600: Loss = -12306.286067795061
2
Iteration 12700: Loss = -12306.201926995445
3
Iteration 12800: Loss = -12306.22237221289
4
Iteration 12900: Loss = -12306.219989401372
5
Iteration 13000: Loss = -12306.214177940476
6
Iteration 13100: Loss = -12306.199072791296
7
Iteration 13200: Loss = -12306.258936970735
8
Iteration 13300: Loss = -12306.228348675942
9
Iteration 13400: Loss = -12306.206919099668
10
Iteration 13500: Loss = -12306.296303045248
11
Iteration 13600: Loss = -12306.200309514728
12
Iteration 13700: Loss = -12306.235656281655
13
Iteration 13800: Loss = -12306.198697502496
Iteration 13900: Loss = -12306.198743129928
Iteration 14000: Loss = -12306.200544326317
1
Iteration 14100: Loss = -12306.204560988572
2
Iteration 14200: Loss = -12306.198760279707
Iteration 14300: Loss = -12306.215645668231
1
Iteration 14400: Loss = -12306.199591988187
2
Iteration 14500: Loss = -12306.368813875251
3
Iteration 14600: Loss = -12306.201146055424
4
Iteration 14700: Loss = -12306.198822303619
Iteration 14800: Loss = -12306.198756300273
Iteration 14900: Loss = -12306.20099652013
1
Iteration 15000: Loss = -12306.20142768714
2
Iteration 15100: Loss = -12306.198638383397
Iteration 15200: Loss = -12306.213706328772
1
Iteration 15300: Loss = -12306.220019857643
2
Iteration 15400: Loss = -12306.249722104614
3
Iteration 15500: Loss = -12306.210695804159
4
Iteration 15600: Loss = -12306.25523387889
5
Iteration 15700: Loss = -12306.198697500226
Iteration 15800: Loss = -12306.198670819185
Iteration 15900: Loss = -12306.19867919213
Iteration 16000: Loss = -12306.198622889848
Iteration 16100: Loss = -12306.199806112985
1
Iteration 16200: Loss = -12306.19869920677
Iteration 16300: Loss = -12306.19939724922
1
Iteration 16400: Loss = -12306.198763781586
Iteration 16500: Loss = -12306.200655610488
1
Iteration 16600: Loss = -12306.207153218667
2
Iteration 16700: Loss = -12306.219655194423
3
Iteration 16800: Loss = -12306.225981124171
4
Iteration 16900: Loss = -12306.198727842408
Iteration 17000: Loss = -12306.202618963525
1
Iteration 17100: Loss = -12306.198887031369
2
Iteration 17200: Loss = -12306.201330074839
3
Iteration 17300: Loss = -12306.207181684118
4
Iteration 17400: Loss = -12306.199958895933
5
Iteration 17500: Loss = -12306.199388527177
6
Iteration 17600: Loss = -12306.215719120324
7
Iteration 17700: Loss = -12306.198687481397
Iteration 17800: Loss = -12306.30515000328
1
Iteration 17900: Loss = -12306.198595760225
Iteration 18000: Loss = -12306.218050146537
1
Iteration 18100: Loss = -12306.198630028017
Iteration 18200: Loss = -12306.19896510574
1
Iteration 18300: Loss = -12306.198675413956
Iteration 18400: Loss = -12306.198842721707
1
Iteration 18500: Loss = -12306.209147477784
2
Iteration 18600: Loss = -12306.200469568648
3
Iteration 18700: Loss = -12306.222494223019
4
Iteration 18800: Loss = -12306.199001596136
5
Iteration 18900: Loss = -12306.21367618471
6
Iteration 19000: Loss = -12306.198616063113
Iteration 19100: Loss = -12306.200985646148
1
Iteration 19200: Loss = -12306.198670418587
Iteration 19300: Loss = -12306.198565970057
Iteration 19400: Loss = -12306.198639112094
Iteration 19500: Loss = -12306.198674900694
Iteration 19600: Loss = -12306.198606390402
Iteration 19700: Loss = -12306.198593289599
Iteration 19800: Loss = -12306.19918777039
1
Iteration 19900: Loss = -12306.204000745865
2
pi: tensor([[3.3742e-06, 1.0000e+00],
        [4.3767e-02, 9.5623e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3827, 0.6173], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2074, 0.2021],
         [0.6034, 0.1963]],

        [[0.5566, 0.1826],
         [0.5931, 0.5747]],

        [[0.6918, 0.1315],
         [0.6915, 0.5528]],

        [[0.6387, 0.2681],
         [0.5225, 0.6622]],

        [[0.5646, 0.2203],
         [0.5381, 0.5848]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00036641132057026226
Average Adjusted Rand Index: -0.0005926784880256398
11863.412817879685
[-0.00036641132057026226, -0.00036641132057026226] [-0.0005926784880256398, -0.0005926784880256398] [12306.199635643618, 12306.250140298333]
-------------------------------------
This iteration is 54
True Objective function: Loss = -11909.81335217151
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22405.82297624297
Iteration 100: Loss = -12062.394810218871
Iteration 200: Loss = -11903.478681085553
Iteration 300: Loss = -11902.906092567227
Iteration 400: Loss = -11902.701765940095
Iteration 500: Loss = -11902.57188577191
Iteration 600: Loss = -11902.36558910698
Iteration 700: Loss = -11902.32471916002
Iteration 800: Loss = -11902.295430691307
Iteration 900: Loss = -11902.275877443642
Iteration 1000: Loss = -11902.261001496548
Iteration 1100: Loss = -11902.249960342233
Iteration 1200: Loss = -11902.241255651656
Iteration 1300: Loss = -11902.234301609971
Iteration 1400: Loss = -11902.228577628634
Iteration 1500: Loss = -11902.224047016627
Iteration 1600: Loss = -11902.219993223896
Iteration 1700: Loss = -11902.2167099657
Iteration 1800: Loss = -11902.214428688376
Iteration 1900: Loss = -11902.211410443075
Iteration 2000: Loss = -11902.209359678796
Iteration 2100: Loss = -11902.207941849843
Iteration 2200: Loss = -11902.205858471785
Iteration 2300: Loss = -11902.20445384603
Iteration 2400: Loss = -11902.224986866433
1
Iteration 2500: Loss = -11902.201979363675
Iteration 2600: Loss = -11902.201015890763
Iteration 2700: Loss = -11902.200001870831
Iteration 2800: Loss = -11902.199369523218
Iteration 2900: Loss = -11902.19845369334
Iteration 3000: Loss = -11902.198713835662
1
Iteration 3100: Loss = -11902.197587430319
Iteration 3200: Loss = -11902.201922479362
1
Iteration 3300: Loss = -11902.196068698107
Iteration 3400: Loss = -11902.195625827773
Iteration 3500: Loss = -11902.195231215086
Iteration 3600: Loss = -11902.195171982381
Iteration 3700: Loss = -11902.195454428671
1
Iteration 3800: Loss = -11902.195076904874
Iteration 3900: Loss = -11902.195181138923
1
Iteration 4000: Loss = -11902.20128926666
2
Iteration 4100: Loss = -11902.193333977986
Iteration 4200: Loss = -11902.193529400007
1
Iteration 4300: Loss = -11902.193525517017
2
Iteration 4400: Loss = -11902.19269214246
Iteration 4500: Loss = -11902.192490026571
Iteration 4600: Loss = -11902.19467872446
1
Iteration 4700: Loss = -11902.197749151546
2
Iteration 4800: Loss = -11902.19240884075
Iteration 4900: Loss = -11902.191961156224
Iteration 5000: Loss = -11902.191715022598
Iteration 5100: Loss = -11902.191631117797
Iteration 5200: Loss = -11902.191545425218
Iteration 5300: Loss = -11902.191367294587
Iteration 5400: Loss = -11902.191315168586
Iteration 5500: Loss = -11902.191173391482
Iteration 5600: Loss = -11902.19102858227
Iteration 5700: Loss = -11902.191002584834
Iteration 5800: Loss = -11902.190900110485
Iteration 5900: Loss = -11902.190806134287
Iteration 6000: Loss = -11902.190818782405
Iteration 6100: Loss = -11902.190591663348
Iteration 6200: Loss = -11902.190514291722
Iteration 6300: Loss = -11902.190624619978
1
Iteration 6400: Loss = -11902.190423296293
Iteration 6500: Loss = -11902.19034124532
Iteration 6600: Loss = -11902.190381381903
Iteration 6700: Loss = -11902.190274277062
Iteration 6800: Loss = -11902.190241273001
Iteration 6900: Loss = -11902.190212211639
Iteration 7000: Loss = -11902.190147210544
Iteration 7100: Loss = -11902.200128812503
1
Iteration 7200: Loss = -11902.190069194297
Iteration 7300: Loss = -11902.19006389759
Iteration 7400: Loss = -11902.190070807193
Iteration 7500: Loss = -11902.18998315463
Iteration 7600: Loss = -11902.19216972389
1
Iteration 7700: Loss = -11902.191520476033
2
Iteration 7800: Loss = -11902.189905242194
Iteration 7900: Loss = -11902.194483881227
1
Iteration 8000: Loss = -11902.18985049816
Iteration 8100: Loss = -11902.210018295456
1
Iteration 8200: Loss = -11902.189830312862
Iteration 8300: Loss = -11902.189805421858
Iteration 8400: Loss = -11902.190054605402
1
Iteration 8500: Loss = -11902.189767244108
Iteration 8600: Loss = -11902.189738752617
Iteration 8700: Loss = -11902.18984575689
1
Iteration 8800: Loss = -11902.191226659977
2
Iteration 8900: Loss = -11902.189698514752
Iteration 9000: Loss = -11902.189808194209
1
Iteration 9100: Loss = -11902.19461584759
2
Iteration 9200: Loss = -11902.190394971814
3
Iteration 9300: Loss = -11902.19014258388
4
Iteration 9400: Loss = -11902.189833601624
5
Iteration 9500: Loss = -11902.39697884507
6
Iteration 9600: Loss = -11902.19360246249
7
Iteration 9700: Loss = -11902.211009601366
8
Iteration 9800: Loss = -11902.204891380732
9
Iteration 9900: Loss = -11902.190785186693
10
Iteration 10000: Loss = -11902.201615754462
11
Iteration 10100: Loss = -11902.190415701141
12
Iteration 10200: Loss = -11902.205417099016
13
Iteration 10300: Loss = -11902.191089293467
14
Iteration 10400: Loss = -11902.197258785853
15
Stopping early at iteration 10400 due to no improvement.
pi: tensor([[0.7867, 0.2133],
        [0.2525, 0.7475]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4545, 0.5455], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3031, 0.1027],
         [0.6643, 0.2970]],

        [[0.6070, 0.1072],
         [0.6767, 0.6239]],

        [[0.6819, 0.0937],
         [0.6730, 0.5537]],

        [[0.5366, 0.1021],
         [0.5187, 0.6672]],

        [[0.5504, 0.1052],
         [0.5448, 0.7157]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999691484065
Average Adjusted Rand Index: 0.992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19886.180698319895
Iteration 100: Loss = -12410.204702839053
Iteration 200: Loss = -12409.916302923204
Iteration 300: Loss = -12409.888689443298
Iteration 400: Loss = -12409.876681028767
Iteration 500: Loss = -12409.868647613566
Iteration 600: Loss = -12409.862641452606
Iteration 700: Loss = -12409.857692654567
Iteration 800: Loss = -12409.853334247518
Iteration 900: Loss = -12409.849128148808
Iteration 1000: Loss = -12409.844842739958
Iteration 1100: Loss = -12409.84013301398
Iteration 1200: Loss = -12409.834726349574
Iteration 1300: Loss = -12409.828160135403
Iteration 1400: Loss = -12409.819830991837
Iteration 1500: Loss = -12409.808225119159
Iteration 1600: Loss = -12409.788606626178
Iteration 1700: Loss = -12409.736977184808
Iteration 1800: Loss = -12409.509144345437
Iteration 1900: Loss = -12409.11050214576
Iteration 2000: Loss = -12408.705238320623
Iteration 2100: Loss = -12408.505819841459
Iteration 2200: Loss = -12408.414865164654
Iteration 2300: Loss = -12408.361249297333
Iteration 2400: Loss = -12408.325143105247
Iteration 2500: Loss = -12408.299312792795
Iteration 2600: Loss = -12408.280304763543
Iteration 2700: Loss = -12408.26614647739
Iteration 2800: Loss = -12408.25562841016
Iteration 2900: Loss = -12408.248897625677
Iteration 3000: Loss = -12408.241642541085
Iteration 3100: Loss = -12408.237046463692
Iteration 3200: Loss = -12408.233630428727
Iteration 3300: Loss = -12408.230769500466
Iteration 3400: Loss = -12408.228499927865
Iteration 3500: Loss = -12408.226732007723
Iteration 3600: Loss = -12408.225265364774
Iteration 3700: Loss = -12408.224140038845
Iteration 3800: Loss = -12408.222990380262
Iteration 3900: Loss = -12408.222119449703
Iteration 4000: Loss = -12408.221300741723
Iteration 4100: Loss = -12408.220603044863
Iteration 4200: Loss = -12408.219984823223
Iteration 4300: Loss = -12408.21944975389
Iteration 4400: Loss = -12408.21895184474
Iteration 4500: Loss = -12408.21851531723
Iteration 4600: Loss = -12408.218049858135
Iteration 4700: Loss = -12408.217652209414
Iteration 4800: Loss = -12408.217352086136
Iteration 4900: Loss = -12408.21699999301
Iteration 5000: Loss = -12408.216690424193
Iteration 5100: Loss = -12408.216420919409
Iteration 5200: Loss = -12408.216196640511
Iteration 5300: Loss = -12408.215999152246
Iteration 5400: Loss = -12408.215763212855
Iteration 5500: Loss = -12408.215542874235
Iteration 5600: Loss = -12408.215366822731
Iteration 5700: Loss = -12408.215161919927
Iteration 5800: Loss = -12408.214993998099
Iteration 5900: Loss = -12408.214862831983
Iteration 6000: Loss = -12408.214814218434
Iteration 6100: Loss = -12408.21460401969
Iteration 6200: Loss = -12408.214605862779
Iteration 6300: Loss = -12408.214349653146
Iteration 6400: Loss = -12408.21425195545
Iteration 6500: Loss = -12408.214157500906
Iteration 6600: Loss = -12408.214043354435
Iteration 6700: Loss = -12408.213994316055
Iteration 6800: Loss = -12408.213868466883
Iteration 6900: Loss = -12408.214620425226
1
Iteration 7000: Loss = -12408.213865646278
Iteration 7100: Loss = -12408.21921374495
1
Iteration 7200: Loss = -12408.219578268265
2
Iteration 7300: Loss = -12408.288197884685
3
Iteration 7400: Loss = -12408.213465419887
Iteration 7500: Loss = -12408.214173564249
1
Iteration 7600: Loss = -12408.213354538086
Iteration 7700: Loss = -12408.296947495533
1
Iteration 7800: Loss = -12408.213305068908
Iteration 7900: Loss = -12408.213250565144
Iteration 8000: Loss = -12408.21345651306
1
Iteration 8100: Loss = -12408.213143499659
Iteration 8200: Loss = -12408.213152415725
Iteration 8300: Loss = -12408.26620382462
1
Iteration 8400: Loss = -12408.213096514608
Iteration 8500: Loss = -12408.213040697588
Iteration 8600: Loss = -12408.234971463855
1
Iteration 8700: Loss = -12408.213029823064
Iteration 8800: Loss = -12408.212966813511
Iteration 8900: Loss = -12408.212963865002
Iteration 9000: Loss = -12408.305030908354
1
Iteration 9100: Loss = -12408.212888763195
Iteration 9200: Loss = -12408.212872567807
Iteration 9300: Loss = -12408.21286781003
Iteration 9400: Loss = -12408.213007069204
1
Iteration 9500: Loss = -12408.212803412773
Iteration 9600: Loss = -12408.212805469473
Iteration 9700: Loss = -12408.21998531506
1
Iteration 9800: Loss = -12408.212758067744
Iteration 9900: Loss = -12408.212777792962
Iteration 10000: Loss = -12408.230590162513
1
Iteration 10100: Loss = -12408.212733536486
Iteration 10200: Loss = -12408.212701761975
Iteration 10300: Loss = -12408.212720580417
Iteration 10400: Loss = -12408.218928703815
1
Iteration 10500: Loss = -12408.212746134062
Iteration 10600: Loss = -12408.212662162521
Iteration 10700: Loss = -12408.215147666602
1
Iteration 10800: Loss = -12408.212657094771
Iteration 10900: Loss = -12408.212664145341
Iteration 11000: Loss = -12408.661803403344
1
Iteration 11100: Loss = -12408.212635723625
Iteration 11200: Loss = -12408.212635531163
Iteration 11300: Loss = -12408.541350458347
1
Iteration 11400: Loss = -12408.212594369279
Iteration 11500: Loss = -12408.212588564797
Iteration 11600: Loss = -12408.416355967056
1
Iteration 11700: Loss = -12408.212627365461
Iteration 11800: Loss = -12408.212608937925
Iteration 11900: Loss = -12408.214136686822
1
Iteration 12000: Loss = -12408.212600948833
Iteration 12100: Loss = -12408.212606569426
Iteration 12200: Loss = -12408.212604398399
Iteration 12300: Loss = -12408.212797024624
1
Iteration 12400: Loss = -12408.212557402503
Iteration 12500: Loss = -12408.212584099458
Iteration 12600: Loss = -12408.25448962371
1
Iteration 12700: Loss = -12408.212562024777
Iteration 12800: Loss = -12408.212578217217
Iteration 12900: Loss = -12408.212583208508
Iteration 13000: Loss = -12408.212661992242
Iteration 13100: Loss = -12408.212559291083
Iteration 13200: Loss = -12408.2616719825
1
Iteration 13300: Loss = -12408.216684219624
2
Iteration 13400: Loss = -12408.213405610357
3
Iteration 13500: Loss = -12408.213109864342
4
Iteration 13600: Loss = -12408.219356185207
5
Iteration 13700: Loss = -12408.212561543445
Iteration 13800: Loss = -12408.231327181282
1
Iteration 13900: Loss = -12408.212536561747
Iteration 14000: Loss = -12408.212544227388
Iteration 14100: Loss = -12408.212669603026
1
Iteration 14200: Loss = -12408.220383429432
2
Iteration 14300: Loss = -12408.214618504195
3
Iteration 14400: Loss = -12408.238734530982
4
Iteration 14500: Loss = -12408.212530951665
Iteration 14600: Loss = -12408.240579416988
1
Iteration 14700: Loss = -12408.212524110568
Iteration 14800: Loss = -12408.574523482393
1
Iteration 14900: Loss = -12408.212519730794
Iteration 15000: Loss = -12408.212511572572
Iteration 15100: Loss = -12408.212554970067
Iteration 15200: Loss = -12408.212483753863
Iteration 15300: Loss = -12408.226127799015
1
Iteration 15400: Loss = -12408.212505157551
Iteration 15500: Loss = -12408.212501866505
Iteration 15600: Loss = -12408.214221927461
1
Iteration 15700: Loss = -12408.212520637859
Iteration 15800: Loss = -12408.215113712764
1
Iteration 15900: Loss = -12408.212517353253
Iteration 16000: Loss = -12408.21251915575
Iteration 16100: Loss = -12408.34480323075
1
Iteration 16200: Loss = -12408.212502723143
Iteration 16300: Loss = -12408.212496392844
Iteration 16400: Loss = -12408.237919509114
1
Iteration 16500: Loss = -12408.212510571902
Iteration 16600: Loss = -12408.212478512474
Iteration 16700: Loss = -12408.82622092428
1
Iteration 16800: Loss = -12408.21248293177
Iteration 16900: Loss = -12408.21250317756
Iteration 17000: Loss = -12408.27901485075
1
Iteration 17100: Loss = -12408.212514747063
Iteration 17200: Loss = -12408.219348114648
1
Iteration 17300: Loss = -12408.212496778773
Iteration 17400: Loss = -12408.212640060172
1
Iteration 17500: Loss = -12408.212580873165
Iteration 17600: Loss = -12408.212525216084
Iteration 17700: Loss = -12408.222112346744
1
Iteration 17800: Loss = -12408.21247049259
Iteration 17900: Loss = -12408.212608244594
1
Iteration 18000: Loss = -12408.216131041712
2
Iteration 18100: Loss = -12408.212489587298
Iteration 18200: Loss = -12408.212739164075
1
Iteration 18300: Loss = -12408.217073144597
2
Iteration 18400: Loss = -12408.21254648979
Iteration 18500: Loss = -12408.213896162
1
Iteration 18600: Loss = -12408.212497392236
Iteration 18700: Loss = -12408.21248197457
Iteration 18800: Loss = -12408.216437615549
1
Iteration 18900: Loss = -12408.212517162066
Iteration 19000: Loss = -12408.212504090263
Iteration 19100: Loss = -12408.212756173996
1
Iteration 19200: Loss = -12408.212514781455
Iteration 19300: Loss = -12408.212473044525
Iteration 19400: Loss = -12408.212733406235
1
Iteration 19500: Loss = -12408.212476163102
Iteration 19600: Loss = -12408.22329531807
1
Iteration 19700: Loss = -12408.212495266476
Iteration 19800: Loss = -12408.212488506844
Iteration 19900: Loss = -12408.213051548018
1
pi: tensor([[1.0000e+00, 8.5482e-09],
        [2.9553e-01, 7.0447e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9638, 0.0362], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2021, 0.1464],
         [0.5217, 0.0864]],

        [[0.6965, 0.1269],
         [0.6869, 0.6451]],

        [[0.5830, 0.2035],
         [0.5511, 0.6057]],

        [[0.5864, 0.2018],
         [0.7290, 0.6446]],

        [[0.6043, 0.2835],
         [0.6951, 0.5966]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.00013024830091843
Average Adjusted Rand Index: -0.0033573033817044956
11909.81335217151
[0.9919999691484065, 0.00013024830091843] [0.992, -0.0033573033817044956] [11902.197258785853, 12408.212475483344]
-------------------------------------
This iteration is 55
True Objective function: Loss = -11941.55944592151
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20143.606717077226
Iteration 100: Loss = -12506.187787122324
Iteration 200: Loss = -12506.049909322113
Iteration 300: Loss = -12506.010085299735
Iteration 400: Loss = -12505.992396120946
Iteration 500: Loss = -12505.980640677104
Iteration 600: Loss = -12505.969506112075
Iteration 700: Loss = -12505.957311085844
Iteration 800: Loss = -12505.942378159563
Iteration 900: Loss = -12505.921751466805
Iteration 1000: Loss = -12505.888518002796
Iteration 1100: Loss = -12505.82067179253
Iteration 1200: Loss = -12505.635615262147
Iteration 1300: Loss = -12505.365511738291
Iteration 1400: Loss = -12505.267407776193
Iteration 1500: Loss = -12505.204229656063
Iteration 1600: Loss = -12505.161673944858
Iteration 1700: Loss = -12505.13411014227
Iteration 1800: Loss = -12505.117216199127
Iteration 1900: Loss = -12505.107443287601
Iteration 2000: Loss = -12505.101803764639
Iteration 2100: Loss = -12505.098609604762
Iteration 2200: Loss = -12505.096531104628
Iteration 2300: Loss = -12505.095204716508
Iteration 2400: Loss = -12505.09418207133
Iteration 2500: Loss = -12505.093352242448
Iteration 2600: Loss = -12505.09274729736
Iteration 2700: Loss = -12505.092232451858
Iteration 2800: Loss = -12505.091731949997
Iteration 2900: Loss = -12505.091359277121
Iteration 3000: Loss = -12505.09098633787
Iteration 3100: Loss = -12505.090708023956
Iteration 3200: Loss = -12505.090420222015
Iteration 3300: Loss = -12505.090157902254
Iteration 3400: Loss = -12505.089933814785
Iteration 3500: Loss = -12505.08969353761
Iteration 3600: Loss = -12505.089491003308
Iteration 3700: Loss = -12505.089297728451
Iteration 3800: Loss = -12505.089126562178
Iteration 3900: Loss = -12505.088940154956
Iteration 4000: Loss = -12505.088752913312
Iteration 4100: Loss = -12505.088545656186
Iteration 4200: Loss = -12505.088379614626
Iteration 4300: Loss = -12505.088182211588
Iteration 4400: Loss = -12505.08800265585
Iteration 4500: Loss = -12505.087772797931
Iteration 4600: Loss = -12505.087531330577
Iteration 4700: Loss = -12505.087327242753
Iteration 4800: Loss = -12505.087056032025
Iteration 4900: Loss = -12505.08668117319
Iteration 5000: Loss = -12505.08617391745
Iteration 5100: Loss = -12505.085416205553
Iteration 5200: Loss = -12505.08416856805
Iteration 5300: Loss = -12505.081785858043
Iteration 5400: Loss = -12505.076972426596
Iteration 5500: Loss = -12505.069703497598
Iteration 5600: Loss = -12505.062761886487
Iteration 5700: Loss = -12505.058829856864
Iteration 5800: Loss = -12505.056859744274
Iteration 5900: Loss = -12505.055863093654
Iteration 6000: Loss = -12505.055592004252
Iteration 6100: Loss = -12505.054919934955
Iteration 6200: Loss = -12505.05467585624
Iteration 6300: Loss = -12505.054457210534
Iteration 6400: Loss = -12505.054316747519
Iteration 6500: Loss = -12505.0601880232
1
Iteration 6600: Loss = -12505.092781401456
2
Iteration 6700: Loss = -12505.054014145415
Iteration 6800: Loss = -12505.424746121269
1
Iteration 6900: Loss = -12505.053834946704
Iteration 7000: Loss = -12505.053715969636
Iteration 7100: Loss = -12505.060715220287
1
Iteration 7200: Loss = -12505.053587613185
Iteration 7300: Loss = -12505.053513326096
Iteration 7400: Loss = -12505.116677871127
1
Iteration 7500: Loss = -12505.053394303774
Iteration 7600: Loss = -12505.05334254517
Iteration 7700: Loss = -12505.053440963666
Iteration 7800: Loss = -12505.05327541507
Iteration 7900: Loss = -12505.053109620652
Iteration 8000: Loss = -12505.05312933017
Iteration 8100: Loss = -12505.067656609697
1
Iteration 8200: Loss = -12505.052995732582
Iteration 8300: Loss = -12505.053000588692
Iteration 8400: Loss = -12505.169738914805
1
Iteration 8500: Loss = -12505.052890187264
Iteration 8600: Loss = -12505.052828654985
Iteration 8700: Loss = -12505.05394087041
1
Iteration 8800: Loss = -12505.052777498346
Iteration 8900: Loss = -12505.052682935022
Iteration 9000: Loss = -12505.05267358646
Iteration 9100: Loss = -12505.071786534454
1
Iteration 9200: Loss = -12505.052607254085
Iteration 9300: Loss = -12505.052602430856
Iteration 9400: Loss = -12505.07659173522
1
Iteration 9500: Loss = -12505.052515195086
Iteration 9600: Loss = -12505.052506669059
Iteration 9700: Loss = -12505.053770017756
1
Iteration 9800: Loss = -12505.05251600969
Iteration 9900: Loss = -12505.052432912715
Iteration 10000: Loss = -12505.05234971729
Iteration 10100: Loss = -12505.052596009251
1
Iteration 10200: Loss = -12505.052346555685
Iteration 10300: Loss = -12505.05234397819
Iteration 10400: Loss = -12505.129447537027
1
Iteration 10500: Loss = -12505.052296632099
Iteration 10600: Loss = -12505.052268632944
Iteration 10700: Loss = -12505.052259705782
Iteration 10800: Loss = -12505.052370563675
1
Iteration 10900: Loss = -12505.05222918503
Iteration 11000: Loss = -12505.05221884557
Iteration 11100: Loss = -12505.05218578066
Iteration 11200: Loss = -12505.052815024865
1
Iteration 11300: Loss = -12505.052175475956
Iteration 11400: Loss = -12505.052167162596
Iteration 11500: Loss = -12505.052357205865
1
Iteration 11600: Loss = -12505.052114287177
Iteration 11700: Loss = -12505.052127047522
Iteration 11800: Loss = -12505.058091893212
1
Iteration 11900: Loss = -12505.052117589563
Iteration 12000: Loss = -12505.052164096489
Iteration 12100: Loss = -12505.071649499836
1
Iteration 12200: Loss = -12505.052118808224
Iteration 12300: Loss = -12505.116530651208
1
Iteration 12400: Loss = -12505.05206653828
Iteration 12500: Loss = -12505.053408138763
1
Iteration 12600: Loss = -12505.052047407375
Iteration 12700: Loss = -12505.052982786732
1
Iteration 12800: Loss = -12505.052866038937
2
Iteration 12900: Loss = -12505.054387040795
3
Iteration 13000: Loss = -12505.052110491337
Iteration 13100: Loss = -12505.052201171446
Iteration 13200: Loss = -12505.05957996582
1
Iteration 13300: Loss = -12505.052039476868
Iteration 13400: Loss = -12505.056325545886
1
Iteration 13500: Loss = -12505.052013910661
Iteration 13600: Loss = -12505.05317350522
1
Iteration 13700: Loss = -12505.052026711995
Iteration 13800: Loss = -12505.056802865602
1
Iteration 13900: Loss = -12505.052570805568
2
Iteration 14000: Loss = -12505.053003629397
3
Iteration 14100: Loss = -12505.052137523104
4
Iteration 14200: Loss = -12505.086043246722
5
Iteration 14300: Loss = -12505.052066511054
Iteration 14400: Loss = -12505.500099945988
1
Iteration 14500: Loss = -12505.052018526972
Iteration 14600: Loss = -12505.051915247004
Iteration 14700: Loss = -12505.052688173277
1
Iteration 14800: Loss = -12505.051989273426
Iteration 14900: Loss = -12505.310010991961
1
Iteration 15000: Loss = -12505.051977617943
Iteration 15100: Loss = -12505.160127786105
1
Iteration 15200: Loss = -12505.051937823077
Iteration 15300: Loss = -12505.052037003732
Iteration 15400: Loss = -12505.052197665613
1
Iteration 15500: Loss = -12505.300304172424
2
Iteration 15600: Loss = -12505.051948495422
Iteration 15700: Loss = -12505.05194840462
Iteration 15800: Loss = -12505.052144471585
1
Iteration 15900: Loss = -12505.051936288623
Iteration 16000: Loss = -12505.054068360309
1
Iteration 16100: Loss = -12505.05195452821
Iteration 16200: Loss = -12505.07289611362
1
Iteration 16300: Loss = -12505.051941391588
Iteration 16400: Loss = -12505.052022658696
Iteration 16500: Loss = -12505.051942475056
Iteration 16600: Loss = -12505.052628068472
1
Iteration 16700: Loss = -12505.062802279997
2
Iteration 16800: Loss = -12505.051977721852
Iteration 16900: Loss = -12505.051969462123
Iteration 17000: Loss = -12505.05292496501
1
Iteration 17100: Loss = -12505.05248334163
2
Iteration 17200: Loss = -12505.051984246053
Iteration 17300: Loss = -12505.052005442838
Iteration 17400: Loss = -12505.051950713641
Iteration 17500: Loss = -12505.053140282615
1
Iteration 17600: Loss = -12505.051939087518
Iteration 17700: Loss = -12505.060728399501
1
Iteration 17800: Loss = -12505.051940762582
Iteration 17900: Loss = -12505.074077051879
1
Iteration 18000: Loss = -12505.051970412542
Iteration 18100: Loss = -12505.06354506031
1
Iteration 18200: Loss = -12505.05197396152
Iteration 18300: Loss = -12505.05192760318
Iteration 18400: Loss = -12505.055132737878
1
Iteration 18500: Loss = -12505.051914605912
Iteration 18600: Loss = -12505.061762590341
1
Iteration 18700: Loss = -12505.051938289818
Iteration 18800: Loss = -12505.17326417075
1
Iteration 18900: Loss = -12505.05213735117
2
Iteration 19000: Loss = -12505.207457833987
3
Iteration 19100: Loss = -12505.052262003579
4
Iteration 19200: Loss = -12505.051959399905
Iteration 19300: Loss = -12505.052745153718
1
Iteration 19400: Loss = -12505.05228641933
2
Iteration 19500: Loss = -12505.05212069572
3
Iteration 19600: Loss = -12505.051925624113
Iteration 19700: Loss = -12505.055112072909
1
Iteration 19800: Loss = -12505.05192680768
Iteration 19900: Loss = -12505.053025660698
1
pi: tensor([[1.5150e-05, 9.9998e-01],
        [3.2749e-03, 9.9673e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0698, 0.9302], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1284, 0.1519],
         [0.6911, 0.2051]],

        [[0.6310, 0.2008],
         [0.5736, 0.6939]],

        [[0.5692, 0.2083],
         [0.6081, 0.5627]],

        [[0.5431, 0.2225],
         [0.6676, 0.5073]],

        [[0.6866, 0.1043],
         [0.5012, 0.5914]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -6.349304693531479e-05
Average Adjusted Rand Index: -0.0005926784880256398
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21017.737451082638
Iteration 100: Loss = -12506.93633122321
Iteration 200: Loss = -12506.311697851144
Iteration 300: Loss = -12506.185934667476
Iteration 400: Loss = -12506.11869954426
Iteration 500: Loss = -12506.071556189536
Iteration 600: Loss = -12506.033582372069
Iteration 700: Loss = -12505.99998675484
Iteration 800: Loss = -12505.967027359276
Iteration 900: Loss = -12505.930331728563
Iteration 1000: Loss = -12505.883211175493
Iteration 1100: Loss = -12505.81542335548
Iteration 1200: Loss = -12505.723124351682
Iteration 1300: Loss = -12505.632005883177
Iteration 1400: Loss = -12505.555578491558
Iteration 1500: Loss = -12505.489433067061
Iteration 1600: Loss = -12505.430489432927
Iteration 1700: Loss = -12505.376922048827
Iteration 1800: Loss = -12505.327906896511
Iteration 1900: Loss = -12505.283337405437
Iteration 2000: Loss = -12505.243794672991
Iteration 2100: Loss = -12505.209694419862
Iteration 2200: Loss = -12505.181444711563
Iteration 2300: Loss = -12505.159012223792
Iteration 2400: Loss = -12505.14196228364
Iteration 2500: Loss = -12505.129318366025
Iteration 2600: Loss = -12505.120136646723
Iteration 2700: Loss = -12505.113456316541
Iteration 2800: Loss = -12505.108631197392
Iteration 2900: Loss = -12505.10520509155
Iteration 3000: Loss = -12505.102573069371
Iteration 3100: Loss = -12505.100644678712
Iteration 3200: Loss = -12505.099144803768
Iteration 3300: Loss = -12505.09792125593
Iteration 3400: Loss = -12505.096941210537
Iteration 3500: Loss = -12505.096084459614
Iteration 3600: Loss = -12505.095375594223
Iteration 3700: Loss = -12505.09471463923
Iteration 3800: Loss = -12505.094209808936
Iteration 3900: Loss = -12505.093681011398
Iteration 4000: Loss = -12505.093267100207
Iteration 4100: Loss = -12505.092751543694
Iteration 4200: Loss = -12505.092411454252
Iteration 4300: Loss = -12505.091985736672
Iteration 4400: Loss = -12505.091642915864
Iteration 4500: Loss = -12505.091343555745
Iteration 4600: Loss = -12505.091020660077
Iteration 4700: Loss = -12505.090704185233
Iteration 4800: Loss = -12505.09039246719
Iteration 4900: Loss = -12505.090124945835
Iteration 5000: Loss = -12505.089810175814
Iteration 5100: Loss = -12505.089519512658
Iteration 5200: Loss = -12505.089239208304
Iteration 5300: Loss = -12505.088887246484
Iteration 5400: Loss = -12505.088514474839
Iteration 5500: Loss = -12505.088041262865
Iteration 5600: Loss = -12505.087427888273
Iteration 5700: Loss = -12505.086493680104
Iteration 5800: Loss = -12505.084820276044
Iteration 5900: Loss = -12505.081828473962
Iteration 6000: Loss = -12505.075449327263
Iteration 6100: Loss = -12505.06808583937
Iteration 6200: Loss = -12505.062674097655
Iteration 6300: Loss = -12505.059612566767
Iteration 6400: Loss = -12505.0579267596
Iteration 6500: Loss = -12505.056947269257
Iteration 6600: Loss = -12505.056326722108
Iteration 6700: Loss = -12505.056294001255
Iteration 6800: Loss = -12505.05559100811
Iteration 6900: Loss = -12505.062375386642
1
Iteration 7000: Loss = -12505.055248161805
Iteration 7100: Loss = -12505.055079061352
Iteration 7200: Loss = -12505.056808602349
1
Iteration 7300: Loss = -12505.054740404406
Iteration 7400: Loss = -12505.054526334288
Iteration 7500: Loss = -12505.054770626642
1
Iteration 7600: Loss = -12505.054330758245
Iteration 7700: Loss = -12505.140081883525
1
Iteration 7800: Loss = -12505.054157577755
Iteration 7900: Loss = -12505.054105161082
Iteration 8000: Loss = -12505.085212084348
1
Iteration 8100: Loss = -12505.053938210925
Iteration 8200: Loss = -12505.053821562527
Iteration 8300: Loss = -12505.054832449809
1
Iteration 8400: Loss = -12505.053742773418
Iteration 8500: Loss = -12505.053672662229
Iteration 8600: Loss = -12505.053614600529
Iteration 8700: Loss = -12505.05387029905
1
Iteration 8800: Loss = -12505.053523584891
Iteration 8900: Loss = -12505.053463828035
Iteration 9000: Loss = -12505.837583303344
1
Iteration 9100: Loss = -12505.053343333759
Iteration 9200: Loss = -12505.053290983851
Iteration 9300: Loss = -12505.053249920871
Iteration 9400: Loss = -12505.070107763338
1
Iteration 9500: Loss = -12505.053175611876
Iteration 9600: Loss = -12505.053137134275
Iteration 9700: Loss = -12505.078420581194
1
Iteration 9800: Loss = -12505.05303969463
Iteration 9900: Loss = -12505.05300610115
Iteration 10000: Loss = -12505.053976128233
1
Iteration 10100: Loss = -12505.052931026405
Iteration 10200: Loss = -12505.052852922181
Iteration 10300: Loss = -12505.052847839544
Iteration 10400: Loss = -12505.053008490562
1
Iteration 10500: Loss = -12505.05280489593
Iteration 10600: Loss = -12505.052785769038
Iteration 10700: Loss = -12505.05496572539
1
Iteration 10800: Loss = -12505.05271023403
Iteration 10900: Loss = -12505.052718669423
Iteration 11000: Loss = -12505.082364954007
1
Iteration 11100: Loss = -12505.052595013187
Iteration 11200: Loss = -12505.05258444445
Iteration 11300: Loss = -12505.10576912474
1
Iteration 11400: Loss = -12505.052549146529
Iteration 11500: Loss = -12505.052541356452
Iteration 11600: Loss = -12505.056618884848
1
Iteration 11700: Loss = -12505.05249907635
Iteration 11800: Loss = -12505.05244598572
Iteration 11900: Loss = -12505.05658786431
1
Iteration 12000: Loss = -12505.052425026222
Iteration 12100: Loss = -12505.053972115966
1
Iteration 12200: Loss = -12505.052414431435
Iteration 12300: Loss = -12505.052348048275
Iteration 12400: Loss = -12505.064645512268
1
Iteration 12500: Loss = -12505.052359994992
Iteration 12600: Loss = -12505.052352549965
Iteration 12700: Loss = -12505.052312311018
Iteration 12800: Loss = -12505.133518441358
1
Iteration 12900: Loss = -12505.052283084142
Iteration 13000: Loss = -12505.054079831054
1
Iteration 13100: Loss = -12505.101701949803
2
Iteration 13200: Loss = -12505.056677069926
3
Iteration 13300: Loss = -12505.060399339853
4
Iteration 13400: Loss = -12505.05237482013
Iteration 13500: Loss = -12505.05229667151
Iteration 13600: Loss = -12505.05324587264
1
Iteration 13700: Loss = -12505.052592446227
2
Iteration 13800: Loss = -12505.052489361575
3
Iteration 13900: Loss = -12505.05331539825
4
Iteration 14000: Loss = -12505.052212366954
Iteration 14100: Loss = -12505.056537131448
1
Iteration 14200: Loss = -12505.052219376472
Iteration 14300: Loss = -12505.05581063518
1
Iteration 14400: Loss = -12505.052228009126
Iteration 14500: Loss = -12505.124196267994
1
Iteration 14600: Loss = -12505.053841724202
2
Iteration 14700: Loss = -12505.177157706134
3
Iteration 14800: Loss = -12505.052124278664
Iteration 14900: Loss = -12505.137890849726
1
Iteration 15000: Loss = -12505.052853860907
2
Iteration 15100: Loss = -12505.052151262582
Iteration 15200: Loss = -12505.052759996952
1
Iteration 15300: Loss = -12505.052301342588
2
Iteration 15400: Loss = -12505.070494807112
3
Iteration 15500: Loss = -12505.052408993037
4
Iteration 15600: Loss = -12505.052731693871
5
Iteration 15700: Loss = -12505.057543624504
6
Iteration 15800: Loss = -12505.052127999952
Iteration 15900: Loss = -12505.052637821504
1
Iteration 16000: Loss = -12505.052166287547
Iteration 16100: Loss = -12505.052129656335
Iteration 16200: Loss = -12505.055255371324
1
Iteration 16300: Loss = -12505.052119385024
Iteration 16400: Loss = -12505.052854923344
1
Iteration 16500: Loss = -12505.056986817452
2
Iteration 16600: Loss = -12505.067097570669
3
Iteration 16700: Loss = -12505.05227744531
4
Iteration 16800: Loss = -12505.061487185285
5
Iteration 16900: Loss = -12505.052087355038
Iteration 17000: Loss = -12505.065810593613
1
Iteration 17100: Loss = -12505.052101513254
Iteration 17200: Loss = -12505.052891974245
1
Iteration 17300: Loss = -12505.052367810233
2
Iteration 17400: Loss = -12505.05211098975
Iteration 17500: Loss = -12505.079179341416
1
Iteration 17600: Loss = -12505.05209281046
Iteration 17700: Loss = -12505.052264521533
1
Iteration 17800: Loss = -12505.05464508617
2
Iteration 17900: Loss = -12505.052122182007
Iteration 18000: Loss = -12505.056481350046
1
Iteration 18100: Loss = -12505.052089959947
Iteration 18200: Loss = -12505.179925843768
1
Iteration 18300: Loss = -12505.052131857023
Iteration 18400: Loss = -12505.323250474277
1
Iteration 18500: Loss = -12505.05236518657
2
Iteration 18600: Loss = -12505.091887921937
3
Iteration 18700: Loss = -12505.056522946701
4
Iteration 18800: Loss = -12505.100401283391
5
Iteration 18900: Loss = -12505.054285261755
6
Iteration 19000: Loss = -12505.059877125688
7
Iteration 19100: Loss = -12505.0527074369
8
Iteration 19200: Loss = -12505.060906041223
9
Iteration 19300: Loss = -12505.052125345705
Iteration 19400: Loss = -12505.055949264291
1
Iteration 19500: Loss = -12505.24027908369
2
Iteration 19600: Loss = -12505.052057816636
Iteration 19700: Loss = -12505.05217542819
1
Iteration 19800: Loss = -12505.054745163923
2
Iteration 19900: Loss = -12505.079569406897
3
pi: tensor([[3.6798e-05, 9.9996e-01],
        [3.4417e-03, 9.9656e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0701, 0.9299], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1279, 0.1515],
         [0.6547, 0.2052]],

        [[0.6378, 0.2002],
         [0.6670, 0.6900]],

        [[0.5101, 0.2076],
         [0.5333, 0.6313]],

        [[0.5898, 0.1768],
         [0.5208, 0.7200]],

        [[0.5112, 0.1046],
         [0.6103, 0.5278]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -6.349304693531479e-05
Average Adjusted Rand Index: -0.0005926784880256398
11941.55944592151
[-6.349304693531479e-05, -6.349304693531479e-05] [-0.0005926784880256398, -0.0005926784880256398] [12505.052307349139, 12505.052098774697]
-------------------------------------
This iteration is 56
True Objective function: Loss = -11963.68053967151
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21443.42565899219
Iteration 100: Loss = -12465.471517928752
Iteration 200: Loss = -12465.11384898803
Iteration 300: Loss = -12465.030819298885
Iteration 400: Loss = -12464.985050559031
Iteration 500: Loss = -12464.941312034654
Iteration 600: Loss = -12464.870678226822
Iteration 700: Loss = -12464.70522692243
Iteration 800: Loss = -12464.50434922863
Iteration 900: Loss = -12464.35782513883
Iteration 1000: Loss = -12464.2196176526
Iteration 1100: Loss = -12464.094894208773
Iteration 1200: Loss = -12464.010834109451
Iteration 1300: Loss = -12463.96417680155
Iteration 1400: Loss = -12463.937119017659
Iteration 1500: Loss = -12463.919568872889
Iteration 1600: Loss = -12463.90788715466
Iteration 1700: Loss = -12463.900541134524
Iteration 1800: Loss = -12463.896120813504
Iteration 1900: Loss = -12463.893506168248
Iteration 2000: Loss = -12463.89203680855
Iteration 2100: Loss = -12463.891027728583
Iteration 2200: Loss = -12463.890257419709
Iteration 2300: Loss = -12463.889611799119
Iteration 2400: Loss = -12463.889034416354
Iteration 2500: Loss = -12463.888415719215
Iteration 2600: Loss = -12463.887839255554
Iteration 2700: Loss = -12463.887164308539
Iteration 2800: Loss = -12463.886461274951
Iteration 2900: Loss = -12463.885677130967
Iteration 3000: Loss = -12463.884823753964
Iteration 3100: Loss = -12463.883735134605
Iteration 3200: Loss = -12463.882168971722
Iteration 3300: Loss = -12463.87902224323
Iteration 3400: Loss = -12463.865906341076
Iteration 3500: Loss = -12463.713419521524
Iteration 3600: Loss = -12463.540360603161
Iteration 3700: Loss = -12463.460213826156
Iteration 3800: Loss = -12463.430606690836
Iteration 3900: Loss = -12463.417828967533
Iteration 4000: Loss = -12463.411223863386
Iteration 4100: Loss = -12463.407316298035
Iteration 4200: Loss = -12463.404718059155
Iteration 4300: Loss = -12463.402817604489
Iteration 4400: Loss = -12463.401441160157
Iteration 4500: Loss = -12463.40045065713
Iteration 4600: Loss = -12463.39931815312
Iteration 4700: Loss = -12463.398548931695
Iteration 4800: Loss = -12463.397808434685
Iteration 4900: Loss = -12463.397240390099
Iteration 5000: Loss = -12463.396741449455
Iteration 5100: Loss = -12463.396180937883
Iteration 5200: Loss = -12463.405998528146
1
Iteration 5300: Loss = -12463.395402544626
Iteration 5400: Loss = -12463.39504528641
Iteration 5500: Loss = -12463.394694379918
Iteration 5600: Loss = -12463.394384385241
Iteration 5700: Loss = -12463.394256597408
Iteration 5800: Loss = -12463.39385350292
Iteration 5900: Loss = -12463.393638203783
Iteration 6000: Loss = -12463.393393923021
Iteration 6100: Loss = -12463.393155984024
Iteration 6200: Loss = -12463.39300602588
Iteration 6300: Loss = -12463.392812719314
Iteration 6400: Loss = -12463.392691382796
Iteration 6500: Loss = -12463.392482888232
Iteration 6600: Loss = -12463.392786938977
1
Iteration 6700: Loss = -12463.392201901399
Iteration 6800: Loss = -12463.39264713962
1
Iteration 6900: Loss = -12463.391921476103
Iteration 7000: Loss = -12463.391834930251
Iteration 7100: Loss = -12463.396071090512
1
Iteration 7200: Loss = -12463.530068188877
2
Iteration 7300: Loss = -12463.391597868795
Iteration 7400: Loss = -12463.391426827005
Iteration 7500: Loss = -12463.424681608794
1
Iteration 7600: Loss = -12463.391280603724
Iteration 7700: Loss = -12463.834015249962
1
Iteration 7800: Loss = -12463.391151643757
Iteration 7900: Loss = -12463.391063566203
Iteration 8000: Loss = -12463.396455150658
1
Iteration 8100: Loss = -12463.390933413792
Iteration 8200: Loss = -12463.390895623523
Iteration 8300: Loss = -12463.397653304597
1
Iteration 8400: Loss = -12463.390763744748
Iteration 8500: Loss = -12463.390748457166
Iteration 8600: Loss = -12463.391069364421
1
Iteration 8700: Loss = -12463.390653787363
Iteration 8800: Loss = -12463.390609634827
Iteration 8900: Loss = -12463.390856478518
1
Iteration 9000: Loss = -12463.390527655232
Iteration 9100: Loss = -12463.390522979304
Iteration 9200: Loss = -12463.390661876932
1
Iteration 9300: Loss = -12463.390458452586
Iteration 9400: Loss = -12463.390419702053
Iteration 9500: Loss = -12463.390687146597
1
Iteration 9600: Loss = -12463.390376248157
Iteration 9700: Loss = -12463.3903125074
Iteration 9800: Loss = -12463.390346836783
Iteration 9900: Loss = -12463.390288755076
Iteration 10000: Loss = -12463.39026078998
Iteration 10100: Loss = -12463.390960349934
1
Iteration 10200: Loss = -12463.390239595883
Iteration 10300: Loss = -12463.395798321526
1
Iteration 10400: Loss = -12463.390227253265
Iteration 10500: Loss = -12463.390175573277
Iteration 10600: Loss = -12463.40083316643
1
Iteration 10700: Loss = -12463.396522912504
2
Iteration 10800: Loss = -12463.390173165923
Iteration 10900: Loss = -12463.390421677084
1
Iteration 11000: Loss = -12463.390101657174
Iteration 11100: Loss = -12463.390510125551
1
Iteration 11200: Loss = -12463.390088657557
Iteration 11300: Loss = -12463.403967086197
1
Iteration 11400: Loss = -12463.390146146201
Iteration 11500: Loss = -12463.390074000015
Iteration 11600: Loss = -12463.39053124645
1
Iteration 11700: Loss = -12463.390106846988
Iteration 11800: Loss = -12463.72584356224
1
Iteration 11900: Loss = -12463.390059302457
Iteration 12000: Loss = -12463.396972723367
1
Iteration 12100: Loss = -12463.390062183516
Iteration 12200: Loss = -12463.391038930256
1
Iteration 12300: Loss = -12463.54990523082
2
Iteration 12400: Loss = -12463.390025365357
Iteration 12500: Loss = -12463.39015992117
1
Iteration 12600: Loss = -12463.389961620866
Iteration 12700: Loss = -12463.39064339784
1
Iteration 12800: Loss = -12463.38998401088
Iteration 12900: Loss = -12463.390072461068
Iteration 13000: Loss = -12463.392511920449
1
Iteration 13100: Loss = -12463.390403014011
2
Iteration 13200: Loss = -12463.390192729972
3
Iteration 13300: Loss = -12463.390187924553
4
Iteration 13400: Loss = -12463.59026530274
5
Iteration 13500: Loss = -12463.398176527455
6
Iteration 13600: Loss = -12463.39891122472
7
Iteration 13700: Loss = -12463.392414234448
8
Iteration 13800: Loss = -12463.391503730534
9
Iteration 13900: Loss = -12463.435639829208
10
Iteration 14000: Loss = -12463.390020958233
Iteration 14100: Loss = -12463.390857984508
1
Iteration 14200: Loss = -12463.506623601397
2
Iteration 14300: Loss = -12463.45213786787
3
Iteration 14400: Loss = -12463.39467402785
4
Iteration 14500: Loss = -12463.399888931961
5
Iteration 14600: Loss = -12463.392730608764
6
Iteration 14700: Loss = -12463.4138247981
7
Iteration 14800: Loss = -12463.389970797714
Iteration 14900: Loss = -12463.389907898732
Iteration 15000: Loss = -12463.39049049377
1
Iteration 15100: Loss = -12463.390028389958
2
Iteration 15200: Loss = -12463.389944698478
Iteration 15300: Loss = -12463.389958420543
Iteration 15400: Loss = -12463.391136027658
1
Iteration 15500: Loss = -12463.389899601661
Iteration 15600: Loss = -12463.389985310192
Iteration 15700: Loss = -12463.389946737412
Iteration 15800: Loss = -12463.39146743669
1
Iteration 15900: Loss = -12463.389924973566
Iteration 16000: Loss = -12463.406050852782
1
Iteration 16100: Loss = -12463.389912243738
Iteration 16200: Loss = -12463.501828074197
1
Iteration 16300: Loss = -12463.389894997994
Iteration 16400: Loss = -12463.390323821872
1
Iteration 16500: Loss = -12463.428968605047
2
Iteration 16600: Loss = -12463.389907330153
Iteration 16700: Loss = -12463.393846207906
1
Iteration 16800: Loss = -12463.391385760597
2
Iteration 16900: Loss = -12463.390209991116
3
Iteration 17000: Loss = -12463.446722447856
4
Iteration 17100: Loss = -12463.390029853186
5
Iteration 17200: Loss = -12463.39694927029
6
Iteration 17300: Loss = -12463.393411654688
7
Iteration 17400: Loss = -12463.430246902013
8
Iteration 17500: Loss = -12463.412190445568
9
Iteration 17600: Loss = -12463.391894905933
10
Iteration 17700: Loss = -12463.390373364064
11
Iteration 17800: Loss = -12463.390212727525
12
Iteration 17900: Loss = -12463.392369303625
13
Iteration 18000: Loss = -12463.425219332166
14
Iteration 18100: Loss = -12463.4131818875
15
Stopping early at iteration 18100 due to no improvement.
pi: tensor([[9.4071e-01, 5.9290e-02],
        [9.9999e-01, 1.2341e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9905, 0.0095], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1992, 0.3254],
         [0.5767, 0.2618]],

        [[0.7111, 0.2318],
         [0.5729, 0.6816]],

        [[0.6278, 0.2171],
         [0.5412, 0.6811]],

        [[0.6938, 0.1991],
         [0.5848, 0.6846]],

        [[0.5890, 0.2641],
         [0.6322, 0.7019]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
Global Adjusted Rand Index: -0.001207690075897785
Average Adjusted Rand Index: -0.0016954205743742834
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21600.176906434277
Iteration 100: Loss = -12465.04778771629
Iteration 200: Loss = -12464.770294348113
Iteration 300: Loss = -12464.626787474876
Iteration 400: Loss = -12464.460713158735
Iteration 500: Loss = -12464.284605976325
Iteration 600: Loss = -12464.114389032891
Iteration 700: Loss = -12464.015470118902
Iteration 800: Loss = -12463.97008729047
Iteration 900: Loss = -12463.945666503758
Iteration 1000: Loss = -12463.9294746609
Iteration 1100: Loss = -12463.917672299829
Iteration 1200: Loss = -12463.909117137317
Iteration 1300: Loss = -12463.903007306664
Iteration 1400: Loss = -12463.898747219691
Iteration 1500: Loss = -12463.895771224088
Iteration 1600: Loss = -12463.89365955732
Iteration 1700: Loss = -12463.892011606527
Iteration 1800: Loss = -12463.890644548921
Iteration 1900: Loss = -12463.889311008896
Iteration 2000: Loss = -12463.88793132471
Iteration 2100: Loss = -12463.886310553477
Iteration 2200: Loss = -12463.884085774778
Iteration 2300: Loss = -12463.87972846184
Iteration 2400: Loss = -12463.861607773375
Iteration 2500: Loss = -12463.72913818437
Iteration 2600: Loss = -12463.58543275883
Iteration 2700: Loss = -12463.493565392291
Iteration 2800: Loss = -12463.45107337819
Iteration 2900: Loss = -12463.431086753682
Iteration 3000: Loss = -12463.420425552482
Iteration 3100: Loss = -12463.414020082491
Iteration 3200: Loss = -12463.409759831424
Iteration 3300: Loss = -12463.406811757177
Iteration 3400: Loss = -12463.4045218215
Iteration 3500: Loss = -12463.402781187322
Iteration 3600: Loss = -12463.401320246443
Iteration 3700: Loss = -12463.400160658746
Iteration 3800: Loss = -12463.399144913104
Iteration 3900: Loss = -12463.398346076443
Iteration 4000: Loss = -12463.397594540344
Iteration 4100: Loss = -12463.396991087773
Iteration 4200: Loss = -12463.396338696552
Iteration 4300: Loss = -12463.395826350228
Iteration 4400: Loss = -12463.395584473456
Iteration 4500: Loss = -12463.394982372052
Iteration 4600: Loss = -12463.394580140764
Iteration 4700: Loss = -12463.394254823146
Iteration 4800: Loss = -12463.393933263264
Iteration 4900: Loss = -12463.393795903316
Iteration 5000: Loss = -12463.393350036278
Iteration 5100: Loss = -12463.393136338032
Iteration 5200: Loss = -12463.39292767296
Iteration 5300: Loss = -12463.392718226722
Iteration 5400: Loss = -12463.39312717026
1
Iteration 5500: Loss = -12463.392364309864
Iteration 5600: Loss = -12463.392248991537
Iteration 5700: Loss = -12463.392045964512
Iteration 5800: Loss = -12463.391914153333
Iteration 5900: Loss = -12463.39189313869
Iteration 6000: Loss = -12463.391659831395
Iteration 6100: Loss = -12463.394245803986
1
Iteration 6200: Loss = -12463.391452862377
Iteration 6300: Loss = -12463.391391249863
Iteration 6400: Loss = -12463.391318568547
Iteration 6500: Loss = -12463.391203361158
Iteration 6600: Loss = -12463.392330252393
1
Iteration 6700: Loss = -12463.391075042218
Iteration 6800: Loss = -12463.39216707696
1
Iteration 6900: Loss = -12463.390912876395
Iteration 7000: Loss = -12463.390974653923
Iteration 7100: Loss = -12463.390840333248
Iteration 7200: Loss = -12463.394137892692
1
Iteration 7300: Loss = -12463.390725024958
Iteration 7400: Loss = -12463.390786080745
Iteration 7500: Loss = -12463.439118204526
1
Iteration 7600: Loss = -12463.39059488199
Iteration 7700: Loss = -12463.40116640128
1
Iteration 7800: Loss = -12463.390500291553
Iteration 7900: Loss = -12463.390453036152
Iteration 8000: Loss = -12463.420167944401
1
Iteration 8100: Loss = -12463.390412188553
Iteration 8200: Loss = -12463.390352651431
Iteration 8300: Loss = -12463.404580385937
1
Iteration 8400: Loss = -12463.390334225265
Iteration 8500: Loss = -12463.390334287375
Iteration 8600: Loss = -12463.390859929115
1
Iteration 8700: Loss = -12463.390285432051
Iteration 8800: Loss = -12463.390207712093
Iteration 8900: Loss = -12463.391365094332
1
Iteration 9000: Loss = -12463.390216923593
Iteration 9100: Loss = -12463.390193077485
Iteration 9200: Loss = -12463.466307781246
1
Iteration 9300: Loss = -12463.390141561913
Iteration 9400: Loss = -12463.390128165429
Iteration 9500: Loss = -12463.391108031341
1
Iteration 9600: Loss = -12463.390135309255
Iteration 9700: Loss = -12463.39009803716
Iteration 9800: Loss = -12463.390215615802
1
Iteration 9900: Loss = -12463.3900613839
Iteration 10000: Loss = -12463.390169080007
1
Iteration 10100: Loss = -12463.39008582635
Iteration 10200: Loss = -12463.390052070998
Iteration 10300: Loss = -12463.40148988954
1
Iteration 10400: Loss = -12463.390090868426
Iteration 10500: Loss = -12463.390040455277
Iteration 10600: Loss = -12463.392726647156
1
Iteration 10700: Loss = -12463.390090102515
Iteration 10800: Loss = -12463.401006926117
1
Iteration 10900: Loss = -12463.390023350275
Iteration 11000: Loss = -12463.39525511979
1
Iteration 11100: Loss = -12463.390037197008
Iteration 11200: Loss = -12463.405093733794
1
Iteration 11300: Loss = -12463.389991372622
Iteration 11400: Loss = -12463.39169491579
1
Iteration 11500: Loss = -12463.402413726344
2
Iteration 11600: Loss = -12463.390934750869
3
Iteration 11700: Loss = -12463.391968309214
4
Iteration 11800: Loss = -12463.39660136528
5
Iteration 11900: Loss = -12463.3903165172
6
Iteration 12000: Loss = -12463.390344678324
7
Iteration 12100: Loss = -12463.39035736892
8
Iteration 12200: Loss = -12463.390017213873
Iteration 12300: Loss = -12463.389953798696
Iteration 12400: Loss = -12463.395564123128
1
Iteration 12500: Loss = -12463.391645719306
2
Iteration 12600: Loss = -12463.396677343735
3
Iteration 12700: Loss = -12463.390187775845
4
Iteration 12800: Loss = -12463.391132267965
5
Iteration 12900: Loss = -12463.405235972337
6
Iteration 13000: Loss = -12463.390019985258
Iteration 13100: Loss = -12463.390673366359
1
Iteration 13200: Loss = -12463.390820707767
2
Iteration 13300: Loss = -12463.392050908456
3
Iteration 13400: Loss = -12463.393498414876
4
Iteration 13500: Loss = -12463.390445957166
5
Iteration 13600: Loss = -12463.394882568471
6
Iteration 13700: Loss = -12463.410340539054
7
Iteration 13800: Loss = -12463.39000638191
Iteration 13900: Loss = -12463.399867567128
1
Iteration 14000: Loss = -12463.409624231927
2
Iteration 14100: Loss = -12463.390219424877
3
Iteration 14200: Loss = -12463.396098161757
4
Iteration 14300: Loss = -12463.409049876955
5
Iteration 14400: Loss = -12463.39461485096
6
Iteration 14500: Loss = -12463.396103049305
7
Iteration 14600: Loss = -12463.397045229081
8
Iteration 14700: Loss = -12463.390930257638
9
Iteration 14800: Loss = -12463.390695420823
10
Iteration 14900: Loss = -12463.389959001359
Iteration 15000: Loss = -12463.39019432238
1
Iteration 15100: Loss = -12463.389969240778
Iteration 15200: Loss = -12463.390685751157
1
Iteration 15300: Loss = -12463.391822770587
2
Iteration 15400: Loss = -12463.390302604446
3
Iteration 15500: Loss = -12463.390064465242
Iteration 15600: Loss = -12463.389974107784
Iteration 15700: Loss = -12463.38992629123
Iteration 15800: Loss = -12463.390040829241
1
Iteration 15900: Loss = -12463.390006080435
Iteration 16000: Loss = -12463.389868076945
Iteration 16100: Loss = -12463.390620399518
1
Iteration 16200: Loss = -12463.389894272259
Iteration 16300: Loss = -12463.628579030548
1
Iteration 16400: Loss = -12463.38988410482
Iteration 16500: Loss = -12463.390059767155
1
Iteration 16600: Loss = -12463.532629628824
2
Iteration 16700: Loss = -12463.390175331317
3
Iteration 16800: Loss = -12463.390974767211
4
Iteration 16900: Loss = -12463.394150454567
5
Iteration 17000: Loss = -12463.39007867777
6
Iteration 17100: Loss = -12463.390152793221
7
Iteration 17200: Loss = -12463.542365648564
8
Iteration 17300: Loss = -12463.389874432512
Iteration 17400: Loss = -12463.402589513842
1
Iteration 17500: Loss = -12463.389912829813
Iteration 17600: Loss = -12463.398483239578
1
Iteration 17700: Loss = -12463.397753699415
2
Iteration 17800: Loss = -12463.45986142966
3
Iteration 17900: Loss = -12463.391696000901
4
Iteration 18000: Loss = -12463.407102571227
5
Iteration 18100: Loss = -12463.391754630824
6
Iteration 18200: Loss = -12463.38989757372
Iteration 18300: Loss = -12463.393102556756
1
Iteration 18400: Loss = -12463.393759377535
2
Iteration 18500: Loss = -12463.403980604093
3
Iteration 18600: Loss = -12463.389973592359
Iteration 18700: Loss = -12463.390337388637
1
Iteration 18800: Loss = -12463.392035308145
2
Iteration 18900: Loss = -12463.445918952606
3
Iteration 19000: Loss = -12463.389914329151
Iteration 19100: Loss = -12463.390553279114
1
Iteration 19200: Loss = -12463.399754270944
2
Iteration 19300: Loss = -12463.405096152352
3
Iteration 19400: Loss = -12463.475743529336
4
Iteration 19500: Loss = -12463.390286964275
5
Iteration 19600: Loss = -12463.462185529035
6
Iteration 19700: Loss = -12463.39459080796
7
Iteration 19800: Loss = -12463.389982345436
Iteration 19900: Loss = -12463.390078409404
pi: tensor([[2.4803e-06, 1.0000e+00],
        [5.9204e-02, 9.4080e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0094, 0.9906], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2602, 0.3267],
         [0.6496, 0.2001]],

        [[0.5691, 0.2297],
         [0.5457, 0.5419]],

        [[0.6285, 0.2153],
         [0.5615, 0.6919]],

        [[0.5801, 0.1977],
         [0.6331, 0.6385]],

        [[0.5378, 0.2619],
         [0.5714, 0.5625]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
Global Adjusted Rand Index: -0.001207690075897785
Average Adjusted Rand Index: -0.0016954205743742834
11963.68053967151
[-0.001207690075897785, -0.001207690075897785] [-0.0016954205743742834, -0.0016954205743742834] [12463.4131818875, 12463.436046202485]
-------------------------------------
This iteration is 57
True Objective function: Loss = -11978.68932873401
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21045.21195934893
Iteration 100: Loss = -12439.366720648977
Iteration 200: Loss = -12438.988911741542
Iteration 300: Loss = -12438.891678408503
Iteration 400: Loss = -12438.817056493273
Iteration 500: Loss = -12438.732150555692
Iteration 600: Loss = -12438.532618828041
Iteration 700: Loss = -12437.619109994765
Iteration 800: Loss = -12436.84005604021
Iteration 900: Loss = -12435.397777135482
Iteration 1000: Loss = -12434.062061070697
Iteration 1100: Loss = -12433.171741933178
Iteration 1200: Loss = -12432.518864897891
Iteration 1300: Loss = -12432.255390847975
Iteration 1400: Loss = -12431.921401575553
Iteration 1500: Loss = -12426.777511559938
Iteration 1600: Loss = -12350.366965390629
Iteration 1700: Loss = -12347.58693707059
Iteration 1800: Loss = -12347.476525939368
Iteration 1900: Loss = -12347.434546851118
Iteration 2000: Loss = -12347.408159013841
Iteration 2100: Loss = -12347.389600661785
Iteration 2200: Loss = -12347.37572509736
Iteration 2300: Loss = -12347.364482712788
Iteration 2400: Loss = -12347.354787050968
Iteration 2500: Loss = -12347.344760675733
Iteration 2600: Loss = -12347.334843942566
Iteration 2700: Loss = -12347.329853998323
Iteration 2800: Loss = -12347.325659197168
Iteration 2900: Loss = -12347.322016699321
Iteration 3000: Loss = -12347.318822067848
Iteration 3100: Loss = -12347.31602631019
Iteration 3200: Loss = -12347.313527776661
Iteration 3300: Loss = -12347.311330558825
Iteration 3400: Loss = -12347.309329036387
Iteration 3500: Loss = -12347.307526277207
Iteration 3600: Loss = -12347.305947689545
Iteration 3700: Loss = -12347.304467206102
Iteration 3800: Loss = -12347.303125766453
Iteration 3900: Loss = -12347.30195212449
Iteration 4000: Loss = -12347.30080566067
Iteration 4100: Loss = -12347.299751775152
Iteration 4200: Loss = -12347.29883733488
Iteration 4300: Loss = -12347.297989621145
Iteration 4400: Loss = -12347.297174703719
Iteration 4500: Loss = -12347.296491101912
Iteration 4600: Loss = -12347.29569987338
Iteration 4700: Loss = -12347.29491397769
Iteration 4800: Loss = -12347.294335632032
Iteration 4900: Loss = -12347.293649759305
Iteration 5000: Loss = -12347.293143306295
Iteration 5100: Loss = -12347.29262587696
Iteration 5200: Loss = -12347.295224013005
1
Iteration 5300: Loss = -12347.291775174717
Iteration 5400: Loss = -12347.291696786648
Iteration 5500: Loss = -12347.29099846042
Iteration 5600: Loss = -12347.290660070932
Iteration 5700: Loss = -12347.290962220379
1
Iteration 5800: Loss = -12347.290043803881
Iteration 5900: Loss = -12347.28991094836
Iteration 6000: Loss = -12347.289507318797
Iteration 6100: Loss = -12347.289738082633
1
Iteration 6200: Loss = -12347.289055538191
Iteration 6300: Loss = -12347.291615412465
1
Iteration 6400: Loss = -12347.288624523566
Iteration 6500: Loss = -12347.289049955476
1
Iteration 6600: Loss = -12347.28830914389
Iteration 6700: Loss = -12347.288125058783
Iteration 6800: Loss = -12347.287936152004
Iteration 6900: Loss = -12347.287877925306
Iteration 7000: Loss = -12347.287631777162
Iteration 7100: Loss = -12347.292137917819
1
Iteration 7200: Loss = -12347.287357595542
Iteration 7300: Loss = -12347.289225798508
1
Iteration 7400: Loss = -12347.287477848944
2
Iteration 7500: Loss = -12347.287197343336
Iteration 7600: Loss = -12347.287879054973
1
Iteration 7700: Loss = -12347.289645553641
2
Iteration 7800: Loss = -12347.286778510383
Iteration 7900: Loss = -12347.29761607694
1
Iteration 8000: Loss = -12347.286537574513
Iteration 8100: Loss = -12347.288005089149
1
Iteration 8200: Loss = -12347.286440828819
Iteration 8300: Loss = -12347.291659071241
1
Iteration 8400: Loss = -12347.286265532432
Iteration 8500: Loss = -12347.360204597873
1
Iteration 8600: Loss = -12347.286201117915
Iteration 8700: Loss = -12347.286150749098
Iteration 8800: Loss = -12347.288624310362
1
Iteration 8900: Loss = -12347.28614406808
Iteration 9000: Loss = -12347.317576724554
1
Iteration 9100: Loss = -12347.285930719216
Iteration 9200: Loss = -12347.28590389552
Iteration 9300: Loss = -12347.287145536424
1
Iteration 9400: Loss = -12347.285796719792
Iteration 9500: Loss = -12347.299266607455
1
Iteration 9600: Loss = -12347.285765811148
Iteration 9700: Loss = -12347.285689562517
Iteration 9800: Loss = -12347.526346257333
1
Iteration 9900: Loss = -12347.28564820727
Iteration 10000: Loss = -12347.285635118938
Iteration 10100: Loss = -12347.377434830163
1
Iteration 10200: Loss = -12347.304988519754
2
Iteration 10300: Loss = -12347.466512724797
3
Iteration 10400: Loss = -12347.28551423207
Iteration 10500: Loss = -12347.288583470248
1
Iteration 10600: Loss = -12347.301215337047
2
Iteration 10700: Loss = -12347.302156665999
3
Iteration 10800: Loss = -12347.285422453373
Iteration 10900: Loss = -12347.287292045632
1
Iteration 11000: Loss = -12347.285426010902
Iteration 11100: Loss = -12347.285938314923
1
Iteration 11200: Loss = -12347.285406754436
Iteration 11300: Loss = -12347.28735892911
1
Iteration 11400: Loss = -12347.286279205315
2
Iteration 11500: Loss = -12347.285434459523
Iteration 11600: Loss = -12347.285732047621
1
Iteration 11700: Loss = -12347.287404090854
2
Iteration 11800: Loss = -12347.389568671562
3
Iteration 11900: Loss = -12347.28535217241
Iteration 12000: Loss = -12347.285350264538
Iteration 12100: Loss = -12347.54238585965
1
Iteration 12200: Loss = -12347.285293025727
Iteration 12300: Loss = -12347.39186816054
1
Iteration 12400: Loss = -12347.285267417883
Iteration 12500: Loss = -12347.298401841814
1
Iteration 12600: Loss = -12347.287795964126
2
Iteration 12700: Loss = -12347.287116033694
3
Iteration 12800: Loss = -12347.307373888809
4
Iteration 12900: Loss = -12347.319159769262
5
Iteration 13000: Loss = -12347.28522949719
Iteration 13100: Loss = -12347.285539582488
1
Iteration 13200: Loss = -12347.285215958938
Iteration 13300: Loss = -12347.285289143038
Iteration 13400: Loss = -12347.285410550267
1
Iteration 13500: Loss = -12347.285873663697
2
Iteration 13600: Loss = -12347.285211801382
Iteration 13700: Loss = -12347.285308852046
Iteration 13800: Loss = -12347.285199866332
Iteration 13900: Loss = -12347.285456037514
1
Iteration 14000: Loss = -12347.285656097669
2
Iteration 14100: Loss = -12347.28634540482
3
Iteration 14200: Loss = -12347.2857220048
4
Iteration 14300: Loss = -12347.297321818489
5
Iteration 14400: Loss = -12347.28520942575
Iteration 14500: Loss = -12347.299119677155
1
Iteration 14600: Loss = -12347.285198607264
Iteration 14700: Loss = -12347.285173391305
Iteration 14800: Loss = -12347.285531397822
1
Iteration 14900: Loss = -12347.285167434911
Iteration 15000: Loss = -12347.71951105675
1
Iteration 15100: Loss = -12347.285166805459
Iteration 15200: Loss = -12347.286448288443
1
Iteration 15300: Loss = -12347.285601901169
2
Iteration 15400: Loss = -12347.289714605417
3
Iteration 15500: Loss = -12347.285164164412
Iteration 15600: Loss = -12347.290209440987
1
Iteration 15700: Loss = -12347.285168372502
Iteration 15800: Loss = -12347.320838398586
1
Iteration 15900: Loss = -12347.289041140219
2
Iteration 16000: Loss = -12347.34290195343
3
Iteration 16100: Loss = -12347.285227741757
Iteration 16200: Loss = -12347.285702956397
1
Iteration 16300: Loss = -12347.293679016799
2
Iteration 16400: Loss = -12347.285669509012
3
Iteration 16500: Loss = -12347.285213540492
Iteration 16600: Loss = -12347.286147766186
1
Iteration 16700: Loss = -12347.285208386562
Iteration 16800: Loss = -12347.289325387714
1
Iteration 16900: Loss = -12347.285227479162
Iteration 17000: Loss = -12347.287001272622
1
Iteration 17100: Loss = -12347.285247664242
Iteration 17200: Loss = -12347.286634881395
1
Iteration 17300: Loss = -12347.285628077314
2
Iteration 17400: Loss = -12347.285318709062
Iteration 17500: Loss = -12347.285446778033
1
Iteration 17600: Loss = -12347.28530851907
Iteration 17700: Loss = -12347.31078986168
1
Iteration 17800: Loss = -12347.28523508601
Iteration 17900: Loss = -12347.296747292725
1
Iteration 18000: Loss = -12347.28521268504
Iteration 18100: Loss = -12347.303265838127
1
Iteration 18200: Loss = -12347.2852285434
Iteration 18300: Loss = -12347.358549909832
1
Iteration 18400: Loss = -12347.28521796452
Iteration 18500: Loss = -12347.317160658375
1
Iteration 18600: Loss = -12347.285207954652
Iteration 18700: Loss = -12347.288009355958
1
Iteration 18800: Loss = -12347.285287632909
Iteration 18900: Loss = -12347.285433349813
1
Iteration 19000: Loss = -12347.285430480191
2
Iteration 19100: Loss = -12347.285263983564
Iteration 19200: Loss = -12347.285212025748
Iteration 19300: Loss = -12347.285228010318
Iteration 19400: Loss = -12347.289171359227
1
Iteration 19500: Loss = -12347.292515249446
2
Iteration 19600: Loss = -12347.28519634601
Iteration 19700: Loss = -12347.2858631071
1
Iteration 19800: Loss = -12347.286077556222
2
Iteration 19900: Loss = -12347.285236031665
pi: tensor([[1.0000e+00, 5.8067e-09],
        [6.5416e-01, 3.4584e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5752, 0.4248], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2145, 0.0872],
         [0.6623, 0.2971]],

        [[0.5876, 0.1537],
         [0.6693, 0.6448]],

        [[0.7196, 0.1259],
         [0.7217, 0.6649]],

        [[0.7028, 0.3505],
         [0.7140, 0.5165]],

        [[0.6806, 0.2827],
         [0.7232, 0.5580]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 68
Adjusted Rand Index: 0.12355518668147587
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0009975514204148314
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.07799673416054068
Average Adjusted Rand Index: 0.22333130776299664
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24500.97466235582
Iteration 100: Loss = -12439.305300141481
Iteration 200: Loss = -12438.500773420034
Iteration 300: Loss = -12438.229822732565
Iteration 400: Loss = -12438.059912360428
Iteration 500: Loss = -12437.91318240101
Iteration 600: Loss = -12437.738495832797
Iteration 700: Loss = -12437.453248610567
Iteration 800: Loss = -12437.075720667242
Iteration 900: Loss = -12436.553380776119
Iteration 1000: Loss = -12435.194125243635
Iteration 1100: Loss = -12434.273819817847
Iteration 1200: Loss = -12433.320827367299
Iteration 1300: Loss = -12432.608334131406
Iteration 1400: Loss = -12432.26523742581
Iteration 1500: Loss = -12430.970749253072
Iteration 1600: Loss = -12406.307703812356
Iteration 1700: Loss = -12350.466748545416
Iteration 1800: Loss = -12347.565039990752
Iteration 1900: Loss = -12347.469311203271
Iteration 2000: Loss = -12347.389236699299
Iteration 2100: Loss = -12347.37153605737
Iteration 2200: Loss = -12347.358747368948
Iteration 2300: Loss = -12347.348793988633
Iteration 2400: Loss = -12347.340847544361
Iteration 2500: Loss = -12347.334311833829
Iteration 2600: Loss = -12347.32888158387
Iteration 2700: Loss = -12347.324286235948
Iteration 2800: Loss = -12347.320459095887
Iteration 2900: Loss = -12347.317212402266
Iteration 3000: Loss = -12347.314418123435
Iteration 3100: Loss = -12347.311973525602
Iteration 3200: Loss = -12347.3097408445
Iteration 3300: Loss = -12347.307843776267
Iteration 3400: Loss = -12347.306065302473
Iteration 3500: Loss = -12347.304543364908
Iteration 3600: Loss = -12347.303147226929
Iteration 3700: Loss = -12347.301861283395
Iteration 3800: Loss = -12347.300711501453
Iteration 3900: Loss = -12347.299695132158
Iteration 4000: Loss = -12347.29876065127
Iteration 4100: Loss = -12347.297820728032
Iteration 4200: Loss = -12347.299962811181
1
Iteration 4300: Loss = -12347.296313559476
Iteration 4400: Loss = -12347.295607031232
Iteration 4500: Loss = -12347.29518795701
Iteration 4600: Loss = -12347.29436223236
Iteration 4700: Loss = -12347.293872441958
Iteration 4800: Loss = -12347.293313144575
Iteration 4900: Loss = -12347.292873124548
Iteration 5000: Loss = -12347.292662644388
Iteration 5100: Loss = -12347.291971319692
Iteration 5200: Loss = -12347.291519600707
Iteration 5300: Loss = -12347.291091809178
Iteration 5400: Loss = -12347.290670952656
Iteration 5500: Loss = -12347.292190118409
1
Iteration 5600: Loss = -12347.289949831755
Iteration 5700: Loss = -12347.291477325176
1
Iteration 5800: Loss = -12347.2894043045
Iteration 5900: Loss = -12347.289206681107
Iteration 6000: Loss = -12347.288953311447
Iteration 6100: Loss = -12347.289559619569
1
Iteration 6200: Loss = -12347.288516327744
Iteration 6300: Loss = -12347.289575458732
1
Iteration 6400: Loss = -12347.288156238812
Iteration 6500: Loss = -12347.287983673114
Iteration 6600: Loss = -12347.288064358423
Iteration 6700: Loss = -12347.28771834646
Iteration 6800: Loss = -12347.287590465621
Iteration 6900: Loss = -12347.287451854976
Iteration 7000: Loss = -12347.287452181365
Iteration 7100: Loss = -12347.28718152552
Iteration 7200: Loss = -12347.2872722741
Iteration 7300: Loss = -12347.28710772214
Iteration 7400: Loss = -12347.287215611745
1
Iteration 7500: Loss = -12347.286816468077
Iteration 7600: Loss = -12347.28779047246
1
Iteration 7700: Loss = -12347.289809328477
2
Iteration 7800: Loss = -12347.286928807915
3
Iteration 7900: Loss = -12347.286916820996
4
Iteration 8000: Loss = -12347.28649700312
Iteration 8100: Loss = -12347.286445637366
Iteration 8200: Loss = -12347.31038186311
1
Iteration 8300: Loss = -12347.383102496395
2
Iteration 8400: Loss = -12347.288765918229
3
Iteration 8500: Loss = -12347.286113119957
Iteration 8600: Loss = -12347.286549116798
1
Iteration 8700: Loss = -12347.379158644439
2
Iteration 8800: Loss = -12347.285980935036
Iteration 8900: Loss = -12347.402971611526
1
Iteration 9000: Loss = -12347.285883045772
Iteration 9100: Loss = -12347.317988951998
1
Iteration 9200: Loss = -12347.285828423663
Iteration 9300: Loss = -12347.285789465572
Iteration 9400: Loss = -12347.287355282022
1
Iteration 9500: Loss = -12347.285712116536
Iteration 9600: Loss = -12347.286334114326
1
Iteration 9700: Loss = -12347.285680786448
Iteration 9800: Loss = -12347.285599665067
Iteration 9900: Loss = -12347.286516497463
1
Iteration 10000: Loss = -12347.285579358262
Iteration 10100: Loss = -12347.285692302117
1
Iteration 10200: Loss = -12347.285557147055
Iteration 10300: Loss = -12347.28607626253
1
Iteration 10400: Loss = -12347.29361886051
2
Iteration 10500: Loss = -12347.28688156046
3
Iteration 10600: Loss = -12347.553056302948
4
Iteration 10700: Loss = -12347.285452078384
Iteration 10800: Loss = -12347.294610115698
1
Iteration 10900: Loss = -12347.363069360059
2
Iteration 11000: Loss = -12347.285357184242
Iteration 11100: Loss = -12347.287299727031
1
Iteration 11200: Loss = -12347.285440854788
Iteration 11300: Loss = -12347.295265476407
1
Iteration 11400: Loss = -12347.28531904698
Iteration 11500: Loss = -12347.308872623455
1
Iteration 11600: Loss = -12347.285342961311
Iteration 11700: Loss = -12347.288560912857
1
Iteration 11800: Loss = -12347.285348426207
Iteration 11900: Loss = -12347.285349943904
Iteration 12000: Loss = -12347.288136187355
1
Iteration 12100: Loss = -12347.285688656466
2
Iteration 12200: Loss = -12347.285373347957
Iteration 12300: Loss = -12347.391535625688
1
Iteration 12400: Loss = -12347.28524373816
Iteration 12500: Loss = -12347.312775889795
1
Iteration 12600: Loss = -12347.285299988935
Iteration 12700: Loss = -12347.508506269049
1
Iteration 12800: Loss = -12347.285240028623
Iteration 12900: Loss = -12347.355817239662
1
Iteration 13000: Loss = -12347.285214205187
Iteration 13100: Loss = -12347.285191545065
Iteration 13200: Loss = -12347.285381385402
1
Iteration 13300: Loss = -12347.28523072745
Iteration 13400: Loss = -12347.474006131893
1
Iteration 13500: Loss = -12347.285184371613
Iteration 13600: Loss = -12347.2851959468
Iteration 13700: Loss = -12347.285469324524
1
Iteration 13800: Loss = -12347.287148200692
2
Iteration 13900: Loss = -12347.28522417999
Iteration 14000: Loss = -12347.285167903618
Iteration 14100: Loss = -12347.286224509677
1
Iteration 14200: Loss = -12347.28559499284
2
Iteration 14300: Loss = -12347.406857094042
3
Iteration 14400: Loss = -12347.285165827745
Iteration 14500: Loss = -12347.36308588055
1
Iteration 14600: Loss = -12347.285158579189
Iteration 14700: Loss = -12347.328383943894
1
Iteration 14800: Loss = -12347.285174483175
Iteration 14900: Loss = -12347.306726461462
1
Iteration 15000: Loss = -12347.285162543492
Iteration 15100: Loss = -12347.28521749923
Iteration 15200: Loss = -12347.3044132197
1
Iteration 15300: Loss = -12347.28515930928
Iteration 15400: Loss = -12347.287575056373
1
Iteration 15500: Loss = -12347.285190542809
Iteration 15600: Loss = -12347.287033899762
1
Iteration 15700: Loss = -12347.33564978822
2
Iteration 15800: Loss = -12347.285218777552
Iteration 15900: Loss = -12347.295438898213
1
Iteration 16000: Loss = -12347.369823802397
2
Iteration 16100: Loss = -12347.28527121537
Iteration 16200: Loss = -12347.348667851256
1
Iteration 16300: Loss = -12347.285204453503
Iteration 16400: Loss = -12347.285395923951
1
Iteration 16500: Loss = -12347.39861267723
2
Iteration 16600: Loss = -12347.285204924332
Iteration 16700: Loss = -12347.312484601962
1
Iteration 16800: Loss = -12347.300910325119
2
Iteration 16900: Loss = -12347.285227123091
Iteration 17000: Loss = -12347.286256206116
1
Iteration 17100: Loss = -12347.298013853768
2
Iteration 17200: Loss = -12347.285209867483
Iteration 17300: Loss = -12347.286153317356
1
Iteration 17400: Loss = -12347.285777123094
2
Iteration 17500: Loss = -12347.330504365758
3
Iteration 17600: Loss = -12347.285218992762
Iteration 17700: Loss = -12347.287491053954
1
Iteration 17800: Loss = -12347.285550099912
2
Iteration 17900: Loss = -12347.2856347149
3
Iteration 18000: Loss = -12347.285286165501
Iteration 18100: Loss = -12347.288881716951
1
Iteration 18200: Loss = -12347.285225206911
Iteration 18300: Loss = -12347.28722616212
1
Iteration 18400: Loss = -12347.285305714906
Iteration 18500: Loss = -12347.287351376166
1
Iteration 18600: Loss = -12347.285446742884
2
Iteration 18700: Loss = -12347.285534740704
3
Iteration 18800: Loss = -12347.29680650659
4
Iteration 18900: Loss = -12347.442891347928
5
Iteration 19000: Loss = -12347.28521174198
Iteration 19100: Loss = -12347.28543564346
1
Iteration 19200: Loss = -12347.285389514627
2
Iteration 19300: Loss = -12347.285429986721
3
Iteration 19400: Loss = -12347.287698078062
4
Iteration 19500: Loss = -12347.438046805017
5
Iteration 19600: Loss = -12347.2854066315
6
Iteration 19700: Loss = -12347.285224596602
Iteration 19800: Loss = -12347.42079542111
1
Iteration 19900: Loss = -12347.290608741503
2
pi: tensor([[1.0000e+00, 5.6105e-09],
        [6.5409e-01, 3.4591e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5756, 0.4244], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2143, 0.0875],
         [0.6057, 0.2970]],

        [[0.6802, 0.1541],
         [0.6115, 0.5842]],

        [[0.6839, 0.1257],
         [0.6555, 0.6161]],

        [[0.7195, 0.3510],
         [0.5535, 0.7268]],

        [[0.6432, 0.2829],
         [0.6268, 0.7039]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 68
Adjusted Rand Index: 0.12355518668147587
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0009975514204148314
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.07799673416054068
Average Adjusted Rand Index: 0.22333130776299664
11978.68932873401
[0.07799673416054068, 0.07799673416054068] [0.22333130776299664, 0.22333130776299664] [12347.310446589945, 12347.313561876255]
-------------------------------------
This iteration is 58
True Objective function: Loss = -11764.366024975014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22172.894745376547
Iteration 100: Loss = -12260.472901902398
Iteration 200: Loss = -12259.58094082903
Iteration 300: Loss = -12259.386409412626
Iteration 400: Loss = -12259.299102842439
Iteration 500: Loss = -12259.244031353335
Iteration 600: Loss = -12259.202489909847
Iteration 700: Loss = -12259.167459707016
Iteration 800: Loss = -12259.135144611166
Iteration 900: Loss = -12259.102581653224
Iteration 1000: Loss = -12259.067311957668
Iteration 1100: Loss = -12259.029206554671
Iteration 1200: Loss = -12258.99531923782
Iteration 1300: Loss = -12258.973403007654
Iteration 1400: Loss = -12258.959631547232
Iteration 1500: Loss = -12258.948767836051
Iteration 1600: Loss = -12258.939020855654
Iteration 1700: Loss = -12258.929658920602
Iteration 1800: Loss = -12258.920254189634
Iteration 1900: Loss = -12258.910559785114
Iteration 2000: Loss = -12258.90013311747
Iteration 2100: Loss = -12258.888751830564
Iteration 2200: Loss = -12258.87559025964
Iteration 2300: Loss = -12258.859578317786
Iteration 2400: Loss = -12258.838275699087
Iteration 2500: Loss = -12258.80778436368
Iteration 2600: Loss = -12258.768166793468
Iteration 2700: Loss = -12258.726678854453
Iteration 2800: Loss = -12258.68836003576
Iteration 2900: Loss = -12258.65542293293
Iteration 3000: Loss = -12258.628480457282
Iteration 3100: Loss = -12258.606439780637
Iteration 3200: Loss = -12258.587898151065
Iteration 3300: Loss = -12258.57168536781
Iteration 3400: Loss = -12258.556800076994
Iteration 3500: Loss = -12258.542826422216
Iteration 3600: Loss = -12258.529314381054
Iteration 3700: Loss = -12258.51637609959
Iteration 3800: Loss = -12258.503560291081
Iteration 3900: Loss = -12258.491311229285
Iteration 4000: Loss = -12258.479056123704
Iteration 4100: Loss = -12258.467472868924
Iteration 4200: Loss = -12258.45595043751
Iteration 4300: Loss = -12258.44485500077
Iteration 4400: Loss = -12258.433763387684
Iteration 4500: Loss = -12258.42288904065
Iteration 4600: Loss = -12258.412149246544
Iteration 4700: Loss = -12258.402087519002
Iteration 4800: Loss = -12258.393161678392
Iteration 4900: Loss = -12258.38571002401
Iteration 5000: Loss = -12258.379864350312
Iteration 5100: Loss = -12258.37541596624
Iteration 5200: Loss = -12258.371995523627
Iteration 5300: Loss = -12258.369374096786
Iteration 5400: Loss = -12258.367217827907
Iteration 5500: Loss = -12258.365510891561
Iteration 5600: Loss = -12258.36404881528
Iteration 5700: Loss = -12258.362806846113
Iteration 5800: Loss = -12258.36183983374
Iteration 5900: Loss = -12258.360824505477
Iteration 6000: Loss = -12258.360063414611
Iteration 6100: Loss = -12258.360698604489
1
Iteration 6200: Loss = -12258.358727853823
Iteration 6300: Loss = -12258.358171226193
Iteration 6400: Loss = -12258.364357724397
1
Iteration 6500: Loss = -12258.357227949182
Iteration 6600: Loss = -12258.356869034033
Iteration 6700: Loss = -12258.356592928843
Iteration 6800: Loss = -12258.356240209228
Iteration 6900: Loss = -12258.356404404989
1
Iteration 7000: Loss = -12258.355704525178
Iteration 7100: Loss = -12258.355496880624
Iteration 7200: Loss = -12258.355428890902
Iteration 7300: Loss = -12258.355113935666
Iteration 7400: Loss = -12258.354925942156
Iteration 7500: Loss = -12258.35477878769
Iteration 7600: Loss = -12258.354989290574
1
Iteration 7700: Loss = -12258.405886004044
2
Iteration 7800: Loss = -12258.354436648402
Iteration 7900: Loss = -12258.404885665554
1
Iteration 8000: Loss = -12258.354211082522
Iteration 8100: Loss = -12258.362564867191
1
Iteration 8200: Loss = -12258.354075147821
Iteration 8300: Loss = -12258.353977640354
Iteration 8400: Loss = -12258.356459509549
1
Iteration 8500: Loss = -12258.364157592061
2
Iteration 8600: Loss = -12258.353868377053
Iteration 8700: Loss = -12258.355342771722
1
Iteration 8800: Loss = -12258.353912975763
Iteration 8900: Loss = -12258.356245043504
1
Iteration 9000: Loss = -12258.358509385556
2
Iteration 9100: Loss = -12258.353722556172
Iteration 9200: Loss = -12258.41938630309
1
Iteration 9300: Loss = -12258.395338864279
2
Iteration 9400: Loss = -12258.475301933408
3
Iteration 9500: Loss = -12258.353606716286
Iteration 9600: Loss = -12258.355473035832
1
Iteration 9700: Loss = -12258.353587637293
Iteration 9800: Loss = -12258.353708107908
1
Iteration 9900: Loss = -12258.353491301492
Iteration 10000: Loss = -12258.354068487939
1
Iteration 10100: Loss = -12258.353501951067
Iteration 10200: Loss = -12258.375311334548
1
Iteration 10300: Loss = -12258.353491831083
Iteration 10400: Loss = -12258.353464426216
Iteration 10500: Loss = -12258.353492872708
Iteration 10600: Loss = -12258.353442754304
Iteration 10700: Loss = -12258.413705614528
1
Iteration 10800: Loss = -12258.35347256646
Iteration 10900: Loss = -12258.353685617456
1
Iteration 11000: Loss = -12258.353419347568
Iteration 11100: Loss = -12258.35343904521
Iteration 11200: Loss = -12258.353440149302
Iteration 11300: Loss = -12258.374541908413
1
Iteration 11400: Loss = -12258.353383769407
Iteration 11500: Loss = -12258.353437067959
Iteration 11600: Loss = -12258.35359940831
1
Iteration 11700: Loss = -12258.407638140245
2
Iteration 11800: Loss = -12258.353380717052
Iteration 11900: Loss = -12258.373829084849
1
Iteration 12000: Loss = -12258.367317221517
2
Iteration 12100: Loss = -12258.473637248295
3
Iteration 12200: Loss = -12258.353698757617
4
Iteration 12300: Loss = -12258.353459265134
Iteration 12400: Loss = -12258.355129382699
1
Iteration 12500: Loss = -12258.354132743449
2
Iteration 12600: Loss = -12258.361400549149
3
Iteration 12700: Loss = -12258.368832368667
4
Iteration 12800: Loss = -12258.375810651276
5
Iteration 12900: Loss = -12258.35343061343
Iteration 13000: Loss = -12258.353762876943
1
Iteration 13100: Loss = -12258.43829236808
2
Iteration 13200: Loss = -12258.38583260334
3
Iteration 13300: Loss = -12258.368546250238
4
Iteration 13400: Loss = -12258.435397839443
5
Iteration 13500: Loss = -12258.353806758832
6
Iteration 13600: Loss = -12258.395954776995
7
Iteration 13700: Loss = -12258.355569257195
8
Iteration 13800: Loss = -12258.353919473411
9
Iteration 13900: Loss = -12258.35357974794
10
Iteration 14000: Loss = -12258.360098747635
11
Iteration 14100: Loss = -12258.357111559846
12
Iteration 14200: Loss = -12258.359555680381
13
Iteration 14300: Loss = -12258.35871342539
14
Iteration 14400: Loss = -12258.354453402553
15
Stopping early at iteration 14400 due to no improvement.
pi: tensor([[0.9918, 0.0082],
        [0.8989, 0.1011]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0106, 0.9894], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1950, 0.2036],
         [0.5797, 0.2000]],

        [[0.5435, 0.1854],
         [0.6600, 0.6340]],

        [[0.5389, 0.2274],
         [0.7191, 0.5676]],

        [[0.6287, 0.3123],
         [0.7133, 0.6971]],

        [[0.6965, 0.1738],
         [0.6093, 0.5210]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011723448338714507
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24468.107239319495
Iteration 100: Loss = -12259.845731308817
Iteration 200: Loss = -12259.322417703379
Iteration 300: Loss = -12259.218698180775
Iteration 400: Loss = -12259.15764159656
Iteration 500: Loss = -12259.104862493368
Iteration 600: Loss = -12259.045989106064
Iteration 700: Loss = -12258.972378242417
Iteration 800: Loss = -12258.898996056096
Iteration 900: Loss = -12258.855794174518
Iteration 1000: Loss = -12258.823910393307
Iteration 1100: Loss = -12258.794448449724
Iteration 1200: Loss = -12258.766086265841
Iteration 1300: Loss = -12258.738344293355
Iteration 1400: Loss = -12258.711540899103
Iteration 1500: Loss = -12258.68540519877
Iteration 1600: Loss = -12258.658773551286
Iteration 1700: Loss = -12258.63153729777
Iteration 1800: Loss = -12258.60568206129
Iteration 1900: Loss = -12258.584443663214
Iteration 2000: Loss = -12258.56852336545
Iteration 2100: Loss = -12258.555929984941
Iteration 2200: Loss = -12258.544829836617
Iteration 2300: Loss = -12258.53421078983
Iteration 2400: Loss = -12258.523825151658
Iteration 2500: Loss = -12258.5135364334
Iteration 2600: Loss = -12258.503299422511
Iteration 2700: Loss = -12258.493233695744
Iteration 2800: Loss = -12258.48350432831
Iteration 2900: Loss = -12258.474311144839
Iteration 3000: Loss = -12258.465745251862
Iteration 3100: Loss = -12258.457986594001
Iteration 3200: Loss = -12258.450862003334
Iteration 3300: Loss = -12258.444360723393
Iteration 3400: Loss = -12258.438246488222
Iteration 3500: Loss = -12258.432260840667
Iteration 3600: Loss = -12258.426223559132
Iteration 3700: Loss = -12258.419810618647
Iteration 3800: Loss = -12258.41282311477
Iteration 3900: Loss = -12258.404977572654
Iteration 4000: Loss = -12258.396219852395
Iteration 4100: Loss = -12258.386910295332
Iteration 4200: Loss = -12258.378050734695
Iteration 4300: Loss = -12258.371003547307
Iteration 4400: Loss = -12258.366304765234
Iteration 4500: Loss = -12258.363301679568
Iteration 4600: Loss = -12258.36141881177
Iteration 4700: Loss = -12258.360031609787
Iteration 4800: Loss = -12258.359025429583
Iteration 4900: Loss = -12258.358193805652
Iteration 5000: Loss = -12258.357529029352
Iteration 5100: Loss = -12258.35698815195
Iteration 5200: Loss = -12258.356560851566
Iteration 5300: Loss = -12258.356176268175
Iteration 5400: Loss = -12258.355825951068
Iteration 5500: Loss = -12258.355553837011
Iteration 5600: Loss = -12258.355608544787
Iteration 5700: Loss = -12258.355108594049
Iteration 5800: Loss = -12258.354901766932
Iteration 5900: Loss = -12258.35471709988
Iteration 6000: Loss = -12258.354636370463
Iteration 6100: Loss = -12258.356283692638
1
Iteration 6200: Loss = -12258.354351705744
Iteration 6300: Loss = -12258.354278071654
Iteration 6400: Loss = -12258.354218262488
Iteration 6500: Loss = -12258.35410241771
Iteration 6600: Loss = -12258.354160005125
Iteration 6700: Loss = -12258.353950827133
Iteration 6800: Loss = -12258.353893227342
Iteration 6900: Loss = -12258.353832997078
Iteration 7000: Loss = -12258.353798242351
Iteration 7100: Loss = -12258.353761871884
Iteration 7200: Loss = -12258.353714949602
Iteration 7300: Loss = -12258.354368537397
1
Iteration 7400: Loss = -12258.354001837477
2
Iteration 7500: Loss = -12258.35360643592
Iteration 7600: Loss = -12258.353593715703
Iteration 7700: Loss = -12258.353656033638
Iteration 7800: Loss = -12258.353541222365
Iteration 7900: Loss = -12258.35354925383
Iteration 8000: Loss = -12258.362013583292
1
Iteration 8100: Loss = -12258.355476479404
2
Iteration 8200: Loss = -12258.393276963412
3
Iteration 8300: Loss = -12258.353490138183
Iteration 8400: Loss = -12258.356535333802
1
Iteration 8500: Loss = -12258.353468285448
Iteration 8600: Loss = -12258.35883846196
1
Iteration 8700: Loss = -12258.353425874251
Iteration 8800: Loss = -12258.389500428055
1
Iteration 8900: Loss = -12258.35342175308
Iteration 9000: Loss = -12258.353749946564
1
Iteration 9100: Loss = -12258.379184341538
2
Iteration 9200: Loss = -12258.358649096466
3
Iteration 9300: Loss = -12258.354672216987
4
Iteration 9400: Loss = -12258.354016107925
5
Iteration 9500: Loss = -12258.445587007058
6
Iteration 9600: Loss = -12258.353411594837
Iteration 9700: Loss = -12258.353381258554
Iteration 9800: Loss = -12258.35606052203
1
Iteration 9900: Loss = -12258.353365511464
Iteration 10000: Loss = -12258.353414059497
Iteration 10100: Loss = -12258.353500073075
Iteration 10200: Loss = -12258.353412398326
Iteration 10300: Loss = -12258.374043011721
1
Iteration 10400: Loss = -12258.353430315417
Iteration 10500: Loss = -12258.35339194677
Iteration 10600: Loss = -12258.353821119237
1
Iteration 10700: Loss = -12258.353613114983
2
Iteration 10800: Loss = -12258.590178512706
3
Iteration 10900: Loss = -12258.35339021941
Iteration 11000: Loss = -12258.42612665306
1
Iteration 11100: Loss = -12258.353389979848
Iteration 11200: Loss = -12258.365035178234
1
Iteration 11300: Loss = -12258.35521804415
2
Iteration 11400: Loss = -12258.35349242263
3
Iteration 11500: Loss = -12258.35368043694
4
Iteration 11600: Loss = -12258.353385329605
Iteration 11700: Loss = -12258.353435812769
Iteration 11800: Loss = -12258.360770811936
1
Iteration 11900: Loss = -12258.353698238077
2
Iteration 12000: Loss = -12258.353440278606
Iteration 12100: Loss = -12258.38151561314
1
Iteration 12200: Loss = -12258.400945314026
2
Iteration 12300: Loss = -12258.35363076679
3
Iteration 12400: Loss = -12258.354648241775
4
Iteration 12500: Loss = -12258.354319100079
5
Iteration 12600: Loss = -12258.357428999016
6
Iteration 12700: Loss = -12258.354361031144
7
Iteration 12800: Loss = -12258.354110794202
8
Iteration 12900: Loss = -12258.361036310527
9
Iteration 13000: Loss = -12258.35840386967
10
Iteration 13100: Loss = -12258.353413685525
Iteration 13200: Loss = -12258.353645394069
1
Iteration 13300: Loss = -12258.354816714116
2
Iteration 13400: Loss = -12258.35335518365
Iteration 13500: Loss = -12258.353447871716
Iteration 13600: Loss = -12258.3762288544
1
Iteration 13700: Loss = -12258.353368071208
Iteration 13800: Loss = -12258.360306002289
1
Iteration 13900: Loss = -12258.35338000549
Iteration 14000: Loss = -12258.365479502463
1
Iteration 14100: Loss = -12258.353875133153
2
Iteration 14200: Loss = -12258.353445768073
Iteration 14300: Loss = -12258.627999577262
1
Iteration 14400: Loss = -12258.3535846577
2
Iteration 14500: Loss = -12258.371472215653
3
Iteration 14600: Loss = -12258.387356551824
4
Iteration 14700: Loss = -12258.357765109782
5
Iteration 14800: Loss = -12258.353550240388
6
Iteration 14900: Loss = -12258.35542092711
7
Iteration 15000: Loss = -12258.356390423049
8
Iteration 15100: Loss = -12258.358819201958
9
Iteration 15200: Loss = -12258.353427143453
Iteration 15300: Loss = -12258.35394938465
1
Iteration 15400: Loss = -12258.353414714242
Iteration 15500: Loss = -12258.353465840524
Iteration 15600: Loss = -12258.39713419539
1
Iteration 15700: Loss = -12258.353640007472
2
Iteration 15800: Loss = -12258.353551018143
Iteration 15900: Loss = -12258.36218555074
1
Iteration 16000: Loss = -12258.361173961199
2
Iteration 16100: Loss = -12258.359247908262
3
Iteration 16200: Loss = -12258.354456187135
4
Iteration 16300: Loss = -12258.356662204798
5
Iteration 16400: Loss = -12258.353430955895
Iteration 16500: Loss = -12258.353599510865
1
Iteration 16600: Loss = -12258.357415756887
2
Iteration 16700: Loss = -12258.372620762873
3
Iteration 16800: Loss = -12258.353431246525
Iteration 16900: Loss = -12258.355223556562
1
Iteration 17000: Loss = -12258.446497257935
2
Iteration 17100: Loss = -12258.36413960726
3
Iteration 17200: Loss = -12258.355579243042
4
Iteration 17300: Loss = -12258.355674021797
5
Iteration 17400: Loss = -12258.35350149937
Iteration 17500: Loss = -12258.35339091563
Iteration 17600: Loss = -12258.356203984888
1
Iteration 17700: Loss = -12258.35722653529
2
Iteration 17800: Loss = -12258.354748808986
3
Iteration 17900: Loss = -12258.36103719683
4
Iteration 18000: Loss = -12258.35432502001
5
Iteration 18100: Loss = -12258.353486700891
Iteration 18200: Loss = -12258.363240813673
1
Iteration 18300: Loss = -12258.360185846705
2
Iteration 18400: Loss = -12258.429994671415
3
Iteration 18500: Loss = -12258.364976129416
4
Iteration 18600: Loss = -12258.364455287254
5
Iteration 18700: Loss = -12258.455826641792
6
Iteration 18800: Loss = -12258.466407409947
7
Iteration 18900: Loss = -12258.353830718797
8
Iteration 19000: Loss = -12258.36245185451
9
Iteration 19100: Loss = -12258.375580382266
10
Iteration 19200: Loss = -12258.354232675072
11
Iteration 19300: Loss = -12258.354202905803
12
Iteration 19400: Loss = -12258.356970931354
13
Iteration 19500: Loss = -12258.529647113017
14
Iteration 19600: Loss = -12258.389406272092
15
Stopping early at iteration 19600 due to no improvement.
pi: tensor([[0.1004, 0.8996],
        [0.0082, 0.9918]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9919, 0.0081], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2016, 0.2025],
         [0.5455, 0.1950]],

        [[0.7239, 0.1855],
         [0.6527, 0.5847]],

        [[0.6109, 0.2272],
         [0.5056, 0.5121]],

        [[0.5730, 0.3126],
         [0.5665, 0.6397]],

        [[0.5623, 0.1738],
         [0.5865, 0.6621]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011723448338714507
Average Adjusted Rand Index: 0.0
11764.366024975014
[-0.0011723448338714507, -0.0011723448338714507] [0.0, 0.0] [12258.354453402553, 12258.389406272092]
-------------------------------------
This iteration is 59
True Objective function: Loss = -11786.806064709708
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19164.435129409267
Iteration 100: Loss = -12294.441440058667
Iteration 200: Loss = -12294.106511155336
Iteration 300: Loss = -12294.018717876339
Iteration 400: Loss = -12293.883309493891
Iteration 500: Loss = -12293.634853687903
Iteration 600: Loss = -12293.281810213935
Iteration 700: Loss = -12293.023141954749
Iteration 800: Loss = -12292.838406726934
Iteration 900: Loss = -12292.699326223224
Iteration 1000: Loss = -12292.60381548515
Iteration 1100: Loss = -12292.544767671903
Iteration 1200: Loss = -12292.504954348547
Iteration 1300: Loss = -12292.472076166921
Iteration 1400: Loss = -12292.446485841769
Iteration 1500: Loss = -12292.412540322448
Iteration 1600: Loss = -12292.285611210415
Iteration 1700: Loss = -12292.083798044201
Iteration 1800: Loss = -12292.001802200113
Iteration 1900: Loss = -12291.968651017574
Iteration 2000: Loss = -12291.94674691485
Iteration 2100: Loss = -12291.923541035429
Iteration 2200: Loss = -12291.893219978514
Iteration 2300: Loss = -12291.855000827081
Iteration 2400: Loss = -12291.819364693025
Iteration 2500: Loss = -12291.79088399368
Iteration 2600: Loss = -12291.806513118334
1
Iteration 2700: Loss = -12291.755461975466
Iteration 2800: Loss = -12291.74444953431
Iteration 2900: Loss = -12291.737277127071
Iteration 3000: Loss = -12291.731037034982
Iteration 3100: Loss = -12291.72696262185
Iteration 3200: Loss = -12291.723942918225
Iteration 3300: Loss = -12291.721832839836
Iteration 3400: Loss = -12291.720272616996
Iteration 3500: Loss = -12291.719840321697
Iteration 3600: Loss = -12291.718364552118
Iteration 3700: Loss = -12291.717852820755
Iteration 3800: Loss = -12291.717449510703
Iteration 3900: Loss = -12291.717187551141
Iteration 4000: Loss = -12291.762548078032
1
Iteration 4100: Loss = -12291.716891341815
Iteration 4200: Loss = -12291.716790665181
Iteration 4300: Loss = -12291.716776094341
Iteration 4400: Loss = -12291.716726456129
Iteration 4500: Loss = -12291.718175114303
1
Iteration 4600: Loss = -12291.716752448192
Iteration 4700: Loss = -12291.731685981838
1
Iteration 4800: Loss = -12291.716711007935
Iteration 4900: Loss = -12291.71693849818
1
Iteration 5000: Loss = -12291.716707267868
Iteration 5100: Loss = -12291.71669187813
Iteration 5200: Loss = -12291.716688946897
Iteration 5300: Loss = -12291.716661721473
Iteration 5400: Loss = -12291.823391949909
1
Iteration 5500: Loss = -12291.716680476444
Iteration 5600: Loss = -12291.71703113041
1
Iteration 5700: Loss = -12291.716721674587
Iteration 5800: Loss = -12291.716736799206
Iteration 5900: Loss = -12291.716691950907
Iteration 6000: Loss = -12291.7167457413
Iteration 6100: Loss = -12291.941991268532
1
Iteration 6200: Loss = -12291.716692392909
Iteration 6300: Loss = -12291.813070818122
1
Iteration 6400: Loss = -12291.716691784697
Iteration 6500: Loss = -12291.770225009554
1
Iteration 6600: Loss = -12291.716685900456
Iteration 6700: Loss = -12291.71927765074
1
Iteration 6800: Loss = -12291.718792678514
2
Iteration 6900: Loss = -12291.723755658371
3
Iteration 7000: Loss = -12291.717811074439
4
Iteration 7100: Loss = -12291.746298850847
5
Iteration 7200: Loss = -12291.716902703967
6
Iteration 7300: Loss = -12291.71685737418
7
Iteration 7400: Loss = -12291.838699379246
8
Iteration 7500: Loss = -12291.717752370143
9
Iteration 7600: Loss = -12291.720604771846
10
Iteration 7700: Loss = -12291.740612778482
11
Iteration 7800: Loss = -12291.716685488556
Iteration 7900: Loss = -12291.722725962474
1
Iteration 8000: Loss = -12291.724060493178
2
Iteration 8100: Loss = -12291.725845273753
3
Iteration 8200: Loss = -12291.732418355396
4
Iteration 8300: Loss = -12291.746264660953
5
Iteration 8400: Loss = -12291.730535171087
6
Iteration 8500: Loss = -12291.723355691596
7
Iteration 8600: Loss = -12291.72990257776
8
Iteration 8700: Loss = -12291.874660325871
9
Iteration 8800: Loss = -12291.724125130206
10
Iteration 8900: Loss = -12291.734960656424
11
Iteration 9000: Loss = -12291.713725599117
Iteration 9100: Loss = -12291.71431750931
1
Iteration 9200: Loss = -12291.713715498603
Iteration 9300: Loss = -12291.729782297278
1
Iteration 9400: Loss = -12291.71816808687
2
Iteration 9500: Loss = -12291.822025420395
3
Iteration 9600: Loss = -12291.720661205556
4
Iteration 9700: Loss = -12291.716156847824
5
Iteration 9800: Loss = -12291.716578295896
6
Iteration 9900: Loss = -12291.714443345676
7
Iteration 10000: Loss = -12291.724134041227
8
Iteration 10100: Loss = -12291.713723010205
Iteration 10200: Loss = -12291.822719960115
1
Iteration 10300: Loss = -12291.74625273047
2
Iteration 10400: Loss = -12291.716409437355
3
Iteration 10500: Loss = -12291.713723274703
Iteration 10600: Loss = -12291.720388050766
1
Iteration 10700: Loss = -12291.986303387604
2
Iteration 10800: Loss = -12291.714471189045
3
Iteration 10900: Loss = -12291.728895115255
4
Iteration 11000: Loss = -12291.71373887623
Iteration 11100: Loss = -12291.713724748128
Iteration 11200: Loss = -12291.718053677523
1
Iteration 11300: Loss = -12291.747894631279
2
Iteration 11400: Loss = -12291.714056067794
3
Iteration 11500: Loss = -12291.71369144866
Iteration 11600: Loss = -12291.714068697798
1
Iteration 11700: Loss = -12291.713789407668
Iteration 11800: Loss = -12291.715230567486
1
Iteration 11900: Loss = -12291.714091987224
2
Iteration 12000: Loss = -12291.784493522551
3
Iteration 12100: Loss = -12291.713750794142
Iteration 12200: Loss = -12291.713682428424
Iteration 12300: Loss = -12291.726816033386
1
Iteration 12400: Loss = -12291.73447974014
2
Iteration 12500: Loss = -12291.71368192252
Iteration 12600: Loss = -12291.715675432446
1
Iteration 12700: Loss = -12291.714763517637
2
Iteration 12800: Loss = -12291.713787438382
3
Iteration 12900: Loss = -12291.714714129
4
Iteration 13000: Loss = -12291.713899689574
5
Iteration 13100: Loss = -12291.795020300555
6
Iteration 13200: Loss = -12291.752552220367
7
Iteration 13300: Loss = -12291.71494028576
8
Iteration 13400: Loss = -12291.713997326999
9
Iteration 13500: Loss = -12291.716002471236
10
Iteration 13600: Loss = -12291.726930786304
11
Iteration 13700: Loss = -12291.719297108828
12
Iteration 13800: Loss = -12291.750171883485
13
Iteration 13900: Loss = -12291.718157884427
14
Iteration 14000: Loss = -12291.762437197649
15
Stopping early at iteration 14000 due to no improvement.
pi: tensor([[0.7197, 0.2803],
        [0.9214, 0.0786]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0048, 0.9952], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2032, 0.2349],
         [0.6228, 0.1887]],

        [[0.6337, 0.2100],
         [0.5369, 0.7135]],

        [[0.5278, 0.2023],
         [0.6772, 0.6669]],

        [[0.5682, 0.1920],
         [0.6391, 0.7044]],

        [[0.5813, 0.1790],
         [0.7242, 0.6176]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007304767002590059
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23143.572710386765
Iteration 100: Loss = -12294.778423095753
Iteration 200: Loss = -12294.242768885095
Iteration 300: Loss = -12294.083293955055
Iteration 400: Loss = -12293.957380833233
Iteration 500: Loss = -12293.837030393055
Iteration 600: Loss = -12293.720167996382
Iteration 700: Loss = -12293.601930017207
Iteration 800: Loss = -12293.472889226186
Iteration 900: Loss = -12293.329863033163
Iteration 1000: Loss = -12293.17319586123
Iteration 1100: Loss = -12293.008227569375
Iteration 1200: Loss = -12292.851499308892
Iteration 1300: Loss = -12292.723119773445
Iteration 1400: Loss = -12292.605490436392
Iteration 1500: Loss = -12292.43822374361
Iteration 1600: Loss = -12292.209837712679
Iteration 1700: Loss = -12292.079044680604
Iteration 1800: Loss = -12292.0221815228
Iteration 1900: Loss = -12291.997366752796
Iteration 2000: Loss = -12291.98314691179
Iteration 2100: Loss = -12291.974148415165
Iteration 2200: Loss = -12291.9663416675
Iteration 2300: Loss = -12291.958136959322
Iteration 2400: Loss = -12291.94812419723
Iteration 2500: Loss = -12291.935213094082
Iteration 2600: Loss = -12291.918261686253
Iteration 2700: Loss = -12291.89851669842
Iteration 2800: Loss = -12291.871785837737
Iteration 2900: Loss = -12291.844879786195
Iteration 3000: Loss = -12291.820607229198
Iteration 3100: Loss = -12291.799066082178
Iteration 3200: Loss = -12291.781678392988
Iteration 3300: Loss = -12291.76705916568
Iteration 3400: Loss = -12291.755295031004
Iteration 3500: Loss = -12291.745395974589
Iteration 3600: Loss = -12291.742939705848
Iteration 3700: Loss = -12291.731454945046
Iteration 3800: Loss = -12291.726940652079
Iteration 3900: Loss = -12291.723885257348
Iteration 4000: Loss = -12291.72126945869
Iteration 4100: Loss = -12291.71965680494
Iteration 4200: Loss = -12291.718575074885
Iteration 4300: Loss = -12291.717875326669
Iteration 4400: Loss = -12291.722964848908
1
Iteration 4500: Loss = -12291.717192227377
Iteration 4600: Loss = -12291.71699180041
Iteration 4700: Loss = -12291.717072737278
Iteration 4800: Loss = -12291.716845253453
Iteration 4900: Loss = -12291.718869481036
1
Iteration 5000: Loss = -12291.716829267394
Iteration 5100: Loss = -12291.716768672648
Iteration 5200: Loss = -12291.716938523465
1
Iteration 5300: Loss = -12291.716742097893
Iteration 5400: Loss = -12291.770151786779
1
Iteration 5500: Loss = -12291.716679931844
Iteration 5600: Loss = -12291.716704018774
Iteration 5700: Loss = -12291.716772090831
Iteration 5800: Loss = -12291.716703465057
Iteration 5900: Loss = -12291.781929114499
1
Iteration 6000: Loss = -12291.716699232467
Iteration 6100: Loss = -12291.71670379207
Iteration 6200: Loss = -12291.716701157718
Iteration 6300: Loss = -12291.71666897672
Iteration 6400: Loss = -12291.71795031663
1
Iteration 6500: Loss = -12291.716630056284
Iteration 6600: Loss = -12291.738996587736
1
Iteration 6700: Loss = -12291.71754763053
2
Iteration 6800: Loss = -12291.736719043594
3
Iteration 6900: Loss = -12291.71662320266
Iteration 7000: Loss = -12291.716704538396
Iteration 7100: Loss = -12291.769029180201
1
Iteration 7200: Loss = -12291.717355515422
2
Iteration 7300: Loss = -12291.742752130542
3
Iteration 7400: Loss = -12291.744454915517
4
Iteration 7500: Loss = -12291.720992663713
5
Iteration 7600: Loss = -12291.716420650679
Iteration 7700: Loss = -12291.716903059585
1
Iteration 7800: Loss = -12291.730631838664
2
Iteration 7900: Loss = -12291.71450896055
Iteration 8000: Loss = -12291.715060312634
1
Iteration 8100: Loss = -12291.728033041922
2
Iteration 8200: Loss = -12291.714057346766
Iteration 8300: Loss = -12291.713894311433
Iteration 8400: Loss = -12291.713957272326
Iteration 8500: Loss = -12291.72961652357
1
Iteration 8600: Loss = -12291.733883226141
2
Iteration 8700: Loss = -12291.718391145965
3
Iteration 8800: Loss = -12291.716292258863
4
Iteration 8900: Loss = -12291.722587669266
5
Iteration 9000: Loss = -12291.71422164843
6
Iteration 9100: Loss = -12291.713806836015
Iteration 9200: Loss = -12291.71637037043
1
Iteration 9300: Loss = -12291.713910701996
2
Iteration 9400: Loss = -12291.72123065052
3
Iteration 9500: Loss = -12291.713710115157
Iteration 9600: Loss = -12291.714317544183
1
Iteration 9700: Loss = -12291.71775847587
2
Iteration 9800: Loss = -12291.713750456047
Iteration 9900: Loss = -12291.716564843451
1
Iteration 10000: Loss = -12291.722448381392
2
Iteration 10100: Loss = -12291.713694716598
Iteration 10200: Loss = -12291.741669079996
1
Iteration 10300: Loss = -12291.713695680543
Iteration 10400: Loss = -12291.71376284842
Iteration 10500: Loss = -12291.724640350256
1
Iteration 10600: Loss = -12291.713753317803
Iteration 10700: Loss = -12291.789632493184
1
Iteration 10800: Loss = -12291.713720859909
Iteration 10900: Loss = -12291.713904554184
1
Iteration 11000: Loss = -12291.71447243729
2
Iteration 11100: Loss = -12291.988707961977
3
Iteration 11200: Loss = -12291.719024399503
4
Iteration 11300: Loss = -12291.745440511968
5
Iteration 11400: Loss = -12291.765051551502
6
Iteration 11500: Loss = -12291.713932662191
7
Iteration 11600: Loss = -12291.71384394279
8
Iteration 11700: Loss = -12291.71919557856
9
Iteration 11800: Loss = -12291.713721966717
Iteration 11900: Loss = -12291.775660362542
1
Iteration 12000: Loss = -12291.720433576416
2
Iteration 12100: Loss = -12291.716670817083
3
Iteration 12200: Loss = -12291.71405778396
4
Iteration 12300: Loss = -12291.721411054494
5
Iteration 12400: Loss = -12291.713954022736
6
Iteration 12500: Loss = -12291.714637484833
7
Iteration 12600: Loss = -12291.714087537532
8
Iteration 12700: Loss = -12291.713808872848
Iteration 12800: Loss = -12291.714505813205
1
Iteration 12900: Loss = -12291.713726265836
Iteration 13000: Loss = -12291.72224681009
1
Iteration 13100: Loss = -12291.713789172945
Iteration 13200: Loss = -12291.72749107727
1
Iteration 13300: Loss = -12291.719676332566
2
Iteration 13400: Loss = -12291.7137705769
Iteration 13500: Loss = -12292.014206504644
1
Iteration 13600: Loss = -12291.713724670617
Iteration 13700: Loss = -12291.715838501923
1
Iteration 13800: Loss = -12291.713733549883
Iteration 13900: Loss = -12291.713967390695
1
Iteration 14000: Loss = -12291.715237735123
2
Iteration 14100: Loss = -12291.715625763103
3
Iteration 14200: Loss = -12291.728451461488
4
Iteration 14300: Loss = -12291.731996616561
5
Iteration 14400: Loss = -12291.713744925968
Iteration 14500: Loss = -12291.72698395812
1
Iteration 14600: Loss = -12291.715381383521
2
Iteration 14700: Loss = -12291.713815949415
Iteration 14800: Loss = -12291.714073700472
1
Iteration 14900: Loss = -12291.713717557817
Iteration 15000: Loss = -12291.743067447265
1
Iteration 15100: Loss = -12291.727303869995
2
Iteration 15200: Loss = -12291.750130037437
3
Iteration 15300: Loss = -12291.71388301632
4
Iteration 15400: Loss = -12291.714688648866
5
Iteration 15500: Loss = -12291.713761599045
Iteration 15600: Loss = -12291.714233149647
1
Iteration 15700: Loss = -12291.713783065154
Iteration 15800: Loss = -12291.71389912736
1
Iteration 15900: Loss = -12291.71988079501
2
Iteration 16000: Loss = -12291.716449399066
3
Iteration 16100: Loss = -12291.713768920892
Iteration 16200: Loss = -12291.76229703267
1
Iteration 16300: Loss = -12291.71376504627
Iteration 16400: Loss = -12291.716101400507
1
Iteration 16500: Loss = -12291.713907534055
2
Iteration 16600: Loss = -12291.713977421545
3
Iteration 16700: Loss = -12291.723364333182
4
Iteration 16800: Loss = -12291.796714113963
5
Iteration 16900: Loss = -12291.715403133867
6
Iteration 17000: Loss = -12291.716930974295
7
Iteration 17100: Loss = -12291.722728223138
8
Iteration 17200: Loss = -12291.714121934867
9
Iteration 17300: Loss = -12291.802935627913
10
Iteration 17400: Loss = -12291.713732610011
Iteration 17500: Loss = -12291.716076049877
1
Iteration 17600: Loss = -12291.713722361454
Iteration 17700: Loss = -12291.714296398779
1
Iteration 17800: Loss = -12291.713715137466
Iteration 17900: Loss = -12291.71482657833
1
Iteration 18000: Loss = -12291.714170681946
2
Iteration 18100: Loss = -12291.71390791527
3
Iteration 18200: Loss = -12291.732746603133
4
Iteration 18300: Loss = -12291.715018705669
5
Iteration 18400: Loss = -12291.719945057128
6
Iteration 18500: Loss = -12291.716336052472
7
Iteration 18600: Loss = -12291.71399361098
8
Iteration 18700: Loss = -12291.721831196573
9
Iteration 18800: Loss = -12291.729551441776
10
Iteration 18900: Loss = -12291.72033202988
11
Iteration 19000: Loss = -12291.756547534156
12
Iteration 19100: Loss = -12291.74554850387
13
Iteration 19200: Loss = -12291.722624260268
14
Iteration 19300: Loss = -12291.717517093919
15
Stopping early at iteration 19300 due to no improvement.
pi: tensor([[0.0794, 0.9206],
        [0.2803, 0.7197]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9954, 0.0046], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1895, 0.2354],
         [0.6620, 0.2023]],

        [[0.5162, 0.2107],
         [0.6246, 0.6825]],

        [[0.5535, 0.2031],
         [0.6708, 0.7121]],

        [[0.5241, 0.1928],
         [0.6380, 0.5020]],

        [[0.5984, 0.1798],
         [0.6035, 0.5625]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007304767002590059
Average Adjusted Rand Index: 0.0
11786.806064709708
[-0.0007304767002590059, -0.0007304767002590059] [0.0, 0.0] [12291.762437197649, 12291.717517093919]
-------------------------------------
This iteration is 60
True Objective function: Loss = -11952.803607070342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21853.449020721513
Iteration 100: Loss = -12457.704990937713
Iteration 200: Loss = -12457.168359386254
Iteration 300: Loss = -12457.033821465187
Iteration 400: Loss = -12456.953801303627
Iteration 500: Loss = -12456.89990363411
Iteration 600: Loss = -12456.860733219484
Iteration 700: Loss = -12456.83096456533
Iteration 800: Loss = -12456.807712354468
Iteration 900: Loss = -12456.78923150449
Iteration 1000: Loss = -12456.774332206702
Iteration 1100: Loss = -12456.762149081396
Iteration 1200: Loss = -12456.75216524304
Iteration 1300: Loss = -12456.743835308152
Iteration 1400: Loss = -12456.73680368951
Iteration 1500: Loss = -12456.73085150253
Iteration 1600: Loss = -12456.725742953588
Iteration 1700: Loss = -12456.72142929938
Iteration 1800: Loss = -12456.717620792911
Iteration 1900: Loss = -12456.714332224352
Iteration 2000: Loss = -12456.711440640029
Iteration 2100: Loss = -12456.708883704583
Iteration 2200: Loss = -12456.706643385005
Iteration 2300: Loss = -12456.704693261243
Iteration 2400: Loss = -12456.702886380352
Iteration 2500: Loss = -12456.70125686672
Iteration 2600: Loss = -12456.69978420424
Iteration 2700: Loss = -12456.698433363728
Iteration 2800: Loss = -12456.697264242044
Iteration 2900: Loss = -12456.696167427208
Iteration 3000: Loss = -12456.695103741811
Iteration 3100: Loss = -12456.69415775786
Iteration 3200: Loss = -12456.693204439827
Iteration 3300: Loss = -12456.692383159147
Iteration 3400: Loss = -12456.691578191629
Iteration 3500: Loss = -12456.690729712152
Iteration 3600: Loss = -12456.689944222122
Iteration 3700: Loss = -12456.689103724835
Iteration 3800: Loss = -12456.688163979816
Iteration 3900: Loss = -12456.687117649646
Iteration 4000: Loss = -12456.685882180596
Iteration 4100: Loss = -12456.684466409757
Iteration 4200: Loss = -12456.682897757
Iteration 4300: Loss = -12456.681380314665
Iteration 4400: Loss = -12456.680085140386
Iteration 4500: Loss = -12456.678878710865
Iteration 4600: Loss = -12456.677827701056
Iteration 4700: Loss = -12456.676884283412
Iteration 4800: Loss = -12456.676007142147
Iteration 4900: Loss = -12456.675152385942
Iteration 5000: Loss = -12456.674409445915
Iteration 5100: Loss = -12456.67375126507
Iteration 5200: Loss = -12456.673045807833
Iteration 5300: Loss = -12456.672378143903
Iteration 5400: Loss = -12456.671701267092
Iteration 5500: Loss = -12456.671007259012
Iteration 5600: Loss = -12456.6702436184
Iteration 5700: Loss = -12456.669449830633
Iteration 5800: Loss = -12456.668508249757
Iteration 5900: Loss = -12456.667382512707
Iteration 6000: Loss = -12456.666034559643
Iteration 6100: Loss = -12456.664349603363
Iteration 6200: Loss = -12456.66202755697
Iteration 6300: Loss = -12456.658768293702
Iteration 6400: Loss = -12456.653670827776
Iteration 6500: Loss = -12456.643770821122
Iteration 6600: Loss = -12456.61028901944
Iteration 6700: Loss = -12456.498516757403
Iteration 6800: Loss = -12456.43917572436
Iteration 6900: Loss = -12456.40098258743
Iteration 7000: Loss = -12456.37220829539
Iteration 7100: Loss = -12456.349830584977
Iteration 7200: Loss = -12456.334331007714
Iteration 7300: Loss = -12456.323295845445
Iteration 7400: Loss = -12456.315282853613
Iteration 7500: Loss = -12456.309170551043
Iteration 7600: Loss = -12456.304480349154
Iteration 7700: Loss = -12456.300864554974
Iteration 7800: Loss = -12456.297814597232
Iteration 7900: Loss = -12456.295306379736
Iteration 8000: Loss = -12456.297562493308
1
Iteration 8100: Loss = -12456.291589828961
Iteration 8200: Loss = -12456.290085194116
Iteration 8300: Loss = -12456.288831063668
Iteration 8400: Loss = -12456.287824661353
Iteration 8500: Loss = -12456.286716053892
Iteration 8600: Loss = -12456.285865117852
Iteration 8700: Loss = -12456.2866489103
1
Iteration 8800: Loss = -12456.284352576708
Iteration 8900: Loss = -12456.283763656049
Iteration 9000: Loss = -12456.308540743683
1
Iteration 9100: Loss = -12456.282592951358
Iteration 9200: Loss = -12456.282036338374
Iteration 9300: Loss = -12456.282242620024
1
Iteration 9400: Loss = -12456.279693745139
Iteration 9500: Loss = -12456.235257424281
Iteration 9600: Loss = -12454.089115072098
Iteration 9700: Loss = -12454.080167465507
Iteration 9800: Loss = -12454.098848202675
1
Iteration 9900: Loss = -12454.075412307602
Iteration 10000: Loss = -12454.074308740615
Iteration 10100: Loss = -12454.073565299546
Iteration 10200: Loss = -12454.07316420688
Iteration 10300: Loss = -12454.078376935475
1
Iteration 10400: Loss = -12454.072313840894
Iteration 10500: Loss = -12454.071961914864
Iteration 10600: Loss = -12454.073541860784
1
Iteration 10700: Loss = -12454.071916057108
Iteration 10800: Loss = -12454.07353278191
1
Iteration 10900: Loss = -12454.071851828961
Iteration 11000: Loss = -12454.072136606605
1
Iteration 11100: Loss = -12454.070945595053
Iteration 11200: Loss = -12454.071389663406
1
Iteration 11300: Loss = -12454.070710892176
Iteration 11400: Loss = -12454.071023631052
1
Iteration 11500: Loss = -12454.070530587984
Iteration 11600: Loss = -12454.093448810463
1
Iteration 11700: Loss = -12454.070386940351
Iteration 11800: Loss = -12454.072080848991
1
Iteration 11900: Loss = -12454.07980912862
2
Iteration 12000: Loss = -12454.183696285892
3
Iteration 12100: Loss = -12454.078577023214
4
Iteration 12200: Loss = -12454.070146407326
Iteration 12300: Loss = -12454.073665350024
1
Iteration 12400: Loss = -12454.070100059034
Iteration 12500: Loss = -12454.071985622782
1
Iteration 12600: Loss = -12454.070022293341
Iteration 12700: Loss = -12454.073411370826
1
Iteration 12800: Loss = -12454.145321683367
2
Iteration 12900: Loss = -12454.07078338847
3
Iteration 13000: Loss = -12454.070683333652
4
Iteration 13100: Loss = -12454.070061381326
Iteration 13200: Loss = -12454.073897696835
1
Iteration 13300: Loss = -12454.070751231213
2
Iteration 13400: Loss = -12454.07031571576
3
Iteration 13500: Loss = -12454.072274020946
4
Iteration 13600: Loss = -12454.069757614729
Iteration 13700: Loss = -12454.069896757346
1
Iteration 13800: Loss = -12454.073785215225
2
Iteration 13900: Loss = -12454.069708219882
Iteration 14000: Loss = -12454.070127467081
1
Iteration 14100: Loss = -12454.074769432165
2
Iteration 14200: Loss = -12454.06985080009
3
Iteration 14300: Loss = -12454.071378908506
4
Iteration 14400: Loss = -12454.073131365723
5
Iteration 14500: Loss = -12454.074641846044
6
Iteration 14600: Loss = -12454.069673013904
Iteration 14700: Loss = -12454.093649477181
1
Iteration 14800: Loss = -12454.0696403242
Iteration 14900: Loss = -12454.070581734211
1
Iteration 15000: Loss = -12454.070731101401
2
Iteration 15100: Loss = -12454.090081598804
3
Iteration 15200: Loss = -12454.07049594049
4
Iteration 15300: Loss = -12454.077931580772
5
Iteration 15400: Loss = -12454.073879606667
6
Iteration 15500: Loss = -12454.069638842073
Iteration 15600: Loss = -12454.070145770096
1
Iteration 15700: Loss = -12454.071753900413
2
Iteration 15800: Loss = -12454.06987476205
3
Iteration 15900: Loss = -12454.071882682472
4
Iteration 16000: Loss = -12454.091086168068
5
Iteration 16100: Loss = -12454.06977985904
6
Iteration 16200: Loss = -12454.136693891933
7
Iteration 16300: Loss = -12454.069593251865
Iteration 16400: Loss = -12454.069680845267
Iteration 16500: Loss = -12454.069763839314
Iteration 16600: Loss = -12454.23213022325
1
Iteration 16700: Loss = -12454.071234738185
2
Iteration 16800: Loss = -12454.069692522733
Iteration 16900: Loss = -12454.07378121518
1
Iteration 17000: Loss = -12454.072182937485
2
Iteration 17100: Loss = -12454.07028563865
3
Iteration 17200: Loss = -12454.069697858642
Iteration 17300: Loss = -12454.074331954524
1
Iteration 17400: Loss = -12454.378760174854
2
Iteration 17500: Loss = -12454.069564590616
Iteration 17600: Loss = -12454.071955223093
1
Iteration 17700: Loss = -12454.29030050349
2
Iteration 17800: Loss = -12454.06951889755
Iteration 17900: Loss = -12454.084706295447
1
Iteration 18000: Loss = -12454.06952692523
Iteration 18100: Loss = -12454.078673115135
1
Iteration 18200: Loss = -12454.069556132865
Iteration 18300: Loss = -12454.069951519312
1
Iteration 18400: Loss = -12454.116491331504
2
Iteration 18500: Loss = -12454.074739680778
3
Iteration 18600: Loss = -12454.117109536852
4
Iteration 18700: Loss = -12454.069844151501
5
Iteration 18800: Loss = -12454.07082237253
6
Iteration 18900: Loss = -12454.069627322342
Iteration 19000: Loss = -12454.069734654082
1
Iteration 19100: Loss = -12454.074955125538
2
Iteration 19200: Loss = -12454.07045417177
3
Iteration 19300: Loss = -12454.069626949477
Iteration 19400: Loss = -12454.089754052544
1
Iteration 19500: Loss = -12454.07072207266
2
Iteration 19600: Loss = -12454.069640816719
Iteration 19700: Loss = -12454.073476931382
1
Iteration 19800: Loss = -12454.243534744726
2
Iteration 19900: Loss = -12454.070398973055
3
pi: tensor([[1.0000e+00, 3.1906e-07],
        [9.5472e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0586, 0.9414], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0184, 0.2247],
         [0.6088, 0.2003]],

        [[0.6850, 0.1799],
         [0.5779, 0.7177]],

        [[0.7081, 0.1900],
         [0.6157, 0.7236]],

        [[0.5073, 0.2484],
         [0.6677, 0.6188]],

        [[0.6607, 0.2644],
         [0.6416, 0.6461]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0049666994303792225
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0019209323236274677
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0014658390783536795
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006803382252891437
Global Adjusted Rand Index: 0.0008318990318478607
Average Adjusted Rand Index: 0.0003829159729329157
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22893.247840101183
Iteration 100: Loss = -12457.492425832284
Iteration 200: Loss = -12456.899066478438
Iteration 300: Loss = -12456.809770790183
Iteration 400: Loss = -12456.770635387978
Iteration 500: Loss = -12456.7486020003
Iteration 600: Loss = -12456.734636551279
Iteration 700: Loss = -12456.725255273765
Iteration 800: Loss = -12456.718627438393
Iteration 900: Loss = -12456.713750693056
Iteration 1000: Loss = -12456.710146021336
Iteration 1100: Loss = -12456.707335050985
Iteration 1200: Loss = -12456.705071821258
Iteration 1300: Loss = -12456.703284791325
Iteration 1400: Loss = -12456.701827774184
Iteration 1500: Loss = -12456.700560415728
Iteration 1600: Loss = -12456.699394414989
Iteration 1700: Loss = -12456.698400378116
Iteration 1800: Loss = -12456.697485860852
Iteration 1900: Loss = -12456.696669634137
Iteration 2000: Loss = -12456.695862602464
Iteration 2100: Loss = -12456.695064804184
Iteration 2200: Loss = -12456.694267265771
Iteration 2300: Loss = -12456.693507939994
Iteration 2400: Loss = -12456.69268893179
Iteration 2500: Loss = -12456.691844785433
Iteration 2600: Loss = -12456.690918613722
Iteration 2700: Loss = -12456.689914831946
Iteration 2800: Loss = -12456.688858629099
Iteration 2900: Loss = -12456.68754932212
Iteration 3000: Loss = -12456.68610551209
Iteration 3100: Loss = -12456.684303719703
Iteration 3200: Loss = -12456.682002368774
Iteration 3300: Loss = -12456.6788766516
Iteration 3400: Loss = -12456.674160299137
Iteration 3500: Loss = -12456.665945999011
Iteration 3600: Loss = -12456.648629965895
Iteration 3700: Loss = -12456.60911802289
Iteration 3800: Loss = -12456.576375072977
Iteration 3900: Loss = -12456.557433037231
Iteration 4000: Loss = -12456.55877186538
1
Iteration 4100: Loss = -12456.539981474461
Iteration 4200: Loss = -12456.536889917903
Iteration 4300: Loss = -12456.532911059176
Iteration 4400: Loss = -12456.531830038528
Iteration 4500: Loss = -12456.52923890807
Iteration 4600: Loss = -12456.528049904064
Iteration 4700: Loss = -12456.52697262305
Iteration 4800: Loss = -12456.525979275066
Iteration 4900: Loss = -12456.525672772084
Iteration 5000: Loss = -12456.524866277077
Iteration 5100: Loss = -12456.527754560571
1
Iteration 5200: Loss = -12456.524132064993
Iteration 5300: Loss = -12456.523772868377
Iteration 5400: Loss = -12456.523698596557
Iteration 5500: Loss = -12456.523358471353
Iteration 5600: Loss = -12456.56764562866
1
Iteration 5700: Loss = -12456.523020673985
Iteration 5800: Loss = -12456.522834815834
Iteration 5900: Loss = -12456.522790483854
Iteration 6000: Loss = -12456.522601515479
Iteration 6100: Loss = -12456.572144489815
1
Iteration 6200: Loss = -12456.522404719872
Iteration 6300: Loss = -12456.522261817612
Iteration 6400: Loss = -12456.522364456814
1
Iteration 6500: Loss = -12456.522163858857
Iteration 6600: Loss = -12456.522058372717
Iteration 6700: Loss = -12456.52207655385
Iteration 6800: Loss = -12456.521968179299
Iteration 6900: Loss = -12456.566244998032
1
Iteration 7000: Loss = -12456.52193509574
Iteration 7100: Loss = -12456.521852040152
Iteration 7200: Loss = -12456.521910718877
Iteration 7300: Loss = -12456.521796739677
Iteration 7400: Loss = -12456.529140471264
1
Iteration 7500: Loss = -12456.52177178343
Iteration 7600: Loss = -12456.521653128513
Iteration 7700: Loss = -12456.52170040376
Iteration 7800: Loss = -12456.521602917675
Iteration 7900: Loss = -12456.524517095915
1
Iteration 8000: Loss = -12456.521635875226
Iteration 8100: Loss = -12456.521598796826
Iteration 8200: Loss = -12456.5218261603
1
Iteration 8300: Loss = -12456.550181795406
2
Iteration 8400: Loss = -12456.521539226433
Iteration 8500: Loss = -12456.536957169494
1
Iteration 8600: Loss = -12456.52159745377
Iteration 8700: Loss = -12456.612502034337
1
Iteration 8800: Loss = -12456.521514356016
Iteration 8900: Loss = -12456.524714214542
1
Iteration 9000: Loss = -12456.521517006962
Iteration 9100: Loss = -12456.550967165182
1
Iteration 9200: Loss = -12456.521514885419
Iteration 9300: Loss = -12456.614095775532
1
Iteration 9400: Loss = -12456.521466380924
Iteration 9500: Loss = -12456.52367049235
1
Iteration 9600: Loss = -12456.522330948006
2
Iteration 9700: Loss = -12456.55696273004
3
Iteration 9800: Loss = -12456.521525262242
Iteration 9900: Loss = -12456.521475073043
Iteration 10000: Loss = -12456.523004848365
1
Iteration 10100: Loss = -12456.59231379923
2
Iteration 10200: Loss = -12456.521779066323
3
Iteration 10300: Loss = -12456.521541579112
Iteration 10400: Loss = -12456.541895129538
1
Iteration 10500: Loss = -12456.52144513317
Iteration 10600: Loss = -12456.522732581154
1
Iteration 10700: Loss = -12456.521507778882
Iteration 10800: Loss = -12456.52386670115
1
Iteration 10900: Loss = -12456.521702835034
2
Iteration 11000: Loss = -12456.773992497705
3
Iteration 11100: Loss = -12456.521447470794
Iteration 11200: Loss = -12456.521426626605
Iteration 11300: Loss = -12456.521957047977
1
Iteration 11400: Loss = -12456.52140360462
Iteration 11500: Loss = -12456.89833715777
1
Iteration 11600: Loss = -12456.521407147413
Iteration 11700: Loss = -12456.660525846431
1
Iteration 11800: Loss = -12456.521446493905
Iteration 11900: Loss = -12456.525439933674
1
Iteration 12000: Loss = -12456.521456316808
Iteration 12100: Loss = -12456.521349374238
Iteration 12200: Loss = -12456.784293783527
1
Iteration 12300: Loss = -12456.521411952715
Iteration 12400: Loss = -12456.672269183824
1
Iteration 12500: Loss = -12456.521400154868
Iteration 12600: Loss = -12456.563992894371
1
Iteration 12700: Loss = -12456.521378436724
Iteration 12800: Loss = -12456.528956369879
1
Iteration 12900: Loss = -12456.559217889677
2
Iteration 13000: Loss = -12456.521404353996
Iteration 13100: Loss = -12456.521453423775
Iteration 13200: Loss = -12456.521820302778
1
Iteration 13300: Loss = -12456.521827041624
2
Iteration 13400: Loss = -12456.521681112097
3
Iteration 13500: Loss = -12456.521451547498
Iteration 13600: Loss = -12456.521493216087
Iteration 13700: Loss = -12456.521574359027
Iteration 13800: Loss = -12456.524269745016
1
Iteration 13900: Loss = -12456.521504744476
Iteration 14000: Loss = -12456.52171224448
1
Iteration 14100: Loss = -12456.53534311075
2
Iteration 14200: Loss = -12456.52426927471
3
Iteration 14300: Loss = -12456.521332263044
Iteration 14400: Loss = -12456.521598529504
1
Iteration 14500: Loss = -12456.875755974814
2
Iteration 14600: Loss = -12456.521403061855
Iteration 14700: Loss = -12456.521872753156
1
Iteration 14800: Loss = -12456.52146276754
Iteration 14900: Loss = -12456.521356084677
Iteration 15000: Loss = -12456.52143599702
Iteration 15100: Loss = -12456.521529906053
Iteration 15200: Loss = -12456.521394965555
Iteration 15300: Loss = -12456.521366376599
Iteration 15400: Loss = -12456.521474011064
1
Iteration 15500: Loss = -12456.522225529125
2
Iteration 15600: Loss = -12456.640312923384
3
Iteration 15700: Loss = -12456.521398487297
Iteration 15800: Loss = -12456.54961672065
1
Iteration 15900: Loss = -12456.521332161692
Iteration 16000: Loss = -12456.52168714177
1
Iteration 16100: Loss = -12456.521368957567
Iteration 16200: Loss = -12456.521663873895
1
Iteration 16300: Loss = -12456.523339560355
2
Iteration 16400: Loss = -12456.521499793853
3
Iteration 16500: Loss = -12456.522033184669
4
Iteration 16600: Loss = -12456.522249838476
5
Iteration 16700: Loss = -12456.521493521268
6
Iteration 16800: Loss = -12456.638380005259
7
Iteration 16900: Loss = -12456.521422506903
Iteration 17000: Loss = -12456.52179955062
1
Iteration 17100: Loss = -12456.521363328036
Iteration 17200: Loss = -12456.521509825585
1
Iteration 17300: Loss = -12456.521387858278
Iteration 17400: Loss = -12456.521614133244
1
Iteration 17500: Loss = -12456.5213657171
Iteration 17600: Loss = -12456.58015162806
1
Iteration 17700: Loss = -12456.521436166175
Iteration 17800: Loss = -12456.523360495486
1
Iteration 17900: Loss = -12456.612046991286
2
Iteration 18000: Loss = -12456.521371986202
Iteration 18100: Loss = -12456.61207158607
1
Iteration 18200: Loss = -12456.558184316418
2
Iteration 18300: Loss = -12456.521560694253
3
Iteration 18400: Loss = -12456.52138457236
Iteration 18500: Loss = -12456.52231916668
1
Iteration 18600: Loss = -12456.52136273666
Iteration 18700: Loss = -12456.660644302454
1
Iteration 18800: Loss = -12456.521360256482
Iteration 18900: Loss = -12456.54476791481
1
Iteration 19000: Loss = -12456.525732796135
2
Iteration 19100: Loss = -12456.521441349581
Iteration 19200: Loss = -12456.521712129914
1
Iteration 19300: Loss = -12456.526477354992
2
Iteration 19400: Loss = -12456.530464218964
3
Iteration 19500: Loss = -12456.521503006808
Iteration 19600: Loss = -12456.539900545817
1
Iteration 19700: Loss = -12456.521384776233
Iteration 19800: Loss = -12456.539353869453
1
Iteration 19900: Loss = -12456.521347016896
pi: tensor([[9.9994e-01, 5.8539e-05],
        [7.0902e-01, 2.9098e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.4662e-04, 9.9905e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2032, 0.1993],
         [0.5704, 0.1992]],

        [[0.5967, 0.2008],
         [0.6842, 0.5383]],

        [[0.5406, 0.2033],
         [0.5855, 0.5065]],

        [[0.6402, 0.1996],
         [0.6398, 0.6395]],

        [[0.5047, 0.2051],
         [0.6472, 0.7213]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011457807238060699
Average Adjusted Rand Index: 0.0
11952.803607070342
[0.0008318990318478607, -0.0011457807238060699] [0.0003829159729329157, 0.0] [12454.069703766754, 12456.521603444731]
-------------------------------------
This iteration is 61
True Objective function: Loss = -11894.842566951182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24342.920247292426
Iteration 100: Loss = -11893.77967495297
Iteration 200: Loss = -11890.924017292547
Iteration 300: Loss = -11890.544351612205
Iteration 400: Loss = -11890.385931264525
Iteration 500: Loss = -11890.302844560181
Iteration 600: Loss = -11890.252832204877
Iteration 700: Loss = -11890.220088611857
Iteration 800: Loss = -11890.197389864394
Iteration 900: Loss = -11890.18094665299
Iteration 1000: Loss = -11890.16854399502
Iteration 1100: Loss = -11890.159040362385
Iteration 1200: Loss = -11890.151509696656
Iteration 1300: Loss = -11890.145482403685
Iteration 1400: Loss = -11890.140567698345
Iteration 1500: Loss = -11890.136525629492
Iteration 1600: Loss = -11890.133120013643
Iteration 1700: Loss = -11890.130313426504
Iteration 1800: Loss = -11890.127852382588
Iteration 1900: Loss = -11890.1257838003
Iteration 2000: Loss = -11890.12395773144
Iteration 2100: Loss = -11890.12233531589
Iteration 2200: Loss = -11890.12099277445
Iteration 2300: Loss = -11890.119764460602
Iteration 2400: Loss = -11890.119227326677
Iteration 2500: Loss = -11890.11768499902
Iteration 2600: Loss = -11890.11682954496
Iteration 2700: Loss = -11890.116034431585
Iteration 2800: Loss = -11890.115367377715
Iteration 2900: Loss = -11890.115136811428
Iteration 3000: Loss = -11890.114191617451
Iteration 3100: Loss = -11890.113673967324
Iteration 3200: Loss = -11890.113330717282
Iteration 3300: Loss = -11890.112782084443
Iteration 3400: Loss = -11890.112358941813
Iteration 3500: Loss = -11890.11446532378
1
Iteration 3600: Loss = -11890.111703539804
Iteration 3700: Loss = -11890.111384478121
Iteration 3800: Loss = -11890.111107636732
Iteration 3900: Loss = -11890.110855189998
Iteration 4000: Loss = -11890.110624881412
Iteration 4100: Loss = -11890.110375527129
Iteration 4200: Loss = -11890.110521317189
1
Iteration 4300: Loss = -11890.109985754094
Iteration 4400: Loss = -11890.109808583222
Iteration 4500: Loss = -11890.109687523249
Iteration 4600: Loss = -11890.10954502062
Iteration 4700: Loss = -11890.10941398924
Iteration 4800: Loss = -11890.109253448527
Iteration 4900: Loss = -11890.109115328894
Iteration 5000: Loss = -11890.109070529583
Iteration 5100: Loss = -11890.108894589639
Iteration 5200: Loss = -11890.108848354226
Iteration 5300: Loss = -11890.119973656854
1
Iteration 5400: Loss = -11890.10861955769
Iteration 5500: Loss = -11890.10956507929
1
Iteration 5600: Loss = -11890.108454541503
Iteration 5700: Loss = -11890.109087522278
1
Iteration 5800: Loss = -11890.108297702078
Iteration 5900: Loss = -11890.108595613563
1
Iteration 6000: Loss = -11890.108185562833
Iteration 6100: Loss = -11890.108431840905
1
Iteration 6200: Loss = -11890.108281531939
Iteration 6300: Loss = -11890.108058162086
Iteration 6400: Loss = -11890.113910349372
1
Iteration 6500: Loss = -11890.107958408009
Iteration 6600: Loss = -11890.107923853466
Iteration 6700: Loss = -11890.107877757911
Iteration 6800: Loss = -11890.107833464417
Iteration 6900: Loss = -11890.109595132766
1
Iteration 7000: Loss = -11890.107948849132
2
Iteration 7100: Loss = -11890.114218510324
3
Iteration 7200: Loss = -11890.107700046723
Iteration 7300: Loss = -11890.107747029035
Iteration 7400: Loss = -11890.107632777073
Iteration 7500: Loss = -11890.107936176768
1
Iteration 7600: Loss = -11890.107610438175
Iteration 7700: Loss = -11890.110172493429
1
Iteration 7800: Loss = -11890.10765067741
Iteration 7900: Loss = -11890.10760361091
Iteration 8000: Loss = -11890.10750246899
Iteration 8100: Loss = -11890.107504627944
Iteration 8200: Loss = -11890.112757097299
1
Iteration 8300: Loss = -11890.107580846437
Iteration 8400: Loss = -11890.107514394875
Iteration 8500: Loss = -11890.108008814988
1
Iteration 8600: Loss = -11890.107423060155
Iteration 8700: Loss = -11890.107707992509
1
Iteration 8800: Loss = -11890.162926987448
2
Iteration 8900: Loss = -11890.10784629286
3
Iteration 9000: Loss = -11890.107653010295
4
Iteration 9100: Loss = -11890.108707313795
5
Iteration 9200: Loss = -11890.107404138389
Iteration 9300: Loss = -11890.10770501501
1
Iteration 9400: Loss = -11890.114272780209
2
Iteration 9500: Loss = -11890.147204623425
3
Iteration 9600: Loss = -11890.107363029758
Iteration 9700: Loss = -11890.110492933034
1
Iteration 9800: Loss = -11890.109284879518
2
Iteration 9900: Loss = -11890.107778189844
3
Iteration 10000: Loss = -11890.119570305913
4
Iteration 10100: Loss = -11890.112054881522
5
Iteration 10200: Loss = -11890.151328895354
6
Iteration 10300: Loss = -11890.16386481646
7
Iteration 10400: Loss = -11890.115650977748
8
Iteration 10500: Loss = -11890.107518676445
9
Iteration 10600: Loss = -11890.110273025368
10
Iteration 10700: Loss = -11890.107563879501
11
Iteration 10800: Loss = -11890.298011624935
12
Iteration 10900: Loss = -11890.10782967851
13
Iteration 11000: Loss = -11890.108796692642
14
Iteration 11100: Loss = -11890.107536422121
15
Stopping early at iteration 11100 due to no improvement.
pi: tensor([[0.7504, 0.2496],
        [0.2368, 0.7632]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4762, 0.5238], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3066, 0.0970],
         [0.7234, 0.2985]],

        [[0.5955, 0.1062],
         [0.7094, 0.6023]],

        [[0.7103, 0.0965],
         [0.7221, 0.7237]],

        [[0.7042, 0.0991],
         [0.6624, 0.7082]],

        [[0.7062, 0.1004],
         [0.5892, 0.6060]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9760961002009502
Average Adjusted Rand Index: 0.9759964884296097
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20574.7964047623
Iteration 100: Loss = -12426.897716648695
Iteration 200: Loss = -12426.567425193027
Iteration 300: Loss = -12426.500178700939
Iteration 400: Loss = -12426.463378667037
Iteration 500: Loss = -12426.434197971415
Iteration 600: Loss = -12426.390147357039
Iteration 700: Loss = -12426.262697053377
Iteration 800: Loss = -12425.941254618134
Iteration 900: Loss = -12425.74433480745
Iteration 1000: Loss = -12425.662791020355
Iteration 1100: Loss = -12425.608210338462
Iteration 1200: Loss = -12425.56582844534
Iteration 1300: Loss = -12425.533229777988
Iteration 1400: Loss = -12425.51085460756
Iteration 1500: Loss = -12425.495844918225
Iteration 1600: Loss = -12425.48489357153
Iteration 1700: Loss = -12425.476237637215
Iteration 1800: Loss = -12425.469140551273
Iteration 1900: Loss = -12425.46316804079
Iteration 2000: Loss = -12425.458049047364
Iteration 2100: Loss = -12425.45356891776
Iteration 2200: Loss = -12425.449686193746
Iteration 2300: Loss = -12425.446134433321
Iteration 2400: Loss = -12425.442804967066
Iteration 2500: Loss = -12425.439540459842
Iteration 2600: Loss = -12425.4362547299
Iteration 2700: Loss = -12425.432913935507
Iteration 2800: Loss = -12425.429465190447
Iteration 2900: Loss = -12425.425883924048
Iteration 3000: Loss = -12425.421934048516
Iteration 3100: Loss = -12425.417409414205
Iteration 3200: Loss = -12425.411928258547
Iteration 3300: Loss = -12425.405016802968
Iteration 3400: Loss = -12425.395993109052
Iteration 3500: Loss = -12425.38250765061
Iteration 3600: Loss = -12425.358472345786
Iteration 3700: Loss = -12425.293124791111
Iteration 3800: Loss = -12425.168318273678
Iteration 3900: Loss = -12425.034455692581
Iteration 4000: Loss = -12424.59099987038
Iteration 4100: Loss = -12424.336928689872
Iteration 4200: Loss = -12424.259943343823
Iteration 4300: Loss = -12424.223922791693
Iteration 4400: Loss = -12424.203419972871
Iteration 4500: Loss = -12424.190314573494
Iteration 4600: Loss = -12424.181284801563
Iteration 4700: Loss = -12424.17474601243
Iteration 4800: Loss = -12424.16984378413
Iteration 4900: Loss = -12424.166004005767
Iteration 5000: Loss = -12424.163050504325
Iteration 5100: Loss = -12424.160561979235
Iteration 5200: Loss = -12424.15854178781
Iteration 5300: Loss = -12424.156866753221
Iteration 5400: Loss = -12424.155425784491
Iteration 5500: Loss = -12424.154218485008
Iteration 5600: Loss = -12424.15318659902
Iteration 5700: Loss = -12424.152245951936
Iteration 5800: Loss = -12424.151431855136
Iteration 5900: Loss = -12424.150692186899
Iteration 6000: Loss = -12424.1500821177
Iteration 6100: Loss = -12424.149495200136
Iteration 6200: Loss = -12424.148997371578
Iteration 6300: Loss = -12424.148536443001
Iteration 6400: Loss = -12424.148161785666
Iteration 6500: Loss = -12424.147755550353
Iteration 6600: Loss = -12424.147408358858
Iteration 6700: Loss = -12424.147122643002
Iteration 6800: Loss = -12424.1468625194
Iteration 6900: Loss = -12424.146602793708
Iteration 7000: Loss = -12424.14633061158
Iteration 7100: Loss = -12424.146104946363
Iteration 7200: Loss = -12424.150529130104
1
Iteration 7300: Loss = -12424.145695830272
Iteration 7400: Loss = -12424.145533527932
Iteration 7500: Loss = -12424.19140572184
1
Iteration 7600: Loss = -12424.145242400991
Iteration 7700: Loss = -12424.1450976962
Iteration 7800: Loss = -12424.144957374483
Iteration 7900: Loss = -12424.145949406278
1
Iteration 8000: Loss = -12424.144737804047
Iteration 8100: Loss = -12424.144619960869
Iteration 8200: Loss = -12424.14449979326
Iteration 8300: Loss = -12424.336990375674
1
Iteration 8400: Loss = -12424.144354515533
Iteration 8500: Loss = -12424.1442781062
Iteration 8600: Loss = -12424.144158559408
Iteration 8700: Loss = -12424.14510248191
1
Iteration 8800: Loss = -12424.144059345017
Iteration 8900: Loss = -12424.143964989627
Iteration 9000: Loss = -12424.143876477787
Iteration 9100: Loss = -12424.143887382654
Iteration 9200: Loss = -12424.143699723914
Iteration 9300: Loss = -12424.143664721925
Iteration 9400: Loss = -12424.143603777928
Iteration 9500: Loss = -12424.143703023645
Iteration 9600: Loss = -12424.143514978341
Iteration 9700: Loss = -12424.143472030119
Iteration 9800: Loss = -12424.143419531312
Iteration 9900: Loss = -12424.14359119069
1
Iteration 10000: Loss = -12424.143385824123
Iteration 10100: Loss = -12424.143319022774
Iteration 10200: Loss = -12424.255167392555
1
Iteration 10300: Loss = -12424.143284153786
Iteration 10400: Loss = -12424.143236852064
Iteration 10500: Loss = -12424.143187757472
Iteration 10600: Loss = -12424.145307633939
1
Iteration 10700: Loss = -12424.143215537344
Iteration 10800: Loss = -12424.143129873304
Iteration 10900: Loss = -12424.143142240138
Iteration 11000: Loss = -12424.15997425348
1
Iteration 11100: Loss = -12424.143078864388
Iteration 11200: Loss = -12424.143089947105
Iteration 11300: Loss = -12424.255599673874
1
Iteration 11400: Loss = -12424.142984099772
Iteration 11500: Loss = -12424.142990217339
Iteration 11600: Loss = -12424.142952792272
Iteration 11700: Loss = -12424.1441673805
1
Iteration 11800: Loss = -12424.142937472161
Iteration 11900: Loss = -12424.142965296473
Iteration 12000: Loss = -12424.737851796453
1
Iteration 12100: Loss = -12424.142922008434
Iteration 12200: Loss = -12424.142864730595
Iteration 12300: Loss = -12424.142883247854
Iteration 12400: Loss = -12424.143533173703
1
Iteration 12500: Loss = -12424.142877402335
Iteration 12600: Loss = -12424.142872214286
Iteration 12700: Loss = -12424.170984885972
1
Iteration 12800: Loss = -12424.142841682837
Iteration 12900: Loss = -12424.142836312169
Iteration 13000: Loss = -12424.142803398516
Iteration 13100: Loss = -12424.142882116983
Iteration 13200: Loss = -12424.142849896423
Iteration 13300: Loss = -12424.142803022503
Iteration 13400: Loss = -12424.279029804109
1
Iteration 13500: Loss = -12424.142784325977
Iteration 13600: Loss = -12424.142772072346
Iteration 13700: Loss = -12424.14281825373
Iteration 13800: Loss = -12424.150686529489
1
Iteration 13900: Loss = -12424.142842345114
Iteration 14000: Loss = -12424.14315919034
1
Iteration 14100: Loss = -12424.20790765637
2
Iteration 14200: Loss = -12424.144048933369
3
Iteration 14300: Loss = -12424.252106073074
4
Iteration 14400: Loss = -12424.142953072938
5
Iteration 14500: Loss = -12424.212698411346
6
Iteration 14600: Loss = -12424.143135691742
7
Iteration 14700: Loss = -12424.404158946474
8
Iteration 14800: Loss = -12424.144724320111
9
Iteration 14900: Loss = -12424.149517238695
10
Iteration 15000: Loss = -12424.142605450874
Iteration 15100: Loss = -12424.142901517907
1
Iteration 15200: Loss = -12424.14342346664
2
Iteration 15300: Loss = -12424.143542924925
3
Iteration 15400: Loss = -12424.14408602465
4
Iteration 15500: Loss = -12424.142558774056
Iteration 15600: Loss = -12424.144078333402
1
Iteration 15700: Loss = -12424.142536133724
Iteration 15800: Loss = -12424.142980917057
1
Iteration 15900: Loss = -12424.142561733353
Iteration 16000: Loss = -12424.144529314422
1
Iteration 16100: Loss = -12424.143874920417
2
Iteration 16200: Loss = -12424.142818481569
3
Iteration 16300: Loss = -12424.144125304583
4
Iteration 16400: Loss = -12424.14264123666
Iteration 16500: Loss = -12424.144905719866
1
Iteration 16600: Loss = -12424.142536809872
Iteration 16700: Loss = -12424.144625579336
1
Iteration 16800: Loss = -12424.142517987142
Iteration 16900: Loss = -12424.452610385068
1
Iteration 17000: Loss = -12424.142755631614
2
Iteration 17100: Loss = -12424.168671677244
3
Iteration 17200: Loss = -12424.143232563261
4
Iteration 17300: Loss = -12424.147354435592
5
Iteration 17400: Loss = -12424.142937162573
6
Iteration 17500: Loss = -12424.142700732991
7
Iteration 17600: Loss = -12424.143028911305
8
Iteration 17700: Loss = -12424.142619095186
9
Iteration 17800: Loss = -12424.142917591076
10
Iteration 17900: Loss = -12424.143146540973
11
Iteration 18000: Loss = -12424.146216880052
12
Iteration 18100: Loss = -12424.14265812472
13
Iteration 18200: Loss = -12424.14274279584
14
Iteration 18300: Loss = -12424.142567488794
Iteration 18400: Loss = -12424.143569177224
1
Iteration 18500: Loss = -12424.1427371572
2
Iteration 18600: Loss = -12424.142399935463
Iteration 18700: Loss = -12424.148804147962
1
Iteration 18800: Loss = -12424.142482247002
Iteration 18900: Loss = -12424.143335177549
1
Iteration 19000: Loss = -12424.142432528164
Iteration 19100: Loss = -12424.144441085216
1
Iteration 19200: Loss = -12424.142629068485
2
Iteration 19300: Loss = -12424.175993591913
3
Iteration 19400: Loss = -12424.14270536396
4
Iteration 19500: Loss = -12424.158000647965
5
Iteration 19600: Loss = -12424.143556893803
6
Iteration 19700: Loss = -12424.145087547358
7
Iteration 19800: Loss = -12424.142887553626
8
Iteration 19900: Loss = -12424.14608651558
9
pi: tensor([[1.0000e+00, 1.2451e-06],
        [1.0158e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0181, 0.9819], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1866, 0.1698],
         [0.6490, 0.2023]],

        [[0.6438, 0.2147],
         [0.5908, 0.6674]],

        [[0.5016, 0.2241],
         [0.5370, 0.7145]],

        [[0.6453, 0.0881],
         [0.5865, 0.6422]],

        [[0.6710, 0.1656],
         [0.5431, 0.6285]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.00035341041046094813
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0012662455124815629
Global Adjusted Rand Index: 3.992205347343545e-05
Average Adjusted Rand Index: 0.001610608441576648
11894.842566951182
[0.9760961002009502, 3.992205347343545e-05] [0.9759964884296097, 0.001610608441576648] [11890.107536422121, 12424.142562880284]
-------------------------------------
This iteration is 62
True Objective function: Loss = -11874.263342245691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21158.50641779188
Iteration 100: Loss = -12314.65520015439
Iteration 200: Loss = -12314.227248747378
Iteration 300: Loss = -12314.089023057117
Iteration 400: Loss = -12314.01659352305
Iteration 500: Loss = -12313.972267577054
Iteration 600: Loss = -12313.940903752886
Iteration 700: Loss = -12313.915835193538
Iteration 800: Loss = -12313.894492994454
Iteration 900: Loss = -12313.876670375837
Iteration 1000: Loss = -12313.861403704073
Iteration 1100: Loss = -12313.846632133125
Iteration 1200: Loss = -12313.830968741458
Iteration 1300: Loss = -12313.813459243183
Iteration 1400: Loss = -12313.793172652913
Iteration 1500: Loss = -12313.76913912935
Iteration 1600: Loss = -12313.741048756507
Iteration 1700: Loss = -12313.709971937771
Iteration 1800: Loss = -12313.679426540524
Iteration 1900: Loss = -12313.651789892718
Iteration 2000: Loss = -12313.629998795192
Iteration 2100: Loss = -12313.619943905334
Iteration 2200: Loss = -12313.60065887837
Iteration 2300: Loss = -12313.591779386772
Iteration 2400: Loss = -12313.582897649625
Iteration 2500: Loss = -12313.576028918313
Iteration 2600: Loss = -12313.569662626656
Iteration 2700: Loss = -12313.563204123117
Iteration 2800: Loss = -12313.556042193357
Iteration 2900: Loss = -12313.546985661562
Iteration 3000: Loss = -12313.534003671815
Iteration 3100: Loss = -12313.512091749157
Iteration 3200: Loss = -12313.468830031583
Iteration 3300: Loss = -12313.377512744295
Iteration 3400: Loss = -12313.209229840246
Iteration 3500: Loss = -12313.03418728378
Iteration 3600: Loss = -12312.98074263194
Iteration 3700: Loss = -12312.95054942249
Iteration 3800: Loss = -12312.925988131039
Iteration 3900: Loss = -12312.905884622305
Iteration 4000: Loss = -12312.889067501019
Iteration 4100: Loss = -12312.874355499878
Iteration 4200: Loss = -12312.851822774066
Iteration 4300: Loss = -12312.82421670243
Iteration 4400: Loss = -12312.792737575064
Iteration 4500: Loss = -12312.764871494637
Iteration 4600: Loss = -12312.732540337836
Iteration 4700: Loss = -12312.638331768363
Iteration 4800: Loss = -12312.498360991272
Iteration 4900: Loss = -12312.457079679716
Iteration 5000: Loss = -12312.439585096234
Iteration 5100: Loss = -12312.429541568594
Iteration 5200: Loss = -12312.4229245429
Iteration 5300: Loss = -12312.418142948312
Iteration 5400: Loss = -12312.414577690022
Iteration 5500: Loss = -12312.411803980907
Iteration 5600: Loss = -12312.409571093347
Iteration 5700: Loss = -12312.407724369277
Iteration 5800: Loss = -12312.40619715757
Iteration 5900: Loss = -12312.404935460712
Iteration 6000: Loss = -12312.403819939509
Iteration 6100: Loss = -12312.402896611422
Iteration 6200: Loss = -12312.402016403921
Iteration 6300: Loss = -12312.401295657619
Iteration 6400: Loss = -12312.400652161998
Iteration 6500: Loss = -12312.400119726932
Iteration 6600: Loss = -12312.39962526911
Iteration 6700: Loss = -12312.399147136608
Iteration 6800: Loss = -12312.398787975475
Iteration 6900: Loss = -12312.398403369876
Iteration 7000: Loss = -12312.398043836414
Iteration 7100: Loss = -12312.397764036352
Iteration 7200: Loss = -12312.397529860285
Iteration 7300: Loss = -12312.397230345567
Iteration 7400: Loss = -12312.397117458562
Iteration 7500: Loss = -12312.396763338344
Iteration 7600: Loss = -12312.397646996806
1
Iteration 7700: Loss = -12312.396374341648
Iteration 7800: Loss = -12312.396237041323
Iteration 7900: Loss = -12312.396110223986
Iteration 8000: Loss = -12312.400040671562
1
Iteration 8100: Loss = -12312.395794457561
Iteration 8200: Loss = -12312.396261431162
1
Iteration 8300: Loss = -12312.395568878947
Iteration 8400: Loss = -12312.395435749664
Iteration 8500: Loss = -12312.467629893097
1
Iteration 8600: Loss = -12312.39525463456
Iteration 8700: Loss = -12312.395139228203
Iteration 8800: Loss = -12312.395015954387
Iteration 8900: Loss = -12312.395030034602
Iteration 9000: Loss = -12312.394888217641
Iteration 9100: Loss = -12312.394802321418
Iteration 9200: Loss = -12312.478927284468
1
Iteration 9300: Loss = -12312.39471842913
Iteration 9400: Loss = -12312.394637855841
Iteration 9500: Loss = -12312.394535350679
Iteration 9600: Loss = -12312.394981485304
1
Iteration 9700: Loss = -12312.394479113524
Iteration 9800: Loss = -12312.394426298006
Iteration 9900: Loss = -12312.395000529898
1
Iteration 10000: Loss = -12312.394339662738
Iteration 10100: Loss = -12312.39431054246
Iteration 10200: Loss = -12312.394276137798
Iteration 10300: Loss = -12312.39446108925
1
Iteration 10400: Loss = -12312.394197224794
Iteration 10500: Loss = -12312.394180786114
Iteration 10600: Loss = -12312.394118055208
Iteration 10700: Loss = -12312.394153919122
Iteration 10800: Loss = -12312.394080938115
Iteration 10900: Loss = -12312.394044775552
Iteration 11000: Loss = -12312.408473219359
1
Iteration 11100: Loss = -12312.394021364982
Iteration 11200: Loss = -12312.39396113416
Iteration 11300: Loss = -12312.394670868738
1
Iteration 11400: Loss = -12312.393996689527
Iteration 11500: Loss = -12312.393923442343
Iteration 11600: Loss = -12312.393934340087
Iteration 11700: Loss = -12312.429801955193
1
Iteration 11800: Loss = -12312.39387866161
Iteration 11900: Loss = -12312.39387337175
Iteration 12000: Loss = -12312.490186413721
1
Iteration 12100: Loss = -12312.393853456037
Iteration 12200: Loss = -12312.393855325998
Iteration 12300: Loss = -12312.48685989535
1
Iteration 12400: Loss = -12312.393825187773
Iteration 12500: Loss = -12312.397193638351
1
Iteration 12600: Loss = -12312.394120167108
2
Iteration 12700: Loss = -12312.395181195352
3
Iteration 12800: Loss = -12312.393911942761
Iteration 12900: Loss = -12312.393781058658
Iteration 13000: Loss = -12312.399244693434
1
Iteration 13100: Loss = -12312.393779113874
Iteration 13200: Loss = -12312.393736151562
Iteration 13300: Loss = -12312.395251412454
1
Iteration 13400: Loss = -12312.393742623375
Iteration 13500: Loss = -12312.408604915023
1
Iteration 13600: Loss = -12312.39748063063
2
Iteration 13700: Loss = -12312.39366614197
Iteration 13800: Loss = -12312.393853283747
1
Iteration 13900: Loss = -12312.393670711263
Iteration 14000: Loss = -12312.478007088102
1
Iteration 14100: Loss = -12312.393698234444
Iteration 14200: Loss = -12312.393729824356
Iteration 14300: Loss = -12312.393849923381
1
Iteration 14400: Loss = -12312.393674345712
Iteration 14500: Loss = -12312.39364540296
Iteration 14600: Loss = -12312.39376311217
1
Iteration 14700: Loss = -12312.39364839188
Iteration 14800: Loss = -12312.394310106169
1
Iteration 14900: Loss = -12312.393686236843
Iteration 15000: Loss = -12312.4117177752
1
Iteration 15100: Loss = -12312.397245498254
2
Iteration 15200: Loss = -12312.394539075314
3
Iteration 15300: Loss = -12312.393991905219
4
Iteration 15400: Loss = -12312.702927830836
5
Iteration 15500: Loss = -12312.39365156699
Iteration 15600: Loss = -12312.495651233401
1
Iteration 15700: Loss = -12312.579636425018
2
Iteration 15800: Loss = -12312.394074255255
3
Iteration 15900: Loss = -12312.39509451811
4
Iteration 16000: Loss = -12312.393635220344
Iteration 16100: Loss = -12312.405876149727
1
Iteration 16200: Loss = -12312.393618691212
Iteration 16300: Loss = -12312.48583162898
1
Iteration 16400: Loss = -12312.39363065441
Iteration 16500: Loss = -12312.393633637788
Iteration 16600: Loss = -12312.393692911215
Iteration 16700: Loss = -12312.393608931672
Iteration 16800: Loss = -12312.394156024146
1
Iteration 16900: Loss = -12312.393587984347
Iteration 17000: Loss = -12312.394307270526
1
Iteration 17100: Loss = -12312.39377731412
2
Iteration 17200: Loss = -12312.394930923263
3
Iteration 17300: Loss = -12312.393635000124
Iteration 17400: Loss = -12312.3940586894
1
Iteration 17500: Loss = -12312.393616786241
Iteration 17600: Loss = -12312.399092107478
1
Iteration 17700: Loss = -12312.393599320874
Iteration 17800: Loss = -12312.393987241341
1
Iteration 17900: Loss = -12312.393594757787
Iteration 18000: Loss = -12312.399253070103
1
Iteration 18100: Loss = -12312.3935876074
Iteration 18200: Loss = -12312.393921164474
1
Iteration 18300: Loss = -12312.39364197897
Iteration 18400: Loss = -12312.393623176345
Iteration 18500: Loss = -12312.39358908875
Iteration 18600: Loss = -12312.393601224585
Iteration 18700: Loss = -12312.396945232762
1
Iteration 18800: Loss = -12312.393587502609
Iteration 18900: Loss = -12312.39590164405
1
Iteration 19000: Loss = -12312.393613232474
Iteration 19100: Loss = -12312.44203950794
1
Iteration 19200: Loss = -12312.393579130836
Iteration 19300: Loss = -12312.606990442233
1
Iteration 19400: Loss = -12312.393674766787
Iteration 19500: Loss = -12312.51763150752
1
Iteration 19600: Loss = -12312.396417052443
2
Iteration 19700: Loss = -12312.39589520248
3
Iteration 19800: Loss = -12312.393619546503
Iteration 19900: Loss = -12312.395714156255
1
pi: tensor([[1.0000e+00, 1.5061e-06],
        [2.3950e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0238, 0.9762], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1970, 0.2020],
         [0.6004, 0.1977]],

        [[0.6448, 0.2008],
         [0.7296, 0.5545]],

        [[0.7175, 0.2543],
         [0.5252, 0.7255]],

        [[0.6971, 0.1223],
         [0.7116, 0.5679]],

        [[0.7105, 0.2517],
         [0.5883, 0.7058]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0009197044489078545
Average Adjusted Rand Index: -0.001106962750739997
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22393.50329317221
Iteration 100: Loss = -12314.674945638426
Iteration 200: Loss = -12314.165973850566
Iteration 300: Loss = -12314.025680277811
Iteration 400: Loss = -12313.962867799066
Iteration 500: Loss = -12313.927755795386
Iteration 600: Loss = -12313.904308589063
Iteration 700: Loss = -12313.886809861924
Iteration 800: Loss = -12313.872418664856
Iteration 900: Loss = -12313.859686687081
Iteration 1000: Loss = -12313.847619351933
Iteration 1100: Loss = -12313.835565538395
Iteration 1200: Loss = -12313.822880113992
Iteration 1300: Loss = -12313.8093150146
Iteration 1400: Loss = -12313.794578417888
Iteration 1500: Loss = -12313.778465285737
Iteration 1600: Loss = -12313.760699689681
Iteration 1700: Loss = -12313.740889513181
Iteration 1800: Loss = -12313.718187953742
Iteration 1900: Loss = -12313.691662880241
Iteration 2000: Loss = -12313.660002488206
Iteration 2100: Loss = -12313.62198489268
Iteration 2200: Loss = -12313.576773107492
Iteration 2300: Loss = -12313.524943876573
Iteration 2400: Loss = -12313.469286823818
Iteration 2500: Loss = -12313.41476012732
Iteration 2600: Loss = -12313.366341207442
Iteration 2700: Loss = -12313.326615446575
Iteration 2800: Loss = -12313.29493109906
Iteration 2900: Loss = -12313.268411451561
Iteration 3000: Loss = -12313.246086139583
Iteration 3100: Loss = -12313.227911690668
Iteration 3200: Loss = -12313.213434641524
Iteration 3300: Loss = -12313.202199548832
Iteration 3400: Loss = -12313.193329748983
Iteration 3500: Loss = -12313.186463301117
Iteration 3600: Loss = -12313.180834118275
Iteration 3700: Loss = -12313.176270816326
Iteration 3800: Loss = -12313.172336518399
Iteration 3900: Loss = -12313.168820556906
Iteration 4000: Loss = -12313.165568890883
Iteration 4100: Loss = -12313.162173239627
Iteration 4200: Loss = -12313.157972626204
Iteration 4300: Loss = -12313.152393978691
Iteration 4400: Loss = -12313.152583445099
1
Iteration 4500: Loss = -12313.124874659217
Iteration 4600: Loss = -12313.102954826198
Iteration 4700: Loss = -12313.080397522013
Iteration 4800: Loss = -12313.053791784638
Iteration 4900: Loss = -12313.017858305631
Iteration 5000: Loss = -12312.973793474484
Iteration 5100: Loss = -12312.939220142982
Iteration 5200: Loss = -12312.907360177838
Iteration 5300: Loss = -12312.889495737983
Iteration 5400: Loss = -12312.876451029271
Iteration 5500: Loss = -12312.864474826818
Iteration 5600: Loss = -12312.851267485792
Iteration 5700: Loss = -12312.829732938944
Iteration 5800: Loss = -12312.801099439655
Iteration 5900: Loss = -12312.77037651601
Iteration 6000: Loss = -12312.742516956872
Iteration 6100: Loss = -12312.69457031541
Iteration 6200: Loss = -12312.537281137895
Iteration 6300: Loss = -12312.45518650129
Iteration 6400: Loss = -12312.432384267648
Iteration 6500: Loss = -12312.421912060543
Iteration 6600: Loss = -12312.415764315687
Iteration 6700: Loss = -12312.411662886941
Iteration 6800: Loss = -12312.408754387674
Iteration 6900: Loss = -12312.406574819912
Iteration 7000: Loss = -12312.404836607637
Iteration 7100: Loss = -12312.40347235628
Iteration 7200: Loss = -12312.402363833662
Iteration 7300: Loss = -12312.40142872119
Iteration 7400: Loss = -12312.400663919625
Iteration 7500: Loss = -12312.400003085511
Iteration 7600: Loss = -12312.400605601959
1
Iteration 7700: Loss = -12312.398922790399
Iteration 7800: Loss = -12312.398526721563
Iteration 7900: Loss = -12312.40842118031
1
Iteration 8000: Loss = -12312.39771245111
Iteration 8100: Loss = -12312.397419009418
Iteration 8200: Loss = -12312.399579087149
1
Iteration 8300: Loss = -12312.396885626951
Iteration 8400: Loss = -12312.396681576762
Iteration 8500: Loss = -12312.39645413036
Iteration 8600: Loss = -12312.39699010502
1
Iteration 8700: Loss = -12312.396116466667
Iteration 8800: Loss = -12312.395967050888
Iteration 8900: Loss = -12312.398773509036
1
Iteration 9000: Loss = -12312.39567684835
Iteration 9100: Loss = -12312.395554784176
Iteration 9200: Loss = -12312.39721293917
1
Iteration 9300: Loss = -12312.395352949141
Iteration 9400: Loss = -12312.395245167914
Iteration 9500: Loss = -12312.395138450125
Iteration 9600: Loss = -12312.395206205558
Iteration 9700: Loss = -12312.394974756497
Iteration 9800: Loss = -12312.394904656332
Iteration 9900: Loss = -12312.398977309509
1
Iteration 10000: Loss = -12312.394790101966
Iteration 10100: Loss = -12312.394736145412
Iteration 10200: Loss = -12312.39467575611
Iteration 10300: Loss = -12312.395336869055
1
Iteration 10400: Loss = -12312.394604691788
Iteration 10500: Loss = -12312.394532100023
Iteration 10600: Loss = -12312.566177122657
1
Iteration 10700: Loss = -12312.394475000194
Iteration 10800: Loss = -12312.39441921258
Iteration 10900: Loss = -12312.394377540708
Iteration 11000: Loss = -12312.394685607613
1
Iteration 11100: Loss = -12312.394299867701
Iteration 11200: Loss = -12312.394281058318
Iteration 11300: Loss = -12312.398187419625
1
Iteration 11400: Loss = -12312.394128075364
Iteration 11500: Loss = -12312.394115227815
Iteration 11600: Loss = -12312.431836353639
1
Iteration 11700: Loss = -12312.394072009352
Iteration 11800: Loss = -12312.394078556574
Iteration 11900: Loss = -12312.394128692107
Iteration 12000: Loss = -12312.39407954272
Iteration 12100: Loss = -12312.393960641757
Iteration 12200: Loss = -12312.393935700651
Iteration 12300: Loss = -12312.394045606476
1
Iteration 12400: Loss = -12312.393936355422
Iteration 12500: Loss = -12312.39391280716
Iteration 12600: Loss = -12312.423889184924
1
Iteration 12700: Loss = -12312.393921378733
Iteration 12800: Loss = -12312.393897648857
Iteration 12900: Loss = -12312.393851773513
Iteration 13000: Loss = -12312.395855538234
1
Iteration 13100: Loss = -12312.3937913571
Iteration 13200: Loss = -12312.393814141777
Iteration 13300: Loss = -12312.403610552705
1
Iteration 13400: Loss = -12312.39387505609
Iteration 13500: Loss = -12312.394947406046
1
Iteration 13600: Loss = -12312.394014075599
2
Iteration 13700: Loss = -12312.393792855773
Iteration 13800: Loss = -12312.39582156183
1
Iteration 13900: Loss = -12312.393754531007
Iteration 14000: Loss = -12312.393830396404
Iteration 14100: Loss = -12312.393732434819
Iteration 14200: Loss = -12312.393837103298
1
Iteration 14300: Loss = -12312.39375167143
Iteration 14400: Loss = -12312.40971900877
1
Iteration 14500: Loss = -12312.393710039552
Iteration 14600: Loss = -12312.39368998787
Iteration 14700: Loss = -12312.393836405514
1
Iteration 14800: Loss = -12312.393702392086
Iteration 14900: Loss = -12312.396696090373
1
Iteration 15000: Loss = -12312.393697286512
Iteration 15100: Loss = -12312.393811791982
1
Iteration 15200: Loss = -12312.39375858649
Iteration 15300: Loss = -12312.39370933931
Iteration 15400: Loss = -12312.398952911777
1
Iteration 15500: Loss = -12312.393663951896
Iteration 15600: Loss = -12312.39368377353
Iteration 15700: Loss = -12312.420906323674
1
Iteration 15800: Loss = -12312.39366202265
Iteration 15900: Loss = -12312.393693890283
Iteration 16000: Loss = -12312.393881540866
1
Iteration 16100: Loss = -12312.393665468668
Iteration 16200: Loss = -12312.399172837058
1
Iteration 16300: Loss = -12312.393673257435
Iteration 16400: Loss = -12312.39431815405
1
Iteration 16500: Loss = -12312.393680838457
Iteration 16600: Loss = -12312.393635626651
Iteration 16700: Loss = -12312.400754556676
1
Iteration 16800: Loss = -12312.393632081072
Iteration 16900: Loss = -12312.39921906931
1
Iteration 17000: Loss = -12312.402710234534
2
Iteration 17100: Loss = -12312.395220942919
3
Iteration 17200: Loss = -12312.3942248197
4
Iteration 17300: Loss = -12312.396161492874
5
Iteration 17400: Loss = -12312.393665180149
Iteration 17500: Loss = -12312.393594552395
Iteration 17600: Loss = -12312.394510600907
1
Iteration 17700: Loss = -12312.393611799074
Iteration 17800: Loss = -12312.8186661355
1
Iteration 17900: Loss = -12312.3936054836
Iteration 18000: Loss = -12312.393601449268
Iteration 18100: Loss = -12312.393996553381
1
Iteration 18200: Loss = -12312.394183182323
2
Iteration 18300: Loss = -12312.393611945638
Iteration 18400: Loss = -12312.394385236115
1
Iteration 18500: Loss = -12312.393563456828
Iteration 18600: Loss = -12312.401649976853
1
Iteration 18700: Loss = -12312.39360619588
Iteration 18800: Loss = -12312.394814794876
1
Iteration 18900: Loss = -12312.393676831889
Iteration 19000: Loss = -12312.396854772856
1
Iteration 19100: Loss = -12312.393683300874
Iteration 19200: Loss = -12312.394001079805
1
Iteration 19300: Loss = -12312.393684620309
Iteration 19400: Loss = -12312.709478198642
1
Iteration 19500: Loss = -12312.393576218587
Iteration 19600: Loss = -12312.393587823657
Iteration 19700: Loss = -12312.39577685478
1
Iteration 19800: Loss = -12312.393666394986
Iteration 19900: Loss = -12312.393625064604
pi: tensor([[1.0000e+00, 3.0796e-08],
        [2.1155e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9759, 0.0241], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1970, 0.2032],
         [0.6539, 0.1974]],

        [[0.5663, 0.2023],
         [0.5973, 0.5349]],

        [[0.5948, 0.2539],
         [0.5410, 0.6310]],

        [[0.6365, 0.1233],
         [0.5346, 0.6390]],

        [[0.7273, 0.2496],
         [0.5364, 0.5628]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0009197044489078545
Average Adjusted Rand Index: -0.001106962750739997
11874.263342245691
[-0.0009197044489078545, -0.0009197044489078545] [-0.001106962750739997, -0.001106962750739997] [12312.46716647102, 12312.406981644917]
-------------------------------------
This iteration is 63
True Objective function: Loss = -11858.46689824616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24118.181330952077
Iteration 100: Loss = -12352.228417372871
Iteration 200: Loss = -12351.57529499851
Iteration 300: Loss = -12351.389400655597
Iteration 400: Loss = -12351.21901367791
Iteration 500: Loss = -12350.996764778301
Iteration 600: Loss = -12350.774666512269
Iteration 700: Loss = -12350.603431040472
Iteration 800: Loss = -12350.442506478972
Iteration 900: Loss = -12350.227695436863
Iteration 1000: Loss = -12349.786166196267
Iteration 1100: Loss = -12348.445711174947
Iteration 1200: Loss = -12347.99861114759
Iteration 1300: Loss = -12347.876095345775
Iteration 1400: Loss = -12347.813426393976
Iteration 1500: Loss = -12347.776237154205
Iteration 1600: Loss = -12347.751910443556
Iteration 1700: Loss = -12347.734674095836
Iteration 1800: Loss = -12347.7216549758
Iteration 1900: Loss = -12347.711282800881
Iteration 2000: Loss = -12347.702591756428
Iteration 2100: Loss = -12347.6952150415
Iteration 2200: Loss = -12347.688701790446
Iteration 2300: Loss = -12347.682876584975
Iteration 2400: Loss = -12347.677649247276
Iteration 2500: Loss = -12347.67289272783
Iteration 2600: Loss = -12347.668735334728
Iteration 2700: Loss = -12347.665063070483
Iteration 2800: Loss = -12347.661982110234
Iteration 2900: Loss = -12347.659384719664
Iteration 3000: Loss = -12347.657298252701
Iteration 3100: Loss = -12347.655533558245
Iteration 3200: Loss = -12347.654117586537
Iteration 3300: Loss = -12347.65289443849
Iteration 3400: Loss = -12347.651876478201
Iteration 3500: Loss = -12347.650999236259
Iteration 3600: Loss = -12347.650208569949
Iteration 3700: Loss = -12347.649507108514
Iteration 3800: Loss = -12347.648859012155
Iteration 3900: Loss = -12347.648352494354
Iteration 4000: Loss = -12347.64779720574
Iteration 4100: Loss = -12347.647318755558
Iteration 4200: Loss = -12347.646882865996
Iteration 4300: Loss = -12347.646509752987
Iteration 4400: Loss = -12347.646118452485
Iteration 4500: Loss = -12347.64577958158
Iteration 4600: Loss = -12347.645449545727
Iteration 4700: Loss = -12347.645160689486
Iteration 4800: Loss = -12347.644898006069
Iteration 4900: Loss = -12347.644666950147
Iteration 5000: Loss = -12347.644380434456
Iteration 5100: Loss = -12347.644162707376
Iteration 5200: Loss = -12347.643946357208
Iteration 5300: Loss = -12347.643773071877
Iteration 5400: Loss = -12347.643849586182
Iteration 5500: Loss = -12347.643457314653
Iteration 5600: Loss = -12347.643375593188
Iteration 5700: Loss = -12347.644408036529
1
Iteration 5800: Loss = -12347.642957845941
Iteration 5900: Loss = -12347.644907211323
1
Iteration 6000: Loss = -12347.642700958797
Iteration 6100: Loss = -12347.642535903366
Iteration 6200: Loss = -12347.642498672792
Iteration 6300: Loss = -12347.642348484775
Iteration 6400: Loss = -12347.644561194482
1
Iteration 6500: Loss = -12347.642183392494
Iteration 6600: Loss = -12347.642084403224
Iteration 6700: Loss = -12347.642080822021
Iteration 6800: Loss = -12347.64194784595
Iteration 6900: Loss = -12347.644979494593
1
Iteration 7000: Loss = -12347.641773121253
Iteration 7100: Loss = -12347.64173902925
Iteration 7200: Loss = -12347.641671762658
Iteration 7300: Loss = -12347.641625704524
Iteration 7400: Loss = -12347.6415410335
Iteration 7500: Loss = -12347.6414978917
Iteration 7600: Loss = -12347.641451121855
Iteration 7700: Loss = -12347.642229291487
1
Iteration 7800: Loss = -12347.641370418576
Iteration 7900: Loss = -12347.641658058234
1
Iteration 8000: Loss = -12347.65399244722
2
Iteration 8100: Loss = -12347.641217950726
Iteration 8200: Loss = -12347.641948633438
1
Iteration 8300: Loss = -12347.641138700828
Iteration 8400: Loss = -12347.698929332024
1
Iteration 8500: Loss = -12347.641112545714
Iteration 8600: Loss = -12347.641084292833
Iteration 8700: Loss = -12347.643344887207
1
Iteration 8800: Loss = -12347.641018837152
Iteration 8900: Loss = -12347.640979241529
Iteration 9000: Loss = -12347.640983796517
Iteration 9100: Loss = -12347.641508121304
1
Iteration 9200: Loss = -12347.640923838613
Iteration 9300: Loss = -12347.640870051524
Iteration 9400: Loss = -12347.641329797538
1
Iteration 9500: Loss = -12347.640853794466
Iteration 9600: Loss = -12347.753935746165
1
Iteration 9700: Loss = -12347.640841326514
Iteration 9800: Loss = -12347.640807776967
Iteration 9900: Loss = -12347.654812163935
1
Iteration 10000: Loss = -12347.640753491836
Iteration 10100: Loss = -12347.640782647408
Iteration 10200: Loss = -12347.642909708235
1
Iteration 10300: Loss = -12347.640748469994
Iteration 10400: Loss = -12347.640740059336
Iteration 10500: Loss = -12347.6416652986
1
Iteration 10600: Loss = -12347.640733520173
Iteration 10700: Loss = -12347.640858517314
1
Iteration 10800: Loss = -12347.640709230292
Iteration 10900: Loss = -12347.640724794845
Iteration 11000: Loss = -12347.640671138612
Iteration 11100: Loss = -12347.640732217222
Iteration 11200: Loss = -12347.640619890106
Iteration 11300: Loss = -12347.64165707422
1
Iteration 11400: Loss = -12347.640649942807
Iteration 11500: Loss = -12347.69252001745
1
Iteration 11600: Loss = -12347.640656590429
Iteration 11700: Loss = -12347.640640805326
Iteration 11800: Loss = -12347.806332042222
1
Iteration 11900: Loss = -12347.640606090677
Iteration 12000: Loss = -12347.640610161645
Iteration 12100: Loss = -12347.641267105973
1
Iteration 12200: Loss = -12347.640634748033
Iteration 12300: Loss = -12347.64058992533
Iteration 12400: Loss = -12347.641316768924
1
Iteration 12500: Loss = -12347.640588691112
Iteration 12600: Loss = -12347.64060105329
Iteration 12700: Loss = -12347.640620759583
Iteration 12800: Loss = -12347.640578808307
Iteration 12900: Loss = -12347.640533703205
Iteration 13000: Loss = -12347.640968706924
1
Iteration 13100: Loss = -12347.640564553816
Iteration 13200: Loss = -12347.681114807838
1
Iteration 13300: Loss = -12347.640565310154
Iteration 13400: Loss = -12347.640554582746
Iteration 13500: Loss = -12347.666256404907
1
Iteration 13600: Loss = -12347.640534831848
Iteration 13700: Loss = -12347.64056279631
Iteration 13800: Loss = -12347.702762681405
1
Iteration 13900: Loss = -12347.64054184599
Iteration 14000: Loss = -12347.640542422994
Iteration 14100: Loss = -12347.677965081199
1
Iteration 14200: Loss = -12347.640562925963
Iteration 14300: Loss = -12347.640507024462
Iteration 14400: Loss = -12347.641924393656
1
Iteration 14500: Loss = -12347.640506274869
Iteration 14600: Loss = -12347.640525043627
Iteration 14700: Loss = -12347.641398971246
1
Iteration 14800: Loss = -12347.640497123946
Iteration 14900: Loss = -12347.640531008332
Iteration 15000: Loss = -12347.64059238044
Iteration 15100: Loss = -12347.640522246109
Iteration 15200: Loss = -12347.641852995852
1
Iteration 15300: Loss = -12347.640565327594
Iteration 15400: Loss = -12347.640507305092
Iteration 15500: Loss = -12347.70583225701
1
Iteration 15600: Loss = -12347.641082346752
2
Iteration 15700: Loss = -12347.640543091135
Iteration 15800: Loss = -12347.646612388495
1
Iteration 15900: Loss = -12347.640556617436
Iteration 16000: Loss = -12347.640606394443
Iteration 16100: Loss = -12347.640526489773
Iteration 16200: Loss = -12347.640917317143
1
Iteration 16300: Loss = -12347.640500694326
Iteration 16400: Loss = -12348.029744378315
1
Iteration 16500: Loss = -12347.640536227455
Iteration 16600: Loss = -12347.64051865979
Iteration 16700: Loss = -12347.640623885909
1
Iteration 16800: Loss = -12347.640481487766
Iteration 16900: Loss = -12347.641990667928
1
Iteration 17000: Loss = -12347.640560536798
Iteration 17100: Loss = -12347.64049183917
Iteration 17200: Loss = -12348.28538414664
1
Iteration 17300: Loss = -12347.64055036191
Iteration 17400: Loss = -12347.640507405387
Iteration 17500: Loss = -12348.187574232279
1
Iteration 17600: Loss = -12347.640552687353
Iteration 17700: Loss = -12347.64050426331
Iteration 17800: Loss = -12347.646879067019
1
Iteration 17900: Loss = -12347.640507370912
Iteration 18000: Loss = -12347.640505233532
Iteration 18100: Loss = -12347.686822951628
1
Iteration 18200: Loss = -12347.640519548513
Iteration 18300: Loss = -12347.640516458567
Iteration 18400: Loss = -12347.641340310473
1
Iteration 18500: Loss = -12347.640506938906
Iteration 18600: Loss = -12347.640498380057
Iteration 18700: Loss = -12347.640601958641
1
Iteration 18800: Loss = -12347.64054584479
Iteration 18900: Loss = -12347.641890337145
1
Iteration 19000: Loss = -12347.640572641261
Iteration 19100: Loss = -12347.640541781184
Iteration 19200: Loss = -12347.64058722687
Iteration 19300: Loss = -12347.640517101392
Iteration 19400: Loss = -12347.640508114384
Iteration 19500: Loss = -12347.64271822359
1
Iteration 19600: Loss = -12347.640560820835
Iteration 19700: Loss = -12347.64058441976
Iteration 19800: Loss = -12347.927666443316
1
Iteration 19900: Loss = -12347.640531924839
pi: tensor([[2.4538e-07, 1.0000e+00],
        [5.5749e-02, 9.4425e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0884, 0.9116], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2311, 0.2143],
         [0.5828, 0.2033]],

        [[0.5325, 0.1790],
         [0.5547, 0.6132]],

        [[0.5836, 0.1063],
         [0.6888, 0.6709]],

        [[0.7134, 0.2218],
         [0.7158, 0.5593]],

        [[0.5695, 0.1196],
         [0.5823, 0.6628]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.02664820903270462
Global Adjusted Rand Index: 0.004468136446998702
Average Adjusted Rand Index: 0.0061630257584330455
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23093.074064561348
Iteration 100: Loss = -12352.650337060839
Iteration 200: Loss = -12351.928301051363
Iteration 300: Loss = -12351.745709564471
Iteration 400: Loss = -12351.662691485904
Iteration 500: Loss = -12351.60797838956
Iteration 600: Loss = -12351.56369296045
Iteration 700: Loss = -12351.524100125545
Iteration 800: Loss = -12351.487455172952
Iteration 900: Loss = -12351.453504852723
Iteration 1000: Loss = -12351.421440709
Iteration 1100: Loss = -12351.389181363284
Iteration 1200: Loss = -12351.351850766538
Iteration 1300: Loss = -12351.298178543519
Iteration 1400: Loss = -12351.192741152296
Iteration 1500: Loss = -12350.95640329385
Iteration 1600: Loss = -12350.691978536854
Iteration 1700: Loss = -12350.50795470628
Iteration 1800: Loss = -12350.361716930576
Iteration 1900: Loss = -12350.226868297988
Iteration 2000: Loss = -12350.066424737513
Iteration 2100: Loss = -12349.780020385491
Iteration 2200: Loss = -12349.094336355654
Iteration 2300: Loss = -12348.33541934187
Iteration 2400: Loss = -12348.051688367457
Iteration 2500: Loss = -12347.93040933032
Iteration 2600: Loss = -12347.864296834094
Iteration 2700: Loss = -12347.822154149888
Iteration 2800: Loss = -12347.792625481488
Iteration 2900: Loss = -12347.7705225777
Iteration 3000: Loss = -12347.753321995939
Iteration 3100: Loss = -12347.739503691959
Iteration 3200: Loss = -12347.72790994926
Iteration 3300: Loss = -12347.718262148988
Iteration 3400: Loss = -12347.709990415906
Iteration 3500: Loss = -12347.702740474771
Iteration 3600: Loss = -12347.70141074792
Iteration 3700: Loss = -12347.690920327896
Iteration 3800: Loss = -12347.6860722253
Iteration 3900: Loss = -12347.682086281078
Iteration 4000: Loss = -12347.677981787214
Iteration 4100: Loss = -12347.674617750901
Iteration 4200: Loss = -12347.671668538402
Iteration 4300: Loss = -12347.669028876366
Iteration 4400: Loss = -12347.666735451749
Iteration 4500: Loss = -12347.66470330968
Iteration 4600: Loss = -12347.662894185658
Iteration 4700: Loss = -12347.661262107551
Iteration 4800: Loss = -12347.659806752905
Iteration 4900: Loss = -12347.65843774816
Iteration 5000: Loss = -12347.659550166905
1
Iteration 5100: Loss = -12347.656103056504
Iteration 5200: Loss = -12347.655151193376
Iteration 5300: Loss = -12347.655982561186
1
Iteration 5400: Loss = -12347.65336817943
Iteration 5500: Loss = -12347.652569876076
Iteration 5600: Loss = -12347.653919645196
1
Iteration 5700: Loss = -12347.651147684812
Iteration 5800: Loss = -12347.650565058337
Iteration 5900: Loss = -12347.650572129167
Iteration 6000: Loss = -12347.649364055771
Iteration 6100: Loss = -12347.648896088269
Iteration 6200: Loss = -12347.64849854762
Iteration 6300: Loss = -12347.647969425421
Iteration 6400: Loss = -12347.64753107794
Iteration 6500: Loss = -12347.647141076492
Iteration 6600: Loss = -12347.646777554739
Iteration 6700: Loss = -12347.646936513102
1
Iteration 6800: Loss = -12347.647272495286
2
Iteration 6900: Loss = -12347.646471975228
Iteration 7000: Loss = -12347.650391237736
1
Iteration 7100: Loss = -12347.64553226913
Iteration 7200: Loss = -12347.647805896748
1
Iteration 7300: Loss = -12347.644794696047
Iteration 7400: Loss = -12347.64520684264
1
Iteration 7500: Loss = -12347.644324201421
Iteration 7600: Loss = -12347.64710289764
1
Iteration 7700: Loss = -12347.643933941446
Iteration 7800: Loss = -12347.643765862342
Iteration 7900: Loss = -12347.647760235077
1
Iteration 8000: Loss = -12347.643432500336
Iteration 8100: Loss = -12347.643283356436
Iteration 8200: Loss = -12347.65575971016
1
Iteration 8300: Loss = -12347.643041318277
Iteration 8400: Loss = -12347.642871035341
Iteration 8500: Loss = -12347.642984712682
1
Iteration 8600: Loss = -12347.642667172604
Iteration 8700: Loss = -12347.6425325058
Iteration 8800: Loss = -12347.642424483412
Iteration 8900: Loss = -12347.64241512375
Iteration 9000: Loss = -12347.642271895502
Iteration 9100: Loss = -12347.642179128377
Iteration 9200: Loss = -12347.642987039288
1
Iteration 9300: Loss = -12347.642066488434
Iteration 9400: Loss = -12347.641933007988
Iteration 9500: Loss = -12347.663501132944
1
Iteration 9600: Loss = -12347.641836425157
Iteration 9700: Loss = -12347.641736403404
Iteration 9800: Loss = -12347.706163200186
1
Iteration 9900: Loss = -12347.641626933648
Iteration 10000: Loss = -12347.6415560568
Iteration 10100: Loss = -12347.641542648307
Iteration 10200: Loss = -12347.6415138396
Iteration 10300: Loss = -12347.641397001904
Iteration 10400: Loss = -12347.641339446933
Iteration 10500: Loss = -12347.642145492582
1
Iteration 10600: Loss = -12347.64129259616
Iteration 10700: Loss = -12347.641265294933
Iteration 10800: Loss = -12347.645121807547
1
Iteration 10900: Loss = -12347.641192825478
Iteration 11000: Loss = -12347.641121932878
Iteration 11100: Loss = -12347.653879188181
1
Iteration 11200: Loss = -12347.641081679736
Iteration 11300: Loss = -12347.641065462472
Iteration 11400: Loss = -12347.641478317437
1
Iteration 11500: Loss = -12347.641056383152
Iteration 11600: Loss = -12347.640960113073
Iteration 11700: Loss = -12347.95123175961
1
Iteration 11800: Loss = -12347.640949055254
Iteration 11900: Loss = -12347.682335708043
1
Iteration 12000: Loss = -12347.640914054131
Iteration 12100: Loss = -12347.645301809976
1
Iteration 12200: Loss = -12347.640922317543
Iteration 12300: Loss = -12347.648251841518
1
Iteration 12400: Loss = -12347.640856383285
Iteration 12500: Loss = -12347.644864311469
1
Iteration 12600: Loss = -12347.640795424484
Iteration 12700: Loss = -12347.641080156636
1
Iteration 12800: Loss = -12347.640745119561
Iteration 12900: Loss = -12347.641377893615
1
Iteration 13000: Loss = -12347.64071399313
Iteration 13100: Loss = -12347.680791211513
1
Iteration 13200: Loss = -12347.640742408865
Iteration 13300: Loss = -12347.640720939797
Iteration 13400: Loss = -12347.640957805945
1
Iteration 13500: Loss = -12347.640674795199
Iteration 13600: Loss = -12348.093769289007
1
Iteration 13700: Loss = -12347.640729694585
Iteration 13800: Loss = -12347.640621226265
Iteration 13900: Loss = -12347.642932866222
1
Iteration 14000: Loss = -12347.640660912888
Iteration 14100: Loss = -12347.640650846717
Iteration 14200: Loss = -12347.64070083781
Iteration 14300: Loss = -12347.640625876029
Iteration 14400: Loss = -12348.196956701579
1
Iteration 14500: Loss = -12347.640638734765
Iteration 14600: Loss = -12347.640643922872
Iteration 14700: Loss = -12347.684888978774
1
Iteration 14800: Loss = -12347.64063377747
Iteration 14900: Loss = -12347.640627184355
Iteration 15000: Loss = -12347.643080530444
1
Iteration 15100: Loss = -12347.640633305442
Iteration 15200: Loss = -12347.640585242845
Iteration 15300: Loss = -12347.640782503318
1
Iteration 15400: Loss = -12347.64059226777
Iteration 15500: Loss = -12347.64261473782
1
Iteration 15600: Loss = -12347.642423762069
2
Iteration 15700: Loss = -12347.64057478894
Iteration 15800: Loss = -12347.642217803792
1
Iteration 15900: Loss = -12347.640583966248
Iteration 16000: Loss = -12347.640618588333
Iteration 16100: Loss = -12347.640583390437
Iteration 16200: Loss = -12347.640807517486
1
Iteration 16300: Loss = -12347.640536457673
Iteration 16400: Loss = -12347.655873951184
1
Iteration 16500: Loss = -12347.64052862478
Iteration 16600: Loss = -12348.161575280392
1
Iteration 16700: Loss = -12347.640537445319
Iteration 16800: Loss = -12347.640558385898
Iteration 16900: Loss = -12347.64199158687
1
Iteration 17000: Loss = -12347.640540398183
Iteration 17100: Loss = -12347.687533832755
1
Iteration 17200: Loss = -12347.640572012184
Iteration 17300: Loss = -12347.640480950206
Iteration 17400: Loss = -12347.64640777567
1
Iteration 17500: Loss = -12347.640504338578
Iteration 17600: Loss = -12347.93937837784
1
Iteration 17700: Loss = -12347.640549599308
Iteration 17800: Loss = -12347.641293903234
1
Iteration 17900: Loss = -12347.640526841207
Iteration 18000: Loss = -12347.67025093335
1
Iteration 18100: Loss = -12347.640540198208
Iteration 18200: Loss = -12347.649817364327
1
Iteration 18300: Loss = -12347.640512230666
Iteration 18400: Loss = -12347.652796213666
1
Iteration 18500: Loss = -12347.640505171017
Iteration 18600: Loss = -12347.700741275667
1
Iteration 18700: Loss = -12347.640535881244
Iteration 18800: Loss = -12348.000180728772
1
Iteration 18900: Loss = -12347.6405222086
Iteration 19000: Loss = -12347.64056737729
Iteration 19100: Loss = -12347.640547034407
Iteration 19200: Loss = -12347.640490311102
Iteration 19300: Loss = -12347.642630238866
1
Iteration 19400: Loss = -12347.640521904555
Iteration 19500: Loss = -12347.640521130095
Iteration 19600: Loss = -12347.640564495805
Iteration 19700: Loss = -12347.640517163863
Iteration 19800: Loss = -12348.222066517128
1
Iteration 19900: Loss = -12347.640531577994
pi: tensor([[9.4453e-01, 5.5465e-02],
        [1.0000e+00, 9.3749e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9081, 0.0919], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.2149],
         [0.6918, 0.2315]],

        [[0.6375, 0.1801],
         [0.7252, 0.5629]],

        [[0.5396, 0.1058],
         [0.5933, 0.5911]],

        [[0.6541, 0.2229],
         [0.6959, 0.6611]],

        [[0.6729, 0.1189],
         [0.5672, 0.5728]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.02664820903270462
Global Adjusted Rand Index: 0.004468136446998702
Average Adjusted Rand Index: 0.0061630257584330455
11858.46689824616
[0.004468136446998702, 0.004468136446998702] [0.0061630257584330455, 0.0061630257584330455] [12347.972567676135, 12347.6405178284]
-------------------------------------
This iteration is 64
True Objective function: Loss = -11951.181126281203
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21490.440004729648
Iteration 100: Loss = -12430.85930218739
Iteration 200: Loss = -12430.571804709252
Iteration 300: Loss = -12430.344708324912
Iteration 400: Loss = -12429.861380868666
Iteration 500: Loss = -12429.531794886107
Iteration 600: Loss = -12429.299004510527
Iteration 700: Loss = -12429.063789534808
Iteration 800: Loss = -12428.895894645091
Iteration 900: Loss = -12428.775930911017
Iteration 1000: Loss = -12428.683135534064
Iteration 1100: Loss = -12428.613029589902
Iteration 1200: Loss = -12428.559213403005
Iteration 1300: Loss = -12428.521012844234
Iteration 1400: Loss = -12428.493400371026
Iteration 1500: Loss = -12428.472508169136
Iteration 1600: Loss = -12428.45729722615
Iteration 1700: Loss = -12428.447039617278
Iteration 1800: Loss = -12428.440058386812
Iteration 1900: Loss = -12428.43589724338
Iteration 2000: Loss = -12428.433972496736
Iteration 2100: Loss = -12428.43173788576
Iteration 2200: Loss = -12428.430716119798
Iteration 2300: Loss = -12428.429903619697
Iteration 2400: Loss = -12428.42917573232
Iteration 2500: Loss = -12428.428614631022
Iteration 2600: Loss = -12428.428020828536
Iteration 2700: Loss = -12428.427477007663
Iteration 2800: Loss = -12428.43112240311
1
Iteration 2900: Loss = -12428.426419828145
Iteration 3000: Loss = -12428.425940403515
Iteration 3100: Loss = -12428.425626504008
Iteration 3200: Loss = -12428.425011825932
Iteration 3300: Loss = -12428.424584790568
Iteration 3400: Loss = -12428.424160185732
Iteration 3500: Loss = -12428.423765204789
Iteration 3600: Loss = -12428.423345834508
Iteration 3700: Loss = -12428.422969185602
Iteration 3800: Loss = -12428.422658704974
Iteration 3900: Loss = -12428.42230422396
Iteration 4000: Loss = -12428.421953582536
Iteration 4100: Loss = -12428.421818379991
Iteration 4200: Loss = -12428.421343672682
Iteration 4300: Loss = -12428.421086046023
Iteration 4400: Loss = -12428.420798383535
Iteration 4500: Loss = -12428.420537507402
Iteration 4600: Loss = -12428.420369601155
Iteration 4700: Loss = -12428.420127736048
Iteration 4800: Loss = -12428.419870508893
Iteration 4900: Loss = -12428.4197208502
Iteration 5000: Loss = -12428.419459142218
Iteration 5100: Loss = -12428.420199141023
1
Iteration 5200: Loss = -12428.41907546989
Iteration 5300: Loss = -12428.419690053544
1
Iteration 5400: Loss = -12428.418765741992
Iteration 5500: Loss = -12428.41861952496
Iteration 5600: Loss = -12428.418511312482
Iteration 5700: Loss = -12428.418403151409
Iteration 5800: Loss = -12428.418154852689
Iteration 5900: Loss = -12428.41810010155
Iteration 6000: Loss = -12428.418086721245
Iteration 6100: Loss = -12428.417834762202
Iteration 6200: Loss = -12428.4177441242
Iteration 6300: Loss = -12428.417912427112
1
Iteration 6400: Loss = -12428.417531980054
Iteration 6500: Loss = -12428.420848749174
1
Iteration 6600: Loss = -12428.417344164836
Iteration 6700: Loss = -12428.417325695791
Iteration 6800: Loss = -12428.417202161741
Iteration 6900: Loss = -12428.417123462103
Iteration 7000: Loss = -12428.417803735347
1
Iteration 7100: Loss = -12428.416965999319
Iteration 7200: Loss = -12428.421448311263
1
Iteration 7300: Loss = -12428.416850763957
Iteration 7400: Loss = -12428.417232730195
1
Iteration 7500: Loss = -12428.435492937408
2
Iteration 7600: Loss = -12428.416691247838
Iteration 7700: Loss = -12428.417560972799
1
Iteration 7800: Loss = -12428.416584957573
Iteration 7900: Loss = -12428.417352375573
1
Iteration 8000: Loss = -12428.416527457588
Iteration 8100: Loss = -12428.664962202056
1
Iteration 8200: Loss = -12428.416443084325
Iteration 8300: Loss = -12428.416435064946
Iteration 8400: Loss = -12428.470602335667
1
Iteration 8500: Loss = -12428.4163654299
Iteration 8600: Loss = -12428.416353744435
Iteration 8700: Loss = -12428.417615840028
1
Iteration 8800: Loss = -12428.416261897479
Iteration 8900: Loss = -12428.41642251145
1
Iteration 9000: Loss = -12428.416274142988
Iteration 9100: Loss = -12428.4269150233
1
Iteration 9200: Loss = -12428.81724519223
2
Iteration 9300: Loss = -12428.443916269685
3
Iteration 9400: Loss = -12428.416127165658
Iteration 9500: Loss = -12428.417089122566
1
Iteration 9600: Loss = -12428.451041437182
2
Iteration 9700: Loss = -12428.666068173588
3
Iteration 9800: Loss = -12428.41822458857
4
Iteration 9900: Loss = -12428.45121921779
5
Iteration 10000: Loss = -12428.416358324299
6
Iteration 10100: Loss = -12428.439270730869
7
Iteration 10200: Loss = -12428.41612683042
Iteration 10300: Loss = -12428.42860205318
1
Iteration 10400: Loss = -12428.752467424874
2
Iteration 10500: Loss = -12428.434827703602
3
Iteration 10600: Loss = -12428.440787836895
4
Iteration 10700: Loss = -12428.415949231143
Iteration 10800: Loss = -12428.416014490924
Iteration 10900: Loss = -12428.415991553176
Iteration 11000: Loss = -12428.416355611063
1
Iteration 11100: Loss = -12428.415888258207
Iteration 11200: Loss = -12428.445057910363
1
Iteration 11300: Loss = -12428.497925523
2
Iteration 11400: Loss = -12428.41588278956
Iteration 11500: Loss = -12428.416275336871
1
Iteration 11600: Loss = -12428.687081381646
2
Iteration 11700: Loss = -12428.41660560111
3
Iteration 11800: Loss = -12428.421928303831
4
Iteration 11900: Loss = -12428.416234770073
5
Iteration 12000: Loss = -12428.416089636481
6
Iteration 12100: Loss = -12428.417594958724
7
Iteration 12200: Loss = -12428.446194073987
8
Iteration 12300: Loss = -12428.420935617476
9
Iteration 12400: Loss = -12428.418052114306
10
Iteration 12500: Loss = -12428.415985731102
11
Iteration 12600: Loss = -12428.421802659686
12
Iteration 12700: Loss = -12428.416019830152
13
Iteration 12800: Loss = -12428.459730898472
14
Iteration 12900: Loss = -12428.445362156754
15
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[4.1521e-01, 5.8479e-01],
        [7.5223e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2454, 0.7546], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2530, 0.2216],
         [0.6088, 0.1973]],

        [[0.5477, 0.2292],
         [0.6364, 0.6249]],

        [[0.5368, 0.1997],
         [0.5917, 0.7142]],

        [[0.6493, 0.2258],
         [0.5291, 0.7221]],

        [[0.5968, 0.2009],
         [0.6018, 0.7113]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.011957532380478733
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0019182131166299715
Average Adjusted Rand Index: -0.0029841849641213864
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20897.26457782739
Iteration 100: Loss = -12431.083271321173
Iteration 200: Loss = -12430.609052231877
Iteration 300: Loss = -12430.350279672984
Iteration 400: Loss = -12430.11857554547
Iteration 500: Loss = -12429.964189921835
Iteration 600: Loss = -12429.812325809815
Iteration 700: Loss = -12429.616520717767
Iteration 800: Loss = -12429.38703506501
Iteration 900: Loss = -12429.223364442161
Iteration 1000: Loss = -12429.110216370394
Iteration 1100: Loss = -12428.884496492248
Iteration 1200: Loss = -12428.698762183523
Iteration 1300: Loss = -12428.624320907134
Iteration 1400: Loss = -12428.580428974738
Iteration 1500: Loss = -12428.552930608706
Iteration 1600: Loss = -12428.533040669967
Iteration 1700: Loss = -12428.517086567557
Iteration 1800: Loss = -12428.503474430734
Iteration 1900: Loss = -12428.490838014332
Iteration 2000: Loss = -12428.478560159989
Iteration 2100: Loss = -12428.466706078898
Iteration 2200: Loss = -12428.45603710303
Iteration 2300: Loss = -12428.447218146513
Iteration 2400: Loss = -12428.440404895779
Iteration 2500: Loss = -12428.435393960825
Iteration 2600: Loss = -12428.431799504082
Iteration 2700: Loss = -12428.4294681847
Iteration 2800: Loss = -12428.427922741628
Iteration 2900: Loss = -12428.426951910707
Iteration 3000: Loss = -12428.426291803413
Iteration 3100: Loss = -12428.42581074524
Iteration 3200: Loss = -12428.425413346315
Iteration 3300: Loss = -12428.425077313492
Iteration 3400: Loss = -12428.42480284784
Iteration 3500: Loss = -12428.42445330515
Iteration 3600: Loss = -12428.42417814429
Iteration 3700: Loss = -12428.423887906218
Iteration 3800: Loss = -12428.423587118037
Iteration 3900: Loss = -12428.42333333439
Iteration 4000: Loss = -12428.42304493422
Iteration 4100: Loss = -12428.422787203328
Iteration 4200: Loss = -12428.422501463212
Iteration 4300: Loss = -12428.422303938487
Iteration 4400: Loss = -12428.422045871916
Iteration 4500: Loss = -12428.422188252049
1
Iteration 4600: Loss = -12428.421501546482
Iteration 4700: Loss = -12428.421316570228
Iteration 4800: Loss = -12428.421049630604
Iteration 4900: Loss = -12428.420855540642
Iteration 5000: Loss = -12428.421568869924
1
Iteration 5100: Loss = -12428.420462516797
Iteration 5200: Loss = -12428.420212636875
Iteration 5300: Loss = -12428.420022170192
Iteration 5400: Loss = -12428.420058671583
Iteration 5500: Loss = -12428.419653934634
Iteration 5600: Loss = -12428.419555732857
Iteration 5700: Loss = -12428.419311393514
Iteration 5800: Loss = -12428.420799696478
1
Iteration 5900: Loss = -12428.419021461988
Iteration 6000: Loss = -12428.418847384439
Iteration 6100: Loss = -12428.419028976989
1
Iteration 6200: Loss = -12428.418590022633
Iteration 6300: Loss = -12428.419377729251
1
Iteration 6400: Loss = -12428.418336447487
Iteration 6500: Loss = -12428.42243847351
1
Iteration 6600: Loss = -12428.418110589908
Iteration 6700: Loss = -12428.417951708894
Iteration 6800: Loss = -12428.417895730203
Iteration 6900: Loss = -12428.423551120328
1
Iteration 7000: Loss = -12428.417758512003
Iteration 7100: Loss = -12428.41763970983
Iteration 7200: Loss = -12428.417805176514
1
Iteration 7300: Loss = -12428.417425441372
Iteration 7400: Loss = -12428.417371512262
Iteration 7500: Loss = -12428.417905577991
1
Iteration 7600: Loss = -12428.417194342972
Iteration 7700: Loss = -12428.428997210918
1
Iteration 7800: Loss = -12428.41706247228
Iteration 7900: Loss = -12428.416980695783
Iteration 8000: Loss = -12428.84826578597
1
Iteration 8100: Loss = -12428.416867150596
Iteration 8200: Loss = -12428.416819998516
Iteration 8300: Loss = -12428.437394618337
1
Iteration 8400: Loss = -12428.416734615183
Iteration 8500: Loss = -12428.416683607164
Iteration 8600: Loss = -12428.417420326876
1
Iteration 8700: Loss = -12428.416591343987
Iteration 8800: Loss = -12428.416555444783
Iteration 8900: Loss = -12428.416824354237
1
Iteration 9000: Loss = -12428.41646153432
Iteration 9100: Loss = -12428.480233688313
1
Iteration 9200: Loss = -12428.416407248744
Iteration 9300: Loss = -12428.416362524924
Iteration 9400: Loss = -12428.416351065704
Iteration 9500: Loss = -12428.416523715514
1
Iteration 9600: Loss = -12428.416338903226
Iteration 9700: Loss = -12428.417807421409
1
Iteration 9800: Loss = -12428.440010202572
2
Iteration 9900: Loss = -12428.422599849568
3
Iteration 10000: Loss = -12428.416165173356
Iteration 10100: Loss = -12428.428319576027
1
Iteration 10200: Loss = -12428.416151057074
Iteration 10300: Loss = -12428.457641385863
1
Iteration 10400: Loss = -12428.416096670133
Iteration 10500: Loss = -12428.416093659316
Iteration 10600: Loss = -12428.416280156112
1
Iteration 10700: Loss = -12428.416098203608
Iteration 10800: Loss = -12428.42598251534
1
Iteration 10900: Loss = -12428.427054988166
2
Iteration 11000: Loss = -12428.416154493265
Iteration 11100: Loss = -12428.416087328784
Iteration 11200: Loss = -12428.418278787687
1
Iteration 11300: Loss = -12428.417244341894
2
Iteration 11400: Loss = -12428.41605154667
Iteration 11500: Loss = -12428.417699288568
1
Iteration 11600: Loss = -12428.425794316558
2
Iteration 11700: Loss = -12428.417012993377
3
Iteration 11800: Loss = -12428.416127471672
Iteration 11900: Loss = -12428.42086546123
1
Iteration 12000: Loss = -12428.45498690891
2
Iteration 12100: Loss = -12428.416430460888
3
Iteration 12200: Loss = -12428.422348318047
4
Iteration 12300: Loss = -12428.427079969742
5
Iteration 12400: Loss = -12428.435604431252
6
Iteration 12500: Loss = -12428.436966780173
7
Iteration 12600: Loss = -12428.466310258167
8
Iteration 12700: Loss = -12428.4158576006
Iteration 12800: Loss = -12428.416134748557
1
Iteration 12900: Loss = -12428.415962553197
2
Iteration 13000: Loss = -12428.437890992505
3
Iteration 13100: Loss = -12428.41585080393
Iteration 13200: Loss = -12428.417332166964
1
Iteration 13300: Loss = -12428.429403368486
2
Iteration 13400: Loss = -12428.416845923093
3
Iteration 13500: Loss = -12428.425158872657
4
Iteration 13600: Loss = -12428.475540704905
5
Iteration 13700: Loss = -12428.422789627495
6
Iteration 13800: Loss = -12428.432809172435
7
Iteration 13900: Loss = -12428.41625965046
8
Iteration 14000: Loss = -12428.588752731408
9
Iteration 14100: Loss = -12428.417110027942
10
Iteration 14200: Loss = -12428.416735689601
11
Iteration 14300: Loss = -12428.42004697356
12
Iteration 14400: Loss = -12428.421708168107
13
Iteration 14500: Loss = -12428.444325418453
14
Iteration 14600: Loss = -12428.416303003069
15
Stopping early at iteration 14600 due to no improvement.
pi: tensor([[4.1471e-01, 5.8529e-01],
        [5.0300e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2450, 0.7550], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2541, 0.2229],
         [0.6966, 0.1971]],

        [[0.6894, 0.2304],
         [0.7074, 0.5406]],

        [[0.5117, 0.2004],
         [0.6297, 0.5757]],

        [[0.5918, 0.2267],
         [0.5448, 0.5158]],

        [[0.6430, 0.2010],
         [0.6779, 0.6741]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.011957532380478733
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0019182131166299715
Average Adjusted Rand Index: -0.0029841849641213864
11951.181126281203
[-0.0019182131166299715, -0.0019182131166299715] [-0.0029841849641213864, -0.0029841849641213864] [12428.445362156754, 12428.416303003069]
-------------------------------------
This iteration is 65
True Objective function: Loss = -11886.564267162514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23416.62495447746
Iteration 100: Loss = -12345.542214985133
Iteration 200: Loss = -12344.909080091147
Iteration 300: Loss = -12344.77565181438
Iteration 400: Loss = -12344.718222922063
Iteration 500: Loss = -12344.686558918613
Iteration 600: Loss = -12344.66586440363
Iteration 700: Loss = -12344.650502915103
Iteration 800: Loss = -12344.637808357882
Iteration 900: Loss = -12344.62615548974
Iteration 1000: Loss = -12344.614480421542
Iteration 1100: Loss = -12344.601600258882
Iteration 1200: Loss = -12344.586595724018
Iteration 1300: Loss = -12344.568616627615
Iteration 1400: Loss = -12344.547736241882
Iteration 1500: Loss = -12344.525072315102
Iteration 1600: Loss = -12344.501625265922
Iteration 1700: Loss = -12344.476631877442
Iteration 1800: Loss = -12344.448879857215
Iteration 1900: Loss = -12344.416630580768
Iteration 2000: Loss = -12344.37724834627
Iteration 2100: Loss = -12344.326252421823
Iteration 2200: Loss = -12344.25816595389
Iteration 2300: Loss = -12344.172976862874
Iteration 2400: Loss = -12344.093734629398
Iteration 2500: Loss = -12344.038615140948
Iteration 2600: Loss = -12343.977932349624
Iteration 2700: Loss = -12343.827792075404
Iteration 2800: Loss = -12343.56352151828
Iteration 2900: Loss = -12343.478392358991
Iteration 3000: Loss = -12343.42596032976
Iteration 3100: Loss = -12343.420693289037
Iteration 3200: Loss = -12343.419144756761
Iteration 3300: Loss = -12343.420801029883
1
Iteration 3400: Loss = -12343.418663949149
Iteration 3500: Loss = -12343.41876991668
1
Iteration 3600: Loss = -12343.418164051556
Iteration 3700: Loss = -12343.417644495747
Iteration 3800: Loss = -12343.417387006559
Iteration 3900: Loss = -12343.41727970156
Iteration 4000: Loss = -12343.418227462724
1
Iteration 4100: Loss = -12343.416867505917
Iteration 4200: Loss = -12343.416748074644
Iteration 4300: Loss = -12343.41659825887
Iteration 4400: Loss = -12343.416505006158
Iteration 4500: Loss = -12343.41625547949
Iteration 4600: Loss = -12343.416071721394
Iteration 4700: Loss = -12343.415966539002
Iteration 4800: Loss = -12343.416023290682
Iteration 4900: Loss = -12343.415710942027
Iteration 5000: Loss = -12343.420703925
1
Iteration 5100: Loss = -12343.415411895296
Iteration 5200: Loss = -12343.415951576595
1
Iteration 5300: Loss = -12343.415604909593
2
Iteration 5400: Loss = -12343.419700975433
3
Iteration 5500: Loss = -12343.423976604758
4
Iteration 5600: Loss = -12343.414988437273
Iteration 5700: Loss = -12343.414895499922
Iteration 5800: Loss = -12343.41472170867
Iteration 5900: Loss = -12343.414732869947
Iteration 6000: Loss = -12343.414909519352
1
Iteration 6100: Loss = -12343.414451849396
Iteration 6200: Loss = -12343.414999635803
1
Iteration 6300: Loss = -12343.414389448817
Iteration 6400: Loss = -12343.414307071878
Iteration 6500: Loss = -12343.415242764413
1
Iteration 6600: Loss = -12343.41462005561
2
Iteration 6700: Loss = -12343.430770486306
3
Iteration 6800: Loss = -12343.413968417337
Iteration 6900: Loss = -12343.414084067183
1
Iteration 7000: Loss = -12343.414196799746
2
Iteration 7100: Loss = -12343.426139744668
3
Iteration 7200: Loss = -12343.458920069548
4
Iteration 7300: Loss = -12343.413772677059
Iteration 7400: Loss = -12343.414613990262
1
Iteration 7500: Loss = -12343.413645854183
Iteration 7600: Loss = -12343.41576513459
1
Iteration 7700: Loss = -12343.413546334947
Iteration 7800: Loss = -12343.413555541785
Iteration 7900: Loss = -12343.413557791373
Iteration 8000: Loss = -12343.413502767698
Iteration 8100: Loss = -12343.413500572882
Iteration 8200: Loss = -12343.413443413778
Iteration 8300: Loss = -12343.413441922543
Iteration 8400: Loss = -12343.41461138743
1
Iteration 8500: Loss = -12343.413531709297
Iteration 8600: Loss = -12343.41508883547
1
Iteration 8700: Loss = -12343.41839697373
2
Iteration 8800: Loss = -12343.419210284343
3
Iteration 8900: Loss = -12343.418224241708
4
Iteration 9000: Loss = -12343.42253928495
5
Iteration 9100: Loss = -12343.417577293154
6
Iteration 9200: Loss = -12343.419652901837
7
Iteration 9300: Loss = -12343.420747722275
8
Iteration 9400: Loss = -12343.41323223817
Iteration 9500: Loss = -12343.414909185627
1
Iteration 9600: Loss = -12343.416956316187
2
Iteration 9700: Loss = -12343.41318788034
Iteration 9800: Loss = -12343.413865531253
1
Iteration 9900: Loss = -12343.413078877604
Iteration 10000: Loss = -12343.41325097345
1
Iteration 10100: Loss = -12343.44400816852
2
Iteration 10200: Loss = -12343.412994832408
Iteration 10300: Loss = -12343.413934869073
1
Iteration 10400: Loss = -12343.413058548613
Iteration 10500: Loss = -12343.413143513275
Iteration 10600: Loss = -12343.436946013131
1
Iteration 10700: Loss = -12343.436717202281
2
Iteration 10800: Loss = -12343.492548205324
3
Iteration 10900: Loss = -12343.427680809491
4
Iteration 11000: Loss = -12343.467429596256
5
Iteration 11100: Loss = -12343.412978570426
Iteration 11200: Loss = -12343.414409484183
1
Iteration 11300: Loss = -12343.41298264386
Iteration 11400: Loss = -12343.41297021672
Iteration 11500: Loss = -12343.430621078045
1
Iteration 11600: Loss = -12343.522507341046
2
Iteration 11700: Loss = -12343.41399141376
3
Iteration 11800: Loss = -12343.413030985266
Iteration 11900: Loss = -12343.414810943921
1
Iteration 12000: Loss = -12343.415371133133
2
Iteration 12100: Loss = -12343.413148028105
3
Iteration 12200: Loss = -12343.413327101429
4
Iteration 12300: Loss = -12343.41301440956
Iteration 12400: Loss = -12343.415656732353
1
Iteration 12500: Loss = -12343.570760128232
2
Iteration 12600: Loss = -12343.413035525786
Iteration 12700: Loss = -12343.425979238173
1
Iteration 12800: Loss = -12343.412955196132
Iteration 12900: Loss = -12343.413524615942
1
Iteration 13000: Loss = -12343.417764003236
2
Iteration 13100: Loss = -12343.419315130363
3
Iteration 13200: Loss = -12343.413384195883
4
Iteration 13300: Loss = -12343.416296252995
5
Iteration 13400: Loss = -12343.415074772993
6
Iteration 13500: Loss = -12343.416379458893
7
Iteration 13600: Loss = -12343.426236315208
8
Iteration 13700: Loss = -12343.415590097064
9
Iteration 13800: Loss = -12343.450614001791
10
Iteration 13900: Loss = -12343.415420039893
11
Iteration 14000: Loss = -12343.412915930918
Iteration 14100: Loss = -12343.41313237285
1
Iteration 14200: Loss = -12343.42088406498
2
Iteration 14300: Loss = -12343.484906534046
3
Iteration 14400: Loss = -12343.414152947224
4
Iteration 14500: Loss = -12343.41290322321
Iteration 14600: Loss = -12343.415577025038
1
Iteration 14700: Loss = -12343.428835266564
2
Iteration 14800: Loss = -12343.421925133855
3
Iteration 14900: Loss = -12343.428629524882
4
Iteration 15000: Loss = -12343.437164824472
5
Iteration 15100: Loss = -12343.414981214959
6
Iteration 15200: Loss = -12343.4225849114
7
Iteration 15300: Loss = -12343.41334329698
8
Iteration 15400: Loss = -12343.413559362498
9
Iteration 15500: Loss = -12343.413873452975
10
Iteration 15600: Loss = -12343.435890515077
11
Iteration 15700: Loss = -12343.412875475302
Iteration 15800: Loss = -12343.412964222456
Iteration 15900: Loss = -12343.412842800883
Iteration 16000: Loss = -12343.412951939883
1
Iteration 16100: Loss = -12343.412832069313
Iteration 16200: Loss = -12343.413079397764
1
Iteration 16300: Loss = -12343.41293383544
2
Iteration 16400: Loss = -12343.412868374087
Iteration 16500: Loss = -12343.47292903015
1
Iteration 16600: Loss = -12343.412822241176
Iteration 16700: Loss = -12343.420660436042
1
Iteration 16800: Loss = -12343.412822426895
Iteration 16900: Loss = -12343.413533744033
1
Iteration 17000: Loss = -12343.412805813214
Iteration 17100: Loss = -12343.416078249506
1
Iteration 17200: Loss = -12343.4128666714
Iteration 17300: Loss = -12343.414442878548
1
Iteration 17400: Loss = -12343.430276594625
2
Iteration 17500: Loss = -12343.427988591604
3
Iteration 17600: Loss = -12343.413103362467
4
Iteration 17700: Loss = -12343.412926156496
Iteration 17800: Loss = -12343.413446733071
1
Iteration 17900: Loss = -12343.465456389522
2
Iteration 18000: Loss = -12343.412825171084
Iteration 18100: Loss = -12343.42632480192
1
Iteration 18200: Loss = -12343.443331394026
2
Iteration 18300: Loss = -12343.412950576221
3
Iteration 18400: Loss = -12343.412993098309
4
Iteration 18500: Loss = -12343.650493516072
5
Iteration 18600: Loss = -12343.412833040333
Iteration 18700: Loss = -12343.436594043622
1
Iteration 18800: Loss = -12343.413087401615
2
Iteration 18900: Loss = -12343.414916517782
3
Iteration 19000: Loss = -12343.415956095523
4
Iteration 19100: Loss = -12343.41344645069
5
Iteration 19200: Loss = -12343.430671546701
6
Iteration 19300: Loss = -12343.412803044977
Iteration 19400: Loss = -12343.413127476537
1
Iteration 19500: Loss = -12343.44086470222
2
Iteration 19600: Loss = -12343.507307212309
3
Iteration 19700: Loss = -12343.41294580979
4
Iteration 19800: Loss = -12343.412887178256
Iteration 19900: Loss = -12343.422177294682
1
pi: tensor([[1.0000e+00, 1.6967e-06],
        [1.9418e-01, 8.0582e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5297, 0.4703], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2079, 0.1947],
         [0.6691, 0.1808]],

        [[0.7266, 0.1922],
         [0.7086, 0.5993]],

        [[0.5970, 0.1913],
         [0.6746, 0.7079]],

        [[0.7184, 0.1985],
         [0.6446, 0.5927]],

        [[0.6854, 0.1969],
         [0.5548, 0.5637]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.009610677805697876
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.003904091822363822
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005914731726761669
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008579267441788926
Average Adjusted Rand Index: -0.003246189898078551
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21107.978581814157
Iteration 100: Loss = -12346.00037855637
Iteration 200: Loss = -12345.114578140792
Iteration 300: Loss = -12344.90969662345
Iteration 400: Loss = -12344.813984907207
Iteration 500: Loss = -12344.754605201251
Iteration 600: Loss = -12344.711671860749
Iteration 700: Loss = -12344.678515558242
Iteration 800: Loss = -12344.651887535749
Iteration 900: Loss = -12344.62970592618
Iteration 1000: Loss = -12344.610699999916
Iteration 1100: Loss = -12344.59385068357
Iteration 1200: Loss = -12344.578350722872
Iteration 1300: Loss = -12344.56370503515
Iteration 1400: Loss = -12344.549416972599
Iteration 1500: Loss = -12344.535008709094
Iteration 1600: Loss = -12344.519986852309
Iteration 1700: Loss = -12344.503870652126
Iteration 1800: Loss = -12344.486145906038
Iteration 1900: Loss = -12344.466345372737
Iteration 2000: Loss = -12344.444176366102
Iteration 2100: Loss = -12344.419557380128
Iteration 2200: Loss = -12344.39296979445
Iteration 2300: Loss = -12344.365040007295
Iteration 2400: Loss = -12344.33671735263
Iteration 2500: Loss = -12344.30909424879
Iteration 2600: Loss = -12344.283519288405
Iteration 2700: Loss = -12344.261089386962
Iteration 2800: Loss = -12344.242097323235
Iteration 2900: Loss = -12344.226011559273
Iteration 3000: Loss = -12344.212069237894
Iteration 3100: Loss = -12344.199538180057
Iteration 3200: Loss = -12344.187918006746
Iteration 3300: Loss = -12344.176854888576
Iteration 3400: Loss = -12344.166050938627
Iteration 3500: Loss = -12344.155447596811
Iteration 3600: Loss = -12344.144902628119
Iteration 3700: Loss = -12344.134192605628
Iteration 3800: Loss = -12344.122682772992
Iteration 3900: Loss = -12344.109493052109
Iteration 4000: Loss = -12344.092601678187
Iteration 4100: Loss = -12344.070597515725
Iteration 4200: Loss = -12344.037871684903
Iteration 4300: Loss = -12343.997010814797
Iteration 4400: Loss = -12343.94064483545
Iteration 4500: Loss = -12343.868834515113
Iteration 4600: Loss = -12343.797899936957
Iteration 4700: Loss = -12343.73039011959
Iteration 4800: Loss = -12343.692480574266
Iteration 4900: Loss = -12343.668576261054
Iteration 5000: Loss = -12343.652354406056
Iteration 5100: Loss = -12343.641482361247
Iteration 5200: Loss = -12343.629214568577
Iteration 5300: Loss = -12343.61762266923
Iteration 5400: Loss = -12343.605576190957
Iteration 5500: Loss = -12343.59910765848
Iteration 5600: Loss = -12343.595862418482
Iteration 5700: Loss = -12343.592131061152
Iteration 5800: Loss = -12343.589282482373
Iteration 5900: Loss = -12343.586560234773
Iteration 6000: Loss = -12343.583732749734
Iteration 6100: Loss = -12343.581684952283
Iteration 6200: Loss = -12343.581824230034
1
Iteration 6300: Loss = -12343.575300054103
Iteration 6400: Loss = -12343.572340286939
Iteration 6500: Loss = -12343.568163046075
Iteration 6600: Loss = -12343.56325341752
Iteration 6700: Loss = -12343.55578805758
Iteration 6800: Loss = -12343.547141729052
Iteration 6900: Loss = -12343.510420719298
Iteration 7000: Loss = -12343.45815298008
Iteration 7100: Loss = -12343.44044746787
Iteration 7200: Loss = -12343.50941744388
1
Iteration 7300: Loss = -12343.423982706507
Iteration 7400: Loss = -12343.422848318742
Iteration 7500: Loss = -12343.421131071993
Iteration 7600: Loss = -12343.420594208188
Iteration 7700: Loss = -12343.419463848262
Iteration 7800: Loss = -12343.420010367172
1
Iteration 7900: Loss = -12343.41828466591
Iteration 8000: Loss = -12343.531690995247
1
Iteration 8100: Loss = -12343.417354581754
Iteration 8200: Loss = -12343.424074214146
1
Iteration 8300: Loss = -12343.419218691737
2
Iteration 8400: Loss = -12343.496279882238
3
Iteration 8500: Loss = -12343.41829986624
4
Iteration 8600: Loss = -12343.41584239807
Iteration 8700: Loss = -12343.422947813984
1
Iteration 8800: Loss = -12343.415401157945
Iteration 8900: Loss = -12343.415458359583
Iteration 9000: Loss = -12343.42365956997
1
Iteration 9100: Loss = -12343.41501633057
Iteration 9200: Loss = -12343.414896488854
Iteration 9300: Loss = -12343.477896223008
1
Iteration 9400: Loss = -12343.41449969629
Iteration 9500: Loss = -12343.421673824843
1
Iteration 9600: Loss = -12343.414355659314
Iteration 9700: Loss = -12343.636653605203
1
Iteration 9800: Loss = -12343.414172660385
Iteration 9900: Loss = -12343.414185737967
Iteration 10000: Loss = -12343.417610918174
1
Iteration 10100: Loss = -12343.413913130284
Iteration 10200: Loss = -12343.414304108803
1
Iteration 10300: Loss = -12343.413951342547
Iteration 10400: Loss = -12343.413896600723
Iteration 10500: Loss = -12343.415584433205
1
Iteration 10600: Loss = -12343.49981381365
2
Iteration 10700: Loss = -12343.443114154617
3
Iteration 10800: Loss = -12343.44326996048
4
Iteration 10900: Loss = -12343.413603287097
Iteration 11000: Loss = -12343.413652632247
Iteration 11100: Loss = -12343.416910033204
1
Iteration 11200: Loss = -12343.417815566478
2
Iteration 11300: Loss = -12343.41373360745
Iteration 11400: Loss = -12343.413605361198
Iteration 11500: Loss = -12343.414008321517
1
Iteration 11600: Loss = -12343.413552220802
Iteration 11700: Loss = -12343.413337412987
Iteration 11800: Loss = -12343.414021716868
1
Iteration 11900: Loss = -12343.599791602854
2
Iteration 12000: Loss = -12343.414581086086
3
Iteration 12100: Loss = -12343.413240099293
Iteration 12200: Loss = -12343.414439975657
1
Iteration 12300: Loss = -12343.413128629893
Iteration 12400: Loss = -12343.41387092363
1
Iteration 12500: Loss = -12343.431477593504
2
Iteration 12600: Loss = -12343.423798124913
3
Iteration 12700: Loss = -12343.41369093306
4
Iteration 12800: Loss = -12343.43726776429
5
Iteration 12900: Loss = -12343.413309228852
6
Iteration 13000: Loss = -12343.41319728388
Iteration 13100: Loss = -12343.413496018573
1
Iteration 13200: Loss = -12343.416841363996
2
Iteration 13300: Loss = -12343.438376377615
3
Iteration 13400: Loss = -12343.413242958854
Iteration 13500: Loss = -12343.412982562022
Iteration 13600: Loss = -12343.4836983944
1
Iteration 13700: Loss = -12343.416137561493
2
Iteration 13800: Loss = -12343.461308942962
3
Iteration 13900: Loss = -12343.413018275289
Iteration 14000: Loss = -12343.413123398495
1
Iteration 14100: Loss = -12343.45162119754
2
Iteration 14200: Loss = -12343.412917770986
Iteration 14300: Loss = -12343.422980285462
1
Iteration 14400: Loss = -12343.412969403022
Iteration 14500: Loss = -12343.413075703798
1
Iteration 14600: Loss = -12343.445487171652
2
Iteration 14700: Loss = -12343.428910452414
3
Iteration 14800: Loss = -12343.413419450491
4
Iteration 14900: Loss = -12343.41791252793
5
Iteration 15000: Loss = -12343.41315231082
6
Iteration 15100: Loss = -12343.41297937389
Iteration 15200: Loss = -12343.415158077174
1
Iteration 15300: Loss = -12343.412919827786
Iteration 15400: Loss = -12343.41304764647
1
Iteration 15500: Loss = -12343.42153913544
2
Iteration 15600: Loss = -12343.414415989537
3
Iteration 15700: Loss = -12343.425875499377
4
Iteration 15800: Loss = -12343.425525444252
5
Iteration 15900: Loss = -12343.416655811368
6
Iteration 16000: Loss = -12343.526606099424
7
Iteration 16100: Loss = -12343.413874971304
8
Iteration 16200: Loss = -12343.41312181293
9
Iteration 16300: Loss = -12343.412960729982
Iteration 16400: Loss = -12343.412980891133
Iteration 16500: Loss = -12343.43812020809
1
Iteration 16600: Loss = -12343.412896772175
Iteration 16700: Loss = -12343.41417902005
1
Iteration 16800: Loss = -12343.56984898515
2
Iteration 16900: Loss = -12343.412849571534
Iteration 17000: Loss = -12343.415179007083
1
Iteration 17100: Loss = -12343.688007540022
2
Iteration 17200: Loss = -12343.412873852716
Iteration 17300: Loss = -12343.41333763208
1
Iteration 17400: Loss = -12343.429823676539
2
Iteration 17500: Loss = -12343.412864115857
Iteration 17600: Loss = -12343.41348359617
1
Iteration 17700: Loss = -12343.523713094271
2
Iteration 17800: Loss = -12343.413842853113
3
Iteration 17900: Loss = -12343.41298582954
4
Iteration 18000: Loss = -12343.436912527406
5
Iteration 18100: Loss = -12343.412868679907
Iteration 18200: Loss = -12343.413361394096
1
Iteration 18300: Loss = -12343.414433957663
2
Iteration 18400: Loss = -12343.417043324838
3
Iteration 18500: Loss = -12343.543384066892
4
Iteration 18600: Loss = -12343.412839182658
Iteration 18700: Loss = -12343.413135518553
1
Iteration 18800: Loss = -12343.453280404194
2
Iteration 18900: Loss = -12343.415746212513
3
Iteration 19000: Loss = -12343.412905145457
Iteration 19100: Loss = -12343.413109653691
1
Iteration 19200: Loss = -12343.414745643386
2
Iteration 19300: Loss = -12343.437559630893
3
Iteration 19400: Loss = -12343.413845505906
4
Iteration 19500: Loss = -12343.412844531347
Iteration 19600: Loss = -12343.413177972967
1
Iteration 19700: Loss = -12343.631150180041
2
Iteration 19800: Loss = -12343.412867280287
Iteration 19900: Loss = -12343.413857998234
1
pi: tensor([[8.0501e-01, 1.9499e-01],
        [6.3203e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4643, 0.5357], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1806, 0.1943],
         [0.5433, 0.2081]],

        [[0.7015, 0.1917],
         [0.6553, 0.5161]],

        [[0.7296, 0.1911],
         [0.5060, 0.7069]],

        [[0.6138, 0.1986],
         [0.6080, 0.5131]],

        [[0.5479, 0.1972],
         [0.5983, 0.5715]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.009396564381148142
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0030260508815813554
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0037746410354473374
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008720838931646074
Average Adjusted Rand Index: -0.003296853178499512
11886.564267162514
[-0.0008579267441788926, -0.0008720838931646074] [-0.003246189898078551, -0.003296853178499512] [12343.484584805923, 12343.562609163173]
-------------------------------------
This iteration is 66
True Objective function: Loss = -11817.006732070342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21509.81911848616
Iteration 100: Loss = -12299.534862317198
Iteration 200: Loss = -12298.93692378561
Iteration 300: Loss = -12298.764007269021
Iteration 400: Loss = -12298.657515960436
Iteration 500: Loss = -12298.571389728979
Iteration 600: Loss = -12298.485128551814
Iteration 700: Loss = -12298.376449995463
Iteration 800: Loss = -12298.200287473937
Iteration 900: Loss = -12297.917034038494
Iteration 1000: Loss = -12297.670481815168
Iteration 1100: Loss = -12297.417999214522
Iteration 1200: Loss = -12297.125208768288
Iteration 1300: Loss = -12296.838854841339
Iteration 1400: Loss = -12296.575394311185
Iteration 1500: Loss = -12296.331486399988
Iteration 1600: Loss = -12296.110824161124
Iteration 1700: Loss = -12295.934481715676
Iteration 1800: Loss = -12295.805328297196
Iteration 1900: Loss = -12295.716238007475
Iteration 2000: Loss = -12295.655815215187
Iteration 2100: Loss = -12295.613222994849
Iteration 2200: Loss = -12295.582110255029
Iteration 2300: Loss = -12295.558375883345
Iteration 2400: Loss = -12295.539385411415
Iteration 2500: Loss = -12295.523755208887
Iteration 2600: Loss = -12295.51084645201
Iteration 2700: Loss = -12295.500064669617
Iteration 2800: Loss = -12295.490877167464
Iteration 2900: Loss = -12295.483099735691
Iteration 3000: Loss = -12295.476391551983
Iteration 3100: Loss = -12295.470580365925
Iteration 3200: Loss = -12295.465457054592
Iteration 3300: Loss = -12295.460969034295
Iteration 3400: Loss = -12295.4570264251
Iteration 3500: Loss = -12295.453452043526
Iteration 3600: Loss = -12295.450311441205
Iteration 3700: Loss = -12295.447471510728
Iteration 3800: Loss = -12295.444951531046
Iteration 3900: Loss = -12295.442610263426
Iteration 4000: Loss = -12295.440539023759
Iteration 4100: Loss = -12295.438619329376
Iteration 4200: Loss = -12295.436889910898
Iteration 4300: Loss = -12295.435261359604
Iteration 4400: Loss = -12295.433836259124
Iteration 4500: Loss = -12295.432445286118
Iteration 4600: Loss = -12295.43198728515
Iteration 4700: Loss = -12295.43012855441
Iteration 4800: Loss = -12295.429058205524
Iteration 4900: Loss = -12295.42825305328
Iteration 5000: Loss = -12295.427181097053
Iteration 5100: Loss = -12295.426315763898
Iteration 5200: Loss = -12295.42562666489
Iteration 5300: Loss = -12295.424830219968
Iteration 5400: Loss = -12295.424130016392
Iteration 5500: Loss = -12295.423533680261
Iteration 5600: Loss = -12295.422964396203
Iteration 5700: Loss = -12295.422540169533
Iteration 5800: Loss = -12295.421841716494
Iteration 5900: Loss = -12295.421379674823
Iteration 6000: Loss = -12295.421006733732
Iteration 6100: Loss = -12295.420545967534
Iteration 6200: Loss = -12295.420129505601
Iteration 6300: Loss = -12295.419753999131
Iteration 6400: Loss = -12295.419366885475
Iteration 6500: Loss = -12295.419129240605
Iteration 6600: Loss = -12295.418806281563
Iteration 6700: Loss = -12295.420711099528
1
Iteration 6800: Loss = -12295.418217577722
Iteration 6900: Loss = -12295.417973855398
Iteration 7000: Loss = -12295.421266480806
1
Iteration 7100: Loss = -12295.417447507218
Iteration 7200: Loss = -12295.417353657871
Iteration 7300: Loss = -12295.444032558338
1
Iteration 7400: Loss = -12295.416855750307
Iteration 7500: Loss = -12295.41685362337
Iteration 7600: Loss = -12295.4167190297
Iteration 7700: Loss = -12295.41638397446
Iteration 7800: Loss = -12295.416154218909
Iteration 7900: Loss = -12295.552404988239
1
Iteration 8000: Loss = -12295.415870415574
Iteration 8100: Loss = -12295.415744619891
Iteration 8200: Loss = -12295.78946374012
1
Iteration 8300: Loss = -12295.415539876103
Iteration 8400: Loss = -12295.415379854918
Iteration 8500: Loss = -12295.415311973724
Iteration 8600: Loss = -12295.41536521224
Iteration 8700: Loss = -12295.415074590917
Iteration 8800: Loss = -12295.41502629063
Iteration 8900: Loss = -12295.420450904414
1
Iteration 9000: Loss = -12295.414829777772
Iteration 9100: Loss = -12295.41477872437
Iteration 9200: Loss = -12295.43281712481
1
Iteration 9300: Loss = -12295.414652769017
Iteration 9400: Loss = -12295.414542718958
Iteration 9500: Loss = -12295.54625726983
1
Iteration 9600: Loss = -12295.414464065143
Iteration 9700: Loss = -12295.414402771534
Iteration 9800: Loss = -12295.414333462846
Iteration 9900: Loss = -12295.41438637494
Iteration 10000: Loss = -12295.414261569236
Iteration 10100: Loss = -12295.414196768641
Iteration 10200: Loss = -12295.414867907355
1
Iteration 10300: Loss = -12295.414104752284
Iteration 10400: Loss = -12295.414066234842
Iteration 10500: Loss = -12295.427438108552
1
Iteration 10600: Loss = -12295.413991080832
Iteration 10700: Loss = -12295.413965454596
Iteration 10800: Loss = -12295.509701926085
1
Iteration 10900: Loss = -12295.413910761501
Iteration 11000: Loss = -12295.413864711234
Iteration 11100: Loss = -12295.437643352252
1
Iteration 11200: Loss = -12295.413858229302
Iteration 11300: Loss = -12295.413791417994
Iteration 11400: Loss = -12295.415421331316
1
Iteration 11500: Loss = -12295.413751025804
Iteration 11600: Loss = -12295.413737606677
Iteration 11700: Loss = -12295.41371573217
Iteration 11800: Loss = -12295.413674955047
Iteration 11900: Loss = -12295.6816210316
1
Iteration 12000: Loss = -12295.413678145516
Iteration 12100: Loss = -12295.42158385668
1
Iteration 12200: Loss = -12295.41362842765
Iteration 12300: Loss = -12295.424380333809
1
Iteration 12400: Loss = -12295.413630834035
Iteration 12500: Loss = -12295.42201429952
1
Iteration 12600: Loss = -12295.413638900889
Iteration 12700: Loss = -12295.413603159352
Iteration 12800: Loss = -12295.418097870916
1
Iteration 12900: Loss = -12295.413555920197
Iteration 13000: Loss = -12295.420781468638
1
Iteration 13100: Loss = -12295.413502762487
Iteration 13200: Loss = -12295.429250045263
1
Iteration 13300: Loss = -12295.413839254481
2
Iteration 13400: Loss = -12295.413521567069
Iteration 13500: Loss = -12295.41375393123
1
Iteration 13600: Loss = -12295.413794531498
2
Iteration 13700: Loss = -12295.413494125498
Iteration 13800: Loss = -12295.413707224516
1
Iteration 13900: Loss = -12295.41350179679
Iteration 14000: Loss = -12295.436274772637
1
Iteration 14100: Loss = -12295.413462346452
Iteration 14200: Loss = -12295.413488556796
Iteration 14300: Loss = -12295.413555245348
Iteration 14400: Loss = -12295.513542798315
1
Iteration 14500: Loss = -12295.413435513046
Iteration 14600: Loss = -12295.41860548648
1
Iteration 14700: Loss = -12295.636730687324
2
Iteration 14800: Loss = -12295.413411942176
Iteration 14900: Loss = -12295.413494271319
Iteration 15000: Loss = -12295.413460511063
Iteration 15100: Loss = -12295.417006872154
1
Iteration 15200: Loss = -12295.41344979338
Iteration 15300: Loss = -12295.417603884325
1
Iteration 15400: Loss = -12295.41413712192
2
Iteration 15500: Loss = -12295.414505749059
3
Iteration 15600: Loss = -12295.429321948639
4
Iteration 15700: Loss = -12295.41348969186
Iteration 15800: Loss = -12295.413555331768
Iteration 15900: Loss = -12295.42631856506
1
Iteration 16000: Loss = -12295.413582130752
Iteration 16100: Loss = -12295.414123472265
1
Iteration 16200: Loss = -12295.413406355312
Iteration 16300: Loss = -12295.418816709758
1
Iteration 16400: Loss = -12295.415004458046
2
Iteration 16500: Loss = -12295.413575758555
3
Iteration 16600: Loss = -12295.414306813045
4
Iteration 16700: Loss = -12295.413394263416
Iteration 16800: Loss = -12295.413423171336
Iteration 16900: Loss = -12295.413377451856
Iteration 17000: Loss = -12295.429816076863
1
Iteration 17100: Loss = -12295.422224919641
2
Iteration 17200: Loss = -12295.413407463333
Iteration 17300: Loss = -12295.413684752726
1
Iteration 17400: Loss = -12295.415441701016
2
Iteration 17500: Loss = -12295.413410450394
Iteration 17600: Loss = -12295.424585429302
1
Iteration 17700: Loss = -12295.413358172704
Iteration 17800: Loss = -12295.41929871147
1
Iteration 17900: Loss = -12295.42835129015
2
Iteration 18000: Loss = -12295.41357568358
3
Iteration 18100: Loss = -12295.413501196423
4
Iteration 18200: Loss = -12295.413887596842
5
Iteration 18300: Loss = -12295.419042351576
6
Iteration 18400: Loss = -12295.413522852044
7
Iteration 18500: Loss = -12295.41349953001
8
Iteration 18600: Loss = -12295.425405122864
9
Iteration 18700: Loss = -12295.41338385101
Iteration 18800: Loss = -12295.421993276519
1
Iteration 18900: Loss = -12295.41339370644
Iteration 19000: Loss = -12295.414150761731
1
Iteration 19100: Loss = -12295.422294454325
2
Iteration 19200: Loss = -12295.413385398988
Iteration 19300: Loss = -12295.608379342442
1
Iteration 19400: Loss = -12295.413385639291
Iteration 19500: Loss = -12295.414546119915
1
Iteration 19600: Loss = -12295.41782357613
2
Iteration 19700: Loss = -12295.4144434061
3
Iteration 19800: Loss = -12295.415489497233
4
Iteration 19900: Loss = -12295.413572397572
5
pi: tensor([[5.0702e-01, 4.9298e-01],
        [4.5772e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1003, 0.8997], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2187, 0.2056],
         [0.6082, 0.1956]],

        [[0.5203, 0.2447],
         [0.7301, 0.6990]],

        [[0.5946, 0.2987],
         [0.7308, 0.6175]],

        [[0.5163, 0.1042],
         [0.6797, 0.7003]],

        [[0.6747, 0.1651],
         [0.5807, 0.6034]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005382679332116075
Average Adjusted Rand Index: -0.0024338329504051777
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22157.34612534869
Iteration 100: Loss = -12299.154838492334
Iteration 200: Loss = -12298.843307678018
Iteration 300: Loss = -12298.742125334895
Iteration 400: Loss = -12298.679712766707
Iteration 500: Loss = -12298.634552127793
Iteration 600: Loss = -12298.597845235172
Iteration 700: Loss = -12298.564712053752
Iteration 800: Loss = -12298.531110964279
Iteration 900: Loss = -12298.492180875155
Iteration 1000: Loss = -12298.4418399401
Iteration 1100: Loss = -12298.372116015093
Iteration 1200: Loss = -12298.251386335329
Iteration 1300: Loss = -12297.917894654916
Iteration 1400: Loss = -12297.64771616917
Iteration 1500: Loss = -12297.377579016515
Iteration 1600: Loss = -12297.03454106429
Iteration 1700: Loss = -12296.712326637113
Iteration 1800: Loss = -12296.4300639583
Iteration 1900: Loss = -12296.164271667592
Iteration 2000: Loss = -12295.940951948232
Iteration 2100: Loss = -12295.786088421868
Iteration 2200: Loss = -12295.686978680606
Iteration 2300: Loss = -12295.624272385163
Iteration 2400: Loss = -12295.585522455904
Iteration 2500: Loss = -12295.558080814097
Iteration 2600: Loss = -12295.535879608424
Iteration 2700: Loss = -12295.517303914017
Iteration 2800: Loss = -12295.501679891868
Iteration 2900: Loss = -12295.488990201391
Iteration 3000: Loss = -12295.478870124736
Iteration 3100: Loss = -12295.470870905876
Iteration 3200: Loss = -12295.464388479326
Iteration 3300: Loss = -12295.459143352367
Iteration 3400: Loss = -12295.454641635964
Iteration 3500: Loss = -12295.450820933573
Iteration 3600: Loss = -12295.447502350953
Iteration 3700: Loss = -12295.444568952638
Iteration 3800: Loss = -12295.442003251406
Iteration 3900: Loss = -12295.443599586686
1
Iteration 4000: Loss = -12295.437714741025
Iteration 4100: Loss = -12295.43584681622
Iteration 4200: Loss = -12295.435184933298
Iteration 4300: Loss = -12295.432668478674
Iteration 4400: Loss = -12295.431353825068
Iteration 4500: Loss = -12295.430131651952
Iteration 4600: Loss = -12295.428952393648
Iteration 4700: Loss = -12295.427935545027
Iteration 4800: Loss = -12295.426951058675
Iteration 4900: Loss = -12295.426079264149
Iteration 5000: Loss = -12295.425260659977
Iteration 5100: Loss = -12295.42452056785
Iteration 5200: Loss = -12295.423807092358
Iteration 5300: Loss = -12295.423197684098
Iteration 5400: Loss = -12295.422668667843
Iteration 5500: Loss = -12295.42206264452
Iteration 5600: Loss = -12295.421542235255
Iteration 5700: Loss = -12295.421075610418
Iteration 5800: Loss = -12295.420626301799
Iteration 5900: Loss = -12295.42017427932
Iteration 6000: Loss = -12295.419811139842
Iteration 6100: Loss = -12295.41945953761
Iteration 6200: Loss = -12295.419148821939
Iteration 6300: Loss = -12295.418821853187
Iteration 6400: Loss = -12295.418472396224
Iteration 6500: Loss = -12295.41820087453
Iteration 6600: Loss = -12295.41798501829
Iteration 6700: Loss = -12295.417734465036
Iteration 6800: Loss = -12295.417426891676
Iteration 6900: Loss = -12295.417262262587
Iteration 7000: Loss = -12295.417011959255
Iteration 7100: Loss = -12295.416832017827
Iteration 7200: Loss = -12295.416704614841
Iteration 7300: Loss = -12295.416461023442
Iteration 7400: Loss = -12295.42084085543
1
Iteration 7500: Loss = -12295.416133175695
Iteration 7600: Loss = -12295.416037243112
Iteration 7700: Loss = -12295.415849464447
Iteration 7800: Loss = -12295.415737443902
Iteration 7900: Loss = -12295.415578963444
Iteration 8000: Loss = -12295.415455610468
Iteration 8100: Loss = -12295.417852970326
1
Iteration 8200: Loss = -12295.415231393761
Iteration 8300: Loss = -12295.415176751863
Iteration 8400: Loss = -12295.44705626253
1
Iteration 8500: Loss = -12295.414978517761
Iteration 8600: Loss = -12295.414886952154
Iteration 8700: Loss = -12295.414851037272
Iteration 8800: Loss = -12295.415041942828
1
Iteration 8900: Loss = -12295.414707999125
Iteration 9000: Loss = -12295.414592587653
Iteration 9100: Loss = -12295.432225333006
1
Iteration 9200: Loss = -12295.41444407146
Iteration 9300: Loss = -12295.414466799115
Iteration 9400: Loss = -12295.414316690456
Iteration 9500: Loss = -12295.415079767203
1
Iteration 9600: Loss = -12295.414220924144
Iteration 9700: Loss = -12295.414218620264
Iteration 9800: Loss = -12295.421892252665
1
Iteration 9900: Loss = -12295.414203801303
Iteration 10000: Loss = -12295.414147306132
Iteration 10100: Loss = -12295.41404957367
Iteration 10200: Loss = -12295.415800575394
1
Iteration 10300: Loss = -12295.413984008821
Iteration 10400: Loss = -12295.413958426974
Iteration 10500: Loss = -12295.48942793784
1
Iteration 10600: Loss = -12295.413894441344
Iteration 10700: Loss = -12295.41388476097
Iteration 10800: Loss = -12295.413856214293
Iteration 10900: Loss = -12295.414307795736
1
Iteration 11000: Loss = -12295.413798699958
Iteration 11100: Loss = -12295.41378431513
Iteration 11200: Loss = -12295.421306815646
1
Iteration 11300: Loss = -12295.413721903367
Iteration 11400: Loss = -12295.41372049812
Iteration 11500: Loss = -12295.413855336436
1
Iteration 11600: Loss = -12295.413689610516
Iteration 11700: Loss = -12295.413671559138
Iteration 11800: Loss = -12295.413653887415
Iteration 11900: Loss = -12295.413816905744
1
Iteration 12000: Loss = -12295.413587877058
Iteration 12100: Loss = -12295.41358141937
Iteration 12200: Loss = -12295.41383620809
1
Iteration 12300: Loss = -12295.413628326856
Iteration 12400: Loss = -12295.414303259078
1
Iteration 12500: Loss = -12295.413573999453
Iteration 12600: Loss = -12295.414648775582
1
Iteration 12700: Loss = -12295.413827803495
2
Iteration 12800: Loss = -12295.43715069393
3
Iteration 12900: Loss = -12295.413551044641
Iteration 13000: Loss = -12295.420192886208
1
Iteration 13100: Loss = -12295.413475560781
Iteration 13200: Loss = -12295.417198161556
1
Iteration 13300: Loss = -12295.413469611221
Iteration 13400: Loss = -12295.415825410417
1
Iteration 13500: Loss = -12295.413608727158
2
Iteration 13600: Loss = -12295.4962958202
3
Iteration 13700: Loss = -12295.436131381663
4
Iteration 13800: Loss = -12295.41350907025
Iteration 13900: Loss = -12295.413776260892
1
Iteration 14000: Loss = -12295.435061556018
2
Iteration 14100: Loss = -12295.415398259625
3
Iteration 14200: Loss = -12295.413535689706
Iteration 14300: Loss = -12295.49508002977
1
Iteration 14400: Loss = -12295.41353673716
Iteration 14500: Loss = -12295.413970260945
1
Iteration 14600: Loss = -12295.426164579017
2
Iteration 14700: Loss = -12295.415334102734
3
Iteration 14800: Loss = -12295.413826998705
4
Iteration 14900: Loss = -12295.422358653013
5
Iteration 15000: Loss = -12295.413597171257
Iteration 15100: Loss = -12295.413473975257
Iteration 15200: Loss = -12295.537100851934
1
Iteration 15300: Loss = -12295.413384172032
Iteration 15400: Loss = -12295.54351821865
1
Iteration 15500: Loss = -12295.413435915072
Iteration 15600: Loss = -12295.442890555065
1
Iteration 15700: Loss = -12295.413409111985
Iteration 15800: Loss = -12295.41390861046
1
Iteration 15900: Loss = -12295.41367696319
2
Iteration 16000: Loss = -12295.415148798022
3
Iteration 16100: Loss = -12295.444814939585
4
Iteration 16200: Loss = -12295.46224245089
5
Iteration 16300: Loss = -12295.413715971223
6
Iteration 16400: Loss = -12295.413485783776
Iteration 16500: Loss = -12295.511469497602
1
Iteration 16600: Loss = -12295.414175005742
2
Iteration 16700: Loss = -12295.414185720716
3
Iteration 16800: Loss = -12295.431738837904
4
Iteration 16900: Loss = -12295.413407066877
Iteration 17000: Loss = -12295.414795064506
1
Iteration 17100: Loss = -12295.443002245256
2
Iteration 17200: Loss = -12295.41337942481
Iteration 17300: Loss = -12295.41369594791
1
Iteration 17400: Loss = -12295.443044033389
2
Iteration 17500: Loss = -12295.589165958494
3
Iteration 17600: Loss = -12295.413588467405
4
Iteration 17700: Loss = -12295.413683508277
5
Iteration 17800: Loss = -12295.44775012652
6
Iteration 17900: Loss = -12295.465481924062
7
Iteration 18000: Loss = -12295.413605320797
8
Iteration 18100: Loss = -12295.413591245768
9
Iteration 18200: Loss = -12295.414620142094
10
Iteration 18300: Loss = -12295.414803280379
11
Iteration 18400: Loss = -12295.413621206955
12
Iteration 18500: Loss = -12295.413569887249
13
Iteration 18600: Loss = -12295.435276145288
14
Iteration 18700: Loss = -12295.419197835898
15
Stopping early at iteration 18700 due to no improvement.
pi: tensor([[1.0000e+00, 7.4490e-08],
        [4.9322e-01, 5.0678e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9004, 0.0996], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1956, 0.2058],
         [0.7225, 0.2190]],

        [[0.5055, 0.2461],
         [0.6519, 0.6999]],

        [[0.6591, 0.3009],
         [0.5868, 0.5933]],

        [[0.5661, 0.1035],
         [0.6486, 0.7008]],

        [[0.6973, 0.1646],
         [0.5219, 0.7058]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005382679332116075
Average Adjusted Rand Index: -0.0024338329504051777
11817.006732070342
[-0.0005382679332116075, -0.0005382679332116075] [-0.0024338329504051777, -0.0024338329504051777] [12295.41350900877, 12295.419197835898]
-------------------------------------
This iteration is 67
True Objective function: Loss = -11884.006001793823
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21660.14554766348
Iteration 100: Loss = -12449.345717546783
Iteration 200: Loss = -12448.613778585795
Iteration 300: Loss = -12448.365384833676
Iteration 400: Loss = -12448.243628357437
Iteration 500: Loss = -12448.183954965885
Iteration 600: Loss = -12448.152517359122
Iteration 700: Loss = -12448.133191277651
Iteration 800: Loss = -12448.118921233157
Iteration 900: Loss = -12448.106018783148
Iteration 1000: Loss = -12448.090870151538
Iteration 1100: Loss = -12448.064505784072
Iteration 1200: Loss = -12447.962176669982
Iteration 1300: Loss = -12447.181122440124
Iteration 1400: Loss = -12368.249320812942
Iteration 1500: Loss = -12359.04368023017
Iteration 1600: Loss = -12359.02025120796
Iteration 1700: Loss = -12359.009069627054
Iteration 1800: Loss = -12359.002212736928
Iteration 1900: Loss = -12358.99753366799
Iteration 2000: Loss = -12358.994361357369
Iteration 2100: Loss = -12358.992070638007
Iteration 2200: Loss = -12358.990329923501
Iteration 2300: Loss = -12358.988979725158
Iteration 2400: Loss = -12358.987802403326
Iteration 2500: Loss = -12358.986838183253
Iteration 2600: Loss = -12358.985998916018
Iteration 2700: Loss = -12358.985226414114
Iteration 2800: Loss = -12358.984460502506
Iteration 2900: Loss = -12358.983757480193
Iteration 3000: Loss = -12358.983046236479
Iteration 3100: Loss = -12358.98214798549
Iteration 3200: Loss = -12358.980286199663
Iteration 3300: Loss = -12358.978464783504
Iteration 3400: Loss = -12358.978190102054
Iteration 3500: Loss = -12358.977914724668
Iteration 3600: Loss = -12358.977668819973
Iteration 3700: Loss = -12358.977387868124
Iteration 3800: Loss = -12358.977106313687
Iteration 3900: Loss = -12358.977455021763
1
Iteration 4000: Loss = -12358.975895548689
Iteration 4100: Loss = -12358.97062441644
Iteration 4200: Loss = -12358.969433665936
Iteration 4300: Loss = -12358.969323987034
Iteration 4400: Loss = -12358.969142823
Iteration 4500: Loss = -12358.968986426498
Iteration 4600: Loss = -12358.968822909284
Iteration 4700: Loss = -12358.968707342636
Iteration 4800: Loss = -12358.968699807223
Iteration 4900: Loss = -12358.968502990096
Iteration 5000: Loss = -12358.96856849763
Iteration 5100: Loss = -12358.968370000443
Iteration 5200: Loss = -12358.968476953225
1
Iteration 5300: Loss = -12358.968154995857
Iteration 5400: Loss = -12358.968142395046
Iteration 5500: Loss = -12358.970898061083
1
Iteration 5600: Loss = -12358.96730746324
Iteration 5700: Loss = -12358.96595818585
Iteration 5800: Loss = -12358.968417869559
1
Iteration 5900: Loss = -12358.965884401532
Iteration 6000: Loss = -12358.965822436865
Iteration 6100: Loss = -12358.966350229475
1
Iteration 6200: Loss = -12358.965764807253
Iteration 6300: Loss = -12358.965817154362
Iteration 6400: Loss = -12358.965725939703
Iteration 6500: Loss = -12358.96583556336
1
Iteration 6600: Loss = -12358.96908559247
2
Iteration 6700: Loss = -12358.965637486193
Iteration 6800: Loss = -12358.965782076886
1
Iteration 6900: Loss = -12358.966480074992
2
Iteration 7000: Loss = -12358.966314437079
3
Iteration 7100: Loss = -12358.966439571355
4
Iteration 7200: Loss = -12358.965810889575
5
Iteration 7300: Loss = -12359.126330614212
6
Iteration 7400: Loss = -12358.968007458894
7
Iteration 7500: Loss = -12358.965502029123
Iteration 7600: Loss = -12358.96764091521
1
Iteration 7700: Loss = -12358.965471990961
Iteration 7800: Loss = -12358.965442599552
Iteration 7900: Loss = -12359.013329294372
1
Iteration 8000: Loss = -12358.965439780895
Iteration 8100: Loss = -12358.965445691172
Iteration 8200: Loss = -12359.48253726176
1
Iteration 8300: Loss = -12358.965503436613
Iteration 8400: Loss = -12358.965426599058
Iteration 8500: Loss = -12358.997091023733
1
Iteration 8600: Loss = -12358.966533077883
2
Iteration 8700: Loss = -12358.966638812462
3
Iteration 8800: Loss = -12358.967807073257
4
Iteration 8900: Loss = -12358.965379982652
Iteration 9000: Loss = -12358.99074154643
1
Iteration 9100: Loss = -12358.96649574094
2
Iteration 9200: Loss = -12358.976403006916
3
Iteration 9300: Loss = -12358.965321458129
Iteration 9400: Loss = -12358.972905883149
1
Iteration 9500: Loss = -12358.97026130816
2
Iteration 9600: Loss = -12358.966098833816
3
Iteration 9700: Loss = -12358.974824832363
4
Iteration 9800: Loss = -12358.966582028706
5
Iteration 9900: Loss = -12359.095346030253
6
Iteration 10000: Loss = -12358.965558528507
7
Iteration 10100: Loss = -12358.965387629098
Iteration 10200: Loss = -12358.987736670244
1
Iteration 10300: Loss = -12358.965373920308
Iteration 10400: Loss = -12358.971830980918
1
Iteration 10500: Loss = -12358.96610844184
2
Iteration 10600: Loss = -12358.967147506577
3
Iteration 10700: Loss = -12358.965350885492
Iteration 10800: Loss = -12358.965404944252
Iteration 10900: Loss = -12358.965342028701
Iteration 11000: Loss = -12358.972568786085
1
Iteration 11100: Loss = -12359.001655525311
2
Iteration 11200: Loss = -12358.965383037877
Iteration 11300: Loss = -12358.965288206871
Iteration 11400: Loss = -12358.988874553237
1
Iteration 11500: Loss = -12358.966503185158
2
Iteration 11600: Loss = -12358.971186950812
3
Iteration 11700: Loss = -12358.979854252677
4
Iteration 11800: Loss = -12359.173518414627
5
Iteration 11900: Loss = -12358.965484387467
6
Iteration 12000: Loss = -12358.999255669429
7
Iteration 12100: Loss = -12358.97048828655
8
Iteration 12200: Loss = -12358.966810163312
9
Iteration 12300: Loss = -12358.96716802681
10
Iteration 12400: Loss = -12359.173655090253
11
Iteration 12500: Loss = -12358.966465634141
12
Iteration 12600: Loss = -12358.967142036408
13
Iteration 12700: Loss = -12358.966623590315
14
Iteration 12800: Loss = -12358.965845835319
15
Stopping early at iteration 12800 due to no improvement.
pi: tensor([[5.0874e-07, 1.0000e+00],
        [4.3867e-02, 9.5613e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5782, 0.4218], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3167, 0.0993],
         [0.7143, 0.2046]],

        [[0.5373, 0.0955],
         [0.5654, 0.5173]],

        [[0.5148, 0.2260],
         [0.5370, 0.5621]],

        [[0.7267, 0.2061],
         [0.5618, 0.6675]],

        [[0.5820, 0.2691],
         [0.6528, 0.5964]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.05474626699938599
Average Adjusted Rand Index: 0.19912050216985638
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20562.618091865854
Iteration 100: Loss = -12452.003295839233
Iteration 200: Loss = -12451.474779157068
Iteration 300: Loss = -12451.233096318316
Iteration 400: Loss = -12450.95527618047
Iteration 500: Loss = -12450.289627956507
Iteration 600: Loss = -12448.999710118169
Iteration 700: Loss = -12447.970407362513
Iteration 800: Loss = -12447.154310901136
Iteration 900: Loss = -12446.799642823064
Iteration 1000: Loss = -12446.621292608575
Iteration 1100: Loss = -12446.511707246435
Iteration 1200: Loss = -12446.43480828017
Iteration 1300: Loss = -12446.376374513147
Iteration 1400: Loss = -12446.329414966182
Iteration 1500: Loss = -12446.290178198702
Iteration 1600: Loss = -12446.256131553639
Iteration 1700: Loss = -12446.225752323027
Iteration 1800: Loss = -12446.198061820614
Iteration 1900: Loss = -12446.172348880102
Iteration 2000: Loss = -12446.148374206025
Iteration 2100: Loss = -12446.125974800409
Iteration 2200: Loss = -12446.105031218862
Iteration 2300: Loss = -12446.085728254511
Iteration 2400: Loss = -12446.067996332275
Iteration 2500: Loss = -12446.051929707744
Iteration 2600: Loss = -12446.03725383279
Iteration 2700: Loss = -12446.023825432238
Iteration 2800: Loss = -12446.011243480003
Iteration 2900: Loss = -12445.998847492112
Iteration 3000: Loss = -12445.983418476202
Iteration 3100: Loss = -12445.783371927515
Iteration 3200: Loss = -12359.310349654312
Iteration 3300: Loss = -12359.15671221555
Iteration 3400: Loss = -12359.109920707297
Iteration 3500: Loss = -12359.085719350485
Iteration 3600: Loss = -12359.07059389008
Iteration 3700: Loss = -12359.059815512046
Iteration 3800: Loss = -12359.051722296388
Iteration 3900: Loss = -12359.04527741235
Iteration 4000: Loss = -12359.040051473887
Iteration 4100: Loss = -12359.0355921575
Iteration 4200: Loss = -12359.031762408576
Iteration 4300: Loss = -12359.028071318773
Iteration 4400: Loss = -12359.02364171106
Iteration 4500: Loss = -12359.00580190336
Iteration 4600: Loss = -12358.99159617517
Iteration 4700: Loss = -12358.989615912922
Iteration 4800: Loss = -12358.987811265617
Iteration 4900: Loss = -12358.98606503626
Iteration 5000: Loss = -12358.984510252332
Iteration 5100: Loss = -12358.98320440355
Iteration 5200: Loss = -12358.98172789953
Iteration 5300: Loss = -12358.980507102446
Iteration 5400: Loss = -12358.9794433824
Iteration 5500: Loss = -12358.978617050649
Iteration 5600: Loss = -12358.977878281297
Iteration 5700: Loss = -12358.977164415572
Iteration 5800: Loss = -12358.976702181013
Iteration 5900: Loss = -12358.982790884422
1
Iteration 6000: Loss = -12358.975293612279
Iteration 6100: Loss = -12358.974753764694
Iteration 6200: Loss = -12358.974248899767
Iteration 6300: Loss = -12358.973748991215
Iteration 6400: Loss = -12358.97334414134
Iteration 6500: Loss = -12358.972782582132
Iteration 6600: Loss = -12358.980322718178
1
Iteration 6700: Loss = -12358.995922459273
2
Iteration 6800: Loss = -12358.97103444332
Iteration 6900: Loss = -12358.96979544649
Iteration 7000: Loss = -12359.00439884257
1
Iteration 7100: Loss = -12358.969239955075
Iteration 7200: Loss = -12358.979372232676
1
Iteration 7300: Loss = -12358.968834821359
Iteration 7400: Loss = -12358.968671037212
Iteration 7500: Loss = -12358.968573370343
Iteration 7600: Loss = -12358.968254363655
Iteration 7700: Loss = -12359.001494563672
1
Iteration 7800: Loss = -12358.967943173906
Iteration 7900: Loss = -12358.967781511174
Iteration 8000: Loss = -12358.967855614883
Iteration 8100: Loss = -12358.967619474295
Iteration 8200: Loss = -12358.967403539544
Iteration 8300: Loss = -12358.96737803235
Iteration 8400: Loss = -12358.969864667022
1
Iteration 8500: Loss = -12358.967069600068
Iteration 8600: Loss = -12358.968348570268
1
Iteration 8700: Loss = -12358.96691028257
Iteration 8800: Loss = -12358.966814764564
Iteration 8900: Loss = -12358.98327520725
1
Iteration 9000: Loss = -12358.967474879684
2
Iteration 9100: Loss = -12358.966566026917
Iteration 9200: Loss = -12358.978295015815
1
Iteration 9300: Loss = -12358.966474098956
Iteration 9400: Loss = -12358.966375163463
Iteration 9500: Loss = -12358.96657592506
1
Iteration 9600: Loss = -12358.973821023505
2
Iteration 9700: Loss = -12358.972167858125
3
Iteration 9800: Loss = -12358.96619143899
Iteration 9900: Loss = -12358.968548632542
1
Iteration 10000: Loss = -12358.966152720735
Iteration 10100: Loss = -12359.076202403396
1
Iteration 10200: Loss = -12358.966014269257
Iteration 10300: Loss = -12358.96943288054
1
Iteration 10400: Loss = -12358.965887075425
Iteration 10500: Loss = -12359.018742118702
1
Iteration 10600: Loss = -12358.965843870385
Iteration 10700: Loss = -12359.096201869623
1
Iteration 10800: Loss = -12358.96616220304
2
Iteration 10900: Loss = -12358.966226235776
3
Iteration 11000: Loss = -12358.977080191573
4
Iteration 11100: Loss = -12358.974295061713
5
Iteration 11200: Loss = -12358.96609665037
6
Iteration 11300: Loss = -12358.97341061694
7
Iteration 11400: Loss = -12358.97476424405
8
Iteration 11500: Loss = -12358.971340772474
9
Iteration 11600: Loss = -12358.966321720687
10
Iteration 11700: Loss = -12358.965616608315
Iteration 11800: Loss = -12358.977331117836
1
Iteration 11900: Loss = -12358.971578912564
2
Iteration 12000: Loss = -12358.965972154934
3
Iteration 12100: Loss = -12358.967583832977
4
Iteration 12200: Loss = -12359.092349681294
5
Iteration 12300: Loss = -12358.965957049653
6
Iteration 12400: Loss = -12358.967462686238
7
Iteration 12500: Loss = -12358.986707056569
8
Iteration 12600: Loss = -12358.967607199065
9
Iteration 12700: Loss = -12358.968607017072
10
Iteration 12800: Loss = -12359.043544292255
11
Iteration 12900: Loss = -12358.965953768333
12
Iteration 13000: Loss = -12358.965993634763
13
Iteration 13100: Loss = -12358.980183664387
14
Iteration 13200: Loss = -12358.967219126096
15
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[9.5613e-01, 4.3869e-02],
        [9.9999e-01, 5.3987e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4201, 0.5799], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2045, 0.0991],
         [0.6863, 0.3169]],

        [[0.6666, 0.0955],
         [0.6423, 0.6497]],

        [[0.7212, 0.2261],
         [0.5197, 0.5530]],

        [[0.6571, 0.2062],
         [0.6412, 0.5980]],

        [[0.6927, 0.2690],
         [0.7208, 0.5289]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.05474626699938599
Average Adjusted Rand Index: 0.19912050216985638
11884.006001793823
[0.05474626699938599, 0.05474626699938599] [0.19912050216985638, 0.19912050216985638] [12358.965845835319, 12358.967219126096]
-------------------------------------
This iteration is 68
True Objective function: Loss = -11844.607502722329
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22521.955976242443
Iteration 100: Loss = -12375.865725211172
Iteration 200: Loss = -12375.420910622417
Iteration 300: Loss = -12375.321725903752
Iteration 400: Loss = -12375.241305401283
Iteration 500: Loss = -12375.128099397378
Iteration 600: Loss = -12374.96423793875
Iteration 700: Loss = -12374.88420971147
Iteration 800: Loss = -12374.695364828167
Iteration 900: Loss = -12374.508630927203
Iteration 1000: Loss = -12374.358471802083
Iteration 1100: Loss = -12374.211718404482
Iteration 1200: Loss = -12374.06460955623
Iteration 1300: Loss = -12373.930512385492
Iteration 1400: Loss = -12373.820346159966
Iteration 1500: Loss = -12373.724599656089
Iteration 1600: Loss = -12373.646274168927
Iteration 1700: Loss = -12373.586501294312
Iteration 1800: Loss = -12373.542323795624
Iteration 1900: Loss = -12373.50684634149
Iteration 2000: Loss = -12373.481785533022
Iteration 2100: Loss = -12373.462403116166
Iteration 2200: Loss = -12373.448218557758
Iteration 2300: Loss = -12373.43735461845
Iteration 2400: Loss = -12373.428305142073
Iteration 2500: Loss = -12373.419392560381
Iteration 2600: Loss = -12373.41191329352
Iteration 2700: Loss = -12373.406325499016
Iteration 2800: Loss = -12373.401769110023
Iteration 2900: Loss = -12373.397824464386
Iteration 3000: Loss = -12373.394490530318
Iteration 3100: Loss = -12373.391537790536
Iteration 3200: Loss = -12373.388728555234
Iteration 3300: Loss = -12373.385234180481
Iteration 3400: Loss = -12373.379398794586
Iteration 3500: Loss = -12373.376624874216
Iteration 3600: Loss = -12373.37474832247
Iteration 3700: Loss = -12373.373057969331
Iteration 3800: Loss = -12373.371429244104
Iteration 3900: Loss = -12373.369927276302
Iteration 4000: Loss = -12373.368320713736
Iteration 4100: Loss = -12373.36653466585
Iteration 4200: Loss = -12373.364760471208
Iteration 4300: Loss = -12373.363429699979
Iteration 4400: Loss = -12373.36252311839
Iteration 4500: Loss = -12373.361698446583
Iteration 4600: Loss = -12373.360935658202
Iteration 4700: Loss = -12373.360227322864
Iteration 4800: Loss = -12373.35955024004
Iteration 4900: Loss = -12373.358810270895
Iteration 5000: Loss = -12373.357997270701
Iteration 5100: Loss = -12373.356978800739
Iteration 5200: Loss = -12373.356147885705
Iteration 5300: Loss = -12373.355621851964
Iteration 5400: Loss = -12373.355231048237
Iteration 5500: Loss = -12373.354835730412
Iteration 5600: Loss = -12373.354500393638
Iteration 5700: Loss = -12373.354168269856
Iteration 5800: Loss = -12373.353786138665
Iteration 5900: Loss = -12373.353357125954
Iteration 6000: Loss = -12373.3529128579
Iteration 6100: Loss = -12373.352430012297
Iteration 6200: Loss = -12373.351972743472
Iteration 6300: Loss = -12373.35163872606
Iteration 6400: Loss = -12373.35123773197
Iteration 6500: Loss = -12373.350436904519
Iteration 6600: Loss = -12373.348348586676
Iteration 6700: Loss = -12373.34763828393
Iteration 6800: Loss = -12373.347379581302
Iteration 6900: Loss = -12373.34717526515
Iteration 7000: Loss = -12373.346958815118
Iteration 7100: Loss = -12373.346327554897
Iteration 7200: Loss = -12373.346264700827
Iteration 7300: Loss = -12373.345860325837
Iteration 7400: Loss = -12373.34575151644
Iteration 7500: Loss = -12373.346064858357
1
Iteration 7600: Loss = -12373.345260139786
Iteration 7700: Loss = -12373.345634129299
1
Iteration 7800: Loss = -12373.34485435944
Iteration 7900: Loss = -12373.34479808848
Iteration 8000: Loss = -12373.34614197147
1
Iteration 8100: Loss = -12373.344583383145
Iteration 8200: Loss = -12373.344502073256
Iteration 8300: Loss = -12373.345176443741
1
Iteration 8400: Loss = -12373.344325859585
Iteration 8500: Loss = -12373.343971321883
Iteration 8600: Loss = -12373.343781307407
Iteration 8700: Loss = -12373.343068828899
Iteration 8800: Loss = -12373.354972204064
1
Iteration 8900: Loss = -12373.341673265651
Iteration 9000: Loss = -12373.341310537406
Iteration 9100: Loss = -12373.341255985191
Iteration 9200: Loss = -12373.341529113963
1
Iteration 9300: Loss = -12373.341165016072
Iteration 9400: Loss = -12373.341039410485
Iteration 9500: Loss = -12373.923692644863
1
Iteration 9600: Loss = -12373.339944267329
Iteration 9700: Loss = -12373.339932334891
Iteration 9800: Loss = -12373.33984429151
Iteration 9900: Loss = -12373.339854876285
Iteration 10000: Loss = -12373.339764511138
Iteration 10100: Loss = -12373.339701963861
Iteration 10200: Loss = -12373.339579085472
Iteration 10300: Loss = -12373.339526346652
Iteration 10400: Loss = -12373.339487340038
Iteration 10500: Loss = -12373.339877681537
1
Iteration 10600: Loss = -12373.339459956946
Iteration 10700: Loss = -12373.33943823531
Iteration 10800: Loss = -12373.33982673395
1
Iteration 10900: Loss = -12373.339354755231
Iteration 11000: Loss = -12373.33902944232
Iteration 11100: Loss = -12373.338818206146
Iteration 11200: Loss = -12373.338360962383
Iteration 11300: Loss = -12373.337930801445
Iteration 11400: Loss = -12373.35425822092
1
Iteration 11500: Loss = -12373.337856531965
Iteration 11600: Loss = -12373.337757939777
Iteration 11700: Loss = -12373.383410627592
1
Iteration 11800: Loss = -12373.337671697585
Iteration 11900: Loss = -12373.337687461655
Iteration 12000: Loss = -12373.337686603743
Iteration 12100: Loss = -12373.337453359774
Iteration 12200: Loss = -12373.337415381407
Iteration 12300: Loss = -12373.342134510012
1
Iteration 12400: Loss = -12373.336390542265
Iteration 12500: Loss = -12373.336424221967
Iteration 12600: Loss = -12373.337796352615
1
Iteration 12700: Loss = -12373.336386446865
Iteration 12800: Loss = -12373.336504864388
1
Iteration 12900: Loss = -12373.336454096281
Iteration 13000: Loss = -12373.336809829005
1
Iteration 13100: Loss = -12373.341597418508
2
Iteration 13200: Loss = -12373.337166969954
3
Iteration 13300: Loss = -12373.3367618054
4
Iteration 13400: Loss = -12373.336416452068
Iteration 13500: Loss = -12373.336573241319
1
Iteration 13600: Loss = -12373.336432129523
Iteration 13700: Loss = -12373.337912908015
1
Iteration 13800: Loss = -12373.336450955647
Iteration 13900: Loss = -12373.336384643035
Iteration 14000: Loss = -12373.336438165003
Iteration 14100: Loss = -12373.33637951487
Iteration 14200: Loss = -12373.336397385687
Iteration 14300: Loss = -12373.336468886571
Iteration 14400: Loss = -12373.368026311287
1
Iteration 14500: Loss = -12373.337278314912
2
Iteration 14600: Loss = -12373.336818497695
3
Iteration 14700: Loss = -12373.336424709625
Iteration 14800: Loss = -12373.337914744194
1
Iteration 14900: Loss = -12373.336378255282
Iteration 15000: Loss = -12373.3363359711
Iteration 15100: Loss = -12373.336412737173
Iteration 15200: Loss = -12373.33632094331
Iteration 15300: Loss = -12373.336592364554
1
Iteration 15400: Loss = -12373.336042983676
Iteration 15500: Loss = -12373.335969995569
Iteration 15600: Loss = -12373.338907275958
1
Iteration 15700: Loss = -12373.336462259384
2
Iteration 15800: Loss = -12373.336101597448
3
Iteration 15900: Loss = -12373.336520199478
4
Iteration 16000: Loss = -12373.335520251865
Iteration 16100: Loss = -12373.336081620471
1
Iteration 16200: Loss = -12373.340410100056
2
Iteration 16300: Loss = -12373.335478621231
Iteration 16400: Loss = -12373.335482260405
Iteration 16500: Loss = -12373.360567628493
1
Iteration 16600: Loss = -12373.335110587454
Iteration 16700: Loss = -12373.337308537515
1
Iteration 16800: Loss = -12373.334374616652
Iteration 16900: Loss = -12373.341258383849
1
Iteration 17000: Loss = -12373.334343956605
Iteration 17100: Loss = -12373.348588183773
1
Iteration 17200: Loss = -12373.334392567142
Iteration 17300: Loss = -12373.343806641318
1
Iteration 17400: Loss = -12373.334343742528
Iteration 17500: Loss = -12373.338728527042
1
Iteration 17600: Loss = -12373.336544883177
2
Iteration 17700: Loss = -12373.345985490114
3
Iteration 17800: Loss = -12373.33465112677
4
Iteration 17900: Loss = -12373.335000035742
5
Iteration 18000: Loss = -12373.334473011802
6
Iteration 18100: Loss = -12373.334523416135
7
Iteration 18200: Loss = -12373.334719642004
8
Iteration 18300: Loss = -12373.334385869895
Iteration 18400: Loss = -12373.334448888421
Iteration 18500: Loss = -12373.334262022532
Iteration 18600: Loss = -12373.336404937218
1
Iteration 18700: Loss = -12373.335476389242
2
Iteration 18800: Loss = -12373.334441174127
3
Iteration 18900: Loss = -12373.334598818952
4
Iteration 19000: Loss = -12373.33422686363
Iteration 19100: Loss = -12373.334398654923
1
Iteration 19200: Loss = -12373.334162396106
Iteration 19300: Loss = -12373.334163616635
Iteration 19400: Loss = -12373.335399851198
1
Iteration 19500: Loss = -12373.336361472184
2
Iteration 19600: Loss = -12373.334178052339
Iteration 19700: Loss = -12373.334842542246
1
Iteration 19800: Loss = -12373.334143846598
Iteration 19900: Loss = -12373.375065555874
1
pi: tensor([[1.0000e+00, 8.4037e-07],
        [2.7000e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0299, 0.9701], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2391, 0.2870],
         [0.6541, 0.1976]],

        [[0.6219, 0.2603],
         [0.5610, 0.6166]],

        [[0.6626, 0.2082],
         [0.5245, 0.5337]],

        [[0.5622, 0.2051],
         [0.5907, 0.5125]],

        [[0.6036, 0.2019],
         [0.6885, 0.6188]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0012662455124815629
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: -0.0008938930955338496
Average Adjusted Rand Index: -0.001833563216132023
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21349.003434963135
Iteration 100: Loss = -12375.545343154945
Iteration 200: Loss = -12375.30342040329
Iteration 300: Loss = -12375.218314127387
Iteration 400: Loss = -12375.144716976423
Iteration 500: Loss = -12375.043787412429
Iteration 600: Loss = -12374.938363554136
Iteration 700: Loss = -12374.893088841269
Iteration 800: Loss = -12374.855782831775
Iteration 900: Loss = -12374.816253920448
Iteration 1000: Loss = -12374.77380605567
Iteration 1100: Loss = -12374.725220397853
Iteration 1200: Loss = -12374.659548971917
Iteration 1300: Loss = -12374.545957848384
Iteration 1400: Loss = -12374.310540963374
Iteration 1500: Loss = -12374.064696997477
Iteration 1600: Loss = -12373.945160826821
Iteration 1700: Loss = -12373.876095532254
Iteration 1800: Loss = -12373.827787828688
Iteration 1900: Loss = -12373.790960076645
Iteration 2000: Loss = -12373.760697457848
Iteration 2100: Loss = -12373.734414410466
Iteration 2200: Loss = -12373.711141715856
Iteration 2300: Loss = -12373.690398935149
Iteration 2400: Loss = -12373.672058073951
Iteration 2500: Loss = -12373.65618357154
Iteration 2600: Loss = -12373.642773127316
Iteration 2700: Loss = -12373.631896569856
Iteration 2800: Loss = -12373.62350525755
Iteration 2900: Loss = -12373.617001178996
Iteration 3000: Loss = -12373.611986822898
Iteration 3100: Loss = -12373.608061723628
Iteration 3200: Loss = -12373.604893467491
Iteration 3300: Loss = -12373.602224536438
Iteration 3400: Loss = -12373.634365305701
1
Iteration 3500: Loss = -12373.597871222924
Iteration 3600: Loss = -12373.595787461354
Iteration 3700: Loss = -12373.59404373552
Iteration 3800: Loss = -12373.591515475262
Iteration 3900: Loss = -12373.588713527106
Iteration 4000: Loss = -12373.585638983923
Iteration 4100: Loss = -12373.581350349483
Iteration 4200: Loss = -12373.57650413853
Iteration 4300: Loss = -12373.57007938951
Iteration 4400: Loss = -12373.564033311106
Iteration 4500: Loss = -12373.558299501488
Iteration 4600: Loss = -12373.554443393654
Iteration 4700: Loss = -12373.55259021208
Iteration 4800: Loss = -12373.55173425175
Iteration 4900: Loss = -12373.574012696667
1
Iteration 5000: Loss = -12373.551216575432
Iteration 5100: Loss = -12373.551010420015
Iteration 5200: Loss = -12373.55151545074
1
Iteration 5300: Loss = -12373.550735074788
Iteration 5400: Loss = -12373.550611762419
Iteration 5500: Loss = -12373.550451249555
Iteration 5600: Loss = -12373.550381725778
Iteration 5700: Loss = -12373.550421408516
Iteration 5800: Loss = -12373.550131099346
Iteration 5900: Loss = -12373.562970214545
1
Iteration 6000: Loss = -12373.549883489273
Iteration 6100: Loss = -12373.549825141088
Iteration 6200: Loss = -12373.54986753497
Iteration 6300: Loss = -12373.549925121935
Iteration 6400: Loss = -12373.5503274687
1
Iteration 6500: Loss = -12373.551229553728
2
Iteration 6600: Loss = -12373.549412380478
Iteration 6700: Loss = -12373.549356303876
Iteration 6800: Loss = -12373.549591951145
1
Iteration 6900: Loss = -12373.549403930525
Iteration 7000: Loss = -12373.549235639657
Iteration 7100: Loss = -12373.5491405329
Iteration 7200: Loss = -12373.549210793197
Iteration 7300: Loss = -12373.556810071654
1
Iteration 7400: Loss = -12373.55831805612
2
Iteration 7500: Loss = -12373.558858773542
3
Iteration 7600: Loss = -12373.549467663639
4
Iteration 7700: Loss = -12373.550356266787
5
Iteration 7800: Loss = -12373.553953315426
6
Iteration 7900: Loss = -12373.558757458166
7
Iteration 8000: Loss = -12373.55164099564
8
Iteration 8100: Loss = -12373.548654546386
Iteration 8200: Loss = -12373.548870501367
1
Iteration 8300: Loss = -12373.552435884007
2
Iteration 8400: Loss = -12373.548644635051
Iteration 8500: Loss = -12373.551294753908
1
Iteration 8600: Loss = -12373.549019246944
2
Iteration 8700: Loss = -12373.548452617208
Iteration 8800: Loss = -12373.549750162978
1
Iteration 8900: Loss = -12373.548452705232
Iteration 9000: Loss = -12373.553155994065
1
Iteration 9100: Loss = -12373.549253731648
2
Iteration 9200: Loss = -12373.56458026847
3
Iteration 9300: Loss = -12373.613141926682
4
Iteration 9400: Loss = -12373.562395562347
5
Iteration 9500: Loss = -12373.562507251132
6
Iteration 9600: Loss = -12373.5489883556
7
Iteration 9700: Loss = -12373.548455183494
Iteration 9800: Loss = -12373.588085704194
1
Iteration 9900: Loss = -12373.648197558256
2
Iteration 10000: Loss = -12373.5492481207
3
Iteration 10100: Loss = -12373.548330004038
Iteration 10200: Loss = -12373.548264698045
Iteration 10300: Loss = -12373.59651244657
1
Iteration 10400: Loss = -12373.548140066667
Iteration 10500: Loss = -12373.552507144674
1
Iteration 10600: Loss = -12373.561620347251
2
Iteration 10700: Loss = -12373.549597902644
3
Iteration 10800: Loss = -12373.549762507362
4
Iteration 10900: Loss = -12373.548455101834
5
Iteration 11000: Loss = -12373.55015985709
6
Iteration 11100: Loss = -12373.548237034707
Iteration 11200: Loss = -12373.565362425366
1
Iteration 11300: Loss = -12373.579119888633
2
Iteration 11400: Loss = -12373.740389126631
3
Iteration 11500: Loss = -12373.548912263814
4
Iteration 11600: Loss = -12373.548366449739
5
Iteration 11700: Loss = -12373.565287871397
6
Iteration 11800: Loss = -12373.598406706646
7
Iteration 11900: Loss = -12373.577245570228
8
Iteration 12000: Loss = -12373.548071333445
Iteration 12100: Loss = -12373.549091228888
1
Iteration 12200: Loss = -12373.568203292121
2
Iteration 12300: Loss = -12373.548049238978
Iteration 12400: Loss = -12373.757230228166
1
Iteration 12500: Loss = -12373.548024137352
Iteration 12600: Loss = -12373.548025957114
Iteration 12700: Loss = -12373.54820423722
1
Iteration 12800: Loss = -12373.554659513553
2
Iteration 12900: Loss = -12373.5929540862
3
Iteration 13000: Loss = -12373.548003691672
Iteration 13100: Loss = -12373.550752429237
1
Iteration 13200: Loss = -12373.616044547998
2
Iteration 13300: Loss = -12373.64360514542
3
Iteration 13400: Loss = -12373.749691415927
4
Iteration 13500: Loss = -12373.548964366119
5
Iteration 13600: Loss = -12373.548235437542
6
Iteration 13700: Loss = -12373.58364378255
7
Iteration 13800: Loss = -12373.547963916888
Iteration 13900: Loss = -12373.54809627789
1
Iteration 14000: Loss = -12373.54832098381
2
Iteration 14100: Loss = -12373.547985843314
Iteration 14200: Loss = -12373.551071554033
1
Iteration 14300: Loss = -12373.547976654701
Iteration 14400: Loss = -12373.548367205356
1
Iteration 14500: Loss = -12373.548911390544
2
Iteration 14600: Loss = -12373.548030044502
Iteration 14700: Loss = -12373.548380444856
1
Iteration 14800: Loss = -12373.548025224412
Iteration 14900: Loss = -12373.556666462438
1
Iteration 15000: Loss = -12373.54803869849
Iteration 15100: Loss = -12373.548100069003
Iteration 15200: Loss = -12373.599654567031
1
Iteration 15300: Loss = -12373.548012332476
Iteration 15400: Loss = -12373.548895070331
1
Iteration 15500: Loss = -12373.548150571163
2
Iteration 15600: Loss = -12373.55126977448
3
Iteration 15700: Loss = -12373.55286911481
4
Iteration 15800: Loss = -12373.569292450122
5
Iteration 15900: Loss = -12373.625356770826
6
Iteration 16000: Loss = -12373.54899450276
7
Iteration 16100: Loss = -12373.548920882224
8
Iteration 16200: Loss = -12373.651639959086
9
Iteration 16300: Loss = -12373.547981614625
Iteration 16400: Loss = -12373.55795124284
1
Iteration 16500: Loss = -12373.585198971115
2
Iteration 16600: Loss = -12373.62578773733
3
Iteration 16700: Loss = -12373.58006289831
4
Iteration 16800: Loss = -12373.728414752906
5
Iteration 16900: Loss = -12373.550649158979
6
Iteration 17000: Loss = -12373.7042812458
7
Iteration 17100: Loss = -12373.5479497857
Iteration 17200: Loss = -12373.556515883167
1
Iteration 17300: Loss = -12373.548006629728
Iteration 17400: Loss = -12373.626822768876
1
Iteration 17500: Loss = -12373.572143210651
2
Iteration 17600: Loss = -12373.56270238094
3
Iteration 17700: Loss = -12373.548617104603
4
Iteration 17800: Loss = -12373.701144060373
5
Iteration 17900: Loss = -12373.54876971163
6
Iteration 18000: Loss = -12373.553051505667
7
Iteration 18100: Loss = -12373.566158153424
8
Iteration 18200: Loss = -12373.54993363476
9
Iteration 18300: Loss = -12373.585824872625
10
Iteration 18400: Loss = -12373.547962911973
Iteration 18500: Loss = -12373.554518067898
1
Iteration 18600: Loss = -12373.584836076969
2
Iteration 18700: Loss = -12373.56608376198
3
Iteration 18800: Loss = -12373.549414171508
4
Iteration 18900: Loss = -12373.549558135965
5
Iteration 19000: Loss = -12373.54830926972
6
Iteration 19100: Loss = -12373.547992290445
Iteration 19200: Loss = -12373.549427798447
1
Iteration 19300: Loss = -12373.550217301434
2
Iteration 19400: Loss = -12373.579061125269
3
Iteration 19500: Loss = -12373.565929750503
4
Iteration 19600: Loss = -12373.559489605197
5
Iteration 19700: Loss = -12373.555342973188
6
Iteration 19800: Loss = -12373.5853097863
7
Iteration 19900: Loss = -12373.549803702497
8
pi: tensor([[4.4389e-06, 1.0000e+00],
        [4.1081e-01, 5.8919e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9360, 0.0640], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2062, 0.2001],
         [0.5662, 0.1971]],

        [[0.5190, 0.1388],
         [0.6576, 0.5491]],

        [[0.5689, 0.2038],
         [0.5139, 0.6197]],

        [[0.5228, 0.2050],
         [0.7120, 0.6689]],

        [[0.6611, 0.1939],
         [0.6358, 0.6815]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0033335915111145534
Average Adjusted Rand Index: 0.0
11844.607502722329
[-0.0008938930955338496, -0.0033335915111145534] [-0.001833563216132023, 0.0] [12373.335892507876, 12373.548050197975]
-------------------------------------
This iteration is 69
True Objective function: Loss = -11919.522500737165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21780.370621448812
Iteration 100: Loss = -12420.086165484556
Iteration 200: Loss = -12419.249998139638
Iteration 300: Loss = -12418.109167723647
Iteration 400: Loss = -12417.696211701184
Iteration 500: Loss = -12417.574221514256
Iteration 600: Loss = -12417.505379136142
Iteration 700: Loss = -12417.452396975774
Iteration 800: Loss = -12417.412865700915
Iteration 900: Loss = -12417.386174639836
Iteration 1000: Loss = -12417.36680744527
Iteration 1100: Loss = -12417.34985769948
Iteration 1200: Loss = -12417.3325442947
Iteration 1300: Loss = -12417.31336795172
Iteration 1400: Loss = -12417.29255852939
Iteration 1500: Loss = -12417.271691605745
Iteration 1600: Loss = -12417.252515342288
Iteration 1700: Loss = -12417.234628048856
Iteration 1800: Loss = -12417.217191478487
Iteration 1900: Loss = -12417.199684848034
Iteration 2000: Loss = -12417.181687842402
Iteration 2100: Loss = -12417.162138018333
Iteration 2200: Loss = -12417.139587112146
Iteration 2300: Loss = -12417.112822358551
Iteration 2400: Loss = -12417.08187188646
Iteration 2500: Loss = -12417.04904588856
Iteration 2600: Loss = -12417.018928949885
Iteration 2700: Loss = -12416.99557303985
Iteration 2800: Loss = -12416.980152633318
Iteration 2900: Loss = -12416.97121154992
Iteration 3000: Loss = -12416.966460320627
Iteration 3100: Loss = -12416.968650944951
1
Iteration 3200: Loss = -12416.963076144222
Iteration 3300: Loss = -12416.962557169767
Iteration 3400: Loss = -12416.96955422785
1
Iteration 3500: Loss = -12416.962195199352
Iteration 3600: Loss = -12416.962166619734
Iteration 3700: Loss = -12416.962305837358
1
Iteration 3800: Loss = -12416.962158977192
Iteration 3900: Loss = -12416.962140895248
Iteration 4000: Loss = -12416.962117032315
Iteration 4100: Loss = -12416.962111483726
Iteration 4200: Loss = -12416.962137540528
Iteration 4300: Loss = -12416.962101868474
Iteration 4400: Loss = -12416.962128858015
Iteration 4500: Loss = -12416.962203324483
Iteration 4600: Loss = -12416.962100722883
Iteration 4700: Loss = -12416.962207774399
1
Iteration 4800: Loss = -12416.962341456547
2
Iteration 4900: Loss = -12416.967163415111
3
Iteration 5000: Loss = -12416.96325313872
4
Iteration 5100: Loss = -12416.96294539079
5
Iteration 5200: Loss = -12416.962172488062
Iteration 5300: Loss = -12416.962406455059
1
Iteration 5400: Loss = -12416.962098763073
Iteration 5500: Loss = -12416.962248775497
1
Iteration 5600: Loss = -12416.962489121712
2
Iteration 5700: Loss = -12416.962145859783
Iteration 5800: Loss = -12416.963718556433
1
Iteration 5900: Loss = -12416.96212568746
Iteration 6000: Loss = -12416.981107446354
1
Iteration 6100: Loss = -12416.962082699969
Iteration 6200: Loss = -12416.96540800607
1
Iteration 6300: Loss = -12416.96361481967
2
Iteration 6400: Loss = -12416.962423175324
3
Iteration 6500: Loss = -12416.962183761172
4
Iteration 6600: Loss = -12416.962114263357
Iteration 6700: Loss = -12416.979412491808
1
Iteration 6800: Loss = -12416.96217915442
Iteration 6900: Loss = -12416.962220875375
Iteration 7000: Loss = -12416.962375929055
1
Iteration 7100: Loss = -12416.96264954825
2
Iteration 7200: Loss = -12416.962340018817
3
Iteration 7300: Loss = -12416.962478145677
4
Iteration 7400: Loss = -12416.963321306433
5
Iteration 7500: Loss = -12416.962298525132
Iteration 7600: Loss = -12417.024536689583
1
Iteration 7700: Loss = -12416.962122734878
Iteration 7800: Loss = -12416.967227464496
1
Iteration 7900: Loss = -12416.962103852482
Iteration 8000: Loss = -12416.962095640425
Iteration 8100: Loss = -12416.963391568088
1
Iteration 8200: Loss = -12417.01482364792
2
Iteration 8300: Loss = -12416.962114094073
Iteration 8400: Loss = -12416.962198615702
Iteration 8500: Loss = -12416.96468643973
1
Iteration 8600: Loss = -12416.962463101485
2
Iteration 8700: Loss = -12416.962284627525
Iteration 8800: Loss = -12416.96370134466
1
Iteration 8900: Loss = -12416.99782539974
2
Iteration 9000: Loss = -12416.962100731922
Iteration 9100: Loss = -12416.962357107122
1
Iteration 9200: Loss = -12416.962969683978
2
Iteration 9300: Loss = -12416.973300894519
3
Iteration 9400: Loss = -12417.086131596508
4
Iteration 9500: Loss = -12416.964301225495
5
Iteration 9600: Loss = -12416.965349051854
6
Iteration 9700: Loss = -12416.96333463868
7
Iteration 9800: Loss = -12416.966294694392
8
Iteration 9900: Loss = -12416.968958677842
9
Iteration 10000: Loss = -12416.962276480608
10
Iteration 10100: Loss = -12416.963581476479
11
Iteration 10200: Loss = -12417.002598402172
12
Iteration 10300: Loss = -12416.984939057094
13
Iteration 10400: Loss = -12416.970414689398
14
Iteration 10500: Loss = -12417.025205855203
15
Stopping early at iteration 10500 due to no improvement.
pi: tensor([[0.8770, 0.1230],
        [0.8800, 0.1200]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6145, 0.3855], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1944, 0.2126],
         [0.5831, 0.2340]],

        [[0.6799, 0.2074],
         [0.5852, 0.5452]],

        [[0.6688, 0.2049],
         [0.6134, 0.6681]],

        [[0.7246, 0.2001],
         [0.6212, 0.5402]],

        [[0.5226, 0.2311],
         [0.6465, 0.5717]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0017656651454180695
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002130692687060691
Average Adjusted Rand Index: -0.00035313302908361393
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20428.747843394023
Iteration 100: Loss = -12420.042115843571
Iteration 200: Loss = -12419.491224128966
Iteration 300: Loss = -12418.973041610025
Iteration 400: Loss = -12417.836655045083
Iteration 500: Loss = -12417.72984860123
Iteration 600: Loss = -12417.675989220488
Iteration 700: Loss = -12417.635262717324
Iteration 800: Loss = -12417.598709969616
Iteration 900: Loss = -12417.562473102633
Iteration 1000: Loss = -12417.524781268126
Iteration 1100: Loss = -12417.485329059802
Iteration 1200: Loss = -12417.445076708653
Iteration 1300: Loss = -12417.405727896212
Iteration 1400: Loss = -12417.369762023109
Iteration 1500: Loss = -12417.339250994399
Iteration 1600: Loss = -12417.313763721926
Iteration 1700: Loss = -12417.291344037647
Iteration 1800: Loss = -12417.270173404098
Iteration 1900: Loss = -12417.248910594131
Iteration 2000: Loss = -12417.226624421783
Iteration 2100: Loss = -12417.202586743788
Iteration 2200: Loss = -12417.176318030355
Iteration 2300: Loss = -12417.147587371934
Iteration 2400: Loss = -12417.116713734484
Iteration 2500: Loss = -12417.084844745572
Iteration 2600: Loss = -12417.054020882828
Iteration 2700: Loss = -12417.026608291075
Iteration 2800: Loss = -12417.004621901813
Iteration 2900: Loss = -12416.988632387378
Iteration 3000: Loss = -12416.97794603396
Iteration 3100: Loss = -12416.971260064563
Iteration 3200: Loss = -12416.967251141954
Iteration 3300: Loss = -12416.964937792203
Iteration 3400: Loss = -12416.963662769109
Iteration 3500: Loss = -12416.963011555375
Iteration 3600: Loss = -12416.962626568284
Iteration 3700: Loss = -12416.962376748781
Iteration 3800: Loss = -12416.967627476683
1
Iteration 3900: Loss = -12416.96221794484
Iteration 4000: Loss = -12416.962189215603
Iteration 4100: Loss = -12416.96217802172
Iteration 4200: Loss = -12416.962143027777
Iteration 4300: Loss = -12416.96220138531
Iteration 4400: Loss = -12416.962195373886
Iteration 4500: Loss = -12416.962136771086
Iteration 4600: Loss = -12416.963927918348
1
Iteration 4700: Loss = -12416.962212096098
Iteration 4800: Loss = -12416.962138205316
Iteration 4900: Loss = -12416.962815269357
1
Iteration 5000: Loss = -12416.962133834799
Iteration 5100: Loss = -12416.96216188299
Iteration 5200: Loss = -12416.962222038801
Iteration 5300: Loss = -12416.962116616129
Iteration 5400: Loss = -12416.962429377487
1
Iteration 5500: Loss = -12416.962102650976
Iteration 5600: Loss = -12416.962397241912
1
Iteration 5700: Loss = -12416.962123730404
Iteration 5800: Loss = -12416.963264953096
1
Iteration 5900: Loss = -12416.962126818697
Iteration 6000: Loss = -12416.962123241003
Iteration 6100: Loss = -12416.962221714199
Iteration 6200: Loss = -12416.962176216963
Iteration 6300: Loss = -12416.968396397537
1
Iteration 6400: Loss = -12416.96215606861
Iteration 6500: Loss = -12416.962521045412
1
Iteration 6600: Loss = -12416.9621247959
Iteration 6700: Loss = -12416.962701282091
1
Iteration 6800: Loss = -12416.96224533281
2
Iteration 6900: Loss = -12416.962906494542
3
Iteration 7000: Loss = -12416.972736003117
4
Iteration 7100: Loss = -12416.96318125693
5
Iteration 7200: Loss = -12416.963141119664
6
Iteration 7300: Loss = -12416.96268803012
7
Iteration 7400: Loss = -12416.963241660014
8
Iteration 7500: Loss = -12416.995091442861
9
Iteration 7600: Loss = -12416.965565281389
10
Iteration 7700: Loss = -12416.962244652668
11
Iteration 7800: Loss = -12416.980345454811
12
Iteration 7900: Loss = -12416.96264465217
13
Iteration 8000: Loss = -12416.963153750732
14
Iteration 8100: Loss = -12416.964747355063
15
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[0.8753, 0.1247],
        [0.8792, 0.1208]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6120, 0.3880], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1946, 0.2132],
         [0.6798, 0.2337]],

        [[0.5392, 0.2075],
         [0.7304, 0.5162]],

        [[0.6096, 0.2050],
         [0.6981, 0.5704]],

        [[0.6180, 0.2002],
         [0.5125, 0.5097]],

        [[0.6758, 0.2313],
         [0.6519, 0.6047]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.004693286400469328
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0017183948066689541
Average Adjusted Rand Index: -0.0009386572800938657
11919.522500737165
[0.002130692687060691, 0.0017183948066689541] [-0.00035313302908361393, -0.0009386572800938657] [12417.025205855203, 12416.964747355063]
-------------------------------------
This iteration is 70
True Objective function: Loss = -11944.689308210178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21330.02772518277
Iteration 100: Loss = -12449.514385289807
Iteration 200: Loss = -12448.812995662462
Iteration 300: Loss = -12448.662756615151
Iteration 400: Loss = -12448.590141819672
Iteration 500: Loss = -12448.547105233829
Iteration 600: Loss = -12448.518100730613
Iteration 700: Loss = -12448.496696612894
Iteration 800: Loss = -12448.479560477585
Iteration 900: Loss = -12448.464587627846
Iteration 1000: Loss = -12448.450151641367
Iteration 1100: Loss = -12448.43405497792
Iteration 1200: Loss = -12448.411303887546
Iteration 1300: Loss = -12448.360763874292
Iteration 1400: Loss = -12448.057252517305
Iteration 1500: Loss = -12447.381967911282
Iteration 1600: Loss = -12447.140721447033
Iteration 1700: Loss = -12447.023668752621
Iteration 1800: Loss = -12446.950495609279
Iteration 1900: Loss = -12446.895770382851
Iteration 2000: Loss = -12446.8500687815
Iteration 2100: Loss = -12446.809743248894
Iteration 2200: Loss = -12446.77345960392
Iteration 2300: Loss = -12446.741238644674
Iteration 2400: Loss = -12446.71357861496
Iteration 2500: Loss = -12446.691202328693
Iteration 2600: Loss = -12446.674362569835
Iteration 2700: Loss = -12446.662549716506
Iteration 2800: Loss = -12446.654800374634
Iteration 2900: Loss = -12446.649725776053
Iteration 3000: Loss = -12446.646357411606
Iteration 3100: Loss = -12446.643924672973
Iteration 3200: Loss = -12446.642048414615
Iteration 3300: Loss = -12446.640408904856
Iteration 3400: Loss = -12446.638761957702
Iteration 3500: Loss = -12446.636947714815
Iteration 3600: Loss = -12446.634827355976
Iteration 3700: Loss = -12446.632065030399
Iteration 3800: Loss = -12446.628349260085
Iteration 3900: Loss = -12446.623102653042
Iteration 4000: Loss = -12446.615473391492
Iteration 4100: Loss = -12446.604353072871
Iteration 4200: Loss = -12446.588710015503
Iteration 4300: Loss = -12446.569783471761
Iteration 4400: Loss = -12446.55172237696
Iteration 4500: Loss = -12446.542336884637
Iteration 4600: Loss = -12446.530596171351
Iteration 4700: Loss = -12446.525506869564
Iteration 4800: Loss = -12446.52263932305
Iteration 4900: Loss = -12446.520555035395
Iteration 5000: Loss = -12446.519245141952
Iteration 5100: Loss = -12446.518378189156
Iteration 5200: Loss = -12446.517671658858
Iteration 5300: Loss = -12446.518310182755
1
Iteration 5400: Loss = -12446.516784568206
Iteration 5500: Loss = -12446.516481636678
Iteration 5600: Loss = -12446.516301311362
Iteration 5700: Loss = -12446.516083289993
Iteration 5800: Loss = -12446.520536154769
1
Iteration 5900: Loss = -12446.515851485256
Iteration 6000: Loss = -12446.515731025036
Iteration 6100: Loss = -12446.519032331094
1
Iteration 6200: Loss = -12446.515591257474
Iteration 6300: Loss = -12446.515524535753
Iteration 6400: Loss = -12446.51548318792
Iteration 6500: Loss = -12446.515417164122
Iteration 6600: Loss = -12446.515345391317
Iteration 6700: Loss = -12446.515369262706
Iteration 6800: Loss = -12446.515221070385
Iteration 6900: Loss = -12446.515190005399
Iteration 7000: Loss = -12446.5151919553
Iteration 7100: Loss = -12446.515105972316
Iteration 7200: Loss = -12446.519380101246
1
Iteration 7300: Loss = -12446.515026037756
Iteration 7400: Loss = -12446.514905053593
Iteration 7500: Loss = -12446.515194484422
1
Iteration 7600: Loss = -12446.545087803855
2
Iteration 7700: Loss = -12446.514991790315
Iteration 7800: Loss = -12446.514479049918
Iteration 7900: Loss = -12446.516129223633
1
Iteration 8000: Loss = -12446.513909501842
Iteration 8100: Loss = -12446.525801472486
1
Iteration 8200: Loss = -12446.565673907187
2
Iteration 8300: Loss = -12446.516536970052
3
Iteration 8400: Loss = -12446.516522107095
4
Iteration 8500: Loss = -12446.504202975182
Iteration 8600: Loss = -12446.475461525959
Iteration 8700: Loss = -12446.304093921231
Iteration 8800: Loss = -12203.69909145907
Iteration 8900: Loss = -12067.723666260434
Iteration 9000: Loss = -12040.838534130276
Iteration 9100: Loss = -12003.474587080873
Iteration 9200: Loss = -11961.484787537063
Iteration 9300: Loss = -11961.331525969592
Iteration 9400: Loss = -11961.326641883255
Iteration 9500: Loss = -11961.321761167937
Iteration 9600: Loss = -11952.128273059656
Iteration 9700: Loss = -11952.099810056381
Iteration 9800: Loss = -11951.836405836226
Iteration 9900: Loss = -11951.840544597853
1
Iteration 10000: Loss = -11951.835406437454
Iteration 10100: Loss = -11951.851088983101
1
Iteration 10200: Loss = -11951.83729740431
2
Iteration 10300: Loss = -11951.835998285447
3
Iteration 10400: Loss = -11951.839533132706
4
Iteration 10500: Loss = -11951.840420429056
5
Iteration 10600: Loss = -11951.874360203357
6
Iteration 10700: Loss = -11951.849211305942
7
Iteration 10800: Loss = -11951.840423186908
8
Iteration 10900: Loss = -11951.83584722512
9
Iteration 11000: Loss = -11951.85603090526
10
Iteration 11100: Loss = -11951.837370213812
11
Iteration 11200: Loss = -11951.846324254364
12
Iteration 11300: Loss = -11951.833879347763
Iteration 11400: Loss = -11951.825997021664
Iteration 11500: Loss = -11951.823015309315
Iteration 11600: Loss = -11951.82753208056
1
Iteration 11700: Loss = -11951.818435853515
Iteration 11800: Loss = -11951.81550432666
Iteration 11900: Loss = -11951.802837395908
Iteration 12000: Loss = -11951.786955259393
Iteration 12100: Loss = -11951.786758821521
Iteration 12200: Loss = -11951.793657593224
1
Iteration 12300: Loss = -11951.784816396477
Iteration 12400: Loss = -11951.821376300919
1
Iteration 12500: Loss = -11951.78457361188
Iteration 12600: Loss = -11951.781506124891
Iteration 12700: Loss = -11951.786884883286
1
Iteration 12800: Loss = -11951.894722575249
2
Iteration 12900: Loss = -11951.788375371005
3
Iteration 13000: Loss = -11951.519719682417
Iteration 13100: Loss = -11951.518609220338
Iteration 13200: Loss = -11951.52359891218
1
Iteration 13300: Loss = -11951.589192561873
2
Iteration 13400: Loss = -11951.518448832327
Iteration 13500: Loss = -11951.519462223601
1
Iteration 13600: Loss = -11951.518448805817
Iteration 13700: Loss = -11946.52156557596
Iteration 13800: Loss = -11946.519627832075
Iteration 13900: Loss = -11946.502653282352
Iteration 14000: Loss = -11946.504789674798
1
Iteration 14100: Loss = -11946.515679458078
2
Iteration 14200: Loss = -11946.501941220607
Iteration 14300: Loss = -11946.505504862329
1
Iteration 14400: Loss = -11946.513297651813
2
Iteration 14500: Loss = -11946.500359560148
Iteration 14600: Loss = -11946.523359911465
1
Iteration 14700: Loss = -11946.50723289398
2
Iteration 14800: Loss = -11946.499947763316
Iteration 14900: Loss = -11946.499686654079
Iteration 15000: Loss = -11946.501212508243
1
Iteration 15100: Loss = -11946.50919895226
2
Iteration 15200: Loss = -11946.499478329777
Iteration 15300: Loss = -11946.503465588125
1
Iteration 15400: Loss = -11946.500123019463
2
Iteration 15500: Loss = -11946.536052403722
3
Iteration 15600: Loss = -11946.499278740603
Iteration 15700: Loss = -11946.502316748785
1
Iteration 15800: Loss = -11946.557593055591
2
Iteration 15900: Loss = -11946.506777893514
3
Iteration 16000: Loss = -11946.499411474737
4
Iteration 16100: Loss = -11946.49930441469
Iteration 16200: Loss = -11946.523038426358
1
Iteration 16300: Loss = -11946.506177638636
2
Iteration 16400: Loss = -11946.499422255505
3
Iteration 16500: Loss = -11946.504256364664
4
Iteration 16600: Loss = -11946.701829619753
5
Iteration 16700: Loss = -11946.502223913516
6
Iteration 16800: Loss = -11946.509564789436
7
Iteration 16900: Loss = -11946.503516213937
8
Iteration 17000: Loss = -11946.510326963686
9
Iteration 17100: Loss = -11946.498980454862
Iteration 17200: Loss = -11946.499336539311
1
Iteration 17300: Loss = -11946.513641033775
2
Iteration 17400: Loss = -11946.566032643526
3
Iteration 17500: Loss = -11946.500975186902
4
Iteration 17600: Loss = -11946.499301612017
5
Iteration 17700: Loss = -11946.502975325082
6
Iteration 17800: Loss = -11946.542326392104
7
Iteration 17900: Loss = -11946.4983682905
Iteration 18000: Loss = -11946.49904849949
1
Iteration 18100: Loss = -11946.519774255861
2
Iteration 18200: Loss = -11946.49898233923
3
Iteration 18300: Loss = -11946.498477972189
4
Iteration 18400: Loss = -11946.516602180987
5
Iteration 18500: Loss = -11946.5302402857
6
Iteration 18600: Loss = -11946.49816102382
Iteration 18700: Loss = -11946.506766822862
1
Iteration 18800: Loss = -11946.52078613421
2
Iteration 18900: Loss = -11946.507919981206
3
Iteration 19000: Loss = -11946.502937955014
4
Iteration 19100: Loss = -11946.49802689559
Iteration 19200: Loss = -11946.504720840974
1
Iteration 19300: Loss = -11940.072325735991
Iteration 19400: Loss = -11940.055564605924
Iteration 19500: Loss = -11940.055158382425
Iteration 19600: Loss = -11940.128145426235
1
Iteration 19700: Loss = -11940.05341761493
Iteration 19800: Loss = -11940.051065719797
Iteration 19900: Loss = -11940.051167887494
1
pi: tensor([[0.7413, 0.2587],
        [0.2080, 0.7920]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4687, 0.5313], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2986, 0.1020],
         [0.7250, 0.3035]],

        [[0.5827, 0.1066],
         [0.6282, 0.6901]],

        [[0.5117, 0.1036],
         [0.6203, 0.5168]],

        [[0.6560, 0.0990],
         [0.5489, 0.7032]],

        [[0.6403, 0.1018],
         [0.5576, 0.5031]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760953012611165
Average Adjusted Rand Index: 0.9761612186588084
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22103.835201811482
Iteration 100: Loss = -12449.123789552907
Iteration 200: Loss = -12448.632618179741
Iteration 300: Loss = -12448.507105924045
Iteration 400: Loss = -12448.415327305032
Iteration 500: Loss = -12448.175390572262
Iteration 600: Loss = -12447.591821637618
Iteration 700: Loss = -12447.302648597733
Iteration 800: Loss = -12447.178468210686
Iteration 900: Loss = -12447.110747201541
Iteration 1000: Loss = -12447.064418363363
Iteration 1100: Loss = -12447.026906375708
Iteration 1200: Loss = -12446.993309523858
Iteration 1300: Loss = -12446.96125968429
Iteration 1400: Loss = -12446.929473920596
Iteration 1500: Loss = -12446.897290108598
Iteration 1600: Loss = -12446.864356745704
Iteration 1700: Loss = -12446.830504501428
Iteration 1800: Loss = -12446.795672551827
Iteration 1900: Loss = -12446.760625360637
Iteration 2000: Loss = -12446.72651924667
Iteration 2100: Loss = -12446.695298185736
Iteration 2200: Loss = -12446.668955860969
Iteration 2300: Loss = -12446.648297399373
Iteration 2400: Loss = -12446.632700720505
Iteration 2500: Loss = -12446.620482706083
Iteration 2600: Loss = -12446.609815541422
Iteration 2700: Loss = -12446.598905053323
Iteration 2800: Loss = -12446.586550175538
Iteration 2900: Loss = -12446.5723165369
Iteration 3000: Loss = -12446.55740472628
Iteration 3100: Loss = -12446.54431865314
Iteration 3200: Loss = -12446.534683921796
Iteration 3300: Loss = -12446.52842016391
Iteration 3400: Loss = -12446.5244843921
Iteration 3500: Loss = -12446.527288786125
1
Iteration 3600: Loss = -12446.520353655205
Iteration 3700: Loss = -12446.51920020451
Iteration 3800: Loss = -12446.51842666781
Iteration 3900: Loss = -12446.517824081855
Iteration 4000: Loss = -12446.517669131254
Iteration 4100: Loss = -12446.516962385776
Iteration 4200: Loss = -12446.516679318256
Iteration 4300: Loss = -12446.516548511787
Iteration 4400: Loss = -12446.5163345607
Iteration 4500: Loss = -12446.520115459427
1
Iteration 4600: Loss = -12446.516103095533
Iteration 4700: Loss = -12446.515954838058
Iteration 4800: Loss = -12446.51636749982
1
Iteration 4900: Loss = -12446.515838389312
Iteration 5000: Loss = -12446.515758794772
Iteration 5100: Loss = -12446.51576732682
Iteration 5200: Loss = -12446.51567157808
Iteration 5300: Loss = -12446.515614542273
Iteration 5400: Loss = -12446.515588495417
Iteration 5500: Loss = -12446.51637751234
1
Iteration 5600: Loss = -12446.515520364948
Iteration 5700: Loss = -12446.515536710609
Iteration 5800: Loss = -12446.515434702524
Iteration 5900: Loss = -12446.515872909496
1
Iteration 6000: Loss = -12446.515359367551
Iteration 6100: Loss = -12446.515398141895
Iteration 6200: Loss = -12446.515271025448
Iteration 6300: Loss = -12446.51537411506
1
Iteration 6400: Loss = -12446.515163230906
Iteration 6500: Loss = -12446.515127957835
Iteration 6600: Loss = -12446.529258983028
1
Iteration 6700: Loss = -12446.514966679748
Iteration 6800: Loss = -12446.524938920016
1
Iteration 6900: Loss = -12446.514718301485
Iteration 7000: Loss = -12446.55975977557
1
Iteration 7100: Loss = -12446.514333273693
Iteration 7200: Loss = -12446.5140154498
Iteration 7300: Loss = -12446.51374419097
Iteration 7400: Loss = -12446.516671352365
1
Iteration 7500: Loss = -12446.512324272762
Iteration 7600: Loss = -12446.510525200907
Iteration 7700: Loss = -12446.50674595429
Iteration 7800: Loss = -12446.496257435978
Iteration 7900: Loss = -12446.335054388068
Iteration 8000: Loss = -12178.533553162588
Iteration 8100: Loss = -12063.432947444624
Iteration 8200: Loss = -12041.385356679162
Iteration 8300: Loss = -12002.619724909397
Iteration 8400: Loss = -11979.874772314315
Iteration 8500: Loss = -11979.843067204447
Iteration 8600: Loss = -11969.888120103755
Iteration 8700: Loss = -11957.525150007166
Iteration 8800: Loss = -11950.005613886748
Iteration 8900: Loss = -11949.99762222692
Iteration 9000: Loss = -11949.992284104417
Iteration 9100: Loss = -11950.02274405223
1
Iteration 9200: Loss = -11949.989896091653
Iteration 9300: Loss = -11949.976907385342
Iteration 9400: Loss = -11949.980570654825
1
Iteration 9500: Loss = -11949.987323579408
2
Iteration 9600: Loss = -11950.024910619086
3
Iteration 9700: Loss = -11949.979850973836
4
Iteration 9800: Loss = -11949.975847555193
Iteration 9900: Loss = -11950.006545652197
1
Iteration 10000: Loss = -11949.97624500352
2
Iteration 10100: Loss = -11949.97841084125
3
Iteration 10200: Loss = -11949.973653714485
Iteration 10300: Loss = -11949.970425078425
Iteration 10400: Loss = -11949.972203471023
1
Iteration 10500: Loss = -11949.982307729246
2
Iteration 10600: Loss = -11950.011268204278
3
Iteration 10700: Loss = -11949.971742239286
4
Iteration 10800: Loss = -11949.97234105304
5
Iteration 10900: Loss = -11949.980607940628
6
Iteration 11000: Loss = -11945.271729763219
Iteration 11100: Loss = -11945.27044132851
Iteration 11200: Loss = -11945.274258228776
1
Iteration 11300: Loss = -11945.289035000134
2
Iteration 11400: Loss = -11945.372564718917
3
Iteration 11500: Loss = -11945.285224865449
4
Iteration 11600: Loss = -11945.25229206212
Iteration 11700: Loss = -11945.25152009939
Iteration 11800: Loss = -11945.248452797206
Iteration 11900: Loss = -11945.248112591536
Iteration 12000: Loss = -11945.24844740684
1
Iteration 12100: Loss = -11945.336524358776
2
Iteration 12200: Loss = -11945.24745300931
Iteration 12300: Loss = -11945.246961105633
Iteration 12400: Loss = -11945.283625407845
1
Iteration 12500: Loss = -11945.25401016052
2
Iteration 12600: Loss = -11945.251386477377
3
Iteration 12700: Loss = -11945.247096568277
4
Iteration 12800: Loss = -11945.243883861373
Iteration 12900: Loss = -11945.40353260251
1
Iteration 13000: Loss = -11945.2497413864
2
Iteration 13100: Loss = -11945.244010326745
3
Iteration 13200: Loss = -11945.243917382957
Iteration 13300: Loss = -11945.243665269862
Iteration 13400: Loss = -11945.244355279929
1
Iteration 13500: Loss = -11945.244489867924
2
Iteration 13600: Loss = -11945.289057264594
3
Iteration 13700: Loss = -11945.244762655488
4
Iteration 13800: Loss = -11945.248227183
5
Iteration 13900: Loss = -11945.244245430036
6
Iteration 14000: Loss = -11945.244133160062
7
Iteration 14100: Loss = -11945.246604057107
8
Iteration 14200: Loss = -11945.409054910864
9
Iteration 14300: Loss = -11945.254894852966
10
Iteration 14400: Loss = -11945.242670860662
Iteration 14500: Loss = -11945.241951971111
Iteration 14600: Loss = -11945.24236370877
1
Iteration 14700: Loss = -11945.241742562208
Iteration 14800: Loss = -11945.2861907082
1
Iteration 14900: Loss = -11945.2451689107
2
Iteration 15000: Loss = -11945.24105190155
Iteration 15100: Loss = -11945.288817953917
1
Iteration 15200: Loss = -11944.97579970128
Iteration 15300: Loss = -11944.979648076973
1
Iteration 15400: Loss = -11944.982938292602
2
Iteration 15500: Loss = -11944.983823643195
3
Iteration 15600: Loss = -11944.973456224001
Iteration 15700: Loss = -11938.776470758063
Iteration 15800: Loss = -11938.777570545499
1
Iteration 15900: Loss = -11938.838943118672
2
Iteration 16000: Loss = -11938.777381448526
3
Iteration 16100: Loss = -11938.781608371532
4
Iteration 16200: Loss = -11938.875011460525
5
Iteration 16300: Loss = -11938.787390605887
6
Iteration 16400: Loss = -11938.778905351615
7
Iteration 16500: Loss = -11938.809571709353
8
Iteration 16600: Loss = -11938.770299188809
Iteration 16700: Loss = -11938.769808315907
Iteration 16800: Loss = -11938.811907954447
1
Iteration 16900: Loss = -11938.771448600308
2
Iteration 17000: Loss = -11938.774506240405
3
Iteration 17100: Loss = -11938.79637497998
4
Iteration 17200: Loss = -11938.784819844816
5
Iteration 17300: Loss = -11938.766416601495
Iteration 17400: Loss = -11938.767240819432
1
Iteration 17500: Loss = -11938.793377486814
2
Iteration 17600: Loss = -11938.766032867363
Iteration 17700: Loss = -11938.765844415893
Iteration 17800: Loss = -11938.80266672882
1
Iteration 17900: Loss = -11938.766194466365
2
Iteration 18000: Loss = -11938.766082017948
3
Iteration 18100: Loss = -11938.7674429695
4
Iteration 18200: Loss = -11938.773511832542
5
Iteration 18300: Loss = -11938.772940920084
6
Iteration 18400: Loss = -11938.771415350573
7
Iteration 18500: Loss = -11938.911236733305
8
Iteration 18600: Loss = -11938.76831524942
9
Iteration 18700: Loss = -11938.765867266935
Iteration 18800: Loss = -11938.766144738804
1
Iteration 18900: Loss = -11938.770705921186
2
Iteration 19000: Loss = -11938.76705104194
3
Iteration 19100: Loss = -11938.766042727913
4
Iteration 19200: Loss = -11938.780254371246
5
Iteration 19300: Loss = -11938.767861515244
6
Iteration 19400: Loss = -11938.775605025397
7
Iteration 19500: Loss = -11938.765429570742
Iteration 19600: Loss = -11938.76645023973
1
Iteration 19700: Loss = -11938.766584830411
2
Iteration 19800: Loss = -11938.765544897347
3
Iteration 19900: Loss = -11938.78329032282
4
pi: tensor([[0.7899, 0.2101],
        [0.2602, 0.7398]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5382, 0.4618], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3031, 0.1013],
         [0.6744, 0.2994]],

        [[0.5866, 0.1066],
         [0.5093, 0.7162]],

        [[0.6842, 0.1037],
         [0.6444, 0.6598]],

        [[0.5147, 0.0990],
         [0.5663, 0.6070]],

        [[0.7039, 0.1019],
         [0.6629, 0.5845]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840313942303629
Average Adjusted Rand Index: 0.9841614067256721
11944.689308210178
[0.9760953012611165, 0.9840313942303629] [0.9761612186588084, 0.9841614067256721] [11940.050653984446, 11938.765395269738]
-------------------------------------
This iteration is 71
True Objective function: Loss = -11857.410216573497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21759.66550721764
Iteration 100: Loss = -12488.691715730634
Iteration 200: Loss = -12488.406921731823
Iteration 300: Loss = -12488.345344397945
Iteration 400: Loss = -12488.315981049067
Iteration 500: Loss = -12488.2972717636
Iteration 600: Loss = -12488.28367382259
Iteration 700: Loss = -12488.272734781696
Iteration 800: Loss = -12488.263277692771
Iteration 900: Loss = -12488.254533906393
Iteration 1000: Loss = -12488.246006356938
Iteration 1100: Loss = -12488.237288674454
Iteration 1200: Loss = -12488.22817258825
Iteration 1300: Loss = -12488.218660637602
Iteration 1400: Loss = -12488.209189269579
Iteration 1500: Loss = -12488.20044561186
Iteration 1600: Loss = -12488.19286699087
Iteration 1700: Loss = -12488.186171347626
Iteration 1800: Loss = -12488.17976135897
Iteration 1900: Loss = -12488.172971393007
Iteration 2000: Loss = -12488.165195094902
Iteration 2100: Loss = -12488.15555744691
Iteration 2200: Loss = -12488.14310209363
Iteration 2300: Loss = -12488.12686102809
Iteration 2400: Loss = -12488.105894226655
Iteration 2500: Loss = -12488.077930730316
Iteration 2600: Loss = -12488.0241262883
Iteration 2700: Loss = -12487.416866969232
Iteration 2800: Loss = -12486.057320760909
Iteration 2900: Loss = -12485.78601284824
Iteration 3000: Loss = -12485.731212846471
Iteration 3100: Loss = -12485.707130439792
Iteration 3200: Loss = -12485.692808023558
Iteration 3300: Loss = -12485.683024919415
Iteration 3400: Loss = -12485.675753884458
Iteration 3500: Loss = -12485.669956060392
Iteration 3600: Loss = -12485.665044508842
Iteration 3700: Loss = -12485.660000936883
Iteration 3800: Loss = -12485.64777330777
Iteration 3900: Loss = -12484.007400295928
Iteration 4000: Loss = -12483.774726372676
Iteration 4100: Loss = -12483.751653381216
Iteration 4200: Loss = -12483.742175135973
Iteration 4300: Loss = -12483.736338053122
Iteration 4400: Loss = -12483.732298627066
Iteration 4500: Loss = -12483.729297040869
Iteration 4600: Loss = -12483.727114061114
Iteration 4700: Loss = -12483.725208000189
Iteration 4800: Loss = -12483.723820450366
Iteration 4900: Loss = -12483.722614770777
Iteration 5000: Loss = -12483.721630722846
Iteration 5100: Loss = -12483.72075726959
Iteration 5200: Loss = -12483.720012495185
Iteration 5300: Loss = -12483.719383098298
Iteration 5400: Loss = -12483.718868413152
Iteration 5500: Loss = -12483.718359733975
Iteration 5600: Loss = -12483.717915107769
Iteration 5700: Loss = -12483.717568375754
Iteration 5800: Loss = -12483.717191532445
Iteration 5900: Loss = -12483.716922703406
Iteration 6000: Loss = -12483.716620289906
Iteration 6100: Loss = -12483.716336687483
Iteration 6200: Loss = -12483.716151955095
Iteration 6300: Loss = -12483.715917400454
Iteration 6400: Loss = -12483.7157072215
Iteration 6500: Loss = -12483.715551578565
Iteration 6600: Loss = -12483.715389278086
Iteration 6700: Loss = -12483.715243580407
Iteration 6800: Loss = -12483.715089206144
Iteration 6900: Loss = -12483.714934482578
Iteration 7000: Loss = -12483.71483574727
Iteration 7100: Loss = -12483.714747897284
Iteration 7200: Loss = -12483.714616114012
Iteration 7300: Loss = -12483.714550473018
Iteration 7400: Loss = -12483.714769936261
1
Iteration 7500: Loss = -12483.714351912202
Iteration 7600: Loss = -12483.714597146376
1
Iteration 7700: Loss = -12483.714231016444
Iteration 7800: Loss = -12483.71409767719
Iteration 7900: Loss = -12483.716426092229
1
Iteration 8000: Loss = -12483.714001724871
Iteration 8100: Loss = -12483.713964481696
Iteration 8200: Loss = -12483.71796997503
1
Iteration 8300: Loss = -12483.713864293222
Iteration 8400: Loss = -12483.713778804775
Iteration 8500: Loss = -12483.714332290516
1
Iteration 8600: Loss = -12483.71377375586
Iteration 8700: Loss = -12483.713705330669
Iteration 8800: Loss = -12483.713706578032
Iteration 8900: Loss = -12483.713721356562
Iteration 9000: Loss = -12483.71357857774
Iteration 9100: Loss = -12483.71353368094
Iteration 9200: Loss = -12483.966321166476
1
Iteration 9300: Loss = -12483.713490944401
Iteration 9400: Loss = -12483.713489135223
Iteration 9500: Loss = -12483.71343471731
Iteration 9600: Loss = -12483.713511194504
Iteration 9700: Loss = -12483.713408446392
Iteration 9800: Loss = -12483.713397126523
Iteration 9900: Loss = -12483.726338576022
1
Iteration 10000: Loss = -12483.713374974863
Iteration 10100: Loss = -12483.713317997039
Iteration 10200: Loss = -12483.75833614219
1
Iteration 10300: Loss = -12483.713304547444
Iteration 10400: Loss = -12483.713288095412
Iteration 10500: Loss = -12484.276827476398
1
Iteration 10600: Loss = -12483.713288524908
Iteration 10700: Loss = -12483.71324505866
Iteration 10800: Loss = -12483.713278340643
Iteration 10900: Loss = -12483.71321427329
Iteration 11000: Loss = -12483.713227450091
Iteration 11100: Loss = -12483.713177689247
Iteration 11200: Loss = -12483.717866518065
1
Iteration 11300: Loss = -12483.713167732214
Iteration 11400: Loss = -12483.713186080384
Iteration 11500: Loss = -12484.004360484667
1
Iteration 11600: Loss = -12483.713170043535
Iteration 11700: Loss = -12483.713121811605
Iteration 11800: Loss = -12483.71313107107
Iteration 11900: Loss = -12483.713302585604
1
Iteration 12000: Loss = -12483.713163670032
Iteration 12100: Loss = -12483.713142725728
Iteration 12200: Loss = -12483.721973249001
1
Iteration 12300: Loss = -12483.71317688021
Iteration 12400: Loss = -12483.713082935328
Iteration 12500: Loss = -12483.713082892924
Iteration 12600: Loss = -12483.713470501565
1
Iteration 12700: Loss = -12483.713052513398
Iteration 12800: Loss = -12483.71379945715
1
Iteration 12900: Loss = -12483.713172731075
2
Iteration 13000: Loss = -12483.713175118939
3
Iteration 13100: Loss = -12483.71315855093
4
Iteration 13200: Loss = -12483.71317016115
5
Iteration 13300: Loss = -12483.71328128317
6
Iteration 13400: Loss = -12483.713105852328
Iteration 13500: Loss = -12483.713076309317
Iteration 13600: Loss = -12483.713037780992
Iteration 13700: Loss = -12483.713075467636
Iteration 13800: Loss = -12483.713171148838
Iteration 13900: Loss = -12483.719490111534
1
Iteration 14000: Loss = -12483.713092051294
Iteration 14100: Loss = -12483.714639882466
1
Iteration 14200: Loss = -12483.714063330195
2
Iteration 14300: Loss = -12483.713366678974
3
Iteration 14400: Loss = -12484.020555169203
4
Iteration 14500: Loss = -12483.713162158027
Iteration 14600: Loss = -12483.810092369407
1
Iteration 14700: Loss = -12483.713080151436
Iteration 14800: Loss = -12483.855953294385
1
Iteration 14900: Loss = -12483.713086203947
Iteration 15000: Loss = -12483.71773042925
1
Iteration 15100: Loss = -12483.713092426873
Iteration 15200: Loss = -12483.713098866103
Iteration 15300: Loss = -12484.11999267386
1
Iteration 15400: Loss = -12483.713124292106
Iteration 15500: Loss = -12483.713113672318
Iteration 15600: Loss = -12483.713102218719
Iteration 15700: Loss = -12483.713890039731
1
Iteration 15800: Loss = -12483.713115827824
Iteration 15900: Loss = -12483.713148502684
Iteration 16000: Loss = -12483.730006689622
1
Iteration 16100: Loss = -12483.713076967033
Iteration 16200: Loss = -12483.713241743326
1
Iteration 16300: Loss = -12483.713001429289
Iteration 16400: Loss = -12483.71306316157
Iteration 16500: Loss = -12483.71300925632
Iteration 16600: Loss = -12483.713626529972
1
Iteration 16700: Loss = -12483.712988125628
Iteration 16800: Loss = -12483.713049678485
Iteration 16900: Loss = -12483.71302038328
Iteration 17000: Loss = -12483.772982618579
1
Iteration 17100: Loss = -12483.712995588632
Iteration 17200: Loss = -12483.732292285371
1
Iteration 17300: Loss = -12483.71302185999
Iteration 17400: Loss = -12483.71306017189
Iteration 17500: Loss = -12483.713029309556
Iteration 17600: Loss = -12483.713030859548
Iteration 17700: Loss = -12483.782818795067
1
Iteration 17800: Loss = -12483.713217040919
2
Iteration 17900: Loss = -12483.713386173906
3
Iteration 18000: Loss = -12483.713009813237
Iteration 18100: Loss = -12483.71301271735
Iteration 18200: Loss = -12483.713161859207
1
Iteration 18300: Loss = -12483.713000614867
Iteration 18400: Loss = -12483.714534341023
1
Iteration 18500: Loss = -12483.712992457815
Iteration 18600: Loss = -12483.713061045864
Iteration 18700: Loss = -12483.717505565412
1
Iteration 18800: Loss = -12483.811183932643
2
Iteration 18900: Loss = -12483.713012595663
Iteration 19000: Loss = -12483.713005551483
Iteration 19100: Loss = -12483.713312062553
1
Iteration 19200: Loss = -12483.713006316006
Iteration 19300: Loss = -12483.760765970314
1
Iteration 19400: Loss = -12483.712983270669
Iteration 19500: Loss = -12483.729857845336
1
Iteration 19600: Loss = -12483.71301245769
Iteration 19700: Loss = -12483.715828032235
1
Iteration 19800: Loss = -12483.713319122699
2
Iteration 19900: Loss = -12483.712985943423
pi: tensor([[2.4024e-07, 1.0000e+00],
        [1.3804e-02, 9.8620e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0873e-04, 9.9989e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3108, 0.2024],
         [0.7196, 0.2020]],

        [[0.6511, 0.2258],
         [0.5146, 0.5951]],

        [[0.5410, 0.0867],
         [0.5600, 0.6786]],

        [[0.5672, 0.0610],
         [0.6485, 0.6440]],

        [[0.6147, 0.2532],
         [0.5274, 0.6607]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0009811988161079529
Average Adjusted Rand Index: 0.00033344555058593435
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22041.00414284504
Iteration 100: Loss = -12488.926317358111
Iteration 200: Loss = -12488.412472963366
Iteration 300: Loss = -12488.31804431909
Iteration 400: Loss = -12488.273391240271
Iteration 500: Loss = -12488.247459998014
Iteration 600: Loss = -12488.229117649871
Iteration 700: Loss = -12488.2126498318
Iteration 800: Loss = -12488.190771615835
Iteration 900: Loss = -12488.120418475339
Iteration 1000: Loss = -12485.704464351347
Iteration 1100: Loss = -12483.706726175482
Iteration 1200: Loss = -12482.548546915605
Iteration 1300: Loss = -12482.290523594836
Iteration 1400: Loss = -12482.190891475366
Iteration 1500: Loss = -12482.136995077584
Iteration 1600: Loss = -12482.103351721724
Iteration 1700: Loss = -12482.080429316793
Iteration 1800: Loss = -12482.06381497892
Iteration 1900: Loss = -12482.05129219442
Iteration 2000: Loss = -12482.041543229518
Iteration 2100: Loss = -12482.03374960154
Iteration 2200: Loss = -12482.027464374825
Iteration 2300: Loss = -12482.022205654772
Iteration 2400: Loss = -12482.017791991633
Iteration 2500: Loss = -12482.014075193169
Iteration 2600: Loss = -12482.010907611826
Iteration 2700: Loss = -12482.008080919535
Iteration 2800: Loss = -12482.005706129105
Iteration 2900: Loss = -12482.003580949578
Iteration 3000: Loss = -12482.001759335279
Iteration 3100: Loss = -12482.00008766182
Iteration 3200: Loss = -12481.998624471611
Iteration 3300: Loss = -12481.997325390485
Iteration 3400: Loss = -12481.996099757933
Iteration 3500: Loss = -12481.995053152554
Iteration 3600: Loss = -12481.994062014795
Iteration 3700: Loss = -12481.993214067454
Iteration 3800: Loss = -12481.99239488966
Iteration 3900: Loss = -12481.991636853618
Iteration 4000: Loss = -12481.99094254822
Iteration 4100: Loss = -12481.990362487402
Iteration 4200: Loss = -12481.989807173315
Iteration 4300: Loss = -12481.98925384525
Iteration 4400: Loss = -12481.988769465746
Iteration 4500: Loss = -12481.988330974922
Iteration 4600: Loss = -12481.987895690767
Iteration 4700: Loss = -12481.98749975755
Iteration 4800: Loss = -12481.987151343861
Iteration 4900: Loss = -12481.986832749368
Iteration 5000: Loss = -12481.986541979155
Iteration 5100: Loss = -12481.986224951801
Iteration 5200: Loss = -12481.98595525957
Iteration 5300: Loss = -12481.98568246806
Iteration 5400: Loss = -12481.985439741089
Iteration 5500: Loss = -12481.98525880873
Iteration 5600: Loss = -12481.985016629413
Iteration 5700: Loss = -12481.984834840378
Iteration 5800: Loss = -12481.984650356982
Iteration 5900: Loss = -12481.984491213678
Iteration 6000: Loss = -12481.985097733488
1
Iteration 6100: Loss = -12481.984137110294
Iteration 6200: Loss = -12481.98398499903
Iteration 6300: Loss = -12481.983889254567
Iteration 6400: Loss = -12481.983736642162
Iteration 6500: Loss = -12481.983617126483
Iteration 6600: Loss = -12481.9835583522
Iteration 6700: Loss = -12481.983414767192
Iteration 6800: Loss = -12481.983314067376
Iteration 6900: Loss = -12481.984138645366
1
Iteration 7000: Loss = -12481.983105670517
Iteration 7100: Loss = -12481.983069153772
Iteration 7200: Loss = -12481.982950443573
Iteration 7300: Loss = -12481.982868319825
Iteration 7400: Loss = -12481.982755352637
Iteration 7500: Loss = -12481.982674315614
Iteration 7600: Loss = -12482.044839503764
1
Iteration 7700: Loss = -12481.98250632634
Iteration 7800: Loss = -12481.98242328022
Iteration 7900: Loss = -12481.982388175504
Iteration 8000: Loss = -12482.071169835723
1
Iteration 8100: Loss = -12481.982248559723
Iteration 8200: Loss = -12481.982221774899
Iteration 8300: Loss = -12481.98215485706
Iteration 8400: Loss = -12481.98316609382
1
Iteration 8500: Loss = -12481.98207350614
Iteration 8600: Loss = -12481.982040583891
Iteration 8700: Loss = -12481.981979713843
Iteration 8800: Loss = -12481.983710246428
1
Iteration 8900: Loss = -12481.981996938981
Iteration 9000: Loss = -12481.981919809365
Iteration 9100: Loss = -12481.981929533318
Iteration 9200: Loss = -12481.982166899032
1
Iteration 9300: Loss = -12481.981902819556
Iteration 9400: Loss = -12481.981875228032
Iteration 9500: Loss = -12481.9818362504
Iteration 9600: Loss = -12481.983434821143
1
Iteration 9700: Loss = -12481.981787818724
Iteration 9800: Loss = -12481.98179045917
Iteration 9900: Loss = -12481.981773814623
Iteration 10000: Loss = -12481.98272376161
1
Iteration 10100: Loss = -12481.981711263248
Iteration 10200: Loss = -12481.981680462402
Iteration 10300: Loss = -12481.981669176685
Iteration 10400: Loss = -12481.982836136243
1
Iteration 10500: Loss = -12481.981628165857
Iteration 10600: Loss = -12481.982134565038
1
Iteration 10700: Loss = -12481.981630235778
Iteration 10800: Loss = -12481.981876516313
1
Iteration 10900: Loss = -12481.981637132392
Iteration 11000: Loss = -12481.981613605318
Iteration 11100: Loss = -12481.997012459178
1
Iteration 11200: Loss = -12481.98158625941
Iteration 11300: Loss = -12481.981623116486
Iteration 11400: Loss = -12481.981196042658
Iteration 11500: Loss = -12482.053338789377
1
Iteration 11600: Loss = -12481.981176000698
Iteration 11700: Loss = -12481.981144942089
Iteration 11800: Loss = -12481.98118714164
Iteration 11900: Loss = -12482.005379835022
1
Iteration 12000: Loss = -12481.98114412987
Iteration 12100: Loss = -12481.98116223374
Iteration 12200: Loss = -12481.981117042618
Iteration 12300: Loss = -12481.988252339486
1
Iteration 12400: Loss = -12481.9811115549
Iteration 12500: Loss = -12481.981106369414
Iteration 12600: Loss = -12481.98108371068
Iteration 12700: Loss = -12481.981141356753
Iteration 12800: Loss = -12481.98108402878
Iteration 12900: Loss = -12481.98109676995
Iteration 13000: Loss = -12482.024264163254
1
Iteration 13100: Loss = -12481.981104539442
Iteration 13200: Loss = -12481.981093525344
Iteration 13300: Loss = -12481.981070304906
Iteration 13400: Loss = -12481.981300189436
1
Iteration 13500: Loss = -12481.981076051185
Iteration 13600: Loss = -12481.981195705457
1
Iteration 13700: Loss = -12481.981152682325
Iteration 13800: Loss = -12481.98107920927
Iteration 13900: Loss = -12481.981057010347
Iteration 14000: Loss = -12481.981265749895
1
Iteration 14100: Loss = -12481.981064205856
Iteration 14200: Loss = -12481.981054289208
Iteration 14300: Loss = -12481.981228529608
1
Iteration 14400: Loss = -12481.981071483993
Iteration 14500: Loss = -12481.981819771943
1
Iteration 14600: Loss = -12481.981044579072
Iteration 14700: Loss = -12481.981058277832
Iteration 14800: Loss = -12481.981043496067
Iteration 14900: Loss = -12481.981110994915
Iteration 15000: Loss = -12481.98110734468
Iteration 15100: Loss = -12481.98101647866
Iteration 15200: Loss = -12481.98142100161
1
Iteration 15300: Loss = -12481.98100558583
Iteration 15400: Loss = -12481.981013822291
Iteration 15500: Loss = -12481.981023743192
Iteration 15600: Loss = -12481.98260594343
1
Iteration 15700: Loss = -12481.982117115032
2
Iteration 15800: Loss = -12481.98100647062
Iteration 15900: Loss = -12481.996132724049
1
Iteration 16000: Loss = -12481.981506025491
2
Iteration 16100: Loss = -12481.981048180796
Iteration 16200: Loss = -12482.005173126872
1
Iteration 16300: Loss = -12481.981425767262
2
Iteration 16400: Loss = -12481.9812284681
3
Iteration 16500: Loss = -12482.00453722271
4
Iteration 16600: Loss = -12481.981617972251
5
Iteration 16700: Loss = -12481.981036844127
Iteration 16800: Loss = -12481.982508985137
1
Iteration 16900: Loss = -12481.981026798798
Iteration 17000: Loss = -12481.98125906386
1
Iteration 17100: Loss = -12481.981581796404
2
Iteration 17200: Loss = -12481.981219484975
3
Iteration 17300: Loss = -12482.004593700927
4
Iteration 17400: Loss = -12481.98132238437
5
Iteration 17500: Loss = -12481.981963772358
6
Iteration 17600: Loss = -12481.981059102714
Iteration 17700: Loss = -12481.981048440693
Iteration 17800: Loss = -12482.117790855424
1
Iteration 17900: Loss = -12481.981351348972
2
Iteration 18000: Loss = -12481.981048259779
Iteration 18100: Loss = -12481.982470558769
1
Iteration 18200: Loss = -12481.981126580782
Iteration 18300: Loss = -12481.981041230458
Iteration 18400: Loss = -12481.985047011693
1
Iteration 18500: Loss = -12481.981870090262
2
Iteration 18600: Loss = -12481.988426620404
3
Iteration 18700: Loss = -12481.981239031331
4
Iteration 18800: Loss = -12481.981056104798
Iteration 18900: Loss = -12482.001897676591
1
Iteration 19000: Loss = -12481.98105807152
Iteration 19100: Loss = -12481.981198351818
1
Iteration 19200: Loss = -12481.983170447014
2
Iteration 19300: Loss = -12481.981678144619
3
Iteration 19400: Loss = -12482.320041639989
4
Iteration 19500: Loss = -12481.98144537829
5
Iteration 19600: Loss = -12481.981660597605
6
Iteration 19700: Loss = -12481.9818572946
7
Iteration 19800: Loss = -12481.981218530844
8
Iteration 19900: Loss = -12481.981824728076
9
pi: tensor([[1.0000e+00, 4.6026e-07],
        [1.6203e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0100, 0.9900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1828, 0.2628],
         [0.5732, 0.2035]],

        [[0.5748, 0.2020],
         [0.5988, 0.6981]],

        [[0.6797, 0.2427],
         [0.5040, 0.5708]],

        [[0.5353, 0.0608],
         [0.6943, 0.5946]],

        [[0.6748, 0.1212],
         [0.6152, 0.6867]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.01126387000386094
Global Adjusted Rand Index: 0.0026934008494996467
Average Adjusted Rand Index: 0.002789193185806826
11857.410216573497
[0.0009811988161079529, 0.0026934008494996467] [0.00033344555058593435, 0.002789193185806826] [12483.93687992905, 12481.981057380734]
-------------------------------------
This iteration is 72
True Objective function: Loss = -11841.756752594172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22959.102428209244
Iteration 100: Loss = -12281.146074093129
Iteration 200: Loss = -11938.694460597675
Iteration 300: Loss = -11843.216362581872
Iteration 400: Loss = -11842.68344301446
Iteration 500: Loss = -11842.50738618042
Iteration 600: Loss = -11835.855173128353
Iteration 700: Loss = -11835.73259608432
Iteration 800: Loss = -11835.69190041504
Iteration 900: Loss = -11835.664317940036
Iteration 1000: Loss = -11835.64428894108
Iteration 1100: Loss = -11835.629088819363
Iteration 1200: Loss = -11835.61719711563
Iteration 1300: Loss = -11835.607675303598
Iteration 1400: Loss = -11835.59999924774
Iteration 1500: Loss = -11835.593623028579
Iteration 1600: Loss = -11835.588241657253
Iteration 1700: Loss = -11835.583753832243
Iteration 1800: Loss = -11835.579862735349
Iteration 1900: Loss = -11835.576541651002
Iteration 2000: Loss = -11835.57364990499
Iteration 2100: Loss = -11835.571073129986
Iteration 2200: Loss = -11835.568829429738
Iteration 2300: Loss = -11835.566754823114
Iteration 2400: Loss = -11835.564792255054
Iteration 2500: Loss = -11835.563378398869
Iteration 2600: Loss = -11835.561051281256
Iteration 2700: Loss = -11835.559606482999
Iteration 2800: Loss = -11835.558734592938
Iteration 2900: Loss = -11835.557405717722
Iteration 3000: Loss = -11835.556449102687
Iteration 3100: Loss = -11835.574596075774
1
Iteration 3200: Loss = -11835.554873882822
Iteration 3300: Loss = -11835.554192636562
Iteration 3400: Loss = -11835.566442237476
1
Iteration 3500: Loss = -11835.553024796638
Iteration 3600: Loss = -11835.552521054871
Iteration 3700: Loss = -11835.55877514896
1
Iteration 3800: Loss = -11835.552999841124
2
Iteration 3900: Loss = -11835.553775069791
3
Iteration 4000: Loss = -11835.550828818836
Iteration 4100: Loss = -11835.550463514364
Iteration 4200: Loss = -11835.55040489977
Iteration 4300: Loss = -11835.549824326428
Iteration 4400: Loss = -11835.574067245985
1
Iteration 4500: Loss = -11835.54926871691
Iteration 4600: Loss = -11835.548956225348
Iteration 4700: Loss = -11835.550889509475
1
Iteration 4800: Loss = -11835.557931878611
2
Iteration 4900: Loss = -11835.549060137362
3
Iteration 5000: Loss = -11835.548124482479
Iteration 5100: Loss = -11835.547977444963
Iteration 5200: Loss = -11835.548286457122
1
Iteration 5300: Loss = -11835.547642177085
Iteration 5400: Loss = -11835.547550394236
Iteration 5500: Loss = -11835.547508285443
Iteration 5600: Loss = -11835.547272201582
Iteration 5700: Loss = -11835.547777560067
1
Iteration 5800: Loss = -11835.54709369888
Iteration 5900: Loss = -11835.55012831746
1
Iteration 6000: Loss = -11835.548599916214
2
Iteration 6100: Loss = -11835.54682096801
Iteration 6200: Loss = -11835.546993526497
1
Iteration 6300: Loss = -11835.54660093152
Iteration 6400: Loss = -11835.546800868842
1
Iteration 6500: Loss = -11835.546634423044
Iteration 6600: Loss = -11835.546409044271
Iteration 6700: Loss = -11835.546547222684
1
Iteration 6800: Loss = -11835.546316729582
Iteration 6900: Loss = -11835.546257233489
Iteration 7000: Loss = -11835.546221073015
Iteration 7100: Loss = -11835.546201078585
Iteration 7200: Loss = -11835.546094424513
Iteration 7300: Loss = -11835.546036316211
Iteration 7400: Loss = -11835.546058469814
Iteration 7500: Loss = -11835.546076470135
Iteration 7600: Loss = -11835.54637363392
1
Iteration 7700: Loss = -11835.560840390493
2
Iteration 7800: Loss = -11835.596830732778
3
Iteration 7900: Loss = -11835.546791055986
4
Iteration 8000: Loss = -11835.546420913091
5
Iteration 8100: Loss = -11835.545923642721
Iteration 8200: Loss = -11835.545907104788
Iteration 8300: Loss = -11835.54600764192
1
Iteration 8400: Loss = -11835.546876947694
2
Iteration 8500: Loss = -11835.545695977551
Iteration 8600: Loss = -11835.54620323551
1
Iteration 8700: Loss = -11835.552090923262
2
Iteration 8800: Loss = -11835.574901903205
3
Iteration 8900: Loss = -11835.622381521702
4
Iteration 9000: Loss = -11835.54802498394
5
Iteration 9100: Loss = -11835.585624669025
6
Iteration 9200: Loss = -11835.563601119815
7
Iteration 9300: Loss = -11835.545510282685
Iteration 9400: Loss = -11835.545908851711
1
Iteration 9500: Loss = -11835.556519770813
2
Iteration 9600: Loss = -11835.549358931406
3
Iteration 9700: Loss = -11835.566242707644
4
Iteration 9800: Loss = -11835.551820252984
5
Iteration 9900: Loss = -11835.550557714265
6
Iteration 10000: Loss = -11835.55163796053
7
Iteration 10100: Loss = -11835.599461826889
8
Iteration 10200: Loss = -11835.674078381768
9
Iteration 10300: Loss = -11835.545364524472
Iteration 10400: Loss = -11835.545579943924
1
Iteration 10500: Loss = -11835.550460736802
2
Iteration 10600: Loss = -11835.545458432922
Iteration 10700: Loss = -11835.550038713143
1
Iteration 10800: Loss = -11835.555526759828
2
Iteration 10900: Loss = -11835.58509512616
3
Iteration 11000: Loss = -11835.559905864724
4
Iteration 11100: Loss = -11835.557487530494
5
Iteration 11200: Loss = -11835.546653312193
6
Iteration 11300: Loss = -11835.550007205493
7
Iteration 11400: Loss = -11835.552905218092
8
Iteration 11500: Loss = -11835.55164941412
9
Iteration 11600: Loss = -11835.551652307377
10
Iteration 11700: Loss = -11835.555397584096
11
Iteration 11800: Loss = -11835.557749895743
12
Iteration 11900: Loss = -11835.546335334526
13
Iteration 12000: Loss = -11835.552181069466
14
Iteration 12100: Loss = -11835.545726521526
15
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.7650, 0.2350],
        [0.2208, 0.7792]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4768, 0.5232], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3044, 0.0951],
         [0.6760, 0.2952]],

        [[0.5531, 0.1013],
         [0.7260, 0.6648]],

        [[0.6435, 0.0970],
         [0.6722, 0.6171]],

        [[0.5111, 0.1112],
         [0.6031, 0.5524]],

        [[0.5221, 0.0976],
         [0.5754, 0.6044]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999944811108
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22717.567712606917
Iteration 100: Loss = -12356.720303785816
Iteration 200: Loss = -12356.087391445992
Iteration 300: Loss = -12355.926940083224
Iteration 400: Loss = -12355.85834949807
Iteration 500: Loss = -12355.82273295185
Iteration 600: Loss = -12355.801792216058
Iteration 700: Loss = -12355.787788248379
Iteration 800: Loss = -12355.777344475084
Iteration 900: Loss = -12355.768765411862
Iteration 1000: Loss = -12355.761082380888
Iteration 1100: Loss = -12355.753700450548
Iteration 1200: Loss = -12355.74616175535
Iteration 1300: Loss = -12355.738433178445
Iteration 1400: Loss = -12355.73020000422
Iteration 1500: Loss = -12355.721561800327
Iteration 1600: Loss = -12355.712220089415
Iteration 1700: Loss = -12355.70235330104
Iteration 1800: Loss = -12355.691696495383
Iteration 1900: Loss = -12355.680162862962
Iteration 2000: Loss = -12355.66764753834
Iteration 2100: Loss = -12355.65375195829
Iteration 2200: Loss = -12355.637886843024
Iteration 2300: Loss = -12355.619195823081
Iteration 2400: Loss = -12355.596308284621
Iteration 2500: Loss = -12355.56731742677
Iteration 2600: Loss = -12355.528533301722
Iteration 2700: Loss = -12355.47178872904
Iteration 2800: Loss = -12355.373173602988
Iteration 2900: Loss = -12355.14829180804
Iteration 3000: Loss = -12354.739666568355
Iteration 3100: Loss = -12354.453076918415
Iteration 3200: Loss = -12354.21533551283
Iteration 3300: Loss = -12353.877223670539
Iteration 3400: Loss = -12353.679588374038
Iteration 3500: Loss = -12353.615312456403
Iteration 3600: Loss = -12353.580143977357
Iteration 3700: Loss = -12353.557623290075
Iteration 3800: Loss = -12353.543925028289
Iteration 3900: Loss = -12353.535531709162
Iteration 4000: Loss = -12353.530031665117
Iteration 4100: Loss = -12353.526110739791
Iteration 4200: Loss = -12353.523216207006
Iteration 4300: Loss = -12353.520841227302
Iteration 4400: Loss = -12353.518986869836
Iteration 4500: Loss = -12353.517338465259
Iteration 4600: Loss = -12353.51594404689
Iteration 4700: Loss = -12353.514665015095
Iteration 4800: Loss = -12353.513512561982
Iteration 4900: Loss = -12353.512403293511
Iteration 5000: Loss = -12353.511294927323
Iteration 5100: Loss = -12353.510245836382
Iteration 5200: Loss = -12353.509162464245
Iteration 5300: Loss = -12353.50807536378
Iteration 5400: Loss = -12353.506952935364
Iteration 5500: Loss = -12353.505738284673
Iteration 5600: Loss = -12353.50458744308
Iteration 5700: Loss = -12353.503453365007
Iteration 5800: Loss = -12353.50234522765
Iteration 5900: Loss = -12353.501401487993
Iteration 6000: Loss = -12353.500573203723
Iteration 6100: Loss = -12353.499862880877
Iteration 6200: Loss = -12353.49921372205
Iteration 6300: Loss = -12353.498774965065
Iteration 6400: Loss = -12353.498363913086
Iteration 6500: Loss = -12353.498063020865
Iteration 6600: Loss = -12353.498285251962
1
Iteration 6700: Loss = -12353.497552553541
Iteration 6800: Loss = -12353.497376221201
Iteration 6900: Loss = -12353.498169799968
1
Iteration 7000: Loss = -12353.497075165795
Iteration 7100: Loss = -12353.496963319707
Iteration 7200: Loss = -12353.497361406427
1
Iteration 7300: Loss = -12353.4967526833
Iteration 7400: Loss = -12353.496667462758
Iteration 7500: Loss = -12353.497738445134
1
Iteration 7600: Loss = -12353.49654458229
Iteration 7700: Loss = -12353.496453138903
Iteration 7800: Loss = -12353.49652495757
Iteration 7900: Loss = -12353.496311081975
Iteration 8000: Loss = -12353.496546078364
1
Iteration 8100: Loss = -12353.496285869016
Iteration 8200: Loss = -12353.496195546832
Iteration 8300: Loss = -12353.558497507476
1
Iteration 8400: Loss = -12353.496130753605
Iteration 8500: Loss = -12353.496097023348
Iteration 8600: Loss = -12353.624282842828
1
Iteration 8700: Loss = -12353.496019176648
Iteration 8800: Loss = -12353.49597001808
Iteration 8900: Loss = -12353.530659699281
1
Iteration 9000: Loss = -12353.553900691004
2
Iteration 9100: Loss = -12353.49641901037
3
Iteration 9200: Loss = -12353.496913781784
4
Iteration 9300: Loss = -12353.50418791166
5
Iteration 9400: Loss = -12353.496785477326
6
Iteration 9500: Loss = -12353.49633749501
7
Iteration 9600: Loss = -12353.49672794247
8
Iteration 9700: Loss = -12353.496350441963
9
Iteration 9800: Loss = -12353.495809342674
Iteration 9900: Loss = -12353.495738868522
Iteration 10000: Loss = -12353.495810055192
Iteration 10100: Loss = -12353.49573403941
Iteration 10200: Loss = -12353.504261632757
1
Iteration 10300: Loss = -12353.49698348032
2
Iteration 10400: Loss = -12353.503682147379
3
Iteration 10500: Loss = -12353.49992779752
4
Iteration 10600: Loss = -12353.498564450976
5
Iteration 10700: Loss = -12353.540055546484
6
Iteration 10800: Loss = -12353.4956665428
Iteration 10900: Loss = -12353.50089453033
1
Iteration 11000: Loss = -12353.495623741652
Iteration 11100: Loss = -12353.496704779647
1
Iteration 11200: Loss = -12353.500228578096
2
Iteration 11300: Loss = -12353.495610650654
Iteration 11400: Loss = -12353.502813104686
1
Iteration 11500: Loss = -12353.647219628238
2
Iteration 11600: Loss = -12353.496325317132
3
Iteration 11700: Loss = -12353.497357630567
4
Iteration 11800: Loss = -12353.60943027876
5
Iteration 11900: Loss = -12353.49713317586
6
Iteration 12000: Loss = -12353.49559480955
Iteration 12100: Loss = -12353.496445491253
1
Iteration 12200: Loss = -12353.497101735558
2
Iteration 12300: Loss = -12353.496808349695
3
Iteration 12400: Loss = -12353.49555557663
Iteration 12500: Loss = -12353.497035289824
1
Iteration 12600: Loss = -12353.498136570117
2
Iteration 12700: Loss = -12353.503425636987
3
Iteration 12800: Loss = -12353.49561235791
Iteration 12900: Loss = -12353.49633794917
1
Iteration 13000: Loss = -12353.498216884733
2
Iteration 13100: Loss = -12353.515356789536
3
Iteration 13200: Loss = -12353.49629929179
4
Iteration 13300: Loss = -12353.495705715384
Iteration 13400: Loss = -12353.498872386554
1
Iteration 13500: Loss = -12353.497846268448
2
Iteration 13600: Loss = -12353.51760525542
3
Iteration 13700: Loss = -12353.505268461322
4
Iteration 13800: Loss = -12353.50110966357
5
Iteration 13900: Loss = -12353.497990632428
6
Iteration 14000: Loss = -12353.500050721737
7
Iteration 14100: Loss = -12353.496106770914
8
Iteration 14200: Loss = -12353.4956311524
Iteration 14300: Loss = -12353.620480940262
1
Iteration 14400: Loss = -12353.495522793433
Iteration 14500: Loss = -12353.49595683111
1
Iteration 14600: Loss = -12353.50302001134
2
Iteration 14700: Loss = -12353.506314301261
3
Iteration 14800: Loss = -12353.49704342479
4
Iteration 14900: Loss = -12353.521784210165
5
Iteration 15000: Loss = -12353.539501242594
6
Iteration 15100: Loss = -12353.528039954017
7
Iteration 15200: Loss = -12353.535297269887
8
Iteration 15300: Loss = -12353.522142832158
9
Iteration 15400: Loss = -12353.499499124131
10
Iteration 15500: Loss = -12353.631301505888
11
Iteration 15600: Loss = -12353.497016952133
12
Iteration 15700: Loss = -12353.540937028742
13
Iteration 15800: Loss = -12353.643553922348
14
Iteration 15900: Loss = -12353.495501195403
Iteration 16000: Loss = -12353.496202257118
1
Iteration 16100: Loss = -12353.495632357646
2
Iteration 16200: Loss = -12353.545907795817
3
Iteration 16300: Loss = -12353.50196860689
4
Iteration 16400: Loss = -12353.679704495036
5
Iteration 16500: Loss = -12353.495502983335
Iteration 16600: Loss = -12353.49628315582
1
Iteration 16700: Loss = -12353.495573474247
Iteration 16800: Loss = -12353.497702588362
1
Iteration 16900: Loss = -12353.497598929534
2
Iteration 17000: Loss = -12353.49547182672
Iteration 17100: Loss = -12353.64770326399
1
Iteration 17200: Loss = -12353.496699283172
2
Iteration 17300: Loss = -12353.588006619968
3
Iteration 17400: Loss = -12353.49547977213
Iteration 17500: Loss = -12353.497572182456
1
Iteration 17600: Loss = -12353.572469289049
2
Iteration 17700: Loss = -12353.495495435109
Iteration 17800: Loss = -12353.495875989793
1
Iteration 17900: Loss = -12353.496161110177
2
Iteration 18000: Loss = -12353.612046337146
3
Iteration 18100: Loss = -12353.495477538487
Iteration 18200: Loss = -12353.495975586202
1
Iteration 18300: Loss = -12353.534335427183
2
Iteration 18400: Loss = -12353.502213114793
3
Iteration 18500: Loss = -12353.602352520042
4
Iteration 18600: Loss = -12353.496668433652
5
Iteration 18700: Loss = -12353.49548547047
Iteration 18800: Loss = -12353.866036462052
1
Iteration 18900: Loss = -12353.495476068993
Iteration 19000: Loss = -12353.495505111841
Iteration 19100: Loss = -12353.495526777528
Iteration 19200: Loss = -12353.71576905991
1
Iteration 19300: Loss = -12353.515209699233
2
Iteration 19400: Loss = -12353.606631122117
3
Iteration 19500: Loss = -12353.50897491888
4
Iteration 19600: Loss = -12353.514111249311
5
Iteration 19700: Loss = -12353.49663031134
6
Iteration 19800: Loss = -12353.495567989454
Iteration 19900: Loss = -12353.684761173732
1
pi: tensor([[4.4742e-07, 1.0000e+00],
        [9.8231e-01, 1.7693e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0629, 0.9371], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2070, 0.1969],
         [0.6136, 0.1956]],

        [[0.5421, 0.1722],
         [0.6628, 0.6523]],

        [[0.5418, 0.1797],
         [0.6876, 0.6125]],

        [[0.6014, 0.2137],
         [0.6153, 0.5496]],

        [[0.6480, 0.2057],
         [0.5172, 0.6070]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0019610639491580675
Average Adjusted Rand Index: 0.0
11841.756752594172
[0.9919999944811108, -0.0019610639491580675] [0.9919992163297293, 0.0] [11835.545726521526, 12353.569912560824]
-------------------------------------
This iteration is 73
True Objective function: Loss = -11849.225215260527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22303.901769360902
Iteration 100: Loss = -12274.389869429167
Iteration 200: Loss = -11918.484128477816
Iteration 300: Loss = -11870.762789222159
Iteration 400: Loss = -11870.43181905241
Iteration 500: Loss = -11870.283634157511
Iteration 600: Loss = -11870.1569440192
Iteration 700: Loss = -11869.981460832096
Iteration 800: Loss = -11869.947538952334
Iteration 900: Loss = -11869.921184879595
Iteration 1000: Loss = -11869.90516968881
Iteration 1100: Loss = -11869.89301160474
Iteration 1200: Loss = -11869.883461922993
Iteration 1300: Loss = -11869.875744560797
Iteration 1400: Loss = -11869.869510390832
Iteration 1500: Loss = -11869.864295225841
Iteration 1600: Loss = -11869.859875530497
Iteration 1700: Loss = -11869.856113492657
Iteration 1800: Loss = -11869.852875643055
Iteration 1900: Loss = -11869.849872913723
Iteration 2000: Loss = -11869.847066861574
Iteration 2100: Loss = -11869.844603647753
Iteration 2200: Loss = -11869.842663757418
Iteration 2300: Loss = -11869.840990613167
Iteration 2400: Loss = -11869.839580677748
Iteration 2500: Loss = -11869.838236430664
Iteration 2600: Loss = -11869.837114891945
Iteration 2700: Loss = -11869.836070905405
Iteration 2800: Loss = -11869.835232359237
Iteration 2900: Loss = -11869.834367138468
Iteration 3000: Loss = -11869.83363780794
Iteration 3100: Loss = -11869.833098307601
Iteration 3200: Loss = -11869.832341568514
Iteration 3300: Loss = -11869.832220593291
Iteration 3400: Loss = -11869.83368943571
1
Iteration 3500: Loss = -11869.831755791045
Iteration 3600: Loss = -11869.831015873586
Iteration 3700: Loss = -11869.830144056248
Iteration 3800: Loss = -11869.834405309643
1
Iteration 3900: Loss = -11869.830550436365
2
Iteration 4000: Loss = -11869.829331589566
Iteration 4100: Loss = -11869.828900686394
Iteration 4200: Loss = -11869.828503356783
Iteration 4300: Loss = -11869.828253055259
Iteration 4400: Loss = -11869.828304948338
Iteration 4500: Loss = -11869.827951650766
Iteration 4600: Loss = -11869.827764525826
Iteration 4700: Loss = -11869.830654815205
1
Iteration 4800: Loss = -11869.837633421477
2
Iteration 4900: Loss = -11869.846694218691
3
Iteration 5000: Loss = -11869.826917307184
Iteration 5100: Loss = -11869.827129517962
1
Iteration 5200: Loss = -11869.826673850928
Iteration 5300: Loss = -11869.826530422177
Iteration 5400: Loss = -11869.826368586331
Iteration 5500: Loss = -11869.826285726753
Iteration 5600: Loss = -11869.826143627752
Iteration 5700: Loss = -11869.826107489005
Iteration 5800: Loss = -11869.826000337951
Iteration 5900: Loss = -11869.8327404062
1
Iteration 6000: Loss = -11869.830432453706
2
Iteration 6100: Loss = -11869.825993592867
Iteration 6200: Loss = -11869.825589090473
Iteration 6300: Loss = -11869.826956204655
1
Iteration 6400: Loss = -11869.820776365414
Iteration 6500: Loss = -11869.824664652591
1
Iteration 6600: Loss = -11869.827069165665
2
Iteration 6700: Loss = -11869.82986399832
3
Iteration 6800: Loss = -11869.818649381716
Iteration 6900: Loss = -11869.81855103228
Iteration 7000: Loss = -11869.818654788833
1
Iteration 7100: Loss = -11869.81881439029
2
Iteration 7200: Loss = -11869.819553032878
3
Iteration 7300: Loss = -11869.818819099892
4
Iteration 7400: Loss = -11869.819427681066
5
Iteration 7500: Loss = -11869.818632731436
Iteration 7600: Loss = -11869.819100527888
1
Iteration 7700: Loss = -11869.81840774416
Iteration 7800: Loss = -11869.819578309622
1
Iteration 7900: Loss = -11869.821030756757
2
Iteration 8000: Loss = -11869.818155412266
Iteration 8100: Loss = -11869.818155850147
Iteration 8200: Loss = -11869.819701148777
1
Iteration 8300: Loss = -11869.817814420187
Iteration 8400: Loss = -11869.819315376546
1
Iteration 8500: Loss = -11869.817799674965
Iteration 8600: Loss = -11869.823945656446
1
Iteration 8700: Loss = -11869.817713317376
Iteration 8800: Loss = -11869.817716723679
Iteration 8900: Loss = -11869.817725900788
Iteration 9000: Loss = -11869.817649531451
Iteration 9100: Loss = -11869.831409209495
1
Iteration 9200: Loss = -11869.818987751687
2
Iteration 9300: Loss = -11869.817602866406
Iteration 9400: Loss = -11869.817550562604
Iteration 9500: Loss = -11869.898731465339
1
Iteration 9600: Loss = -11869.817364453895
Iteration 9700: Loss = -11869.817273974599
Iteration 9800: Loss = -11869.82027565299
1
Iteration 9900: Loss = -11869.856196862118
2
Iteration 10000: Loss = -11869.819460280607
3
Iteration 10100: Loss = -11869.817261001539
Iteration 10200: Loss = -11869.817269084475
Iteration 10300: Loss = -11869.817591780717
1
Iteration 10400: Loss = -11869.817324445996
Iteration 10500: Loss = -11869.819943066808
1
Iteration 10600: Loss = -11869.818801550788
2
Iteration 10700: Loss = -11869.82078616347
3
Iteration 10800: Loss = -11869.818503387374
4
Iteration 10900: Loss = -11869.916548830863
5
Iteration 11000: Loss = -11869.822309856925
6
Iteration 11100: Loss = -11869.820462403784
7
Iteration 11200: Loss = -11869.813932910149
Iteration 11300: Loss = -11869.817401858762
1
Iteration 11400: Loss = -11869.818273107088
2
Iteration 11500: Loss = -11869.813991771087
Iteration 11600: Loss = -11869.813805710659
Iteration 11700: Loss = -11869.817050857466
1
Iteration 11800: Loss = -11869.856651904785
2
Iteration 11900: Loss = -11869.814392521412
3
Iteration 12000: Loss = -11869.821270809953
4
Iteration 12100: Loss = -11869.824617125721
5
Iteration 12200: Loss = -11869.823912165903
6
Iteration 12300: Loss = -11869.81388442491
Iteration 12400: Loss = -11869.820523806417
1
Iteration 12500: Loss = -11869.814541473088
2
Iteration 12600: Loss = -11869.819429948162
3
Iteration 12700: Loss = -11869.81367327731
Iteration 12800: Loss = -11869.81542447413
1
Iteration 12900: Loss = -11869.819699994126
2
Iteration 13000: Loss = -11869.824376381765
3
Iteration 13100: Loss = -11869.81563487383
4
Iteration 13200: Loss = -11870.019877087572
5
Iteration 13300: Loss = -11869.8195574723
6
Iteration 13400: Loss = -11869.814572132733
7
Iteration 13500: Loss = -11869.854699788253
8
Iteration 13600: Loss = -11869.81971591162
9
Iteration 13700: Loss = -11869.819140493702
10
Iteration 13800: Loss = -11869.814002313387
11
Iteration 13900: Loss = -11869.818146750787
12
Iteration 14000: Loss = -11869.815775861236
13
Iteration 14100: Loss = -11869.937221972627
14
Iteration 14200: Loss = -11869.812995207447
Iteration 14300: Loss = -11869.816950588098
1
Iteration 14400: Loss = -11869.820596373425
2
Iteration 14500: Loss = -11869.810425276266
Iteration 14600: Loss = -11869.848618128546
1
Iteration 14700: Loss = -11869.811058941366
2
Iteration 14800: Loss = -11869.822714266995
3
Iteration 14900: Loss = -11869.816920738956
4
Iteration 15000: Loss = -11869.813027809989
5
Iteration 15100: Loss = -11869.898823926049
6
Iteration 15200: Loss = -11869.95130874301
7
Iteration 15300: Loss = -11869.809865296715
Iteration 15400: Loss = -11869.81109608202
1
Iteration 15500: Loss = -11869.812060775212
2
Iteration 15600: Loss = -11869.894740236921
3
Iteration 15700: Loss = -11869.81227067946
4
Iteration 15800: Loss = -11869.812316404781
5
Iteration 15900: Loss = -11869.826188236053
6
Iteration 16000: Loss = -11869.80991673908
Iteration 16100: Loss = -11869.813955776237
1
Iteration 16200: Loss = -11869.859996133064
2
Iteration 16300: Loss = -11870.002703967333
3
Iteration 16400: Loss = -11869.81373071166
4
Iteration 16500: Loss = -11869.81205237859
5
Iteration 16600: Loss = -11869.81029288267
6
Iteration 16700: Loss = -11869.810868092783
7
Iteration 16800: Loss = -11869.871378919306
8
Iteration 16900: Loss = -11869.812570655775
9
Iteration 17000: Loss = -11869.811503148583
10
Iteration 17100: Loss = -11869.810660148205
11
Iteration 17200: Loss = -11869.810544904702
12
Iteration 17300: Loss = -11869.817913860146
13
Iteration 17400: Loss = -11869.832029510171
14
Iteration 17500: Loss = -11869.809631387161
Iteration 17600: Loss = -11869.810865761247
1
Iteration 17700: Loss = -11869.818271256905
2
Iteration 17800: Loss = -11869.818225578834
3
Iteration 17900: Loss = -11869.809228950808
Iteration 18000: Loss = -11869.810627039586
1
Iteration 18100: Loss = -11869.809767040557
2
Iteration 18200: Loss = -11869.813588288605
3
Iteration 18300: Loss = -11869.82947132869
4
Iteration 18400: Loss = -11869.809196598753
Iteration 18500: Loss = -11869.810187412615
1
Iteration 18600: Loss = -11869.809394932267
2
Iteration 18700: Loss = -11869.80926903313
Iteration 18800: Loss = -11869.998564091551
1
Iteration 18900: Loss = -11869.80979505803
2
Iteration 19000: Loss = -11869.81266218255
3
Iteration 19100: Loss = -11869.86563392304
4
Iteration 19200: Loss = -11869.811465540046
5
Iteration 19300: Loss = -11869.80938430372
6
Iteration 19400: Loss = -11869.812874883866
7
Iteration 19500: Loss = -11869.81161932401
8
Iteration 19600: Loss = -11869.809983483432
9
Iteration 19700: Loss = -11869.83082663182
10
Iteration 19800: Loss = -11869.824147613594
11
Iteration 19900: Loss = -11869.809212506452
pi: tensor([[0.6355, 0.3645],
        [0.3166, 0.6834]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4739, 0.5261], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2923, 0.0977],
         [0.5200, 0.3014]],

        [[0.5159, 0.0981],
         [0.6437, 0.6055]],

        [[0.6825, 0.0966],
         [0.5029, 0.6730]],

        [[0.7282, 0.1018],
         [0.6926, 0.5101]],

        [[0.5909, 0.1028],
         [0.5420, 0.6480]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.039690087979238675
Average Adjusted Rand Index: 0.9919850525031807
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22077.65043684097
Iteration 100: Loss = -12344.945448561304
Iteration 200: Loss = -12344.491768326474
Iteration 300: Loss = -12344.337286980257
Iteration 400: Loss = -12344.216802456915
Iteration 500: Loss = -12344.104335593353
Iteration 600: Loss = -12343.927280452961
Iteration 700: Loss = -12343.405512697853
Iteration 800: Loss = -12342.877100652488
Iteration 900: Loss = -12342.752134998233
Iteration 1000: Loss = -12342.712090200503
Iteration 1100: Loss = -12342.69107930421
Iteration 1200: Loss = -12342.67400806912
Iteration 1300: Loss = -12342.64861013111
Iteration 1400: Loss = -12342.607519570203
Iteration 1500: Loss = -12342.57616958672
Iteration 1600: Loss = -12342.559113837284
Iteration 1700: Loss = -12342.534267322555
Iteration 1800: Loss = -12342.480881259182
Iteration 1900: Loss = -12342.35021739542
Iteration 2000: Loss = -12342.164462397861
Iteration 2100: Loss = -12342.064929169515
Iteration 2200: Loss = -12342.024431531998
Iteration 2300: Loss = -12342.004445368906
Iteration 2400: Loss = -12341.992914547554
Iteration 2500: Loss = -12341.985344871246
Iteration 2600: Loss = -12341.980080897298
Iteration 2700: Loss = -12341.976225622619
Iteration 2800: Loss = -12341.973231343645
Iteration 2900: Loss = -12341.970881569987
Iteration 3000: Loss = -12341.968981788832
Iteration 3100: Loss = -12341.967423713513
Iteration 3200: Loss = -12341.96611626215
Iteration 3300: Loss = -12341.965034714103
Iteration 3400: Loss = -12341.964082148783
Iteration 3500: Loss = -12341.96329638579
Iteration 3600: Loss = -12341.962596534939
Iteration 3700: Loss = -12341.96194304535
Iteration 3800: Loss = -12341.961400658642
Iteration 3900: Loss = -12341.96091594923
Iteration 4000: Loss = -12341.960478682311
Iteration 4100: Loss = -12341.960094355449
Iteration 4200: Loss = -12341.959765502772
Iteration 4300: Loss = -12341.9594342905
Iteration 4400: Loss = -12341.959381937651
Iteration 4500: Loss = -12341.958871516614
Iteration 4600: Loss = -12341.958958122332
Iteration 4700: Loss = -12341.958434870752
Iteration 4800: Loss = -12341.961950570249
1
Iteration 4900: Loss = -12341.958071380668
Iteration 5000: Loss = -12341.958260977388
1
Iteration 5100: Loss = -12341.957770820612
Iteration 5200: Loss = -12341.9575799207
Iteration 5300: Loss = -12341.957507817819
Iteration 5400: Loss = -12341.957371554157
Iteration 5500: Loss = -12341.957764398712
1
Iteration 5600: Loss = -12341.957131798388
Iteration 5700: Loss = -12341.961482289318
1
Iteration 5800: Loss = -12341.95692531895
Iteration 5900: Loss = -12341.974073557705
1
Iteration 6000: Loss = -12341.956758187973
Iteration 6100: Loss = -12341.956751909911
Iteration 6200: Loss = -12341.956625209206
Iteration 6300: Loss = -12341.95660415943
Iteration 6400: Loss = -12341.956517576547
Iteration 6500: Loss = -12341.956442731764
Iteration 6600: Loss = -12341.956432986497
Iteration 6700: Loss = -12341.956366585853
Iteration 6800: Loss = -12341.988938828714
1
Iteration 6900: Loss = -12341.95625356765
Iteration 7000: Loss = -12342.020619867997
1
Iteration 7100: Loss = -12341.956190212633
Iteration 7200: Loss = -12341.956128565776
Iteration 7300: Loss = -12341.956123019963
Iteration 7400: Loss = -12341.956072935718
Iteration 7500: Loss = -12341.956056967889
Iteration 7600: Loss = -12341.956031486878
Iteration 7700: Loss = -12341.961096464378
1
Iteration 7800: Loss = -12341.955968967586
Iteration 7900: Loss = -12341.955976994597
Iteration 8000: Loss = -12341.955922744564
Iteration 8100: Loss = -12341.95614752471
1
Iteration 8200: Loss = -12341.985176888305
2
Iteration 8300: Loss = -12341.962700924969
3
Iteration 8400: Loss = -12341.956342103444
4
Iteration 8500: Loss = -12341.956034518817
5
Iteration 8600: Loss = -12341.956024328916
6
Iteration 8700: Loss = -12342.016525096307
7
Iteration 8800: Loss = -12341.980995148822
8
Iteration 8900: Loss = -12341.977146986765
9
Iteration 9000: Loss = -12341.956499634141
10
Iteration 9100: Loss = -12341.95819816169
11
Iteration 9200: Loss = -12342.060024959388
12
Iteration 9300: Loss = -12342.139719062312
13
Iteration 9400: Loss = -12341.958148722677
14
Iteration 9500: Loss = -12341.955881931824
Iteration 9600: Loss = -12341.985587578736
1
Iteration 9700: Loss = -12341.971484744192
2
Iteration 9800: Loss = -12341.989119443362
3
Iteration 9900: Loss = -12341.981655530968
4
Iteration 10000: Loss = -12341.967516513563
5
Iteration 10100: Loss = -12341.956690281508
6
Iteration 10200: Loss = -12341.973510091439
7
Iteration 10300: Loss = -12341.957304395086
8
Iteration 10400: Loss = -12341.957465156187
9
Iteration 10500: Loss = -12341.990854698402
10
Iteration 10600: Loss = -12341.97680349149
11
Iteration 10700: Loss = -12341.955752767455
Iteration 10800: Loss = -12341.955649951884
Iteration 10900: Loss = -12341.95704886496
1
Iteration 11000: Loss = -12341.956088107114
2
Iteration 11100: Loss = -12341.955822866186
3
Iteration 11200: Loss = -12341.95642128595
4
Iteration 11300: Loss = -12341.95678314375
5
Iteration 11400: Loss = -12341.955836009674
6
Iteration 11500: Loss = -12341.95679936389
7
Iteration 11600: Loss = -12341.977886882732
8
Iteration 11700: Loss = -12342.045388134302
9
Iteration 11800: Loss = -12341.95652093495
10
Iteration 11900: Loss = -12341.967172632254
11
Iteration 12000: Loss = -12341.958430250672
12
Iteration 12100: Loss = -12341.95617923563
13
Iteration 12200: Loss = -12341.957441770117
14
Iteration 12300: Loss = -12341.956178615983
15
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[8.7652e-01, 1.2348e-01],
        [4.9401e-05, 9.9995e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9543, 0.0457], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1900, 0.2437],
         [0.7003, 0.2309]],

        [[0.6589, 0.1965],
         [0.6982, 0.5764]],

        [[0.5789, 0.2028],
         [0.5626, 0.7273]],

        [[0.6592, 0.2128],
         [0.7232, 0.6860]],

        [[0.7076, 0.2013],
         [0.7286, 0.6223]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.005131431169739117
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.013865953948865653
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 69
Adjusted Rand Index: 0.13575757575757577
Global Adjusted Rand Index: 0.00920210240428126
Average Adjusted Rand Index: 0.02289909156585135
11849.225215260527
[0.039690087979238675, 0.00920210240428126] [0.9919850525031807, 0.02289909156585135] [11869.810224120314, 12341.956178615983]
-------------------------------------
This iteration is 74
True Objective function: Loss = -11899.890439037514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23043.02628393465
Iteration 100: Loss = -12384.719687576207
Iteration 200: Loss = -12383.880853065322
Iteration 300: Loss = -12383.70670479206
Iteration 400: Loss = -12383.619913374077
Iteration 500: Loss = -12383.56640654397
Iteration 600: Loss = -12383.530022112807
Iteration 700: Loss = -12383.50309122437
Iteration 800: Loss = -12383.481814601144
Iteration 900: Loss = -12383.464234326235
Iteration 1000: Loss = -12383.449367571448
Iteration 1100: Loss = -12383.436447137718
Iteration 1200: Loss = -12383.424883814838
Iteration 1300: Loss = -12383.41434527704
Iteration 1400: Loss = -12383.40443886601
Iteration 1500: Loss = -12383.395030130427
Iteration 1600: Loss = -12383.386002715619
Iteration 1700: Loss = -12383.37721284861
Iteration 1800: Loss = -12383.368629667073
Iteration 1900: Loss = -12383.360119719648
Iteration 2000: Loss = -12383.351791475616
Iteration 2100: Loss = -12383.343639516574
Iteration 2200: Loss = -12383.335755293041
Iteration 2300: Loss = -12383.328253356192
Iteration 2400: Loss = -12383.321323794084
Iteration 2500: Loss = -12383.315001369208
Iteration 2600: Loss = -12383.309579014358
Iteration 2700: Loss = -12383.304967232534
Iteration 2800: Loss = -12383.301427236904
Iteration 2900: Loss = -12383.298666257833
Iteration 3000: Loss = -12383.296585148079
Iteration 3100: Loss = -12383.29504785462
Iteration 3200: Loss = -12383.293892317133
Iteration 3300: Loss = -12383.293036610461
Iteration 3400: Loss = -12383.292348437102
Iteration 3500: Loss = -12383.291786575279
Iteration 3600: Loss = -12383.291373872102
Iteration 3700: Loss = -12383.291050305632
Iteration 3800: Loss = -12383.290799501761
Iteration 3900: Loss = -12383.29054858029
Iteration 4000: Loss = -12383.29034861711
Iteration 4100: Loss = -12383.290161424184
Iteration 4200: Loss = -12383.289985169853
Iteration 4300: Loss = -12383.289849001147
Iteration 4400: Loss = -12383.28967351212
Iteration 4500: Loss = -12383.289513808983
Iteration 4600: Loss = -12383.28936403273
Iteration 4700: Loss = -12383.28920043166
Iteration 4800: Loss = -12383.2890249767
Iteration 4900: Loss = -12383.288909301413
Iteration 5000: Loss = -12383.288708640159
Iteration 5100: Loss = -12383.28850694139
Iteration 5200: Loss = -12383.28834253621
Iteration 5300: Loss = -12383.288222624298
Iteration 5400: Loss = -12383.289830894424
1
Iteration 5500: Loss = -12383.287805432523
Iteration 5600: Loss = -12383.28765628753
Iteration 5700: Loss = -12383.287736469665
Iteration 5800: Loss = -12383.28729647603
Iteration 5900: Loss = -12383.287073511561
Iteration 6000: Loss = -12383.286909103022
Iteration 6100: Loss = -12383.286756145046
Iteration 6200: Loss = -12383.286768007509
Iteration 6300: Loss = -12383.28639022045
Iteration 6400: Loss = -12383.286256032487
Iteration 6500: Loss = -12383.286075814547
Iteration 6600: Loss = -12383.285896382022
Iteration 6700: Loss = -12383.285775741875
Iteration 6800: Loss = -12383.285628210586
Iteration 6900: Loss = -12383.28548264204
Iteration 7000: Loss = -12383.285305883579
Iteration 7100: Loss = -12383.287247769838
1
Iteration 7200: Loss = -12383.285059952894
Iteration 7300: Loss = -12383.284950394234
Iteration 7400: Loss = -12383.284819656837
Iteration 7500: Loss = -12383.284690418071
Iteration 7600: Loss = -12383.284743043274
Iteration 7700: Loss = -12383.286983729902
1
Iteration 7800: Loss = -12383.338515008625
2
Iteration 7900: Loss = -12383.284280427273
Iteration 8000: Loss = -12383.284201827999
Iteration 8100: Loss = -12383.284216903874
Iteration 8200: Loss = -12383.284004113464
Iteration 8300: Loss = -12383.29361210214
1
Iteration 8400: Loss = -12383.2838634963
Iteration 8500: Loss = -12383.283759883407
Iteration 8600: Loss = -12383.28414039098
1
Iteration 8700: Loss = -12383.283635428976
Iteration 8800: Loss = -12383.28358747172
Iteration 8900: Loss = -12383.283658865887
Iteration 9000: Loss = -12383.283457517658
Iteration 9100: Loss = -12383.2833935861
Iteration 9200: Loss = -12383.297649653789
1
Iteration 9300: Loss = -12383.283319588996
Iteration 9400: Loss = -12383.283247950596
Iteration 9500: Loss = -12383.283196512328
Iteration 9600: Loss = -12383.29720815601
1
Iteration 9700: Loss = -12383.28312061458
Iteration 9800: Loss = -12383.28307614991
Iteration 9900: Loss = -12383.478210009122
1
Iteration 10000: Loss = -12383.283035972778
Iteration 10100: Loss = -12383.283019852342
Iteration 10200: Loss = -12383.29285625604
1
Iteration 10300: Loss = -12383.282961474046
Iteration 10400: Loss = -12383.282901209039
Iteration 10500: Loss = -12383.283031218483
1
Iteration 10600: Loss = -12383.282900808066
Iteration 10700: Loss = -12383.282880514369
Iteration 10800: Loss = -12383.282862056833
Iteration 10900: Loss = -12383.282911554246
Iteration 11000: Loss = -12383.282796469282
Iteration 11100: Loss = -12383.282819039321
Iteration 11200: Loss = -12383.282876876263
Iteration 11300: Loss = -12383.28271831924
Iteration 11400: Loss = -12383.282894884018
1
Iteration 11500: Loss = -12383.282699593316
Iteration 11600: Loss = -12383.28436631821
1
Iteration 11700: Loss = -12383.282689936763
Iteration 11800: Loss = -12383.282684418726
Iteration 11900: Loss = -12383.282898200187
1
Iteration 12000: Loss = -12383.282678000045
Iteration 12100: Loss = -12383.282811846266
1
Iteration 12200: Loss = -12383.282802680955
2
Iteration 12300: Loss = -12383.4613228641
3
Iteration 12400: Loss = -12383.282648469325
Iteration 12500: Loss = -12383.290197095524
1
Iteration 12600: Loss = -12383.282603701608
Iteration 12700: Loss = -12383.283614193779
1
Iteration 12800: Loss = -12383.28260621706
Iteration 12900: Loss = -12383.292658205308
1
Iteration 13000: Loss = -12383.28260829759
Iteration 13100: Loss = -12383.282607933543
Iteration 13200: Loss = -12383.307665304212
1
Iteration 13300: Loss = -12383.283885630472
2
Iteration 13400: Loss = -12383.290525672608
3
Iteration 13500: Loss = -12383.282559224985
Iteration 13600: Loss = -12383.284683563019
1
Iteration 13700: Loss = -12383.297323073042
2
Iteration 13800: Loss = -12383.282524204757
Iteration 13900: Loss = -12383.282786820035
1
Iteration 14000: Loss = -12383.282564357098
Iteration 14100: Loss = -12383.282688399317
1
Iteration 14200: Loss = -12383.356916789317
2
Iteration 14300: Loss = -12383.282538783822
Iteration 14400: Loss = -12383.285382985394
1
Iteration 14500: Loss = -12383.38033474012
2
Iteration 14600: Loss = -12383.282544756312
Iteration 14700: Loss = -12383.598741508145
1
Iteration 14800: Loss = -12383.282548054282
Iteration 14900: Loss = -12383.282538554393
Iteration 15000: Loss = -12383.282881674468
1
Iteration 15100: Loss = -12383.289400569014
2
Iteration 15200: Loss = -12383.282478749677
Iteration 15300: Loss = -12383.283913513513
1
Iteration 15400: Loss = -12383.28252903538
Iteration 15500: Loss = -12383.28265476817
1
Iteration 15600: Loss = -12383.301627740155
2
Iteration 15700: Loss = -12383.282557884171
Iteration 15800: Loss = -12383.282996231055
1
Iteration 15900: Loss = -12383.302604333796
2
Iteration 16000: Loss = -12383.282557733766
Iteration 16100: Loss = -12383.282789635137
1
Iteration 16200: Loss = -12383.283831469356
2
Iteration 16300: Loss = -12383.282485970767
Iteration 16400: Loss = -12383.282599484779
1
Iteration 16500: Loss = -12383.28403080791
2
Iteration 16600: Loss = -12383.282581043635
Iteration 16700: Loss = -12383.323942564564
1
Iteration 16800: Loss = -12383.282525273156
Iteration 16900: Loss = -12383.282884555036
1
Iteration 17000: Loss = -12383.345425732314
2
Iteration 17100: Loss = -12383.282846535336
3
Iteration 17200: Loss = -12383.417975134835
4
Iteration 17300: Loss = -12383.282479888734
Iteration 17400: Loss = -12383.319980501963
1
Iteration 17500: Loss = -12383.282491983186
Iteration 17600: Loss = -12383.290790788005
1
Iteration 17700: Loss = -12383.282487827068
Iteration 17800: Loss = -12383.283556987364
1
Iteration 17900: Loss = -12383.28254050315
Iteration 18000: Loss = -12383.41408452728
1
Iteration 18100: Loss = -12383.282479502017
Iteration 18200: Loss = -12383.315813744794
1
Iteration 18300: Loss = -12383.282440189028
Iteration 18400: Loss = -12383.283154106959
1
Iteration 18500: Loss = -12383.282497757456
Iteration 18600: Loss = -12383.503252418457
1
Iteration 18700: Loss = -12383.282490187006
Iteration 18800: Loss = -12383.282714953004
1
Iteration 18900: Loss = -12383.302630997765
2
Iteration 19000: Loss = -12383.282517580335
Iteration 19100: Loss = -12383.28448659336
1
Iteration 19200: Loss = -12383.282540176526
Iteration 19300: Loss = -12383.282512140873
Iteration 19400: Loss = -12383.357448562438
1
Iteration 19500: Loss = -12383.282490716865
Iteration 19600: Loss = -12383.294921402172
1
Iteration 19700: Loss = -12383.282455339018
Iteration 19800: Loss = -12383.286496929748
1
Iteration 19900: Loss = -12383.282468012365
pi: tensor([[9.6525e-01, 3.4748e-02],
        [9.9997e-01, 3.2000e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9899, 0.0101], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1992, 0.1567],
         [0.6441, 0.2355]],

        [[0.5629, 0.2085],
         [0.6066, 0.5436]],

        [[0.5120, 0.2032],
         [0.6040, 0.7050]],

        [[0.6015, 0.2460],
         [0.5731, 0.5054]],

        [[0.6272, 0.2071],
         [0.6758, 0.5222]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21770.960464713582
Iteration 100: Loss = -12272.492870732372
Iteration 200: Loss = -11940.22146707161
Iteration 300: Loss = -11896.814497858713
Iteration 400: Loss = -11896.466265734383
Iteration 500: Loss = -11896.061749178216
Iteration 600: Loss = -11895.9857829662
Iteration 700: Loss = -11895.938987665599
Iteration 800: Loss = -11895.907674020273
Iteration 900: Loss = -11895.88545374856
Iteration 1000: Loss = -11895.869017377518
Iteration 1100: Loss = -11895.85656538593
Iteration 1200: Loss = -11895.846800046615
Iteration 1300: Loss = -11895.838998113197
Iteration 1400: Loss = -11895.832703842669
Iteration 1500: Loss = -11895.82753225599
Iteration 1600: Loss = -11895.82310946905
Iteration 1700: Loss = -11895.819456756613
Iteration 1800: Loss = -11895.816367411902
Iteration 1900: Loss = -11895.813677515496
Iteration 2000: Loss = -11895.811351980625
Iteration 2100: Loss = -11895.80928985098
Iteration 2200: Loss = -11895.807533716843
Iteration 2300: Loss = -11895.805995859828
Iteration 2400: Loss = -11895.804603451299
Iteration 2500: Loss = -11895.803352120416
Iteration 2600: Loss = -11895.802224804711
Iteration 2700: Loss = -11895.801204722471
Iteration 2800: Loss = -11895.800518979468
Iteration 2900: Loss = -11895.799485663765
Iteration 3000: Loss = -11895.798758877507
Iteration 3100: Loss = -11895.79821200792
Iteration 3200: Loss = -11895.797466431342
Iteration 3300: Loss = -11895.797147341955
Iteration 3400: Loss = -11895.796428937221
Iteration 3500: Loss = -11895.79585380203
Iteration 3600: Loss = -11895.796008395351
1
Iteration 3700: Loss = -11895.794995851145
Iteration 3800: Loss = -11895.794624255383
Iteration 3900: Loss = -11895.794450244419
Iteration 4000: Loss = -11895.79406439784
Iteration 4100: Loss = -11895.793697273777
Iteration 4200: Loss = -11895.793445337104
Iteration 4300: Loss = -11895.793208345603
Iteration 4400: Loss = -11895.795255160489
1
Iteration 4500: Loss = -11895.792717507506
Iteration 4600: Loss = -11895.793358143233
1
Iteration 4700: Loss = -11895.792391423094
Iteration 4800: Loss = -11895.792162209345
Iteration 4900: Loss = -11895.793422694294
1
Iteration 5000: Loss = -11895.795441289203
2
Iteration 5100: Loss = -11895.791917133458
Iteration 5200: Loss = -11895.791564607885
Iteration 5300: Loss = -11895.79283722177
1
Iteration 5400: Loss = -11895.79171984766
2
Iteration 5500: Loss = -11895.791350459433
Iteration 5600: Loss = -11895.791236793362
Iteration 5700: Loss = -11895.792881836334
1
Iteration 5800: Loss = -11895.791097988056
Iteration 5900: Loss = -11895.800792595746
1
Iteration 6000: Loss = -11895.791084529494
Iteration 6100: Loss = -11895.790928122642
Iteration 6200: Loss = -11895.790648535472
Iteration 6300: Loss = -11895.790815325121
1
Iteration 6400: Loss = -11895.790506603384
Iteration 6500: Loss = -11895.790436115776
Iteration 6600: Loss = -11895.790565886717
1
Iteration 6700: Loss = -11895.791351076243
2
Iteration 6800: Loss = -11895.79028444555
Iteration 6900: Loss = -11895.790477075032
1
Iteration 7000: Loss = -11895.792522425056
2
Iteration 7100: Loss = -11895.779277278929
Iteration 7200: Loss = -11895.77888931065
Iteration 7300: Loss = -11895.77887509512
Iteration 7400: Loss = -11895.780383366238
1
Iteration 7500: Loss = -11895.778751483123
Iteration 7600: Loss = -11895.778745874753
Iteration 7700: Loss = -11895.77900450781
1
Iteration 7800: Loss = -11895.778726594563
Iteration 7900: Loss = -11895.778870660439
1
Iteration 8000: Loss = -11895.827556018377
2
Iteration 8100: Loss = -11895.778623654518
Iteration 8200: Loss = -11895.778613945771
Iteration 8300: Loss = -11895.77879172709
1
Iteration 8400: Loss = -11895.778543163591
Iteration 8500: Loss = -11895.812531842012
1
Iteration 8600: Loss = -11895.77851331878
Iteration 8700: Loss = -11895.78068890841
1
Iteration 8800: Loss = -11895.778562888168
Iteration 8900: Loss = -11895.779513002637
1
Iteration 9000: Loss = -11895.782529572449
2
Iteration 9100: Loss = -11895.778478162914
Iteration 9200: Loss = -11895.790264306484
1
Iteration 9300: Loss = -11895.78623175698
2
Iteration 9400: Loss = -11895.786717606992
3
Iteration 9500: Loss = -11895.778414819973
Iteration 9600: Loss = -11895.778528563213
1
Iteration 9700: Loss = -11895.78379389395
2
Iteration 9800: Loss = -11895.800859442488
3
Iteration 9900: Loss = -11895.787548381899
4
Iteration 10000: Loss = -11895.78464511962
5
Iteration 10100: Loss = -11895.783367681408
6
Iteration 10200: Loss = -11895.927179458684
7
Iteration 10300: Loss = -11895.778346349778
Iteration 10400: Loss = -11895.779026900931
1
Iteration 10500: Loss = -11895.78025789669
2
Iteration 10600: Loss = -11895.779083998426
3
Iteration 10700: Loss = -11895.811204877482
4
Iteration 10800: Loss = -11895.79016979664
5
Iteration 10900: Loss = -11895.781631914677
6
Iteration 11000: Loss = -11895.78142675924
7
Iteration 11100: Loss = -11895.780345656798
8
Iteration 11200: Loss = -11895.784182269723
9
Iteration 11300: Loss = -11895.80140724849
10
Iteration 11400: Loss = -11895.778308621833
Iteration 11500: Loss = -11895.778587909885
1
Iteration 11600: Loss = -11895.78386196848
2
Iteration 11700: Loss = -11895.784412817553
3
Iteration 11800: Loss = -11895.779112332426
4
Iteration 11900: Loss = -11895.778225566675
Iteration 12000: Loss = -11895.77959140469
1
Iteration 12100: Loss = -11895.842135712648
2
Iteration 12200: Loss = -11895.782437429401
3
Iteration 12300: Loss = -11895.779575216508
4
Iteration 12400: Loss = -11895.781465187552
5
Iteration 12500: Loss = -11895.785472327143
6
Iteration 12600: Loss = -11895.778682135737
7
Iteration 12700: Loss = -11895.78230824585
8
Iteration 12800: Loss = -11895.842848891494
9
Iteration 12900: Loss = -11895.88898618826
10
Iteration 13000: Loss = -11895.790004344411
11
Iteration 13100: Loss = -11895.879357892256
12
Iteration 13200: Loss = -11895.780853704788
13
Iteration 13300: Loss = -11895.798582114365
14
Iteration 13400: Loss = -11895.778842500376
15
Stopping early at iteration 13400 due to no improvement.
pi: tensor([[0.7561, 0.2439],
        [0.2435, 0.7565]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5117, 0.4883], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3031, 0.1031],
         [0.6414, 0.2946]],

        [[0.5485, 0.1005],
         [0.5213, 0.5095]],

        [[0.6591, 0.1039],
         [0.5471, 0.7061]],

        [[0.6007, 0.1061],
         [0.7062, 0.6492]],

        [[0.5142, 0.0992],
         [0.5749, 0.5851]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.992
11899.890439037514
[0.0, 0.9919999775871758] [0.0, 0.992] [12383.282573203634, 11895.778842500376]
-------------------------------------
This iteration is 75
True Objective function: Loss = -11747.484373752
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22631.78535822519
Iteration 100: Loss = -12258.558298354967
Iteration 200: Loss = -12257.833822100429
Iteration 300: Loss = -12257.71461414561
Iteration 400: Loss = -12257.66057821979
Iteration 500: Loss = -12257.622761802044
Iteration 600: Loss = -12257.588470373357
Iteration 700: Loss = -12257.5527745956
Iteration 800: Loss = -12257.513805541388
Iteration 900: Loss = -12257.470891961297
Iteration 1000: Loss = -12257.421728910274
Iteration 1100: Loss = -12257.36138712594
Iteration 1200: Loss = -12257.283436834083
Iteration 1300: Loss = -12257.17223717804
Iteration 1400: Loss = -12257.006596127063
Iteration 1500: Loss = -12256.863970781074
Iteration 1600: Loss = -12256.777462189268
Iteration 1700: Loss = -12256.72404370184
Iteration 1800: Loss = -12256.69263548867
Iteration 1900: Loss = -12256.673040866008
Iteration 2000: Loss = -12256.659470626435
Iteration 2100: Loss = -12256.665621374805
1
Iteration 2200: Loss = -12256.636062920843
Iteration 2300: Loss = -12256.611480672918
Iteration 2400: Loss = -12256.512539634077
Iteration 2500: Loss = -12256.257702789091
Iteration 2600: Loss = -12256.144177742624
Iteration 2700: Loss = -12256.10905764356
Iteration 2800: Loss = -12256.093020057308
Iteration 2900: Loss = -12256.083005492066
Iteration 3000: Loss = -12256.074993694865
Iteration 3100: Loss = -12256.067162255356
Iteration 3200: Loss = -12256.058118967738
Iteration 3300: Loss = -12256.045229626341
Iteration 3400: Loss = -12256.021053887001
Iteration 3500: Loss = -12255.95278756455
Iteration 3600: Loss = -12255.739239343671
Iteration 3700: Loss = -12255.56182641867
Iteration 3800: Loss = -12255.501379619798
Iteration 3900: Loss = -12255.479128384068
Iteration 4000: Loss = -12255.468646751131
Iteration 4100: Loss = -12255.462672192063
Iteration 4200: Loss = -12255.458853603152
Iteration 4300: Loss = -12255.456207007095
Iteration 4400: Loss = -12255.454209464458
Iteration 4500: Loss = -12255.45269719548
Iteration 4600: Loss = -12255.45158515583
Iteration 4700: Loss = -12255.450548516086
Iteration 4800: Loss = -12255.449725582963
Iteration 4900: Loss = -12255.449078734331
Iteration 5000: Loss = -12255.448505502734
Iteration 5100: Loss = -12255.448017642295
Iteration 5200: Loss = -12255.447538585859
Iteration 5300: Loss = -12255.44715232102
Iteration 5400: Loss = -12255.446825656905
Iteration 5500: Loss = -12255.446486975858
Iteration 5600: Loss = -12255.446215526941
Iteration 5700: Loss = -12255.445977617428
Iteration 5800: Loss = -12255.44574871269
Iteration 5900: Loss = -12255.44556738227
Iteration 6000: Loss = -12255.445317642161
Iteration 6100: Loss = -12255.445166119644
Iteration 6200: Loss = -12255.444995517464
Iteration 6300: Loss = -12255.444823681868
Iteration 6400: Loss = -12255.444721609538
Iteration 6500: Loss = -12255.444814112983
Iteration 6600: Loss = -12255.444510144021
Iteration 6700: Loss = -12255.444376408286
Iteration 6800: Loss = -12255.444269884669
Iteration 6900: Loss = -12255.44453361668
1
Iteration 7000: Loss = -12255.444102305573
Iteration 7100: Loss = -12255.44408450197
Iteration 7200: Loss = -12255.443928340257
Iteration 7300: Loss = -12255.446262678059
1
Iteration 7400: Loss = -12255.443755609404
Iteration 7500: Loss = -12255.443947001224
1
Iteration 7600: Loss = -12255.443648460947
Iteration 7700: Loss = -12255.443607505713
Iteration 7800: Loss = -12255.443557271328
Iteration 7900: Loss = -12255.445857863531
1
Iteration 8000: Loss = -12255.443453574597
Iteration 8100: Loss = -12255.443397134672
Iteration 8200: Loss = -12255.44337116465
Iteration 8300: Loss = -12255.443327760262
Iteration 8400: Loss = -12255.447451191632
1
Iteration 8500: Loss = -12255.443251818699
Iteration 8600: Loss = -12255.443654404971
1
Iteration 8700: Loss = -12255.444349544676
2
Iteration 8800: Loss = -12255.444391520523
3
Iteration 8900: Loss = -12255.443901450584
4
Iteration 9000: Loss = -12255.444172744144
5
Iteration 9100: Loss = -12255.445183077947
6
Iteration 9200: Loss = -12255.443118781977
Iteration 9300: Loss = -12255.451169311284
1
Iteration 9400: Loss = -12255.443019664575
Iteration 9500: Loss = -12255.443063914128
Iteration 9600: Loss = -12255.442974167107
Iteration 9700: Loss = -12255.44722803585
1
Iteration 9800: Loss = -12255.442922219132
Iteration 9900: Loss = -12255.443888227002
1
Iteration 10000: Loss = -12255.442930878733
Iteration 10100: Loss = -12255.4428763463
Iteration 10200: Loss = -12255.522082555852
1
Iteration 10300: Loss = -12255.442854687224
Iteration 10400: Loss = -12255.644142502735
1
Iteration 10500: Loss = -12255.445468424376
2
Iteration 10600: Loss = -12255.442836156823
Iteration 10700: Loss = -12255.442902480083
Iteration 10800: Loss = -12255.442835744885
Iteration 10900: Loss = -12255.4536268178
1
Iteration 11000: Loss = -12255.442776216174
Iteration 11100: Loss = -12255.445607066185
1
Iteration 11200: Loss = -12255.51300225343
2
Iteration 11300: Loss = -12255.44291623252
3
Iteration 11400: Loss = -12255.452321055433
4
Iteration 11500: Loss = -12255.442773310824
Iteration 11600: Loss = -12255.443846814856
1
Iteration 11700: Loss = -12255.44270013923
Iteration 11800: Loss = -12255.442711088535
Iteration 11900: Loss = -12255.442695204089
Iteration 12000: Loss = -12255.502202087375
1
Iteration 12100: Loss = -12255.442695219908
Iteration 12200: Loss = -12255.44266598657
Iteration 12300: Loss = -12255.443542618317
1
Iteration 12400: Loss = -12255.442716622063
Iteration 12500: Loss = -12255.525797711081
1
Iteration 12600: Loss = -12255.442727049314
Iteration 12700: Loss = -12255.444305366867
1
Iteration 12800: Loss = -12255.46399698302
2
Iteration 12900: Loss = -12255.487923968754
3
Iteration 13000: Loss = -12255.445290582193
4
Iteration 13100: Loss = -12255.442660916324
Iteration 13200: Loss = -12255.44456797521
1
Iteration 13300: Loss = -12255.464098757242
2
Iteration 13400: Loss = -12255.444468529358
3
Iteration 13500: Loss = -12255.447078425887
4
Iteration 13600: Loss = -12255.44474017922
5
Iteration 13700: Loss = -12255.445284507645
6
Iteration 13800: Loss = -12255.442830357126
7
Iteration 13900: Loss = -12255.442652766496
Iteration 14000: Loss = -12255.444892204196
1
Iteration 14100: Loss = -12255.470670098046
2
Iteration 14200: Loss = -12255.44263731779
Iteration 14300: Loss = -12255.44748784108
1
Iteration 14400: Loss = -12255.442698809748
Iteration 14500: Loss = -12255.44305020743
1
Iteration 14600: Loss = -12255.444966490666
2
Iteration 14700: Loss = -12255.45970108673
3
Iteration 14800: Loss = -12255.548378451142
4
Iteration 14900: Loss = -12255.442629862318
Iteration 15000: Loss = -12255.4437139773
1
Iteration 15100: Loss = -12255.44697736661
2
Iteration 15200: Loss = -12255.539592987747
3
Iteration 15300: Loss = -12255.442595382861
Iteration 15400: Loss = -12255.46662827755
1
Iteration 15500: Loss = -12255.44257710585
Iteration 15600: Loss = -12255.442826574574
1
Iteration 15700: Loss = -12255.449290290095
2
Iteration 15800: Loss = -12255.442727908563
3
Iteration 15900: Loss = -12255.442888978749
4
Iteration 16000: Loss = -12255.442839073403
5
Iteration 16100: Loss = -12255.444552256922
6
Iteration 16200: Loss = -12255.444609782378
7
Iteration 16300: Loss = -12255.506686315195
8
Iteration 16400: Loss = -12255.443131780294
9
Iteration 16500: Loss = -12255.47103582557
10
Iteration 16600: Loss = -12255.450948111513
11
Iteration 16700: Loss = -12255.451103022298
12
Iteration 16800: Loss = -12255.443471581266
13
Iteration 16900: Loss = -12255.442689819589
14
Iteration 17000: Loss = -12255.541158201282
15
Stopping early at iteration 17000 due to no improvement.
pi: tensor([[9.9039e-08, 1.0000e+00],
        [1.0000e+00, 5.5797e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9770, 0.0230], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1953, 0.1637],
         [0.5323, 0.2019]],

        [[0.5596, 0.1736],
         [0.6395, 0.6570]],

        [[0.5390, 0.2099],
         [0.6517, 0.6004]],

        [[0.5578, 0.2226],
         [0.6567, 0.6008]],

        [[0.5929, 0.1178],
         [0.6571, 0.6409]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012951702941708292
Average Adjusted Rand Index: 0.0028778189149000063
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21575.409466987257
Iteration 100: Loss = -12202.409144794316
Iteration 200: Loss = -12201.679314351668
Iteration 300: Loss = -12201.543620162864
Iteration 400: Loss = -12201.481178743534
Iteration 500: Loss = -12201.445983681642
Iteration 600: Loss = -12201.423712271393
Iteration 700: Loss = -12201.408605123124
Iteration 800: Loss = -12201.397708599445
Iteration 900: Loss = -12201.389482803861
Iteration 1000: Loss = -12201.382961209498
Iteration 1100: Loss = -12201.377610475482
Iteration 1200: Loss = -12201.372918393305
Iteration 1300: Loss = -12201.368780303172
Iteration 1400: Loss = -12201.364705023678
Iteration 1500: Loss = -12201.360472341416
Iteration 1600: Loss = -12201.35559472979
Iteration 1700: Loss = -12201.349596698221
Iteration 1800: Loss = -12201.341774902672
Iteration 1900: Loss = -12201.33349962245
Iteration 2000: Loss = -12201.324376055967
Iteration 2100: Loss = -12201.30963388566
Iteration 2200: Loss = -12201.276963592507
Iteration 2300: Loss = -12201.109207079682
Iteration 2400: Loss = -11749.996123215516
Iteration 2500: Loss = -11749.50004172745
Iteration 2600: Loss = -11742.717633526827
Iteration 2700: Loss = -11742.70566691156
Iteration 2800: Loss = -11742.699886549362
Iteration 2900: Loss = -11742.696120000173
Iteration 3000: Loss = -11742.693460078506
Iteration 3100: Loss = -11742.691466651855
Iteration 3200: Loss = -11742.68902828769
Iteration 3300: Loss = -11742.685304391118
Iteration 3400: Loss = -11742.684196942628
Iteration 3500: Loss = -11742.683363188362
Iteration 3600: Loss = -11742.682722626909
Iteration 3700: Loss = -11742.682169042084
Iteration 3800: Loss = -11742.692051165135
1
Iteration 3900: Loss = -11742.681221487039
Iteration 4000: Loss = -11742.68084434232
Iteration 4100: Loss = -11742.680694692617
Iteration 4200: Loss = -11742.680178557473
Iteration 4300: Loss = -11742.681735818536
1
Iteration 4400: Loss = -11742.679670318845
Iteration 4500: Loss = -11742.679384857161
Iteration 4600: Loss = -11742.67918116325
Iteration 4700: Loss = -11742.680984671777
1
Iteration 4800: Loss = -11742.685135534912
2
Iteration 4900: Loss = -11742.678314155246
Iteration 5000: Loss = -11742.678092491098
Iteration 5100: Loss = -11742.67782258656
Iteration 5200: Loss = -11742.677605330593
Iteration 5300: Loss = -11742.677383450078
Iteration 5400: Loss = -11742.677219964586
Iteration 5500: Loss = -11742.677072852151
Iteration 5600: Loss = -11742.676080657644
Iteration 5700: Loss = -11742.611519630113
Iteration 5800: Loss = -11742.61119794195
Iteration 5900: Loss = -11742.611042152923
Iteration 6000: Loss = -11742.61087604931
Iteration 6100: Loss = -11742.610391587457
Iteration 6200: Loss = -11742.615450812384
1
Iteration 6300: Loss = -11742.610005175386
Iteration 6400: Loss = -11742.609819221021
Iteration 6500: Loss = -11742.610108493833
1
Iteration 6600: Loss = -11742.60964901383
Iteration 6700: Loss = -11742.609714949547
Iteration 6800: Loss = -11742.610019349242
1
Iteration 6900: Loss = -11742.608017124401
Iteration 7000: Loss = -11742.603162743148
Iteration 7100: Loss = -11742.595133963707
Iteration 7200: Loss = -11742.597128223095
1
Iteration 7300: Loss = -11742.593493123743
Iteration 7400: Loss = -11742.465835910725
Iteration 7500: Loss = -11742.43850394384
Iteration 7600: Loss = -11742.480718686558
1
Iteration 7700: Loss = -11742.489692377734
2
Iteration 7800: Loss = -11742.438411811745
Iteration 7900: Loss = -11742.438362821385
Iteration 8000: Loss = -11742.438075852675
Iteration 8100: Loss = -11742.439031105452
1
Iteration 8200: Loss = -11742.460554576584
2
Iteration 8300: Loss = -11742.43695811668
Iteration 8400: Loss = -11742.436138728397
Iteration 8500: Loss = -11742.435990039792
Iteration 8600: Loss = -11742.438216803825
1
Iteration 8700: Loss = -11742.451657324866
2
Iteration 8800: Loss = -11742.435815355062
Iteration 8900: Loss = -11742.431984580297
Iteration 9000: Loss = -11742.494011354993
1
Iteration 9100: Loss = -11742.433231780493
2
Iteration 9200: Loss = -11742.432631949188
3
Iteration 9300: Loss = -11742.432594619944
4
Iteration 9400: Loss = -11742.4332470047
5
Iteration 9500: Loss = -11742.446463445678
6
Iteration 9600: Loss = -11742.447128865679
7
Iteration 9700: Loss = -11742.44652322657
8
Iteration 9800: Loss = -11742.433438829781
9
Iteration 9900: Loss = -11742.430575303057
Iteration 10000: Loss = -11742.43265108518
1
Iteration 10100: Loss = -11742.45341789853
2
Iteration 10200: Loss = -11742.431945252723
3
Iteration 10300: Loss = -11742.431050572883
4
Iteration 10400: Loss = -11742.439394362033
5
Iteration 10500: Loss = -11742.435926327524
6
Iteration 10600: Loss = -11742.444624879672
7
Iteration 10700: Loss = -11742.433789698662
8
Iteration 10800: Loss = -11742.429227818166
Iteration 10900: Loss = -11742.430146250883
1
Iteration 11000: Loss = -11742.461419717532
2
Iteration 11100: Loss = -11742.42776236063
Iteration 11200: Loss = -11742.427393532276
Iteration 11300: Loss = -11742.426202640678
Iteration 11400: Loss = -11742.428285688993
1
Iteration 11500: Loss = -11742.43616049125
2
Iteration 11600: Loss = -11742.431148896701
3
Iteration 11700: Loss = -11742.437102482485
4
Iteration 11800: Loss = -11742.428353991432
5
Iteration 11900: Loss = -11742.428007784638
6
Iteration 12000: Loss = -11742.446751038742
7
Iteration 12100: Loss = -11742.432933534565
8
Iteration 12200: Loss = -11742.426012840893
Iteration 12300: Loss = -11742.515930638974
1
Iteration 12400: Loss = -11742.426039779719
Iteration 12500: Loss = -11742.426712357825
1
Iteration 12600: Loss = -11742.435719489093
2
Iteration 12700: Loss = -11742.46738833167
3
Iteration 12800: Loss = -11742.540928400656
4
Iteration 12900: Loss = -11742.426065458962
Iteration 13000: Loss = -11742.42810543668
1
Iteration 13100: Loss = -11742.426487222367
2
Iteration 13200: Loss = -11742.428007636972
3
Iteration 13300: Loss = -11742.42684611457
4
Iteration 13400: Loss = -11742.426774010411
5
Iteration 13500: Loss = -11742.429304802134
6
Iteration 13600: Loss = -11742.440642239499
7
Iteration 13700: Loss = -11742.437199545391
8
Iteration 13800: Loss = -11742.428599269162
9
Iteration 13900: Loss = -11742.43111003837
10
Iteration 14000: Loss = -11742.447620807876
11
Iteration 14100: Loss = -11742.425793444667
Iteration 14200: Loss = -11742.432470304477
1
Iteration 14300: Loss = -11742.439394359752
2
Iteration 14400: Loss = -11742.427289373762
3
Iteration 14500: Loss = -11742.427002116852
4
Iteration 14600: Loss = -11742.427976815672
5
Iteration 14700: Loss = -11742.417100628207
Iteration 14800: Loss = -11742.415733000727
Iteration 14900: Loss = -11742.417622373974
1
Iteration 15000: Loss = -11742.43011215598
2
Iteration 15100: Loss = -11742.413809703974
Iteration 15200: Loss = -11742.435615320217
1
Iteration 15300: Loss = -11742.413222963454
Iteration 15400: Loss = -11742.41677822455
1
Iteration 15500: Loss = -11742.420497865827
2
Iteration 15600: Loss = -11742.419440162426
3
Iteration 15700: Loss = -11742.424858249542
4
Iteration 15800: Loss = -11742.447113378323
5
Iteration 15900: Loss = -11742.416991570688
6
Iteration 16000: Loss = -11742.4126722828
Iteration 16100: Loss = -11742.418982087833
1
Iteration 16200: Loss = -11742.419319954844
2
Iteration 16300: Loss = -11742.398934298752
Iteration 16400: Loss = -11742.399087430545
1
Iteration 16500: Loss = -11742.397856996657
Iteration 16600: Loss = -11742.39727278395
Iteration 16700: Loss = -11742.426997538038
1
Iteration 16800: Loss = -11742.397473551218
2
Iteration 16900: Loss = -11742.397627300943
3
Iteration 17000: Loss = -11742.397168029274
Iteration 17100: Loss = -11742.396986869935
Iteration 17200: Loss = -11742.398320721855
1
Iteration 17300: Loss = -11742.400420176416
2
Iteration 17400: Loss = -11742.399302183616
3
Iteration 17500: Loss = -11742.416379716815
4
Iteration 17600: Loss = -11742.401910029226
5
Iteration 17700: Loss = -11742.40235762512
6
Iteration 17800: Loss = -11742.397539322856
7
Iteration 17900: Loss = -11742.398352015936
8
Iteration 18000: Loss = -11742.404574594684
9
Iteration 18100: Loss = -11742.401726250879
10
Iteration 18200: Loss = -11742.396901065911
Iteration 18300: Loss = -11742.405877462845
1
Iteration 18400: Loss = -11742.39582045501
Iteration 18500: Loss = -11742.39724535955
1
Iteration 18600: Loss = -11742.3960635932
2
Iteration 18700: Loss = -11742.39869856981
3
Iteration 18800: Loss = -11742.412087474882
4
Iteration 18900: Loss = -11742.402126284116
5
Iteration 19000: Loss = -11742.394413380394
Iteration 19100: Loss = -11742.39444130985
Iteration 19200: Loss = -11742.410740634003
1
Iteration 19300: Loss = -11742.397155389277
2
Iteration 19400: Loss = -11742.401159337162
3
Iteration 19500: Loss = -11742.393910136221
Iteration 19600: Loss = -11742.411309735075
1
Iteration 19700: Loss = -11742.396576045969
2
Iteration 19800: Loss = -11742.394014721804
3
Iteration 19900: Loss = -11742.395549027126
4
pi: tensor([[0.7776, 0.2224],
        [0.2178, 0.7822]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5806, 0.4194], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2959, 0.1003],
         [0.6367, 0.2943]],

        [[0.7115, 0.0922],
         [0.6817, 0.6048]],

        [[0.5610, 0.1022],
         [0.5787, 0.6377]],

        [[0.7023, 0.1013],
         [0.5284, 0.6293]],

        [[0.6339, 0.0949],
         [0.5044, 0.6798]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11747.484373752
[-0.0012951702941708292, 1.0] [0.0028778189149000063, 1.0] [12255.541158201282, 11742.39895918322]
-------------------------------------
This iteration is 76
True Objective function: Loss = -11701.221350058191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19648.365786903883
Iteration 100: Loss = -12201.692637531594
Iteration 200: Loss = -12201.323999341379
Iteration 300: Loss = -12201.203315269231
Iteration 400: Loss = -12201.141076766522
Iteration 500: Loss = -12201.097340730388
Iteration 600: Loss = -12201.056133498587
Iteration 700: Loss = -12201.007989742411
Iteration 800: Loss = -12200.945312835043
Iteration 900: Loss = -12200.859481120246
Iteration 1000: Loss = -12200.74016596148
Iteration 1100: Loss = -12200.581029842684
Iteration 1200: Loss = -12200.38094252771
Iteration 1300: Loss = -12200.167199256402
Iteration 1400: Loss = -12199.981253577964
Iteration 1500: Loss = -12199.809097521791
Iteration 1600: Loss = -12199.663116046051
Iteration 1700: Loss = -12199.553559631337
Iteration 1800: Loss = -12199.474053526155
Iteration 1900: Loss = -12199.41815104951
Iteration 2000: Loss = -12199.37886498398
Iteration 2100: Loss = -12199.34969156577
Iteration 2200: Loss = -12199.328706893533
Iteration 2300: Loss = -12199.313855201648
Iteration 2400: Loss = -12199.303282247736
Iteration 2500: Loss = -12199.295856015982
Iteration 2600: Loss = -12199.290374274677
Iteration 2700: Loss = -12199.286639034495
Iteration 2800: Loss = -12199.28358424203
Iteration 2900: Loss = -12199.2814730709
Iteration 3000: Loss = -12199.279851671487
Iteration 3100: Loss = -12199.278633905871
Iteration 3200: Loss = -12199.278187971055
Iteration 3300: Loss = -12199.27700355859
Iteration 3400: Loss = -12199.276360961343
Iteration 3500: Loss = -12199.27602478926
Iteration 3600: Loss = -12199.275649188246
Iteration 3700: Loss = -12199.276414956372
1
Iteration 3800: Loss = -12199.275122641066
Iteration 3900: Loss = -12199.27613956389
1
Iteration 4000: Loss = -12199.27475936839
Iteration 4100: Loss = -12199.274645279835
Iteration 4200: Loss = -12199.274507734686
Iteration 4300: Loss = -12199.2744377345
Iteration 4400: Loss = -12199.274497594964
Iteration 4500: Loss = -12199.274282457867
Iteration 4600: Loss = -12199.2797150575
1
Iteration 4700: Loss = -12199.274138807892
Iteration 4800: Loss = -12199.276692671952
1
Iteration 4900: Loss = -12199.2740348001
Iteration 5000: Loss = -12199.280634163717
1
Iteration 5100: Loss = -12199.273944864177
Iteration 5200: Loss = -12199.273967473297
Iteration 5300: Loss = -12199.273882581283
Iteration 5400: Loss = -12199.274457454281
1
Iteration 5500: Loss = -12199.273768379224
Iteration 5600: Loss = -12199.274458024653
1
Iteration 5700: Loss = -12199.273634944882
Iteration 5800: Loss = -12199.27378107477
1
Iteration 5900: Loss = -12199.273269499134
Iteration 6000: Loss = -12199.277045183246
1
Iteration 6100: Loss = -12199.269717111756
Iteration 6200: Loss = -12198.89129734525
Iteration 6300: Loss = -12196.950193519919
Iteration 6400: Loss = -12196.469724002905
Iteration 6500: Loss = -12196.432733665984
Iteration 6600: Loss = -12196.421806758191
Iteration 6700: Loss = -12196.416026981959
Iteration 6800: Loss = -12196.409821771034
Iteration 6900: Loss = -12196.316290589144
Iteration 7000: Loss = -12196.302847258175
Iteration 7100: Loss = -12196.332146487226
1
Iteration 7200: Loss = -12196.298056938736
Iteration 7300: Loss = -12196.29693227153
Iteration 7400: Loss = -12196.296018350218
Iteration 7500: Loss = -12196.295396496113
Iteration 7600: Loss = -12196.294770240087
Iteration 7700: Loss = -12196.294273539937
Iteration 7800: Loss = -12196.29391061672
Iteration 7900: Loss = -12196.293512296817
Iteration 8000: Loss = -12196.293247674706
Iteration 8100: Loss = -12196.292972841322
Iteration 8200: Loss = -12196.298501420853
1
Iteration 8300: Loss = -12196.292507292977
Iteration 8400: Loss = -12196.292276015056
Iteration 8500: Loss = -12196.295772659487
1
Iteration 8600: Loss = -12196.29205019966
Iteration 8700: Loss = -12196.291845617365
Iteration 8800: Loss = -12196.291721347643
Iteration 8900: Loss = -12196.298620946976
1
Iteration 9000: Loss = -12196.29150507901
Iteration 9100: Loss = -12196.291440673156
Iteration 9200: Loss = -12196.291369679331
Iteration 9300: Loss = -12196.291704461353
1
Iteration 9400: Loss = -12196.291188250316
Iteration 9500: Loss = -12196.291144502098
Iteration 9600: Loss = -12196.302320376164
1
Iteration 9700: Loss = -12196.29103255887
Iteration 9800: Loss = -12196.290916276195
Iteration 9900: Loss = -12196.290873928767
Iteration 10000: Loss = -12196.329501392334
1
Iteration 10100: Loss = -12196.290769855708
Iteration 10200: Loss = -12196.29075804056
Iteration 10300: Loss = -12196.290710725183
Iteration 10400: Loss = -12196.291148138256
1
Iteration 10500: Loss = -12196.290633194312
Iteration 10600: Loss = -12196.29077856095
1
Iteration 10700: Loss = -12196.623920235896
2
Iteration 10800: Loss = -12196.29054169899
Iteration 10900: Loss = -12196.290505118537
Iteration 11000: Loss = -12196.361015132055
1
Iteration 11100: Loss = -12196.290482388365
Iteration 11200: Loss = -12196.290425631469
Iteration 11300: Loss = -12196.29065249333
1
Iteration 11400: Loss = -12196.29064467145
2
Iteration 11500: Loss = -12196.29035056014
Iteration 11600: Loss = -12196.290363598337
Iteration 11700: Loss = -12196.290398164074
Iteration 11800: Loss = -12196.290477828481
Iteration 11900: Loss = -12196.290281260322
Iteration 12000: Loss = -12196.290947460788
1
Iteration 12100: Loss = -12196.290269435829
Iteration 12200: Loss = -12196.290201688113
Iteration 12300: Loss = -12196.29047005316
1
Iteration 12400: Loss = -12196.290169925158
Iteration 12500: Loss = -12196.290170560227
Iteration 12600: Loss = -12196.29014956185
Iteration 12700: Loss = -12196.34248910496
1
Iteration 12800: Loss = -12196.290178448218
Iteration 12900: Loss = -12196.29579828298
1
Iteration 13000: Loss = -12196.290111760343
Iteration 13100: Loss = -12196.67539484069
1
Iteration 13200: Loss = -12196.290153552985
Iteration 13300: Loss = -12196.290236873105
Iteration 13400: Loss = -12196.290116825669
Iteration 13500: Loss = -12196.316770150224
1
Iteration 13600: Loss = -12196.29079849383
2
Iteration 13700: Loss = -12196.290146093417
Iteration 13800: Loss = -12196.291020798933
1
Iteration 13900: Loss = -12196.295096286773
2
Iteration 14000: Loss = -12196.290081194957
Iteration 14100: Loss = -12196.417621164162
1
Iteration 14200: Loss = -12196.290059197718
Iteration 14300: Loss = -12196.299005652767
1
Iteration 14400: Loss = -12196.291474132191
2
Iteration 14500: Loss = -12196.290803949352
3
Iteration 14600: Loss = -12196.290129068286
Iteration 14700: Loss = -12196.53081961691
1
Iteration 14800: Loss = -12196.289990332545
Iteration 14900: Loss = -12196.30202752804
1
Iteration 15000: Loss = -12196.290563521989
2
Iteration 15100: Loss = -12196.290330281807
3
Iteration 15200: Loss = -12196.34531263449
4
Iteration 15300: Loss = -12196.291589529514
5
Iteration 15400: Loss = -12196.290604903408
6
Iteration 15500: Loss = -12196.290034379172
Iteration 15600: Loss = -12196.29124446886
1
Iteration 15700: Loss = -12196.294021051752
2
Iteration 15800: Loss = -12196.290096429837
Iteration 15900: Loss = -12196.456670433605
1
Iteration 16000: Loss = -12196.290461095117
2
Iteration 16100: Loss = -12196.294141797058
3
Iteration 16200: Loss = -12196.290049195462
Iteration 16300: Loss = -12196.290088585889
Iteration 16400: Loss = -12196.290543232242
1
Iteration 16500: Loss = -12196.290094889873
Iteration 16600: Loss = -12196.299880053544
1
Iteration 16700: Loss = -12196.290922294156
2
Iteration 16800: Loss = -12196.292501435513
3
Iteration 16900: Loss = -12196.290030948667
Iteration 17000: Loss = -12196.2913741969
1
Iteration 17100: Loss = -12196.29004430574
Iteration 17200: Loss = -12196.290231132232
1
Iteration 17300: Loss = -12196.290022932531
Iteration 17400: Loss = -12196.290381694836
1
Iteration 17500: Loss = -12196.290072675229
Iteration 17600: Loss = -12196.29163818361
1
Iteration 17700: Loss = -12196.290008071332
Iteration 17800: Loss = -12196.509762614967
1
Iteration 17900: Loss = -12196.290008606215
Iteration 18000: Loss = -12196.290004723991
Iteration 18100: Loss = -12196.291233544238
1
Iteration 18200: Loss = -12196.29021967995
2
Iteration 18300: Loss = -12196.322063361227
3
Iteration 18400: Loss = -12196.292405469849
4
Iteration 18500: Loss = -12196.291612997613
5
Iteration 18600: Loss = -12196.290353819846
6
Iteration 18700: Loss = -12196.475074637721
7
Iteration 18800: Loss = -12196.294532937523
8
Iteration 18900: Loss = -12196.351249388561
9
Iteration 19000: Loss = -12196.290441172669
10
Iteration 19100: Loss = -12196.290101426102
Iteration 19200: Loss = -12196.290357038768
1
Iteration 19300: Loss = -12196.290227023723
2
Iteration 19400: Loss = -12196.290392869505
3
Iteration 19500: Loss = -12196.290086027078
Iteration 19600: Loss = -12196.290280237099
1
Iteration 19700: Loss = -12196.293482592968
2
Iteration 19800: Loss = -12196.290036408183
Iteration 19900: Loss = -12196.296148366107
1
pi: tensor([[8.4099e-01, 1.5901e-01],
        [8.3352e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0223, 0.9777], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5656, 0.1359],
         [0.5939, 0.1962]],

        [[0.7210, 0.2440],
         [0.5451, 0.5311]],

        [[0.6210, 0.0920],
         [0.6612, 0.6694]],

        [[0.6563, 0.1418],
         [0.6640, 0.5933]],

        [[0.6877, 0.1266],
         [0.5513, 0.6470]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.002942569065291439
Average Adjusted Rand Index: 0.002863854581165372
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22710.19934844396
Iteration 100: Loss = -12203.01154999276
Iteration 200: Loss = -12201.881742522846
Iteration 300: Loss = -12201.572008000532
Iteration 400: Loss = -12201.41326721019
Iteration 500: Loss = -12201.316681531445
Iteration 600: Loss = -12201.247570287649
Iteration 700: Loss = -12201.189129324011
Iteration 800: Loss = -12201.129313594398
Iteration 900: Loss = -12201.049716437767
Iteration 1000: Loss = -12200.899616078594
Iteration 1100: Loss = -12200.599012573008
Iteration 1200: Loss = -12200.275353837875
Iteration 1300: Loss = -12200.06029522679
Iteration 1400: Loss = -12199.918591952373
Iteration 1500: Loss = -12199.818986705508
Iteration 1600: Loss = -12199.733445318185
Iteration 1700: Loss = -12199.627820003396
Iteration 1800: Loss = -12199.371930904897
Iteration 1900: Loss = -12198.685408196148
Iteration 2000: Loss = -12197.960203370405
Iteration 2100: Loss = -12197.560908038697
Iteration 2200: Loss = -12197.364377726604
Iteration 2300: Loss = -12197.252375975826
Iteration 2400: Loss = -12197.169011931313
Iteration 2500: Loss = -12197.105211333774
Iteration 2600: Loss = -12197.052286442582
Iteration 2700: Loss = -12197.011927903608
Iteration 2800: Loss = -12196.97810697834
Iteration 2900: Loss = -12196.946253730017
Iteration 3000: Loss = -12196.912593725703
Iteration 3100: Loss = -12196.867924748305
Iteration 3200: Loss = -12196.79321316739
Iteration 3300: Loss = -12196.699347416985
Iteration 3400: Loss = -12196.617889474803
Iteration 3500: Loss = -12196.556828866233
Iteration 3600: Loss = -12196.51385051448
Iteration 3700: Loss = -12196.484613894925
Iteration 3800: Loss = -12196.464960426936
Iteration 3900: Loss = -12196.451628257475
Iteration 4000: Loss = -12196.442382737323
Iteration 4100: Loss = -12196.435721365662
Iteration 4200: Loss = -12196.430725298014
Iteration 4300: Loss = -12196.426861217531
Iteration 4400: Loss = -12196.423743986988
Iteration 4500: Loss = -12196.4211414839
Iteration 4600: Loss = -12196.41892890331
Iteration 4700: Loss = -12196.417060145935
Iteration 4800: Loss = -12196.415357541728
Iteration 4900: Loss = -12196.413831304893
Iteration 5000: Loss = -12196.412478845416
Iteration 5100: Loss = -12196.411029901567
Iteration 5200: Loss = -12196.409602171585
Iteration 5300: Loss = -12196.407549998019
Iteration 5400: Loss = -12196.403085023154
Iteration 5500: Loss = -12196.37870628841
Iteration 5600: Loss = -12196.331301813025
Iteration 5700: Loss = -12196.312039513012
Iteration 5800: Loss = -12196.304838136402
Iteration 5900: Loss = -12196.301781496011
Iteration 6000: Loss = -12196.300106434148
Iteration 6100: Loss = -12196.299021237206
Iteration 6200: Loss = -12196.298171052571
Iteration 6300: Loss = -12196.297435324108
Iteration 6400: Loss = -12196.296789307651
Iteration 6500: Loss = -12196.296263215232
Iteration 6600: Loss = -12196.29580743249
Iteration 6700: Loss = -12196.295386080086
Iteration 6800: Loss = -12196.294975767805
Iteration 6900: Loss = -12196.294681110356
Iteration 7000: Loss = -12196.294366230792
Iteration 7100: Loss = -12196.294057381521
Iteration 7200: Loss = -12196.293827944679
Iteration 7300: Loss = -12196.293584735979
Iteration 7400: Loss = -12196.29333661429
Iteration 7500: Loss = -12196.293113185659
Iteration 7600: Loss = -12196.29313444948
Iteration 7700: Loss = -12196.292794454905
Iteration 7800: Loss = -12196.292589991212
Iteration 7900: Loss = -12196.49149598404
1
Iteration 8000: Loss = -12196.292314715305
Iteration 8100: Loss = -12196.292159621365
Iteration 8200: Loss = -12196.292066387048
Iteration 8300: Loss = -12196.29899124105
1
Iteration 8400: Loss = -12196.291813589105
Iteration 8500: Loss = -12196.291694640178
Iteration 8600: Loss = -12196.291616301798
Iteration 8700: Loss = -12196.291886947167
1
Iteration 8800: Loss = -12196.291411430024
Iteration 8900: Loss = -12196.291368603737
Iteration 9000: Loss = -12196.317684229036
1
Iteration 9100: Loss = -12196.291210488396
Iteration 9200: Loss = -12196.291152502574
Iteration 9300: Loss = -12196.291074294453
Iteration 9400: Loss = -12196.392478818949
1
Iteration 9500: Loss = -12196.290977072798
Iteration 9600: Loss = -12196.290933201914
Iteration 9700: Loss = -12196.290934760713
Iteration 9800: Loss = -12196.290856125128
Iteration 9900: Loss = -12196.29080338535
Iteration 10000: Loss = -12196.291445907213
1
Iteration 10100: Loss = -12196.29073011709
Iteration 10200: Loss = -12196.29068744473
Iteration 10300: Loss = -12196.290637904489
Iteration 10400: Loss = -12196.292208659821
1
Iteration 10500: Loss = -12196.290572476057
Iteration 10600: Loss = -12196.290553292809
Iteration 10700: Loss = -12196.3119472354
1
Iteration 10800: Loss = -12196.290501354873
Iteration 10900: Loss = -12196.290665622324
1
Iteration 11000: Loss = -12196.290638181106
2
Iteration 11100: Loss = -12196.290517219448
Iteration 11200: Loss = -12196.314405785015
1
Iteration 11300: Loss = -12196.290562301134
Iteration 11400: Loss = -12196.290342812867
Iteration 11500: Loss = -12196.2903049461
Iteration 11600: Loss = -12196.290317356103
Iteration 11700: Loss = -12196.29028883626
Iteration 11800: Loss = -12196.29036003041
Iteration 11900: Loss = -12196.290291840207
Iteration 12000: Loss = -12196.29026301142
Iteration 12100: Loss = -12196.290341925967
Iteration 12200: Loss = -12196.290245468808
Iteration 12300: Loss = -12196.290254983205
Iteration 12400: Loss = -12196.290268811563
Iteration 12500: Loss = -12196.290159899962
Iteration 12600: Loss = -12196.290531344914
1
Iteration 12700: Loss = -12196.290172942297
Iteration 12800: Loss = -12196.290137297337
Iteration 12900: Loss = -12196.290135959189
Iteration 13000: Loss = -12196.290508221882
1
Iteration 13100: Loss = -12196.290132458349
Iteration 13200: Loss = -12196.294161359905
1
Iteration 13300: Loss = -12196.290102827103
Iteration 13400: Loss = -12196.290323514295
1
Iteration 13500: Loss = -12196.291428359737
2
Iteration 13600: Loss = -12196.304471341198
3
Iteration 13700: Loss = -12196.29011649192
Iteration 13800: Loss = -12196.346944117948
1
Iteration 13900: Loss = -12196.291742863918
2
Iteration 14000: Loss = -12196.291236360363
3
Iteration 14100: Loss = -12196.290210103896
Iteration 14200: Loss = -12196.290463291427
1
Iteration 14300: Loss = -12196.290059599512
Iteration 14400: Loss = -12196.290043627369
Iteration 14500: Loss = -12196.290581638837
1
Iteration 14600: Loss = -12196.29069736655
2
Iteration 14700: Loss = -12196.295797364
3
Iteration 14800: Loss = -12196.290272315793
4
Iteration 14900: Loss = -12196.718340399326
5
Iteration 15000: Loss = -12196.289989126491
Iteration 15100: Loss = -12196.290023456255
Iteration 15200: Loss = -12196.290092424339
Iteration 15300: Loss = -12196.290013254895
Iteration 15400: Loss = -12196.293773473733
1
Iteration 15500: Loss = -12196.29002808315
Iteration 15600: Loss = -12196.47950668626
1
Iteration 15700: Loss = -12196.292893305455
2
Iteration 15800: Loss = -12196.290166351686
3
Iteration 15900: Loss = -12196.290052871134
Iteration 16000: Loss = -12196.29038553796
1
Iteration 16100: Loss = -12196.295367634875
2
Iteration 16200: Loss = -12196.29003717432
Iteration 16300: Loss = -12196.296559534823
1
Iteration 16400: Loss = -12196.290053691955
Iteration 16500: Loss = -12196.290009069917
Iteration 16600: Loss = -12196.291647714143
1
Iteration 16700: Loss = -12196.289981934997
Iteration 16800: Loss = -12196.297048250452
1
Iteration 16900: Loss = -12196.290163293617
2
Iteration 17000: Loss = -12196.290586568337
3
Iteration 17100: Loss = -12196.290016757986
Iteration 17200: Loss = -12196.323494478032
1
Iteration 17300: Loss = -12196.290109302852
Iteration 17400: Loss = -12196.290212241709
1
Iteration 17500: Loss = -12196.29003469316
Iteration 17600: Loss = -12196.29146000844
1
Iteration 17700: Loss = -12196.28997406795
Iteration 17800: Loss = -12196.29013919978
1
Iteration 17900: Loss = -12196.290023696913
Iteration 18000: Loss = -12196.290393894627
1
Iteration 18100: Loss = -12196.290277306072
2
Iteration 18200: Loss = -12196.290134516343
3
Iteration 18300: Loss = -12196.50398011726
4
Iteration 18400: Loss = -12196.29108220554
5
Iteration 18500: Loss = -12196.29056605478
6
Iteration 18600: Loss = -12196.290418011638
7
Iteration 18700: Loss = -12196.29011544062
Iteration 18800: Loss = -12196.510395377209
1
Iteration 18900: Loss = -12196.290045151294
Iteration 19000: Loss = -12196.506097502363
1
Iteration 19100: Loss = -12196.290050003812
Iteration 19200: Loss = -12196.299952814566
1
Iteration 19300: Loss = -12196.296192116564
2
Iteration 19400: Loss = -12196.290144717668
Iteration 19500: Loss = -12196.407566113665
1
Iteration 19600: Loss = -12196.290403120824
2
Iteration 19700: Loss = -12196.301008083023
3
Iteration 19800: Loss = -12196.2900852287
Iteration 19900: Loss = -12196.295477752305
1
pi: tensor([[1.0000e+00, 1.0419e-08],
        [1.5944e-01, 8.4056e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9777, 0.0223], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.1360],
         [0.5740, 0.5658]],

        [[0.6565, 0.2443],
         [0.6906, 0.5301]],

        [[0.6419, 0.0919],
         [0.7228, 0.6510]],

        [[0.6093, 0.1415],
         [0.6831, 0.7174]],

        [[0.6488, 0.1264],
         [0.6396, 0.5656]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.002942569065291439
Average Adjusted Rand Index: 0.002863854581165372
11701.221350058191
[0.002942569065291439, 0.002942569065291439] [0.002863854581165372, 0.002863854581165372] [12196.290022093732, 12196.290292575011]
-------------------------------------
This iteration is 77
True Objective function: Loss = -11777.871904873846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20258.64595776764
Iteration 100: Loss = -12259.584603535188
Iteration 200: Loss = -12259.1397062424
Iteration 300: Loss = -12258.977103825486
Iteration 400: Loss = -12258.858149323754
Iteration 500: Loss = -12258.770822732335
Iteration 600: Loss = -12258.714431042
Iteration 700: Loss = -12258.675231845573
Iteration 800: Loss = -12258.634004082893
Iteration 900: Loss = -12258.55574049958
Iteration 1000: Loss = -12258.3814275184
Iteration 1100: Loss = -12258.210824982394
Iteration 1200: Loss = -12258.04235168407
Iteration 1300: Loss = -12257.888856827863
Iteration 1400: Loss = -12257.758766691179
Iteration 1500: Loss = -12257.611627089604
Iteration 1600: Loss = -12257.387503416172
Iteration 1700: Loss = -12256.804145648033
Iteration 1800: Loss = -12255.533985454585
Iteration 1900: Loss = -12082.8687592921
Iteration 2000: Loss = -12008.216729762518
Iteration 2100: Loss = -11963.477850350675
Iteration 2200: Loss = -11927.78674154918
Iteration 2300: Loss = -11904.464864214868
Iteration 2400: Loss = -11881.409666000533
Iteration 2500: Loss = -11881.399278619796
Iteration 2600: Loss = -11876.788153132004
Iteration 2700: Loss = -11876.761498837777
Iteration 2800: Loss = -11876.757067545237
Iteration 2900: Loss = -11876.753330102936
Iteration 3000: Loss = -11876.748650474628
Iteration 3100: Loss = -11868.143755753466
Iteration 3200: Loss = -11867.67243664992
Iteration 3300: Loss = -11867.669225245509
Iteration 3400: Loss = -11865.000548584792
Iteration 3500: Loss = -11864.983371137745
Iteration 3600: Loss = -11859.158220462945
Iteration 3700: Loss = -11858.9870049169
Iteration 3800: Loss = -11858.902539558532
Iteration 3900: Loss = -11848.414250740658
Iteration 4000: Loss = -11848.41051023021
Iteration 4100: Loss = -11848.410448678362
Iteration 4200: Loss = -11848.408623173227
Iteration 4300: Loss = -11848.407605079778
Iteration 4400: Loss = -11848.405777076507
Iteration 4500: Loss = -11847.205416773613
Iteration 4600: Loss = -11847.202316985664
Iteration 4700: Loss = -11847.200904581683
Iteration 4800: Loss = -11847.20052106475
Iteration 4900: Loss = -11847.200280022025
Iteration 5000: Loss = -11847.189702817119
Iteration 5100: Loss = -11847.188357684896
Iteration 5200: Loss = -11847.188608233593
1
Iteration 5300: Loss = -11847.187793180907
Iteration 5400: Loss = -11847.187821698453
Iteration 5500: Loss = -11837.095750819492
Iteration 5600: Loss = -11836.851068726803
Iteration 5700: Loss = -11836.85079087793
Iteration 5800: Loss = -11834.212066858314
Iteration 5900: Loss = -11834.207765318819
Iteration 6000: Loss = -11834.20665825813
Iteration 6100: Loss = -11834.209335122834
1
Iteration 6200: Loss = -11834.206058218497
Iteration 6300: Loss = -11832.76285021233
Iteration 6400: Loss = -11832.755206933018
Iteration 6500: Loss = -11832.755176569843
Iteration 6600: Loss = -11832.754621119031
Iteration 6700: Loss = -11832.755766756713
1
Iteration 6800: Loss = -11832.754469187239
Iteration 6900: Loss = -11832.756532952868
1
Iteration 7000: Loss = -11832.75450237274
Iteration 7100: Loss = -11832.754321386488
Iteration 7200: Loss = -11832.755558751314
1
Iteration 7300: Loss = -11832.754211300813
Iteration 7400: Loss = -11832.755105841607
1
Iteration 7500: Loss = -11832.756040148217
2
Iteration 7600: Loss = -11832.799010787608
3
Iteration 7700: Loss = -11832.753933306845
Iteration 7800: Loss = -11832.7562505395
1
Iteration 7900: Loss = -11832.754377177527
2
Iteration 8000: Loss = -11832.845021176097
3
Iteration 8100: Loss = -11832.75009419178
Iteration 8200: Loss = -11832.674473707419
Iteration 8300: Loss = -11832.673996915317
Iteration 8400: Loss = -11832.681228739175
1
Iteration 8500: Loss = -11832.67260896194
Iteration 8600: Loss = -11832.67245124745
Iteration 8700: Loss = -11832.682028901296
1
Iteration 8800: Loss = -11832.67233452538
Iteration 8900: Loss = -11832.681379097392
1
Iteration 9000: Loss = -11832.67256296256
2
Iteration 9100: Loss = -11832.70715622379
3
Iteration 9200: Loss = -11832.671536485073
Iteration 9300: Loss = -11832.674544156436
1
Iteration 9400: Loss = -11832.70872359877
2
Iteration 9500: Loss = -11832.669546902867
Iteration 9600: Loss = -11832.676573797036
1
Iteration 9700: Loss = -11832.74059653289
2
Iteration 9800: Loss = -11832.687010882742
3
Iteration 9900: Loss = -11832.671015646763
4
Iteration 10000: Loss = -11827.031538642892
Iteration 10100: Loss = -11827.037048614951
1
Iteration 10200: Loss = -11827.029449911945
Iteration 10300: Loss = -11827.030236036457
1
Iteration 10400: Loss = -11827.02959696722
2
Iteration 10500: Loss = -11827.036354263555
3
Iteration 10600: Loss = -11827.038225815619
4
Iteration 10700: Loss = -11827.037805607524
5
Iteration 10800: Loss = -11827.029201761885
Iteration 10900: Loss = -11827.078313042219
1
Iteration 11000: Loss = -11827.057892918785
2
Iteration 11100: Loss = -11827.070839868082
3
Iteration 11200: Loss = -11827.006885193647
Iteration 11300: Loss = -11827.008062973133
1
Iteration 11400: Loss = -11827.010689770876
2
Iteration 11500: Loss = -11827.006672162917
Iteration 11600: Loss = -11827.00815759919
1
Iteration 11700: Loss = -11827.03172446411
2
Iteration 11800: Loss = -11827.01508100991
3
Iteration 11900: Loss = -11827.007829430846
4
Iteration 12000: Loss = -11827.011076840414
5
Iteration 12100: Loss = -11827.019112163476
6
Iteration 12200: Loss = -11827.028916955978
7
Iteration 12300: Loss = -11827.195138174133
8
Iteration 12400: Loss = -11827.006538105687
Iteration 12500: Loss = -11827.00180858379
Iteration 12600: Loss = -11826.995615357118
Iteration 12700: Loss = -11826.999771616833
1
Iteration 12800: Loss = -11827.162554621184
2
Iteration 12900: Loss = -11826.994015174581
Iteration 13000: Loss = -11826.994829302983
1
Iteration 13100: Loss = -11826.997831017841
2
Iteration 13200: Loss = -11827.001302378014
3
Iteration 13300: Loss = -11827.11700235972
4
Iteration 13400: Loss = -11826.996434896062
5
Iteration 13500: Loss = -11826.993885897007
Iteration 13600: Loss = -11826.997976683999
1
Iteration 13700: Loss = -11826.996317243034
2
Iteration 13800: Loss = -11827.02329648294
3
Iteration 13900: Loss = -11826.999026625013
4
Iteration 14000: Loss = -11826.99214774654
Iteration 14100: Loss = -11827.000603820788
1
Iteration 14200: Loss = -11826.99344908581
2
Iteration 14300: Loss = -11826.992575070202
3
Iteration 14400: Loss = -11827.00162923787
4
Iteration 14500: Loss = -11827.003279448902
5
Iteration 14600: Loss = -11827.00201063608
6
Iteration 14700: Loss = -11826.992547792293
7
Iteration 14800: Loss = -11826.998952733216
8
Iteration 14900: Loss = -11827.018098583825
9
Iteration 15000: Loss = -11826.992931544719
10
Iteration 15100: Loss = -11826.99234601241
11
Iteration 15200: Loss = -11827.004268649129
12
Iteration 15300: Loss = -11826.998684056687
13
Iteration 15400: Loss = -11826.992642491192
14
Iteration 15500: Loss = -11826.99215634996
Iteration 15600: Loss = -11826.990144977332
Iteration 15700: Loss = -11827.022938600638
1
Iteration 15800: Loss = -11826.989582874585
Iteration 15900: Loss = -11826.993250085798
1
Iteration 16000: Loss = -11827.010065745135
2
Iteration 16100: Loss = -11826.992234495156
3
Iteration 16200: Loss = -11827.003653186186
4
Iteration 16300: Loss = -11826.990686207533
5
Iteration 16400: Loss = -11826.99028515976
6
Iteration 16500: Loss = -11826.989266592866
Iteration 16600: Loss = -11827.03272928233
1
Iteration 16700: Loss = -11826.979468804555
Iteration 16800: Loss = -11826.934095852364
Iteration 16900: Loss = -11826.937797013685
1
Iteration 17000: Loss = -11826.931295677365
Iteration 17100: Loss = -11826.93233409465
1
Iteration 17200: Loss = -11826.92967512186
Iteration 17300: Loss = -11826.929739349358
Iteration 17400: Loss = -11826.929878226467
1
Iteration 17500: Loss = -11826.930577394021
2
Iteration 17600: Loss = -11826.962701804681
3
Iteration 17700: Loss = -11826.92969660845
Iteration 17800: Loss = -11826.93541979337
1
Iteration 17900: Loss = -11826.944683441523
2
Iteration 18000: Loss = -11826.93474192997
3
Iteration 18100: Loss = -11826.930228056426
4
Iteration 18200: Loss = -11826.940110561414
5
Iteration 18300: Loss = -11826.932689766976
6
Iteration 18400: Loss = -11826.930625020466
7
Iteration 18500: Loss = -11826.93188762317
8
Iteration 18600: Loss = -11826.931545428686
9
Iteration 18700: Loss = -11826.96323504977
10
Iteration 18800: Loss = -11826.929148945415
Iteration 18900: Loss = -11826.946877076132
1
Iteration 19000: Loss = -11826.927169101109
Iteration 19100: Loss = -11826.937558353078
1
Iteration 19200: Loss = -11826.92662520115
Iteration 19300: Loss = -11826.92882757291
1
Iteration 19400: Loss = -11826.970213969873
2
Iteration 19500: Loss = -11826.924869839691
Iteration 19600: Loss = -11826.921026043372
Iteration 19700: Loss = -11826.921431208128
1
Iteration 19800: Loss = -11826.995206850897
2
Iteration 19900: Loss = -11826.921153367492
3
pi: tensor([[0.2896, 0.7104],
        [0.6107, 0.3893]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4304, 0.5696], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2982, 0.0988],
         [0.5869, 0.2854]],

        [[0.6865, 0.1141],
         [0.5396, 0.7306]],

        [[0.6136, 0.0909],
         [0.5762, 0.5711]],

        [[0.5945, 0.0910],
         [0.5453, 0.6826]],

        [[0.5984, 0.1028],
         [0.6740, 0.6521]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03801502439316291
Average Adjusted Rand Index: 0.9683121900436232
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22122.79560351117
Iteration 100: Loss = -12259.942402221901
Iteration 200: Loss = -12259.164286943624
Iteration 300: Loss = -12258.909589080034
Iteration 400: Loss = -12258.752675547328
Iteration 500: Loss = -12258.635040771178
Iteration 600: Loss = -12258.536127759933
Iteration 700: Loss = -12258.439847330206
Iteration 800: Loss = -12258.327013808643
Iteration 900: Loss = -12258.048941000847
Iteration 1000: Loss = -12256.341125049063
Iteration 1100: Loss = -12256.108752735436
Iteration 1200: Loss = -12256.008274225944
Iteration 1300: Loss = -12255.959632797845
Iteration 1400: Loss = -12255.922746467793
Iteration 1500: Loss = -12255.858866508719
Iteration 1600: Loss = -12255.385371662831
Iteration 1700: Loss = -12255.188043717604
Iteration 1800: Loss = -12255.14600621143
Iteration 1900: Loss = -12255.120913846276
Iteration 2000: Loss = -12255.102613824605
Iteration 2100: Loss = -12255.087201166274
Iteration 2200: Loss = -12255.072227225824
Iteration 2300: Loss = -12255.056129763208
Iteration 2400: Loss = -12255.037047186128
Iteration 2500: Loss = -12255.01213516906
Iteration 2600: Loss = -12254.973640031132
Iteration 2700: Loss = -12254.891896999836
Iteration 2800: Loss = -11854.660495740594
Iteration 2900: Loss = -11771.724559917666
Iteration 3000: Loss = -11771.024401360273
Iteration 3100: Loss = -11769.680978398836
Iteration 3200: Loss = -11769.648498288292
Iteration 3300: Loss = -11769.63535099119
Iteration 3400: Loss = -11769.622710166812
Iteration 3500: Loss = -11769.616038704786
Iteration 3600: Loss = -11769.61033884855
Iteration 3700: Loss = -11769.598496853165
Iteration 3800: Loss = -11769.595549615715
Iteration 3900: Loss = -11769.587930866122
Iteration 4000: Loss = -11769.580154294898
Iteration 4100: Loss = -11769.585140330957
1
Iteration 4200: Loss = -11769.570581931135
Iteration 4300: Loss = -11769.568750913577
Iteration 4400: Loss = -11769.56756813164
Iteration 4500: Loss = -11769.57840258939
1
Iteration 4600: Loss = -11769.564701832189
Iteration 4700: Loss = -11769.563371945245
Iteration 4800: Loss = -11769.562916196674
Iteration 4900: Loss = -11769.56331347176
1
Iteration 5000: Loss = -11769.567577639438
2
Iteration 5100: Loss = -11769.564201775342
3
Iteration 5200: Loss = -11769.560481590224
Iteration 5300: Loss = -11769.559978372536
Iteration 5400: Loss = -11769.559444692017
Iteration 5500: Loss = -11769.558817700905
Iteration 5600: Loss = -11769.557913714456
Iteration 5700: Loss = -11769.558956498673
1
Iteration 5800: Loss = -11769.55069094158
Iteration 5900: Loss = -11769.553078496787
1
Iteration 6000: Loss = -11769.549705428324
Iteration 6100: Loss = -11769.549074499288
Iteration 6200: Loss = -11769.554486869767
1
Iteration 6300: Loss = -11769.556121616433
2
Iteration 6400: Loss = -11769.53573814917
Iteration 6500: Loss = -11769.537299655361
1
Iteration 6600: Loss = -11769.537890767042
2
Iteration 6700: Loss = -11769.53437893861
Iteration 6800: Loss = -11769.587608459136
1
Iteration 6900: Loss = -11769.537153898775
2
Iteration 7000: Loss = -11769.53261697648
Iteration 7100: Loss = -11769.535438622535
1
Iteration 7200: Loss = -11769.531194780704
Iteration 7300: Loss = -11769.56541282288
1
Iteration 7400: Loss = -11769.530581572046
Iteration 7500: Loss = -11769.530836076476
1
Iteration 7600: Loss = -11769.53012575929
Iteration 7700: Loss = -11769.53003102562
Iteration 7800: Loss = -11769.529215304396
Iteration 7900: Loss = -11769.797030449718
1
Iteration 8000: Loss = -11769.52874232991
Iteration 8100: Loss = -11769.528568219099
Iteration 8200: Loss = -11769.578667215861
1
Iteration 8300: Loss = -11769.528285178634
Iteration 8400: Loss = -11769.528077292607
Iteration 8500: Loss = -11769.527918376392
Iteration 8600: Loss = -11769.528681416587
1
Iteration 8700: Loss = -11769.52752222144
Iteration 8800: Loss = -11769.527691548414
1
Iteration 8900: Loss = -11769.527025347588
Iteration 9000: Loss = -11769.516272138671
Iteration 9100: Loss = -11769.51633178453
Iteration 9200: Loss = -11769.516199931551
Iteration 9300: Loss = -11769.540955717779
1
Iteration 9400: Loss = -11769.516470081418
2
Iteration 9500: Loss = -11769.518110908877
3
Iteration 9600: Loss = -11769.516925330441
4
Iteration 9700: Loss = -11769.518470110865
5
Iteration 9800: Loss = -11769.516077913826
Iteration 9900: Loss = -11769.516252702459
1
Iteration 10000: Loss = -11769.520791050485
2
Iteration 10100: Loss = -11769.529247280045
3
Iteration 10200: Loss = -11769.51643916566
4
Iteration 10300: Loss = -11769.51785123336
5
Iteration 10400: Loss = -11769.52257461408
6
Iteration 10500: Loss = -11769.576797540116
7
Iteration 10600: Loss = -11769.52077869791
8
Iteration 10700: Loss = -11769.52631220457
9
Iteration 10800: Loss = -11769.516252026986
10
Iteration 10900: Loss = -11769.525349074122
11
Iteration 11000: Loss = -11769.541505871806
12
Iteration 11100: Loss = -11769.528314764515
13
Iteration 11200: Loss = -11769.516059738064
Iteration 11300: Loss = -11769.5190478774
1
Iteration 11400: Loss = -11769.519308549523
2
Iteration 11500: Loss = -11769.515788427278
Iteration 11600: Loss = -11769.515700259477
Iteration 11700: Loss = -11769.52596418944
1
Iteration 11800: Loss = -11769.51614856741
2
Iteration 11900: Loss = -11769.54943087073
3
Iteration 12000: Loss = -11769.515532893583
Iteration 12100: Loss = -11769.521814537417
1
Iteration 12200: Loss = -11769.517765145642
2
Iteration 12300: Loss = -11769.516078674245
3
Iteration 12400: Loss = -11769.61599020325
4
Iteration 12500: Loss = -11769.525246149256
5
Iteration 12600: Loss = -11769.51574428813
6
Iteration 12700: Loss = -11769.520344930903
7
Iteration 12800: Loss = -11769.520135933126
8
Iteration 12900: Loss = -11769.539619322817
9
Iteration 13000: Loss = -11769.546550562241
10
Iteration 13100: Loss = -11769.517980236225
11
Iteration 13200: Loss = -11769.530648553502
12
Iteration 13300: Loss = -11769.634029165352
13
Iteration 13400: Loss = -11769.515877902917
14
Iteration 13500: Loss = -11769.520256851943
15
Stopping early at iteration 13500 due to no improvement.
pi: tensor([[0.7355, 0.2645],
        [0.2216, 0.7784]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4301, 0.5699], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2938, 0.0988],
         [0.6698, 0.2912]],

        [[0.5822, 0.1053],
         [0.7097, 0.6709]],

        [[0.7284, 0.0910],
         [0.5221, 0.6450]],

        [[0.7216, 0.0917],
         [0.6882, 0.6446]],

        [[0.5595, 0.1028],
         [0.6616, 0.5728]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840282591735267
Average Adjusted Rand Index: 0.9841540496839629
11777.871904873846
[0.03801502439316291, 0.9840282591735267] [0.9683121900436232, 0.9841540496839629] [11826.928399468085, 11769.520256851943]
-------------------------------------
This iteration is 78
True Objective function: Loss = -11785.555703862165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22167.47174318233
Iteration 100: Loss = -12258.2599616856
Iteration 200: Loss = -12257.9129000372
Iteration 300: Loss = -12257.810425731142
Iteration 400: Loss = -12257.747337413977
Iteration 500: Loss = -12257.691459000469
Iteration 600: Loss = -12257.61322958472
Iteration 700: Loss = -12257.387885390104
Iteration 800: Loss = -12257.051535479774
Iteration 900: Loss = -12256.87734203771
Iteration 1000: Loss = -12256.783514181936
Iteration 1100: Loss = -12256.72824310384
Iteration 1200: Loss = -12256.691894395462
Iteration 1300: Loss = -12256.66403896272
Iteration 1400: Loss = -12256.638494668872
Iteration 1500: Loss = -12256.61092646699
Iteration 1600: Loss = -12256.576864302451
Iteration 1700: Loss = -12256.530504072784
Iteration 1800: Loss = -12256.46434635614
Iteration 1900: Loss = -12256.371167238907
Iteration 2000: Loss = -12256.251546491276
Iteration 2100: Loss = -12256.125294821159
Iteration 2200: Loss = -12256.023303933915
Iteration 2300: Loss = -12255.953078205472
Iteration 2400: Loss = -12255.907871491168
Iteration 2500: Loss = -12255.878112095843
Iteration 2600: Loss = -12255.857889367859
Iteration 2700: Loss = -12255.843542950945
Iteration 2800: Loss = -12255.833116841619
Iteration 2900: Loss = -12255.825213478276
Iteration 3000: Loss = -12255.819121952249
Iteration 3100: Loss = -12255.814365454195
Iteration 3200: Loss = -12255.810500724943
Iteration 3300: Loss = -12255.807212231126
Iteration 3400: Loss = -12255.80446901389
Iteration 3500: Loss = -12255.801930782403
Iteration 3600: Loss = -12255.799573761391
Iteration 3700: Loss = -12255.797163487352
Iteration 3800: Loss = -12255.794585794563
Iteration 3900: Loss = -12255.791599560993
Iteration 4000: Loss = -12255.78827015883
Iteration 4100: Loss = -12255.78466935714
Iteration 4200: Loss = -12255.781083102665
Iteration 4300: Loss = -12255.777931566377
Iteration 4400: Loss = -12255.77529022639
Iteration 4500: Loss = -12255.773280484535
Iteration 4600: Loss = -12255.771792092815
Iteration 4700: Loss = -12255.770645702754
Iteration 4800: Loss = -12255.769929680871
Iteration 4900: Loss = -12255.769132833011
Iteration 5000: Loss = -12255.773965437174
1
Iteration 5100: Loss = -12255.768176238575
Iteration 5200: Loss = -12255.76785346724
Iteration 5300: Loss = -12255.767554846261
Iteration 5400: Loss = -12255.767299799007
Iteration 5500: Loss = -12255.767125426368
Iteration 5600: Loss = -12255.766907819787
Iteration 5700: Loss = -12255.776378511804
1
Iteration 5800: Loss = -12255.76671198573
Iteration 5900: Loss = -12255.766564758787
Iteration 6000: Loss = -12255.766495855292
Iteration 6100: Loss = -12255.766369592115
Iteration 6200: Loss = -12255.766452682836
Iteration 6300: Loss = -12255.76624772318
Iteration 6400: Loss = -12255.76721700419
1
Iteration 6500: Loss = -12255.766185767796
Iteration 6600: Loss = -12255.775337976775
1
Iteration 6700: Loss = -12255.766087519876
Iteration 6800: Loss = -12255.76606256154
Iteration 6900: Loss = -12255.766042475943
Iteration 7000: Loss = -12255.766037265184
Iteration 7100: Loss = -12255.76597754612
Iteration 7200: Loss = -12255.766309629933
1
Iteration 7300: Loss = -12255.765925520001
Iteration 7400: Loss = -12255.765946581176
Iteration 7500: Loss = -12255.765890158356
Iteration 7600: Loss = -12255.765893119953
Iteration 7700: Loss = -12255.766259516504
1
Iteration 7800: Loss = -12255.78341036292
2
Iteration 7900: Loss = -12255.79582333383
3
Iteration 8000: Loss = -12255.77064274111
4
Iteration 8100: Loss = -12255.765856470423
Iteration 8200: Loss = -12255.768948521705
1
Iteration 8300: Loss = -12255.76582186865
Iteration 8400: Loss = -12255.76612615964
1
Iteration 8500: Loss = -12255.76585132627
Iteration 8600: Loss = -12255.766435182575
1
Iteration 8700: Loss = -12255.765842872572
Iteration 8800: Loss = -12255.773792144275
1
Iteration 8900: Loss = -12255.769657354667
2
Iteration 9000: Loss = -12255.76584255975
Iteration 9100: Loss = -12255.76966884157
1
Iteration 9200: Loss = -12255.76845211794
2
Iteration 9300: Loss = -12255.773216190406
3
Iteration 9400: Loss = -12255.946576324253
4
Iteration 9500: Loss = -12255.76582559731
Iteration 9600: Loss = -12255.769234576337
1
Iteration 9700: Loss = -12255.76582283119
Iteration 9800: Loss = -12255.770546909964
1
Iteration 9900: Loss = -12255.765792793989
Iteration 10000: Loss = -12255.76657594074
1
Iteration 10100: Loss = -12255.765782532471
Iteration 10200: Loss = -12255.76591037321
1
Iteration 10300: Loss = -12255.765801929962
Iteration 10400: Loss = -12255.765800550329
Iteration 10500: Loss = -12255.772500314864
1
Iteration 10600: Loss = -12255.765780625616
Iteration 10700: Loss = -12255.76783509891
1
Iteration 10800: Loss = -12255.765946128115
2
Iteration 10900: Loss = -12255.775222412227
3
Iteration 11000: Loss = -12255.765942600658
4
Iteration 11100: Loss = -12255.765863133878
Iteration 11200: Loss = -12255.799153025142
1
Iteration 11300: Loss = -12255.803014841511
2
Iteration 11400: Loss = -12255.77064213813
3
Iteration 11500: Loss = -12255.765889809247
Iteration 11600: Loss = -12255.767374225272
1
Iteration 11700: Loss = -12255.807616330983
2
Iteration 11800: Loss = -12255.776693008303
3
Iteration 11900: Loss = -12255.778849394323
4
Iteration 12000: Loss = -12255.76580930144
Iteration 12100: Loss = -12255.770559620769
1
Iteration 12200: Loss = -12255.766565976766
2
Iteration 12300: Loss = -12255.766953734306
3
Iteration 12400: Loss = -12255.79767532626
4
Iteration 12500: Loss = -12255.769584457019
5
Iteration 12600: Loss = -12255.765782954242
Iteration 12700: Loss = -12255.766668790182
1
Iteration 12800: Loss = -12255.778368132102
2
Iteration 12900: Loss = -12255.766000042462
3
Iteration 13000: Loss = -12255.765880942308
Iteration 13100: Loss = -12255.777383245992
1
Iteration 13200: Loss = -12255.779115546255
2
Iteration 13300: Loss = -12255.76771015584
3
Iteration 13400: Loss = -12255.767778729585
4
Iteration 13500: Loss = -12255.79299411837
5
Iteration 13600: Loss = -12255.76676492982
6
Iteration 13700: Loss = -12255.773000698067
7
Iteration 13800: Loss = -12255.765904652199
Iteration 13900: Loss = -12255.76581296053
Iteration 14000: Loss = -12255.816412896454
1
Iteration 14100: Loss = -12255.768145919614
2
Iteration 14200: Loss = -12255.766756107625
3
Iteration 14300: Loss = -12255.76581430602
Iteration 14400: Loss = -12255.766021846988
1
Iteration 14500: Loss = -12255.765890677261
Iteration 14600: Loss = -12255.767317224496
1
Iteration 14700: Loss = -12255.765812397553
Iteration 14800: Loss = -12255.76598352646
1
Iteration 14900: Loss = -12255.76785575447
2
Iteration 15000: Loss = -12255.765936476804
3
Iteration 15100: Loss = -12255.790581651148
4
Iteration 15200: Loss = -12255.765803931961
Iteration 15300: Loss = -12255.766145083991
1
Iteration 15400: Loss = -12255.83989610142
2
Iteration 15500: Loss = -12255.765828635786
Iteration 15600: Loss = -12255.766534892617
1
Iteration 15700: Loss = -12255.765936499107
2
Iteration 15800: Loss = -12255.773600420853
3
Iteration 15900: Loss = -12255.765803119322
Iteration 16000: Loss = -12255.766285520967
1
Iteration 16100: Loss = -12255.774731808555
2
Iteration 16200: Loss = -12255.765838608731
Iteration 16300: Loss = -12255.774678918728
1
Iteration 16400: Loss = -12255.78133455581
2
Iteration 16500: Loss = -12255.766894878141
3
Iteration 16600: Loss = -12255.765802335081
Iteration 16700: Loss = -12255.767597237997
1
Iteration 16800: Loss = -12255.765794097842
Iteration 16900: Loss = -12255.775579972718
1
Iteration 17000: Loss = -12255.7658054294
Iteration 17100: Loss = -12255.766008998662
1
Iteration 17200: Loss = -12255.773792503654
2
Iteration 17300: Loss = -12255.812455768655
3
Iteration 17400: Loss = -12255.780061607067
4
Iteration 17500: Loss = -12255.7659362148
5
Iteration 17600: Loss = -12255.766270353912
6
Iteration 17700: Loss = -12255.781429748033
7
Iteration 17800: Loss = -12255.765811024978
Iteration 17900: Loss = -12255.793742354834
1
Iteration 18000: Loss = -12255.765800636227
Iteration 18100: Loss = -12255.779621934233
1
Iteration 18200: Loss = -12255.814254327473
2
Iteration 18300: Loss = -12255.766728563012
3
Iteration 18400: Loss = -12255.765859244426
Iteration 18500: Loss = -12255.768284015776
1
Iteration 18600: Loss = -12255.797170357651
2
Iteration 18700: Loss = -12255.765838558276
Iteration 18800: Loss = -12255.768075186152
1
Iteration 18900: Loss = -12255.768618164648
2
Iteration 19000: Loss = -12255.768762408074
3
Iteration 19100: Loss = -12255.863556018083
4
Iteration 19200: Loss = -12255.765812085205
Iteration 19300: Loss = -12255.765898911168
Iteration 19400: Loss = -12255.765981362272
Iteration 19500: Loss = -12255.768026101137
1
Iteration 19600: Loss = -12255.766097548736
2
Iteration 19700: Loss = -12255.765929322883
Iteration 19800: Loss = -12255.841246312948
1
Iteration 19900: Loss = -12255.78585240812
2
pi: tensor([[0.1310, 0.8690],
        [0.0037, 0.9963]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9863, 0.0137], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2024, 0.2046],
         [0.6418, 0.1941]],

        [[0.5463, 0.1945],
         [0.6209, 0.7107]],

        [[0.5060, 0.2968],
         [0.6649, 0.5613]],

        [[0.6082, 0.2416],
         [0.7248, 0.5139]],

        [[0.7131, 0.2237],
         [0.6784, 0.6232]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0010694376982404467
Average Adjusted Rand Index: -0.0004293504388350744
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20622.113313349153
Iteration 100: Loss = -12258.13835597644
Iteration 200: Loss = -12257.85488697615
Iteration 300: Loss = -12257.773686647337
Iteration 400: Loss = -12257.731110108645
Iteration 500: Loss = -12257.703702323879
Iteration 600: Loss = -12257.681574458324
Iteration 700: Loss = -12257.661325501958
Iteration 800: Loss = -12257.641852462275
Iteration 900: Loss = -12257.622593927565
Iteration 1000: Loss = -12257.602484952042
Iteration 1100: Loss = -12257.580619057706
Iteration 1200: Loss = -12257.555804409085
Iteration 1300: Loss = -12257.526669523022
Iteration 1400: Loss = -12257.491785449234
Iteration 1500: Loss = -12257.449394356554
Iteration 1600: Loss = -12257.397048683037
Iteration 1700: Loss = -12257.33282065336
Iteration 1800: Loss = -12257.255508067723
Iteration 1900: Loss = -12257.167259023747
Iteration 2000: Loss = -12257.074644351665
Iteration 2100: Loss = -12256.987488297958
Iteration 2200: Loss = -12256.914611381095
Iteration 2300: Loss = -12256.857002491199
Iteration 2400: Loss = -12256.81418534664
Iteration 2500: Loss = -12256.78242484598
Iteration 2600: Loss = -12256.760167776727
Iteration 2700: Loss = -12256.740316809179
Iteration 2800: Loss = -12256.725967246704
Iteration 2900: Loss = -12256.714367814873
Iteration 3000: Loss = -12256.704947107984
Iteration 3100: Loss = -12256.69719684992
Iteration 3200: Loss = -12256.690701804777
Iteration 3300: Loss = -12256.68532029828
Iteration 3400: Loss = -12256.680845522886
Iteration 3500: Loss = -12256.677206357555
Iteration 3600: Loss = -12256.67419608043
Iteration 3700: Loss = -12256.671716077852
Iteration 3800: Loss = -12256.669613638162
Iteration 3900: Loss = -12256.667872460463
Iteration 4000: Loss = -12256.66642032248
Iteration 4100: Loss = -12256.665166362629
Iteration 4200: Loss = -12256.664017601697
Iteration 4300: Loss = -12256.663112955746
Iteration 4400: Loss = -12256.662318453686
Iteration 4500: Loss = -12256.661580508917
Iteration 4600: Loss = -12256.660930409513
Iteration 4700: Loss = -12256.660359624102
Iteration 4800: Loss = -12256.65984247761
Iteration 4900: Loss = -12256.659431857108
Iteration 5000: Loss = -12256.659010484389
Iteration 5100: Loss = -12256.658656615682
Iteration 5200: Loss = -12256.658350109861
Iteration 5300: Loss = -12256.658055671302
Iteration 5400: Loss = -12256.657826453225
Iteration 5500: Loss = -12256.657552826036
Iteration 5600: Loss = -12256.657385382192
Iteration 5700: Loss = -12256.657180415164
Iteration 5800: Loss = -12256.657009061611
Iteration 5900: Loss = -12256.656843913965
Iteration 6000: Loss = -12256.65672762603
Iteration 6100: Loss = -12256.656594511007
Iteration 6200: Loss = -12256.656498050317
Iteration 6300: Loss = -12256.656377148667
Iteration 6400: Loss = -12256.656729020457
1
Iteration 6500: Loss = -12256.656177171808
Iteration 6600: Loss = -12256.656113907848
Iteration 6700: Loss = -12256.656030415654
Iteration 6800: Loss = -12256.655980410464
Iteration 6900: Loss = -12256.65611542365
1
Iteration 7000: Loss = -12256.655868576372
Iteration 7100: Loss = -12256.655805488635
Iteration 7200: Loss = -12256.655730604913
Iteration 7300: Loss = -12256.655650118211
Iteration 7400: Loss = -12256.695634033178
1
Iteration 7500: Loss = -12256.655626649273
Iteration 7600: Loss = -12256.655580619885
Iteration 7700: Loss = -12256.655544164476
Iteration 7800: Loss = -12256.669122597546
1
Iteration 7900: Loss = -12256.655563386514
Iteration 8000: Loss = -12256.655547051188
Iteration 8100: Loss = -12256.655475906111
Iteration 8200: Loss = -12256.66346257347
1
Iteration 8300: Loss = -12256.679437729943
2
Iteration 8400: Loss = -12256.655826769227
3
Iteration 8500: Loss = -12256.730233457507
4
Iteration 8600: Loss = -12256.697646158998
5
Iteration 8700: Loss = -12256.662136689942
6
Iteration 8800: Loss = -12256.661337248028
7
Iteration 8900: Loss = -12256.655319005557
Iteration 9000: Loss = -12256.673087090187
1
Iteration 9100: Loss = -12256.6565038655
2
Iteration 9200: Loss = -12256.656356793752
3
Iteration 9300: Loss = -12256.657782236005
4
Iteration 9400: Loss = -12256.658597660175
5
Iteration 9500: Loss = -12256.655536905939
6
Iteration 9600: Loss = -12256.655386431841
Iteration 9700: Loss = -12256.661310479307
1
Iteration 9800: Loss = -12256.655226576775
Iteration 9900: Loss = -12256.655265110803
Iteration 10000: Loss = -12256.656210743786
1
Iteration 10100: Loss = -12256.656310392213
2
Iteration 10200: Loss = -12256.655448833113
3
Iteration 10300: Loss = -12256.655389134517
4
Iteration 10400: Loss = -12256.655247594066
Iteration 10500: Loss = -12256.655367458588
1
Iteration 10600: Loss = -12256.739257247156
2
Iteration 10700: Loss = -12256.655131143123
Iteration 10800: Loss = -12256.661617393089
1
Iteration 10900: Loss = -12256.795965149515
2
Iteration 11000: Loss = -12256.656224847191
3
Iteration 11100: Loss = -12256.655210835497
Iteration 11200: Loss = -12256.658109754911
1
Iteration 11300: Loss = -12256.66389289031
2
Iteration 11400: Loss = -12256.661887675462
3
Iteration 11500: Loss = -12256.655483615932
4
Iteration 11600: Loss = -12256.65673951795
5
Iteration 11700: Loss = -12256.655355795485
6
Iteration 11800: Loss = -12256.65514337168
Iteration 11900: Loss = -12256.655113460683
Iteration 12000: Loss = -12256.655763240005
1
Iteration 12100: Loss = -12256.655119251263
Iteration 12200: Loss = -12256.65518247962
Iteration 12300: Loss = -12256.65529565864
1
Iteration 12400: Loss = -12256.655080650504
Iteration 12500: Loss = -12256.66304520991
1
Iteration 12600: Loss = -12256.6560702956
2
Iteration 12700: Loss = -12256.655878131554
3
Iteration 12800: Loss = -12256.655394906564
4
Iteration 12900: Loss = -12256.661454479767
5
Iteration 13000: Loss = -12256.84494257856
6
Iteration 13100: Loss = -12256.663211447152
7
Iteration 13200: Loss = -12256.655126997219
Iteration 13300: Loss = -12256.657345351225
1
Iteration 13400: Loss = -12256.65737011542
2
Iteration 13500: Loss = -12256.656746759278
3
Iteration 13600: Loss = -12256.66527956064
4
Iteration 13700: Loss = -12256.656082420503
5
Iteration 13800: Loss = -12256.655097020664
Iteration 13900: Loss = -12256.758661858456
1
Iteration 14000: Loss = -12256.65504156087
Iteration 14100: Loss = -12256.674206340098
1
Iteration 14200: Loss = -12256.65502564483
Iteration 14300: Loss = -12256.659849204827
1
Iteration 14400: Loss = -12256.697625489254
2
Iteration 14500: Loss = -12256.6554925747
3
Iteration 14600: Loss = -12256.65509678562
Iteration 14700: Loss = -12256.65738872685
1
Iteration 14800: Loss = -12256.655043853374
Iteration 14900: Loss = -12256.655402410082
1
Iteration 15000: Loss = -12256.655047404776
Iteration 15100: Loss = -12256.655332315206
1
Iteration 15200: Loss = -12256.655332176855
2
Iteration 15300: Loss = -12256.656235597493
3
Iteration 15400: Loss = -12256.658096552523
4
Iteration 15500: Loss = -12256.68264639309
5
Iteration 15600: Loss = -12256.662765486783
6
Iteration 15700: Loss = -12256.65745897483
7
Iteration 15800: Loss = -12256.691179455522
8
Iteration 15900: Loss = -12256.65651094719
9
Iteration 16000: Loss = -12256.655589672511
10
Iteration 16100: Loss = -12256.655739315178
11
Iteration 16200: Loss = -12256.849985268464
12
Iteration 16300: Loss = -12256.719423405468
13
Iteration 16400: Loss = -12256.656530066908
14
Iteration 16500: Loss = -12256.675185838265
15
Stopping early at iteration 16500 due to no improvement.
pi: tensor([[0.0244, 0.9756],
        [0.0241, 0.9759]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9973, 0.0027], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2040, 0.2035],
         [0.5217, 0.1943]],

        [[0.6351, 0.1887],
         [0.5950, 0.6319]],

        [[0.6014, 0.1598],
         [0.5155, 0.5812]],

        [[0.6299, 0.1843],
         [0.6108, 0.6493]],

        [[0.7014, 0.2092],
         [0.6910, 0.7117]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012431460018472533
Average Adjusted Rand Index: 0.0
11785.555703862165
[-0.0010694376982404467, -0.0012431460018472533] [-0.0004293504388350744, 0.0] [12255.838183443198, 12256.675185838265]
-------------------------------------
This iteration is 79
True Objective function: Loss = -11848.557410701182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21513.442331134193
Iteration 100: Loss = -12343.959979861605
Iteration 200: Loss = -12343.534909347374
Iteration 300: Loss = -12343.413017695457
Iteration 400: Loss = -12343.354988062294
Iteration 500: Loss = -12343.321187312291
Iteration 600: Loss = -12343.298387677984
Iteration 700: Loss = -12343.281048451485
Iteration 800: Loss = -12343.266370564597
Iteration 900: Loss = -12343.252624632409
Iteration 1000: Loss = -12343.23850045726
Iteration 1100: Loss = -12343.222928494122
Iteration 1200: Loss = -12343.204510164727
Iteration 1300: Loss = -12343.181286047022
Iteration 1400: Loss = -12343.149481765828
Iteration 1500: Loss = -12343.101370356433
Iteration 1600: Loss = -12343.024227720776
Iteration 1700: Loss = -12342.911062604326
Iteration 1800: Loss = -12342.791768615232
Iteration 1900: Loss = -12342.711298547176
Iteration 2000: Loss = -12342.659693364629
Iteration 2100: Loss = -12342.619528021469
Iteration 2200: Loss = -12342.589702803622
Iteration 2300: Loss = -12342.569242658818
Iteration 2400: Loss = -12342.553976324523
Iteration 2500: Loss = -12342.537479412606
Iteration 2600: Loss = -12342.5055402045
Iteration 2700: Loss = -12342.429265749142
Iteration 2800: Loss = -12342.398417556102
Iteration 2900: Loss = -12342.393057350371
Iteration 3000: Loss = -12342.391301955806
Iteration 3100: Loss = -12342.390757577448
Iteration 3200: Loss = -12342.390592903861
Iteration 3300: Loss = -12342.39040733988
Iteration 3400: Loss = -12342.39035699038
Iteration 3500: Loss = -12342.390258140507
Iteration 3600: Loss = -12342.390153551854
Iteration 3700: Loss = -12342.390452936457
1
Iteration 3800: Loss = -12342.389977306959
Iteration 3900: Loss = -12342.392249854276
1
Iteration 4000: Loss = -12342.389791131443
Iteration 4100: Loss = -12342.38965896984
Iteration 4200: Loss = -12342.39533030791
1
Iteration 4300: Loss = -12342.389428756982
Iteration 4400: Loss = -12342.389307593234
Iteration 4500: Loss = -12342.38957211578
1
Iteration 4600: Loss = -12342.38895252247
Iteration 4700: Loss = -12342.388758145053
Iteration 4800: Loss = -12342.388522501135
Iteration 4900: Loss = -12342.38822417902
Iteration 5000: Loss = -12342.389170675533
1
Iteration 5100: Loss = -12342.387289850869
Iteration 5200: Loss = -12342.386505787166
Iteration 5300: Loss = -12342.38540442185
Iteration 5400: Loss = -12342.383251468149
Iteration 5500: Loss = -12342.379489681993
Iteration 5600: Loss = -12342.373292950399
Iteration 5700: Loss = -12342.36727598873
Iteration 5800: Loss = -12342.364163578213
Iteration 5900: Loss = -12342.363609197973
Iteration 6000: Loss = -12342.363520915007
Iteration 6100: Loss = -12342.362975203256
Iteration 6200: Loss = -12342.362685364362
Iteration 6300: Loss = -12342.361855691497
Iteration 6400: Loss = -12342.361023496918
Iteration 6500: Loss = -12342.35976826038
Iteration 6600: Loss = -12342.358093183551
Iteration 6700: Loss = -12342.355226668229
Iteration 6800: Loss = -12342.351814761063
Iteration 6900: Loss = -12342.36168254149
1
Iteration 7000: Loss = -12342.349173717612
Iteration 7100: Loss = -12342.349236545564
Iteration 7200: Loss = -12342.351632061289
1
Iteration 7300: Loss = -12342.349116799098
Iteration 7400: Loss = -12342.34995652957
1
Iteration 7500: Loss = -12342.349847816693
2
Iteration 7600: Loss = -12342.349120354389
Iteration 7700: Loss = -12342.349566906916
1
Iteration 7800: Loss = -12342.351203394772
2
Iteration 7900: Loss = -12342.349145762062
Iteration 8000: Loss = -12342.349807495375
1
Iteration 8100: Loss = -12342.349201517203
Iteration 8200: Loss = -12342.349140157332
Iteration 8300: Loss = -12342.349149448699
Iteration 8400: Loss = -12342.349109175939
Iteration 8500: Loss = -12342.459873095842
1
Iteration 8600: Loss = -12342.349116663128
Iteration 8700: Loss = -12342.34911534736
Iteration 8800: Loss = -12342.351644409962
1
Iteration 8900: Loss = -12342.34916432284
Iteration 9000: Loss = -12342.384379923178
1
Iteration 9100: Loss = -12342.349122901587
Iteration 9200: Loss = -12342.353363372717
1
Iteration 9300: Loss = -12342.350639167153
2
Iteration 9400: Loss = -12342.35093343249
3
Iteration 9500: Loss = -12342.349302947565
4
Iteration 9600: Loss = -12342.349251603626
5
Iteration 9700: Loss = -12342.35917994566
6
Iteration 9800: Loss = -12342.392675944759
7
Iteration 9900: Loss = -12342.3535680467
8
Iteration 10000: Loss = -12342.39702690812
9
Iteration 10100: Loss = -12342.349162127508
Iteration 10200: Loss = -12342.349309656687
1
Iteration 10300: Loss = -12342.360193865341
2
Iteration 10400: Loss = -12342.544252309439
3
Iteration 10500: Loss = -12342.34914679559
Iteration 10600: Loss = -12342.350151948047
1
Iteration 10700: Loss = -12342.391298075803
2
Iteration 10800: Loss = -12342.350637396985
3
Iteration 10900: Loss = -12342.425815772938
4
Iteration 11000: Loss = -12342.353285202393
5
Iteration 11100: Loss = -12342.349182253445
Iteration 11200: Loss = -12342.35765647099
1
Iteration 11300: Loss = -12342.427627576724
2
Iteration 11400: Loss = -12342.349496639416
3
Iteration 11500: Loss = -12342.349998682446
4
Iteration 11600: Loss = -12342.349545062467
5
Iteration 11700: Loss = -12342.349221738006
Iteration 11800: Loss = -12342.349479238208
1
Iteration 11900: Loss = -12342.609089731455
2
Iteration 12000: Loss = -12342.349142314491
Iteration 12100: Loss = -12342.353089114855
1
Iteration 12200: Loss = -12342.349181136344
Iteration 12300: Loss = -12342.349290520338
1
Iteration 12400: Loss = -12342.372673639455
2
Iteration 12500: Loss = -12342.349718776451
3
Iteration 12600: Loss = -12342.351983125978
4
Iteration 12700: Loss = -12342.349110357478
Iteration 12800: Loss = -12342.349851233737
1
Iteration 12900: Loss = -12342.349134076685
Iteration 13000: Loss = -12342.35136261958
1
Iteration 13100: Loss = -12342.350811959048
2
Iteration 13200: Loss = -12342.349601239723
3
Iteration 13300: Loss = -12342.357110261404
4
Iteration 13400: Loss = -12342.3491176936
Iteration 13500: Loss = -12342.349756768808
1
Iteration 13600: Loss = -12342.349580530612
2
Iteration 13700: Loss = -12342.34915299904
Iteration 13800: Loss = -12342.349267062713
1
Iteration 13900: Loss = -12342.351239502483
2
Iteration 14000: Loss = -12342.423040578275
3
Iteration 14100: Loss = -12342.356252133544
4
Iteration 14200: Loss = -12342.350901587062
5
Iteration 14300: Loss = -12342.367472187938
6
Iteration 14400: Loss = -12342.349152962415
Iteration 14500: Loss = -12342.374022499305
1
Iteration 14600: Loss = -12342.349114639908
Iteration 14700: Loss = -12342.352754840695
1
Iteration 14800: Loss = -12342.349189276822
Iteration 14900: Loss = -12342.352869801363
1
Iteration 15000: Loss = -12342.353894109789
2
Iteration 15100: Loss = -12342.350227460582
3
Iteration 15200: Loss = -12342.349421821911
4
Iteration 15300: Loss = -12342.372954061582
5
Iteration 15400: Loss = -12342.349278067732
Iteration 15500: Loss = -12342.349422850135
1
Iteration 15600: Loss = -12342.350571132776
2
Iteration 15700: Loss = -12342.364008364528
3
Iteration 15800: Loss = -12342.365649652611
4
Iteration 15900: Loss = -12342.34918998129
Iteration 16000: Loss = -12342.351450466416
1
Iteration 16100: Loss = -12342.400813791324
2
Iteration 16200: Loss = -12342.559688455214
3
Iteration 16300: Loss = -12342.3494367552
4
Iteration 16400: Loss = -12342.34938356598
5
Iteration 16500: Loss = -12342.34925841236
Iteration 16600: Loss = -12342.351265083766
1
Iteration 16700: Loss = -12342.414599198984
2
Iteration 16800: Loss = -12342.398007408969
3
Iteration 16900: Loss = -12342.380976632247
4
Iteration 17000: Loss = -12342.350478334034
5
Iteration 17100: Loss = -12342.35044624874
6
Iteration 17200: Loss = -12342.349924244578
7
Iteration 17300: Loss = -12342.464839218637
8
Iteration 17400: Loss = -12342.465151948098
9
Iteration 17500: Loss = -12342.349180291978
Iteration 17600: Loss = -12342.361235383634
1
Iteration 17700: Loss = -12342.35625357718
2
Iteration 17800: Loss = -12342.350732037938
3
Iteration 17900: Loss = -12342.369135653871
4
Iteration 18000: Loss = -12342.462929423466
5
Iteration 18100: Loss = -12342.351826395172
6
Iteration 18200: Loss = -12342.46873996248
7
Iteration 18300: Loss = -12342.489042769532
8
Iteration 18400: Loss = -12342.349143270585
Iteration 18500: Loss = -12342.35835871503
1
Iteration 18600: Loss = -12342.364828828036
2
Iteration 18700: Loss = -12342.350408480419
3
Iteration 18800: Loss = -12342.349220778913
Iteration 18900: Loss = -12342.349320189227
Iteration 19000: Loss = -12342.428763049093
1
Iteration 19100: Loss = -12342.350145985381
2
Iteration 19200: Loss = -12342.349141223569
Iteration 19300: Loss = -12342.440124057497
1
Iteration 19400: Loss = -12342.349154481033
Iteration 19500: Loss = -12342.349350414479
1
Iteration 19600: Loss = -12342.349302577808
2
Iteration 19700: Loss = -12342.378376663679
3
Iteration 19800: Loss = -12342.351479205357
4
Iteration 19900: Loss = -12342.35285885849
5
pi: tensor([[0.4214, 0.5786],
        [0.0777, 0.9223]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0442, 0.9558], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2472, 0.2053],
         [0.6302, 0.1945]],

        [[0.5581, 0.2263],
         [0.7033, 0.5273]],

        [[0.5596, 0.2151],
         [0.5451, 0.6886]],

        [[0.5038, 0.2281],
         [0.7262, 0.6305]],

        [[0.5151, 0.2042],
         [0.5242, 0.6141]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23005.419289687903
Iteration 100: Loss = -12344.189458708544
Iteration 200: Loss = -12343.49198492523
Iteration 300: Loss = -12343.284257444318
Iteration 400: Loss = -12343.207737185878
Iteration 500: Loss = -12343.166535377555
Iteration 600: Loss = -12343.135812820443
Iteration 700: Loss = -12343.106781057817
Iteration 800: Loss = -12343.074962811355
Iteration 900: Loss = -12343.037486875328
Iteration 1000: Loss = -12342.992790590457
Iteration 1100: Loss = -12342.941732393641
Iteration 1200: Loss = -12342.887775759304
Iteration 1300: Loss = -12342.833258752136
Iteration 1400: Loss = -12342.778504589474
Iteration 1500: Loss = -12342.724199247292
Iteration 1600: Loss = -12342.672243601775
Iteration 1700: Loss = -12342.624386422991
Iteration 1800: Loss = -12342.58217111147
Iteration 1900: Loss = -12342.54570044664
Iteration 2000: Loss = -12342.515485679067
Iteration 2100: Loss = -12342.490553080213
Iteration 2200: Loss = -12342.46938130971
Iteration 2300: Loss = -12342.451467646077
Iteration 2400: Loss = -12342.436662074691
Iteration 2500: Loss = -12342.424369155333
Iteration 2600: Loss = -12342.41412302363
Iteration 2700: Loss = -12342.405572609401
Iteration 2800: Loss = -12342.398259426162
Iteration 2900: Loss = -12342.392001271539
Iteration 3000: Loss = -12342.386498650561
Iteration 3100: Loss = -12342.38166900096
Iteration 3200: Loss = -12342.37746817224
Iteration 3300: Loss = -12342.373825357858
Iteration 3400: Loss = -12342.370642719028
Iteration 3500: Loss = -12342.367987074596
Iteration 3600: Loss = -12342.36574673575
Iteration 3700: Loss = -12342.363902403897
Iteration 3800: Loss = -12342.364209384761
1
Iteration 3900: Loss = -12342.36141537857
Iteration 4000: Loss = -12342.360535130025
Iteration 4100: Loss = -12342.359883058216
Iteration 4200: Loss = -12342.35932592444
Iteration 4300: Loss = -12342.358865939343
Iteration 4400: Loss = -12342.358448214
Iteration 4500: Loss = -12342.358048705313
Iteration 4600: Loss = -12342.374085742917
1
Iteration 4700: Loss = -12342.357251058907
Iteration 4800: Loss = -12342.356817901618
Iteration 4900: Loss = -12342.356326289935
Iteration 5000: Loss = -12342.355837929847
Iteration 5100: Loss = -12342.35525025313
Iteration 5200: Loss = -12342.369078991063
1
Iteration 5300: Loss = -12342.35392665095
Iteration 5400: Loss = -12342.353382886982
Iteration 5500: Loss = -12342.35550851418
1
Iteration 5600: Loss = -12342.351778363944
Iteration 5700: Loss = -12342.351044097913
Iteration 5800: Loss = -12342.35049729184
Iteration 5900: Loss = -12342.350866231483
1
Iteration 6000: Loss = -12342.3495843857
Iteration 6100: Loss = -12342.35064291507
1
Iteration 6200: Loss = -12342.349372351131
Iteration 6300: Loss = -12342.349181343674
Iteration 6400: Loss = -12342.35095398532
1
Iteration 6500: Loss = -12342.349385866743
2
Iteration 6600: Loss = -12342.34930484684
3
Iteration 6700: Loss = -12342.349148521178
Iteration 6800: Loss = -12342.349233453424
Iteration 6900: Loss = -12342.349569598451
1
Iteration 7000: Loss = -12342.350883097459
2
Iteration 7100: Loss = -12342.349158381898
Iteration 7200: Loss = -12342.349123014617
Iteration 7300: Loss = -12342.349279682248
1
Iteration 7400: Loss = -12342.349786549335
2
Iteration 7500: Loss = -12342.349145738632
Iteration 7600: Loss = -12342.349310759855
1
Iteration 7700: Loss = -12342.554179456896
2
Iteration 7800: Loss = -12342.349163054127
Iteration 7900: Loss = -12342.366539900217
1
Iteration 8000: Loss = -12342.349165052408
Iteration 8100: Loss = -12342.34916196763
Iteration 8200: Loss = -12342.379682956456
1
Iteration 8300: Loss = -12342.349129524388
Iteration 8400: Loss = -12342.349150985392
Iteration 8500: Loss = -12342.352852168844
1
Iteration 8600: Loss = -12342.34916570596
Iteration 8700: Loss = -12342.34914331982
Iteration 8800: Loss = -12342.456979434994
1
Iteration 8900: Loss = -12342.349113804235
Iteration 9000: Loss = -12342.349078849651
Iteration 9100: Loss = -12342.350529390114
1
Iteration 9200: Loss = -12342.349127654823
Iteration 9300: Loss = -12342.349092799795
Iteration 9400: Loss = -12342.349200232391
1
Iteration 9500: Loss = -12342.349130147837
Iteration 9600: Loss = -12342.349127813659
Iteration 9700: Loss = -12342.349462669492
1
Iteration 9800: Loss = -12342.349136033212
Iteration 9900: Loss = -12342.349153424522
Iteration 10000: Loss = -12342.352646716961
1
Iteration 10100: Loss = -12342.349150028294
Iteration 10200: Loss = -12342.350069282504
1
Iteration 10300: Loss = -12342.349261299776
2
Iteration 10400: Loss = -12342.349140144182
Iteration 10500: Loss = -12342.349153936708
Iteration 10600: Loss = -12342.349328593222
1
Iteration 10700: Loss = -12342.349131298466
Iteration 10800: Loss = -12342.389559086298
1
Iteration 10900: Loss = -12342.34911353261
Iteration 11000: Loss = -12342.362802129588
1
Iteration 11100: Loss = -12342.34947661654
2
Iteration 11200: Loss = -12342.351665124688
3
Iteration 11300: Loss = -12342.394829758465
4
Iteration 11400: Loss = -12342.349159357262
Iteration 11500: Loss = -12342.3544236564
1
Iteration 11600: Loss = -12342.349153218967
Iteration 11700: Loss = -12342.349714120523
1
Iteration 11800: Loss = -12342.350122533919
2
Iteration 11900: Loss = -12342.377649477134
3
Iteration 12000: Loss = -12342.34911796113
Iteration 12100: Loss = -12342.350705459769
1
Iteration 12200: Loss = -12342.349114036337
Iteration 12300: Loss = -12342.349450772776
1
Iteration 12400: Loss = -12342.375357016681
2
Iteration 12500: Loss = -12342.349132016892
Iteration 12600: Loss = -12342.360251934057
1
Iteration 12700: Loss = -12342.349176182412
Iteration 12800: Loss = -12342.352336478434
1
Iteration 12900: Loss = -12342.382269514008
2
Iteration 13000: Loss = -12342.35034097313
3
Iteration 13100: Loss = -12342.372822047546
4
Iteration 13200: Loss = -12342.405660073699
5
Iteration 13300: Loss = -12342.349802018842
6
Iteration 13400: Loss = -12342.349267228463
Iteration 13500: Loss = -12342.362053849194
1
Iteration 13600: Loss = -12342.349161342934
Iteration 13700: Loss = -12342.349486312276
1
Iteration 13800: Loss = -12342.392923849638
2
Iteration 13900: Loss = -12342.478059281812
3
Iteration 14000: Loss = -12342.436644439409
4
Iteration 14100: Loss = -12342.353045427017
5
Iteration 14200: Loss = -12342.349239341724
Iteration 14300: Loss = -12342.350516757675
1
Iteration 14400: Loss = -12342.349265947745
Iteration 14500: Loss = -12342.368848433105
1
Iteration 14600: Loss = -12342.410846715544
2
Iteration 14700: Loss = -12342.408709925232
3
Iteration 14800: Loss = -12342.349641162407
4
Iteration 14900: Loss = -12342.349253003687
Iteration 15000: Loss = -12342.437873258114
1
Iteration 15100: Loss = -12342.349171663247
Iteration 15200: Loss = -12342.349768012456
1
Iteration 15300: Loss = -12342.368337241636
2
Iteration 15400: Loss = -12342.35439273109
3
Iteration 15500: Loss = -12342.350483088332
4
Iteration 15600: Loss = -12342.352660923387
5
Iteration 15700: Loss = -12342.349200649543
Iteration 15800: Loss = -12342.35376595491
1
Iteration 15900: Loss = -12342.350843958497
2
Iteration 16000: Loss = -12342.350661035773
3
Iteration 16100: Loss = -12342.3580176532
4
Iteration 16200: Loss = -12342.432057326523
5
Iteration 16300: Loss = -12342.367650737078
6
Iteration 16400: Loss = -12342.354987176293
7
Iteration 16500: Loss = -12342.354736415231
8
Iteration 16600: Loss = -12342.349638032221
9
Iteration 16700: Loss = -12342.35092780128
10
Iteration 16800: Loss = -12342.351993003409
11
Iteration 16900: Loss = -12342.370727723635
12
Iteration 17000: Loss = -12342.351696205695
13
Iteration 17100: Loss = -12342.351178075573
14
Iteration 17200: Loss = -12342.352659336779
15
Stopping early at iteration 17200 due to no improvement.
pi: tensor([[0.9249, 0.0751],
        [0.5778, 0.4222]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9576, 0.0424], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1946, 0.2054],
         [0.7065, 0.2483]],

        [[0.5427, 0.2272],
         [0.5533, 0.7290]],

        [[0.6665, 0.2159],
         [0.7167, 0.5632]],

        [[0.6049, 0.2290],
         [0.5400, 0.7202]],

        [[0.7140, 0.2044],
         [0.6584, 0.5873]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
11848.557410701182
[0.0, 0.0] [0.0, 0.0] [12342.352112425597, 12342.352659336779]
-------------------------------------
This iteration is 80
True Objective function: Loss = -11766.089657787514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22934.847416518704
Iteration 100: Loss = -12248.438285947383
Iteration 200: Loss = -12246.30966402305
Iteration 300: Loss = -12245.859054824026
Iteration 400: Loss = -12241.188809115993
Iteration 500: Loss = -12124.830560060182
Iteration 600: Loss = -11888.329594681329
Iteration 700: Loss = -11770.932241990804
Iteration 800: Loss = -11770.607386277841
Iteration 900: Loss = -11770.484051444215
Iteration 1000: Loss = -11762.177000799948
Iteration 1100: Loss = -11762.131469521302
Iteration 1200: Loss = -11762.102230937951
Iteration 1300: Loss = -11762.080647511844
Iteration 1400: Loss = -11762.064146908837
Iteration 1500: Loss = -11762.051425396774
Iteration 1600: Loss = -11762.0411991304
Iteration 1700: Loss = -11762.032638978428
Iteration 1800: Loss = -11762.02480008571
Iteration 1900: Loss = -11762.01520206213
Iteration 2000: Loss = -11761.535424621383
Iteration 2100: Loss = -11761.517630694047
Iteration 2200: Loss = -11761.51275248124
Iteration 2300: Loss = -11761.509394487734
Iteration 2400: Loss = -11761.506178654588
Iteration 2500: Loss = -11761.50245171972
Iteration 2600: Loss = -11760.472787704624
Iteration 2700: Loss = -11759.819381932039
Iteration 2800: Loss = -11759.71593415711
Iteration 2900: Loss = -11759.714154793657
Iteration 3000: Loss = -11759.71254497205
Iteration 3100: Loss = -11759.711532492425
Iteration 3200: Loss = -11759.709698264538
Iteration 3300: Loss = -11759.709787942253
Iteration 3400: Loss = -11759.711054206959
1
Iteration 3500: Loss = -11759.705710402119
Iteration 3600: Loss = -11759.70460496042
Iteration 3700: Loss = -11759.705380014062
1
Iteration 3800: Loss = -11759.708863708225
2
Iteration 3900: Loss = -11759.701389035017
Iteration 4000: Loss = -11759.7012652699
Iteration 4100: Loss = -11759.707107076198
1
Iteration 4200: Loss = -11759.699373225283
Iteration 4300: Loss = -11759.698509289849
Iteration 4400: Loss = -11759.68546575617
Iteration 4500: Loss = -11759.691323123612
1
Iteration 4600: Loss = -11759.684731469497
Iteration 4700: Loss = -11759.683627015964
Iteration 4800: Loss = -11759.682999912877
Iteration 4900: Loss = -11759.682487771075
Iteration 5000: Loss = -11759.682096898398
Iteration 5100: Loss = -11759.692577245767
1
Iteration 5200: Loss = -11759.680684572533
Iteration 5300: Loss = -11759.680333231405
Iteration 5400: Loss = -11759.68056787256
1
Iteration 5500: Loss = -11756.934060474263
Iteration 5600: Loss = -11756.92867536976
Iteration 5700: Loss = -11756.925180069235
Iteration 5800: Loss = -11756.92482245408
Iteration 5900: Loss = -11756.931337784723
1
Iteration 6000: Loss = -11756.925218532146
2
Iteration 6100: Loss = -11756.925423423403
3
Iteration 6200: Loss = -11756.929004576834
4
Iteration 6300: Loss = -11756.930519767107
5
Iteration 6400: Loss = -11756.924421036405
Iteration 6500: Loss = -11756.923780454974
Iteration 6600: Loss = -11756.923623256294
Iteration 6700: Loss = -11756.925730737541
1
Iteration 6800: Loss = -11756.923441476902
Iteration 6900: Loss = -11756.923491707174
Iteration 7000: Loss = -11756.927299953762
1
Iteration 7100: Loss = -11756.924230507902
2
Iteration 7200: Loss = -11756.923565097266
Iteration 7300: Loss = -11756.924985567852
1
Iteration 7400: Loss = -11756.926885038816
2
Iteration 7500: Loss = -11756.925558070332
3
Iteration 7600: Loss = -11756.950345103991
4
Iteration 7700: Loss = -11756.923081646884
Iteration 7800: Loss = -11756.922409660852
Iteration 7900: Loss = -11756.923227339808
1
Iteration 8000: Loss = -11756.922296959974
Iteration 8100: Loss = -11756.922460684456
1
Iteration 8200: Loss = -11756.922120529465
Iteration 8300: Loss = -11756.923651875504
1
Iteration 8400: Loss = -11756.922051059724
Iteration 8500: Loss = -11756.922073152884
Iteration 8600: Loss = -11756.921932846717
Iteration 8700: Loss = -11756.921768784008
Iteration 8800: Loss = -11756.930371466724
1
Iteration 8900: Loss = -11756.894972271282
Iteration 9000: Loss = -11756.88633341591
Iteration 9100: Loss = -11756.88645587834
1
Iteration 9200: Loss = -11756.886313903222
Iteration 9300: Loss = -11756.886385741076
Iteration 9400: Loss = -11756.888534865928
1
Iteration 9500: Loss = -11756.89267344718
2
Iteration 9600: Loss = -11756.887077409385
3
Iteration 9700: Loss = -11756.887127218959
4
Iteration 9800: Loss = -11756.886721808527
5
Iteration 9900: Loss = -11756.887294293085
6
Iteration 10000: Loss = -11756.902504204558
7
Iteration 10100: Loss = -11756.88625837117
Iteration 10200: Loss = -11756.888322458928
1
Iteration 10300: Loss = -11756.886396438524
2
Iteration 10400: Loss = -11756.887090430033
3
Iteration 10500: Loss = -11756.900885111927
4
Iteration 10600: Loss = -11756.90963895212
5
Iteration 10700: Loss = -11756.893569210028
6
Iteration 10800: Loss = -11756.888830765189
7
Iteration 10900: Loss = -11756.931963581523
8
Iteration 11000: Loss = -11756.901951032602
9
Iteration 11100: Loss = -11756.888265131423
10
Iteration 11200: Loss = -11756.901623995265
11
Iteration 11300: Loss = -11756.886581954315
12
Iteration 11400: Loss = -11756.886156009574
Iteration 11500: Loss = -11756.887083084479
1
Iteration 11600: Loss = -11756.889207643531
2
Iteration 11700: Loss = -11756.91350057872
3
Iteration 11800: Loss = -11756.936453597642
4
Iteration 11900: Loss = -11756.892350065145
5
Iteration 12000: Loss = -11756.888610618187
6
Iteration 12100: Loss = -11756.8885390527
7
Iteration 12200: Loss = -11756.88785814135
8
Iteration 12300: Loss = -11756.889178665197
9
Iteration 12400: Loss = -11756.894082833836
10
Iteration 12500: Loss = -11756.896170414115
11
Iteration 12600: Loss = -11756.893854724125
12
Iteration 12700: Loss = -11756.887132845315
13
Iteration 12800: Loss = -11756.887445145518
14
Iteration 12900: Loss = -11756.887247540657
15
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[0.7545, 0.2455],
        [0.2347, 0.7653]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5542, 0.4458], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3006, 0.0935],
         [0.5041, 0.2974]],

        [[0.7196, 0.0917],
         [0.6464, 0.5838]],

        [[0.5223, 0.0886],
         [0.5923, 0.6636]],

        [[0.6662, 0.1127],
         [0.5849, 0.5821]],

        [[0.6906, 0.0979],
         [0.5811, 0.6659]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320626466431
Average Adjusted Rand Index: 0.9839993417272901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24495.085470011632
Iteration 100: Loss = -12306.582774629858
Iteration 200: Loss = -12305.702757263318
Iteration 300: Loss = -12305.509761213056
Iteration 400: Loss = -12305.426592248272
Iteration 500: Loss = -12305.371762333716
Iteration 600: Loss = -12305.321004238192
Iteration 700: Loss = -12305.262610949549
Iteration 800: Loss = -12305.190630854326
Iteration 900: Loss = -12305.09950108757
Iteration 1000: Loss = -12304.987968789577
Iteration 1100: Loss = -12304.85706941201
Iteration 1200: Loss = -12304.703304441971
Iteration 1300: Loss = -12304.514704727984
Iteration 1400: Loss = -12304.27954849353
Iteration 1500: Loss = -12304.008285894375
Iteration 1600: Loss = -12303.726327834334
Iteration 1700: Loss = -12303.512582735251
Iteration 1800: Loss = -12303.352590003931
Iteration 1900: Loss = -12303.225565339595
Iteration 2000: Loss = -12303.159248985532
Iteration 2100: Loss = -12303.128312549725
Iteration 2200: Loss = -12303.11179446285
Iteration 2300: Loss = -12303.101326666383
Iteration 2400: Loss = -12303.09311441589
Iteration 2500: Loss = -12303.086894218108
Iteration 2600: Loss = -12300.877342254906
Iteration 2700: Loss = -11904.740935173339
Iteration 2800: Loss = -11849.001170578142
Iteration 2900: Loss = -11830.132800627554
Iteration 3000: Loss = -11814.698861056117
Iteration 3100: Loss = -11814.66551103382
Iteration 3200: Loss = -11814.652703679238
Iteration 3300: Loss = -11814.585368237247
Iteration 3400: Loss = -11800.160040604047
Iteration 3500: Loss = -11792.363082922779
Iteration 3600: Loss = -11792.307373711561
Iteration 3700: Loss = -11784.150605146788
Iteration 3800: Loss = -11781.585318203372
Iteration 3900: Loss = -11781.582155183449
Iteration 4000: Loss = -11781.579206903405
Iteration 4100: Loss = -11781.563041997433
Iteration 4200: Loss = -11781.490393102917
Iteration 4300: Loss = -11781.48828492655
Iteration 4400: Loss = -11781.487753217385
Iteration 4500: Loss = -11781.48539147431
Iteration 4600: Loss = -11773.808791792735
Iteration 4700: Loss = -11773.807466645101
Iteration 4800: Loss = -11773.806229520764
Iteration 4900: Loss = -11773.804550300181
Iteration 5000: Loss = -11773.801556464356
Iteration 5100: Loss = -11766.12926061565
Iteration 5200: Loss = -11763.966697235499
Iteration 5300: Loss = -11763.966542807155
Iteration 5400: Loss = -11763.965210380138
Iteration 5500: Loss = -11763.965304458034
Iteration 5600: Loss = -11763.965431504337
1
Iteration 5700: Loss = -11763.959501004965
Iteration 5800: Loss = -11758.414515456814
Iteration 5900: Loss = -11758.413362389834
Iteration 6000: Loss = -11758.42813582325
1
Iteration 6100: Loss = -11758.4096399844
Iteration 6200: Loss = -11758.243579298109
Iteration 6300: Loss = -11758.24190844061
Iteration 6400: Loss = -11758.241743513941
Iteration 6500: Loss = -11758.242959753106
1
Iteration 6600: Loss = -11758.240685884184
Iteration 6700: Loss = -11758.244862682744
1
Iteration 6800: Loss = -11758.241542556254
2
Iteration 6900: Loss = -11758.239973335527
Iteration 7000: Loss = -11758.24048234005
1
Iteration 7100: Loss = -11758.24692709429
2
Iteration 7200: Loss = -11758.239254980486
Iteration 7300: Loss = -11758.10532101572
Iteration 7400: Loss = -11756.915635647812
Iteration 7500: Loss = -11756.705163858967
Iteration 7600: Loss = -11756.711073244232
1
Iteration 7700: Loss = -11756.704076101332
Iteration 7800: Loss = -11756.702409788295
Iteration 7900: Loss = -11756.702539138409
1
Iteration 8000: Loss = -11756.706082503959
2
Iteration 8100: Loss = -11756.731087849177
3
Iteration 8200: Loss = -11756.702393076437
Iteration 8300: Loss = -11756.70130640267
Iteration 8400: Loss = -11756.698846285643
Iteration 8500: Loss = -11756.69773484398
Iteration 8600: Loss = -11756.697568555803
Iteration 8700: Loss = -11756.69734898056
Iteration 8800: Loss = -11756.6936171832
Iteration 8900: Loss = -11756.694263952322
1
Iteration 9000: Loss = -11756.694048479541
2
Iteration 9100: Loss = -11756.696734941068
3
Iteration 9200: Loss = -11756.69665940933
4
Iteration 9300: Loss = -11756.693629619533
Iteration 9400: Loss = -11756.693027719202
Iteration 9500: Loss = -11756.692644563711
Iteration 9600: Loss = -11756.817471309232
1
Iteration 9700: Loss = -11756.712276180935
2
Iteration 9800: Loss = -11756.710277502538
3
Iteration 9900: Loss = -11756.713432889495
4
Iteration 10000: Loss = -11756.692676385934
Iteration 10100: Loss = -11756.83993908128
1
Iteration 10200: Loss = -11756.693898376016
2
Iteration 10300: Loss = -11756.691433081487
Iteration 10400: Loss = -11756.691549701502
1
Iteration 10500: Loss = -11756.69335292053
2
Iteration 10600: Loss = -11756.745727781908
3
Iteration 10700: Loss = -11756.691891224735
4
Iteration 10800: Loss = -11756.690061791087
Iteration 10900: Loss = -11756.70693812871
1
Iteration 11000: Loss = -11756.784499274707
2
Iteration 11100: Loss = -11756.731403125981
3
Iteration 11200: Loss = -11756.760174348547
4
Iteration 11300: Loss = -11756.691906773567
5
Iteration 11400: Loss = -11756.691316256718
6
Iteration 11500: Loss = -11756.689646974419
Iteration 11600: Loss = -11756.690786846279
1
Iteration 11700: Loss = -11756.703750106704
2
Iteration 11800: Loss = -11756.745414813653
3
Iteration 11900: Loss = -11756.694042080084
4
Iteration 12000: Loss = -11756.693809346114
5
Iteration 12100: Loss = -11756.690407843585
6
Iteration 12200: Loss = -11756.689439105165
Iteration 12300: Loss = -11756.685690584069
Iteration 12400: Loss = -11756.693557096123
1
Iteration 12500: Loss = -11756.6848246535
Iteration 12600: Loss = -11756.687087748665
1
Iteration 12700: Loss = -11756.685473942773
2
Iteration 12800: Loss = -11756.678631349989
Iteration 12900: Loss = -11756.682302323765
1
Iteration 13000: Loss = -11756.678020294938
Iteration 13100: Loss = -11756.674856064035
Iteration 13200: Loss = -11756.681243103752
1
Iteration 13300: Loss = -11756.685060377824
2
Iteration 13400: Loss = -11756.728984034002
3
Iteration 13500: Loss = -11756.693129996562
4
Iteration 13600: Loss = -11756.685223547489
5
Iteration 13700: Loss = -11756.674288610251
Iteration 13800: Loss = -11756.680753610786
1
Iteration 13900: Loss = -11756.719122915982
2
Iteration 14000: Loss = -11756.682437226891
3
Iteration 14100: Loss = -11756.673833738765
Iteration 14200: Loss = -11756.67407021611
1
Iteration 14300: Loss = -11756.776497160827
2
Iteration 14400: Loss = -11756.673102593646
Iteration 14500: Loss = -11756.675570907337
1
Iteration 14600: Loss = -11756.677374007484
2
Iteration 14700: Loss = -11756.673327304761
3
Iteration 14800: Loss = -11756.674754632228
4
Iteration 14900: Loss = -11756.681080081247
5
Iteration 15000: Loss = -11756.674230502842
6
Iteration 15100: Loss = -11756.676806015683
7
Iteration 15200: Loss = -11756.693242690233
8
Iteration 15300: Loss = -11756.699664188109
9
Iteration 15400: Loss = -11756.70640047552
10
Iteration 15500: Loss = -11756.838829169688
11
Iteration 15600: Loss = -11756.67676641737
12
Iteration 15700: Loss = -11756.675071258138
13
Iteration 15800: Loss = -11756.675306853776
14
Iteration 15900: Loss = -11756.676552551913
15
Stopping early at iteration 15900 due to no improvement.
pi: tensor([[0.2333, 0.7667],
        [0.7539, 0.2461]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5551, 0.4449], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2959, 0.0936],
         [0.6045, 0.3025]],

        [[0.7097, 0.0917],
         [0.5634, 0.7096]],

        [[0.6864, 0.0888],
         [0.7037, 0.5815]],

        [[0.7072, 0.1124],
         [0.6367, 0.6397]],

        [[0.6271, 0.0979],
         [0.5748, 0.5588]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.04134727858608461
Average Adjusted Rand Index: 0.9839993417272901
11766.089657787514
[0.9840320626466431, 0.04134727858608461] [0.9839993417272901, 0.9839993417272901] [11756.887247540657, 11756.676552551913]
-------------------------------------
This iteration is 81
True Objective function: Loss = -11867.356095159359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20605.22218962234
Iteration 100: Loss = -12454.464516572489
Iteration 200: Loss = -12454.114022966523
Iteration 300: Loss = -12454.025888713406
Iteration 400: Loss = -12453.987838262501
Iteration 500: Loss = -12453.966534384344
Iteration 600: Loss = -12453.951368742595
Iteration 700: Loss = -12453.938777689313
Iteration 800: Loss = -12453.926922956707
Iteration 900: Loss = -12453.914668278418
Iteration 1000: Loss = -12453.900701177474
Iteration 1100: Loss = -12453.883926180111
Iteration 1200: Loss = -12453.863599070837
Iteration 1300: Loss = -12453.840242556476
Iteration 1400: Loss = -12453.815513664307
Iteration 1500: Loss = -12453.7905133708
Iteration 1600: Loss = -12453.765489451289
Iteration 1700: Loss = -12453.740727585955
Iteration 1800: Loss = -12453.717208451799
Iteration 1900: Loss = -12453.696329838675
Iteration 2000: Loss = -12453.679201905448
Iteration 2100: Loss = -12453.665894254264
Iteration 2200: Loss = -12453.655737543952
Iteration 2300: Loss = -12453.647686236627
Iteration 2400: Loss = -12453.640818796372
Iteration 2500: Loss = -12453.63417287268
Iteration 2600: Loss = -12453.627267413925
Iteration 2700: Loss = -12453.619521654138
Iteration 2800: Loss = -12453.610877789686
Iteration 2900: Loss = -12453.601070602293
Iteration 3000: Loss = -12453.590600958143
Iteration 3100: Loss = -12453.579714708641
Iteration 3200: Loss = -12453.569310760324
Iteration 3300: Loss = -12453.560472742218
Iteration 3400: Loss = -12453.552271179055
Iteration 3500: Loss = -12453.54610193261
Iteration 3600: Loss = -12453.54121592258
Iteration 3700: Loss = -12453.537494648426
Iteration 3800: Loss = -12453.534553139069
Iteration 3900: Loss = -12453.532372857919
Iteration 4000: Loss = -12453.530375107908
Iteration 4100: Loss = -12453.530197662367
Iteration 4200: Loss = -12453.527640108656
Iteration 4300: Loss = -12453.526721891243
Iteration 4400: Loss = -12453.525774698475
Iteration 4500: Loss = -12453.524962008605
Iteration 4600: Loss = -12453.524433770359
Iteration 4700: Loss = -12453.523710538437
Iteration 4800: Loss = -12453.523247134215
Iteration 4900: Loss = -12453.52278024719
Iteration 5000: Loss = -12453.522364629018
Iteration 5100: Loss = -12453.522634598692
1
Iteration 5200: Loss = -12453.521735520426
Iteration 5300: Loss = -12453.521413498813
Iteration 5400: Loss = -12453.522078815373
1
Iteration 5500: Loss = -12453.520925403433
Iteration 5600: Loss = -12453.520764437422
Iteration 5700: Loss = -12453.520919876997
1
Iteration 5800: Loss = -12453.520343200462
Iteration 5900: Loss = -12453.520186553265
Iteration 6000: Loss = -12453.520038263243
Iteration 6100: Loss = -12453.519934741624
Iteration 6200: Loss = -12453.520679129439
1
Iteration 6300: Loss = -12453.519699369124
Iteration 6400: Loss = -12453.519582637125
Iteration 6500: Loss = -12453.51949511054
Iteration 6600: Loss = -12453.519707356005
1
Iteration 6700: Loss = -12453.519292346196
Iteration 6800: Loss = -12453.519238891276
Iteration 6900: Loss = -12453.519531115535
1
Iteration 7000: Loss = -12453.522303631333
2
Iteration 7100: Loss = -12453.533243628144
3
Iteration 7200: Loss = -12453.519447569719
4
Iteration 7300: Loss = -12453.51946322684
5
Iteration 7400: Loss = -12453.519017457074
Iteration 7500: Loss = -12453.519154751146
1
Iteration 7600: Loss = -12453.523357490934
2
Iteration 7700: Loss = -12453.518896553865
Iteration 7800: Loss = -12453.53208205509
1
Iteration 7900: Loss = -12453.518730695692
Iteration 8000: Loss = -12453.521673069896
1
Iteration 8100: Loss = -12453.520048887607
2
Iteration 8200: Loss = -12453.520431386758
3
Iteration 8300: Loss = -12453.518582974453
Iteration 8400: Loss = -12453.621607481202
1
Iteration 8500: Loss = -12453.51855101784
Iteration 8600: Loss = -12453.518675983985
1
Iteration 8700: Loss = -12453.52213196908
2
Iteration 8800: Loss = -12453.519843521588
3
Iteration 8900: Loss = -12453.518452197262
Iteration 9000: Loss = -12453.617571785848
1
Iteration 9100: Loss = -12453.518358607214
Iteration 9200: Loss = -12453.518703711368
1
Iteration 9300: Loss = -12453.518577279152
2
Iteration 9400: Loss = -12453.536464744773
3
Iteration 9500: Loss = -12453.525010593215
4
Iteration 9600: Loss = -12453.518373618585
Iteration 9700: Loss = -12453.521669646729
1
Iteration 9800: Loss = -12453.528457906397
2
Iteration 9900: Loss = -12453.519811808059
3
Iteration 10000: Loss = -12453.518138894393
Iteration 10100: Loss = -12453.51865409116
1
Iteration 10200: Loss = -12453.51870081005
2
Iteration 10300: Loss = -12453.518060627102
Iteration 10400: Loss = -12453.519587972001
1
Iteration 10500: Loss = -12453.660434453757
2
Iteration 10600: Loss = -12453.517984087435
Iteration 10700: Loss = -12453.518120435021
1
Iteration 10800: Loss = -12453.73002420328
2
Iteration 10900: Loss = -12453.517906009778
Iteration 11000: Loss = -12453.518658579002
1
Iteration 11100: Loss = -12453.518022573533
2
Iteration 11200: Loss = -12453.579775619404
3
Iteration 11300: Loss = -12453.517893437484
Iteration 11400: Loss = -12453.541317325933
1
Iteration 11500: Loss = -12453.517823780174
Iteration 11600: Loss = -12453.52103769234
1
Iteration 11700: Loss = -12453.518776191542
2
Iteration 11800: Loss = -12453.538522437244
3
Iteration 11900: Loss = -12453.595089183062
4
Iteration 12000: Loss = -12453.54209285092
5
Iteration 12100: Loss = -12453.526547715463
6
Iteration 12200: Loss = -12453.51971791926
7
Iteration 12300: Loss = -12453.519135677576
8
Iteration 12400: Loss = -12453.5217521119
9
Iteration 12500: Loss = -12453.5188414888
10
Iteration 12600: Loss = -12453.518717461191
11
Iteration 12700: Loss = -12453.518327719836
12
Iteration 12800: Loss = -12453.517875598558
Iteration 12900: Loss = -12453.727334121018
1
Iteration 13000: Loss = -12453.517716974246
Iteration 13100: Loss = -12453.5195956782
1
Iteration 13200: Loss = -12453.519363244415
2
Iteration 13300: Loss = -12453.520382016712
3
Iteration 13400: Loss = -12453.551808332813
4
Iteration 13500: Loss = -12453.517769630625
Iteration 13600: Loss = -12453.51791870747
1
Iteration 13700: Loss = -12453.628898507752
2
Iteration 13800: Loss = -12453.517764495677
Iteration 13900: Loss = -12453.550325490369
1
Iteration 14000: Loss = -12453.560407425362
2
Iteration 14100: Loss = -12453.541279362853
3
Iteration 14200: Loss = -12453.520740075843
4
Iteration 14300: Loss = -12453.517731469874
Iteration 14400: Loss = -12453.529725109998
1
Iteration 14500: Loss = -12453.517659394274
Iteration 14600: Loss = -12453.532387189462
1
Iteration 14700: Loss = -12453.517712220646
Iteration 14800: Loss = -12453.517890332128
1
Iteration 14900: Loss = -12453.540205082856
2
Iteration 15000: Loss = -12453.524874827126
3
Iteration 15100: Loss = -12453.529644140563
4
Iteration 15200: Loss = -12453.520160201537
5
Iteration 15300: Loss = -12453.51769502745
Iteration 15400: Loss = -12453.563983197164
1
Iteration 15500: Loss = -12453.517694829396
Iteration 15600: Loss = -12453.51971246263
1
Iteration 15700: Loss = -12453.517766355832
Iteration 15800: Loss = -12453.520995422115
1
Iteration 15900: Loss = -12453.518544600282
2
Iteration 16000: Loss = -12453.562605428691
3
Iteration 16100: Loss = -12453.519648078185
4
Iteration 16200: Loss = -12453.519003385069
5
Iteration 16300: Loss = -12453.518206343739
6
Iteration 16400: Loss = -12453.527033368278
7
Iteration 16500: Loss = -12453.522075276433
8
Iteration 16600: Loss = -12453.517821061852
Iteration 16700: Loss = -12453.525194739326
1
Iteration 16800: Loss = -12453.5198750118
2
Iteration 16900: Loss = -12453.517758316138
Iteration 17000: Loss = -12453.53437007703
1
Iteration 17100: Loss = -12453.519981672744
2
Iteration 17200: Loss = -12453.522622148956
3
Iteration 17300: Loss = -12453.548487301601
4
Iteration 17400: Loss = -12453.51780699818
Iteration 17500: Loss = -12453.521684610212
1
Iteration 17600: Loss = -12453.52175763084
2
Iteration 17700: Loss = -12453.521636216898
3
Iteration 17800: Loss = -12453.540841612401
4
Iteration 17900: Loss = -12453.517847794441
Iteration 18000: Loss = -12453.517919637896
Iteration 18100: Loss = -12453.534197790987
1
Iteration 18200: Loss = -12453.607889634002
2
Iteration 18300: Loss = -12453.517714083326
Iteration 18400: Loss = -12453.575287268146
1
Iteration 18500: Loss = -12453.554232757593
2
Iteration 18600: Loss = -12453.519303922789
3
Iteration 18700: Loss = -12453.538667261208
4
Iteration 18800: Loss = -12453.522244792719
5
Iteration 18900: Loss = -12453.520478473947
6
Iteration 19000: Loss = -12453.517975546798
7
Iteration 19100: Loss = -12453.517862957498
8
Iteration 19200: Loss = -12453.52704019301
9
Iteration 19300: Loss = -12453.518219377589
10
Iteration 19400: Loss = -12453.535844353828
11
Iteration 19500: Loss = -12453.517716918052
Iteration 19600: Loss = -12453.517804541923
Iteration 19700: Loss = -12453.652918385053
1
Iteration 19800: Loss = -12453.517633106927
Iteration 19900: Loss = -12453.518287426754
1
pi: tensor([[9.6848e-01, 3.1520e-02],
        [9.9999e-01, 1.4081e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5112, 0.4888], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2008, 0.2050],
         [0.5861, 0.2084]],

        [[0.6013, 0.1967],
         [0.5973, 0.6198]],

        [[0.7176, 0.2607],
         [0.7131, 0.5396]],

        [[0.6326, 0.2161],
         [0.6990, 0.6374]],

        [[0.5354, 0.1920],
         [0.6279, 0.6656]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.04863852081571281
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.006989845724252884
Average Adjusted Rand Index: 0.009727704163142562
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21163.664948675312
Iteration 100: Loss = -12454.844850520009
Iteration 200: Loss = -12454.261482102858
Iteration 300: Loss = -12454.144134056472
Iteration 400: Loss = -12454.082430709213
Iteration 500: Loss = -12454.041857295595
Iteration 600: Loss = -12454.012181791308
Iteration 700: Loss = -12453.989306307212
Iteration 800: Loss = -12453.97081689858
Iteration 900: Loss = -12453.954919462189
Iteration 1000: Loss = -12453.9399753878
Iteration 1100: Loss = -12453.924349988896
Iteration 1200: Loss = -12453.906097103256
Iteration 1300: Loss = -12453.883337895059
Iteration 1400: Loss = -12453.855865376048
Iteration 1500: Loss = -12453.826725913092
Iteration 1600: Loss = -12453.800017148309
Iteration 1700: Loss = -12453.776901617204
Iteration 1800: Loss = -12453.756584375435
Iteration 1900: Loss = -12453.738278459894
Iteration 2000: Loss = -12453.721681131854
Iteration 2100: Loss = -12453.70673114867
Iteration 2200: Loss = -12453.693565552596
Iteration 2300: Loss = -12453.682436408131
Iteration 2400: Loss = -12453.673359628545
Iteration 2500: Loss = -12453.666066018995
Iteration 2600: Loss = -12453.660154337274
Iteration 2700: Loss = -12453.655028702704
Iteration 2800: Loss = -12453.650347946052
Iteration 2900: Loss = -12453.645692623704
Iteration 3000: Loss = -12453.640657380285
Iteration 3100: Loss = -12453.635205339806
Iteration 3200: Loss = -12453.62889264679
Iteration 3300: Loss = -12453.626643616366
Iteration 3400: Loss = -12453.61350698222
Iteration 3500: Loss = -12453.607628600497
Iteration 3600: Loss = -12453.594448833544
Iteration 3700: Loss = -12453.58440835818
Iteration 3800: Loss = -12453.574545106849
Iteration 3900: Loss = -12453.56574742692
Iteration 4000: Loss = -12453.558274094295
Iteration 4100: Loss = -12453.551850831642
Iteration 4200: Loss = -12453.546867452758
Iteration 4300: Loss = -12453.542771174232
Iteration 4400: Loss = -12453.53955452174
Iteration 4500: Loss = -12453.536878209925
Iteration 4600: Loss = -12453.534785501259
Iteration 4700: Loss = -12453.532896647679
Iteration 4800: Loss = -12453.531472098824
Iteration 4900: Loss = -12453.530058061657
Iteration 5000: Loss = -12453.537206089672
1
Iteration 5100: Loss = -12453.527921159466
Iteration 5200: Loss = -12453.527000881348
Iteration 5300: Loss = -12453.526375938427
Iteration 5400: Loss = -12453.525574515064
Iteration 5500: Loss = -12453.524931955892
Iteration 5600: Loss = -12453.52440991133
Iteration 5700: Loss = -12453.523877130076
Iteration 5800: Loss = -12453.525245942596
1
Iteration 5900: Loss = -12453.523057441254
Iteration 6000: Loss = -12453.522638296738
Iteration 6100: Loss = -12453.522349118586
Iteration 6200: Loss = -12453.5220213423
Iteration 6300: Loss = -12453.521730865456
Iteration 6400: Loss = -12453.521471813248
Iteration 6500: Loss = -12453.5212250936
Iteration 6600: Loss = -12453.521868124282
1
Iteration 6700: Loss = -12453.52088189425
Iteration 6800: Loss = -12453.520673987176
Iteration 6900: Loss = -12453.520494168124
Iteration 7000: Loss = -12453.520330364585
Iteration 7100: Loss = -12453.520885046795
1
Iteration 7200: Loss = -12453.599673264645
2
Iteration 7300: Loss = -12453.587651876484
3
Iteration 7400: Loss = -12453.519854681048
Iteration 7500: Loss = -12453.519778521852
Iteration 7600: Loss = -12453.808408308869
1
Iteration 7700: Loss = -12453.519563633687
Iteration 7800: Loss = -12453.519950951153
1
Iteration 7900: Loss = -12453.519446690978
Iteration 8000: Loss = -12453.51931624275
Iteration 8100: Loss = -12453.537746450285
1
Iteration 8200: Loss = -12453.519217511523
Iteration 8300: Loss = -12453.519330763913
1
Iteration 8400: Loss = -12453.520171747366
2
Iteration 8500: Loss = -12453.519073138486
Iteration 8600: Loss = -12453.519136249626
Iteration 8700: Loss = -12453.5214932849
1
Iteration 8800: Loss = -12453.518984417236
Iteration 8900: Loss = -12453.518901835581
Iteration 9000: Loss = -12453.523761793236
1
Iteration 9100: Loss = -12453.518833480637
Iteration 9200: Loss = -12454.12950815503
1
Iteration 9300: Loss = -12453.51878295357
Iteration 9400: Loss = -12453.526389710847
1
Iteration 9500: Loss = -12453.622716216638
2
Iteration 9600: Loss = -12453.518717491488
Iteration 9700: Loss = -12453.533810645522
1
Iteration 9800: Loss = -12453.518726678903
Iteration 9900: Loss = -12453.930043668193
1
Iteration 10000: Loss = -12453.518505234406
Iteration 10100: Loss = -12453.520152345693
1
Iteration 10200: Loss = -12453.524425253627
2
Iteration 10300: Loss = -12453.518362752438
Iteration 10400: Loss = -12453.518706339317
1
Iteration 10500: Loss = -12453.518417580208
Iteration 10600: Loss = -12453.518403949778
Iteration 10700: Loss = -12453.518661629998
1
Iteration 10800: Loss = -12453.5183648357
Iteration 10900: Loss = -12453.535329605014
1
Iteration 11000: Loss = -12453.527210450966
2
Iteration 11100: Loss = -12453.545863023646
3
Iteration 11200: Loss = -12453.520603487334
4
Iteration 11300: Loss = -12453.518089601073
Iteration 11400: Loss = -12453.52150960207
1
Iteration 11500: Loss = -12453.524560165877
2
Iteration 11600: Loss = -12453.51828692102
3
Iteration 11700: Loss = -12453.518003413472
Iteration 11800: Loss = -12453.523757874986
1
Iteration 11900: Loss = -12453.517981188133
Iteration 12000: Loss = -12453.523886084062
1
Iteration 12100: Loss = -12453.525717334103
2
Iteration 12200: Loss = -12453.611379916016
3
Iteration 12300: Loss = -12453.563401577945
4
Iteration 12400: Loss = -12453.619762745919
5
Iteration 12500: Loss = -12453.517938506582
Iteration 12600: Loss = -12453.51839436701
1
Iteration 12700: Loss = -12453.535744394325
2
Iteration 12800: Loss = -12453.526786225291
3
Iteration 12900: Loss = -12453.521237039218
4
Iteration 13000: Loss = -12453.518013269142
Iteration 13100: Loss = -12453.519988224307
1
Iteration 13200: Loss = -12453.520622578852
2
Iteration 13300: Loss = -12453.567541285898
3
Iteration 13400: Loss = -12453.547842401656
4
Iteration 13500: Loss = -12453.51842088533
5
Iteration 13600: Loss = -12453.519772931073
6
Iteration 13700: Loss = -12453.518092275526
Iteration 13800: Loss = -12453.525940116528
1
Iteration 13900: Loss = -12453.517764513674
Iteration 14000: Loss = -12453.525497434175
1
Iteration 14100: Loss = -12453.520578802782
2
Iteration 14200: Loss = -12453.517777638086
Iteration 14300: Loss = -12453.550800172849
1
Iteration 14400: Loss = -12453.517952035292
2
Iteration 14500: Loss = -12453.517785080072
Iteration 14600: Loss = -12453.536262585401
1
Iteration 14700: Loss = -12453.522971685856
2
Iteration 14800: Loss = -12453.518805625063
3
Iteration 14900: Loss = -12453.527869707383
4
Iteration 15000: Loss = -12453.535276250437
5
Iteration 15100: Loss = -12453.525339246653
6
Iteration 15200: Loss = -12453.51962353128
7
Iteration 15300: Loss = -12453.565047510088
8
Iteration 15400: Loss = -12453.517680712339
Iteration 15500: Loss = -12453.527416093642
1
Iteration 15600: Loss = -12453.519833315037
2
Iteration 15700: Loss = -12453.517773357265
Iteration 15800: Loss = -12453.539870452976
1
Iteration 15900: Loss = -12453.51766424065
Iteration 16000: Loss = -12453.517771009176
1
Iteration 16100: Loss = -12453.52173555619
2
Iteration 16200: Loss = -12453.5182397747
3
Iteration 16300: Loss = -12453.522877886218
4
Iteration 16400: Loss = -12453.530567231897
5
Iteration 16500: Loss = -12453.527481850417
6
Iteration 16600: Loss = -12453.522572335709
7
Iteration 16700: Loss = -12453.517916890954
8
Iteration 16800: Loss = -12453.518758853766
9
Iteration 16900: Loss = -12453.522432739328
10
Iteration 17000: Loss = -12453.528689664494
11
Iteration 17100: Loss = -12453.517679438977
Iteration 17200: Loss = -12453.52153299488
1
Iteration 17300: Loss = -12453.517678953565
Iteration 17400: Loss = -12453.520247479928
1
Iteration 17500: Loss = -12453.526644064716
2
Iteration 17600: Loss = -12453.517710559203
Iteration 17700: Loss = -12453.521509041673
1
Iteration 17800: Loss = -12453.518247970858
2
Iteration 17900: Loss = -12453.526750758689
3
Iteration 18000: Loss = -12453.517892534417
4
Iteration 18100: Loss = -12453.519048333314
5
Iteration 18200: Loss = -12453.51887480784
6
Iteration 18300: Loss = -12453.517672332442
Iteration 18400: Loss = -12453.53105643035
1
Iteration 18500: Loss = -12453.517725818008
Iteration 18600: Loss = -12453.527865997985
1
Iteration 18700: Loss = -12453.526899098113
2
Iteration 18800: Loss = -12453.532637074817
3
Iteration 18900: Loss = -12453.524143238508
4
Iteration 19000: Loss = -12453.518265447155
5
Iteration 19100: Loss = -12453.517924826418
6
Iteration 19200: Loss = -12453.55845567702
7
Iteration 19300: Loss = -12453.517645184365
Iteration 19400: Loss = -12453.51796212434
1
Iteration 19500: Loss = -12453.569869724337
2
Iteration 19600: Loss = -12453.51778032582
3
Iteration 19700: Loss = -12453.518035625668
4
Iteration 19800: Loss = -12453.604155871526
5
Iteration 19900: Loss = -12453.517662395983
pi: tensor([[9.6974e-01, 3.0264e-02],
        [9.9997e-01, 2.6038e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5096, 0.4904], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2004, 0.2045],
         [0.7066, 0.2087]],

        [[0.5215, 0.1970],
         [0.6303, 0.5091]],

        [[0.6704, 0.2622],
         [0.5744, 0.7086]],

        [[0.5762, 0.2169],
         [0.6711, 0.6120]],

        [[0.5779, 0.1924],
         [0.7105, 0.5373]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.04863852081571281
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.006989845724252884
Average Adjusted Rand Index: 0.009727704163142562
11867.356095159359
[0.006989845724252884, 0.006989845724252884] [0.009727704163142562, 0.009727704163142562] [12453.525147310253, 12453.547436363302]
-------------------------------------
This iteration is 82
True Objective function: Loss = -11896.091487769521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22989.42745424349
Iteration 100: Loss = -12423.975471447755
Iteration 200: Loss = -12422.901391851145
Iteration 300: Loss = -12422.56509748999
Iteration 400: Loss = -12422.44577202351
Iteration 500: Loss = -12422.38565837593
Iteration 600: Loss = -12422.347165495297
Iteration 700: Loss = -12422.319781421353
Iteration 800: Loss = -12422.299016875539
Iteration 900: Loss = -12422.282637944896
Iteration 1000: Loss = -12422.269330324581
Iteration 1100: Loss = -12422.258301070968
Iteration 1200: Loss = -12422.249041556444
Iteration 1300: Loss = -12422.241080697097
Iteration 1400: Loss = -12422.234038805758
Iteration 1500: Loss = -12422.22768019974
Iteration 1600: Loss = -12422.221671761548
Iteration 1700: Loss = -12422.215799373498
Iteration 1800: Loss = -12422.209890987424
Iteration 1900: Loss = -12422.203680876204
Iteration 2000: Loss = -12422.197084685848
Iteration 2100: Loss = -12422.18983645574
Iteration 2200: Loss = -12422.181701780886
Iteration 2300: Loss = -12422.172430265246
Iteration 2400: Loss = -12422.161705239803
Iteration 2500: Loss = -12422.14894379112
Iteration 2600: Loss = -12422.13379286965
Iteration 2700: Loss = -12422.115506697397
Iteration 2800: Loss = -12422.093409852374
Iteration 2900: Loss = -12422.066859606492
Iteration 3000: Loss = -12422.035462625752
Iteration 3100: Loss = -12422.002270765117
Iteration 3200: Loss = -12421.962014305072
Iteration 3300: Loss = -12421.924708923907
Iteration 3400: Loss = -12421.88945977503
Iteration 3500: Loss = -12421.857610507792
Iteration 3600: Loss = -12421.82736502826
Iteration 3700: Loss = -12421.797475997479
Iteration 3800: Loss = -12421.767127792167
Iteration 3900: Loss = -12421.729342249937
Iteration 4000: Loss = -12421.688377062208
Iteration 4100: Loss = -12421.641676539726
Iteration 4200: Loss = -12421.593613497691
Iteration 4300: Loss = -12421.549502392429
Iteration 4400: Loss = -12421.515486669983
Iteration 4500: Loss = -12421.486404963583
Iteration 4600: Loss = -12421.466679401246
Iteration 4700: Loss = -12421.45260400307
Iteration 4800: Loss = -12421.441696993335
Iteration 4900: Loss = -12421.442449771072
1
Iteration 5000: Loss = -12421.426953113292
Iteration 5100: Loss = -12421.421716967667
Iteration 5200: Loss = -12421.41798789741
Iteration 5300: Loss = -12421.414230195962
Iteration 5400: Loss = -12421.411540176938
Iteration 5500: Loss = -12421.409215800259
Iteration 5600: Loss = -12421.407343456794
Iteration 5700: Loss = -12421.41308208836
1
Iteration 5800: Loss = -12421.404561564254
Iteration 5900: Loss = -12421.403351109544
Iteration 6000: Loss = -12421.40279601336
Iteration 6100: Loss = -12421.401627327257
Iteration 6200: Loss = -12421.400814164299
Iteration 6300: Loss = -12421.400129543554
Iteration 6400: Loss = -12421.399384036584
Iteration 6500: Loss = -12421.400696906627
1
Iteration 6600: Loss = -12421.398233529808
Iteration 6700: Loss = -12421.397597246216
Iteration 6800: Loss = -12421.399140934525
1
Iteration 6900: Loss = -12421.396521957458
Iteration 7000: Loss = -12421.395927469992
Iteration 7100: Loss = -12421.39551114498
Iteration 7200: Loss = -12421.394973060638
Iteration 7300: Loss = -12421.404623315952
1
Iteration 7400: Loss = -12421.394060756615
Iteration 7500: Loss = -12421.39960624593
1
Iteration 7600: Loss = -12421.393323157803
Iteration 7700: Loss = -12421.399799348232
1
Iteration 7800: Loss = -12421.392701498333
Iteration 7900: Loss = -12421.458679341986
1
Iteration 8000: Loss = -12421.395470229876
2
Iteration 8100: Loss = -12421.393505288463
3
Iteration 8200: Loss = -12421.39246493039
Iteration 8300: Loss = -12421.407281227297
1
Iteration 8400: Loss = -12421.393342813895
2
Iteration 8500: Loss = -12421.399289846406
3
Iteration 8600: Loss = -12421.404861978313
4
Iteration 8700: Loss = -12421.391318829197
Iteration 8800: Loss = -12421.392077273398
1
Iteration 8900: Loss = -12421.391211175027
Iteration 9000: Loss = -12421.392837184902
1
Iteration 9100: Loss = -12421.391794319608
2
Iteration 9200: Loss = -12421.391416103623
3
Iteration 9300: Loss = -12421.391559596465
4
Iteration 9400: Loss = -12421.39158588949
5
Iteration 9500: Loss = -12421.391030358749
Iteration 9600: Loss = -12421.39242276762
1
Iteration 9700: Loss = -12421.391120943144
Iteration 9800: Loss = -12421.424786858997
1
Iteration 9900: Loss = -12421.391028425933
Iteration 10000: Loss = -12421.481922415707
1
Iteration 10100: Loss = -12421.405735386159
2
Iteration 10200: Loss = -12421.390973070967
Iteration 10300: Loss = -12421.39292438154
1
Iteration 10400: Loss = -12421.438352748006
2
Iteration 10500: Loss = -12421.406181085578
3
Iteration 10600: Loss = -12421.422885861273
4
Iteration 10700: Loss = -12421.39135518171
5
Iteration 10800: Loss = -12421.40190221009
6
Iteration 10900: Loss = -12421.39103765748
Iteration 11000: Loss = -12421.401797554543
1
Iteration 11100: Loss = -12421.392739301182
2
Iteration 11200: Loss = -12421.393059952343
3
Iteration 11300: Loss = -12421.391768997179
4
Iteration 11400: Loss = -12421.426868160777
5
Iteration 11500: Loss = -12421.390885983428
Iteration 11600: Loss = -12421.391818108637
1
Iteration 11700: Loss = -12421.391456350088
2
Iteration 11800: Loss = -12421.392747384021
3
Iteration 11900: Loss = -12421.390941514945
Iteration 12000: Loss = -12421.39684042327
1
Iteration 12100: Loss = -12421.395776831203
2
Iteration 12200: Loss = -12421.447283724001
3
Iteration 12300: Loss = -12421.390885126764
Iteration 12400: Loss = -12421.391423347954
1
Iteration 12500: Loss = -12421.448182318125
2
Iteration 12600: Loss = -12421.400272032033
3
Iteration 12700: Loss = -12421.39247114598
4
Iteration 12800: Loss = -12421.417568498444
5
Iteration 12900: Loss = -12421.421109115612
6
Iteration 13000: Loss = -12421.392613991133
7
Iteration 13100: Loss = -12421.727076364692
8
Iteration 13200: Loss = -12421.390880563918
Iteration 13300: Loss = -12421.401783806978
1
Iteration 13400: Loss = -12421.404694073657
2
Iteration 13500: Loss = -12421.398891643805
3
Iteration 13600: Loss = -12421.419553531288
4
Iteration 13700: Loss = -12421.391626441897
5
Iteration 13800: Loss = -12421.40594993645
6
Iteration 13900: Loss = -12421.456965182699
7
Iteration 14000: Loss = -12421.411100200037
8
Iteration 14100: Loss = -12421.397376368019
9
Iteration 14200: Loss = -12421.393982041593
10
Iteration 14300: Loss = -12421.393925822667
11
Iteration 14400: Loss = -12421.415666517323
12
Iteration 14500: Loss = -12421.393780181243
13
Iteration 14600: Loss = -12421.424559580342
14
Iteration 14700: Loss = -12421.404259604942
15
Stopping early at iteration 14700 due to no improvement.
pi: tensor([[0.2676, 0.7324],
        [0.3757, 0.6243]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9836, 0.0164], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2053, 0.2086],
         [0.5878, 0.1979]],

        [[0.7075, 0.2017],
         [0.6995, 0.5872]],

        [[0.6982, 0.2024],
         [0.6706, 0.6481]],

        [[0.6910, 0.2070],
         [0.5279, 0.6175]],

        [[0.6838, 0.1940],
         [0.7162, 0.6211]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002805526372414202
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22515.857044724424
Iteration 100: Loss = -12422.88962851876
Iteration 200: Loss = -12422.514614815283
Iteration 300: Loss = -12422.409528331771
Iteration 400: Loss = -12422.334300513212
Iteration 500: Loss = -12422.241091109654
Iteration 600: Loss = -12422.126058873797
Iteration 700: Loss = -12422.030039667477
Iteration 800: Loss = -12421.967015319226
Iteration 900: Loss = -12421.925126976117
Iteration 1000: Loss = -12421.895581427183
Iteration 1100: Loss = -12421.873546702189
Iteration 1200: Loss = -12421.856199277858
Iteration 1300: Loss = -12421.841696354883
Iteration 1400: Loss = -12421.82884388923
Iteration 1500: Loss = -12421.81695598366
Iteration 1600: Loss = -12421.8051392431
Iteration 1700: Loss = -12421.79280840096
Iteration 1800: Loss = -12421.779135965899
Iteration 1900: Loss = -12421.76343796426
Iteration 2000: Loss = -12421.744524912563
Iteration 2100: Loss = -12421.721103131475
Iteration 2200: Loss = -12421.691893793042
Iteration 2300: Loss = -12421.655734295338
Iteration 2400: Loss = -12421.613338603322
Iteration 2500: Loss = -12421.568018257485
Iteration 2600: Loss = -12421.526129378852
Iteration 2700: Loss = -12421.491155835674
Iteration 2800: Loss = -12421.46534973064
Iteration 2900: Loss = -12421.446321802934
Iteration 3000: Loss = -12421.432659899898
Iteration 3100: Loss = -12421.422813850533
Iteration 3200: Loss = -12421.415215280942
Iteration 3300: Loss = -12421.409294444524
Iteration 3400: Loss = -12421.404790361925
Iteration 3500: Loss = -12421.400771139857
Iteration 3600: Loss = -12421.397552646458
Iteration 3700: Loss = -12421.399232358164
1
Iteration 3800: Loss = -12421.392533581919
Iteration 3900: Loss = -12421.39050482072
Iteration 4000: Loss = -12421.388708389888
Iteration 4100: Loss = -12421.387035745007
Iteration 4200: Loss = -12421.385577251478
Iteration 4300: Loss = -12421.384300152098
Iteration 4400: Loss = -12421.38359236018
Iteration 4500: Loss = -12421.382003889825
Iteration 4600: Loss = -12421.380984970376
Iteration 4700: Loss = -12421.380060977372
Iteration 4800: Loss = -12421.379223080887
Iteration 4900: Loss = -12421.378397615925
Iteration 5000: Loss = -12421.377612538443
Iteration 5100: Loss = -12421.37689200563
Iteration 5200: Loss = -12421.376208444986
Iteration 5300: Loss = -12421.375623306027
Iteration 5400: Loss = -12421.375064382795
Iteration 5500: Loss = -12421.374494407455
Iteration 5600: Loss = -12421.374080269348
Iteration 5700: Loss = -12421.373454571609
Iteration 5800: Loss = -12421.373033092994
Iteration 5900: Loss = -12421.372589916984
Iteration 6000: Loss = -12421.37232240955
Iteration 6100: Loss = -12421.371766566828
Iteration 6200: Loss = -12421.371372411297
Iteration 6300: Loss = -12421.37104939007
Iteration 6400: Loss = -12421.370730919089
Iteration 6500: Loss = -12421.370391922272
Iteration 6600: Loss = -12421.370084991351
Iteration 6700: Loss = -12421.369814765598
Iteration 6800: Loss = -12421.36955587666
Iteration 6900: Loss = -12421.369294996512
Iteration 7000: Loss = -12421.36908811853
Iteration 7100: Loss = -12421.368877530127
Iteration 7200: Loss = -12421.368631916019
Iteration 7300: Loss = -12421.368463247103
Iteration 7400: Loss = -12421.368249652855
Iteration 7500: Loss = -12421.368041609807
Iteration 7600: Loss = -12421.367899784964
Iteration 7700: Loss = -12421.367723500896
Iteration 7800: Loss = -12421.36758535786
Iteration 7900: Loss = -12421.367692300111
1
Iteration 8000: Loss = -12421.37742600313
2
Iteration 8100: Loss = -12421.367206888403
Iteration 8200: Loss = -12421.367258368518
Iteration 8300: Loss = -12421.366934679701
Iteration 8400: Loss = -12421.369854264498
1
Iteration 8500: Loss = -12421.384910931312
2
Iteration 8600: Loss = -12421.369403408155
3
Iteration 8700: Loss = -12421.366663576908
Iteration 8800: Loss = -12421.494067176707
1
Iteration 8900: Loss = -12421.366396934498
Iteration 9000: Loss = -12421.366402085796
Iteration 9100: Loss = -12421.366764306626
1
Iteration 9200: Loss = -12421.36939463561
2
Iteration 9300: Loss = -12421.366249161325
Iteration 9400: Loss = -12421.366831171352
1
Iteration 9500: Loss = -12421.366215197175
Iteration 9600: Loss = -12421.366056703866
Iteration 9700: Loss = -12421.365986550041
Iteration 9800: Loss = -12421.36629769541
1
Iteration 9900: Loss = -12421.365880368028
Iteration 10000: Loss = -12421.54788394418
1
Iteration 10100: Loss = -12421.365721441329
Iteration 10200: Loss = -12421.37151184308
1
Iteration 10300: Loss = -12421.3656464901
Iteration 10400: Loss = -12421.36644675633
1
Iteration 10500: Loss = -12421.365608349748
Iteration 10600: Loss = -12421.365639285761
Iteration 10700: Loss = -12421.365594151352
Iteration 10800: Loss = -12421.36641445135
1
Iteration 10900: Loss = -12421.439412906166
2
Iteration 11000: Loss = -12421.365466479032
Iteration 11100: Loss = -12421.394661302214
1
Iteration 11200: Loss = -12421.365413001628
Iteration 11300: Loss = -12421.385485789204
1
Iteration 11400: Loss = -12421.36600965745
2
Iteration 11500: Loss = -12421.402249165207
3
Iteration 11600: Loss = -12421.365345270087
Iteration 11700: Loss = -12421.368157753644
1
Iteration 11800: Loss = -12421.367904370825
2
Iteration 11900: Loss = -12421.368712148136
3
Iteration 12000: Loss = -12421.385834165636
4
Iteration 12100: Loss = -12421.365284058526
Iteration 12200: Loss = -12421.386076599832
1
Iteration 12300: Loss = -12421.365176262632
Iteration 12400: Loss = -12421.370380823653
1
Iteration 12500: Loss = -12421.365153681018
Iteration 12600: Loss = -12421.36756690273
1
Iteration 12700: Loss = -12421.365151846208
Iteration 12800: Loss = -12421.366013903427
1
Iteration 12900: Loss = -12421.36510649463
Iteration 13000: Loss = -12421.394296195454
1
Iteration 13100: Loss = -12421.36508115294
Iteration 13200: Loss = -12421.365106701422
Iteration 13300: Loss = -12421.368155133883
1
Iteration 13400: Loss = -12421.365072516692
Iteration 13500: Loss = -12421.366836930976
1
Iteration 13600: Loss = -12421.36505878806
Iteration 13700: Loss = -12421.386404095769
1
Iteration 13800: Loss = -12421.36504892663
Iteration 13900: Loss = -12421.399571416132
1
Iteration 14000: Loss = -12421.365049122776
Iteration 14100: Loss = -12421.369015808064
1
Iteration 14200: Loss = -12421.365036078396
Iteration 14300: Loss = -12421.36710059766
1
Iteration 14400: Loss = -12421.365036489176
Iteration 14500: Loss = -12421.413322273382
1
Iteration 14600: Loss = -12421.368363264473
2
Iteration 14700: Loss = -12421.373716701342
3
Iteration 14800: Loss = -12421.365769570875
4
Iteration 14900: Loss = -12421.385895714384
5
Iteration 15000: Loss = -12421.380978945055
6
Iteration 15100: Loss = -12421.3650171516
Iteration 15200: Loss = -12421.365820933263
1
Iteration 15300: Loss = -12421.423274572886
2
Iteration 15400: Loss = -12421.416665444234
3
Iteration 15500: Loss = -12421.365189872831
4
Iteration 15600: Loss = -12421.533129332083
5
Iteration 15700: Loss = -12421.364991408402
Iteration 15800: Loss = -12421.41479561448
1
Iteration 15900: Loss = -12421.364919833219
Iteration 16000: Loss = -12421.365390632223
1
Iteration 16100: Loss = -12421.364940519057
Iteration 16200: Loss = -12421.37583012561
1
Iteration 16300: Loss = -12421.3649193062
Iteration 16400: Loss = -12421.405336673855
1
Iteration 16500: Loss = -12421.364925176198
Iteration 16600: Loss = -12421.365071820335
1
Iteration 16700: Loss = -12421.364997856064
Iteration 16800: Loss = -12421.366530886573
1
Iteration 16900: Loss = -12421.364972896306
Iteration 17000: Loss = -12421.612931455815
1
Iteration 17100: Loss = -12421.365963768305
2
Iteration 17200: Loss = -12421.506566428958
3
Iteration 17300: Loss = -12421.36496721513
Iteration 17400: Loss = -12421.371699594349
1
Iteration 17500: Loss = -12421.365008948878
Iteration 17600: Loss = -12421.367677058735
1
Iteration 17700: Loss = -12421.381102711586
2
Iteration 17800: Loss = -12421.365035309966
Iteration 17900: Loss = -12421.371289426677
1
Iteration 18000: Loss = -12421.426704448784
2
Iteration 18100: Loss = -12421.365037733056
Iteration 18200: Loss = -12421.366899753333
1
Iteration 18300: Loss = -12421.364964362827
Iteration 18400: Loss = -12421.364935417967
Iteration 18500: Loss = -12421.391168560021
1
Iteration 18600: Loss = -12421.364924607207
Iteration 18700: Loss = -12421.382464991924
1
Iteration 18800: Loss = -12421.364943015667
Iteration 18900: Loss = -12421.366323796126
1
Iteration 19000: Loss = -12421.364943717648
Iteration 19100: Loss = -12421.505289621495
1
Iteration 19200: Loss = -12421.364979272523
Iteration 19300: Loss = -12421.364921225962
Iteration 19400: Loss = -12421.370814267886
1
Iteration 19500: Loss = -12421.364954307457
Iteration 19600: Loss = -12421.364926933724
Iteration 19700: Loss = -12421.382597375858
1
Iteration 19800: Loss = -12421.364944517478
Iteration 19900: Loss = -12421.364937192957
pi: tensor([[1.1961e-05, 9.9999e-01],
        [9.2443e-03, 9.9076e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9985, 0.0015], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2057, 0.2054],
         [0.5031, 0.1991]],

        [[0.5559, 0.2054],
         [0.6299, 0.5623]],

        [[0.6871, 0.2135],
         [0.5305, 0.6910]],

        [[0.6401, 0.2208],
         [0.5413, 0.7137]],

        [[0.6321, 0.3304],
         [0.6461, 0.6706]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.0028731106445115248
Average Adjusted Rand Index: -0.0003077958928485548
11896.091487769521
[-0.002805526372414202, -0.0028731106445115248] [0.0, -0.0003077958928485548] [12421.404259604942, 12421.370153799799]
-------------------------------------
This iteration is 83
True Objective function: Loss = -11862.760638320342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21969.970102714968
Iteration 100: Loss = -12360.90003709671
Iteration 200: Loss = -12360.175197076713
Iteration 300: Loss = -12360.048303220196
Iteration 400: Loss = -12359.972935832366
Iteration 500: Loss = -12359.907044601017
Iteration 600: Loss = -12359.831922509711
Iteration 700: Loss = -12359.718879181106
Iteration 800: Loss = -12359.461442852533
Iteration 900: Loss = -12358.770083326905
Iteration 1000: Loss = -12358.269368224279
Iteration 1100: Loss = -12358.095148211229
Iteration 1200: Loss = -12358.008781902126
Iteration 1300: Loss = -12357.954596340469
Iteration 1400: Loss = -12357.916931675969
Iteration 1500: Loss = -12357.889400454485
Iteration 1600: Loss = -12357.86869928516
Iteration 1700: Loss = -12357.852995933177
Iteration 1800: Loss = -12357.84094553786
Iteration 1900: Loss = -12357.831716299203
Iteration 2000: Loss = -12357.8247083094
Iteration 2100: Loss = -12357.819376450347
Iteration 2200: Loss = -12357.815364081816
Iteration 2300: Loss = -12357.812317460124
Iteration 2400: Loss = -12357.810004400479
Iteration 2500: Loss = -12357.808129810595
Iteration 2600: Loss = -12357.806636524101
Iteration 2700: Loss = -12357.80534598187
Iteration 2800: Loss = -12357.804178717368
Iteration 2900: Loss = -12357.803043354164
Iteration 3000: Loss = -12357.80192375525
Iteration 3100: Loss = -12357.80072971948
Iteration 3200: Loss = -12357.79948373429
Iteration 3300: Loss = -12357.798075530793
Iteration 3400: Loss = -12357.796550091341
Iteration 3500: Loss = -12357.79485113411
Iteration 3600: Loss = -12357.792880534946
Iteration 3700: Loss = -12357.790484094428
Iteration 3800: Loss = -12357.787570659575
Iteration 3900: Loss = -12357.783696558212
Iteration 4000: Loss = -12357.778207041678
Iteration 4100: Loss = -12357.769474543928
Iteration 4200: Loss = -12357.754113517438
Iteration 4300: Loss = -12357.727050261863
Iteration 4400: Loss = -12357.692446892113
Iteration 4500: Loss = -12357.663832817838
Iteration 4600: Loss = -12357.645463534744
Iteration 4700: Loss = -12357.633537899317
Iteration 4800: Loss = -12357.625700318953
Iteration 4900: Loss = -12357.620188367899
Iteration 5000: Loss = -12357.616232152255
Iteration 5100: Loss = -12357.61322293011
Iteration 5200: Loss = -12357.610954165079
Iteration 5300: Loss = -12357.609146932109
Iteration 5400: Loss = -12357.607697474812
Iteration 5500: Loss = -12357.606521332667
Iteration 5600: Loss = -12357.605535056045
Iteration 5700: Loss = -12357.605227397982
Iteration 5800: Loss = -12357.604058040606
Iteration 5900: Loss = -12357.603479517926
Iteration 6000: Loss = -12357.603346137832
Iteration 6100: Loss = -12357.602596323102
Iteration 6200: Loss = -12357.602247462584
Iteration 6300: Loss = -12357.60192935636
Iteration 6400: Loss = -12357.601854339297
Iteration 6500: Loss = -12357.602890901975
1
Iteration 6600: Loss = -12357.601270230169
Iteration 6700: Loss = -12357.601055126286
Iteration 6800: Loss = -12357.60199744062
1
Iteration 6900: Loss = -12357.600787719728
Iteration 7000: Loss = -12357.601986243517
1
Iteration 7100: Loss = -12357.600541107775
Iteration 7200: Loss = -12357.600481186806
Iteration 7300: Loss = -12357.600438449408
Iteration 7400: Loss = -12357.600283030537
Iteration 7500: Loss = -12357.600476431706
1
Iteration 7600: Loss = -12357.600192363656
Iteration 7700: Loss = -12357.600561920959
1
Iteration 7800: Loss = -12357.600086229471
Iteration 7900: Loss = -12357.600024983889
Iteration 8000: Loss = -12357.601233818832
1
Iteration 8100: Loss = -12357.600048297896
Iteration 8200: Loss = -12357.600408384218
1
Iteration 8300: Loss = -12357.704707692674
2
Iteration 8400: Loss = -12357.599810931684
Iteration 8500: Loss = -12357.610249879193
1
Iteration 8600: Loss = -12357.600454560195
2
Iteration 8700: Loss = -12357.600797907793
3
Iteration 8800: Loss = -12357.59984801515
Iteration 8900: Loss = -12357.600219312813
1
Iteration 9000: Loss = -12357.634464101187
2
Iteration 9100: Loss = -12357.599720604494
Iteration 9200: Loss = -12357.64786654251
1
Iteration 9300: Loss = -12357.600521084865
2
Iteration 9400: Loss = -12357.599689648956
Iteration 9500: Loss = -12357.605244548304
1
Iteration 9600: Loss = -12357.599696835228
Iteration 9700: Loss = -12357.600467853379
1
Iteration 9800: Loss = -12357.599711961468
Iteration 9900: Loss = -12357.60238970491
1
Iteration 10000: Loss = -12357.604335956052
2
Iteration 10100: Loss = -12357.657949249893
3
Iteration 10200: Loss = -12357.599613631106
Iteration 10300: Loss = -12357.599588854733
Iteration 10400: Loss = -12357.615380983132
1
Iteration 10500: Loss = -12357.62292799539
2
Iteration 10600: Loss = -12357.602067265128
3
Iteration 10700: Loss = -12357.59982075761
4
Iteration 10800: Loss = -12357.599603522025
Iteration 10900: Loss = -12357.601522500407
1
Iteration 11000: Loss = -12357.600837260994
2
Iteration 11100: Loss = -12357.599983809054
3
Iteration 11200: Loss = -12357.68021103288
4
Iteration 11300: Loss = -12357.636587883979
5
Iteration 11400: Loss = -12357.659381303054
6
Iteration 11500: Loss = -12357.614284986137
7
Iteration 11600: Loss = -12357.60251795825
8
Iteration 11700: Loss = -12357.609553828508
9
Iteration 11800: Loss = -12357.604327110937
10
Iteration 11900: Loss = -12357.607625131646
11
Iteration 12000: Loss = -12357.605375967783
12
Iteration 12100: Loss = -12357.59987149442
13
Iteration 12200: Loss = -12357.599935302129
14
Iteration 12300: Loss = -12357.672662268626
15
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[0.0489, 0.9511],
        [0.0295, 0.9705]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9859, 0.0141], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1966, 0.1965],
         [0.6964, 0.1999]],

        [[0.5782, 0.2116],
         [0.6129, 0.7229]],

        [[0.6722, 0.2241],
         [0.6191, 0.5309]],

        [[0.6726, 0.2004],
         [0.5572, 0.5538]],

        [[0.5002, 0.1029],
         [0.6617, 0.6241]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
Global Adjusted Rand Index: 0.00018212976671821708
Average Adjusted Rand Index: 0.0011834947531591965
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19658.968779718438
Iteration 100: Loss = -12360.114992421742
Iteration 200: Loss = -12359.994098981444
Iteration 300: Loss = -12359.966115769816
Iteration 400: Loss = -12359.946340206261
Iteration 500: Loss = -12359.926523569447
Iteration 600: Loss = -12359.901583002415
Iteration 700: Loss = -12359.861362049818
Iteration 800: Loss = -12359.76955089176
Iteration 900: Loss = -12359.322461960208
Iteration 1000: Loss = -12358.034107200872
Iteration 1100: Loss = -12357.857151657776
Iteration 1200: Loss = -12357.825874339465
Iteration 1300: Loss = -12357.814825621095
Iteration 1400: Loss = -12357.809452191364
Iteration 1500: Loss = -12357.806145203258
Iteration 1600: Loss = -12357.803699263277
 84%|████████▍ | 84/100 [27:18:30<4:59:56, 1124.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 85%|████████▌ | 85/100 [27:40:10<4:54:21, 1177.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 86%|████████▌ | 86/100 [27:54:26<4:12:13, 1080.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 87%|████████▋ | 87/100 [28:12:12<3:53:15, 1076.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 88%|████████▊ | 88/100 [28:33:28<3:47:14, 1136.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 89%|████████▉ | 89/100 [28:54:43<3:35:56, 1177.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 90%|█████████ | 90/100 [29:11:25<3:07:33, 1125.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 91%|█████████ | 91/100 [29:26:29<2:38:49, 1058.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 92%|█████████▏| 92/100 [29:45:56<2:25:30, 1091.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 93%|█████████▎| 93/100 [30:03:28<2:05:55, 1079.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 94%|█████████▍| 94/100 [30:21:22<1:47:47, 1077.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 95%|█████████▌| 95/100 [30:42:15<1:34:11, 1130.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 96%|█████████▌| 96/100 [31:03:38<1:18:24, 1176.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 97%|█████████▋| 97/100 [31:23:19<58:53, 1177.77s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 98%|█████████▊| 98/100 [31:40:25<37:44, 1132.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 99%|█████████▉| 99/100 [32:01:51<19:38, 1178.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
100%|██████████| 100/100 [32:23:12<00:00, 1209.00s/it]100%|██████████| 100/100 [32:23:12<00:00, 1165.92s/it]
Iteration 1700: Loss = -12357.80150260364
Iteration 1800: Loss = -12357.79930519235
Iteration 1900: Loss = -12357.797092805664
Iteration 2000: Loss = -12357.79452972662
Iteration 2100: Loss = -12357.791393020369
Iteration 2200: Loss = -12357.78713138089
Iteration 2300: Loss = -12357.780113291607
Iteration 2400: Loss = -12357.765522760405
Iteration 2500: Loss = -12357.724124718025
Iteration 2600: Loss = -12357.665141604182
Iteration 2700: Loss = -12357.6355543622
Iteration 2800: Loss = -12357.621973484192
Iteration 2900: Loss = -12357.614980770206
Iteration 3000: Loss = -12357.610826310547
Iteration 3100: Loss = -12357.608180923156
Iteration 3200: Loss = -12357.606353562682
Iteration 3300: Loss = -12357.605034159586
Iteration 3400: Loss = -12357.604059592937
Iteration 3500: Loss = -12357.603319891983
Iteration 3600: Loss = -12357.602690532392
Iteration 3700: Loss = -12357.602236401042
Iteration 3800: Loss = -12357.601854293725
Iteration 3900: Loss = -12357.601557457001
Iteration 4000: Loss = -12357.601290841158
Iteration 4100: Loss = -12357.601073411828
Iteration 4200: Loss = -12357.600839717054
Iteration 4300: Loss = -12357.600683216811
Iteration 4400: Loss = -12357.600568269969
Iteration 4500: Loss = -12357.600447811908
Iteration 4600: Loss = -12357.600354020897
Iteration 4700: Loss = -12357.600263175378
Iteration 4800: Loss = -12357.600165654349
Iteration 4900: Loss = -12357.602213065895
1
Iteration 5000: Loss = -12357.600028578645
Iteration 5100: Loss = -12357.600141179291
1
Iteration 5200: Loss = -12357.605560608667
2
Iteration 5300: Loss = -12357.599876213853
Iteration 5400: Loss = -12357.599892820188
Iteration 5500: Loss = -12357.599846419605
Iteration 5600: Loss = -12357.599762679469
Iteration 5700: Loss = -12357.605299941213
1
Iteration 5800: Loss = -12357.599757487074
Iteration 5900: Loss = -12357.599714909868
Iteration 6000: Loss = -12357.599682917353
Iteration 6100: Loss = -12357.599695932155
Iteration 6200: Loss = -12357.599680910042
Iteration 6300: Loss = -12357.599638995736
Iteration 6400: Loss = -12357.599653609803
Iteration 6500: Loss = -12357.599620055884
Iteration 6600: Loss = -12357.600217137506
1
Iteration 6700: Loss = -12357.599616657511
Iteration 6800: Loss = -12357.59960489333
Iteration 6900: Loss = -12357.599868371168
1
Iteration 7000: Loss = -12357.602667703564
2
Iteration 7100: Loss = -12357.60027973951
3
Iteration 7200: Loss = -12357.600019747144
4
Iteration 7300: Loss = -12357.599529377792
Iteration 7400: Loss = -12357.599635338203
1
Iteration 7500: Loss = -12357.59951640481
Iteration 7600: Loss = -12357.600901596075
1
Iteration 7700: Loss = -12357.60478976489
2
Iteration 7800: Loss = -12357.603766350769
3
Iteration 7900: Loss = -12357.599545850677
Iteration 8000: Loss = -12357.601233490343
1
Iteration 8100: Loss = -12357.604321223616
2
Iteration 8200: Loss = -12357.606822874428
3
Iteration 8300: Loss = -12357.599500157889
Iteration 8400: Loss = -12357.599799787795
1
Iteration 8500: Loss = -12357.625051338813
2
Iteration 8600: Loss = -12357.599553733025
Iteration 8700: Loss = -12357.602454185931
1
Iteration 8800: Loss = -12357.600344233586
2
Iteration 8900: Loss = -12357.618812979279
3
Iteration 9000: Loss = -12357.601516505216
4
Iteration 9100: Loss = -12357.602410379592
5
Iteration 9200: Loss = -12357.600699934954
6
Iteration 9300: Loss = -12357.599625113915
Iteration 9400: Loss = -12357.601822945244
1
Iteration 9500: Loss = -12357.609816706668
2
Iteration 9600: Loss = -12357.599534542029
Iteration 9700: Loss = -12357.59993664838
1
Iteration 9800: Loss = -12357.599637445071
2
Iteration 9900: Loss = -12357.599588890853
Iteration 10000: Loss = -12357.59949305524
Iteration 10100: Loss = -12357.620777570077
1
Iteration 10200: Loss = -12357.600570562243
2
Iteration 10300: Loss = -12357.604785548127
3
Iteration 10400: Loss = -12357.606162331203
4
Iteration 10500: Loss = -12357.627944589462
5
Iteration 10600: Loss = -12357.60515856949
6
Iteration 10700: Loss = -12357.60066929669
7
Iteration 10800: Loss = -12357.599569325996
Iteration 10900: Loss = -12357.600750384017
1
Iteration 11000: Loss = -12357.601375083781
2
Iteration 11100: Loss = -12357.637775147015
3
Iteration 11200: Loss = -12357.721270371934
4
Iteration 11300: Loss = -12357.601615442645
5
Iteration 11400: Loss = -12357.599566814019
Iteration 11500: Loss = -12357.599606339016
Iteration 11600: Loss = -12357.600708283695
1
Iteration 11700: Loss = -12357.600351814232
2
Iteration 11800: Loss = -12357.608251617394
3
Iteration 11900: Loss = -12357.602145812154
4
Iteration 12000: Loss = -12357.599525918338
Iteration 12100: Loss = -12357.619061639158
1
Iteration 12200: Loss = -12357.599450068621
Iteration 12300: Loss = -12357.599964376323
1
Iteration 12400: Loss = -12357.605849574325
2
Iteration 12500: Loss = -12357.59946499345
Iteration 12600: Loss = -12357.62011169874
1
Iteration 12700: Loss = -12357.728959708349
2
Iteration 12800: Loss = -12357.599494494803
Iteration 12900: Loss = -12357.599809967518
1
Iteration 13000: Loss = -12357.61018734917
2
Iteration 13100: Loss = -12357.60801127809
3
Iteration 13200: Loss = -12357.599473012711
Iteration 13300: Loss = -12357.601216753548
1
Iteration 13400: Loss = -12357.599857674186
2
Iteration 13500: Loss = -12357.600073981373
3
Iteration 13600: Loss = -12357.602355389432
4
Iteration 13700: Loss = -12357.60077721707
5
Iteration 13800: Loss = -12357.599548821609
Iteration 13900: Loss = -12357.599831521873
1
Iteration 14000: Loss = -12357.609454102958
2
Iteration 14100: Loss = -12357.668538994896
3
Iteration 14200: Loss = -12357.612459020704
4
Iteration 14300: Loss = -12357.60073058518
5
Iteration 14400: Loss = -12357.599701764644
6
Iteration 14500: Loss = -12357.599674869023
7
Iteration 14600: Loss = -12357.67912310203
8
Iteration 14700: Loss = -12357.790172098248
9
Iteration 14800: Loss = -12357.62438108685
10
Iteration 14900: Loss = -12357.644892902696
11
Iteration 15000: Loss = -12357.599512099632
Iteration 15100: Loss = -12357.599513836765
Iteration 15200: Loss = -12357.60857883671
1
Iteration 15300: Loss = -12357.599514559104
Iteration 15400: Loss = -12357.604308390131
1
Iteration 15500: Loss = -12357.60019590864
2
Iteration 15600: Loss = -12357.599479516213
Iteration 15700: Loss = -12357.622744853563
1
Iteration 15800: Loss = -12357.622260530803
2
Iteration 15900: Loss = -12357.614957232048
3
Iteration 16000: Loss = -12357.605224672183
4
Iteration 16100: Loss = -12357.659560931514
5
Iteration 16200: Loss = -12357.769769941047
6
Iteration 16300: Loss = -12357.599574260912
Iteration 16400: Loss = -12357.599493847589
Iteration 16500: Loss = -12357.60416471409
1
Iteration 16600: Loss = -12357.637508650376
2
Iteration 16700: Loss = -12357.59945315957
Iteration 16800: Loss = -12357.599881985325
1
Iteration 16900: Loss = -12357.600371278248
2
Iteration 17000: Loss = -12357.60024551274
3
Iteration 17100: Loss = -12357.622550798304
4
Iteration 17200: Loss = -12357.632775250799
5
Iteration 17300: Loss = -12357.60677198693
6
Iteration 17400: Loss = -12357.627361158899
7
Iteration 17500: Loss = -12357.600683735644
8
Iteration 17600: Loss = -12357.60190460625
9
Iteration 17700: Loss = -12357.603708410252
10
Iteration 17800: Loss = -12357.635913292785
11
Iteration 17900: Loss = -12357.686788209481
12
Iteration 18000: Loss = -12357.633989294727
13
Iteration 18100: Loss = -12357.600893504152
14
Iteration 18200: Loss = -12357.59975585922
15
Stopping early at iteration 18200 due to no improvement.
pi: tensor([[0.0500, 0.9500],
        [0.0297, 0.9703]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9983, 0.0017], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1964, 0.1962],
         [0.7141, 0.2009]],

        [[0.5917, 0.2104],
         [0.7150, 0.6121]],

        [[0.7168, 0.2231],
         [0.6379, 0.6278]],

        [[0.6820, 0.1996],
         [0.7200, 0.6861]],

        [[0.7274, 0.1031],
         [0.5747, 0.5413]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
Global Adjusted Rand Index: 0.00018212976671821708
Average Adjusted Rand Index: 0.0011834947531591965
11862.760638320342
[0.00018212976671821708, 0.00018212976671821708] [0.0011834947531591965, 0.0011834947531591965] [12357.672662268626, 12357.59975585922]
-------------------------------------
This iteration is 84
True Objective function: Loss = -11667.034847144521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23596.96798959947
Iteration 100: Loss = -12184.64732991438
Iteration 200: Loss = -12183.424821156379
Iteration 300: Loss = -12183.073700902985
Iteration 400: Loss = -12182.943184140975
Iteration 500: Loss = -12182.878877810459
Iteration 600: Loss = -12182.838563777483
Iteration 700: Loss = -12182.809395093678
Iteration 800: Loss = -12182.786733250416
Iteration 900: Loss = -12182.768361796858
Iteration 1000: Loss = -12182.753105156877
Iteration 1100: Loss = -12182.740203872367
Iteration 1200: Loss = -12182.72902623963
Iteration 1300: Loss = -12182.71927335299
Iteration 1400: Loss = -12182.71058842539
Iteration 1500: Loss = -12182.70273647396
Iteration 1600: Loss = -12182.695538233895
Iteration 1700: Loss = -12182.688821365095
Iteration 1800: Loss = -12182.682442621266
Iteration 1900: Loss = -12182.676183829837
Iteration 2000: Loss = -12182.670056539506
Iteration 2100: Loss = -12182.663832201733
Iteration 2200: Loss = -12182.657421171107
Iteration 2300: Loss = -12182.650810031992
Iteration 2400: Loss = -12182.64380297512
Iteration 2500: Loss = -12182.636362213643
Iteration 2600: Loss = -12182.628446088445
Iteration 2700: Loss = -12182.620150946657
Iteration 2800: Loss = -12182.611402421919
Iteration 2900: Loss = -12182.60236040792
Iteration 3000: Loss = -12182.592833656588
Iteration 3100: Loss = -12182.582758066568
Iteration 3200: Loss = -12182.571998155092
Iteration 3300: Loss = -12182.56026884508
Iteration 3400: Loss = -12182.547442538864
Iteration 3500: Loss = -12182.53333782146
Iteration 3600: Loss = -12182.517890053323
Iteration 3700: Loss = -12182.500880963926
Iteration 3800: Loss = -12182.482034405386
Iteration 3900: Loss = -12182.46145814149
Iteration 4000: Loss = -12182.438961954289
Iteration 4100: Loss = -12182.414678991809
Iteration 4200: Loss = -12182.388363975128
Iteration 4300: Loss = -12182.359650833687
Iteration 4400: Loss = -12182.328373977307
Iteration 4500: Loss = -12182.294712400395
Iteration 4600: Loss = -12182.259314558572
Iteration 4700: Loss = -12182.22315555538
Iteration 4800: Loss = -12182.187204528242
Iteration 4900: Loss = -12182.147787510557
Iteration 5000: Loss = -12182.09855295436
Iteration 5100: Loss = -12182.041348591156
Iteration 5200: Loss = -12181.973016742751
Iteration 5300: Loss = -12181.936170610215
Iteration 5400: Loss = -12181.91367938456
Iteration 5500: Loss = -12181.93109208289
1
Iteration 5600: Loss = -12181.891100231996
Iteration 5700: Loss = -12181.884687638665
Iteration 5800: Loss = -12181.880566736048
Iteration 5900: Loss = -12181.877165562999
Iteration 6000: Loss = -12181.879610517994
1
Iteration 6100: Loss = -12181.872969426899
Iteration 6200: Loss = -12181.87347743334
1
Iteration 6300: Loss = -12181.870750664006
Iteration 6400: Loss = -12181.869307179235
Iteration 6500: Loss = -12181.868588247957
Iteration 6600: Loss = -12181.867833304825
Iteration 6700: Loss = -12181.878387624227
1
Iteration 6800: Loss = -12181.866732008775
Iteration 6900: Loss = -12181.866277402023
Iteration 7000: Loss = -12181.87911260565
1
Iteration 7100: Loss = -12181.865529474988
Iteration 7200: Loss = -12181.865252445405
Iteration 7300: Loss = -12181.871489002255
1
Iteration 7400: Loss = -12181.864687281504
Iteration 7500: Loss = -12181.883141147124
1
Iteration 7600: Loss = -12181.865297604325
2
Iteration 7700: Loss = -12181.864097348538
Iteration 7800: Loss = -12182.000927076691
1
Iteration 7900: Loss = -12181.863772853238
Iteration 8000: Loss = -12181.86415401038
1
Iteration 8100: Loss = -12181.870317891686
2
Iteration 8200: Loss = -12181.8633501153
Iteration 8300: Loss = -12181.863274278205
Iteration 8400: Loss = -12181.86313889268
Iteration 8500: Loss = -12181.871564398794
1
Iteration 8600: Loss = -12181.862931943522
Iteration 8700: Loss = -12181.877656313934
1
Iteration 8800: Loss = -12181.86281037157
Iteration 8900: Loss = -12181.863471739587
1
Iteration 9000: Loss = -12181.865788867352
2
Iteration 9100: Loss = -12181.862632952978
Iteration 9200: Loss = -12181.863016355943
1
Iteration 9300: Loss = -12181.86264543535
Iteration 9400: Loss = -12181.862495406767
Iteration 9500: Loss = -12181.878298784652
1
Iteration 9600: Loss = -12181.866913530195
2
Iteration 9700: Loss = -12181.865477376045
3
Iteration 9800: Loss = -12181.865256314148
4
Iteration 9900: Loss = -12181.868567271067
5
Iteration 10000: Loss = -12181.865613151904
6
Iteration 10100: Loss = -12181.862235157005
Iteration 10200: Loss = -12181.898186700886
1
Iteration 10300: Loss = -12181.868378797859
2
Iteration 10400: Loss = -12181.932713437896
3
Iteration 10500: Loss = -12181.86206994689
Iteration 10600: Loss = -12181.918267426247
1
Iteration 10700: Loss = -12181.86203024957
Iteration 10800: Loss = -12181.985116063914
1
Iteration 10900: Loss = -12181.861975176653
Iteration 11000: Loss = -12181.86409436224
1
Iteration 11100: Loss = -12181.866143781088
2
Iteration 11200: Loss = -12181.862498525476
3
Iteration 11300: Loss = -12181.861960793107
Iteration 11400: Loss = -12181.863258338126
1
Iteration 11500: Loss = -12181.862843081746
2
Iteration 11600: Loss = -12181.865351433542
3
Iteration 11700: Loss = -12181.867579447297
4
Iteration 11800: Loss = -12181.883575886863
5
Iteration 11900: Loss = -12181.869565132409
6
Iteration 12000: Loss = -12181.86171998628
Iteration 12100: Loss = -12181.907765555112
1
Iteration 12200: Loss = -12181.861643757877
Iteration 12300: Loss = -12181.879191848573
1
Iteration 12400: Loss = -12181.861613470439
Iteration 12500: Loss = -12181.861883194171
1
Iteration 12600: Loss = -12181.863130624542
2
Iteration 12700: Loss = -12181.863539559807
3
Iteration 12800: Loss = -12181.863917087054
4
Iteration 12900: Loss = -12181.871074639608
5
Iteration 13000: Loss = -12181.861501015104
Iteration 13100: Loss = -12181.862573217368
1
Iteration 13200: Loss = -12181.861523327287
Iteration 13300: Loss = -12181.86152035528
Iteration 13400: Loss = -12181.861739721602
1
Iteration 13500: Loss = -12181.861567249201
Iteration 13600: Loss = -12181.87617034389
1
Iteration 13700: Loss = -12181.908781747623
2
Iteration 13800: Loss = -12181.862337317227
3
Iteration 13900: Loss = -12181.862617030474
4
Iteration 14000: Loss = -12181.875257286889
5
Iteration 14100: Loss = -12181.86143697507
Iteration 14200: Loss = -12181.865987889301
1
Iteration 14300: Loss = -12181.863561745155
2
Iteration 14400: Loss = -12181.90406976414
3
Iteration 14500: Loss = -12181.861407917017
Iteration 14600: Loss = -12181.876013869758
1
Iteration 14700: Loss = -12181.88384717257
2
Iteration 14800: Loss = -12181.86752461645
3
Iteration 14900: Loss = -12181.863525099978
4
Iteration 15000: Loss = -12181.864154951236
5
Iteration 15100: Loss = -12181.88124186404
6
Iteration 15200: Loss = -12181.861909915831
7
Iteration 15300: Loss = -12181.861487363363
Iteration 15400: Loss = -12181.86197291314
1
Iteration 15500: Loss = -12181.862161588348
2
Iteration 15600: Loss = -12181.905801474053
3
Iteration 15700: Loss = -12181.861389832991
Iteration 15800: Loss = -12181.861781751888
1
Iteration 15900: Loss = -12181.874012216742
2
Iteration 16000: Loss = -12181.861467223838
Iteration 16100: Loss = -12181.882262783214
1
Iteration 16200: Loss = -12181.862372826816
2
Iteration 16300: Loss = -12181.932295821038
3
Iteration 16400: Loss = -12181.86136820836
Iteration 16500: Loss = -12181.861541127597
1
Iteration 16600: Loss = -12181.874707949151
2
Iteration 16700: Loss = -12181.862863666724
3
Iteration 16800: Loss = -12182.294833768077
4
Iteration 16900: Loss = -12181.861310525028
Iteration 17000: Loss = -12181.998166778274
1
Iteration 17100: Loss = -12181.861997478538
2
Iteration 17200: Loss = -12181.861378331389
Iteration 17300: Loss = -12181.862331646356
1
Iteration 17400: Loss = -12181.861713211661
2
Iteration 17500: Loss = -12181.867467447877
3
Iteration 17600: Loss = -12181.99939918182
4
Iteration 17700: Loss = -12181.861380947923
Iteration 17800: Loss = -12181.86958213264
1
Iteration 17900: Loss = -12181.865480958422
2
Iteration 18000: Loss = -12181.861784523391
3
Iteration 18100: Loss = -12181.873735859901
4
Iteration 18200: Loss = -12181.861314015412
Iteration 18300: Loss = -12181.865318254893
1
Iteration 18400: Loss = -12181.861532042854
2
Iteration 18500: Loss = -12181.872988359748
3
Iteration 18600: Loss = -12181.861453640602
4
Iteration 18700: Loss = -12181.863881526893
5
Iteration 18800: Loss = -12181.861683066136
6
Iteration 18900: Loss = -12181.876136268695
7
Iteration 19000: Loss = -12181.86130987761
Iteration 19100: Loss = -12181.862044551608
1
Iteration 19200: Loss = -12181.861861954925
2
Iteration 19300: Loss = -12181.865360985708
3
Iteration 19400: Loss = -12181.86201049034
4
Iteration 19500: Loss = -12181.86148816544
5
Iteration 19600: Loss = -12181.861319051419
Iteration 19700: Loss = -12181.863580019764
1
Iteration 19800: Loss = -12181.863168956952
2
Iteration 19900: Loss = -12181.87056748368
3
pi: tensor([[0.0130, 0.9870],
        [0.2087, 0.7913]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9984, 0.0016], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1899, 0.1893],
         [0.6184, 0.1963]],

        [[0.6982, 0.1981],
         [0.5628, 0.5618]],

        [[0.7287, 0.1929],
         [0.6309, 0.6687]],

        [[0.5430, 0.1826],
         [0.5154, 0.5978]],

        [[0.7125, 0.2015],
         [0.7020, 0.6001]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002179557964427878
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22608.29890736677
Iteration 100: Loss = -12183.3162859669
Iteration 200: Loss = -12182.95086197765
Iteration 300: Loss = -12182.849273874232
Iteration 400: Loss = -12182.800812642938
Iteration 500: Loss = -12182.769708750957
Iteration 600: Loss = -12182.746777752403
Iteration 700: Loss = -12182.728446546484
Iteration 800: Loss = -12182.712988659716
Iteration 900: Loss = -12182.699319143678
Iteration 1000: Loss = -12182.686919805408
Iteration 1100: Loss = -12182.675485763242
Iteration 1200: Loss = -12182.664867358444
Iteration 1300: Loss = -12182.654687725602
Iteration 1400: Loss = -12182.645045173791
Iteration 1500: Loss = -12182.635665344565
Iteration 1600: Loss = -12182.626549831344
Iteration 1700: Loss = -12182.617462099503
Iteration 1800: Loss = -12182.608408271146
Iteration 1900: Loss = -12182.599172542983
Iteration 2000: Loss = -12182.589674686764
Iteration 2100: Loss = -12182.579617465679
Iteration 2200: Loss = -12182.568848715186
Iteration 2300: Loss = -12182.557086157964
Iteration 2400: Loss = -12182.543970959061
Iteration 2500: Loss = -12182.529042764703
Iteration 2600: Loss = -12182.511923058775
Iteration 2700: Loss = -12182.492123642138
Iteration 2800: Loss = -12182.469184153175
Iteration 2900: Loss = -12182.442976958722
Iteration 3000: Loss = -12182.413583856012
Iteration 3100: Loss = -12182.381709459567
Iteration 3200: Loss = -12182.348440991336
Iteration 3300: Loss = -12182.315007619027
Iteration 3400: Loss = -12182.28279555308
Iteration 3500: Loss = -12182.253265445896
Iteration 3600: Loss = -12182.227617806846
Iteration 3700: Loss = -12182.206004831609
Iteration 3800: Loss = -12182.189126339454
Iteration 3900: Loss = -12182.172746562175
Iteration 4000: Loss = -12182.15940176076
Iteration 4100: Loss = -12182.147545601312
Iteration 4200: Loss = -12182.15429473801
1
Iteration 4300: Loss = -12182.128416172696
Iteration 4400: Loss = -12182.113770894302
Iteration 4500: Loss = -12182.064100380378
Iteration 4600: Loss = -12181.976550482606
Iteration 4700: Loss = -12181.925237379648
Iteration 4800: Loss = -12181.904871170638
Iteration 4900: Loss = -12181.930863220734
1
Iteration 5000: Loss = -12181.884956368553
Iteration 5100: Loss = -12181.880263701232
Iteration 5200: Loss = -12181.876835458246
Iteration 5300: Loss = -12181.874386278058
Iteration 5400: Loss = -12181.883138465873
1
Iteration 5500: Loss = -12181.870960075326
Iteration 5600: Loss = -12181.872524382552
1
Iteration 5700: Loss = -12181.868850524123
Iteration 5800: Loss = -12181.868074746215
Iteration 5900: Loss = -12181.867355972501
Iteration 6000: Loss = -12181.866771177776
Iteration 6100: Loss = -12181.86633672886
Iteration 6200: Loss = -12181.865880178673
Iteration 6300: Loss = -12181.866197347565
1
Iteration 6400: Loss = -12181.86521955485
Iteration 6500: Loss = -12181.864919244385
Iteration 6600: Loss = -12181.864628332665
Iteration 6700: Loss = -12181.864423436697
Iteration 6800: Loss = -12181.864229984765
Iteration 6900: Loss = -12181.864117549072
Iteration 7000: Loss = -12181.86408024688
Iteration 7100: Loss = -12181.86407824474
Iteration 7200: Loss = -12181.863522029407
Iteration 7300: Loss = -12181.86345882608
Iteration 7400: Loss = -12181.863320833121
Iteration 7500: Loss = -12181.863233397815
Iteration 7600: Loss = -12181.863065214617
Iteration 7700: Loss = -12181.863180188317
1
Iteration 7800: Loss = -12181.862915957354
Iteration 7900: Loss = -12181.86436476079
1
Iteration 8000: Loss = -12181.862725427023
Iteration 8100: Loss = -12181.86380097143
1
Iteration 8200: Loss = -12181.86268758221
Iteration 8300: Loss = -12181.862516292284
Iteration 8400: Loss = -12181.862479462072
Iteration 8500: Loss = -12181.862691659302
1
Iteration 8600: Loss = -12181.862822499319
2
Iteration 8700: Loss = -12181.897966104107
3
Iteration 8800: Loss = -12181.863854011015
4
Iteration 8900: Loss = -12181.881560310541
5
Iteration 9000: Loss = -12181.88468779724
6
Iteration 9100: Loss = -12181.912209049275
7
Iteration 9200: Loss = -12181.862410910757
Iteration 9300: Loss = -12181.862521588067
1
Iteration 9400: Loss = -12181.862439994511
Iteration 9500: Loss = -12181.863983924994
1
Iteration 9600: Loss = -12181.8639839062
2
Iteration 9700: Loss = -12181.862141659632
Iteration 9800: Loss = -12181.873853742922
1
Iteration 9900: Loss = -12181.946956869018
2
Iteration 10000: Loss = -12181.86501888268
3
Iteration 10100: Loss = -12181.862014586568
Iteration 10200: Loss = -12181.869936861536
1
Iteration 10300: Loss = -12182.112739262193
2
Iteration 10400: Loss = -12181.861868472748
Iteration 10500: Loss = -12181.881010802597
1
Iteration 10600: Loss = -12181.86183641615
Iteration 10700: Loss = -12181.866004595433
1
Iteration 10800: Loss = -12181.874324602817
2
Iteration 10900: Loss = -12181.89485708717
3
Iteration 11000: Loss = -12181.861711452424
Iteration 11100: Loss = -12181.867143985219
1
Iteration 11200: Loss = -12181.861906901202
2
Iteration 11300: Loss = -12181.87437010321
3
Iteration 11400: Loss = -12181.877809792877
4
Iteration 11500: Loss = -12181.861903473942
5
Iteration 11600: Loss = -12181.86448266599
6
Iteration 11700: Loss = -12181.867056318366
7
Iteration 11800: Loss = -12181.86611963766
8
Iteration 11900: Loss = -12181.904482568838
9
Iteration 12000: Loss = -12181.86165282089
Iteration 12100: Loss = -12181.861596121951
Iteration 12200: Loss = -12181.8637053205
1
Iteration 12300: Loss = -12181.862735844168
2
Iteration 12400: Loss = -12181.931087311712
3
Iteration 12500: Loss = -12181.861528121157
Iteration 12600: Loss = -12181.862569519091
1
Iteration 12700: Loss = -12181.866360594748
2
Iteration 12800: Loss = -12182.039889041715
3
Iteration 12900: Loss = -12181.861733302278
4
Iteration 13000: Loss = -12181.86154257498
Iteration 13100: Loss = -12182.073333432521
1
Iteration 13200: Loss = -12181.862482968665
2
Iteration 13300: Loss = -12181.864028239164
3
Iteration 13400: Loss = -12181.898488604493
4
Iteration 13500: Loss = -12181.86142861106
Iteration 13600: Loss = -12181.86168665652
1
Iteration 13700: Loss = -12181.863783744693
2
Iteration 13800: Loss = -12181.862334827701
3
Iteration 13900: Loss = -12181.952984113223
4
Iteration 14000: Loss = -12181.862213223285
5
Iteration 14100: Loss = -12181.867208942202
6
Iteration 14200: Loss = -12181.869781341506
7
Iteration 14300: Loss = -12181.863423571343
8
Iteration 14400: Loss = -12181.86140524585
Iteration 14500: Loss = -12181.861510193352
1
Iteration 14600: Loss = -12181.916260311691
2
Iteration 14700: Loss = -12181.899623234644
3
Iteration 14800: Loss = -12181.862782423274
4
Iteration 14900: Loss = -12181.956048992142
5
Iteration 15000: Loss = -12181.89555747392
6
Iteration 15100: Loss = -12181.86352689129
7
Iteration 15200: Loss = -12181.942483525128
8
Iteration 15300: Loss = -12181.885783396427
9
Iteration 15400: Loss = -12181.863052939267
10
Iteration 15500: Loss = -12181.969459130147
11
Iteration 15600: Loss = -12181.862290673067
12
Iteration 15700: Loss = -12181.862046032666
13
Iteration 15800: Loss = -12182.243940959592
14
Iteration 15900: Loss = -12181.861361850051
Iteration 16000: Loss = -12181.895192470056
1
Iteration 16100: Loss = -12181.863381648336
2
Iteration 16200: Loss = -12181.861422322685
Iteration 16300: Loss = -12181.862924126708
1
Iteration 16400: Loss = -12181.861383200114
Iteration 16500: Loss = -12181.862078448798
1
Iteration 16600: Loss = -12181.876612142034
2
Iteration 16700: Loss = -12181.861533734436
3
Iteration 16800: Loss = -12181.861387438814
Iteration 16900: Loss = -12182.18127721121
1
Iteration 17000: Loss = -12181.861336649463
Iteration 17100: Loss = -12181.861432915775
Iteration 17200: Loss = -12181.861399801743
Iteration 17300: Loss = -12181.866909336015
1
Iteration 17400: Loss = -12181.869760188982
2
Iteration 17500: Loss = -12181.861353378823
Iteration 17600: Loss = -12181.880682089419
1
Iteration 17700: Loss = -12181.864784336842
2
Iteration 17800: Loss = -12181.968403382212
3
Iteration 17900: Loss = -12181.861370220806
Iteration 18000: Loss = -12181.86336859238
1
Iteration 18100: Loss = -12181.863262409226
2
Iteration 18200: Loss = -12181.861556325091
3
Iteration 18300: Loss = -12181.862682009447
4
Iteration 18400: Loss = -12181.861409898256
Iteration 18500: Loss = -12181.879710998035
1
Iteration 18600: Loss = -12182.000173584725
2
Iteration 18700: Loss = -12181.863814352742
3
Iteration 18800: Loss = -12181.865794660094
4
Iteration 18900: Loss = -12181.861320686745
Iteration 19000: Loss = -12181.912995415298
1
Iteration 19100: Loss = -12181.861550118962
2
Iteration 19200: Loss = -12181.861456722096
3
Iteration 19300: Loss = -12181.901035451889
4
Iteration 19400: Loss = -12181.861592997708
5
Iteration 19500: Loss = -12181.861329019885
Iteration 19600: Loss = -12181.86154931319
1
Iteration 19700: Loss = -12181.862105623512
2
Iteration 19800: Loss = -12181.861551370974
3
Iteration 19900: Loss = -12181.863143353927
4
pi: tensor([[0.8008, 0.1992],
        [0.9883, 0.0117]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0012, 0.9988], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1958, 0.1892],
         [0.6267, 0.1902]],

        [[0.6341, 0.1990],
         [0.5173, 0.6966]],

        [[0.7291, 0.1932],
         [0.6753, 0.6930]],

        [[0.7274, 0.1824],
         [0.7112, 0.5155]],

        [[0.7174, 0.2021],
         [0.6486, 0.5957]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002179557964427878
Average Adjusted Rand Index: 0.0
11667.034847144521
[-0.002179557964427878, -0.002179557964427878] [0.0, 0.0] [12181.861371227835, 12181.878768369048]
-------------------------------------
This iteration is 85
True Objective function: Loss = -12064.444252594172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22826.81310098382
Iteration 100: Loss = -12521.462467972899
Iteration 200: Loss = -12520.84957898373
Iteration 300: Loss = -12520.42011142484
Iteration 400: Loss = -12519.558551231741
Iteration 500: Loss = -12518.158846421016
Iteration 600: Loss = -12515.803933291503
Iteration 700: Loss = -12507.781775269816
Iteration 800: Loss = -12285.108809185313
Iteration 900: Loss = -12131.491145779262
Iteration 1000: Loss = -12082.157788794932
Iteration 1100: Loss = -12071.731571728882
Iteration 1200: Loss = -12059.15726923378
Iteration 1300: Loss = -12059.051406803352
Iteration 1400: Loss = -12058.985861595209
Iteration 1500: Loss = -12058.932993767377
Iteration 1600: Loss = -12058.877450024447
Iteration 1700: Loss = -12050.992442824134
Iteration 1800: Loss = -12050.96725261293
Iteration 1900: Loss = -12050.950272563332
Iteration 2000: Loss = -12050.936181238361
Iteration 2100: Loss = -12050.924931579522
Iteration 2200: Loss = -12050.915998186954
Iteration 2300: Loss = -12050.908690447792
Iteration 2400: Loss = -12050.902928810998
Iteration 2500: Loss = -12050.896749801133
Iteration 2600: Loss = -12050.893384584964
Iteration 2700: Loss = -12050.88691813596
Iteration 2800: Loss = -12050.88178586927
Iteration 2900: Loss = -12050.878937242253
Iteration 3000: Loss = -12050.875658397823
Iteration 3100: Loss = -12050.872585810872
Iteration 3200: Loss = -12050.874117624398
1
Iteration 3300: Loss = -12050.868381274582
Iteration 3400: Loss = -12050.873996897995
1
Iteration 3500: Loss = -12050.864834118554
Iteration 3600: Loss = -12050.868588693473
1
Iteration 3700: Loss = -12050.861887784686
Iteration 3800: Loss = -12050.860597187211
Iteration 3900: Loss = -12050.859929021077
Iteration 4000: Loss = -12050.858276798783
Iteration 4100: Loss = -12050.85735064041
Iteration 4200: Loss = -12050.85642353597
Iteration 4300: Loss = -12050.859619412648
1
Iteration 4400: Loss = -12050.854702783836
Iteration 4500: Loss = -12050.854691318851
Iteration 4600: Loss = -12050.853258991896
Iteration 4700: Loss = -12050.853451839896
1
Iteration 4800: Loss = -12050.851668140233
Iteration 4900: Loss = -12050.850440670929
Iteration 5000: Loss = -12050.809619424217
Iteration 5100: Loss = -12050.796961726539
Iteration 5200: Loss = -12050.79569067439
Iteration 5300: Loss = -12050.792188669615
Iteration 5400: Loss = -12050.791587972373
Iteration 5500: Loss = -12050.787558987722
Iteration 5600: Loss = -12050.787090778555
Iteration 5700: Loss = -12050.786927931149
Iteration 5800: Loss = -12050.786440532494
Iteration 5900: Loss = -12050.786101104815
Iteration 6000: Loss = -12050.786192164782
Iteration 6100: Loss = -12050.791192812623
1
Iteration 6200: Loss = -12050.78652773772
2
Iteration 6300: Loss = -12050.789410855423
3
Iteration 6400: Loss = -12050.78659833115
4
Iteration 6500: Loss = -12050.785675525995
Iteration 6600: Loss = -12050.787730074504
1
Iteration 6700: Loss = -12050.785402774793
Iteration 6800: Loss = -12050.790229700026
1
Iteration 6900: Loss = -12050.78765550629
2
Iteration 7000: Loss = -12050.785926663617
3
Iteration 7100: Loss = -12050.790543757192
4
Iteration 7200: Loss = -12050.789632885047
5
Iteration 7300: Loss = -12050.78854916901
6
Iteration 7400: Loss = -12050.96467051874
7
Iteration 7500: Loss = -12050.783292489412
Iteration 7600: Loss = -12050.796535036287
1
Iteration 7700: Loss = -12050.784357590434
2
Iteration 7800: Loss = -12050.868421076184
3
Iteration 7900: Loss = -12050.782905756427
Iteration 8000: Loss = -12050.788462461589
1
Iteration 8100: Loss = -12050.783447174532
2
Iteration 8200: Loss = -12050.782351873591
Iteration 8300: Loss = -12050.781764363323
Iteration 8400: Loss = -12050.782112652936
1
Iteration 8500: Loss = -12050.7815269274
Iteration 8600: Loss = -12050.787275780527
1
Iteration 8700: Loss = -12050.782117409877
2
Iteration 8800: Loss = -12050.781427975846
Iteration 8900: Loss = -12050.7918421816
1
Iteration 9000: Loss = -12050.785184007973
2
Iteration 9100: Loss = -12050.78124947008
Iteration 9200: Loss = -12050.784368606612
1
Iteration 9300: Loss = -12050.78110207447
Iteration 9400: Loss = -12050.781375645318
1
Iteration 9500: Loss = -12050.781039445017
Iteration 9600: Loss = -12050.781380396787
1
Iteration 9700: Loss = -12050.7959627793
2
Iteration 9800: Loss = -12050.795540055775
3
Iteration 9900: Loss = -12050.784963158896
4
Iteration 10000: Loss = -12050.781436886204
5
Iteration 10100: Loss = -12050.781453770853
6
Iteration 10200: Loss = -12050.797451236427
7
Iteration 10300: Loss = -12050.840694660568
8
Iteration 10400: Loss = -12050.781662657864
9
Iteration 10500: Loss = -12050.784291169777
10
Iteration 10600: Loss = -12050.781724940518
11
Iteration 10700: Loss = -12050.773481859698
Iteration 10800: Loss = -12050.781302035311
1
Iteration 10900: Loss = -12050.775558571317
2
Iteration 11000: Loss = -12050.77898484877
3
Iteration 11100: Loss = -12050.780588817302
4
Iteration 11200: Loss = -12050.774149175424
5
Iteration 11300: Loss = -12050.781879602075
6
Iteration 11400: Loss = -12050.821667518587
7
Iteration 11500: Loss = -12050.773557811108
Iteration 11600: Loss = -12050.776794610192
1
Iteration 11700: Loss = -12050.804475925645
2
Iteration 11800: Loss = -12050.772719026589
Iteration 11900: Loss = -12050.781776292046
1
Iteration 12000: Loss = -12050.802435467149
2
Iteration 12100: Loss = -12050.78116365177
3
Iteration 12200: Loss = -12050.774352049093
4
Iteration 12300: Loss = -12050.782668787107
5
Iteration 12400: Loss = -12050.785898615795
6
Iteration 12500: Loss = -12050.77651402009
7
Iteration 12600: Loss = -12050.774358591816
8
Iteration 12700: Loss = -12050.786394037825
9
Iteration 12800: Loss = -12050.772980311458
10
Iteration 12900: Loss = -12050.779816105713
11
Iteration 13000: Loss = -12050.776265868595
12
Iteration 13100: Loss = -12050.808455446513
13
Iteration 13200: Loss = -12050.773116690458
14
Iteration 13300: Loss = -12050.774143414297
15
Stopping early at iteration 13300 due to no improvement.
pi: tensor([[0.7056, 0.2944],
        [0.1868, 0.8132]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3770, 0.6230], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3059, 0.1124],
         [0.5592, 0.2913]],

        [[0.5311, 0.0991],
         [0.6364, 0.6301]],

        [[0.6932, 0.1001],
         [0.6803, 0.6015]],

        [[0.5784, 0.1022],
         [0.5409, 0.5935]],

        [[0.6124, 0.0991],
         [0.6726, 0.5153]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9206289602688308
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9594778233178521
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760244164760237
Average Adjusted Rand Index: 0.9760213567173366
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21462.584371248526
Iteration 100: Loss = -12521.522506331272
Iteration 200: Loss = -12521.019043012291
Iteration 300: Loss = -12520.371126697737
Iteration 400: Loss = -12519.473673194663
Iteration 500: Loss = -12519.29375951987
Iteration 600: Loss = -12519.02996202025
Iteration 700: Loss = -12518.563341935493
Iteration 800: Loss = -12517.012284095694
Iteration 900: Loss = -12516.512110890804
Iteration 1000: Loss = -12516.065200142244
Iteration 1100: Loss = -12483.375983972339
Iteration 1200: Loss = -12139.714372826658
Iteration 1300: Loss = -12101.449060212683
Iteration 1400: Loss = -12083.965000280305
Iteration 1500: Loss = -12072.88445127269
Iteration 1600: Loss = -12068.183638239452
Iteration 1700: Loss = -12064.040283297678
Iteration 1800: Loss = -12059.089209278722
Iteration 1900: Loss = -12059.066041333703
Iteration 2000: Loss = -12059.045871321207
Iteration 2100: Loss = -12050.94316921586
Iteration 2200: Loss = -12050.88740005527
Iteration 2300: Loss = -12050.870960270522
Iteration 2400: Loss = -12050.859831406859
Iteration 2500: Loss = -12050.84005433469
Iteration 2600: Loss = -12050.834298712529
Iteration 2700: Loss = -12050.830156975386
Iteration 2800: Loss = -12050.826024593665
Iteration 2900: Loss = -12050.821899762825
Iteration 3000: Loss = -12050.819191274652
Iteration 3100: Loss = -12050.817298828204
Iteration 3200: Loss = -12050.815695759315
Iteration 3300: Loss = -12050.814217018744
Iteration 3400: Loss = -12050.812850035798
Iteration 3500: Loss = -12050.811827225249
Iteration 3600: Loss = -12050.810449641142
Iteration 3700: Loss = -12050.809288784301
Iteration 3800: Loss = -12050.808077118996
Iteration 3900: Loss = -12050.806273543956
Iteration 4000: Loss = -12050.80471701495
Iteration 4100: Loss = -12050.80217359435
Iteration 4200: Loss = -12050.800486224567
Iteration 4300: Loss = -12050.798915798736
Iteration 4400: Loss = -12050.80072634119
1
Iteration 4500: Loss = -12050.800148078522
2
Iteration 4600: Loss = -12050.79778231795
Iteration 4700: Loss = -12050.7986207561
1
Iteration 4800: Loss = -12050.796234914964
Iteration 4900: Loss = -12050.794650033735
Iteration 5000: Loss = -12050.794063786956
Iteration 5100: Loss = -12050.795741254502
1
Iteration 5200: Loss = -12050.793774591528
Iteration 5300: Loss = -12050.793252049645
Iteration 5400: Loss = -12050.79666203742
1
Iteration 5500: Loss = -12050.792775114014
Iteration 5600: Loss = -12050.792298460263
Iteration 5700: Loss = -12050.792664912151
1
Iteration 5800: Loss = -12050.793606250947
2
Iteration 5900: Loss = -12050.791191416874
Iteration 6000: Loss = -12050.790778136776
Iteration 6100: Loss = -12050.790855369969
Iteration 6200: Loss = -12050.79446546257
1
Iteration 6300: Loss = -12050.789546589753
Iteration 6400: Loss = -12050.78453520483
Iteration 6500: Loss = -12050.873358991323
1
Iteration 6600: Loss = -12050.785104758568
2
Iteration 6700: Loss = -12050.785146135138
3
Iteration 6800: Loss = -12050.78593187255
4
Iteration 6900: Loss = -12050.783736153773
Iteration 7000: Loss = -12050.788358136146
1
Iteration 7100: Loss = -12050.784121062596
2
Iteration 7200: Loss = -12050.789061743766
3
Iteration 7300: Loss = -12050.783808517102
Iteration 7400: Loss = -12050.787182495882
1
Iteration 7500: Loss = -12050.785767915098
2
Iteration 7600: Loss = -12050.782740661845
Iteration 7700: Loss = -12050.79352089095
1
Iteration 7800: Loss = -12050.782504329309
Iteration 7900: Loss = -12050.782665337567
1
Iteration 8000: Loss = -12050.798430139334
2
Iteration 8100: Loss = -12050.784341911189
3
Iteration 8200: Loss = -12050.782798684417
4
Iteration 8300: Loss = -12050.786273667514
5
Iteration 8400: Loss = -12050.786689583543
6
Iteration 8500: Loss = -12050.783226491905
7
Iteration 8600: Loss = -12050.878921977133
8
Iteration 8700: Loss = -12050.778256409913
Iteration 8800: Loss = -12050.77861351921
1
Iteration 8900: Loss = -12050.777765676703
Iteration 9000: Loss = -12050.786699092681
1
Iteration 9100: Loss = -12050.777693698219
Iteration 9200: Loss = -12050.779971310016
1
Iteration 9300: Loss = -12050.82419811784
2
Iteration 9400: Loss = -12050.810084639746
3
Iteration 9500: Loss = -12050.778909334218
4
Iteration 9600: Loss = -12050.778037268426
5
Iteration 9700: Loss = -12050.780870115228
6
Iteration 9800: Loss = -12050.77446729522
Iteration 9900: Loss = -12050.77943116613
1
Iteration 10000: Loss = -12050.782306346719
2
Iteration 10100: Loss = -12050.77536038995
3
Iteration 10200: Loss = -12050.77881309214
4
Iteration 10300: Loss = -12050.780097839834
5
Iteration 10400: Loss = -12050.788205891615
6
Iteration 10500: Loss = -12050.776281224124
7
Iteration 10600: Loss = -12050.814177676411
8
Iteration 10700: Loss = -12050.777108529594
9
Iteration 10800: Loss = -12050.77618262291
10
Iteration 10900: Loss = -12050.941023919362
11
Iteration 11000: Loss = -12050.77383563932
Iteration 11100: Loss = -12050.784756324385
1
Iteration 11200: Loss = -12050.776995114931
2
Iteration 11300: Loss = -12050.801268330024
3
Iteration 11400: Loss = -12050.827678222395
4
Iteration 11500: Loss = -12050.776049796
5
Iteration 11600: Loss = -12050.773311841529
Iteration 11700: Loss = -12050.775050997952
1
Iteration 11800: Loss = -12050.959635581212
2
Iteration 11900: Loss = -12050.77326424554
Iteration 12000: Loss = -12050.773977866218
1
Iteration 12100: Loss = -12050.99578727022
2
Iteration 12200: Loss = -12050.773439443205
3
Iteration 12300: Loss = -12050.786183646533
4
Iteration 12400: Loss = -12050.77597233093
5
Iteration 12500: Loss = -12050.774195122174
6
Iteration 12600: Loss = -12050.77501954769
7
Iteration 12700: Loss = -12050.77371828066
8
Iteration 12800: Loss = -12050.789351472145
9
Iteration 12900: Loss = -12050.778438095336
10
Iteration 13000: Loss = -12050.776664691697
11
Iteration 13100: Loss = -12050.788069933215
12
Iteration 13200: Loss = -12050.826878333357
13
Iteration 13300: Loss = -12050.777031187357
14
Iteration 13400: Loss = -12050.773456891171
15
Stopping early at iteration 13400 due to no improvement.
pi: tensor([[0.8131, 0.1869],
        [0.2950, 0.7050]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6223, 0.3777], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2914, 0.1126],
         [0.5814, 0.3058]],

        [[0.6654, 0.0991],
         [0.5096, 0.5140]],

        [[0.7007, 0.1001],
         [0.7298, 0.5561]],

        [[0.5237, 0.1020],
         [0.7127, 0.5700]],

        [[0.5929, 0.0991],
         [0.6575, 0.6390]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9206289602688308
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9594778233178521
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760244164760237
Average Adjusted Rand Index: 0.9760213567173366
12064.444252594172
[0.9760244164760237, 0.9760244164760237] [0.9760213567173366, 0.9760213567173366] [12050.774143414297, 12050.773456891171]
-------------------------------------
This iteration is 86
True Objective function: Loss = -11934.126602971859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22981.490747770466
Iteration 100: Loss = -12466.905963129291
Iteration 200: Loss = -12466.369655103816
Iteration 300: Loss = -12466.03387818836
Iteration 400: Loss = -12465.597939787449
Iteration 500: Loss = -12465.172367746887
Iteration 600: Loss = -12464.748795696612
Iteration 700: Loss = -12464.332754494482
Iteration 800: Loss = -12463.976640763945
Iteration 900: Loss = -12463.664362489346
Iteration 1000: Loss = -12463.371067489777
Iteration 1100: Loss = -12463.12098916292
Iteration 1200: Loss = -12462.93571694229
Iteration 1300: Loss = -12462.80768903156
Iteration 1400: Loss = -12462.716115181673
Iteration 1500: Loss = -12462.65140232173
Iteration 1600: Loss = -12462.606736718768
Iteration 1700: Loss = -12462.576001226469
Iteration 1800: Loss = -12462.554704614795
Iteration 1900: Loss = -12462.539085343522
Iteration 2000: Loss = -12462.529251719825
Iteration 2100: Loss = -12462.5219520631
Iteration 2200: Loss = -12462.516199794385
Iteration 2300: Loss = -12462.511442444626
Iteration 2400: Loss = -12462.507261737004
Iteration 2500: Loss = -12462.503523833333
Iteration 2600: Loss = -12462.500108901577
Iteration 2700: Loss = -12462.497032859215
Iteration 2800: Loss = -12462.494229966349
Iteration 2900: Loss = -12462.491555283681
Iteration 3000: Loss = -12462.48927058265
Iteration 3100: Loss = -12462.486943431968
Iteration 3200: Loss = -12462.484914588504
Iteration 3300: Loss = -12462.509572142028
1
Iteration 3400: Loss = -12462.481270342687
Iteration 3500: Loss = -12462.479675412204
Iteration 3600: Loss = -12462.487340311995
1
Iteration 3700: Loss = -12462.476761834218
Iteration 3800: Loss = -12462.475504351898
Iteration 3900: Loss = -12462.47424829014
Iteration 4000: Loss = -12462.473180247895
Iteration 4100: Loss = -12462.47202155615
Iteration 4200: Loss = -12462.48750807315
1
Iteration 4300: Loss = -12462.470032143234
Iteration 4400: Loss = -12462.469100766746
Iteration 4500: Loss = -12462.469203935541
1
Iteration 4600: Loss = -12462.467413690274
Iteration 4700: Loss = -12462.466619265668
Iteration 4800: Loss = -12462.465846870586
Iteration 4900: Loss = -12462.465149191316
Iteration 5000: Loss = -12462.484263668088
1
Iteration 5100: Loss = -12462.463812936714
Iteration 5200: Loss = -12462.46323639678
Iteration 5300: Loss = -12462.46274935246
Iteration 5400: Loss = -12462.462079529876
Iteration 5500: Loss = -12462.462042808751
Iteration 5600: Loss = -12462.46105594727
Iteration 5700: Loss = -12462.462884848841
1
Iteration 5800: Loss = -12462.460125155885
Iteration 5900: Loss = -12462.459738276508
Iteration 6000: Loss = -12462.459362798254
Iteration 6100: Loss = -12462.458960561507
Iteration 6200: Loss = -12462.462559819305
1
Iteration 6300: Loss = -12462.458257883642
Iteration 6400: Loss = -12462.458473770297
1
Iteration 6500: Loss = -12462.457701596179
Iteration 6600: Loss = -12462.457403126939
Iteration 6700: Loss = -12462.457147696394
Iteration 6800: Loss = -12462.456905236213
Iteration 6900: Loss = -12462.45692154351
Iteration 7000: Loss = -12462.456431391383
Iteration 7100: Loss = -12462.459321324834
1
Iteration 7200: Loss = -12462.456029019271
Iteration 7300: Loss = -12462.472762035124
1
Iteration 7400: Loss = -12462.455674190318
Iteration 7500: Loss = -12462.475286246185
1
Iteration 7600: Loss = -12462.45535854793
Iteration 7700: Loss = -12462.456776370951
1
Iteration 7800: Loss = -12462.456817092383
2
Iteration 7900: Loss = -12462.454956834523
Iteration 8000: Loss = -12462.895161625585
1
Iteration 8100: Loss = -12462.45468060976
Iteration 8200: Loss = -12462.454602581214
Iteration 8300: Loss = -12462.45625805178
1
Iteration 8400: Loss = -12462.454399545602
Iteration 8500: Loss = -12462.454298304821
Iteration 8600: Loss = -12462.455787952289
1
Iteration 8700: Loss = -12462.454113040942
Iteration 8800: Loss = -12462.45400336105
Iteration 8900: Loss = -12462.453954009712
Iteration 9000: Loss = -12462.52473508896
1
Iteration 9100: Loss = -12462.467791108105
2
Iteration 9200: Loss = -12462.657205887519
3
Iteration 9300: Loss = -12462.456376434517
4
Iteration 9400: Loss = -12462.454134885262
5
Iteration 9500: Loss = -12462.4623616209
6
Iteration 9600: Loss = -12462.455681835314
7
Iteration 9700: Loss = -12462.45484608505
8
Iteration 9800: Loss = -12462.453540616394
Iteration 9900: Loss = -12462.547299366497
1
Iteration 10000: Loss = -12462.469812050545
2
Iteration 10100: Loss = -12462.453412733466
Iteration 10200: Loss = -12462.45403564996
1
Iteration 10300: Loss = -12462.503953887184
2
Iteration 10400: Loss = -12462.453189738391
Iteration 10500: Loss = -12462.478457087838
1
Iteration 10600: Loss = -12462.454103458978
2
Iteration 10700: Loss = -12462.453145043986
Iteration 10800: Loss = -12462.453303331049
1
Iteration 10900: Loss = -12462.46541993675
2
Iteration 11000: Loss = -12462.526881032303
3
Iteration 11100: Loss = -12462.48051850051
4
Iteration 11200: Loss = -12462.47489301824
5
Iteration 11300: Loss = -12462.453028924783
Iteration 11400: Loss = -12462.453166186806
1
Iteration 11500: Loss = -12462.469654675162
2
Iteration 11600: Loss = -12462.528593657189
3
Iteration 11700: Loss = -12462.453432264812
4
Iteration 11800: Loss = -12462.47035563794
5
Iteration 11900: Loss = -12462.452891161349
Iteration 12000: Loss = -12462.453782231834
1
Iteration 12100: Loss = -12462.453049563857
2
Iteration 12200: Loss = -12462.453080995907
3
Iteration 12300: Loss = -12462.452977839193
Iteration 12400: Loss = -12462.454541647236
1
Iteration 12500: Loss = -12462.453566393036
2
Iteration 12600: Loss = -12462.453458655846
3
Iteration 12700: Loss = -12462.4527587625
Iteration 12800: Loss = -12462.453918742574
1
Iteration 12900: Loss = -12462.461748967427
2
Iteration 13000: Loss = -12462.484248599996
3
Iteration 13100: Loss = -12462.51290003651
4
Iteration 13200: Loss = -12462.452739619706
Iteration 13300: Loss = -12462.458611420629
1
Iteration 13400: Loss = -12462.454395237035
2
Iteration 13500: Loss = -12462.453988164427
3
Iteration 13600: Loss = -12462.45761567344
4
Iteration 13700: Loss = -12462.45474663563
5
Iteration 13800: Loss = -12462.45508433063
6
Iteration 13900: Loss = -12462.491155856318
7
Iteration 14000: Loss = -12462.454631156183
8
Iteration 14100: Loss = -12462.463169552218
9
Iteration 14200: Loss = -12462.454413748534
10
Iteration 14300: Loss = -12462.455926902612
11
Iteration 14400: Loss = -12462.45272883512
Iteration 14500: Loss = -12462.452723758668
Iteration 14600: Loss = -12462.534535417697
1
Iteration 14700: Loss = -12462.452618242041
Iteration 14800: Loss = -12462.477560242898
1
Iteration 14900: Loss = -12462.453629498314
2
Iteration 15000: Loss = -12462.45279476788
3
Iteration 15100: Loss = -12462.452608888156
Iteration 15200: Loss = -12462.461133124492
1
Iteration 15300: Loss = -12462.452780237372
2
Iteration 15400: Loss = -12462.454920617505
3
Iteration 15500: Loss = -12462.45264310047
Iteration 15600: Loss = -12462.463598681574
1
Iteration 15700: Loss = -12462.457800915156
2
Iteration 15800: Loss = -12462.453174969743
3
Iteration 15900: Loss = -12462.462081307776
4
Iteration 16000: Loss = -12462.498350847227
5
Iteration 16100: Loss = -12462.454594618504
6
Iteration 16200: Loss = -12462.464507887125
7
Iteration 16300: Loss = -12462.458448351239
8
Iteration 16400: Loss = -12462.452737357871
Iteration 16500: Loss = -12462.452861718652
1
Iteration 16600: Loss = -12462.452878005188
2
Iteration 16700: Loss = -12462.454190672255
3
Iteration 16800: Loss = -12462.45607857418
4
Iteration 16900: Loss = -12462.456834489725
5
Iteration 17000: Loss = -12462.46718643507
6
Iteration 17100: Loss = -12462.452768281652
Iteration 17200: Loss = -12462.452868539565
1
Iteration 17300: Loss = -12462.462652964141
2
Iteration 17400: Loss = -12462.927418111898
3
Iteration 17500: Loss = -12462.45254987126
Iteration 17600: Loss = -12462.452944788016
1
Iteration 17700: Loss = -12462.452626531178
Iteration 17800: Loss = -12462.462023919408
1
Iteration 17900: Loss = -12462.456780378274
2
Iteration 18000: Loss = -12462.524326962854
3
Iteration 18100: Loss = -12462.452823853891
4
Iteration 18200: Loss = -12462.622484685464
5
Iteration 18300: Loss = -12462.453168337926
6
Iteration 18400: Loss = -12462.452531597242
Iteration 18500: Loss = -12462.455530548055
1
Iteration 18600: Loss = -12462.480553864501
2
Iteration 18700: Loss = -12462.452494153662
Iteration 18800: Loss = -12462.472378191698
1
Iteration 18900: Loss = -12462.454612245761
2
Iteration 19000: Loss = -12462.453907362684
3
Iteration 19100: Loss = -12462.458876220313
4
Iteration 19200: Loss = -12462.452966890502
5
Iteration 19300: Loss = -12462.465993831702
6
Iteration 19400: Loss = -12462.4706853487
7
Iteration 19500: Loss = -12462.453253538364
8
Iteration 19600: Loss = -12462.452920856944
9
Iteration 19700: Loss = -12462.452941583251
10
Iteration 19800: Loss = -12462.452651698355
11
Iteration 19900: Loss = -12462.481091310698
12
pi: tensor([[3.9997e-06, 1.0000e+00],
        [1.8606e-01, 8.1394e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0256, 0.9744], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2316, 0.2188],
         [0.6513, 0.1981]],

        [[0.5333, 0.2286],
         [0.5900, 0.6954]],

        [[0.5511, 0.1936],
         [0.6314, 0.7300]],

        [[0.7059, 0.1913],
         [0.6371, 0.6612]],

        [[0.5559, 0.2361],
         [0.5365, 0.5055]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 38
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013692364884659383
Average Adjusted Rand Index: -0.0016467245245553673
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20956.858746535843
Iteration 100: Loss = -12466.904162358567
Iteration 200: Loss = -12466.220305517605
Iteration 300: Loss = -12465.780528031637
Iteration 400: Loss = -12465.14120178062
Iteration 500: Loss = -12464.466272446884
Iteration 600: Loss = -12463.98093915793
Iteration 700: Loss = -12463.633858602532
Iteration 800: Loss = -12463.346525184315
Iteration 900: Loss = -12463.086577817283
Iteration 1000: Loss = -12462.879627670996
Iteration 1100: Loss = -12462.743293913505
Iteration 1200: Loss = -12462.661698179325
Iteration 1300: Loss = -12462.610563091659
Iteration 1400: Loss = -12462.576633445015
Iteration 1500: Loss = -12462.553555640312
Iteration 1600: Loss = -12462.537636140643
Iteration 1700: Loss = -12462.527544756898
Iteration 1800: Loss = -12462.520282832416
Iteration 1900: Loss = -12462.51447637094
Iteration 2000: Loss = -12462.509639638733
Iteration 2100: Loss = -12462.505385963195
Iteration 2200: Loss = -12462.501553921718
Iteration 2300: Loss = -12462.498102753761
Iteration 2400: Loss = -12462.49499878798
Iteration 2500: Loss = -12462.492126839485
Iteration 2600: Loss = -12462.489516127363
Iteration 2700: Loss = -12462.487137282982
Iteration 2800: Loss = -12462.484896949052
Iteration 2900: Loss = -12462.48280028344
Iteration 3000: Loss = -12462.481038396176
Iteration 3100: Loss = -12462.481984871005
1
Iteration 3200: Loss = -12462.477692196837
Iteration 3300: Loss = -12462.476218294662
Iteration 3400: Loss = -12462.475047537742
Iteration 3500: Loss = -12462.47361957216
Iteration 3600: Loss = -12462.472431858969
Iteration 3700: Loss = -12462.47134285271
Iteration 3800: Loss = -12462.470287537675
Iteration 3900: Loss = -12462.472669100565
1
Iteration 4000: Loss = -12462.468406477592
Iteration 4100: Loss = -12462.467500397594
Iteration 4200: Loss = -12462.466677983026
Iteration 4300: Loss = -12462.465911029907
Iteration 4400: Loss = -12462.465567294144
Iteration 4500: Loss = -12462.46450199732
Iteration 4600: Loss = -12462.463817666106
Iteration 4700: Loss = -12462.463206068733
Iteration 4800: Loss = -12462.462555533088
Iteration 4900: Loss = -12462.462014272423
Iteration 5000: Loss = -12462.461461946677
Iteration 5100: Loss = -12462.46632607166
1
Iteration 5200: Loss = -12462.460500281055
Iteration 5300: Loss = -12462.460093048048
Iteration 5400: Loss = -12462.459748248359
Iteration 5500: Loss = -12462.459256657148
Iteration 5600: Loss = -12462.459650244879
1
Iteration 5700: Loss = -12462.458534092644
Iteration 5800: Loss = -12462.458189643785
Iteration 5900: Loss = -12462.459506244453
1
Iteration 6000: Loss = -12462.457589242567
Iteration 6100: Loss = -12462.457327484024
Iteration 6200: Loss = -12462.45958385291
1
Iteration 6300: Loss = -12462.45684628379
Iteration 6400: Loss = -12462.469285873367
1
Iteration 6500: Loss = -12462.456389693989
Iteration 6600: Loss = -12462.456210085993
Iteration 6700: Loss = -12462.456037915095
Iteration 6800: Loss = -12462.455801990329
Iteration 6900: Loss = -12462.483591132988
1
Iteration 7000: Loss = -12462.455426447676
Iteration 7100: Loss = -12462.455379330959
Iteration 7200: Loss = -12462.455135345257
Iteration 7300: Loss = -12462.466311962227
1
Iteration 7400: Loss = -12462.52839554759
2
Iteration 7500: Loss = -12462.455602943455
3
Iteration 7600: Loss = -12462.4546920849
Iteration 7700: Loss = -12462.485888232099
1
Iteration 7800: Loss = -12462.454448406945
Iteration 7900: Loss = -12462.454330823673
Iteration 8000: Loss = -12462.454355276715
Iteration 8100: Loss = -12462.454183983857
Iteration 8200: Loss = -12462.454232186554
Iteration 8300: Loss = -12462.454023519758
Iteration 8400: Loss = -12462.45389146954
Iteration 8500: Loss = -12462.486218863387
1
Iteration 8600: Loss = -12462.45378242481
Iteration 8700: Loss = -12462.453824019925
Iteration 8800: Loss = -12462.453786507725
Iteration 8900: Loss = -12462.45361942418
Iteration 9000: Loss = -12462.454424413361
1
Iteration 9100: Loss = -12462.453509285167
Iteration 9200: Loss = -12462.454258164156
1
Iteration 9300: Loss = -12462.49613065759
2
Iteration 9400: Loss = -12462.453350741465
Iteration 9500: Loss = -12462.455815165817
1
Iteration 9600: Loss = -12462.688385517262
2
Iteration 9700: Loss = -12462.453324677279
Iteration 9800: Loss = -12462.45371508847
1
Iteration 9900: Loss = -12462.63346845773
2
Iteration 10000: Loss = -12462.453166890304
Iteration 10100: Loss = -12462.454903605982
1
Iteration 10200: Loss = -12462.453393034331
2
Iteration 10300: Loss = -12462.54320279867
3
Iteration 10400: Loss = -12462.453112105764
Iteration 10500: Loss = -12462.453792213702
1
Iteration 10600: Loss = -12462.472563961746
2
Iteration 10700: Loss = -12462.453200607422
Iteration 10800: Loss = -12462.453001520606
Iteration 10900: Loss = -12462.453969415932
1
Iteration 11000: Loss = -12462.499540486431
2
Iteration 11100: Loss = -12462.452918029265
Iteration 11200: Loss = -12462.453170605091
1
Iteration 11300: Loss = -12462.471703636542
2
Iteration 11400: Loss = -12462.45299211387
Iteration 11500: Loss = -12462.452891012848
Iteration 11600: Loss = -12462.462468451136
1
Iteration 11700: Loss = -12462.519898283954
2
Iteration 11800: Loss = -12462.45280372458
Iteration 11900: Loss = -12462.456202410283
1
Iteration 12000: Loss = -12462.473809741961
2
Iteration 12100: Loss = -12462.452911075348
3
Iteration 12200: Loss = -12462.466097719987
4
Iteration 12300: Loss = -12462.453756088216
5
Iteration 12400: Loss = -12462.453269529704
6
Iteration 12500: Loss = -12462.457157301398
7
Iteration 12600: Loss = -12462.460990804602
8
Iteration 12700: Loss = -12462.500863055675
9
Iteration 12800: Loss = -12462.472102608657
10
Iteration 12900: Loss = -12462.467539209221
11
Iteration 13000: Loss = -12462.45445492479
12
Iteration 13100: Loss = -12462.453299589846
13
Iteration 13200: Loss = -12462.452931311684
14
Iteration 13300: Loss = -12462.511382606486
15
Stopping early at iteration 13300 due to no improvement.
pi: tensor([[8.1742e-01, 1.8258e-01],
        [9.9991e-01, 8.6837e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9744, 0.0256], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.2183],
         [0.5295, 0.2315]],

        [[0.6676, 0.2294],
         [0.5782, 0.7105]],

        [[0.7041, 0.1933],
         [0.7013, 0.6136]],

        [[0.5759, 0.1907],
         [0.6657, 0.6895]],

        [[0.5779, 0.2369],
         [0.6888, 0.7166]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013692364884659383
Average Adjusted Rand Index: -0.0016467245245553673
11934.126602971859
[-0.0013692364884659383, -0.0013692364884659383] [-0.0016467245245553673, -0.0016467245245553673] [12462.455574293313, 12462.511382606486]
-------------------------------------
This iteration is 87
True Objective function: Loss = -11920.296709561346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24173.28759071016
Iteration 100: Loss = -12397.697088608335
Iteration 200: Loss = -12397.394153466308
Iteration 300: Loss = -12397.340726996896
Iteration 400: Loss = -12397.304403186869
Iteration 500: Loss = -12397.26987894213
Iteration 600: Loss = -12397.235371262217
Iteration 700: Loss = -12397.20350592579
Iteration 800: Loss = -12397.17705833296
Iteration 900: Loss = -12397.152473563448
Iteration 1000: Loss = -12397.124249710854
Iteration 1100: Loss = -12397.08826592622
Iteration 1200: Loss = -12397.0440764121
Iteration 1300: Loss = -12396.991272824864
Iteration 1400: Loss = -12396.92316098165
Iteration 1500: Loss = -12396.830966863428
Iteration 1600: Loss = -12396.690657207591
Iteration 1700: Loss = -12396.572852222758
Iteration 1800: Loss = -12396.534308963506
Iteration 1900: Loss = -12396.51567414467
Iteration 2000: Loss = -12396.504364747247
Iteration 2100: Loss = -12396.496607637831
Iteration 2200: Loss = -12396.490752300562
Iteration 2300: Loss = -12396.48619929922
Iteration 2400: Loss = -12396.48240110395
Iteration 2500: Loss = -12396.479342206683
Iteration 2600: Loss = -12396.476705795347
Iteration 2700: Loss = -12396.474518347502
Iteration 2800: Loss = -12396.472679331875
Iteration 2900: Loss = -12396.471029563847
Iteration 3000: Loss = -12396.469639522253
Iteration 3100: Loss = -12396.468438434367
Iteration 3200: Loss = -12396.474253980983
1
Iteration 3300: Loss = -12396.466397089349
Iteration 3400: Loss = -12396.465584274034
Iteration 3500: Loss = -12396.464891099768
Iteration 3600: Loss = -12396.464178558881
Iteration 3700: Loss = -12396.463825414057
Iteration 3800: Loss = -12396.462956964695
Iteration 3900: Loss = -12396.462887640306
Iteration 4000: Loss = -12396.462051552759
Iteration 4100: Loss = -12396.461606558358
Iteration 4200: Loss = -12396.461224223256
Iteration 4300: Loss = -12396.460885201883
Iteration 4400: Loss = -12396.46061357058
Iteration 4500: Loss = -12396.46023385
Iteration 4600: Loss = -12396.459993726252
Iteration 4700: Loss = -12396.459753309178
Iteration 4800: Loss = -12396.459485326905
Iteration 4900: Loss = -12396.459305635182
Iteration 5000: Loss = -12396.459049683603
Iteration 5100: Loss = -12396.46061394763
1
Iteration 5200: Loss = -12396.458713744423
Iteration 5300: Loss = -12396.458515229326
Iteration 5400: Loss = -12396.458448602438
Iteration 5500: Loss = -12396.458216671268
Iteration 5600: Loss = -12396.458191196778
Iteration 5700: Loss = -12396.457968090674
Iteration 5800: Loss = -12396.457890986074
Iteration 5900: Loss = -12396.457735782125
Iteration 6000: Loss = -12396.45973613367
1
Iteration 6100: Loss = -12396.457547891232
Iteration 6200: Loss = -12396.457475810445
Iteration 6300: Loss = -12396.457416071325
Iteration 6400: Loss = -12396.457338240172
Iteration 6500: Loss = -12396.469469658892
1
Iteration 6600: Loss = -12396.457135755236
Iteration 6700: Loss = -12396.457412576987
1
Iteration 6800: Loss = -12396.456973441454
Iteration 6900: Loss = -12396.457191376345
1
Iteration 7000: Loss = -12396.456892978897
Iteration 7100: Loss = -12396.47491681491
1
Iteration 7200: Loss = -12396.456870344553
Iteration 7300: Loss = -12396.457105685698
1
Iteration 7400: Loss = -12396.456743054285
Iteration 7500: Loss = -12396.456644956115
Iteration 7600: Loss = -12396.456630889284
Iteration 7700: Loss = -12396.456700083963
Iteration 7800: Loss = -12396.456758519982
Iteration 7900: Loss = -12396.48324673161
1
Iteration 8000: Loss = -12396.457024802392
2
Iteration 8100: Loss = -12396.45898716407
3
Iteration 8200: Loss = -12396.47906157467
4
Iteration 8300: Loss = -12396.45640620572
Iteration 8400: Loss = -12396.457631072864
1
Iteration 8500: Loss = -12396.456354349748
Iteration 8600: Loss = -12396.46026051747
1
Iteration 8700: Loss = -12396.45630291096
Iteration 8800: Loss = -12396.558551037057
1
Iteration 8900: Loss = -12396.456236481123
Iteration 9000: Loss = -12396.45634923426
1
Iteration 9100: Loss = -12396.456255361041
Iteration 9200: Loss = -12396.529394132422
1
Iteration 9300: Loss = -12396.456155203115
Iteration 9400: Loss = -12396.5445649853
1
Iteration 9500: Loss = -12396.45619961113
Iteration 9600: Loss = -12396.456150659245
Iteration 9700: Loss = -12396.456539866092
1
Iteration 9800: Loss = -12396.45613935044
Iteration 9900: Loss = -12396.475199858147
1
Iteration 10000: Loss = -12396.45612568024
Iteration 10100: Loss = -12396.456106982367
Iteration 10200: Loss = -12396.456281059454
1
Iteration 10300: Loss = -12396.45856024024
2
Iteration 10400: Loss = -12396.502638465354
3
Iteration 10500: Loss = -12396.456063338695
Iteration 10600: Loss = -12396.479858347295
1
Iteration 10700: Loss = -12396.456045725872
Iteration 10800: Loss = -12396.485848597868
1
Iteration 10900: Loss = -12396.456065265014
Iteration 11000: Loss = -12396.64991930297
1
Iteration 11100: Loss = -12396.456038758684
Iteration 11200: Loss = -12396.5278569553
1
Iteration 11300: Loss = -12396.456029068055
Iteration 11400: Loss = -12396.456196844336
1
Iteration 11500: Loss = -12396.456087946555
Iteration 11600: Loss = -12396.456088960602
Iteration 11700: Loss = -12396.571000513179
1
Iteration 11800: Loss = -12396.65492965438
2
Iteration 11900: Loss = -12396.456018467807
Iteration 12000: Loss = -12396.456635470613
1
Iteration 12100: Loss = -12396.47511653792
2
Iteration 12200: Loss = -12396.456223064744
3
Iteration 12300: Loss = -12396.4561003989
Iteration 12400: Loss = -12396.465104109437
1
Iteration 12500: Loss = -12396.455995314895
Iteration 12600: Loss = -12396.458258950119
1
Iteration 12700: Loss = -12396.456734701991
2
Iteration 12800: Loss = -12396.487584698616
3
Iteration 12900: Loss = -12396.455937514398
Iteration 13000: Loss = -12396.463201149896
1
Iteration 13100: Loss = -12396.455916396075
Iteration 13200: Loss = -12396.45692511698
1
Iteration 13300: Loss = -12396.455970811841
Iteration 13400: Loss = -12396.456029971705
Iteration 13500: Loss = -12396.458823883873
1
Iteration 13600: Loss = -12396.455989983131
Iteration 13700: Loss = -12396.469135542713
1
Iteration 13800: Loss = -12396.45592385948
Iteration 13900: Loss = -12396.4566283705
1
Iteration 14000: Loss = -12396.456409793012
2
Iteration 14100: Loss = -12396.456935714345
3
Iteration 14200: Loss = -12396.455931951074
Iteration 14300: Loss = -12396.458051441416
1
Iteration 14400: Loss = -12396.464589897803
2
Iteration 14500: Loss = -12396.467193530305
3
Iteration 14600: Loss = -12396.455886987223
Iteration 14700: Loss = -12396.456200915307
1
Iteration 14800: Loss = -12396.58701124179
2
Iteration 14900: Loss = -12396.455966458581
Iteration 15000: Loss = -12396.456084581643
1
Iteration 15100: Loss = -12396.46211787803
2
Iteration 15200: Loss = -12396.462271033995
3
Iteration 15300: Loss = -12396.455929608044
Iteration 15400: Loss = -12396.456371565222
1
Iteration 15500: Loss = -12396.485177657454
2
Iteration 15600: Loss = -12396.457886560202
3
Iteration 15700: Loss = -12396.455952023054
Iteration 15800: Loss = -12396.456698065927
1
Iteration 15900: Loss = -12396.657245946479
2
Iteration 16000: Loss = -12396.455982925034
Iteration 16100: Loss = -12396.457104868123
1
Iteration 16200: Loss = -12396.464119435352
2
Iteration 16300: Loss = -12396.476336624759
3
Iteration 16400: Loss = -12396.457203268183
4
Iteration 16500: Loss = -12396.570290301173
5
Iteration 16600: Loss = -12396.455899568544
Iteration 16700: Loss = -12396.456097283064
1
Iteration 16800: Loss = -12396.4559051909
Iteration 16900: Loss = -12396.456331776339
1
Iteration 17000: Loss = -12396.457708299324
2
Iteration 17100: Loss = -12396.456272811256
3
Iteration 17200: Loss = -12396.491202034478
4
Iteration 17300: Loss = -12396.45729243565
5
Iteration 17400: Loss = -12396.496847121767
6
Iteration 17500: Loss = -12396.45675663529
7
Iteration 17600: Loss = -12396.455901823505
Iteration 17700: Loss = -12396.456978566237
1
Iteration 17800: Loss = -12396.456800231741
2
Iteration 17900: Loss = -12396.664326678627
3
Iteration 18000: Loss = -12396.456840023993
4
Iteration 18100: Loss = -12396.456089501147
5
Iteration 18200: Loss = -12396.45932638243
6
Iteration 18300: Loss = -12396.457741918784
7
Iteration 18400: Loss = -12396.455923616142
Iteration 18500: Loss = -12396.561363732948
1
Iteration 18600: Loss = -12396.455872467373
Iteration 18700: Loss = -12396.474747106062
1
Iteration 18800: Loss = -12396.455900525458
Iteration 18900: Loss = -12396.455936560447
Iteration 19000: Loss = -12396.456098751745
1
Iteration 19100: Loss = -12396.455907516902
Iteration 19200: Loss = -12396.97886012803
1
Iteration 19300: Loss = -12396.455906392968
Iteration 19400: Loss = -12396.45587597413
Iteration 19500: Loss = -12396.456016345275
1
Iteration 19600: Loss = -12396.455880506073
Iteration 19700: Loss = -12396.735847673739
1
Iteration 19800: Loss = -12396.45594260517
Iteration 19900: Loss = -12396.456751448426
1
pi: tensor([[4.5717e-01, 5.4283e-01],
        [3.7301e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3994, 0.6006], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.2037],
         [0.6512, 0.2026]],

        [[0.5147, 0.1912],
         [0.5734, 0.7270]],

        [[0.6877, 0.1772],
         [0.7106, 0.6802]],

        [[0.6887, 0.1342],
         [0.6320, 0.5411]],

        [[0.7044, 0.2657],
         [0.6666, 0.6766]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.006701438343003505
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.02868762140155424
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0004097267352653994
Average Adjusted Rand Index: 0.005628882214739645
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18964.208527626684
Iteration 100: Loss = -12397.684818402697
Iteration 200: Loss = -12397.433464841968
Iteration 300: Loss = -12397.412496827212
Iteration 400: Loss = -12397.394970895943
Iteration 500: Loss = -12397.37547563135
Iteration 600: Loss = -12397.350363368736
Iteration 700: Loss = -12397.31796531054
Iteration 800: Loss = -12397.29280406246
Iteration 900: Loss = -12397.280647170657
Iteration 1000: Loss = -12397.269655832906
Iteration 1100: Loss = -12397.257590080964
Iteration 1200: Loss = -12397.244514490372
Iteration 1300: Loss = -12397.230372335296
Iteration 1400: Loss = -12397.213954564731
Iteration 1500: Loss = -12397.19278009339
Iteration 1600: Loss = -12397.165893666783
Iteration 1700: Loss = -12397.13803769304
Iteration 1800: Loss = -12397.115095121158
Iteration 1900: Loss = -12397.096556294358
Iteration 2000: Loss = -12397.079418326712
Iteration 2100: Loss = -12397.061320466311
Iteration 2200: Loss = -12397.037833060818
Iteration 2300: Loss = -12396.988646736181
Iteration 2400: Loss = -12396.751804107053
Iteration 2500: Loss = -12396.609254700092
Iteration 2600: Loss = -12396.557902034801
Iteration 2700: Loss = -12396.531695738384
Iteration 2800: Loss = -12396.515379533876
Iteration 2900: Loss = -12396.503656757988
Iteration 3000: Loss = -12396.495030329452
Iteration 3100: Loss = -12396.488765288874
Iteration 3200: Loss = -12396.483957609064
Iteration 3300: Loss = -12396.480216652946
Iteration 3400: Loss = -12396.477275249228
Iteration 3500: Loss = -12396.474732935854
Iteration 3600: Loss = -12396.48423033857
1
Iteration 3700: Loss = -12396.470955595298
Iteration 3800: Loss = -12396.469459177932
Iteration 3900: Loss = -12396.46819099474
Iteration 4000: Loss = -12396.479472706405
1
Iteration 4100: Loss = -12396.466087236773
Iteration 4200: Loss = -12396.465318001867
Iteration 4300: Loss = -12396.46563033493
1
Iteration 4400: Loss = -12396.463808391603
Iteration 4500: Loss = -12396.46345293062
Iteration 4600: Loss = -12396.462694426002
Iteration 4700: Loss = -12396.462187684041
Iteration 4800: Loss = -12396.477766567577
1
Iteration 4900: Loss = -12396.461346932696
Iteration 5000: Loss = -12396.460979527248
Iteration 5100: Loss = -12396.460644597315
Iteration 5200: Loss = -12396.494612212917
1
Iteration 5300: Loss = -12396.460024217944
Iteration 5400: Loss = -12396.460572748541
1
Iteration 5500: Loss = -12396.459483132796
Iteration 5600: Loss = -12396.45929833838
Iteration 5700: Loss = -12396.45912393899
Iteration 5800: Loss = -12396.45889991046
Iteration 5900: Loss = -12396.458687744045
Iteration 6000: Loss = -12396.458498076307
Iteration 6100: Loss = -12396.458635852638
1
Iteration 6200: Loss = -12396.458230651811
Iteration 6300: Loss = -12396.458059071578
Iteration 6400: Loss = -12396.458145112796
Iteration 6500: Loss = -12396.457860282488
Iteration 6600: Loss = -12396.457741074631
Iteration 6700: Loss = -12396.458906969026
1
Iteration 6800: Loss = -12396.457688836983
Iteration 6900: Loss = -12396.457765388202
Iteration 7000: Loss = -12396.476114602296
1
Iteration 7100: Loss = -12396.458116873004
2
Iteration 7200: Loss = -12396.45914471785
3
Iteration 7300: Loss = -12396.46020836524
4
Iteration 7400: Loss = -12396.63705087603
5
Iteration 7500: Loss = -12396.457001173872
Iteration 7600: Loss = -12396.457012486831
Iteration 7700: Loss = -12396.457829371939
1
Iteration 7800: Loss = -12396.456860900322
Iteration 7900: Loss = -12396.507704229596
1
Iteration 8000: Loss = -12396.456782119752
Iteration 8100: Loss = -12396.45676381468
Iteration 8200: Loss = -12396.456733516565
Iteration 8300: Loss = -12396.473427024406
1
Iteration 8400: Loss = -12396.459708112037
2
Iteration 8500: Loss = -12396.456633787742
Iteration 8600: Loss = -12396.457953220191
1
Iteration 8700: Loss = -12396.615578534822
2
Iteration 8800: Loss = -12396.45652288396
Iteration 8900: Loss = -12396.46092720035
1
Iteration 9000: Loss = -12396.536580555729
2
Iteration 9100: Loss = -12396.456647929579
3
Iteration 9200: Loss = -12396.476356474017
4
Iteration 9300: Loss = -12396.463896671326
5
Iteration 9400: Loss = -12396.57273181427
6
Iteration 9500: Loss = -12396.456272220072
Iteration 9600: Loss = -12396.456388211447
1
Iteration 9700: Loss = -12396.46912478332
2
Iteration 9800: Loss = -12396.456213509893
Iteration 9900: Loss = -12396.461330216833
1
Iteration 10000: Loss = -12396.615306599215
2
Iteration 10100: Loss = -12396.467600226035
3
Iteration 10200: Loss = -12396.458847906957
4
Iteration 10300: Loss = -12396.45622621248
Iteration 10400: Loss = -12396.45661647533
1
Iteration 10500: Loss = -12396.456130856051
Iteration 10600: Loss = -12396.45722400929
1
Iteration 10700: Loss = -12396.537016171116
2
Iteration 10800: Loss = -12396.460518269096
3
Iteration 10900: Loss = -12396.456082732559
Iteration 11000: Loss = -12396.456598445611
1
Iteration 11100: Loss = -12396.456089435755
Iteration 11200: Loss = -12396.459591817842
1
Iteration 11300: Loss = -12396.456058239171
Iteration 11400: Loss = -12396.456048910715
Iteration 11500: Loss = -12396.456921786936
1
Iteration 11600: Loss = -12396.456819728393
2
Iteration 11700: Loss = -12396.499359359786
3
Iteration 11800: Loss = -12396.456071920482
Iteration 11900: Loss = -12396.456020075992
Iteration 12000: Loss = -12396.456339508155
1
Iteration 12100: Loss = -12396.499933439765
2
Iteration 12200: Loss = -12396.457441633323
3
Iteration 12300: Loss = -12396.45636829222
4
Iteration 12400: Loss = -12396.467207369633
5
Iteration 12500: Loss = -12396.455944420808
Iteration 12600: Loss = -12396.457503406547
1
Iteration 12700: Loss = -12396.45715840323
2
Iteration 12800: Loss = -12396.455998766165
Iteration 12900: Loss = -12396.463593733904
1
Iteration 13000: Loss = -12396.45598519306
Iteration 13100: Loss = -12396.456547091682
1
Iteration 13200: Loss = -12396.457148840162
2
Iteration 13300: Loss = -12396.456041557542
Iteration 13400: Loss = -12396.49433906029
1
Iteration 13500: Loss = -12396.50976347407
2
Iteration 13600: Loss = -12396.458378883635
3
Iteration 13700: Loss = -12396.455953413684
Iteration 13800: Loss = -12396.456473438418
1
Iteration 13900: Loss = -12396.460117790886
2
Iteration 14000: Loss = -12396.509923563475
3
Iteration 14100: Loss = -12396.456349605829
4
Iteration 14200: Loss = -12396.455917787209
Iteration 14300: Loss = -12396.460126067399
1
Iteration 14400: Loss = -12396.482676826126
2
Iteration 14500: Loss = -12396.787406288451
3
Iteration 14600: Loss = -12396.455927437306
Iteration 14700: Loss = -12396.4652630478
1
Iteration 14800: Loss = -12396.465625121786
2
Iteration 14900: Loss = -12396.545805962769
3
Iteration 15000: Loss = -12396.480236013766
4
Iteration 15100: Loss = -12396.499547697846
5
Iteration 15200: Loss = -12396.455908826772
Iteration 15300: Loss = -12396.45779782785
1
Iteration 15400: Loss = -12396.456146897786
2
Iteration 15500: Loss = -12396.45627311047
3
Iteration 15600: Loss = -12396.457516632187
4
Iteration 15700: Loss = -12396.455894454155
Iteration 15800: Loss = -12396.458976497908
1
Iteration 15900: Loss = -12396.460524971133
2
Iteration 16000: Loss = -12396.45594772368
Iteration 16100: Loss = -12396.457405201189
1
Iteration 16200: Loss = -12396.45591358706
Iteration 16300: Loss = -12396.461218429575
1
Iteration 16400: Loss = -12396.456831231471
2
Iteration 16500: Loss = -12396.48040554864
3
Iteration 16600: Loss = -12396.46931417074
4
Iteration 16700: Loss = -12396.647566775846
5
Iteration 16800: Loss = -12396.459055489855
6
Iteration 16900: Loss = -12396.455926510202
Iteration 17000: Loss = -12396.459477403198
1
Iteration 17100: Loss = -12396.460066004369
2
Iteration 17200: Loss = -12396.470237945172
3
Iteration 17300: Loss = -12396.455899922432
Iteration 17400: Loss = -12396.45721554221
1
Iteration 17500: Loss = -12396.670041311556
2
Iteration 17600: Loss = -12396.455938491416
Iteration 17700: Loss = -12396.456245998055
1
Iteration 17800: Loss = -12396.457600612852
2
Iteration 17900: Loss = -12396.466774861245
3
Iteration 18000: Loss = -12396.457934247097
4
Iteration 18100: Loss = -12396.455908180033
Iteration 18200: Loss = -12396.456127464564
1
Iteration 18300: Loss = -12396.466772280633
2
Iteration 18400: Loss = -12396.456164554824
3
Iteration 18500: Loss = -12396.45588895968
Iteration 18600: Loss = -12396.481474384795
1
Iteration 18700: Loss = -12396.505982279852
2
Iteration 18800: Loss = -12396.45618927355
3
Iteration 18900: Loss = -12396.456107593516
4
Iteration 19000: Loss = -12396.467749031553
5
Iteration 19100: Loss = -12396.470227329055
6
Iteration 19200: Loss = -12396.65630666981
7
Iteration 19300: Loss = -12396.455940555627
Iteration 19400: Loss = -12396.456478473669
1
Iteration 19500: Loss = -12396.478633639326
2
Iteration 19600: Loss = -12396.46745407593
3
Iteration 19700: Loss = -12396.456103026478
4
Iteration 19800: Loss = -12396.490582900959
5
Iteration 19900: Loss = -12396.470679467955
6
pi: tensor([[4.5348e-01, 5.4652e-01],
        [5.1871e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3989, 0.6011], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2013, 0.2055],
         [0.5128, 0.2005]],

        [[0.6551, 0.1934],
         [0.6246, 0.7227]],

        [[0.6204, 0.1795],
         [0.5748, 0.7282]],

        [[0.5817, 0.1343],
         [0.6255, 0.7251]],

        [[0.5523, 0.2680],
         [0.6803, 0.6124]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.006701438343003505
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.018335442091456804
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00021560637723492266
Average Adjusted Rand Index: 0.0035584463527201576
11920.296709561346
[0.0004097267352653994, 0.00021560637723492266] [0.005628882214739645, 0.0035584463527201576] [12396.457259634839, 12396.791396206012]
-------------------------------------
This iteration is 88
True Objective function: Loss = -11784.801612897678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19121.046296988112
Iteration 100: Loss = -12305.915910285825
Iteration 200: Loss = -12305.628418793704
Iteration 300: Loss = -12305.579501415092
Iteration 400: Loss = -12305.552225265921
Iteration 500: Loss = -12305.533187339053
Iteration 600: Loss = -12305.518634115653
Iteration 700: Loss = -12305.506410341604
Iteration 800: Loss = -12305.49411735322
Iteration 900: Loss = -12305.477588920141
Iteration 1000: Loss = -12305.43829179809
Iteration 1100: Loss = -12305.047468016424
Iteration 1200: Loss = -12302.90999680444
Iteration 1300: Loss = -12302.631114677315
Iteration 1400: Loss = -12302.500730888294
Iteration 1500: Loss = -12302.411918309976
Iteration 1600: Loss = -12302.361747725545
Iteration 1700: Loss = -12302.335812372332
Iteration 1800: Loss = -12302.323144599744
Iteration 1900: Loss = -12302.316906028418
Iteration 2000: Loss = -12302.31353585781
Iteration 2100: Loss = -12302.311515107527
Iteration 2200: Loss = -12302.309911873563
Iteration 2300: Loss = -12302.308500460515
Iteration 2400: Loss = -12302.3069123039
Iteration 2500: Loss = -12302.304888972922
Iteration 2600: Loss = -12302.301793164474
Iteration 2700: Loss = -12302.295822708815
Iteration 2800: Loss = -12302.27729079873
Iteration 2900: Loss = -12301.501861009936
Iteration 3000: Loss = -12299.790428258242
Iteration 3100: Loss = -12299.491679391072
Iteration 3200: Loss = -12299.315165742099
Iteration 3300: Loss = -12299.219583623842
Iteration 3400: Loss = -12299.166641416674
Iteration 3500: Loss = -12299.135526729704
Iteration 3600: Loss = -12299.116081009111
Iteration 3700: Loss = -12299.103137386786
Iteration 3800: Loss = -12299.093986101116
Iteration 3900: Loss = -12299.087191346647
Iteration 4000: Loss = -12299.081868815734
Iteration 4100: Loss = -12299.07762880188
Iteration 4200: Loss = -12299.074042052209
Iteration 4300: Loss = -12299.070995105165
Iteration 4400: Loss = -12299.068303021617
Iteration 4500: Loss = -12299.066056190524
Iteration 4600: Loss = -12299.064181983733
Iteration 4700: Loss = -12299.062633717635
Iteration 4800: Loss = -12299.061312067875
Iteration 4900: Loss = -12299.060106430003
Iteration 5000: Loss = -12299.059085409326
Iteration 5100: Loss = -12299.058100776074
Iteration 5200: Loss = -12299.057309199216
Iteration 5300: Loss = -12299.056566684958
Iteration 5400: Loss = -12299.055927114425
Iteration 5500: Loss = -12299.055290098173
Iteration 5600: Loss = -12299.054701491606
Iteration 5700: Loss = -12299.05422514786
Iteration 5800: Loss = -12299.053774557018
Iteration 5900: Loss = -12299.053325726254
Iteration 6000: Loss = -12299.052949064147
Iteration 6100: Loss = -12299.052569225014
Iteration 6200: Loss = -12299.052232299826
Iteration 6300: Loss = -12299.051896733488
Iteration 6400: Loss = -12299.051634015661
Iteration 6500: Loss = -12299.051358163531
Iteration 6600: Loss = -12299.051127908395
Iteration 6700: Loss = -12299.050865405585
Iteration 6800: Loss = -12299.053569576006
1
Iteration 6900: Loss = -12299.050528890624
Iteration 7000: Loss = -12299.050244703822
Iteration 7100: Loss = -12299.050101842286
Iteration 7200: Loss = -12299.116444191899
1
Iteration 7300: Loss = -12299.049749416918
Iteration 7400: Loss = -12299.049621081782
Iteration 7500: Loss = -12299.049501418014
Iteration 7600: Loss = -12299.049392198705
Iteration 7700: Loss = -12299.049251810837
Iteration 7800: Loss = -12299.049167737714
Iteration 7900: Loss = -12299.075352586276
1
Iteration 8000: Loss = -12299.048940302971
Iteration 8100: Loss = -12299.048859376473
Iteration 8200: Loss = -12299.04875487317
Iteration 8300: Loss = -12299.050997747061
1
Iteration 8400: Loss = -12299.048566262672
Iteration 8500: Loss = -12299.048511783845
Iteration 8600: Loss = -12299.054987326308
1
Iteration 8700: Loss = -12299.048408281655
Iteration 8800: Loss = -12299.048339649515
Iteration 8900: Loss = -12299.04826466525
Iteration 9000: Loss = -12299.049122684253
1
Iteration 9100: Loss = -12299.048152184247
Iteration 9200: Loss = -12299.048124195391
Iteration 9300: Loss = -12299.428311435655
1
Iteration 9400: Loss = -12299.048028573394
Iteration 9500: Loss = -12299.047963462575
Iteration 9600: Loss = -12299.047928396163
Iteration 9700: Loss = -12299.0479026541
Iteration 9800: Loss = -12299.047814485963
Iteration 9900: Loss = -12299.047807280102
Iteration 10000: Loss = -12299.072102359776
1
Iteration 10100: Loss = -12299.047729781343
Iteration 10200: Loss = -12299.047689922334
Iteration 10300: Loss = -12299.047658864836
Iteration 10400: Loss = -12299.050117182222
1
Iteration 10500: Loss = -12299.047574631211
Iteration 10600: Loss = -12299.047530701093
Iteration 10700: Loss = -12299.162993599562
1
Iteration 10800: Loss = -12299.047457968123
Iteration 10900: Loss = -12299.047429907458
Iteration 11000: Loss = -12299.047410521065
Iteration 11100: Loss = -12299.047441576638
Iteration 11200: Loss = -12299.047367233572
Iteration 11300: Loss = -12299.047375800174
Iteration 11400: Loss = -12299.047361520436
Iteration 11500: Loss = -12299.12584586405
1
Iteration 11600: Loss = -12299.04735679384
Iteration 11700: Loss = -12299.04732735591
Iteration 11800: Loss = -12299.047320127967
Iteration 11900: Loss = -12299.324862760785
1
Iteration 12000: Loss = -12299.047315619975
Iteration 12100: Loss = -12299.047307764344
Iteration 12200: Loss = -12299.04724230235
Iteration 12300: Loss = -12299.050007560189
1
Iteration 12400: Loss = -12299.047224363654
Iteration 12500: Loss = -12299.047274638853
Iteration 12600: Loss = -12299.047245070205
Iteration 12700: Loss = -12299.051284991629
1
Iteration 12800: Loss = -12299.047172346456
Iteration 12900: Loss = -12299.047183938188
Iteration 13000: Loss = -12299.047173019873
Iteration 13100: Loss = -12299.291266886374
1
Iteration 13200: Loss = -12299.047182634911
Iteration 13300: Loss = -12299.047174574876
Iteration 13400: Loss = -12299.04751322593
1
Iteration 13500: Loss = -12299.047272005755
Iteration 13600: Loss = -12299.086334576838
1
Iteration 13700: Loss = -12299.0471723813
Iteration 13800: Loss = -12299.261146446079
1
Iteration 13900: Loss = -12299.047254983465
Iteration 14000: Loss = -12299.04935551129
1
Iteration 14100: Loss = -12299.048577815991
2
Iteration 14200: Loss = -12299.04793189262
3
Iteration 14300: Loss = -12299.04907704601
4
Iteration 14400: Loss = -12299.047154361948
Iteration 14500: Loss = -12299.047066147481
Iteration 14600: Loss = -12299.04724251361
1
Iteration 14700: Loss = -12299.04719697192
2
Iteration 14800: Loss = -12299.0534049853
3
Iteration 14900: Loss = -12299.05034881221
4
Iteration 15000: Loss = -12299.058986053611
5
Iteration 15100: Loss = -12299.047122193675
Iteration 15200: Loss = -12299.047080700922
Iteration 15300: Loss = -12299.058078714546
1
Iteration 15400: Loss = -12299.047064421598
Iteration 15500: Loss = -12299.119016731514
1
Iteration 15600: Loss = -12299.048841157897
2
Iteration 15700: Loss = -12299.048870708762
3
Iteration 15800: Loss = -12299.048984659947
4
Iteration 15900: Loss = -12299.047554166229
5
Iteration 16000: Loss = -12299.047412403617
6
Iteration 16100: Loss = -12299.047134927778
Iteration 16200: Loss = -12299.053713660422
1
Iteration 16300: Loss = -12299.047097639632
Iteration 16400: Loss = -12299.051886812007
1
Iteration 16500: Loss = -12299.04706100337
Iteration 16600: Loss = -12299.09747286073
1
Iteration 16700: Loss = -12299.04700400293
Iteration 16800: Loss = -12299.04703983403
Iteration 16900: Loss = -12299.047077803096
Iteration 17000: Loss = -12299.047545336241
1
Iteration 17100: Loss = -12299.04806120315
2
Iteration 17200: Loss = -12299.047051510068
Iteration 17300: Loss = -12299.047210362616
1
Iteration 17400: Loss = -12299.047047664802
Iteration 17500: Loss = -12299.047036482669
Iteration 17600: Loss = -12299.047301217654
1
Iteration 17700: Loss = -12299.047707474854
2
Iteration 17800: Loss = -12299.047134800006
Iteration 17900: Loss = -12299.046998778656
Iteration 18000: Loss = -12299.050078089514
1
Iteration 18100: Loss = -12299.050798462404
2
Iteration 18200: Loss = -12299.047050161498
Iteration 18300: Loss = -12299.04737133193
1
Iteration 18400: Loss = -12299.047324872305
2
Iteration 18500: Loss = -12299.047885990256
3
Iteration 18600: Loss = -12299.047173753914
4
Iteration 18700: Loss = -12299.04712279252
Iteration 18800: Loss = -12299.075494653589
1
Iteration 18900: Loss = -12299.047107443203
Iteration 19000: Loss = -12299.300357479695
1
Iteration 19100: Loss = -12299.047304365384
2
Iteration 19200: Loss = -12299.059904606931
3
Iteration 19300: Loss = -12299.046967709755
Iteration 19400: Loss = -12299.047054610748
Iteration 19500: Loss = -12299.047100137546
Iteration 19600: Loss = -12299.04946696864
1
Iteration 19700: Loss = -12299.047141971994
Iteration 19800: Loss = -12299.052329310918
1
Iteration 19900: Loss = -12299.047443997193
2
pi: tensor([[1.0000e+00, 5.2013e-08],
        [2.2703e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9788, 0.0212], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1984, 0.1992],
         [0.6567, 0.4012]],

        [[0.6733, 0.0799],
         [0.5200, 0.5118]],

        [[0.7111, 0.1841],
         [0.5358, 0.7032]],

        [[0.5313, 0.1638],
         [0.5036, 0.7285]],

        [[0.6427, 0.2707],
         [0.7139, 0.6337]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.016025857647765086
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
Global Adjusted Rand Index: 2.3935939507221996e-05
Average Adjusted Rand Index: 0.0015060083662838252
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21973.23088893102
Iteration 100: Loss = -12306.297941887027
Iteration 200: Loss = -12305.840462112954
Iteration 300: Loss = -12305.72364404597
Iteration 400: Loss = -12305.665747302784
Iteration 500: Loss = -12305.629710775509
Iteration 600: Loss = -12305.60390089061
Iteration 700: Loss = -12305.583886204191
Iteration 800: Loss = -12305.567869985018
Iteration 900: Loss = -12305.554567509558
Iteration 1000: Loss = -12305.543467967005
Iteration 1100: Loss = -12305.533875786456
Iteration 1200: Loss = -12305.525385837773
Iteration 1300: Loss = -12305.517814410186
Iteration 1400: Loss = -12305.510761122208
Iteration 1500: Loss = -12305.504099449196
Iteration 1600: Loss = -12305.497666567539
Iteration 1700: Loss = -12305.491254896191
Iteration 1800: Loss = -12305.484589372543
Iteration 1900: Loss = -12305.477416250422
Iteration 2000: Loss = -12305.469108047977
Iteration 2100: Loss = -12305.457798343266
Iteration 2200: Loss = -12305.435126934379
Iteration 2300: Loss = -12305.292927292196
Iteration 2400: Loss = -12302.995206017775
Iteration 2500: Loss = -12302.537599185143
Iteration 2600: Loss = -12300.403691287825
Iteration 2700: Loss = -12299.917432083952
Iteration 2800: Loss = -12299.719519374728
Iteration 2900: Loss = -12299.571449046734
Iteration 3000: Loss = -12299.454105413242
Iteration 3100: Loss = -12299.364666193507
Iteration 3200: Loss = -12299.298308548068
Iteration 3300: Loss = -12299.249243112305
Iteration 3400: Loss = -12299.212769787562
Iteration 3500: Loss = -12299.185234869772
Iteration 3600: Loss = -12299.164172181867
Iteration 3700: Loss = -12299.147729587829
Iteration 3800: Loss = -12299.134707969128
Iteration 3900: Loss = -12299.124091492928
Iteration 4000: Loss = -12299.115370581663
Iteration 4100: Loss = -12299.108067737867
Iteration 4200: Loss = -12299.101850910629
Iteration 4300: Loss = -12299.096504662828
Iteration 4400: Loss = -12299.091853048827
Iteration 4500: Loss = -12299.087755006634
Iteration 4600: Loss = -12299.084193507259
Iteration 4700: Loss = -12299.08104124382
Iteration 4800: Loss = -12299.078287216278
Iteration 4900: Loss = -12299.075820549378
Iteration 5000: Loss = -12299.073611419546
Iteration 5100: Loss = -12299.07163864369
Iteration 5200: Loss = -12299.069871660491
Iteration 5300: Loss = -12299.068277169901
Iteration 5400: Loss = -12299.06679154194
Iteration 5500: Loss = -12299.065435016424
Iteration 5600: Loss = -12299.064203813974
Iteration 5700: Loss = -12299.063076741515
Iteration 5800: Loss = -12299.062094082443
Iteration 5900: Loss = -12299.061096214473
Iteration 6000: Loss = -12299.060236081068
Iteration 6100: Loss = -12299.059396414143
Iteration 6200: Loss = -12299.058701836104
Iteration 6300: Loss = -12299.057992034177
Iteration 6400: Loss = -12299.057313372545
Iteration 6500: Loss = -12299.056734605836
Iteration 6600: Loss = -12299.056144382244
Iteration 6700: Loss = -12299.055632195219
Iteration 6800: Loss = -12299.055146775512
Iteration 6900: Loss = -12299.054706093008
Iteration 7000: Loss = -12299.05428979845
Iteration 7100: Loss = -12299.05387459111
Iteration 7200: Loss = -12299.239311870915
1
Iteration 7300: Loss = -12299.05315994766
Iteration 7400: Loss = -12299.052825748959
Iteration 7500: Loss = -12299.052499369349
Iteration 7600: Loss = -12299.052673751512
1
Iteration 7700: Loss = -12299.051959766388
Iteration 7800: Loss = -12299.051708891557
Iteration 7900: Loss = -12299.051517004282
Iteration 8000: Loss = -12299.05122337812
Iteration 8100: Loss = -12299.051022706746
Iteration 8200: Loss = -12299.050809494667
Iteration 8300: Loss = -12299.214264034139
1
Iteration 8400: Loss = -12299.050450923822
Iteration 8500: Loss = -12299.05027991695
Iteration 8600: Loss = -12299.050126186296
Iteration 8700: Loss = -12299.050099033653
Iteration 8800: Loss = -12299.049807244646
Iteration 8900: Loss = -12299.04965234079
Iteration 9000: Loss = -12299.224096162015
1
Iteration 9100: Loss = -12299.049422594511
Iteration 9200: Loss = -12299.049271455007
Iteration 9300: Loss = -12299.049230312594
Iteration 9400: Loss = -12299.05046670879
1
Iteration 9500: Loss = -12299.049023873928
Iteration 9600: Loss = -12299.048896781578
Iteration 9700: Loss = -12299.05088062959
1
Iteration 9800: Loss = -12299.048758113408
Iteration 9900: Loss = -12299.048623593553
Iteration 10000: Loss = -12299.048595770952
Iteration 10100: Loss = -12299.05255427328
1
Iteration 10200: Loss = -12299.048475275227
Iteration 10300: Loss = -12299.048349288023
Iteration 10400: Loss = -12299.048316925084
Iteration 10500: Loss = -12299.050645441617
1
Iteration 10600: Loss = -12299.048178617551
Iteration 10700: Loss = -12299.048102737146
Iteration 10800: Loss = -12299.048082936411
Iteration 10900: Loss = -12299.04802769571
Iteration 11000: Loss = -12299.047950353799
Iteration 11100: Loss = -12299.047870738757
Iteration 11200: Loss = -12299.048704509614
1
Iteration 11300: Loss = -12299.047812514076
Iteration 11400: Loss = -12299.04775336778
Iteration 11500: Loss = -12299.047728922966
Iteration 11600: Loss = -12299.04795169617
1
Iteration 11700: Loss = -12299.047641073294
Iteration 11800: Loss = -12299.047624940917
Iteration 11900: Loss = -12299.047599442758
Iteration 12000: Loss = -12299.047545394604
Iteration 12100: Loss = -12299.047482402684
Iteration 12200: Loss = -12299.047489938406
Iteration 12300: Loss = -12299.063077477947
1
Iteration 12400: Loss = -12299.047387530678
Iteration 12500: Loss = -12299.047353047781
Iteration 12600: Loss = -12299.051892391606
1
Iteration 12700: Loss = -12299.047335374791
Iteration 12800: Loss = -12299.047292216026
Iteration 12900: Loss = -12299.047276078802
Iteration 13000: Loss = -12299.05034861055
1
Iteration 13100: Loss = -12299.047188084965
Iteration 13200: Loss = -12299.047182083776
Iteration 13300: Loss = -12299.223365706437
1
Iteration 13400: Loss = -12299.047188542092
Iteration 13500: Loss = -12299.047175208223
Iteration 13600: Loss = -12299.047183775474
Iteration 13700: Loss = -12299.047894992476
1
Iteration 13800: Loss = -12299.049969981565
2
Iteration 13900: Loss = -12299.071033669365
3
Iteration 14000: Loss = -12299.048061129037
4
Iteration 14100: Loss = -12299.048152110223
5
Iteration 14200: Loss = -12299.04712594319
Iteration 14300: Loss = -12299.048682692814
1
Iteration 14400: Loss = -12299.047123237382
Iteration 14500: Loss = -12299.047265105215
1
Iteration 14600: Loss = -12299.047286104173
2
Iteration 14700: Loss = -12299.047147752959
Iteration 14800: Loss = -12299.047136230625
Iteration 14900: Loss = -12299.047670754773
1
Iteration 15000: Loss = -12299.047505640181
2
Iteration 15100: Loss = -12299.048098150352
3
Iteration 15200: Loss = -12299.047105316948
Iteration 15300: Loss = -12299.048160617489
1
Iteration 15400: Loss = -12299.05207336472
2
Iteration 15500: Loss = -12299.047134417584
Iteration 15600: Loss = -12299.0474900034
1
Iteration 15700: Loss = -12299.047471414637
2
Iteration 15800: Loss = -12299.047069328417
Iteration 15900: Loss = -12299.129193834526
1
Iteration 16000: Loss = -12299.047130251598
Iteration 16100: Loss = -12299.047076812332
Iteration 16200: Loss = -12299.047802831832
1
Iteration 16300: Loss = -12299.04750292925
2
Iteration 16400: Loss = -12299.059417443392
3
Iteration 16500: Loss = -12299.047002738052
Iteration 16600: Loss = -12299.053459044302
1
Iteration 16700: Loss = -12299.047373165138
2
Iteration 16800: Loss = -12299.052832459525
3
Iteration 16900: Loss = -12299.047310699892
4
Iteration 17000: Loss = -12299.047682037315
5
Iteration 17100: Loss = -12299.047002514853
Iteration 17200: Loss = -12299.050968612048
1
Iteration 17300: Loss = -12299.05088680917
2
Iteration 17400: Loss = -12299.086685282618
3
Iteration 17500: Loss = -12299.047304258813
4
Iteration 17600: Loss = -12299.048263036075
5
Iteration 17700: Loss = -12299.047019950118
Iteration 17800: Loss = -12299.047063127937
Iteration 17900: Loss = -12299.094767641913
1
Iteration 18000: Loss = -12299.047768540322
2
Iteration 18100: Loss = -12299.302397652924
3
Iteration 18200: Loss = -12299.04692004315
Iteration 18300: Loss = -12299.047373477299
1
Iteration 18400: Loss = -12299.047521353785
2
Iteration 18500: Loss = -12299.04696017214
Iteration 18600: Loss = -12299.046900007173
Iteration 18700: Loss = -12299.047696731643
1
Iteration 18800: Loss = -12299.046983402639
Iteration 18900: Loss = -12299.047941527311
1
Iteration 19000: Loss = -12299.04725559994
2
Iteration 19100: Loss = -12299.377300361855
3
Iteration 19200: Loss = -12299.046796924376
Iteration 19300: Loss = -12299.046832039612
Iteration 19400: Loss = -12299.046946854367
1
Iteration 19500: Loss = -12299.046872385376
Iteration 19600: Loss = -12299.0468415114
Iteration 19700: Loss = -12299.047003009771
1
Iteration 19800: Loss = -12299.058020075776
2
Iteration 19900: Loss = -12299.047030459544
3
pi: tensor([[1.0000e+00, 8.3858e-08],
        [6.0266e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9790, 0.0210], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1982, 0.1995],
         [0.6606, 0.4015]],

        [[0.6684, 0.0796],
         [0.5182, 0.7156]],

        [[0.5942, 0.1839],
         [0.6013, 0.6553]],

        [[0.6174, 0.1635],
         [0.6813, 0.6493]],

        [[0.6557, 0.2715],
         [0.6790, 0.5011]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.016025857647765086
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
Global Adjusted Rand Index: 2.3935939507221996e-05
Average Adjusted Rand Index: 0.0015060083662838252
11784.801612897678
[2.3935939507221996e-05, 2.3935939507221996e-05] [0.0015060083662838252, 0.0015060083662838252] [12299.047821641441, 12299.048730704964]
-------------------------------------
This iteration is 89
True Objective function: Loss = -11816.057472272678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21718.572378704095
Iteration 100: Loss = -11806.813333586135
Iteration 200: Loss = -11804.04659443479
Iteration 300: Loss = -11803.778048703765
Iteration 400: Loss = -11803.670238746423
Iteration 500: Loss = -11803.613216342963
Iteration 600: Loss = -11803.578726059717
Iteration 700: Loss = -11803.556140294173
Iteration 800: Loss = -11803.540407566494
Iteration 900: Loss = -11803.529007942943
Iteration 1000: Loss = -11803.520463002844
Iteration 1100: Loss = -11803.513898562978
Iteration 1200: Loss = -11803.508717147097
Iteration 1300: Loss = -11803.504520042814
Iteration 1400: Loss = -11803.501163171179
Iteration 1500: Loss = -11803.498381653044
Iteration 1600: Loss = -11803.496032039862
Iteration 1700: Loss = -11803.494057697222
Iteration 1800: Loss = -11803.492337595475
Iteration 1900: Loss = -11803.490898294023
Iteration 2000: Loss = -11803.489641293923
Iteration 2100: Loss = -11803.488536116845
Iteration 2200: Loss = -11803.487601604322
Iteration 2300: Loss = -11803.486731007235
Iteration 2400: Loss = -11803.485972314998
Iteration 2500: Loss = -11803.485303667992
Iteration 2600: Loss = -11803.48476419159
Iteration 2700: Loss = -11803.484171119195
Iteration 2800: Loss = -11803.483710802862
Iteration 2900: Loss = -11803.484065510622
1
Iteration 3000: Loss = -11803.4828661844
Iteration 3100: Loss = -11803.482506415314
Iteration 3200: Loss = -11803.48219480921
Iteration 3300: Loss = -11803.482004401945
Iteration 3400: Loss = -11803.481752597732
Iteration 3500: Loss = -11803.481401424015
Iteration 3600: Loss = -11803.481175088222
Iteration 3700: Loss = -11803.481749472492
1
Iteration 3800: Loss = -11803.49261730306
2
Iteration 3900: Loss = -11803.480977175559
Iteration 4000: Loss = -11803.480454028173
Iteration 4100: Loss = -11803.48128676593
1
Iteration 4200: Loss = -11803.480625009655
2
Iteration 4300: Loss = -11803.483637507723
3
Iteration 4400: Loss = -11803.480346133907
Iteration 4500: Loss = -11803.48033257109
Iteration 4600: Loss = -11803.485722284096
1
Iteration 4700: Loss = -11803.479600605566
Iteration 4800: Loss = -11803.479511403746
Iteration 4900: Loss = -11803.479580453333
Iteration 5000: Loss = -11803.490444029037
1
Iteration 5100: Loss = -11803.47951811014
Iteration 5200: Loss = -11803.48369390987
1
Iteration 5300: Loss = -11803.479819062515
2
Iteration 5400: Loss = -11803.480429880921
3
Iteration 5500: Loss = -11803.479366016725
Iteration 5600: Loss = -11803.479249641245
Iteration 5700: Loss = -11803.478973111152
Iteration 5800: Loss = -11803.478887737745
Iteration 5900: Loss = -11803.478926459053
Iteration 6000: Loss = -11803.478873682154
Iteration 6100: Loss = -11803.478856051339
Iteration 6200: Loss = -11803.478861463142
Iteration 6300: Loss = -11803.478716562377
Iteration 6400: Loss = -11803.479014011366
1
Iteration 6500: Loss = -11803.478657043976
Iteration 6600: Loss = -11803.478712828235
Iteration 6700: Loss = -11803.479321576697
1
Iteration 6800: Loss = -11803.481378971268
2
Iteration 6900: Loss = -11803.4802244009
3
Iteration 7000: Loss = -11803.486792332871
4
Iteration 7100: Loss = -11803.478490199483
Iteration 7200: Loss = -11803.478748688338
1
Iteration 7300: Loss = -11803.489170252566
2
Iteration 7400: Loss = -11803.480174331613
3
Iteration 7500: Loss = -11803.478586062223
Iteration 7600: Loss = -11803.478544398735
Iteration 7700: Loss = -11803.47841927288
Iteration 7800: Loss = -11803.478461406477
Iteration 7900: Loss = -11803.478418550125
Iteration 8000: Loss = -11803.478385926575
Iteration 8100: Loss = -11803.481152297034
1
Iteration 8200: Loss = -11803.482842763979
2
Iteration 8300: Loss = -11803.501545089839
3
Iteration 8400: Loss = -11803.478330356571
Iteration 8500: Loss = -11803.479314082122
1
Iteration 8600: Loss = -11803.478432672506
2
Iteration 8700: Loss = -11803.478323106565
Iteration 8800: Loss = -11803.478393668353
Iteration 8900: Loss = -11803.495902489778
1
Iteration 9000: Loss = -11803.499039559752
2
Iteration 9100: Loss = -11803.483693087515
3
Iteration 9200: Loss = -11803.479173050608
4
Iteration 9300: Loss = -11803.478325345513
Iteration 9400: Loss = -11803.483429075399
1
Iteration 9500: Loss = -11803.478242758918
Iteration 9600: Loss = -11803.478640413718
1
Iteration 9700: Loss = -11803.47834201133
Iteration 9800: Loss = -11803.48358215033
1
Iteration 9900: Loss = -11803.514428517257
2
Iteration 10000: Loss = -11803.479060724228
3
Iteration 10100: Loss = -11803.496648702312
4
Iteration 10200: Loss = -11803.478960616718
5
Iteration 10300: Loss = -11803.480151334577
6
Iteration 10400: Loss = -11803.49166969475
7
Iteration 10500: Loss = -11803.49132807103
8
Iteration 10600: Loss = -11803.490411773262
9
Iteration 10700: Loss = -11803.490558958607
10
Iteration 10800: Loss = -11803.479662875616
11
Iteration 10900: Loss = -11803.48462954759
12
Iteration 11000: Loss = -11803.484692362617
13
Iteration 11100: Loss = -11803.481835792314
14
Iteration 11200: Loss = -11803.480483801222
15
Stopping early at iteration 11200 due to no improvement.
pi: tensor([[0.7543, 0.2457],
        [0.2240, 0.7760]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4064, 0.5936], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2920, 0.1016],
         [0.6868, 0.3038]],

        [[0.5912, 0.0949],
         [0.5892, 0.5913]],

        [[0.6757, 0.0880],
         [0.5258, 0.6114]],

        [[0.6521, 0.0978],
         [0.5060, 0.6162]],

        [[0.5380, 0.1028],
         [0.5277, 0.5238]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
Global Adjusted Rand Index: 0.9840317580481719
Average Adjusted Rand Index: 0.9841540496839629
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22219.701765468042
Iteration 100: Loss = -12351.538172742565
Iteration 200: Loss = -12350.472609841958
Iteration 300: Loss = -12349.85362957024
Iteration 400: Loss = -12348.739494112106
Iteration 500: Loss = -12348.286490673974
Iteration 600: Loss = -12347.917518204242
Iteration 700: Loss = -12347.409002101256
Iteration 800: Loss = -12346.492454969986
Iteration 900: Loss = -12315.214726423117
Iteration 1000: Loss = -12271.937092353217
Iteration 1100: Loss = -12268.467871121906
Iteration 1200: Loss = -12268.310147447874
Iteration 1300: Loss = -12268.22840855633
Iteration 1400: Loss = -12268.174949204747
Iteration 1500: Loss = -12268.134254375223
Iteration 1600: Loss = -12268.099150592356
Iteration 1700: Loss = -12268.065531788943
Iteration 1800: Loss = -12268.03036814446
Iteration 1900: Loss = -12267.99023303289
Iteration 2000: Loss = -12267.94096054757
Iteration 2100: Loss = -12267.876564475435
Iteration 2200: Loss = -12267.780305866136
Iteration 2300: Loss = -12267.461194711525
Iteration 2400: Loss = -12267.139637791304
Iteration 2500: Loss = -12267.072762873366
Iteration 2600: Loss = -12267.04154887114
Iteration 2700: Loss = -12267.023235557632
Iteration 2800: Loss = -12267.011065353128
Iteration 2900: Loss = -12267.002225997383
Iteration 3000: Loss = -12266.995426137197
Iteration 3100: Loss = -12266.989943751038
Iteration 3200: Loss = -12266.985365879931
Iteration 3300: Loss = -12266.98150332397
Iteration 3400: Loss = -12266.97819483554
Iteration 3500: Loss = -12266.975339607856
Iteration 3600: Loss = -12266.97287846263
Iteration 3700: Loss = -12266.970745803266
Iteration 3800: Loss = -12266.969007312931
Iteration 3900: Loss = -12266.967137559455
Iteration 4000: Loss = -12266.965618953076
Iteration 4100: Loss = -12266.964256567066
Iteration 4200: Loss = -12266.963053142013
Iteration 4300: Loss = -12266.961900052645
Iteration 4400: Loss = -12266.960866671887
Iteration 4500: Loss = -12266.96133217563
1
Iteration 4600: Loss = -12266.9590625558
Iteration 4700: Loss = -12266.958320811718
Iteration 4800: Loss = -12266.957549969711
Iteration 4900: Loss = -12266.956887697534
Iteration 5000: Loss = -12266.956250233783
Iteration 5100: Loss = -12266.955668186463
Iteration 5200: Loss = -12266.955118048403
Iteration 5300: Loss = -12266.954712079938
Iteration 5400: Loss = -12266.95421157245
Iteration 5500: Loss = -12266.953709764728
Iteration 5600: Loss = -12266.953345664204
Iteration 5700: Loss = -12266.952996509235
Iteration 5800: Loss = -12266.952619564903
Iteration 5900: Loss = -12266.952312794301
Iteration 6000: Loss = -12266.95199562793
Iteration 6100: Loss = -12266.951716506203
Iteration 6200: Loss = -12266.951395442777
Iteration 6300: Loss = -12266.951156170535
Iteration 6400: Loss = -12266.951437814196
1
Iteration 6500: Loss = -12266.950716114005
Iteration 6600: Loss = -12266.95052133804
Iteration 6700: Loss = -12266.953572288776
1
Iteration 6800: Loss = -12266.950181253707
Iteration 6900: Loss = -12266.96217226104
1
Iteration 7000: Loss = -12266.949782157855
Iteration 7100: Loss = -12266.94965356733
Iteration 7200: Loss = -12266.94950734205
Iteration 7300: Loss = -12267.006982559697
1
Iteration 7400: Loss = -12266.949226066967
Iteration 7500: Loss = -12267.050863172142
1
Iteration 7600: Loss = -12266.953363171046
2
Iteration 7700: Loss = -12266.948895515376
Iteration 7800: Loss = -12266.948917292406
Iteration 7900: Loss = -12266.948677928824
Iteration 8000: Loss = -12266.949440061999
1
Iteration 8100: Loss = -12266.948508545192
Iteration 8200: Loss = -12266.956065083157
1
Iteration 8300: Loss = -12266.948366365332
Iteration 8400: Loss = -12266.94835602624
Iteration 8500: Loss = -12266.94822068804
Iteration 8600: Loss = -12266.948090627035
Iteration 8700: Loss = -12267.192106279546
1
Iteration 8800: Loss = -12266.9480102414
Iteration 8900: Loss = -12266.94793789523
Iteration 9000: Loss = -12267.28703933836
1
Iteration 9100: Loss = -12266.94788011535
Iteration 9200: Loss = -12266.94780141985
Iteration 9300: Loss = -12267.063413565838
1
Iteration 9400: Loss = -12266.947684496728
Iteration 9500: Loss = -12266.947616294594
Iteration 9600: Loss = -12266.949179437146
1
Iteration 9700: Loss = -12266.947568960184
Iteration 9800: Loss = -12266.94780298589
1
Iteration 9900: Loss = -12266.947568904912
Iteration 10000: Loss = -12266.947453699004
Iteration 10100: Loss = -12266.9474936566
Iteration 10200: Loss = -12266.947502466537
Iteration 10300: Loss = -12266.947406454783
Iteration 10400: Loss = -12266.94758694034
1
Iteration 10500: Loss = -12266.947391917298
Iteration 10600: Loss = -12266.990010360316
1
Iteration 10700: Loss = -12266.947727621064
2
Iteration 10800: Loss = -12266.947324318595
Iteration 10900: Loss = -12266.951134657189
1
Iteration 11000: Loss = -12266.94726504781
Iteration 11100: Loss = -12266.947723339847
1
Iteration 11200: Loss = -12266.956854327744
2
Iteration 11300: Loss = -12266.947218641079
Iteration 11400: Loss = -12266.949854310553
1
Iteration 11500: Loss = -12266.947147607472
Iteration 11600: Loss = -12266.94724836085
1
Iteration 11700: Loss = -12266.94714429074
Iteration 11800: Loss = -12266.947669404753
1
Iteration 11900: Loss = -12266.947110072726
Iteration 12000: Loss = -12266.948210435336
1
Iteration 12100: Loss = -12266.948152154451
2
Iteration 12200: Loss = -12266.947473343454
3
Iteration 12300: Loss = -12266.947185909685
Iteration 12400: Loss = -12266.947436418832
1
Iteration 12500: Loss = -12266.947152771145
Iteration 12600: Loss = -12266.947114253586
Iteration 12700: Loss = -12266.947157558425
Iteration 12800: Loss = -12266.947057331769
Iteration 12900: Loss = -12266.947029070521
Iteration 13000: Loss = -12266.949979536594
1
Iteration 13100: Loss = -12266.946998233921
Iteration 13200: Loss = -12266.947974059989
1
Iteration 13300: Loss = -12266.947034965466
Iteration 13400: Loss = -12266.946986537278
Iteration 13500: Loss = -12266.947537394288
1
Iteration 13600: Loss = -12266.946983815002
Iteration 13700: Loss = -12266.984479508023
1
Iteration 13800: Loss = -12266.946986623238
Iteration 13900: Loss = -12266.946972058717
Iteration 14000: Loss = -12266.949668032474
1
Iteration 14100: Loss = -12266.947368137673
2
Iteration 14200: Loss = -12266.947604618443
3
Iteration 14300: Loss = -12266.946983275368
Iteration 14400: Loss = -12266.95454761132
1
Iteration 14500: Loss = -12266.947088389788
2
Iteration 14600: Loss = -12266.946984064398
Iteration 14700: Loss = -12267.196082610684
1
Iteration 14800: Loss = -12266.94696485719
Iteration 14900: Loss = -12266.96369174185
1
Iteration 15000: Loss = -12266.946949425603
Iteration 15100: Loss = -12266.946935575343
Iteration 15200: Loss = -12266.981047838846
1
Iteration 15300: Loss = -12266.946929214475
Iteration 15400: Loss = -12266.947092080389
1
Iteration 15500: Loss = -12266.947027244798
Iteration 15600: Loss = -12266.946918650581
Iteration 15700: Loss = -12266.981979462222
1
Iteration 15800: Loss = -12266.94693969936
Iteration 15900: Loss = -12266.982350600952
1
Iteration 16000: Loss = -12266.946926601642
Iteration 16100: Loss = -12267.019879917205
1
Iteration 16200: Loss = -12266.946941315826
Iteration 16300: Loss = -12267.043450304513
1
Iteration 16400: Loss = -12266.94693354097
Iteration 16500: Loss = -12266.947014649078
Iteration 16600: Loss = -12266.946926303168
Iteration 16700: Loss = -12266.947004626347
Iteration 16800: Loss = -12266.949210136612
1
Iteration 16900: Loss = -12266.952784256655
2
Iteration 17000: Loss = -12266.946959048326
Iteration 17100: Loss = -12267.021946966704
1
Iteration 17200: Loss = -12266.94691585605
Iteration 17300: Loss = -12267.147466417826
1
Iteration 17400: Loss = -12266.946881189713
Iteration 17500: Loss = -12266.946885855952
Iteration 17600: Loss = -12266.947188581
1
Iteration 17700: Loss = -12266.974419921557
2
Iteration 17800: Loss = -12266.946894599807
Iteration 17900: Loss = -12266.947192336766
1
Iteration 18000: Loss = -12266.946897776083
Iteration 18100: Loss = -12266.94699164544
Iteration 18200: Loss = -12266.946886209696
Iteration 18300: Loss = -12266.947849717402
1
Iteration 18400: Loss = -12266.94689435882
Iteration 18500: Loss = -12266.955159119327
1
Iteration 18600: Loss = -12266.946908853744
Iteration 18700: Loss = -12266.947204159083
1
Iteration 18800: Loss = -12266.94688740609
Iteration 18900: Loss = -12266.963555808405
1
Iteration 19000: Loss = -12266.947775259803
2
Iteration 19100: Loss = -12266.946911396728
Iteration 19200: Loss = -12266.94756078442
1
Iteration 19300: Loss = -12266.946876152733
Iteration 19400: Loss = -12266.94830921097
1
Iteration 19500: Loss = -12266.94688975663
Iteration 19600: Loss = -12266.950977075943
1
Iteration 19700: Loss = -12266.947825683208
2
Iteration 19800: Loss = -12266.949094253954
3
Iteration 19900: Loss = -12267.006411878927
4
pi: tensor([[2.9694e-07, 1.0000e+00],
        [2.3852e-02, 9.7615e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5710, 0.4290], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3226, 0.1048],
         [0.7060, 0.2014]],

        [[0.5940, 0.2012],
         [0.5468, 0.5908]],

        [[0.5249, 0.1014],
         [0.5148, 0.6812]],

        [[0.5627, 0.1777],
         [0.5585, 0.5705]],

        [[0.6238, 0.2444],
         [0.6833, 0.5098]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.018247319683811245
Average Adjusted Rand Index: 0.1869222642688675
11816.057472272678
[0.9840317580481719, 0.018247319683811245] [0.9841540496839629, 0.1869222642688675] [11803.480483801222, 12266.946885525238]
-------------------------------------
This iteration is 90
True Objective function: Loss = -11786.733232998846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20671.39808180569
Iteration 100: Loss = -12266.789602140703
Iteration 200: Loss = -12266.265572749486
Iteration 300: Loss = -12266.147197146003
Iteration 400: Loss = -12266.069408291376
Iteration 500: Loss = -12265.995039771191
Iteration 600: Loss = -12265.92245642457
Iteration 700: Loss = -12265.866444969088
Iteration 800: Loss = -12265.824561924332
Iteration 900: Loss = -12265.785845903176
Iteration 1000: Loss = -12265.747970750373
Iteration 1100: Loss = -12265.710617328872
Iteration 1200: Loss = -12265.673707510443
Iteration 1300: Loss = -12265.636941046094
Iteration 1400: Loss = -12265.599944677182
Iteration 1500: Loss = -12265.56244668973
Iteration 1600: Loss = -12265.524675187065
Iteration 1700: Loss = -12265.487310680788
Iteration 1800: Loss = -12265.451556519673
Iteration 1900: Loss = -12265.418459087976
Iteration 2000: Loss = -12265.389056396354
Iteration 2100: Loss = -12265.363688453726
Iteration 2200: Loss = -12265.342131732037
Iteration 2300: Loss = -12265.323960865544
Iteration 2400: Loss = -12265.308412913339
Iteration 2500: Loss = -12265.295105336156
Iteration 2600: Loss = -12265.283535271145
Iteration 2700: Loss = -12265.273498787767
Iteration 2800: Loss = -12265.264651965283
Iteration 2900: Loss = -12265.256863187782
Iteration 3000: Loss = -12265.250019473868
Iteration 3100: Loss = -12265.243956608578
Iteration 3200: Loss = -12265.238620488857
Iteration 3300: Loss = -12265.233809813946
Iteration 3400: Loss = -12265.229614882559
Iteration 3500: Loss = -12265.225809847678
Iteration 3600: Loss = -12265.22275002451
Iteration 3700: Loss = -12265.21935635851
Iteration 3800: Loss = -12265.216632780668
Iteration 3900: Loss = -12265.214163656628
Iteration 4000: Loss = -12265.212033523869
Iteration 4100: Loss = -12265.210027069243
Iteration 4200: Loss = -12265.208308154613
Iteration 4300: Loss = -12265.206828036005
Iteration 4400: Loss = -12265.205538979944
Iteration 4500: Loss = -12265.204458023021
Iteration 4600: Loss = -12265.203582504299
Iteration 4700: Loss = -12265.202888307133
Iteration 4800: Loss = -12265.202169754191
Iteration 4900: Loss = -12265.201601001241
Iteration 5000: Loss = -12265.201141883323
Iteration 5100: Loss = -12265.200838928507
Iteration 5200: Loss = -12265.200438711452
Iteration 5300: Loss = -12265.200212661519
Iteration 5400: Loss = -12265.199950897048
Iteration 5500: Loss = -12265.199783169257
Iteration 5600: Loss = -12265.199574997348
Iteration 5700: Loss = -12265.199453262985
Iteration 5800: Loss = -12265.199471281743
Iteration 5900: Loss = -12265.199126018995
Iteration 6000: Loss = -12265.199032456789
Iteration 6100: Loss = -12265.199059563287
Iteration 6200: Loss = -12265.199622567383
1
Iteration 6300: Loss = -12265.198758529898
Iteration 6400: Loss = -12265.199056793897
1
Iteration 6500: Loss = -12265.198670564367
Iteration 6600: Loss = -12265.198464720725
Iteration 6700: Loss = -12265.198650940254
1
Iteration 6800: Loss = -12265.19838391965
Iteration 6900: Loss = -12265.198299445594
Iteration 7000: Loss = -12265.198282190406
Iteration 7100: Loss = -12265.198199233102
Iteration 7200: Loss = -12265.198787652653
1
Iteration 7300: Loss = -12265.286176350435
2
Iteration 7400: Loss = -12265.200998092118
3
Iteration 7500: Loss = -12265.198053356777
Iteration 7600: Loss = -12265.226782675369
1
Iteration 7700: Loss = -12265.197944946118
Iteration 7800: Loss = -12265.535677432246
1
Iteration 7900: Loss = -12265.197896974578
Iteration 8000: Loss = -12265.1978584771
Iteration 8100: Loss = -12265.208874906013
1
Iteration 8200: Loss = -12265.197814437339
Iteration 8300: Loss = -12265.20242803334
1
Iteration 8400: Loss = -12265.199816965252
2
Iteration 8500: Loss = -12265.197743779254
Iteration 8600: Loss = -12265.197851769883
1
Iteration 8700: Loss = -12265.20730015674
2
Iteration 8800: Loss = -12265.247897705465
3
Iteration 8900: Loss = -12265.227842677827
4
Iteration 9000: Loss = -12265.19770034746
Iteration 9100: Loss = -12265.198361914217
1
Iteration 9200: Loss = -12265.19765575489
Iteration 9300: Loss = -12265.22320966899
1
Iteration 9400: Loss = -12265.2002354378
2
Iteration 9500: Loss = -12265.202096266443
3
Iteration 9600: Loss = -12265.197833820828
4
Iteration 9700: Loss = -12265.215191824203
5
Iteration 9800: Loss = -12265.326590698194
6
Iteration 9900: Loss = -12265.197659094887
Iteration 10000: Loss = -12265.214412646572
1
Iteration 10100: Loss = -12265.205464355016
2
Iteration 10200: Loss = -12265.198000812285
3
Iteration 10300: Loss = -12265.216034965802
4
Iteration 10400: Loss = -12265.22166244596
5
Iteration 10500: Loss = -12265.197554140166
Iteration 10600: Loss = -12265.198079631937
1
Iteration 10700: Loss = -12265.46489917708
2
Iteration 10800: Loss = -12265.197550579085
Iteration 10900: Loss = -12265.201151404568
1
Iteration 11000: Loss = -12265.197566819672
Iteration 11100: Loss = -12265.19763458946
Iteration 11200: Loss = -12265.199308623794
1
Iteration 11300: Loss = -12265.209715774088
2
Iteration 11400: Loss = -12265.197559446522
Iteration 11500: Loss = -12265.197530105568
Iteration 11600: Loss = -12265.259347997706
1
Iteration 11700: Loss = -12265.202385699236
2
Iteration 11800: Loss = -12265.201572385457
3
Iteration 11900: Loss = -12265.201793854525
4
Iteration 12000: Loss = -12265.197679825982
5
Iteration 12100: Loss = -12265.197653684338
6
Iteration 12200: Loss = -12265.198349289421
7
Iteration 12300: Loss = -12265.300002780223
8
Iteration 12400: Loss = -12265.21297761777
9
Iteration 12500: Loss = -12265.208139931352
10
Iteration 12600: Loss = -12265.266209969066
11
Iteration 12700: Loss = -12265.247425769894
12
Iteration 12800: Loss = -12265.197783635502
13
Iteration 12900: Loss = -12265.19756800171
Iteration 13000: Loss = -12265.20383423106
1
Iteration 13100: Loss = -12265.212744812041
2
Iteration 13200: Loss = -12265.19864795729
3
Iteration 13300: Loss = -12265.207790055241
4
Iteration 13400: Loss = -12265.203362565915
5
Iteration 13500: Loss = -12265.197670271142
6
Iteration 13600: Loss = -12265.205650978489
7
Iteration 13700: Loss = -12265.298370264316
8
Iteration 13800: Loss = -12265.28872378429
9
Iteration 13900: Loss = -12265.197873604535
10
Iteration 14000: Loss = -12265.197613672939
Iteration 14100: Loss = -12265.205173962006
1
Iteration 14200: Loss = -12265.197498917894
Iteration 14300: Loss = -12265.286887084847
1
Iteration 14400: Loss = -12265.197468150198
Iteration 14500: Loss = -12265.206384084659
1
Iteration 14600: Loss = -12265.197510722295
Iteration 14700: Loss = -12265.197604953124
Iteration 14800: Loss = -12265.215100605843
1
Iteration 14900: Loss = -12265.247650056286
2
Iteration 15000: Loss = -12265.282288465369
3
Iteration 15100: Loss = -12265.2081009121
4
Iteration 15200: Loss = -12265.197798840702
5
Iteration 15300: Loss = -12265.2168241301
6
Iteration 15400: Loss = -12265.197454130159
Iteration 15500: Loss = -12265.201867075964
1
Iteration 15600: Loss = -12265.201246878705
2
Iteration 15700: Loss = -12265.198339558532
3
Iteration 15800: Loss = -12265.287243558314
4
Iteration 15900: Loss = -12265.203436407935
5
Iteration 16000: Loss = -12265.280668862846
6
Iteration 16100: Loss = -12265.200499691073
7
Iteration 16200: Loss = -12265.197986824274
8
Iteration 16300: Loss = -12265.198675904256
9
Iteration 16400: Loss = -12265.199169074154
10
Iteration 16500: Loss = -12265.20681866208
11
Iteration 16600: Loss = -12265.197600176662
12
Iteration 16700: Loss = -12265.231909479811
13
Iteration 16800: Loss = -12265.19804477855
14
Iteration 16900: Loss = -12265.320912803592
15
Stopping early at iteration 16900 due to no improvement.
pi: tensor([[0.9731, 0.0269],
        [0.9862, 0.0138]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0040, 0.9960], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1961, 0.2012],
         [0.6882, 0.2019]],

        [[0.6195, 0.1306],
         [0.6349, 0.7043]],

        [[0.6631, 0.2460],
         [0.5242, 0.6154]],

        [[0.5525, 0.1900],
         [0.6622, 0.7043]],

        [[0.5661, 0.2151],
         [0.6627, 0.7084]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0016114094845201586
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24397.249743723118
Iteration 100: Loss = -11780.223677886419
Iteration 200: Loss = -11776.871661840045
Iteration 300: Loss = -11776.56738017844
Iteration 400: Loss = -11776.440558318125
Iteration 500: Loss = -11776.373938792394
Iteration 600: Loss = -11776.334107300992
Iteration 700: Loss = -11776.308215697813
Iteration 800: Loss = -11776.290211294448
Iteration 900: Loss = -11776.27942415392
Iteration 1000: Loss = -11776.267725334283
Iteration 1100: Loss = -11776.260999698296
Iteration 1200: Loss = -11776.254589234975
Iteration 1300: Loss = -11776.250506049355
Iteration 1400: Loss = -11776.251939041253
1
Iteration 1500: Loss = -11776.242397442133
Iteration 1600: Loss = -11776.239755281562
Iteration 1700: Loss = -11776.237662766725
Iteration 1800: Loss = -11776.235618405028
Iteration 1900: Loss = -11776.23384965197
Iteration 2000: Loss = -11776.232449696083
Iteration 2100: Loss = -11776.231450057838
Iteration 2200: Loss = -11776.230097750526
Iteration 2300: Loss = -11776.23021479606
1
Iteration 2400: Loss = -11776.228257313576
Iteration 2500: Loss = -11776.230143981033
1
Iteration 2600: Loss = -11776.226798365284
Iteration 2700: Loss = -11776.229849560159
1
Iteration 2800: Loss = -11776.225635140367
Iteration 2900: Loss = -11776.225151177005
Iteration 3000: Loss = -11776.224712527453
Iteration 3100: Loss = -11776.224276341518
Iteration 3200: Loss = -11776.223920260825
Iteration 3300: Loss = -11776.226450903829
1
Iteration 3400: Loss = -11776.223233592886
Iteration 3500: Loss = -11776.223172006503
Iteration 3600: Loss = -11776.222852804165
Iteration 3700: Loss = -11776.22248097317
Iteration 3800: Loss = -11776.222286489234
Iteration 3900: Loss = -11776.230472365758
1
Iteration 4000: Loss = -11776.22160434171
Iteration 4100: Loss = -11776.221422839375
Iteration 4200: Loss = -11776.221316318168
Iteration 4300: Loss = -11776.221094239474
Iteration 4400: Loss = -11776.220970245033
Iteration 4500: Loss = -11776.244105664373
1
Iteration 4600: Loss = -11776.225734679663
2
Iteration 4700: Loss = -11776.22063035591
Iteration 4800: Loss = -11776.221642248935
1
Iteration 4900: Loss = -11776.220528784697
Iteration 5000: Loss = -11776.22649451218
1
Iteration 5100: Loss = -11776.220223301612
Iteration 5200: Loss = -11776.220174427574
Iteration 5300: Loss = -11776.230028628579
1
Iteration 5400: Loss = -11776.228786456528
2
Iteration 5500: Loss = -11776.220144220135
Iteration 5600: Loss = -11776.220748189791
1
Iteration 5700: Loss = -11776.221034444983
2
Iteration 5800: Loss = -11776.219777214035
Iteration 5900: Loss = -11776.21978546081
Iteration 6000: Loss = -11776.219664067981
Iteration 6100: Loss = -11776.219641229594
Iteration 6200: Loss = -11776.220741298743
1
Iteration 6300: Loss = -11776.22014993001
2
Iteration 6400: Loss = -11776.222401950734
3
Iteration 6500: Loss = -11776.217683270672
Iteration 6600: Loss = -11776.218392029134
1
Iteration 6700: Loss = -11776.220568476583
2
Iteration 6800: Loss = -11776.218401482947
3
Iteration 6900: Loss = -11776.217595937916
Iteration 7000: Loss = -11776.217627369628
Iteration 7100: Loss = -11776.217961473596
1
Iteration 7200: Loss = -11776.217530360755
Iteration 7300: Loss = -11776.217485505449
Iteration 7400: Loss = -11776.217444254002
Iteration 7500: Loss = -11776.23521434835
1
Iteration 7600: Loss = -11776.217571196097
2
Iteration 7700: Loss = -11776.217564021217
3
Iteration 7800: Loss = -11776.217415827536
Iteration 7900: Loss = -11776.218812700847
1
Iteration 8000: Loss = -11776.217348463906
Iteration 8100: Loss = -11776.217548389599
1
Iteration 8200: Loss = -11776.220174407745
2
Iteration 8300: Loss = -11776.311566786731
3
Iteration 8400: Loss = -11776.217176762542
Iteration 8500: Loss = -11776.21728248381
1
Iteration 8600: Loss = -11776.217178821613
Iteration 8700: Loss = -11776.217162013769
Iteration 8800: Loss = -11776.217136319838
Iteration 8900: Loss = -11776.236096665994
1
Iteration 9000: Loss = -11776.217155917633
Iteration 9100: Loss = -11776.217142078434
Iteration 9200: Loss = -11776.217413482176
1
Iteration 9300: Loss = -11776.217114410296
Iteration 9400: Loss = -11776.217132413032
Iteration 9500: Loss = -11776.217126183996
Iteration 9600: Loss = -11776.217085278338
Iteration 9700: Loss = -11776.221696270946
1
Iteration 9800: Loss = -11776.218787956217
2
Iteration 9900: Loss = -11776.21708137845
Iteration 10000: Loss = -11776.219754382068
1
Iteration 10100: Loss = -11776.218683224703
2
Iteration 10200: Loss = -11776.21868748039
3
Iteration 10300: Loss = -11776.225743535182
4
Iteration 10400: Loss = -11776.217933124597
5
Iteration 10500: Loss = -11776.21876732138
6
Iteration 10600: Loss = -11776.219095871977
7
Iteration 10700: Loss = -11776.223349913633
8
Iteration 10800: Loss = -11776.230349587728
9
Iteration 10900: Loss = -11776.218758319856
10
Iteration 11000: Loss = -11776.23116690247
11
Iteration 11100: Loss = -11776.221789804611
12
Iteration 11200: Loss = -11776.222573652163
13
Iteration 11300: Loss = -11776.227550061987
14
Iteration 11400: Loss = -11776.223582624487
15
Stopping early at iteration 11400 due to no improvement.
pi: tensor([[0.7726, 0.2274],
        [0.2718, 0.7282]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5162, 0.4838], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2984, 0.1078],
         [0.5046, 0.2874]],

        [[0.6360, 0.0964],
         [0.7230, 0.5135]],

        [[0.5118, 0.0985],
         [0.6247, 0.5517]],

        [[0.5780, 0.0918],
         [0.6695, 0.6096]],

        [[0.6597, 0.1003],
         [0.6097, 0.5257]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9760956584656854
Average Adjusted Rand Index: 0.9761594684462762
11786.733232998846
[0.0016114094845201586, 0.9760956584656854] [0.0, 0.9761594684462762] [12265.320912803592, 11776.223582624487]
-------------------------------------
This iteration is 91
True Objective function: Loss = -11847.572912558191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20001.22958695826
Iteration 100: Loss = -12308.988096078972
Iteration 200: Loss = -12308.459890824133
Iteration 300: Loss = -12308.289506921172
Iteration 400: Loss = -12308.164886910154
Iteration 500: Loss = -12308.056012854775
Iteration 600: Loss = -12307.992907483986
Iteration 700: Loss = -12307.951644857842
Iteration 800: Loss = -12307.912019695868
Iteration 900: Loss = -12307.870990515657
Iteration 1000: Loss = -12307.828271685625
Iteration 1100: Loss = -12307.784237575714
Iteration 1200: Loss = -12307.740673211627
Iteration 1300: Loss = -12307.699498188354
Iteration 1400: Loss = -12307.662646311846
Iteration 1500: Loss = -12307.631235969677
Iteration 1600: Loss = -12307.605537501915
Iteration 1700: Loss = -12307.584873692966
Iteration 1800: Loss = -12307.568369270304
Iteration 1900: Loss = -12307.554991316572
Iteration 2000: Loss = -12307.543916150114
Iteration 2100: Loss = -12307.534554494307
Iteration 2200: Loss = -12307.526609899582
Iteration 2300: Loss = -12307.519657945259
Iteration 2400: Loss = -12307.513545667893
Iteration 2500: Loss = -12307.50809810202
Iteration 2600: Loss = -12307.50310965983
Iteration 2700: Loss = -12307.49845392759
Iteration 2800: Loss = -12307.494051407288
Iteration 2900: Loss = -12307.489794610512
Iteration 3000: Loss = -12307.485555854082
Iteration 3100: Loss = -12307.481250759245
Iteration 3200: Loss = -12307.476844330085
Iteration 3300: Loss = -12307.47225960604
Iteration 3400: Loss = -12307.467396612596
Iteration 3500: Loss = -12307.462236678219
Iteration 3600: Loss = -12307.45668154087
Iteration 3700: Loss = -12307.450680993954
Iteration 3800: Loss = -12307.444132838527
Iteration 3900: Loss = -12307.43699322941
Iteration 4000: Loss = -12307.429164355206
Iteration 4100: Loss = -12307.420528153742
Iteration 4200: Loss = -12307.411128360673
Iteration 4300: Loss = -12307.400906183724
Iteration 4400: Loss = -12307.389945270608
Iteration 4500: Loss = -12307.378314935992
Iteration 4600: Loss = -12307.367154763248
Iteration 4700: Loss = -12307.35488419629
Iteration 4800: Loss = -12307.345495518855
Iteration 4900: Loss = -12307.331934879561
Iteration 5000: Loss = -12307.323092938095
Iteration 5100: Loss = -12307.313666313434
Iteration 5200: Loss = -12307.319681497776
1
Iteration 5300: Loss = -12307.30426814568
Iteration 5400: Loss = -12307.302195892707
Iteration 5500: Loss = -12307.301251441471
Iteration 5600: Loss = -12307.30057186301
Iteration 5700: Loss = -12307.300111081157
Iteration 5800: Loss = -12307.299753996662
Iteration 5900: Loss = -12307.299450749706
Iteration 6000: Loss = -12307.314286625788
1
Iteration 6100: Loss = -12307.29897987342
Iteration 6200: Loss = -12307.298777697166
Iteration 6300: Loss = -12307.298657613674
Iteration 6400: Loss = -12307.298670213964
Iteration 6500: Loss = -12307.298317851835
Iteration 6600: Loss = -12307.298155582712
Iteration 6700: Loss = -12307.298050330091
Iteration 6800: Loss = -12307.298025426579
Iteration 6900: Loss = -12307.297860629384
Iteration 7000: Loss = -12307.303252237745
1
Iteration 7100: Loss = -12307.297644894445
Iteration 7200: Loss = -12307.298321724918
1
Iteration 7300: Loss = -12307.297521890956
Iteration 7400: Loss = -12307.323675832082
1
Iteration 7500: Loss = -12307.297414568244
Iteration 7600: Loss = -12307.31826072175
1
Iteration 7700: Loss = -12307.297298518986
Iteration 7800: Loss = -12307.31071403278
1
Iteration 7900: Loss = -12307.297221266228
Iteration 8000: Loss = -12307.372890264201
1
Iteration 8100: Loss = -12307.297269810415
Iteration 8200: Loss = -12307.297202843502
Iteration 8300: Loss = -12307.38820111763
1
Iteration 8400: Loss = -12307.297067594784
Iteration 8500: Loss = -12307.305554532915
1
Iteration 8600: Loss = -12307.468324601457
2
Iteration 8700: Loss = -12307.300533409829
3
Iteration 8800: Loss = -12307.352650000566
4
Iteration 8900: Loss = -12307.299753745283
5
Iteration 9000: Loss = -12307.51469790739
6
Iteration 9100: Loss = -12307.30055640452
7
Iteration 9200: Loss = -12307.297285239129
8
Iteration 9300: Loss = -12307.39666003296
9
Iteration 9400: Loss = -12307.296845006238
Iteration 9500: Loss = -12307.297878301139
1
Iteration 9600: Loss = -12307.336844158803
2
Iteration 9700: Loss = -12307.296863521631
Iteration 9800: Loss = -12307.297160538239
1
Iteration 9900: Loss = -12307.296844531485
Iteration 10000: Loss = -12307.296823255154
Iteration 10100: Loss = -12307.296913864386
Iteration 10200: Loss = -12307.359964363508
1
Iteration 10300: Loss = -12307.299388885616
2
Iteration 10400: Loss = -12307.296857542728
Iteration 10500: Loss = -12307.30038914402
1
Iteration 10600: Loss = -12307.328727893317
2
Iteration 10700: Loss = -12307.296898069015
Iteration 10800: Loss = -12307.296871076445
Iteration 10900: Loss = -12307.37466418572
1
Iteration 11000: Loss = -12307.296752206556
Iteration 11100: Loss = -12307.297731701323
1
Iteration 11200: Loss = -12307.309586738138
2
Iteration 11300: Loss = -12307.296771281446
Iteration 11400: Loss = -12307.296973264885
1
Iteration 11500: Loss = -12307.296947035036
2
Iteration 11600: Loss = -12307.349305899912
3
Iteration 11700: Loss = -12307.296818434419
Iteration 11800: Loss = -12307.414121336426
1
Iteration 11900: Loss = -12307.296731813372
Iteration 12000: Loss = -12307.299909299243
1
Iteration 12100: Loss = -12307.342517755576
2
Iteration 12200: Loss = -12307.300041851211
3
Iteration 12300: Loss = -12307.507252340687
4
Iteration 12400: Loss = -12307.297243623982
5
Iteration 12500: Loss = -12307.296713958587
Iteration 12600: Loss = -12307.29717153585
1
Iteration 12700: Loss = -12307.668260062865
2
Iteration 12800: Loss = -12307.29831443814
3
Iteration 12900: Loss = -12307.350613209934
4
Iteration 13000: Loss = -12307.298457707839
5
Iteration 13100: Loss = -12307.335645254792
6
Iteration 13200: Loss = -12307.296700656721
Iteration 13300: Loss = -12307.296752275348
Iteration 13400: Loss = -12307.29678839268
Iteration 13500: Loss = -12307.298527993355
1
Iteration 13600: Loss = -12307.296735127891
Iteration 13700: Loss = -12307.299358044233
1
Iteration 13800: Loss = -12307.297025570293
2
Iteration 13900: Loss = -12307.296907239956
3
Iteration 14000: Loss = -12307.300205171046
4
Iteration 14100: Loss = -12307.296698379816
Iteration 14200: Loss = -12307.296896036112
1
Iteration 14300: Loss = -12307.29670697815
Iteration 14400: Loss = -12307.296928804366
1
Iteration 14500: Loss = -12307.298153260774
2
Iteration 14600: Loss = -12307.296995485949
3
Iteration 14700: Loss = -12307.296781829184
Iteration 14800: Loss = -12307.29670162894
Iteration 14900: Loss = -12307.297223815485
1
Iteration 15000: Loss = -12307.297059030763
2
Iteration 15100: Loss = -12307.299800117233
3
Iteration 15200: Loss = -12307.439995955925
4
Iteration 15300: Loss = -12307.468285573297
5
Iteration 15400: Loss = -12307.29734538068
6
Iteration 15500: Loss = -12307.297031764081
7
Iteration 15600: Loss = -12307.29673744735
Iteration 15700: Loss = -12307.298237659741
1
Iteration 15800: Loss = -12307.296837333126
Iteration 15900: Loss = -12307.299820586242
1
Iteration 16000: Loss = -12307.298922796692
2
Iteration 16100: Loss = -12307.516128172007
3
Iteration 16200: Loss = -12307.296670329755
Iteration 16300: Loss = -12307.296979569755
1
Iteration 16400: Loss = -12307.312684989689
2
Iteration 16500: Loss = -12307.299271775872
3
Iteration 16600: Loss = -12307.301864076368
4
Iteration 16700: Loss = -12307.297562513952
5
Iteration 16800: Loss = -12307.296600154406
Iteration 16900: Loss = -12307.299147334885
1
Iteration 17000: Loss = -12307.297811533088
2
Iteration 17100: Loss = -12307.297939977472
3
Iteration 17200: Loss = -12307.35674889935
4
Iteration 17300: Loss = -12307.31972156395
5
Iteration 17400: Loss = -12307.296661979977
Iteration 17500: Loss = -12307.297354210832
1
Iteration 17600: Loss = -12307.297106643551
2
Iteration 17700: Loss = -12307.29953804569
3
Iteration 17800: Loss = -12307.342329870677
4
Iteration 17900: Loss = -12307.29668159006
Iteration 18000: Loss = -12307.306048733672
1
Iteration 18100: Loss = -12307.296658743473
Iteration 18200: Loss = -12307.297727709903
1
Iteration 18300: Loss = -12307.315585583961
2
Iteration 18400: Loss = -12307.296716193265
Iteration 18500: Loss = -12307.29853896813
1
Iteration 18600: Loss = -12307.296693409568
Iteration 18700: Loss = -12307.297152165347
1
Iteration 18800: Loss = -12307.35536578895
2
Iteration 18900: Loss = -12307.298305137445
3
Iteration 19000: Loss = -12307.298818391782
4
Iteration 19100: Loss = -12307.29947932294
5
Iteration 19200: Loss = -12307.296619137633
Iteration 19300: Loss = -12307.298068249023
1
Iteration 19400: Loss = -12307.296598971738
Iteration 19500: Loss = -12307.316954963191
1
Iteration 19600: Loss = -12307.309591881318
2
Iteration 19700: Loss = -12307.445213723824
3
Iteration 19800: Loss = -12307.478173319594
4
Iteration 19900: Loss = -12307.297752801993
5
pi: tensor([[0.6767, 0.3233],
        [0.9485, 0.0515]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.1020e-05, 9.9991e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1945, 0.2059],
         [0.5806, 0.2039]],

        [[0.6353, 0.1919],
         [0.5224, 0.5827]],

        [[0.5534, 0.1996],
         [0.5540, 0.5804]],

        [[0.5211, 0.2031],
         [0.6940, 0.6970]],

        [[0.7072, 0.1944],
         [0.5535, 0.7038]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008894504923874512
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22602.124590829935
Iteration 100: Loss = -12309.042211919093
Iteration 200: Loss = -12308.434332421071
Iteration 300: Loss = -12308.33486149034
Iteration 400: Loss = -12308.279122644455
Iteration 500: Loss = -12308.238029064632
Iteration 600: Loss = -12308.20477044797
Iteration 700: Loss = -12308.177884585948
Iteration 800: Loss = -12308.155449117097
Iteration 900: Loss = -12308.134687244074
Iteration 1000: Loss = -12308.113271416896
Iteration 1100: Loss = -12308.089735402737
Iteration 1200: Loss = -12308.062746969084
Iteration 1300: Loss = -12308.031109726664
Iteration 1400: Loss = -12307.993652504945
Iteration 1500: Loss = -12307.949468537623
Iteration 1600: Loss = -12307.89815175963
Iteration 1700: Loss = -12307.841036185768
Iteration 1800: Loss = -12307.781457877702
Iteration 1900: Loss = -12307.724276524232
Iteration 2000: Loss = -12307.674021639032
Iteration 2100: Loss = -12307.633446774278
Iteration 2200: Loss = -12307.601119273424
Iteration 2300: Loss = -12307.576891534407
Iteration 2400: Loss = -12307.5564567637
Iteration 2500: Loss = -12307.540660247274
Iteration 2600: Loss = -12307.544456396521
1
Iteration 2700: Loss = -12307.516817925365
Iteration 2800: Loss = -12307.506998066716
Iteration 2900: Loss = -12307.4977581111
Iteration 3000: Loss = -12307.489366383534
Iteration 3100: Loss = -12307.479775239866
Iteration 3200: Loss = -12307.470406155058
Iteration 3300: Loss = -12307.460545274533
Iteration 3400: Loss = -12307.44999309304
Iteration 3500: Loss = -12307.4386690335
Iteration 3600: Loss = -12307.426395794371
Iteration 3700: Loss = -12307.413365291452
Iteration 3800: Loss = -12307.399620388891
Iteration 3900: Loss = -12307.385430193159
Iteration 4000: Loss = -12307.372701284043
Iteration 4100: Loss = -12307.357802540033
Iteration 4200: Loss = -12307.345158687785
Iteration 4300: Loss = -12307.332802381206
Iteration 4400: Loss = -12307.323176072732
Iteration 4500: Loss = -12307.315439597533
Iteration 4600: Loss = -12307.310660475148
Iteration 4700: Loss = -12307.30751126146
Iteration 4800: Loss = -12307.306310497932
Iteration 4900: Loss = -12307.304407033622
Iteration 5000: Loss = -12307.303449967612
Iteration 5100: Loss = -12307.308083548181
1
Iteration 5200: Loss = -12307.302019395853
Iteration 5300: Loss = -12307.301440944475
Iteration 5400: Loss = -12307.30195745444
1
Iteration 5500: Loss = -12307.300628935664
Iteration 5600: Loss = -12307.30028246826
Iteration 5700: Loss = -12307.299992190057
Iteration 5800: Loss = -12307.299694216676
Iteration 5900: Loss = -12307.301005294526
1
Iteration 6000: Loss = -12307.29920360734
Iteration 6100: Loss = -12307.299021221339
Iteration 6200: Loss = -12307.298888400011
Iteration 6300: Loss = -12307.298685540238
Iteration 6400: Loss = -12307.29846697616
Iteration 6500: Loss = -12307.298367215415
Iteration 6600: Loss = -12307.298255890119
Iteration 6700: Loss = -12307.299606510938
1
Iteration 6800: Loss = -12307.298647358146
2
Iteration 6900: Loss = -12307.298184014437
Iteration 7000: Loss = -12307.298139794439
Iteration 7100: Loss = -12307.297910905829
Iteration 7200: Loss = -12307.297640473498
Iteration 7300: Loss = -12307.297594550328
Iteration 7400: Loss = -12307.29890308235
1
Iteration 7500: Loss = -12307.297469840129
Iteration 7600: Loss = -12307.297556323985
Iteration 7700: Loss = -12307.30868263538
1
Iteration 7800: Loss = -12307.298588845133
2
Iteration 7900: Loss = -12307.352434685768
3
Iteration 8000: Loss = -12307.307156596087
4
Iteration 8100: Loss = -12307.297212475962
Iteration 8200: Loss = -12307.297266197293
Iteration 8300: Loss = -12307.297401059213
1
Iteration 8400: Loss = -12307.297519162856
2
Iteration 8500: Loss = -12307.301330657401
3
Iteration 8600: Loss = -12307.297703748545
4
Iteration 8700: Loss = -12307.297124943974
Iteration 8800: Loss = -12307.301171202453
1
Iteration 8900: Loss = -12307.297790251754
2
Iteration 9000: Loss = -12307.299377836556
3
Iteration 9100: Loss = -12307.301258535734
4
Iteration 9200: Loss = -12307.30453294308
5
Iteration 9300: Loss = -12307.297626056155
6
Iteration 9400: Loss = -12307.297351989939
7
Iteration 9500: Loss = -12307.298683086849
8
Iteration 9600: Loss = -12307.30001298897
9
Iteration 9700: Loss = -12307.297252783068
10
Iteration 9800: Loss = -12307.29707188066
Iteration 9900: Loss = -12307.305221637867
1
Iteration 10000: Loss = -12307.296950975084
Iteration 10100: Loss = -12307.304457961198
1
Iteration 10200: Loss = -12307.296891311122
Iteration 10300: Loss = -12307.297685080166
1
Iteration 10400: Loss = -12307.296796016593
Iteration 10500: Loss = -12307.303639622562
1
Iteration 10600: Loss = -12307.305832484573
2
Iteration 10700: Loss = -12307.299689774069
3
Iteration 10800: Loss = -12307.304111593969
4
Iteration 10900: Loss = -12307.299250360418
5
Iteration 11000: Loss = -12307.29709250677
6
Iteration 11100: Loss = -12307.312829297947
7
Iteration 11200: Loss = -12307.297175214093
8
Iteration 11300: Loss = -12307.296842972453
Iteration 11400: Loss = -12307.49062903773
1
Iteration 11500: Loss = -12307.296734725503
Iteration 11600: Loss = -12307.298091854544
1
Iteration 11700: Loss = -12307.3025959702
2
Iteration 11800: Loss = -12307.298315819862
3
Iteration 11900: Loss = -12307.30986149682
4
Iteration 12000: Loss = -12307.296789250078
Iteration 12100: Loss = -12307.311987710833
1
Iteration 12200: Loss = -12307.35714507453
2
Iteration 12300: Loss = -12307.297511650946
3
Iteration 12400: Loss = -12307.476445985258
4
Iteration 12500: Loss = -12307.299834787125
5
Iteration 12600: Loss = -12307.297531892276
6
Iteration 12700: Loss = -12307.298758752857
7
Iteration 12800: Loss = -12307.298180784994
8
Iteration 12900: Loss = -12307.300117495533
9
Iteration 13000: Loss = -12307.350841302597
10
Iteration 13100: Loss = -12307.32883460144
11
Iteration 13200: Loss = -12307.296776437433
Iteration 13300: Loss = -12307.296880768094
1
Iteration 13400: Loss = -12307.296712057754
Iteration 13500: Loss = -12307.297071193521
1
Iteration 13600: Loss = -12307.60661913217
2
Iteration 13700: Loss = -12307.296870690745
3
Iteration 13800: Loss = -12307.33546915202
4
Iteration 13900: Loss = -12307.296686175729
Iteration 14000: Loss = -12307.29708774205
1
Iteration 14100: Loss = -12307.29670879161
Iteration 14200: Loss = -12307.300112705121
1
Iteration 14300: Loss = -12307.30726794817
2
Iteration 14400: Loss = -12307.298625624224
3
Iteration 14500: Loss = -12307.296841752313
4
Iteration 14600: Loss = -12307.297907534192
5
Iteration 14700: Loss = -12307.30438017673
6
Iteration 14800: Loss = -12307.323160124415
7
Iteration 14900: Loss = -12307.303670482228
8
Iteration 15000: Loss = -12307.296719991951
Iteration 15100: Loss = -12307.299724889956
1
Iteration 15200: Loss = -12307.296915020393
2
Iteration 15300: Loss = -12307.313854456128
3
Iteration 15400: Loss = -12307.344278522787
4
Iteration 15500: Loss = -12307.29690534641
5
Iteration 15600: Loss = -12307.297509245043
6
Iteration 15700: Loss = -12307.429239064773
7
Iteration 15800: Loss = -12307.299177001234
8
Iteration 15900: Loss = -12307.29909089056
9
Iteration 16000: Loss = -12307.2968706007
10
Iteration 16100: Loss = -12307.30209917541
11
Iteration 16200: Loss = -12307.29874248032
12
Iteration 16300: Loss = -12307.31829998805
13
Iteration 16400: Loss = -12307.333622391936
14
Iteration 16500: Loss = -12307.302752259311
15
Stopping early at iteration 16500 due to no improvement.
pi: tensor([[0.0511, 0.9489],
        [0.3244, 0.6756]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9951e-01, 4.8826e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2037, 0.2059],
         [0.7117, 0.1945]],

        [[0.7155, 0.1917],
         [0.6456, 0.6134]],

        [[0.6775, 0.1994],
         [0.7056, 0.6847]],

        [[0.5675, 0.2030],
         [0.6676, 0.5567]],

        [[0.6541, 0.1944],
         [0.5095, 0.5626]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008894504923874512
Average Adjusted Rand Index: 0.0
11847.572912558191
[-0.0008894504923874512, -0.0008894504923874512] [0.0, 0.0] [12307.296857554928, 12307.302752259311]
-------------------------------------
This iteration is 92
True Objective function: Loss = -11978.638403817185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21069.6794872274
Iteration 100: Loss = -12497.01171690746
Iteration 200: Loss = -12496.410620160905
Iteration 300: Loss = -12495.612852717211
Iteration 400: Loss = -12493.469140917601
Iteration 500: Loss = -12477.929482065969
Iteration 600: Loss = -12414.526612928792
Iteration 700: Loss = -12363.711401285691
Iteration 800: Loss = -12046.601956661762
Iteration 900: Loss = -11990.400660550622
Iteration 1000: Loss = -11976.527123111306
Iteration 1100: Loss = -11976.135223936773
Iteration 1200: Loss = -11970.937846270319
Iteration 1300: Loss = -11970.81761234393
Iteration 1400: Loss = -11970.735799500773
Iteration 1500: Loss = -11970.674241274333
Iteration 1600: Loss = -11970.618438914467
Iteration 1700: Loss = -11970.554506259627
Iteration 1800: Loss = -11970.510267793099
Iteration 1900: Loss = -11970.48809885084
Iteration 2000: Loss = -11970.469994368983
Iteration 2100: Loss = -11970.45482143179
Iteration 2200: Loss = -11970.441625799198
Iteration 2300: Loss = -11970.429881636162
Iteration 2400: Loss = -11970.418103573724
Iteration 2500: Loss = -11970.09612678377
Iteration 2600: Loss = -11970.07476160429
Iteration 2700: Loss = -11970.067287627942
Iteration 2800: Loss = -11970.0605550179
Iteration 2900: Loss = -11970.05407028009
Iteration 3000: Loss = -11970.047385213658
Iteration 3100: Loss = -11970.038515444909
Iteration 3200: Loss = -11970.027362024593
Iteration 3300: Loss = -11970.023182417533
Iteration 3400: Loss = -11970.019933376096
Iteration 3500: Loss = -11970.018388765577
Iteration 3600: Loss = -11970.014417379618
Iteration 3700: Loss = -11970.012406969261
Iteration 3800: Loss = -11970.009624560125
Iteration 3900: Loss = -11970.007310953357
Iteration 4000: Loss = -11970.00499599994
Iteration 4100: Loss = -11970.002002395257
Iteration 4200: Loss = -11969.99626905426
Iteration 4300: Loss = -11969.987143321761
Iteration 4400: Loss = -11969.986887295963
Iteration 4500: Loss = -11969.985555651177
Iteration 4600: Loss = -11969.982039923996
Iteration 4700: Loss = -11969.980961072617
Iteration 4800: Loss = -11969.980139512867
Iteration 4900: Loss = -11969.980344058013
1
Iteration 5000: Loss = -11969.983369915013
2
Iteration 5100: Loss = -11969.977264498717
Iteration 5200: Loss = -11969.983609571276
1
Iteration 5300: Loss = -11969.976406702233
Iteration 5400: Loss = -11969.97517194477
Iteration 5500: Loss = -11969.974431200892
Iteration 5600: Loss = -11969.979313163007
1
Iteration 5700: Loss = -11969.973221585831
Iteration 5800: Loss = -11969.972746636735
Iteration 5900: Loss = -11969.981718235686
1
Iteration 6000: Loss = -11969.97408013093
2
Iteration 6100: Loss = -11969.971590485142
Iteration 6200: Loss = -11969.97074805685
Iteration 6300: Loss = -11969.97453052873
1
Iteration 6400: Loss = -11969.969998790191
Iteration 6500: Loss = -11969.972377693248
1
Iteration 6600: Loss = -11969.96976216601
Iteration 6700: Loss = -11969.974341544545
1
Iteration 6800: Loss = -11969.977015342849
2
Iteration 6900: Loss = -11970.006799464916
3
Iteration 7000: Loss = -11970.04236248925
4
Iteration 7100: Loss = -11969.980550670558
5
Iteration 7200: Loss = -11969.968199687672
Iteration 7300: Loss = -11969.970074013001
1
Iteration 7400: Loss = -11969.984020065132
2
Iteration 7500: Loss = -11969.974404543638
3
Iteration 7600: Loss = -11969.989109308959
4
Iteration 7700: Loss = -11969.968280763358
Iteration 7800: Loss = -11969.96724008423
Iteration 7900: Loss = -11970.003006755607
1
Iteration 8000: Loss = -11969.969602379648
2
Iteration 8100: Loss = -11969.96807387367
3
Iteration 8200: Loss = -11970.023990799595
4
Iteration 8300: Loss = -11969.96931436608
5
Iteration 8400: Loss = -11969.965977029357
Iteration 8500: Loss = -11969.967362095233
1
Iteration 8600: Loss = -11969.966273169706
2
Iteration 8700: Loss = -11969.988418196617
3
Iteration 8800: Loss = -11969.9682447919
4
Iteration 8900: Loss = -11969.969854751002
5
Iteration 9000: Loss = -11969.965208749041
Iteration 9100: Loss = -11969.965185621144
Iteration 9200: Loss = -11969.967385382624
1
Iteration 9300: Loss = -11969.972206426444
2
Iteration 9400: Loss = -11969.964906600444
Iteration 9500: Loss = -11969.96551060204
1
Iteration 9600: Loss = -11969.998215980688
2
Iteration 9700: Loss = -11970.03605311247
3
Iteration 9800: Loss = -11969.969361097304
4
Iteration 9900: Loss = -11969.965187474092
5
Iteration 10000: Loss = -11969.965062443982
6
Iteration 10100: Loss = -11969.974616006943
7
Iteration 10200: Loss = -11969.967453489862
8
Iteration 10300: Loss = -11969.96576954568
9
Iteration 10400: Loss = -11969.967892954382
10
Iteration 10500: Loss = -11969.967465229169
11
Iteration 10600: Loss = -11969.979187496418
12
Iteration 10700: Loss = -11970.060404760658
13
Iteration 10800: Loss = -11970.010991514106
14
Iteration 10900: Loss = -11969.963813207874
Iteration 11000: Loss = -11969.964185828398
1
Iteration 11100: Loss = -11969.965172216105
2
Iteration 11200: Loss = -11969.969479989535
3
Iteration 11300: Loss = -11969.972345873184
4
Iteration 11400: Loss = -11969.97428189659
5
Iteration 11500: Loss = -11969.978794836275
6
Iteration 11600: Loss = -11969.96546087096
7
Iteration 11700: Loss = -11969.982668639
8
Iteration 11800: Loss = -11969.96412612436
9
Iteration 11900: Loss = -11969.968977639304
10
Iteration 12000: Loss = -11969.963864680301
Iteration 12100: Loss = -11969.979773584917
1
Iteration 12200: Loss = -11969.964288381001
2
Iteration 12300: Loss = -11969.971328806347
3
Iteration 12400: Loss = -11969.97798523007
4
Iteration 12500: Loss = -11969.996925888443
5
Iteration 12600: Loss = -11970.025999706362
6
Iteration 12700: Loss = -11969.983276034118
7
Iteration 12800: Loss = -11970.022347129914
8
Iteration 12900: Loss = -11969.969028781503
9
Iteration 13000: Loss = -11969.97058484672
10
Iteration 13100: Loss = -11969.969691187427
11
Iteration 13200: Loss = -11969.964370058928
12
Iteration 13300: Loss = -11969.972458586679
13
Iteration 13400: Loss = -11969.986270679634
14
Iteration 13500: Loss = -11969.971901613644
15
Stopping early at iteration 13500 due to no improvement.
pi: tensor([[0.7764, 0.2236],
        [0.2764, 0.7236]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6316, 0.3684], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3048, 0.1052],
         [0.5188, 0.2941]],

        [[0.6205, 0.1001],
         [0.6854, 0.6390]],

        [[0.5858, 0.0988],
         [0.7065, 0.7302]],

        [[0.5189, 0.0966],
         [0.5938, 0.6155]],

        [[0.5311, 0.0985],
         [0.6408, 0.7022]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840240618049966
Average Adjusted Rand Index: 0.9839992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22836.48683927858
Iteration 100: Loss = -12496.10346253527
Iteration 200: Loss = -12495.159622964144
Iteration 300: Loss = -12491.315351693596
Iteration 400: Loss = -12470.850199149621
Iteration 500: Loss = -12411.38428408213
Iteration 600: Loss = -12315.902344286962
Iteration 700: Loss = -12024.397677282008
Iteration 800: Loss = -11987.927070889466
Iteration 900: Loss = -11975.878261874052
Iteration 1000: Loss = -11970.683794227774
Iteration 1100: Loss = -11970.589985793149
Iteration 1200: Loss = -11970.53394079475
Iteration 1300: Loss = -11970.49678308012
Iteration 1400: Loss = -11970.469321430732
Iteration 1500: Loss = -11970.448050662646
Iteration 1600: Loss = -11970.430764911374
Iteration 1700: Loss = -11970.415016369256
Iteration 1800: Loss = -11970.39765136848
Iteration 1900: Loss = -11970.388381564566
Iteration 2000: Loss = -11970.380804470187
Iteration 2100: Loss = -11970.374362651946
Iteration 2200: Loss = -11970.368747680785
Iteration 2300: Loss = -11970.363832838033
Iteration 2400: Loss = -11970.359579405274
Iteration 2500: Loss = -11970.355766640745
Iteration 2600: Loss = -11970.352319812142
Iteration 2700: Loss = -11970.34920799112
Iteration 2800: Loss = -11970.346227749736
Iteration 2900: Loss = -11970.343435670908
Iteration 3000: Loss = -11970.340642647952
Iteration 3100: Loss = -11970.338086554488
Iteration 3200: Loss = -11970.33639468655
Iteration 3300: Loss = -11970.334344767258
Iteration 3400: Loss = -11970.332339884626
Iteration 3500: Loss = -11970.321389524534
Iteration 3600: Loss = -11969.996346284468
Iteration 3700: Loss = -11969.994425592029
Iteration 3800: Loss = -11969.991651035489
Iteration 3900: Loss = -11969.982160121725
Iteration 4000: Loss = -11969.977355797511
Iteration 4100: Loss = -11969.976264858587
Iteration 4200: Loss = -11969.975533887355
Iteration 4300: Loss = -11969.974749193396
Iteration 4400: Loss = -11969.97410652799
Iteration 4500: Loss = -11969.974335436214
1
Iteration 4600: Loss = -11969.979890894176
2
Iteration 4700: Loss = -11969.972371546288
Iteration 4800: Loss = -11969.97201148293
Iteration 4900: Loss = -11969.971445833036
Iteration 5000: Loss = -11969.971114233895
Iteration 5100: Loss = -11969.97903254324
1
Iteration 5200: Loss = -11969.977317313638
2
Iteration 5300: Loss = -11969.969905789967
Iteration 5400: Loss = -11969.985738137999
1
Iteration 5500: Loss = -11969.969341749393
Iteration 5600: Loss = -11969.969065371002
Iteration 5700: Loss = -11969.968969306397
Iteration 5800: Loss = -11969.96853331948
Iteration 5900: Loss = -11969.96860315496
Iteration 6000: Loss = -11969.968148597229
Iteration 6100: Loss = -11969.971508813827
1
Iteration 6200: Loss = -11969.967700996605
Iteration 6300: Loss = -11969.971518768494
1
Iteration 6400: Loss = -11969.969371054003
2
Iteration 6500: Loss = -11969.967181729038
Iteration 6600: Loss = -11969.968958756835
1
Iteration 6700: Loss = -11969.981509470323
2
Iteration 6800: Loss = -11969.966766355325
Iteration 6900: Loss = -11969.978264268388
1
Iteration 7000: Loss = -11969.966461690992
Iteration 7100: Loss = -11969.969428554592
1
Iteration 7200: Loss = -11970.00944121112
2
Iteration 7300: Loss = -11969.976368460892
3
Iteration 7400: Loss = -11969.968853615454
4
Iteration 7500: Loss = -11969.966331877838
Iteration 7600: Loss = -11969.970040384538
1
Iteration 7700: Loss = -11969.965945385822
Iteration 7800: Loss = -11969.967738744454
1
Iteration 7900: Loss = -11969.976546873502
2
Iteration 8000: Loss = -11969.966292514508
3
Iteration 8100: Loss = -11969.966123406579
4
Iteration 8200: Loss = -11969.997574560215
5
Iteration 8300: Loss = -11969.970195305297
6
Iteration 8400: Loss = -11969.96961331801
7
Iteration 8500: Loss = -11969.965271856685
Iteration 8600: Loss = -11969.97127982363
1
Iteration 8700: Loss = -11969.965173133292
Iteration 8800: Loss = -11969.965149849466
Iteration 8900: Loss = -11969.964893627002
Iteration 9000: Loss = -11969.965258744269
1
Iteration 9100: Loss = -11969.965426858322
2
Iteration 9200: Loss = -11969.965124856339
3
Iteration 9300: Loss = -11969.966146175979
4
Iteration 9400: Loss = -11969.968220979867
5
Iteration 9500: Loss = -11969.966382780118
6
Iteration 9600: Loss = -11969.964803122512
Iteration 9700: Loss = -11969.964613604445
Iteration 9800: Loss = -11969.967123622788
1
Iteration 9900: Loss = -11969.969464247022
2
Iteration 10000: Loss = -11969.991141994456
3
Iteration 10100: Loss = -11969.965292842187
4
Iteration 10200: Loss = -11969.964969833693
5
Iteration 10300: Loss = -11969.967237078132
6
Iteration 10400: Loss = -11969.97037347502
7
Iteration 10500: Loss = -11969.970249272203
8
Iteration 10600: Loss = -11969.96940506499
9
Iteration 10700: Loss = -11969.966409105577
10
Iteration 10800: Loss = -11969.964591174592
Iteration 10900: Loss = -11969.967470966169
1
Iteration 11000: Loss = -11969.96646221132
2
Iteration 11100: Loss = -11969.96491352026
3
Iteration 11200: Loss = -11969.965569913804
4
Iteration 11300: Loss = -11969.971537048445
5
Iteration 11400: Loss = -11969.980488756326
6
Iteration 11500: Loss = -11969.965998762204
7
Iteration 11600: Loss = -11969.9668665097
8
Iteration 11700: Loss = -11969.96449350222
Iteration 11800: Loss = -11969.976128105178
1
Iteration 11900: Loss = -11969.965277173038
2
Iteration 12000: Loss = -11969.98360091768
3
Iteration 12100: Loss = -11969.965245142346
4
Iteration 12200: Loss = -11969.968640304089
5
Iteration 12300: Loss = -11969.975735401877
6
Iteration 12400: Loss = -11969.993961263426
7
Iteration 12500: Loss = -11969.971730444104
8
Iteration 12600: Loss = -11970.035879275913
9
Iteration 12700: Loss = -11969.963595936677
Iteration 12800: Loss = -11969.991078851712
1
Iteration 12900: Loss = -11969.965356581304
2
Iteration 13000: Loss = -11969.964733980056
3
Iteration 13100: Loss = -11969.96763141847
4
Iteration 13200: Loss = -11969.971353208413
5
Iteration 13300: Loss = -11969.9716022795
6
Iteration 13400: Loss = -11969.9678138541
7
Iteration 13500: Loss = -11969.970945078974
8
Iteration 13600: Loss = -11969.963626798928
Iteration 13700: Loss = -11969.964873141402
1
Iteration 13800: Loss = -11969.982280837035
2
Iteration 13900: Loss = -11969.96482545416
3
Iteration 14000: Loss = -11969.963302848273
Iteration 14100: Loss = -11969.96937934108
1
Iteration 14200: Loss = -11969.965957287406
2
Iteration 14300: Loss = -11969.969947415511
3
Iteration 14400: Loss = -11970.037231100947
4
Iteration 14500: Loss = -11969.984282007366
5
Iteration 14600: Loss = -11969.96464332349
6
Iteration 14700: Loss = -11969.965857381832
7
Iteration 14800: Loss = -11969.975225739912
8
Iteration 14900: Loss = -11969.963516081385
9
Iteration 15000: Loss = -11969.966129483173
10
Iteration 15100: Loss = -11969.965336305004
11
Iteration 15200: Loss = -11969.959409346111
Iteration 15300: Loss = -11969.962127496086
1
Iteration 15400: Loss = -11969.974221316152
2
Iteration 15500: Loss = -11969.963170249423
3
Iteration 15600: Loss = -11969.959314215706
Iteration 15700: Loss = -11969.958828344501
Iteration 15800: Loss = -11969.958602420009
Iteration 15900: Loss = -11969.96303940448
1
Iteration 16000: Loss = -11969.973995820463
2
Iteration 16100: Loss = -11969.960294434803
3
Iteration 16200: Loss = -11969.962807457174
4
Iteration 16300: Loss = -11970.076407496335
5
Iteration 16400: Loss = -11969.96849894535
6
Iteration 16500: Loss = -11969.958716679921
7
Iteration 16600: Loss = -11969.960842612285
8
Iteration 16700: Loss = -11969.964043797412
9
Iteration 16800: Loss = -11969.965706764468
10
Iteration 16900: Loss = -11969.958707888058
11
Iteration 17000: Loss = -11969.960281839865
12
Iteration 17100: Loss = -11969.960554760492
13
Iteration 17200: Loss = -11969.961159326996
14
Iteration 17300: Loss = -11969.958325067235
Iteration 17400: Loss = -11969.958726423343
1
Iteration 17500: Loss = -11969.959165127839
2
Iteration 17600: Loss = -11969.985125839088
3
Iteration 17700: Loss = -11969.958971361692
4
Iteration 17800: Loss = -11969.959489619607
5
Iteration 17900: Loss = -11969.958374492617
Iteration 18000: Loss = -11969.958673130834
1
Iteration 18100: Loss = -11969.960130804413
2
Iteration 18200: Loss = -11969.960348074459
3
Iteration 18300: Loss = -11969.967893920575
4
Iteration 18400: Loss = -11969.988563399284
5
Iteration 18500: Loss = -11969.963167400087
6
Iteration 18600: Loss = -11969.98705898658
7
Iteration 18700: Loss = -11969.972483839196
8
Iteration 18800: Loss = -11969.968159835897
9
Iteration 18900: Loss = -11969.984759355239
10
Iteration 19000: Loss = -11969.9930917053
11
Iteration 19100: Loss = -11970.021992240927
12
Iteration 19200: Loss = -11969.965381555121
13
Iteration 19300: Loss = -11969.960162378651
14
Iteration 19400: Loss = -11969.971564851496
15
Stopping early at iteration 19400 due to no improvement.
pi: tensor([[0.7755, 0.2245],
        [0.2764, 0.7236]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6329, 0.3671], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3049, 0.1056],
         [0.5155, 0.2945]],

        [[0.5443, 0.1002],
         [0.5301, 0.7235]],

        [[0.5241, 0.0991],
         [0.6976, 0.5510]],

        [[0.5376, 0.0969],
         [0.6153, 0.5208]],

        [[0.6337, 0.0990],
         [0.5736, 0.6940]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840240618049966
Average Adjusted Rand Index: 0.9839992163297293
11978.638403817185
[0.9840240618049966, 0.9840240618049966] [0.9839992163297293, 0.9839992163297293] [11969.971901613644, 11969.971564851496]
-------------------------------------
This iteration is 93
True Objective function: Loss = -11919.14039798985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22277.24306929461
Iteration 100: Loss = -12451.51238806516
Iteration 200: Loss = -12450.457841418576
Iteration 300: Loss = -12450.060705062591
Iteration 400: Loss = -12449.801229067916
Iteration 500: Loss = -12449.574526939497
Iteration 600: Loss = -12449.357915054266
Iteration 700: Loss = -12449.15174007662
Iteration 800: Loss = -12448.970209300229
Iteration 900: Loss = -12448.805276822075
Iteration 1000: Loss = -12448.56658783137
Iteration 1100: Loss = -12447.374221702039
Iteration 1200: Loss = -11947.410181108873
Iteration 1300: Loss = -11914.886367007379
Iteration 1400: Loss = -11914.708585309754
Iteration 1500: Loss = -11914.629187832825
Iteration 1600: Loss = -11914.565896174407
Iteration 1700: Loss = -11914.441349110053
Iteration 1800: Loss = -11914.39535263049
Iteration 1900: Loss = -11914.367660085263
Iteration 2000: Loss = -11914.341383393661
Iteration 2100: Loss = -11914.286982484304
Iteration 2200: Loss = -11914.277882611046
Iteration 2300: Loss = -11914.268790777563
Iteration 2400: Loss = -11914.268512900155
Iteration 2500: Loss = -11914.25730467864
Iteration 2600: Loss = -11914.25362687723
Iteration 2700: Loss = -11914.250382025231
Iteration 2800: Loss = -11914.247010273746
Iteration 2900: Loss = -11914.24427341292
Iteration 3000: Loss = -11914.242185453073
Iteration 3100: Loss = -11914.240355085436
Iteration 3200: Loss = -11914.238756062003
Iteration 3300: Loss = -11914.239508598892
1
Iteration 3400: Loss = -11914.235985774189
Iteration 3500: Loss = -11914.234888257783
Iteration 3600: Loss = -11914.23388545474
Iteration 3700: Loss = -11914.232681096546
Iteration 3800: Loss = -11914.233521050723
1
Iteration 3900: Loss = -11914.23046355648
Iteration 4000: Loss = -11914.228650120524
Iteration 4100: Loss = -11914.219618255305
Iteration 4200: Loss = -11914.218809840784
Iteration 4300: Loss = -11914.241280762706
1
Iteration 4400: Loss = -11914.215539048902
Iteration 4500: Loss = -11914.186235418993
Iteration 4600: Loss = -11914.185641620103
Iteration 4700: Loss = -11914.184107607343
Iteration 4800: Loss = -11914.18410140598
Iteration 4900: Loss = -11914.182697784348
Iteration 5000: Loss = -11914.18204534685
Iteration 5100: Loss = -11914.181410642148
Iteration 5200: Loss = -11914.180884885325
Iteration 5300: Loss = -11914.180965666545
Iteration 5400: Loss = -11914.180064571805
Iteration 5500: Loss = -11914.18748029726
1
Iteration 5600: Loss = -11914.17942985867
Iteration 5700: Loss = -11914.179155739916
Iteration 5800: Loss = -11914.179433700032
1
Iteration 5900: Loss = -11914.17875871441
Iteration 6000: Loss = -11914.178572153141
Iteration 6100: Loss = -11914.17857143266
Iteration 6200: Loss = -11914.178199397928
Iteration 6300: Loss = -11914.181096845214
1
Iteration 6400: Loss = -11914.17791557704
Iteration 6500: Loss = -11914.177801374823
Iteration 6600: Loss = -11914.185432448163
1
Iteration 6700: Loss = -11914.17748340968
Iteration 6800: Loss = -11914.178308116396
1
Iteration 6900: Loss = -11914.17735087471
Iteration 7000: Loss = -11914.17706949581
Iteration 7100: Loss = -11914.180394276817
1
Iteration 7200: Loss = -11914.176732607782
Iteration 7300: Loss = -11914.196234892712
1
Iteration 7400: Loss = -11914.176448479178
Iteration 7500: Loss = -11914.176386046089
Iteration 7600: Loss = -11914.176718069219
1
Iteration 7700: Loss = -11914.176199613888
Iteration 7800: Loss = -11914.176096299872
Iteration 7900: Loss = -11914.176026059344
Iteration 8000: Loss = -11914.176198960387
1
Iteration 8100: Loss = -11914.176198156472
2
Iteration 8200: Loss = -11914.176623931802
3
Iteration 8300: Loss = -11914.176229093318
4
Iteration 8400: Loss = -11914.177749903987
5
Iteration 8500: Loss = -11914.17933386362
6
Iteration 8600: Loss = -11914.176334162148
7
Iteration 8700: Loss = -11914.17864060022
8
Iteration 8800: Loss = -11914.18404971542
9
Iteration 8900: Loss = -11914.176161268828
10
Iteration 9000: Loss = -11914.175986342554
Iteration 9100: Loss = -11914.187678148737
1
Iteration 9200: Loss = -11914.17579878379
Iteration 9300: Loss = -11914.17535473161
Iteration 9400: Loss = -11914.110169043792
Iteration 9500: Loss = -11914.100136367166
Iteration 9600: Loss = -11914.0965708479
Iteration 9700: Loss = -11914.094550222728
Iteration 9800: Loss = -11914.090608331373
Iteration 9900: Loss = -11914.092287485335
1
Iteration 10000: Loss = -11914.09064437698
Iteration 10100: Loss = -11914.09435961652
1
Iteration 10200: Loss = -11914.093733262507
2
Iteration 10300: Loss = -11914.09152351199
3
Iteration 10400: Loss = -11914.092149824912
4
Iteration 10500: Loss = -11914.090500062728
Iteration 10600: Loss = -11914.097827592472
1
Iteration 10700: Loss = -11914.095817828505
2
Iteration 10800: Loss = -11914.116947763505
3
Iteration 10900: Loss = -11914.094922173785
4
Iteration 11000: Loss = -11914.109344746346
5
Iteration 11100: Loss = -11914.091087421544
6
Iteration 11200: Loss = -11914.098339300392
7
Iteration 11300: Loss = -11914.090532487162
Iteration 11400: Loss = -11914.11915224787
1
Iteration 11500: Loss = -11914.092860245226
2
Iteration 11600: Loss = -11914.093397428473
3
Iteration 11700: Loss = -11914.08906109708
Iteration 11800: Loss = -11914.08967961692
1
Iteration 11900: Loss = -11914.161620983676
2
Iteration 12000: Loss = -11914.08820841032
Iteration 12100: Loss = -11914.08797805674
Iteration 12200: Loss = -11914.088315569232
1
Iteration 12300: Loss = -11914.087799539562
Iteration 12400: Loss = -11914.109681917398
1
Iteration 12500: Loss = -11914.17233440091
2
Iteration 12600: Loss = -11914.088483743279
3
Iteration 12700: Loss = -11914.089075698155
4
Iteration 12800: Loss = -11914.127890664917
5
Iteration 12900: Loss = -11914.099737113722
6
Iteration 13000: Loss = -11914.096040982135
7
Iteration 13100: Loss = -11914.08842796253
8
Iteration 13200: Loss = -11914.101196898031
9
Iteration 13300: Loss = -11914.091137575506
10
Iteration 13400: Loss = -11914.08692056964
Iteration 13500: Loss = -11914.086910859278
Iteration 13600: Loss = -11914.088473304959
1
Iteration 13700: Loss = -11914.117489778599
2
Iteration 13800: Loss = -11914.097530524883
3
Iteration 13900: Loss = -11914.08940053238
4
Iteration 14000: Loss = -11914.087169427987
5
Iteration 14100: Loss = -11914.087088086879
6
Iteration 14200: Loss = -11914.088348275389
7
Iteration 14300: Loss = -11914.110359253002
8
Iteration 14400: Loss = -11914.086954207492
Iteration 14500: Loss = -11914.089729017105
1
Iteration 14600: Loss = -11914.113308656892
2
Iteration 14700: Loss = -11914.095292618347
3
Iteration 14800: Loss = -11914.086718183997
Iteration 14900: Loss = -11914.092050813322
1
Iteration 15000: Loss = -11914.097722615537
2
Iteration 15100: Loss = -11914.09204733767
3
Iteration 15200: Loss = -11914.086871464659
4
Iteration 15300: Loss = -11914.093538981933
5
Iteration 15400: Loss = -11914.090571320381
6
Iteration 15500: Loss = -11914.092266630127
7
Iteration 15600: Loss = -11914.09792371908
8
Iteration 15700: Loss = -11914.08719510831
9
Iteration 15800: Loss = -11914.17754303793
10
Iteration 15900: Loss = -11914.091035304311
11
Iteration 16000: Loss = -11914.086791588401
Iteration 16100: Loss = -11914.100098310106
1
Iteration 16200: Loss = -11914.146328041317
2
Iteration 16300: Loss = -11914.121587737758
3
Iteration 16400: Loss = -11914.108776808956
4
Iteration 16500: Loss = -11914.091202749643
5
Iteration 16600: Loss = -11914.094353211536
6
Iteration 16700: Loss = -11914.096667364087
7
Iteration 16800: Loss = -11914.086573169083
Iteration 16900: Loss = -11914.087395104176
1
Iteration 17000: Loss = -11914.129866277153
2
Iteration 17100: Loss = -11914.093310444667
3
Iteration 17200: Loss = -11914.089309237077
4
Iteration 17300: Loss = -11914.086297831562
Iteration 17400: Loss = -11914.091236824062
1
Iteration 17500: Loss = -11914.104166454836
2
Iteration 17600: Loss = -11914.087896693562
3
Iteration 17700: Loss = -11914.131845748998
4
Iteration 17800: Loss = -11914.100524833462
5
Iteration 17900: Loss = -11914.08656330011
6
Iteration 18000: Loss = -11914.091571388015
7
Iteration 18100: Loss = -11914.08793267416
8
Iteration 18200: Loss = -11914.087999230922
9
Iteration 18300: Loss = -11914.13980871782
10
Iteration 18400: Loss = -11914.103747223971
11
Iteration 18500: Loss = -11914.087136657346
12
Iteration 18600: Loss = -11914.088265987273
13
Iteration 18700: Loss = -11914.087510188507
14
Iteration 18800: Loss = -11914.11583344887
15
Stopping early at iteration 18800 due to no improvement.
pi: tensor([[0.7224, 0.2776],
        [0.2232, 0.7768]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4702, 0.5298], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3015, 0.1043],
         [0.5993, 0.3042]],

        [[0.6555, 0.0996],
         [0.7154, 0.7204]],

        [[0.5522, 0.0985],
         [0.6643, 0.5779]],

        [[0.6863, 0.1003],
         [0.7305, 0.6648]],

        [[0.7050, 0.0981],
         [0.6597, 0.6342]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840315401535042
Average Adjusted Rand Index: 0.9841611274691303
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21930.88388365466
Iteration 100: Loss = -12451.28324710374
Iteration 200: Loss = -12450.69745519382
Iteration 300: Loss = -12450.453732222912
Iteration 400: Loss = -12450.229575928379
Iteration 500: Loss = -12450.028233987166
Iteration 600: Loss = -12449.837880503892
Iteration 700: Loss = -12449.652936555416
Iteration 800: Loss = -12449.474982599897
Iteration 900: Loss = -12449.311770916222
Iteration 1000: Loss = -12449.173912190232
Iteration 1100: Loss = -12449.068639984936
Iteration 1200: Loss = -12448.995080759225
Iteration 1300: Loss = -12448.944960178274
Iteration 1400: Loss = -12448.907645726118
Iteration 1500: Loss = -12448.873783167712
Iteration 1600: Loss = -12448.835896132587
Iteration 1700: Loss = -12448.782975364024
Iteration 1800: Loss = -12448.662978023509
Iteration 1900: Loss = -12332.214161707503
Iteration 2000: Loss = -11925.272976316335
Iteration 2100: Loss = -11914.465928354733
Iteration 2200: Loss = -11914.401568947778
Iteration 2300: Loss = -11914.365579958367
Iteration 2400: Loss = -11914.341535138485
Iteration 2500: Loss = -11914.239110701192
Iteration 2600: Loss = -11914.224319859137
Iteration 2700: Loss = -11914.21599642367
Iteration 2800: Loss = -11914.214106675707
Iteration 2900: Loss = -11914.20333391877
Iteration 3000: Loss = -11914.197701675288
Iteration 3100: Loss = -11914.192271821608
Iteration 3200: Loss = -11914.18036465195
Iteration 3300: Loss = -11914.131074082712
Iteration 3400: Loss = -11914.129729513294
Iteration 3500: Loss = -11914.124763235803
Iteration 3600: Loss = -11914.122800774561
Iteration 3700: Loss = -11914.135371800283
1
Iteration 3800: Loss = -11914.119596726912
Iteration 3900: Loss = -11914.118162060262
Iteration 4000: Loss = -11914.116808176177
Iteration 4100: Loss = -11914.116159251129
Iteration 4200: Loss = -11914.114374978439
Iteration 4300: Loss = -11914.113283789295
Iteration 4400: Loss = -11914.134041918125
1
Iteration 4500: Loss = -11914.11128619818
Iteration 4600: Loss = -11914.110410340643
Iteration 4700: Loss = -11914.109702538359
Iteration 4800: Loss = -11914.109128047678
Iteration 4900: Loss = -11914.108450676658
Iteration 5000: Loss = -11914.107804428446
Iteration 5100: Loss = -11914.107847005323
Iteration 5200: Loss = -11914.106689531849
Iteration 5300: Loss = -11914.11416478023
1
Iteration 5400: Loss = -11914.10565315295
Iteration 5500: Loss = -11914.10883648172
1
Iteration 5600: Loss = -11914.104178886497
Iteration 5700: Loss = -11914.105854540947
1
Iteration 5800: Loss = -11914.1028280832
Iteration 5900: Loss = -11914.100173003448
Iteration 6000: Loss = -11914.09285466706
Iteration 6100: Loss = -11914.092550617155
Iteration 6200: Loss = -11914.092324941605
Iteration 6300: Loss = -11914.091241788312
Iteration 6400: Loss = -11914.090628869546
Iteration 6500: Loss = -11914.09121013804
1
Iteration 6600: Loss = -11914.090385925247
Iteration 6700: Loss = -11914.089912715206
Iteration 6800: Loss = -11914.089701100738
Iteration 6900: Loss = -11914.08949943243
Iteration 7000: Loss = -11914.08920930782
Iteration 7100: Loss = -11914.090898576069
1
Iteration 7200: Loss = -11914.088876900902
Iteration 7300: Loss = -11914.088720497382
Iteration 7400: Loss = -11914.088560853534
Iteration 7500: Loss = -11914.08830725916
Iteration 7600: Loss = -11914.088419278718
1
Iteration 7700: Loss = -11914.087883574313
Iteration 7800: Loss = -11914.090495004004
1
Iteration 7900: Loss = -11914.08765711078
Iteration 8000: Loss = -11914.093809618695
1
Iteration 8100: Loss = -11914.10878730615
2
Iteration 8200: Loss = -11914.094546427903
3
Iteration 8300: Loss = -11914.08922386656
4
Iteration 8400: Loss = -11914.08736591495
Iteration 8500: Loss = -11914.08857813564
1
Iteration 8600: Loss = -11914.087245334664
Iteration 8700: Loss = -11914.086973310476
Iteration 8800: Loss = -11914.088061342552
1
Iteration 8900: Loss = -11914.086852825794
Iteration 9000: Loss = -11914.086947800553
Iteration 9100: Loss = -11914.089299093414
1
Iteration 9200: Loss = -11914.089451741565
2
Iteration 9300: Loss = -11914.104211497986
3
Iteration 9400: Loss = -11914.123209318617
4
Iteration 9500: Loss = -11914.087305107463
5
Iteration 9600: Loss = -11914.092814554993
6
Iteration 9700: Loss = -11914.088392834303
7
Iteration 9800: Loss = -11914.089656726597
8
Iteration 9900: Loss = -11914.09470183443
9
Iteration 10000: Loss = -11914.137517971778
10
Iteration 10100: Loss = -11914.094086406009
11
Iteration 10200: Loss = -11914.09335778132
12
Iteration 10300: Loss = -11914.086339265901
Iteration 10400: Loss = -11914.087231873968
1
Iteration 10500: Loss = -11914.160177902853
2
Iteration 10600: Loss = -11914.096419214371
3
Iteration 10700: Loss = -11914.086565387202
4
Iteration 10800: Loss = -11914.093230603577
5
Iteration 10900: Loss = -11914.103700607362
6
Iteration 11000: Loss = -11914.086626827153
7
Iteration 11100: Loss = -11914.11281200845
8
Iteration 11200: Loss = -11914.088809975361
9
Iteration 11300: Loss = -11914.089856059869
10
Iteration 11400: Loss = -11914.093873052117
11
Iteration 11500: Loss = -11914.086489231158
12
Iteration 11600: Loss = -11914.088501837978
13
Iteration 11700: Loss = -11914.086168537153
Iteration 11800: Loss = -11914.095842962724
1
Iteration 11900: Loss = -11914.086011667836
Iteration 12000: Loss = -11914.086520089606
1
Iteration 12100: Loss = -11914.204494448222
2
Iteration 12200: Loss = -11914.10513818581
3
Iteration 12300: Loss = -11914.087497605544
4
Iteration 12400: Loss = -11914.102571905809
5
Iteration 12500: Loss = -11914.087410772461
6
Iteration 12600: Loss = -11914.09008555615
7
Iteration 12700: Loss = -11914.08630150716
8
Iteration 12800: Loss = -11914.086089685492
Iteration 12900: Loss = -11914.101038188463
1
Iteration 13000: Loss = -11914.086694069309
2
Iteration 13100: Loss = -11914.09721643749
3
Iteration 13200: Loss = -11914.107780401511
4
Iteration 13300: Loss = -11914.08582377067
Iteration 13400: Loss = -11914.098897550508
1
Iteration 13500: Loss = -11914.08615785817
2
Iteration 13600: Loss = -11914.101562634534
3
Iteration 13700: Loss = -11914.21105312215
4
Iteration 13800: Loss = -11914.089915185168
5
Iteration 13900: Loss = -11914.088726783002
6
Iteration 14000: Loss = -11914.100406031723
7
Iteration 14100: Loss = -11914.131197315312
8
Iteration 14200: Loss = -11914.10590437646
9
Iteration 14300: Loss = -11914.086843634386
10
Iteration 14400: Loss = -11914.086169186647
11
Iteration 14500: Loss = -11914.086493546549
12
Iteration 14600: Loss = -11914.086564627336
13
Iteration 14700: Loss = -11914.0945590252
14
Iteration 14800: Loss = -11914.115369343548
15
Stopping early at iteration 14800 due to no improvement.
pi: tensor([[0.7237, 0.2763],
        [0.2226, 0.7774]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4706, 0.5294], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3019, 0.1044],
         [0.6987, 0.3040]],

        [[0.5045, 0.0997],
         [0.5420, 0.6862]],

        [[0.5450, 0.0979],
         [0.5861, 0.7095]],

        [[0.6943, 0.0999],
         [0.5227, 0.5299]],

        [[0.6925, 0.0974],
         [0.5726, 0.6836]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840315401535042
Average Adjusted Rand Index: 0.9841611274691303
11919.14039798985
[0.9840315401535042, 0.9840315401535042] [0.9841611274691303, 0.9841611274691303] [11914.11583344887, 11914.115369343548]
-------------------------------------
This iteration is 94
True Objective function: Loss = -11945.664422099544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20104.15440621984
Iteration 100: Loss = -12400.663558590377
Iteration 200: Loss = -12400.225360695165
Iteration 300: Loss = -12400.095415503376
Iteration 400: Loss = -12399.930431473142
Iteration 500: Loss = -12397.425448820313
Iteration 600: Loss = -12395.927722907796
Iteration 700: Loss = -12395.291083329128
Iteration 800: Loss = -12394.832005487693
Iteration 900: Loss = -12394.624897961225
Iteration 1000: Loss = -12394.506090125757
Iteration 1100: Loss = -12394.423316271319
Iteration 1200: Loss = -12394.362187931785
Iteration 1300: Loss = -12394.316826926777
Iteration 1400: Loss = -12394.282990474829
Iteration 1500: Loss = -12394.257423038283
Iteration 1600: Loss = -12394.23738187516
Iteration 1700: Loss = -12394.220447237007
Iteration 1800: Loss = -12394.204050618173
Iteration 1900: Loss = -12394.184994279824
Iteration 2000: Loss = -12394.158540661974
Iteration 2100: Loss = -12394.117341601512
Iteration 2200: Loss = -12394.055142005589
Iteration 2300: Loss = -12393.979645045863
Iteration 2400: Loss = -12393.907764924055
Iteration 2500: Loss = -12393.84701646788
Iteration 2600: Loss = -12393.797745624712
Iteration 2700: Loss = -12393.7584935191
Iteration 2800: Loss = -12393.727754870913
Iteration 2900: Loss = -12393.704003382978
Iteration 3000: Loss = -12393.685680182254
Iteration 3100: Loss = -12393.671518428338
Iteration 3200: Loss = -12393.660404825188
Iteration 3300: Loss = -12393.651506925697
Iteration 3400: Loss = -12393.644316773198
Iteration 3500: Loss = -12393.638406041202
Iteration 3600: Loss = -12393.63354445391
Iteration 3700: Loss = -12393.629429212215
Iteration 3800: Loss = -12393.62595708006
Iteration 3900: Loss = -12393.623026455456
Iteration 4000: Loss = -12393.620498757913
Iteration 4100: Loss = -12393.618307836225
Iteration 4200: Loss = -12393.61639591372
Iteration 4300: Loss = -12393.61471944434
Iteration 4400: Loss = -12393.61327667073
Iteration 4500: Loss = -12393.611991189397
Iteration 4600: Loss = -12393.610819059357
Iteration 4700: Loss = -12393.609775989291
Iteration 4800: Loss = -12393.608890587822
Iteration 4900: Loss = -12393.608119553746
Iteration 5000: Loss = -12393.60734340447
Iteration 5100: Loss = -12393.606719040521
Iteration 5200: Loss = -12393.60609343545
Iteration 5300: Loss = -12393.60559614705
Iteration 5400: Loss = -12393.60509745827
Iteration 5500: Loss = -12393.604668957707
Iteration 5600: Loss = -12393.604313619917
Iteration 5700: Loss = -12393.603927511975
Iteration 5800: Loss = -12393.603882179887
Iteration 5900: Loss = -12393.603315661263
Iteration 6000: Loss = -12393.603069487273
Iteration 6100: Loss = -12393.603041772409
Iteration 6200: Loss = -12393.603977362009
1
Iteration 6300: Loss = -12393.602327056014
Iteration 6400: Loss = -12393.602286365194
Iteration 6500: Loss = -12393.601992906437
Iteration 6600: Loss = -12393.601899465955
Iteration 6700: Loss = -12393.60171590539
Iteration 6800: Loss = -12393.601627011638
Iteration 6900: Loss = -12393.60177450475
1
Iteration 7000: Loss = -12393.601578929147
Iteration 7100: Loss = -12393.602761906432
1
Iteration 7200: Loss = -12393.601217894973
Iteration 7300: Loss = -12393.656606103938
1
Iteration 7400: Loss = -12393.6051687878
2
Iteration 7500: Loss = -12393.602783654334
3
Iteration 7600: Loss = -12393.60080879293
Iteration 7700: Loss = -12393.605414471183
1
Iteration 7800: Loss = -12393.636886478022
2
Iteration 7900: Loss = -12393.600687771548
Iteration 8000: Loss = -12393.612299028124
1
Iteration 8100: Loss = -12393.622696749375
2
Iteration 8200: Loss = -12393.610245903486
3
Iteration 8300: Loss = -12393.602145460278
4
Iteration 8400: Loss = -12393.600853788215
5
Iteration 8500: Loss = -12393.600841134492
6
Iteration 8600: Loss = -12393.601502788484
7
Iteration 8700: Loss = -12393.616753606168
8
Iteration 8800: Loss = -12393.600262284206
Iteration 8900: Loss = -12393.60062961242
1
Iteration 9000: Loss = -12393.60025574529
Iteration 9100: Loss = -12393.601038659901
1
Iteration 9200: Loss = -12393.600169196978
Iteration 9300: Loss = -12393.66285502851
1
Iteration 9400: Loss = -12393.600179365045
Iteration 9500: Loss = -12393.60015124848
Iteration 9600: Loss = -12393.604738539014
1
Iteration 9700: Loss = -12393.6001299484
Iteration 9800: Loss = -12393.600094360307
Iteration 9900: Loss = -12393.600798000729
1
Iteration 10000: Loss = -12393.600059589051
Iteration 10100: Loss = -12393.60003899488
Iteration 10200: Loss = -12393.600045920019
Iteration 10300: Loss = -12393.600386286373
1
Iteration 10400: Loss = -12393.600007533109
Iteration 10500: Loss = -12393.60125239154
1
Iteration 10600: Loss = -12393.600675590056
2
Iteration 10700: Loss = -12393.600052334512
Iteration 10800: Loss = -12393.600429229848
1
Iteration 10900: Loss = -12393.850522107337
2
Iteration 11000: Loss = -12393.599974621664
Iteration 11100: Loss = -12393.607557356572
1
Iteration 11200: Loss = -12393.606832755118
2
Iteration 11300: Loss = -12393.599958406587
Iteration 11400: Loss = -12393.608285484506
1
Iteration 11500: Loss = -12393.599905458252
Iteration 11600: Loss = -12393.600677526767
1
Iteration 11700: Loss = -12393.599928920214
Iteration 11800: Loss = -12393.608375033657
1
Iteration 11900: Loss = -12393.600341428682
2
Iteration 12000: Loss = -12393.600765313542
3
Iteration 12100: Loss = -12393.627412466209
4
Iteration 12200: Loss = -12393.600093979481
5
Iteration 12300: Loss = -12393.601620664605
6
Iteration 12400: Loss = -12393.600441775532
7
Iteration 12500: Loss = -12393.600252120968
8
Iteration 12600: Loss = -12393.599947640876
Iteration 12700: Loss = -12393.600833917335
1
Iteration 12800: Loss = -12393.604955218985
2
Iteration 12900: Loss = -12393.600058037691
3
Iteration 13000: Loss = -12393.60360727287
4
Iteration 13100: Loss = -12393.599896113778
Iteration 13200: Loss = -12393.600465506897
1
Iteration 13300: Loss = -12393.601128023753
2
Iteration 13400: Loss = -12393.601501371899
3
Iteration 13500: Loss = -12393.605619833683
4
Iteration 13600: Loss = -12393.599887649953
Iteration 13700: Loss = -12393.600184214834
1
Iteration 13800: Loss = -12393.605692467003
2
Iteration 13900: Loss = -12393.600435964525
3
Iteration 14000: Loss = -12393.600801790879
4
Iteration 14100: Loss = -12393.60086033904
5
Iteration 14200: Loss = -12393.599884915744
Iteration 14300: Loss = -12393.599971895837
Iteration 14400: Loss = -12393.600568593376
1
Iteration 14500: Loss = -12393.627548114964
2
Iteration 14600: Loss = -12393.603569284902
3
Iteration 14700: Loss = -12393.601890778886
4
Iteration 14800: Loss = -12393.60894196017
5
Iteration 14900: Loss = -12393.661964230248
6
Iteration 15000: Loss = -12393.600930487559
7
Iteration 15100: Loss = -12393.60023903729
8
Iteration 15200: Loss = -12393.600573922737
9
Iteration 15300: Loss = -12393.600574371509
10
Iteration 15400: Loss = -12393.619505637922
11
Iteration 15500: Loss = -12393.599862120827
Iteration 15600: Loss = -12393.60519872837
1
Iteration 15700: Loss = -12393.600138877931
2
Iteration 15800: Loss = -12393.600170813801
3
Iteration 15900: Loss = -12393.629888614947
4
Iteration 16000: Loss = -12393.614133021807
5
Iteration 16100: Loss = -12393.600069306936
6
Iteration 16200: Loss = -12393.60129953225
7
Iteration 16300: Loss = -12393.599876353916
Iteration 16400: Loss = -12393.62981833121
1
Iteration 16500: Loss = -12393.610713963839
2
Iteration 16600: Loss = -12393.611832988487
3
Iteration 16700: Loss = -12393.609531916833
4
Iteration 16800: Loss = -12393.600440953069
5
Iteration 16900: Loss = -12393.599846209823
Iteration 17000: Loss = -12393.685797006365
1
Iteration 17100: Loss = -12393.599871995493
Iteration 17200: Loss = -12393.621593352307
1
Iteration 17300: Loss = -12393.634287057414
2
Iteration 17400: Loss = -12393.600303178324
3
Iteration 17500: Loss = -12393.599882500117
Iteration 17600: Loss = -12393.635926368432
1
Iteration 17700: Loss = -12393.5998539263
Iteration 17800: Loss = -12393.607994716485
1
Iteration 17900: Loss = -12393.606572755058
2
Iteration 18000: Loss = -12393.635434222866
3
Iteration 18100: Loss = -12393.601311020924
4
Iteration 18200: Loss = -12393.600093194807
5
Iteration 18300: Loss = -12393.60021873377
6
Iteration 18400: Loss = -12393.600934820448
7
Iteration 18500: Loss = -12393.600546524342
8
Iteration 18600: Loss = -12393.638355760299
9
Iteration 18700: Loss = -12393.603321670586
10
Iteration 18800: Loss = -12393.600218569813
11
Iteration 18900: Loss = -12393.601479979992
12
Iteration 19000: Loss = -12393.599980842251
13
Iteration 19100: Loss = -12393.60033948667
14
Iteration 19200: Loss = -12393.641227246973
15
Stopping early at iteration 19200 due to no improvement.
pi: tensor([[0.0573, 0.9427],
        [0.0263, 0.9737]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9990, 0.0010], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2088, 0.2076],
         [0.7073, 0.1987]],

        [[0.5917, 0.1845],
         [0.5824, 0.6370]],

        [[0.5183, 0.3458],
         [0.5594, 0.5560]],

        [[0.5554, 0.1920],
         [0.7052, 0.7272]],

        [[0.7271, 0.1245],
         [0.5790, 0.7201]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001370415835143931
Average Adjusted Rand Index: -0.0020426905629922355
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21785.536641859155
Iteration 100: Loss = -12400.169945036423
Iteration 200: Loss = -12399.485649292394
Iteration 300: Loss = -12395.96337346617
Iteration 400: Loss = -12395.335709612636
Iteration 500: Loss = -12394.983264179316
Iteration 600: Loss = -12394.841131803421
Iteration 700: Loss = -12394.768599771318
Iteration 800: Loss = -12394.716720488235
Iteration 900: Loss = -12394.672341997153
Iteration 1000: Loss = -12394.629399285232
Iteration 1100: Loss = -12394.58478984851
Iteration 1200: Loss = -12394.535888606852
Iteration 1300: Loss = -12394.480454925184
Iteration 1400: Loss = -12394.416947419953
Iteration 1500: Loss = -12394.344457104997
Iteration 1600: Loss = -12394.262915595022
Iteration 1700: Loss = -12394.175924036854
Iteration 1800: Loss = -12394.100445724041
Iteration 1900: Loss = -12394.053314508456
Iteration 2000: Loss = -12394.02617215515
Iteration 2100: Loss = -12394.007351195036
Iteration 2200: Loss = -12393.991520533711
Iteration 2300: Loss = -12393.977335974549
Iteration 2400: Loss = -12393.964528558077
Iteration 2500: Loss = -12393.953268891233
Iteration 2600: Loss = -12393.943552240855
Iteration 2700: Loss = -12393.935249148799
Iteration 2800: Loss = -12393.928153316374
Iteration 2900: Loss = -12393.92219551862
Iteration 3000: Loss = -12393.917189155929
Iteration 3100: Loss = -12393.91299443121
Iteration 3200: Loss = -12393.909412085246
Iteration 3300: Loss = -12393.906358210616
Iteration 3400: Loss = -12393.903736004038
Iteration 3500: Loss = -12393.901481652927
Iteration 3600: Loss = -12393.899592639042
Iteration 3700: Loss = -12393.897958765234
Iteration 3800: Loss = -12393.896476786666
Iteration 3900: Loss = -12393.895265929183
Iteration 4000: Loss = -12393.89409536703
Iteration 4100: Loss = -12393.893144943177
Iteration 4200: Loss = -12393.892293338964
Iteration 4300: Loss = -12393.89152573046
Iteration 4400: Loss = -12393.890908995169
Iteration 4500: Loss = -12393.890243228321
Iteration 4600: Loss = -12393.889783563242
Iteration 4700: Loss = -12393.889273574958
Iteration 4800: Loss = -12393.888832259805
Iteration 4900: Loss = -12393.888462275732
Iteration 5000: Loss = -12393.888088471318
Iteration 5100: Loss = -12393.887850477862
Iteration 5200: Loss = -12393.887513473777
Iteration 5300: Loss = -12393.887303665548
Iteration 5400: Loss = -12393.887084991444
Iteration 5500: Loss = -12393.886849097455
Iteration 5600: Loss = -12393.886647879155
Iteration 5700: Loss = -12393.886529534915
Iteration 5800: Loss = -12393.886343575621
Iteration 5900: Loss = -12393.886231323126
Iteration 6000: Loss = -12393.886039566964
Iteration 6100: Loss = -12393.885967885111
Iteration 6200: Loss = -12393.885891173792
Iteration 6300: Loss = -12393.885742917266
Iteration 6400: Loss = -12393.887062229285
1
Iteration 6500: Loss = -12393.885561881472
Iteration 6600: Loss = -12393.885544881872
Iteration 6700: Loss = -12393.885427957137
Iteration 6800: Loss = -12393.885418102827
Iteration 6900: Loss = -12393.885393282764
Iteration 7000: Loss = -12393.885280933397
Iteration 7100: Loss = -12393.885214831118
Iteration 7200: Loss = -12393.885152446483
Iteration 7300: Loss = -12393.885149405118
Iteration 7400: Loss = -12393.88512868428
Iteration 7500: Loss = -12393.885070811746
Iteration 7600: Loss = -12393.885034024148
Iteration 7700: Loss = -12393.885193410517
1
Iteration 7800: Loss = -12393.901080690881
2
Iteration 7900: Loss = -12393.885020224981
Iteration 8000: Loss = -12393.887558583347
1
Iteration 8100: Loss = -12393.88490424149
Iteration 8200: Loss = -12393.885539458628
1
Iteration 8300: Loss = -12393.884884849258
Iteration 8400: Loss = -12393.884846750781
Iteration 8500: Loss = -12393.886262066488
1
Iteration 8600: Loss = -12393.884833196822
Iteration 8700: Loss = -12393.885431674747
1
Iteration 8800: Loss = -12393.884974173961
2
Iteration 8900: Loss = -12393.887258661733
3
Iteration 9000: Loss = -12393.88488958483
Iteration 9100: Loss = -12393.884825832136
Iteration 9200: Loss = -12393.885747103996
1
Iteration 9300: Loss = -12393.884832323009
Iteration 9400: Loss = -12393.884718745077
Iteration 9500: Loss = -12394.496115888853
1
Iteration 9600: Loss = -12393.884726890803
Iteration 9700: Loss = -12393.884725577982
Iteration 9800: Loss = -12393.884779229635
Iteration 9900: Loss = -12393.88498210693
1
Iteration 10000: Loss = -12393.887125712656
2
Iteration 10100: Loss = -12393.884726674885
Iteration 10200: Loss = -12393.885287376814
1
Iteration 10300: Loss = -12393.885147225525
2
Iteration 10400: Loss = -12393.94545068488
3
Iteration 10500: Loss = -12393.898870058005
4
Iteration 10600: Loss = -12393.884701520128
Iteration 10700: Loss = -12393.89271657349
1
Iteration 10800: Loss = -12393.884681039453
Iteration 10900: Loss = -12393.983193268063
1
Iteration 11000: Loss = -12393.885225443688
2
Iteration 11100: Loss = -12393.884754525758
Iteration 11200: Loss = -12393.88683652024
1
Iteration 11300: Loss = -12393.891132063443
2
Iteration 11400: Loss = -12393.884728874842
Iteration 11500: Loss = -12393.896361271567
1
Iteration 11600: Loss = -12393.884666076423
Iteration 11700: Loss = -12393.884795090751
1
Iteration 11800: Loss = -12393.89005100548
2
Iteration 11900: Loss = -12393.88969983764
3
Iteration 12000: Loss = -12393.896995286446
4
Iteration 12100: Loss = -12393.932490606196
5
Iteration 12200: Loss = -12393.884686483003
Iteration 12300: Loss = -12393.884770524453
Iteration 12400: Loss = -12393.910112085008
1
Iteration 12500: Loss = -12393.884691218456
Iteration 12600: Loss = -12393.884685682428
Iteration 12700: Loss = -12393.88481561901
1
Iteration 12800: Loss = -12393.884670166335
Iteration 12900: Loss = -12393.884854456126
1
Iteration 13000: Loss = -12393.884666380745
Iteration 13100: Loss = -12393.904416935358
1
Iteration 13200: Loss = -12393.932456921151
2
Iteration 13300: Loss = -12393.884725076783
Iteration 13400: Loss = -12393.884811236281
Iteration 13500: Loss = -12393.968678453695
1
Iteration 13600: Loss = -12393.916751325087
2
Iteration 13700: Loss = -12393.885071659839
3
Iteration 13800: Loss = -12393.88713154044
4
Iteration 13900: Loss = -12393.902111284475
5
Iteration 14000: Loss = -12393.884682036147
Iteration 14100: Loss = -12393.961753000349
1
Iteration 14200: Loss = -12393.983530855221
2
Iteration 14300: Loss = -12393.884686925277
Iteration 14400: Loss = -12393.892239898907
1
Iteration 14500: Loss = -12393.900538563217
2
Iteration 14600: Loss = -12393.904786338791
3
Iteration 14700: Loss = -12393.922089728956
4
Iteration 14800: Loss = -12394.107661270797
5
Iteration 14900: Loss = -12393.88464954367
Iteration 15000: Loss = -12393.894568350122
1
Iteration 15100: Loss = -12393.884728481085
Iteration 15200: Loss = -12393.890431090356
1
Iteration 15300: Loss = -12393.88530620296
2
Iteration 15400: Loss = -12393.884714960686
Iteration 15500: Loss = -12394.008912437768
1
Iteration 15600: Loss = -12393.89790537527
2
Iteration 15700: Loss = -12393.888246273571
3
Iteration 15800: Loss = -12393.890412218687
4
Iteration 15900: Loss = -12393.887339271427
5
Iteration 16000: Loss = -12393.899980939763
6
Iteration 16100: Loss = -12393.88466437768
Iteration 16200: Loss = -12393.885083631947
1
Iteration 16300: Loss = -12393.889104432768
2
Iteration 16400: Loss = -12393.886667168947
3
Iteration 16500: Loss = -12393.91333581541
4
Iteration 16600: Loss = -12393.887215319977
5
Iteration 16700: Loss = -12393.884699617527
Iteration 16800: Loss = -12393.886991937989
1
Iteration 16900: Loss = -12393.887269907062
2
Iteration 17000: Loss = -12393.888862162004
3
Iteration 17100: Loss = -12393.889681412518
4
Iteration 17200: Loss = -12393.884716381917
Iteration 17300: Loss = -12393.885381453127
1
Iteration 17400: Loss = -12393.926045044145
2
Iteration 17500: Loss = -12393.885210959392
3
Iteration 17600: Loss = -12393.885073171328
4
Iteration 17700: Loss = -12393.88937618187
5
Iteration 17800: Loss = -12393.884638916032
Iteration 17900: Loss = -12393.884916739991
1
Iteration 18000: Loss = -12393.890132571667
2
Iteration 18100: Loss = -12393.954384647272
3
Iteration 18200: Loss = -12393.903358688845
4
Iteration 18300: Loss = -12394.098102740416
5
Iteration 18400: Loss = -12393.884975461757
6
Iteration 18500: Loss = -12393.950792162139
7
Iteration 18600: Loss = -12393.885443734176
8
Iteration 18700: Loss = -12393.885027923008
9
Iteration 18800: Loss = -12393.905507791651
10
Iteration 18900: Loss = -12393.888731579083
11
Iteration 19000: Loss = -12393.886465976675
12
Iteration 19100: Loss = -12393.902546212645
13
Iteration 19200: Loss = -12393.884685183268
Iteration 19300: Loss = -12393.924609798041
1
Iteration 19400: Loss = -12393.886137292178
2
Iteration 19500: Loss = -12393.887428601274
3
Iteration 19600: Loss = -12393.950027310771
4
Iteration 19700: Loss = -12393.905838163635
5
Iteration 19800: Loss = -12393.900860841797
6
Iteration 19900: Loss = -12393.927475989209
7
pi: tensor([[0.9827, 0.0173],
        [0.9194, 0.0806]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0029, 0.9971], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.2073],
         [0.5720, 0.2070]],

        [[0.5925, 0.1885],
         [0.6124, 0.5560]],

        [[0.5955, 0.3484],
         [0.6811, 0.5825]],

        [[0.5315, 0.1993],
         [0.5237, 0.5384]],

        [[0.6994, 0.2936],
         [0.5941, 0.5152]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: -0.007726325420627255
Global Adjusted Rand Index: -0.0019205175164879894
Average Adjusted Rand Index: -0.003587955647117687
11945.664422099544
[-0.001370415835143931, -0.0019205175164879894] [-0.0020426905629922355, -0.003587955647117687] [12393.641227246973, 12393.88472023993]
-------------------------------------
This iteration is 95
True Objective function: Loss = -11923.184240683191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21410.288597305393
Iteration 100: Loss = -12437.93331281586
Iteration 200: Loss = -12437.409423922492
Iteration 300: Loss = -12437.181255846344
Iteration 400: Loss = -12436.701137728522
Iteration 500: Loss = -12436.268131923067
Iteration 600: Loss = -12436.171335736914
Iteration 700: Loss = -12436.102235555572
Iteration 800: Loss = -12436.042957345613
Iteration 900: Loss = -12435.992651158975
Iteration 1000: Loss = -12435.953030487977
Iteration 1100: Loss = -12435.923196890275
Iteration 1200: Loss = -12435.900685000162
Iteration 1300: Loss = -12435.883240641891
Iteration 1400: Loss = -12435.869226470235
Iteration 1500: Loss = -12435.857696647685
Iteration 1600: Loss = -12435.847990478765
Iteration 1700: Loss = -12435.83969618932
Iteration 1800: Loss = -12435.832526678934
Iteration 1900: Loss = -12435.826192227658
Iteration 2000: Loss = -12435.820670225452
Iteration 2100: Loss = -12435.815740215041
Iteration 2200: Loss = -12435.811442094457
Iteration 2300: Loss = -12435.807563694108
Iteration 2400: Loss = -12435.804147236147
Iteration 2500: Loss = -12435.80109597156
Iteration 2600: Loss = -12435.798398544408
Iteration 2700: Loss = -12435.795984951617
Iteration 2800: Loss = -12435.793781949822
Iteration 2900: Loss = -12435.791772462544
Iteration 3000: Loss = -12435.790020566179
Iteration 3100: Loss = -12435.788353962118
Iteration 3200: Loss = -12435.786852866246
Iteration 3300: Loss = -12435.785458252567
Iteration 3400: Loss = -12435.78417547807
Iteration 3500: Loss = -12435.782980608921
Iteration 3600: Loss = -12435.781922971591
Iteration 3700: Loss = -12435.780926663207
Iteration 3800: Loss = -12435.780134745108
Iteration 3900: Loss = -12435.779172026845
Iteration 4000: Loss = -12435.778360003855
Iteration 4100: Loss = -12435.778089566518
Iteration 4200: Loss = -12435.776936229002
Iteration 4300: Loss = -12435.776336993573
Iteration 4400: Loss = -12435.775747835556
Iteration 4500: Loss = -12435.77516903115
Iteration 4600: Loss = -12435.775813832197
1
Iteration 4700: Loss = -12435.774184933873
Iteration 4800: Loss = -12435.773772241091
Iteration 4900: Loss = -12435.773335088514
Iteration 5000: Loss = -12435.772890867202
Iteration 5100: Loss = -12435.77548726144
1
Iteration 5200: Loss = -12435.772152447236
Iteration 5300: Loss = -12435.771846238418
Iteration 5400: Loss = -12435.771577728518
Iteration 5500: Loss = -12435.77125388361
Iteration 5600: Loss = -12435.7709887865
Iteration 5700: Loss = -12435.770720316823
Iteration 5800: Loss = -12435.770481790423
Iteration 5900: Loss = -12435.772886466655
1
Iteration 6000: Loss = -12435.769994184435
Iteration 6100: Loss = -12435.769812892062
Iteration 6200: Loss = -12435.76959605139
Iteration 6300: Loss = -12435.769374473814
Iteration 6400: Loss = -12435.77323256791
1
Iteration 6500: Loss = -12435.771894478066
2
Iteration 6600: Loss = -12435.76917568299
Iteration 6700: Loss = -12435.7687426059
Iteration 6800: Loss = -12435.768577611927
Iteration 6900: Loss = -12435.771207863398
1
Iteration 7000: Loss = -12435.857924672737
2
Iteration 7100: Loss = -12435.768253569426
Iteration 7200: Loss = -12435.768157412049
Iteration 7300: Loss = -12435.768291770277
1
Iteration 7400: Loss = -12435.767917147685
Iteration 7500: Loss = -12435.767760045537
Iteration 7600: Loss = -12435.76806707596
1
Iteration 7700: Loss = -12435.767585507241
Iteration 7800: Loss = -12435.769053126187
1
Iteration 7900: Loss = -12435.767428997462
Iteration 8000: Loss = -12435.767356948836
Iteration 8100: Loss = -12435.780071225163
1
Iteration 8200: Loss = -12435.767192569934
Iteration 8300: Loss = -12435.767153367127
Iteration 8400: Loss = -12436.277878785306
1
Iteration 8500: Loss = -12435.767070225218
Iteration 8600: Loss = -12435.76696908798
Iteration 8700: Loss = -12435.76690279018
Iteration 8800: Loss = -12435.773813786383
1
Iteration 8900: Loss = -12435.766832715348
Iteration 9000: Loss = -12435.766785085842
Iteration 9100: Loss = -12435.903973006078
1
Iteration 9200: Loss = -12435.766722095304
Iteration 9300: Loss = -12435.766634171392
Iteration 9400: Loss = -12435.967724803402
1
Iteration 9500: Loss = -12435.766607172622
Iteration 9600: Loss = -12435.766983628098
1
Iteration 9700: Loss = -12435.76877435657
2
Iteration 9800: Loss = -12435.809770858912
3
Iteration 9900: Loss = -12435.766490678883
Iteration 10000: Loss = -12435.76778159327
1
Iteration 10100: Loss = -12435.800612368612
2
Iteration 10200: Loss = -12435.768709543785
3
Iteration 10300: Loss = -12435.787646291146
4
Iteration 10400: Loss = -12435.769729690599
5
Iteration 10500: Loss = -12435.766401125122
Iteration 10600: Loss = -12435.766394906415
Iteration 10700: Loss = -12435.766322639385
Iteration 10800: Loss = -12435.767917252962
1
Iteration 10900: Loss = -12435.76735581586
2
Iteration 11000: Loss = -12435.76629408741
Iteration 11100: Loss = -12435.773273131823
1
Iteration 11200: Loss = -12435.768967916612
2
Iteration 11300: Loss = -12435.773140995543
3
Iteration 11400: Loss = -12435.795241534397
4
Iteration 11500: Loss = -12435.76619519034
Iteration 11600: Loss = -12435.767113401085
1
Iteration 11700: Loss = -12435.766143718347
Iteration 11800: Loss = -12435.767900753024
1
Iteration 11900: Loss = -12435.766134207013
Iteration 12000: Loss = -12435.766485915412
1
Iteration 12100: Loss = -12435.76613033873
Iteration 12200: Loss = -12435.76928957169
1
Iteration 12300: Loss = -12435.766091533102
Iteration 12400: Loss = -12435.792203051817
1
Iteration 12500: Loss = -12435.875162726816
2
Iteration 12600: Loss = -12435.776690549665
3
Iteration 12700: Loss = -12435.766904676275
4
Iteration 12800: Loss = -12435.777611182235
5
Iteration 12900: Loss = -12435.766449526505
6
Iteration 13000: Loss = -12435.77076824603
7
Iteration 13100: Loss = -12435.767991040439
8
Iteration 13200: Loss = -12435.773009101507
9
Iteration 13300: Loss = -12435.766912197632
10
Iteration 13400: Loss = -12435.768312863087
11
Iteration 13500: Loss = -12435.770437432111
12
Iteration 13600: Loss = -12435.766602876976
13
Iteration 13700: Loss = -12435.765995706879
Iteration 13800: Loss = -12435.817672521825
1
Iteration 13900: Loss = -12435.841869573112
2
Iteration 14000: Loss = -12435.7660797462
Iteration 14100: Loss = -12435.76607165905
Iteration 14200: Loss = -12435.766118217532
Iteration 14300: Loss = -12435.766351550077
1
Iteration 14400: Loss = -12435.769258202743
2
Iteration 14500: Loss = -12435.767060634036
3
Iteration 14600: Loss = -12435.766955534194
4
Iteration 14700: Loss = -12435.77064675001
5
Iteration 14800: Loss = -12435.76597868083
Iteration 14900: Loss = -12435.76612583184
1
Iteration 15000: Loss = -12435.778847403491
2
Iteration 15100: Loss = -12435.765988290925
Iteration 15200: Loss = -12435.850912404096
1
Iteration 15300: Loss = -12435.765958443484
Iteration 15400: Loss = -12435.766071857812
1
Iteration 15500: Loss = -12435.892677025628
2
Iteration 15600: Loss = -12435.773284645955
3
Iteration 15700: Loss = -12435.767259900495
4
Iteration 15800: Loss = -12435.768638729482
5
Iteration 15900: Loss = -12435.780546159855
6
Iteration 16000: Loss = -12435.787038991062
7
Iteration 16100: Loss = -12435.778289427715
8
Iteration 16200: Loss = -12435.767418825888
9
Iteration 16300: Loss = -12435.766381923106
10
Iteration 16400: Loss = -12435.767402684305
11
Iteration 16500: Loss = -12435.766413445377
12
Iteration 16600: Loss = -12435.766041526324
Iteration 16700: Loss = -12435.766443784889
1
Iteration 16800: Loss = -12435.857895037887
2
Iteration 16900: Loss = -12435.766696465042
3
Iteration 17000: Loss = -12435.765953596794
Iteration 17100: Loss = -12435.777398777775
1
Iteration 17200: Loss = -12435.774167427446
2
Iteration 17300: Loss = -12435.770368330308
3
Iteration 17400: Loss = -12435.769233412862
4
Iteration 17500: Loss = -12435.78929276094
5
Iteration 17600: Loss = -12435.767279606942
6
Iteration 17700: Loss = -12435.765955482164
Iteration 17800: Loss = -12435.77131783491
1
Iteration 17900: Loss = -12435.873226655629
2
Iteration 18000: Loss = -12435.819264218395
3
Iteration 18100: Loss = -12435.765957939648
Iteration 18200: Loss = -12435.765946787666
Iteration 18300: Loss = -12435.773403677513
1
Iteration 18400: Loss = -12435.768237708666
2
Iteration 18500: Loss = -12435.76593979473
Iteration 18600: Loss = -12435.766431680298
1
Iteration 18700: Loss = -12435.779894378267
2
Iteration 18800: Loss = -12435.766221151503
3
Iteration 18900: Loss = -12435.765995785248
Iteration 19000: Loss = -12435.765929892897
Iteration 19100: Loss = -12435.766871351054
1
Iteration 19200: Loss = -12436.038086192144
2
Iteration 19300: Loss = -12435.765946215584
Iteration 19400: Loss = -12435.776831883259
1
Iteration 19500: Loss = -12435.768061458013
2
Iteration 19600: Loss = -12435.765950041547
Iteration 19700: Loss = -12435.766901888821
1
Iteration 19800: Loss = -12435.795159348394
2
Iteration 19900: Loss = -12435.765945527959
pi: tensor([[4.2846e-06, 1.0000e+00],
        [2.5535e-02, 9.7447e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1720, 0.8280], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2686, 0.2300],
         [0.5449, 0.1979]],

        [[0.5953, 0.2058],
         [0.5709, 0.5796]],

        [[0.6420, 0.2607],
         [0.5704, 0.6394]],

        [[0.5800, 0.2384],
         [0.6424, 0.5013]],

        [[0.6868, 0.1951],
         [0.7299, 0.6123]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00043881608728782266
Average Adjusted Rand Index: 0.001301815690304899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24610.915925150508
Iteration 100: Loss = -12438.196145680391
Iteration 200: Loss = -12437.618975787433
Iteration 300: Loss = -12437.515766258912
Iteration 400: Loss = -12437.439656611492
Iteration 500: Loss = -12437.190970055794
Iteration 600: Loss = -12436.417511544001
Iteration 700: Loss = -12436.250665703528
Iteration 800: Loss = -12436.137688453016
Iteration 900: Loss = -12436.12347603512
Iteration 1000: Loss = -12436.114247699576
Iteration 1100: Loss = -12436.107544325952
Iteration 1200: Loss = -12436.102167600779
Iteration 1300: Loss = -12436.097568295017
Iteration 1400: Loss = -12436.093273298227
Iteration 1500: Loss = -12436.08891259682
Iteration 1600: Loss = -12436.084161985755
Iteration 1700: Loss = -12436.078355739259
Iteration 1800: Loss = -12436.07021956282
Iteration 1900: Loss = -12436.0560391384
Iteration 2000: Loss = -12436.019885043852
Iteration 2100: Loss = -12435.832219242606
Iteration 2200: Loss = -12434.51433368379
Iteration 2300: Loss = -12434.040514379829
Iteration 2400: Loss = -12433.915975816599
Iteration 2500: Loss = -12433.874756422749
Iteration 2600: Loss = -12433.853138394612
Iteration 2700: Loss = -12433.84117988759
Iteration 2800: Loss = -12433.833776519567
Iteration 2900: Loss = -12433.828567359975
Iteration 3000: Loss = -12433.824784294873
Iteration 3100: Loss = -12433.821796864135
Iteration 3200: Loss = -12433.819404290653
Iteration 3300: Loss = -12433.817442039744
Iteration 3400: Loss = -12433.815835981763
Iteration 3500: Loss = -12433.814385464128
Iteration 3600: Loss = -12433.813130398252
Iteration 3700: Loss = -12433.812016272135
Iteration 3800: Loss = -12433.811076985663
Iteration 3900: Loss = -12433.810222951583
Iteration 4000: Loss = -12433.809402381188
Iteration 4100: Loss = -12433.808685973343
Iteration 4200: Loss = -12433.80799775192
Iteration 4300: Loss = -12433.807344690076
Iteration 4400: Loss = -12433.806789129247
Iteration 4500: Loss = -12433.806280853745
Iteration 4600: Loss = -12433.805781459674
Iteration 4700: Loss = -12433.805331804659
Iteration 4800: Loss = -12433.804925638995
Iteration 4900: Loss = -12433.804510158161
Iteration 5000: Loss = -12433.80415450331
Iteration 5100: Loss = -12433.803833179843
Iteration 5200: Loss = -12433.803533816246
Iteration 5300: Loss = -12433.803274238744
Iteration 5400: Loss = -12433.803014144043
Iteration 5500: Loss = -12433.802805961937
Iteration 5600: Loss = -12433.802599660634
Iteration 5700: Loss = -12433.802390119137
Iteration 5800: Loss = -12433.802214489624
Iteration 5900: Loss = -12433.802082631171
Iteration 6000: Loss = -12433.80192496321
Iteration 6100: Loss = -12433.801806698055
Iteration 6200: Loss = -12433.801697682891
Iteration 6300: Loss = -12433.801588889406
Iteration 6400: Loss = -12433.801516614285
Iteration 6500: Loss = -12433.801404248548
Iteration 6600: Loss = -12433.801342729195
Iteration 6700: Loss = -12433.801251333087
Iteration 6800: Loss = -12433.801190841707
Iteration 6900: Loss = -12433.801086233934
Iteration 7000: Loss = -12433.800992803719
Iteration 7100: Loss = -12433.800961972915
Iteration 7200: Loss = -12433.800912001043
Iteration 7300: Loss = -12433.800829902702
Iteration 7400: Loss = -12433.800806920082
Iteration 7500: Loss = -12433.800779619654
Iteration 7600: Loss = -12433.800909683205
1
Iteration 7700: Loss = -12433.800678968017
Iteration 7800: Loss = -12433.800674751243
Iteration 7900: Loss = -12433.801106565073
1
Iteration 8000: Loss = -12433.800609129652
Iteration 8100: Loss = -12433.800488665716
Iteration 8200: Loss = -12433.804029752357
1
Iteration 8300: Loss = -12433.800043570824
Iteration 8400: Loss = -12433.800054141684
Iteration 8500: Loss = -12433.860363081028
1
Iteration 8600: Loss = -12433.79997137439
Iteration 8700: Loss = -12433.799973213809
Iteration 8800: Loss = -12433.800720964793
1
Iteration 8900: Loss = -12433.799954261332
Iteration 9000: Loss = -12433.799876892399
Iteration 9100: Loss = -12433.79987298616
Iteration 9200: Loss = -12433.803843120453
1
Iteration 9300: Loss = -12433.799848462582
Iteration 9400: Loss = -12433.799851422833
Iteration 9500: Loss = -12433.7998813864
Iteration 9600: Loss = -12433.800090967452
1
Iteration 9700: Loss = -12433.79987681148
Iteration 9800: Loss = -12433.799855905952
Iteration 9900: Loss = -12433.847099886898
1
Iteration 10000: Loss = -12433.79984167331
Iteration 10100: Loss = -12433.799851622947
Iteration 10200: Loss = -12433.799843903067
Iteration 10300: Loss = -12433.799955832752
1
Iteration 10400: Loss = -12433.799824309592
Iteration 10500: Loss = -12433.799827931185
Iteration 10600: Loss = -12433.800154595167
1
Iteration 10700: Loss = -12433.799828864654
Iteration 10800: Loss = -12433.799812755968
Iteration 10900: Loss = -12433.799906441225
Iteration 11000: Loss = -12433.799647288484
Iteration 11100: Loss = -12433.799631977456
Iteration 11200: Loss = -12433.799603892543
Iteration 11300: Loss = -12433.806591773171
1
Iteration 11400: Loss = -12433.799598753276
Iteration 11500: Loss = -12433.799576647365
Iteration 11600: Loss = -12433.79957771072
Iteration 11700: Loss = -12433.860358676548
1
Iteration 11800: Loss = -12433.799567141077
Iteration 11900: Loss = -12433.799573858249
Iteration 12000: Loss = -12433.799567030252
Iteration 12100: Loss = -12433.823160800779
1
Iteration 12200: Loss = -12433.79956930264
Iteration 12300: Loss = -12433.799549236644
Iteration 12400: Loss = -12433.799518070136
Iteration 12500: Loss = -12433.800192462271
1
Iteration 12600: Loss = -12433.79951505984
Iteration 12700: Loss = -12433.799530071787
Iteration 12800: Loss = -12433.79953469241
Iteration 12900: Loss = -12433.815982093287
1
Iteration 13000: Loss = -12433.799522120607
Iteration 13100: Loss = -12433.799516893247
Iteration 13200: Loss = -12433.799525709102
Iteration 13300: Loss = -12433.800873298842
1
Iteration 13400: Loss = -12433.799562319351
Iteration 13500: Loss = -12433.799532865854
Iteration 13600: Loss = -12433.799503962578
Iteration 13700: Loss = -12433.81396382635
1
Iteration 13800: Loss = -12433.799530674796
Iteration 13900: Loss = -12433.799518663202
Iteration 14000: Loss = -12433.799516235089
Iteration 14100: Loss = -12433.803685767738
1
Iteration 14200: Loss = -12433.79952301158
Iteration 14300: Loss = -12433.799523775493
Iteration 14400: Loss = -12433.799536620372
Iteration 14500: Loss = -12433.805689213379
1
Iteration 14600: Loss = -12433.7995458751
Iteration 14700: Loss = -12433.799534388596
Iteration 14800: Loss = -12433.799525791952
Iteration 14900: Loss = -12433.800482812034
1
Iteration 15000: Loss = -12433.799526565952
Iteration 15100: Loss = -12433.799517094163
Iteration 15200: Loss = -12433.804882331458
1
Iteration 15300: Loss = -12433.79952033679
Iteration 15400: Loss = -12433.79970713573
1
Iteration 15500: Loss = -12433.799530295119
Iteration 15600: Loss = -12433.800184383192
1
Iteration 15700: Loss = -12433.799486656273
Iteration 15800: Loss = -12433.841253665098
1
Iteration 15900: Loss = -12433.79952424181
Iteration 16000: Loss = -12433.806469616737
1
Iteration 16100: Loss = -12433.799568972006
Iteration 16200: Loss = -12433.799504253684
Iteration 16300: Loss = -12433.801023908138
1
Iteration 16400: Loss = -12433.799507416152
Iteration 16500: Loss = -12433.80531957199
1
Iteration 16600: Loss = -12433.799517668946
Iteration 16700: Loss = -12433.799502946053
Iteration 16800: Loss = -12433.80633394024
1
Iteration 16900: Loss = -12433.79956574085
Iteration 17000: Loss = -12433.7994783721
Iteration 17100: Loss = -12433.79961955922
1
Iteration 17200: Loss = -12433.800249988772
2
Iteration 17300: Loss = -12433.799467713525
Iteration 17400: Loss = -12433.800043719348
1
Iteration 17500: Loss = -12433.799456054006
Iteration 17600: Loss = -12434.165240611776
1
Iteration 17700: Loss = -12433.799460568385
Iteration 17800: Loss = -12433.799456159592
Iteration 17900: Loss = -12433.825353087565
1
Iteration 18000: Loss = -12433.79945224274
Iteration 18100: Loss = -12433.799852027547
1
Iteration 18200: Loss = -12433.799464683196
Iteration 18300: Loss = -12433.799405625923
Iteration 18400: Loss = -12433.835329328374
1
Iteration 18500: Loss = -12433.799417745562
Iteration 18600: Loss = -12433.799422444608
Iteration 18700: Loss = -12433.831703781654
1
Iteration 18800: Loss = -12433.799400952648
Iteration 18900: Loss = -12433.80974264817
1
Iteration 19000: Loss = -12433.799414279152
Iteration 19100: Loss = -12433.799420191861
Iteration 19200: Loss = -12433.799794222854
1
Iteration 19300: Loss = -12433.799430729034
Iteration 19400: Loss = -12434.175152699112
1
Iteration 19500: Loss = -12433.799429802037
Iteration 19600: Loss = -12433.799432690505
Iteration 19700: Loss = -12433.826554348012
1
Iteration 19800: Loss = -12433.799238399595
Iteration 19900: Loss = -12433.80098421516
1
pi: tensor([[1.0000e+00, 2.4491e-09],
        [1.6691e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9886, 0.0114], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2019, 0.2959],
         [0.5155, 0.3608]],

        [[0.5528, 0.1094],
         [0.7143, 0.7118]],

        [[0.7302, 0.1361],
         [0.6219, 0.6397]],

        [[0.7202, 0.2282],
         [0.7199, 0.5992]],

        [[0.6914, 0.1489],
         [0.6896, 0.6708]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.001656498630682711
Average Adjusted Rand Index: -0.00178123834356005
11923.184240683191
[0.00043881608728782266, -0.001656498630682711] [0.001301815690304899, -0.00178123834356005] [12435.776544282307, 12433.799190618753]
-------------------------------------
This iteration is 96
True Objective function: Loss = -12078.411636078867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23545.218701834237
Iteration 100: Loss = -12532.436580278918
Iteration 200: Loss = -12531.91901777652
Iteration 300: Loss = -12531.693887470876
Iteration 400: Loss = -12531.517648931143
Iteration 500: Loss = -12531.307819474834
Iteration 600: Loss = -12531.157643893288
Iteration 700: Loss = -12531.060763534806
Iteration 800: Loss = -12530.964709471222
Iteration 900: Loss = -12530.849371002725
Iteration 1000: Loss = -12530.691802754682
Iteration 1100: Loss = -12530.442340212394
Iteration 1200: Loss = -12529.955898212947
Iteration 1300: Loss = -12529.14935036802
Iteration 1400: Loss = -12528.560215577363
Iteration 1500: Loss = -12528.18989189173
Iteration 1600: Loss = -12527.91014992268
Iteration 1700: Loss = -12527.703919656935
Iteration 1800: Loss = -12527.310278289562
Iteration 1900: Loss = -12526.646967533314
Iteration 2000: Loss = -12148.854708519022
Iteration 2100: Loss = -12098.911112103155
Iteration 2200: Loss = -12073.261060218176
Iteration 2300: Loss = -12073.163926781841
Iteration 2400: Loss = -12073.111991269641
Iteration 2500: Loss = -12073.070221298103
Iteration 2600: Loss = -12073.002518379055
Iteration 2700: Loss = -12072.972374508
Iteration 2800: Loss = -12057.758036426767
Iteration 2900: Loss = -12057.737532746325
Iteration 3000: Loss = -12057.725626989964
Iteration 3100: Loss = -12057.71856360865
Iteration 3200: Loss = -12057.713030443121
Iteration 3300: Loss = -12057.708509016913
Iteration 3400: Loss = -12057.705824601368
Iteration 3500: Loss = -12057.69870056672
Iteration 3600: Loss = -12057.691398310139
Iteration 3700: Loss = -12057.689103627228
Iteration 3800: Loss = -12057.686622996398
Iteration 3900: Loss = -12057.685045645878
Iteration 4000: Loss = -12057.682356667894
Iteration 4100: Loss = -12057.680852550659
Iteration 4200: Loss = -12057.676410582299
Iteration 4300: Loss = -12057.669643354498
Iteration 4400: Loss = -12057.668604692619
Iteration 4500: Loss = -12057.664418763956
Iteration 4600: Loss = -12057.665240774728
1
Iteration 4700: Loss = -12057.662334597135
Iteration 4800: Loss = -12057.66108922149
Iteration 4900: Loss = -12057.660005152762
Iteration 5000: Loss = -12057.659048156032
Iteration 5100: Loss = -12057.658421241846
Iteration 5200: Loss = -12057.65936963203
1
Iteration 5300: Loss = -12057.6556517641
Iteration 5400: Loss = -12057.658402029267
1
Iteration 5500: Loss = -12057.65387677307
Iteration 5600: Loss = -12057.653127957044
Iteration 5700: Loss = -12057.652024674964
Iteration 5800: Loss = -12057.652641424622
1
Iteration 5900: Loss = -12057.656295652967
2
Iteration 6000: Loss = -12057.650758579175
Iteration 6100: Loss = -12057.650539266571
Iteration 6200: Loss = -12057.65012751873
Iteration 6300: Loss = -12057.649900946204
Iteration 6400: Loss = -12057.650414544876
1
Iteration 6500: Loss = -12057.652416361183
2
Iteration 6600: Loss = -12057.652679100851
3
Iteration 6700: Loss = -12057.649835983488
Iteration 6800: Loss = -12057.64866299264
Iteration 6900: Loss = -12057.65091027206
1
Iteration 7000: Loss = -12057.646272139362
Iteration 7100: Loss = -12057.645378537225
Iteration 7200: Loss = -12057.644904992962
Iteration 7300: Loss = -12057.644306369111
Iteration 7400: Loss = -12057.659956830717
1
Iteration 7500: Loss = -12057.643447059025
Iteration 7600: Loss = -12057.64292318943
Iteration 7700: Loss = -12057.643014706427
Iteration 7800: Loss = -12057.642462450907
Iteration 7900: Loss = -12057.684998690946
1
Iteration 8000: Loss = -12057.644875792894
2
Iteration 8100: Loss = -12057.65827332312
3
Iteration 8200: Loss = -12057.641947610768
Iteration 8300: Loss = -12057.652289729898
1
Iteration 8400: Loss = -12057.64183152583
Iteration 8500: Loss = -12057.644306267817
1
Iteration 8600: Loss = -12057.641820835925
Iteration 8700: Loss = -12057.646833137773
1
Iteration 8800: Loss = -12057.64146827753
Iteration 8900: Loss = -12057.656462935805
1
Iteration 9000: Loss = -12057.643339727641
2
Iteration 9100: Loss = -12057.641230078254
Iteration 9200: Loss = -12057.728565021745
1
Iteration 9300: Loss = -12057.646651140356
2
Iteration 9400: Loss = -12057.641009886383
Iteration 9500: Loss = -12057.732380314272
1
Iteration 9600: Loss = -12057.640804789233
Iteration 9700: Loss = -12057.640036720826
Iteration 9800: Loss = -12057.669990485625
1
Iteration 9900: Loss = -12057.639428360464
Iteration 10000: Loss = -12057.640076938287
1
Iteration 10100: Loss = -12057.684392457342
2
Iteration 10200: Loss = -12057.63999670544
3
Iteration 10300: Loss = -12057.645542755818
4
Iteration 10400: Loss = -12057.641043099988
5
Iteration 10500: Loss = -12057.656050638121
6
Iteration 10600: Loss = -12057.68543952957
7
Iteration 10700: Loss = -12057.69130667555
8
Iteration 10800: Loss = -12057.777213939149
9
Iteration 10900: Loss = -12057.647098447655
10
Iteration 11000: Loss = -12057.639239761897
Iteration 11100: Loss = -12057.641251262268
1
Iteration 11200: Loss = -12057.64664251198
2
Iteration 11300: Loss = -12057.640946373629
3
Iteration 11400: Loss = -12057.641935048405
4
Iteration 11500: Loss = -12057.654001863466
5
Iteration 11600: Loss = -12057.64879400589
6
Iteration 11700: Loss = -12057.652888461
7
Iteration 11800: Loss = -12057.639613465262
8
Iteration 11900: Loss = -12057.647569432778
9
Iteration 12000: Loss = -12057.63787197654
Iteration 12100: Loss = -12057.769137421785
1
Iteration 12200: Loss = -12057.637591856179
Iteration 12300: Loss = -12057.637740199189
1
Iteration 12400: Loss = -12057.67558596264
2
Iteration 12500: Loss = -12057.637858677348
3
Iteration 12600: Loss = -12057.641153488386
4
Iteration 12700: Loss = -12057.641984690572
5
Iteration 12800: Loss = -12057.655239643316
6
Iteration 12900: Loss = -12057.639273196537
7
Iteration 13000: Loss = -12057.641781326842
8
Iteration 13100: Loss = -12057.637877681485
9
Iteration 13200: Loss = -12057.63788410214
10
Iteration 13300: Loss = -12057.644277532443
11
Iteration 13400: Loss = -12057.656120311818
12
Iteration 13500: Loss = -12057.63891501932
13
Iteration 13600: Loss = -12057.638360015995
14
Iteration 13700: Loss = -12057.637275234649
Iteration 13800: Loss = -12057.64832730766
1
Iteration 13900: Loss = -12057.66915832234
2
Iteration 14000: Loss = -12057.735369865466
3
Iteration 14100: Loss = -12057.68130004693
4
Iteration 14200: Loss = -12057.637253002453
Iteration 14300: Loss = -12057.648209333744
1
Iteration 14400: Loss = -12057.640635652473
2
Iteration 14500: Loss = -12057.649020711417
3
Iteration 14600: Loss = -12057.638512147743
4
Iteration 14700: Loss = -12057.636117223195
Iteration 14800: Loss = -12057.648721448968
1
Iteration 14900: Loss = -12057.656205267567
2
Iteration 15000: Loss = -12057.711434909788
3
Iteration 15100: Loss = -12057.647147503814
4
Iteration 15200: Loss = -12057.63379745018
Iteration 15300: Loss = -12057.634454380208
1
Iteration 15400: Loss = -12057.669131903976
2
Iteration 15500: Loss = -12057.635217465682
3
Iteration 15600: Loss = -12057.63414984971
4
Iteration 15700: Loss = -12057.722920322773
5
Iteration 15800: Loss = -12057.635901172005
6
Iteration 15900: Loss = -12057.634483801416
7
Iteration 16000: Loss = -12057.633613330912
Iteration 16100: Loss = -12057.763887016597
1
Iteration 16200: Loss = -12057.634179545572
2
Iteration 16300: Loss = -12057.636788339474
3
Iteration 16400: Loss = -12057.648556834714
4
Iteration 16500: Loss = -12057.634424629734
5
Iteration 16600: Loss = -12057.635300955944
6
Iteration 16700: Loss = -12057.704046095621
7
Iteration 16800: Loss = -12057.633797858856
8
Iteration 16900: Loss = -12057.638457202322
9
Iteration 17000: Loss = -12057.63431625754
10
Iteration 17100: Loss = -12057.81288829163
11
Iteration 17200: Loss = -12057.632748047527
Iteration 17300: Loss = -12057.6344308823
1
Iteration 17400: Loss = -12057.642982752915
2
Iteration 17500: Loss = -12057.633909469485
3
Iteration 17600: Loss = -12057.633950686097
4
Iteration 17700: Loss = -12057.632719860172
Iteration 17800: Loss = -12057.6350688972
1
Iteration 17900: Loss = -12057.645521832097
2
Iteration 18000: Loss = -12057.634750986816
3
Iteration 18100: Loss = -12057.640887076235
4
Iteration 18200: Loss = -12057.633677853695
5
Iteration 18300: Loss = -12057.656291694744
6
Iteration 18400: Loss = -12057.639917049899
7
Iteration 18500: Loss = -12057.633028235745
8
Iteration 18600: Loss = -12057.637344190629
9
Iteration 18700: Loss = -12057.67394333374
10
Iteration 18800: Loss = -12057.633856926492
11
Iteration 18900: Loss = -12057.638788358914
12
Iteration 19000: Loss = -12057.644807007953
13
Iteration 19100: Loss = -12057.65011023734
14
Iteration 19200: Loss = -12057.651028883578
15
Stopping early at iteration 19200 due to no improvement.
pi: tensor([[0.2862, 0.7138],
        [0.7292, 0.2708]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4920, 0.5080], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2920, 0.1140],
         [0.5478, 0.3122]],

        [[0.6886, 0.1158],
         [0.6455, 0.5549]],

        [[0.5778, 0.0882],
         [0.6041, 0.6561]],

        [[0.5989, 0.1003],
         [0.6006, 0.6922]],

        [[0.5646, 0.1072],
         [0.6707, 0.5236]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.04134594802661651
Average Adjusted Rand Index: 0.9681518571781368
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23414.90532091523
Iteration 100: Loss = -12532.817180789389
Iteration 200: Loss = -12532.097536566871
Iteration 300: Loss = -12531.857992470703
Iteration 400: Loss = -12531.297449952308
Iteration 500: Loss = -12530.97600585577
Iteration 600: Loss = -12530.814690804847
Iteration 700: Loss = -12530.638169048863
Iteration 800: Loss = -12530.459758768617
Iteration 900: Loss = -12530.328257801333
Iteration 1000: Loss = -12530.252358864676
Iteration 1100: Loss = -12530.19840782075
Iteration 1200: Loss = -12530.13485545984
Iteration 1300: Loss = -12530.04301111307
Iteration 1400: Loss = -12529.945820392159
Iteration 1500: Loss = -12529.805728516021
Iteration 1600: Loss = -12529.33119549428
Iteration 1700: Loss = -12528.492450131913
Iteration 1800: Loss = -12526.979832579662
Iteration 1900: Loss = -12102.773531653667
Iteration 2000: Loss = -12102.380685307602
Iteration 2100: Loss = -12077.893798596659
Iteration 2200: Loss = -12077.87421572973
Iteration 2300: Loss = -12077.864413095758
Iteration 2400: Loss = -12077.856895168308
Iteration 2500: Loss = -12067.564151078068
Iteration 2600: Loss = -12064.846094561542
Iteration 2700: Loss = -12064.325107686635
Iteration 2800: Loss = -12064.315700467467
Iteration 2900: Loss = -12064.291954529765
Iteration 3000: Loss = -12064.274330203498
Iteration 3100: Loss = -12064.271886474582
Iteration 3200: Loss = -12064.270583434587
Iteration 3300: Loss = -12064.272202136577
1
Iteration 3400: Loss = -12064.266040414665
Iteration 3500: Loss = -12064.261286581903
Iteration 3600: Loss = -12064.25999162624
Iteration 3700: Loss = -12063.478897037836
Iteration 3800: Loss = -12057.702507526295
Iteration 3900: Loss = -12057.701797524165
Iteration 4000: Loss = -12057.702145668547
1
Iteration 4100: Loss = -12057.700239786442
Iteration 4200: Loss = -12057.699646346822
Iteration 4300: Loss = -12057.699289750663
Iteration 4400: Loss = -12057.699568293003
1
Iteration 4500: Loss = -12057.698268063597
Iteration 4600: Loss = -12057.696206438552
Iteration 4700: Loss = -12057.689382922914
Iteration 4800: Loss = -12057.687156389196
Iteration 4900: Loss = -12057.687623297312
1
Iteration 5000: Loss = -12057.68724757342
Iteration 5100: Loss = -12057.685375453704
Iteration 5200: Loss = -12057.680565879527
Iteration 5300: Loss = -12057.662252904925
Iteration 5400: Loss = -12057.662314595036
Iteration 5500: Loss = -12057.661840910678
Iteration 5600: Loss = -12057.661572253246
Iteration 5700: Loss = -12057.661858819287
1
Iteration 5800: Loss = -12057.668851598537
2
Iteration 5900: Loss = -12057.662157999554
3
Iteration 6000: Loss = -12057.660929994276
Iteration 6100: Loss = -12057.660905621367
Iteration 6200: Loss = -12057.660181208563
Iteration 6300: Loss = -12057.660582118973
1
Iteration 6400: Loss = -12057.659613228827
Iteration 6500: Loss = -12057.697520469657
1
Iteration 6600: Loss = -12057.659346028717
Iteration 6700: Loss = -12057.659249142025
Iteration 6800: Loss = -12057.66349471258
1
Iteration 6900: Loss = -12057.659965030562
2
Iteration 7000: Loss = -12057.658872280539
Iteration 7100: Loss = -12057.658486470815
Iteration 7200: Loss = -12057.657427444692
Iteration 7300: Loss = -12057.657818695765
1
Iteration 7400: Loss = -12057.674000529381
2
Iteration 7500: Loss = -12057.687136004164
3
Iteration 7600: Loss = -12057.655295716524
Iteration 7700: Loss = -12057.65476450934
Iteration 7800: Loss = -12057.656848599629
1
Iteration 7900: Loss = -12057.667908614922
2
Iteration 8000: Loss = -12057.655094804299
3
Iteration 8100: Loss = -12057.653365961138
Iteration 8200: Loss = -12057.651320625893
Iteration 8300: Loss = -12057.650497394892
Iteration 8400: Loss = -12057.650312149175
Iteration 8500: Loss = -12057.651968648732
1
Iteration 8600: Loss = -12057.650071384118
Iteration 8700: Loss = -12057.666432167713
1
Iteration 8800: Loss = -12057.650119024842
Iteration 8900: Loss = -12057.649835152119
Iteration 9000: Loss = -12057.643767481944
Iteration 9100: Loss = -12057.641665426123
Iteration 9200: Loss = -12057.641665762885
Iteration 9300: Loss = -12057.64249843585
1
Iteration 9400: Loss = -12057.642159307175
2
Iteration 9500: Loss = -12057.646837403641
3
Iteration 9600: Loss = -12057.641070435839
Iteration 9700: Loss = -12057.641468568181
1
Iteration 9800: Loss = -12057.70436301189
2
Iteration 9900: Loss = -12057.641623680924
3
Iteration 10000: Loss = -12057.641488733501
4
Iteration 10100: Loss = -12057.642832400297
5
Iteration 10200: Loss = -12057.643027728456
6
Iteration 10300: Loss = -12057.641387477855
7
Iteration 10400: Loss = -12057.650584620016
8
Iteration 10500: Loss = -12057.665614409341
9
Iteration 10600: Loss = -12057.64141813912
10
Iteration 10700: Loss = -12057.64074463821
Iteration 10800: Loss = -12057.65023727474
1
Iteration 10900: Loss = -12057.64170484534
2
Iteration 11000: Loss = -12057.639670449515
Iteration 11100: Loss = -12057.640496330238
1
Iteration 11200: Loss = -12057.641159448016
2
Iteration 11300: Loss = -12057.725801478628
3
Iteration 11400: Loss = -12057.661150439568
4
Iteration 11500: Loss = -12057.639193117628
Iteration 11600: Loss = -12057.640559708649
1
Iteration 11700: Loss = -12057.642945718459
2
Iteration 11800: Loss = -12057.64273809918
3
Iteration 11900: Loss = -12057.727316082484
4
Iteration 12000: Loss = -12057.646345544965
5
Iteration 12100: Loss = -12057.666619851887
6
Iteration 12200: Loss = -12057.646440151153
7
Iteration 12300: Loss = -12057.652965271183
8
Iteration 12400: Loss = -12057.64465040749
9
Iteration 12500: Loss = -12057.639729920325
10
Iteration 12600: Loss = -12057.644440875878
11
Iteration 12700: Loss = -12057.662705489483
12
Iteration 12800: Loss = -12057.64614601404
13
Iteration 12900: Loss = -12057.638313167727
Iteration 13000: Loss = -12057.63762335188
Iteration 13100: Loss = -12057.637138446446
Iteration 13200: Loss = -12057.639395643668
1
Iteration 13300: Loss = -12057.63938656189
2
Iteration 13400: Loss = -12057.640559636862
3
Iteration 13500: Loss = -12057.655801866227
4
Iteration 13600: Loss = -12057.637110614074
Iteration 13700: Loss = -12057.638029220474
1
Iteration 13800: Loss = -12057.640783954863
2
Iteration 13900: Loss = -12057.639074683164
3
Iteration 14000: Loss = -12057.636546978936
Iteration 14100: Loss = -12057.682829997046
1
Iteration 14200: Loss = -12057.657691932272
2
Iteration 14300: Loss = -12057.637184397116
3
Iteration 14400: Loss = -12057.651075728338
4
Iteration 14500: Loss = -12057.639159584476
5
Iteration 14600: Loss = -12057.638497395932
6
Iteration 14700: Loss = -12057.667152643799
7
Iteration 14800: Loss = -12057.6386559297
8
Iteration 14900: Loss = -12057.639590134117
9
Iteration 15000: Loss = -12057.634371392063
Iteration 15100: Loss = -12057.635189353861
1
Iteration 15200: Loss = -12057.690762194812
2
Iteration 15300: Loss = -12057.75291970365
3
Iteration 15400: Loss = -12057.651847191131
4
Iteration 15500: Loss = -12057.740790749293
5
Iteration 15600: Loss = -12057.634802789189
6
Iteration 15700: Loss = -12057.646277866023
7
Iteration 15800: Loss = -12057.646179277895
8
Iteration 15900: Loss = -12057.640577849725
9
Iteration 16000: Loss = -12057.644584550208
10
Iteration 16100: Loss = -12057.634445233218
Iteration 16200: Loss = -12057.63581435422
1
Iteration 16300: Loss = -12057.636781740224
2
Iteration 16400: Loss = -12057.640063800041
3
Iteration 16500: Loss = -12057.640443999557
4
Iteration 16600: Loss = -12057.63607552012
5
Iteration 16700: Loss = -12057.636611409684
6
Iteration 16800: Loss = -12057.640024210894
7
Iteration 16900: Loss = -12057.637594755439
8
Iteration 17000: Loss = -12057.636830106832
9
Iteration 17100: Loss = -12057.634792931893
10
Iteration 17200: Loss = -12057.63507590711
11
Iteration 17300: Loss = -12057.637909956235
12
Iteration 17400: Loss = -12057.635882414585
13
Iteration 17500: Loss = -12057.636637644995
14
Iteration 17600: Loss = -12057.641728121398
15
Stopping early at iteration 17600 due to no improvement.
pi: tensor([[0.2676, 0.7324],
        [0.7142, 0.2858]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5086, 0.4914], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3122, 0.1143],
         [0.5892, 0.2922]],

        [[0.6635, 0.1158],
         [0.7303, 0.5035]],

        [[0.7246, 0.0883],
         [0.5205, 0.5391]],

        [[0.6630, 0.1005],
         [0.6528, 0.7232]],

        [[0.6518, 0.1068],
         [0.5827, 0.6442]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.04134594802661651
Average Adjusted Rand Index: 0.9681518571781368
12078.411636078867
[0.04134594802661651, 0.04134594802661651] [0.9681518571781368, 0.9681518571781368] [12057.651028883578, 12057.641728121398]
-------------------------------------
This iteration is 97
True Objective function: Loss = -11720.500667457021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21792.363564693955
Iteration 100: Loss = -12275.617896129104
Iteration 200: Loss = -12274.975613106775
Iteration 300: Loss = -12274.822682424265
Iteration 400: Loss = -12274.740899323353
Iteration 500: Loss = -12274.689624682009
Iteration 600: Loss = -12274.654377032828
Iteration 700: Loss = -12274.62871432157
Iteration 800: Loss = -12274.609163547264
Iteration 900: Loss = -12274.593712285712
Iteration 1000: Loss = -12274.580903542323
Iteration 1100: Loss = -12274.569754279992
Iteration 1200: Loss = -12274.559673906862
Iteration 1300: Loss = -12274.549840844624
Iteration 1400: Loss = -12274.5396845011
Iteration 1500: Loss = -12274.528510011896
Iteration 1600: Loss = -12274.515651928552
Iteration 1700: Loss = -12274.500163552191
Iteration 1800: Loss = -12274.48109234436
Iteration 1900: Loss = -12274.457016610935
Iteration 2000: Loss = -12274.425629447303
Iteration 2100: Loss = -12274.383673980661
Iteration 2200: Loss = -12274.326068777353
Iteration 2300: Loss = -12274.245801268671
Iteration 2400: Loss = -12274.134590296617
Iteration 2500: Loss = -12273.996285264699
Iteration 2600: Loss = -12273.886321411883
Iteration 2700: Loss = -12273.838128233701
Iteration 2800: Loss = -12273.819704535641
Iteration 2900: Loss = -12273.811080973128
Iteration 3000: Loss = -12273.805757184131
Iteration 3100: Loss = -12273.801420161242
Iteration 3200: Loss = -12273.79691331864
Iteration 3300: Loss = -12273.791060972202
Iteration 3400: Loss = -12273.781764007446
Iteration 3500: Loss = -12273.76593962513
Iteration 3600: Loss = -12273.747139997873
Iteration 3700: Loss = -12273.714584621204
Iteration 3800: Loss = -12273.686601373694
Iteration 3900: Loss = -12273.658836142478
Iteration 4000: Loss = -12273.625288125933
Iteration 4100: Loss = -12273.580381693964
Iteration 4200: Loss = -12273.535260909206
Iteration 4300: Loss = -12273.500685611789
Iteration 4400: Loss = -12273.47475521963
Iteration 4500: Loss = -12273.463010736608
Iteration 4600: Loss = -12273.4338339773
Iteration 4700: Loss = -12273.41196993294
Iteration 4800: Loss = -12273.384293736026
Iteration 4900: Loss = -12273.345118378584
Iteration 5000: Loss = -12273.301145482628
Iteration 5100: Loss = -12273.236844044292
Iteration 5200: Loss = -12273.21845447595
Iteration 5300: Loss = -12273.212563569417
Iteration 5400: Loss = -12273.203109358728
Iteration 5500: Loss = -12273.154093256853
Iteration 5600: Loss = -12273.018330752633
Iteration 5700: Loss = -12272.995369354014
Iteration 5800: Loss = -12272.986866641742
Iteration 5900: Loss = -12272.982339903416
Iteration 6000: Loss = -12272.97945068112
Iteration 6100: Loss = -12272.977436438327
Iteration 6200: Loss = -12272.975946649813
Iteration 6300: Loss = -12272.975993870927
Iteration 6400: Loss = -12272.973852164936
Iteration 6500: Loss = -12272.973091886337
Iteration 6600: Loss = -12272.97251483629
Iteration 6700: Loss = -12272.971933674564
Iteration 6800: Loss = -12272.971540272196
Iteration 6900: Loss = -12272.971048933243
Iteration 7000: Loss = -12272.970729572568
Iteration 7100: Loss = -12272.970380737503
Iteration 7200: Loss = -12272.970135359907
Iteration 7300: Loss = -12272.969852756898
Iteration 7400: Loss = -12272.969718194185
Iteration 7500: Loss = -12272.969590452509
Iteration 7600: Loss = -12272.969271370523
Iteration 7700: Loss = -12272.971271843884
1
Iteration 7800: Loss = -12272.968924452725
Iteration 7900: Loss = -12272.969689114565
1
Iteration 8000: Loss = -12272.970893585454
2
Iteration 8100: Loss = -12272.981695632276
3
Iteration 8200: Loss = -12272.969667414389
4
Iteration 8300: Loss = -12272.968998415656
Iteration 8400: Loss = -12272.968428225968
Iteration 8500: Loss = -12272.98101539973
1
Iteration 8600: Loss = -12272.97059207082
2
Iteration 8700: Loss = -12272.968126365005
Iteration 8800: Loss = -12272.967920222585
Iteration 8900: Loss = -12272.969625462447
1
Iteration 9000: Loss = -12272.96775035289
Iteration 9100: Loss = -12272.970581800477
1
Iteration 9200: Loss = -12272.968736317069
2
Iteration 9300: Loss = -12272.98404281564
3
Iteration 9400: Loss = -12272.967552658749
Iteration 9500: Loss = -12272.970217820186
1
Iteration 9600: Loss = -12272.979231700978
2
Iteration 9700: Loss = -12272.967403021234
Iteration 9800: Loss = -12272.967634984945
1
Iteration 9900: Loss = -12273.004556248397
2
Iteration 10000: Loss = -12272.967242025481
Iteration 10100: Loss = -12272.971989954107
1
Iteration 10200: Loss = -12272.967525641061
2
Iteration 10300: Loss = -12272.971516662432
3
Iteration 10400: Loss = -12272.967258540173
Iteration 10500: Loss = -12272.968231916968
1
Iteration 10600: Loss = -12273.004445639877
2
Iteration 10700: Loss = -12272.968798405955
3
Iteration 10800: Loss = -12273.035402261368
4
Iteration 10900: Loss = -12273.007147903141
5
Iteration 11000: Loss = -12272.966990416127
Iteration 11100: Loss = -12272.96739228542
1
Iteration 11200: Loss = -12272.96854426566
2
Iteration 11300: Loss = -12272.977973180403
3
Iteration 11400: Loss = -12273.100598105439
4
Iteration 11500: Loss = -12272.966865162345
Iteration 11600: Loss = -12272.966949356947
Iteration 11700: Loss = -12272.969709614412
1
Iteration 11800: Loss = -12273.181162084738
2
Iteration 11900: Loss = -12272.9668304988
Iteration 12000: Loss = -12272.970956295401
1
Iteration 12100: Loss = -12273.161910762235
2
Iteration 12200: Loss = -12272.966789445809
Iteration 12300: Loss = -12272.967121333635
1
Iteration 12400: Loss = -12272.967289909016
2
Iteration 12500: Loss = -12272.967637422633
3
Iteration 12600: Loss = -12272.976534990043
4
Iteration 12700: Loss = -12272.96671986079
Iteration 12800: Loss = -12272.970060212105
1
Iteration 12900: Loss = -12272.967370792956
2
Iteration 13000: Loss = -12272.966729161102
Iteration 13100: Loss = -12272.96704165968
1
Iteration 13200: Loss = -12272.976936244688
2
Iteration 13300: Loss = -12273.007194272534
3
Iteration 13400: Loss = -12272.971257883417
4
Iteration 13500: Loss = -12272.968412433871
5
Iteration 13600: Loss = -12273.009639156631
6
Iteration 13700: Loss = -12272.971536064775
7
Iteration 13800: Loss = -12272.97190724616
8
Iteration 13900: Loss = -12272.974463390743
9
Iteration 14000: Loss = -12272.969656659963
10
Iteration 14100: Loss = -12272.995398737105
11
Iteration 14200: Loss = -12272.980445317071
12
Iteration 14300: Loss = -12272.996568120734
13
Iteration 14400: Loss = -12272.98511760321
14
Iteration 14500: Loss = -12272.968085271701
15
Stopping early at iteration 14500 due to no improvement.
pi: tensor([[4.6951e-05, 9.9995e-01],
        [9.9966e-01, 3.3894e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6646, 0.3354], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1850, 0.1940],
         [0.6917, 0.2112]],

        [[0.5167, 0.1976],
         [0.6056, 0.6489]],

        [[0.5337, 0.2015],
         [0.6119, 0.6424]],

        [[0.7097, 0.1948],
         [0.5607, 0.5576]],

        [[0.5465, 0.1975],
         [0.6192, 0.6475]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.006605112803125404
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.004693858754421813
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.001477551883659358
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.008763529051498655
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.004693858754421813
Global Adjusted Rand Index: -0.0015730611959396674
Average Adjusted Rand Index: -0.0017413706288259465
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20589.02993822949
Iteration 100: Loss = -12275.0658616212
Iteration 200: Loss = -12274.69907575576
Iteration 300: Loss = -12274.646864251588
Iteration 400: Loss = -12274.622311153762
Iteration 500: Loss = -12274.605820935058
Iteration 600: Loss = -12274.593139525092
Iteration 700: Loss = -12274.582559699344
Iteration 800: Loss = -12274.573361809065
Iteration 900: Loss = -12274.564853737844
Iteration 1000: Loss = -12274.556651457458
Iteration 1100: Loss = -12274.548255461284
Iteration 1200: Loss = -12274.539441392453
Iteration 1300: Loss = -12274.529790453742
Iteration 1400: Loss = -12274.518642381703
Iteration 1500: Loss = -12274.50536602959
Iteration 1600: Loss = -12274.488646892747
Iteration 1700: Loss = -12274.46651841797
Iteration 1800: Loss = -12274.43473055586
Iteration 1900: Loss = -12274.38317355518
Iteration 2000: Loss = -12274.283973172509
Iteration 2100: Loss = -12274.090204049671
Iteration 2200: Loss = -12273.881459241275
Iteration 2300: Loss = -12273.771676615728
Iteration 2400: Loss = -12273.704043866099
Iteration 2500: Loss = -12273.658036020932
Iteration 2600: Loss = -12273.621954499806
Iteration 2700: Loss = -12273.592138348491
Iteration 2800: Loss = -12273.568128699051
Iteration 2900: Loss = -12273.549632780774
Iteration 3000: Loss = -12273.534764541415
Iteration 3100: Loss = -12273.521738248268
Iteration 3200: Loss = -12273.509629362276
Iteration 3300: Loss = -12273.498299075522
Iteration 3400: Loss = -12273.487671011871
Iteration 3500: Loss = -12273.47774749324
Iteration 3600: Loss = -12273.46836844315
Iteration 3700: Loss = -12273.459461458739
Iteration 3800: Loss = -12273.450724524959
Iteration 3900: Loss = -12273.442008228922
Iteration 4000: Loss = -12273.43286896023
Iteration 4100: Loss = -12273.422905780182
Iteration 4200: Loss = -12273.412613645252
Iteration 4300: Loss = -12273.399529449596
Iteration 4400: Loss = -12273.384789903954
Iteration 4500: Loss = -12273.365317071939
Iteration 4600: Loss = -12273.342326993488
Iteration 4700: Loss = -12273.313941486384
Iteration 4800: Loss = -12273.284871825635
Iteration 4900: Loss = -12273.251917453708
Iteration 5000: Loss = -12273.23286772363
Iteration 5100: Loss = -12273.22467341014
Iteration 5200: Loss = -12273.221610033152
Iteration 5300: Loss = -12273.219855395799
Iteration 5400: Loss = -12273.218053907829
Iteration 5500: Loss = -12273.220873043734
1
Iteration 5600: Loss = -12273.208436768315
Iteration 5700: Loss = -12273.176258768628
Iteration 5800: Loss = -12273.042908736867
Iteration 5900: Loss = -12273.006689946502
Iteration 6000: Loss = -12272.994142485899
Iteration 6100: Loss = -12272.988434570107
Iteration 6200: Loss = -12272.9837915099
Iteration 6300: Loss = -12272.981461418467
Iteration 6400: Loss = -12272.979050142529
Iteration 6500: Loss = -12272.977569959372
Iteration 6600: Loss = -12272.976286354347
Iteration 6700: Loss = -12272.975518244702
Iteration 6800: Loss = -12272.974497248011
Iteration 6900: Loss = -12272.973749062803
Iteration 7000: Loss = -12272.973139049649
Iteration 7100: Loss = -12273.030613365567
1
Iteration 7200: Loss = -12272.972103486523
Iteration 7300: Loss = -12272.972637524244
1
Iteration 7400: Loss = -12272.971350479016
Iteration 7500: Loss = -12272.97155917292
1
Iteration 7600: Loss = -12272.970697012208
Iteration 7700: Loss = -12272.972647866889
1
Iteration 7800: Loss = -12272.970236110783
Iteration 7900: Loss = -12273.056476512189
1
Iteration 8000: Loss = -12272.969713372528
Iteration 8100: Loss = -12272.969850146961
1
Iteration 8200: Loss = -12272.98265290203
2
Iteration 8300: Loss = -12272.984629953615
3
Iteration 8400: Loss = -12272.98064945374
4
Iteration 8500: Loss = -12272.969166865778
Iteration 8600: Loss = -12272.985810701255
1
Iteration 8700: Loss = -12272.969461533223
2
Iteration 8800: Loss = -12272.968558018454
Iteration 8900: Loss = -12272.969561585407
1
Iteration 9000: Loss = -12272.970052470646
2
Iteration 9100: Loss = -12272.969678027084
3
Iteration 9200: Loss = -12272.97033720469
4
Iteration 9300: Loss = -12272.993032577328
5
Iteration 9400: Loss = -12272.968027685156
Iteration 9500: Loss = -12272.969895217431
1
Iteration 9600: Loss = -12272.968026118144
Iteration 9700: Loss = -12272.975374541289
1
Iteration 9800: Loss = -12272.967743189205
Iteration 9900: Loss = -12272.967754446336
Iteration 10000: Loss = -12273.086767358425
1
Iteration 10100: Loss = -12272.967563964417
Iteration 10200: Loss = -12272.976417545711
1
Iteration 10300: Loss = -12273.16197161889
2
Iteration 10400: Loss = -12272.967420930405
Iteration 10500: Loss = -12272.967809795107
1
Iteration 10600: Loss = -12272.967340000916
Iteration 10700: Loss = -12272.967444181362
1
Iteration 10800: Loss = -12272.967202313921
Iteration 10900: Loss = -12272.969501128593
1
Iteration 11000: Loss = -12272.967192160297
Iteration 11100: Loss = -12272.96724889888
Iteration 11200: Loss = -12272.980145687769
1
Iteration 11300: Loss = -12272.972721659844
2
Iteration 11400: Loss = -12273.007629228032
3
Iteration 11500: Loss = -12272.968982982631
4
Iteration 11600: Loss = -12272.967025439495
Iteration 11700: Loss = -12273.1065371802
1
Iteration 11800: Loss = -12272.967000133689
Iteration 11900: Loss = -12272.967355746787
1
Iteration 12000: Loss = -12272.975460889227
2
Iteration 12100: Loss = -12272.966937335586
Iteration 12200: Loss = -12272.967132008529
1
Iteration 12300: Loss = -12272.994462918135
2
Iteration 12400: Loss = -12272.96883075667
3
Iteration 12500: Loss = -12272.966908919423
Iteration 12600: Loss = -12272.967628308857
1
Iteration 12700: Loss = -12272.966932120175
Iteration 12800: Loss = -12272.966938439462
Iteration 12900: Loss = -12272.968027045523
1
Iteration 13000: Loss = -12273.048876759627
2
Iteration 13100: Loss = -12272.966866966664
Iteration 13200: Loss = -12272.966787657924
Iteration 13300: Loss = -12273.11926033751
1
Iteration 13400: Loss = -12272.966758311932
Iteration 13500: Loss = -12272.966780531737
Iteration 13600: Loss = -12272.967119743622
1
Iteration 13700: Loss = -12272.966894679725
2
Iteration 13800: Loss = -12272.966836341722
Iteration 13900: Loss = -12273.060973444313
1
Iteration 14000: Loss = -12272.966723907344
Iteration 14100: Loss = -12272.967150866492
1
Iteration 14200: Loss = -12272.967047425149
2
Iteration 14300: Loss = -12272.966727697016
Iteration 14400: Loss = -12272.967402608303
1
Iteration 14500: Loss = -12272.97982688738
2
Iteration 14600: Loss = -12272.990731434998
3
Iteration 14700: Loss = -12273.036122090494
4
Iteration 14800: Loss = -12272.966626535297
Iteration 14900: Loss = -12272.968069722845
1
Iteration 15000: Loss = -12272.98933524313
2
Iteration 15100: Loss = -12272.96765702497
3
Iteration 15200: Loss = -12272.970546374849
4
Iteration 15300: Loss = -12272.976553570119
5
Iteration 15400: Loss = -12273.116571624003
6
Iteration 15500: Loss = -12272.966660150214
Iteration 15600: Loss = -12272.974540773908
1
Iteration 15700: Loss = -12272.966646329547
Iteration 15800: Loss = -12272.966788507303
1
Iteration 15900: Loss = -12272.96688341589
2
Iteration 16000: Loss = -12273.002946665247
3
Iteration 16100: Loss = -12272.966592234074
Iteration 16200: Loss = -12272.966951785364
1
Iteration 16300: Loss = -12273.033739487531
2
Iteration 16400: Loss = -12272.969343950152
3
Iteration 16500: Loss = -12272.966850507266
4
Iteration 16600: Loss = -12272.977065847623
5
Iteration 16700: Loss = -12272.97457167281
6
Iteration 16800: Loss = -12272.971884259816
7
Iteration 16900: Loss = -12272.967465568356
8
Iteration 17000: Loss = -12272.968143794686
9
Iteration 17100: Loss = -12272.967159607562
10
Iteration 17200: Loss = -12272.966890942895
11
Iteration 17300: Loss = -12272.968434080294
12
Iteration 17400: Loss = -12272.97502010304
13
Iteration 17500: Loss = -12272.966999644765
14
Iteration 17600: Loss = -12273.001686342584
15
Stopping early at iteration 17600 due to no improvement.
pi: tensor([[1.1785e-04, 9.9988e-01],
        [9.9998e-01, 1.5476e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3337, 0.6663], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2112, 0.1948],
         [0.7233, 0.1852]],

        [[0.6694, 0.1966],
         [0.5281, 0.6932]],

        [[0.5991, 0.2024],
         [0.5388, 0.6742]],

        [[0.6783, 0.1943],
         [0.5192, 0.5565]],

        [[0.5871, 0.1982],
         [0.5641, 0.5829]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.006605112803125404
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.004693858754421813
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.001477551883659358
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.008763529051498655
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.004693858754421813
Global Adjusted Rand Index: -0.0015730611959396674
Average Adjusted Rand Index: -0.0017413706288259465
11720.500667457021
[-0.0015730611959396674, -0.0015730611959396674] [-0.0017413706288259465, -0.0017413706288259465] [12272.968085271701, 12273.001686342584]
-------------------------------------
This iteration is 98
True Objective function: Loss = -11976.520096087863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22906.9349671887
Iteration 100: Loss = -12457.138004549835
Iteration 200: Loss = -12456.759442519024
Iteration 300: Loss = -12456.705715031378
Iteration 400: Loss = -12456.664191404461
Iteration 500: Loss = -12456.615940351761
Iteration 600: Loss = -12456.553555963226
Iteration 700: Loss = -12456.482911927083
Iteration 800: Loss = -12456.42995527062
Iteration 900: Loss = -12456.400762600033
Iteration 1000: Loss = -12456.376077760711
Iteration 1100: Loss = -12456.337860077541
Iteration 1200: Loss = -12456.05386807436
Iteration 1300: Loss = -12452.804426506438
Iteration 1400: Loss = -12451.875998843752
Iteration 1500: Loss = -12451.3692270003
Iteration 1600: Loss = -12451.136272148433
Iteration 1700: Loss = -12451.039137354517
Iteration 1800: Loss = -12450.99404287458
Iteration 1900: Loss = -12450.968122019936
Iteration 2000: Loss = -12450.949181099331
Iteration 2100: Loss = -12450.92802593784
Iteration 2200: Loss = -12450.87366878795
Iteration 2300: Loss = -12450.688823993303
Iteration 2400: Loss = -12450.591820127076
Iteration 2500: Loss = -12450.541094248818
Iteration 2600: Loss = -12450.491503780164
Iteration 2700: Loss = -12450.443682316294
Iteration 2800: Loss = -12450.400278094517
Iteration 2900: Loss = -12450.368565104325
Iteration 3000: Loss = -12450.350655368207
Iteration 3100: Loss = -12450.341639282093
Iteration 3200: Loss = -12450.361036897157
1
Iteration 3300: Loss = -12450.335056503956
Iteration 3400: Loss = -12450.333722734234
Iteration 3500: Loss = -12450.333110251466
Iteration 3600: Loss = -12450.33237901397
Iteration 3700: Loss = -12450.331888913517
Iteration 3800: Loss = -12450.331607380338
Iteration 3900: Loss = -12450.331105261625
Iteration 4000: Loss = -12450.33077379935
Iteration 4100: Loss = -12450.331129826209
1
Iteration 4200: Loss = -12450.330218838544
Iteration 4300: Loss = -12450.329981884553
Iteration 4400: Loss = -12450.329991856066
Iteration 4500: Loss = -12450.329550036531
Iteration 4600: Loss = -12450.329312639422
Iteration 4700: Loss = -12450.32915601286
Iteration 4800: Loss = -12450.32895128978
Iteration 4900: Loss = -12450.337648849942
1
Iteration 5000: Loss = -12450.328596437375
Iteration 5100: Loss = -12450.32845749173
Iteration 5200: Loss = -12450.32835287507
Iteration 5300: Loss = -12450.328153613336
Iteration 5400: Loss = -12450.33384275402
1
Iteration 5500: Loss = -12450.327922022943
Iteration 5600: Loss = -12450.327803362508
Iteration 5700: Loss = -12450.327969148066
1
Iteration 5800: Loss = -12450.32756847442
Iteration 5900: Loss = -12450.327527148313
Iteration 6000: Loss = -12450.327472620387
Iteration 6100: Loss = -12450.327338635347
Iteration 6200: Loss = -12450.331085877913
1
Iteration 6300: Loss = -12450.327173542817
Iteration 6400: Loss = -12450.327123437022
Iteration 6500: Loss = -12450.327062470582
Iteration 6600: Loss = -12450.326957674532
Iteration 6700: Loss = -12450.327200889931
1
Iteration 6800: Loss = -12450.326821177521
Iteration 6900: Loss = -12450.332381239243
1
Iteration 7000: Loss = -12450.326770735162
Iteration 7100: Loss = -12450.326717133785
Iteration 7200: Loss = -12450.326619340218
Iteration 7300: Loss = -12450.326539656879
Iteration 7400: Loss = -12450.326660143173
1
Iteration 7500: Loss = -12450.32648760478
Iteration 7600: Loss = -12450.326577129816
Iteration 7700: Loss = -12450.326439824106
Iteration 7800: Loss = -12450.326840505679
1
Iteration 7900: Loss = -12450.328059054676
2
Iteration 8000: Loss = -12450.327268024354
3
Iteration 8100: Loss = -12450.338400461407
4
Iteration 8200: Loss = -12450.326252462412
Iteration 8300: Loss = -12450.327292321568
1
Iteration 8400: Loss = -12450.32627348941
Iteration 8500: Loss = -12450.365966896035
1
Iteration 8600: Loss = -12450.326118024263
Iteration 8700: Loss = -12450.326204848106
Iteration 8800: Loss = -12450.326356801614
1
Iteration 8900: Loss = -12450.372958283318
2
Iteration 9000: Loss = -12450.32603446284
Iteration 9100: Loss = -12450.340352837991
1
Iteration 9200: Loss = -12450.325999307832
Iteration 9300: Loss = -12450.603509730407
1
Iteration 9400: Loss = -12450.325852302305
Iteration 9500: Loss = -12450.325900888634
Iteration 9600: Loss = -12450.529339663944
1
Iteration 9700: Loss = -12450.325974837271
Iteration 9800: Loss = -12450.325843874722
Iteration 9900: Loss = -12450.331702629936
1
Iteration 10000: Loss = -12450.325862741458
Iteration 10100: Loss = -12450.326108076286
1
Iteration 10200: Loss = -12450.32592252941
Iteration 10300: Loss = -12450.350975852183
1
Iteration 10400: Loss = -12450.326156337273
2
Iteration 10500: Loss = -12450.350349662896
3
Iteration 10600: Loss = -12450.325779536257
Iteration 10700: Loss = -12450.32581761093
Iteration 10800: Loss = -12450.330540153296
1
Iteration 10900: Loss = -12450.325719393026
Iteration 11000: Loss = -12450.326455214876
1
Iteration 11100: Loss = -12450.326119527079
2
Iteration 11200: Loss = -12450.327482419805
3
Iteration 11300: Loss = -12450.370720797977
4
Iteration 11400: Loss = -12450.325755135695
Iteration 11500: Loss = -12450.327526051902
1
Iteration 11600: Loss = -12450.32613076493
2
Iteration 11700: Loss = -12450.32627284196
3
Iteration 11800: Loss = -12450.325673226376
Iteration 11900: Loss = -12450.326036297973
1
Iteration 12000: Loss = -12450.32576974707
Iteration 12100: Loss = -12450.391407556343
1
Iteration 12200: Loss = -12450.325734137245
Iteration 12300: Loss = -12450.327807187807
1
Iteration 12400: Loss = -12450.348271156188
2
Iteration 12500: Loss = -12450.326988142557
3
Iteration 12600: Loss = -12450.41234400465
4
Iteration 12700: Loss = -12450.325840600799
5
Iteration 12800: Loss = -12450.369741625369
6
Iteration 12900: Loss = -12450.325674851645
Iteration 13000: Loss = -12450.333144461074
1
Iteration 13100: Loss = -12450.32567320503
Iteration 13200: Loss = -12450.325955714385
1
Iteration 13300: Loss = -12450.360294894765
2
Iteration 13400: Loss = -12450.331598793844
3
Iteration 13500: Loss = -12450.462329309883
4
Iteration 13600: Loss = -12450.325678826206
Iteration 13700: Loss = -12450.349406480329
1
Iteration 13800: Loss = -12450.325652650183
Iteration 13900: Loss = -12450.326198540695
1
Iteration 14000: Loss = -12450.325793237474
2
Iteration 14100: Loss = -12450.32567957546
Iteration 14200: Loss = -12450.330185991836
1
Iteration 14300: Loss = -12450.326300736868
2
Iteration 14400: Loss = -12450.326230190301
3
Iteration 14500: Loss = -12450.504879570954
4
Iteration 14600: Loss = -12450.325666981653
Iteration 14700: Loss = -12450.325761062963
Iteration 14800: Loss = -12450.333828166426
1
Iteration 14900: Loss = -12450.475545011279
2
Iteration 15000: Loss = -12450.32580749835
Iteration 15100: Loss = -12450.332641092751
1
Iteration 15200: Loss = -12450.341691946396
2
Iteration 15300: Loss = -12450.326278831444
3
Iteration 15400: Loss = -12450.326453795999
4
Iteration 15500: Loss = -12450.32792286095
5
Iteration 15600: Loss = -12450.370139997654
6
Iteration 15700: Loss = -12450.325825612223
Iteration 15800: Loss = -12450.32574660667
Iteration 15900: Loss = -12450.331092401742
1
Iteration 16000: Loss = -12450.325660619981
Iteration 16100: Loss = -12450.383192706982
1
Iteration 16200: Loss = -12450.407276538615
2
Iteration 16300: Loss = -12450.325678118086
Iteration 16400: Loss = -12450.326053885427
1
Iteration 16500: Loss = -12450.326927238882
2
Iteration 16600: Loss = -12450.343390455047
3
Iteration 16700: Loss = -12450.39390204578
4
Iteration 16800: Loss = -12450.325599039888
Iteration 16900: Loss = -12450.328527746047
1
Iteration 17000: Loss = -12450.326551461243
2
Iteration 17100: Loss = -12450.35151447562
3
Iteration 17200: Loss = -12450.330844217933
4
Iteration 17300: Loss = -12450.327041515015
5
Iteration 17400: Loss = -12450.335859304298
6
Iteration 17500: Loss = -12450.325643108092
Iteration 17600: Loss = -12450.375887409406
1
Iteration 17700: Loss = -12450.327730952873
2
Iteration 17800: Loss = -12450.326346005935
3
Iteration 17900: Loss = -12450.328388829445
4
Iteration 18000: Loss = -12450.325606795488
Iteration 18100: Loss = -12450.32643060001
1
Iteration 18200: Loss = -12450.483202399244
2
Iteration 18300: Loss = -12450.346018837252
3
Iteration 18400: Loss = -12450.325607419398
Iteration 18500: Loss = -12450.32601627449
1
Iteration 18600: Loss = -12450.325719191973
2
Iteration 18700: Loss = -12450.325700488658
Iteration 18800: Loss = -12450.325666249753
Iteration 18900: Loss = -12450.325674205076
Iteration 19000: Loss = -12450.325587919688
Iteration 19100: Loss = -12450.325938220907
1
Iteration 19200: Loss = -12450.334016393334
2
Iteration 19300: Loss = -12450.325624358296
Iteration 19400: Loss = -12450.325776193815
1
Iteration 19500: Loss = -12450.32885013378
2
Iteration 19600: Loss = -12450.32688137497
3
Iteration 19700: Loss = -12450.325602485951
Iteration 19800: Loss = -12450.325800971144
1
Iteration 19900: Loss = -12450.325583460377
pi: tensor([[8.7539e-01, 1.2461e-01],
        [1.0000e+00, 1.4313e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9898, 0.0102], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2017, 0.0413],
         [0.6255, 0.2122]],

        [[0.6500, 0.2170],
         [0.5325, 0.6931]],

        [[0.6858, 0.2036],
         [0.5662, 0.5794]],

        [[0.6034, 0.2172],
         [0.6241, 0.7179]],

        [[0.5929, 0.1854],
         [0.6502, 0.7237]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005836786776225259
Average Adjusted Rand Index: 0.0015228343097195556
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21398.93391223433
Iteration 100: Loss = -12457.539370518252
Iteration 200: Loss = -12456.899966523306
Iteration 300: Loss = -12456.291537591309
Iteration 400: Loss = -12454.258979606986
Iteration 500: Loss = -12453.09385758884
Iteration 600: Loss = -12449.32446857195
Iteration 700: Loss = -12435.00004295571
Iteration 800: Loss = -12430.16542030588
Iteration 900: Loss = -12417.124090866153
Iteration 1000: Loss = -12416.626389753803
Iteration 1100: Loss = -12416.493818009965
Iteration 1200: Loss = -12416.41598885502
Iteration 1300: Loss = -12416.371759030435
Iteration 1400: Loss = -12416.323502713722
Iteration 1500: Loss = -12416.262824810876
Iteration 1600: Loss = -12416.174927620566
Iteration 1700: Loss = -12416.023986463246
Iteration 1800: Loss = -12415.765692056719
Iteration 1900: Loss = -12415.595280121703
Iteration 2000: Loss = -12415.549994697049
Iteration 2100: Loss = -12415.53289447928
Iteration 2200: Loss = -12415.524064195215
Iteration 2300: Loss = -12415.518612773163
Iteration 2400: Loss = -12415.51487393831
Iteration 2500: Loss = -12415.512161580446
Iteration 2600: Loss = -12415.510062816767
Iteration 2700: Loss = -12415.508420653865
Iteration 2800: Loss = -12415.507089867215
Iteration 2900: Loss = -12415.50594202033
Iteration 3000: Loss = -12415.505032994275
Iteration 3100: Loss = -12415.504252695251
Iteration 3200: Loss = -12415.503586045863
Iteration 3300: Loss = -12415.503039964135
Iteration 3400: Loss = -12415.502511861621
Iteration 3500: Loss = -12415.516602738264
1
Iteration 3600: Loss = -12415.50169227144
Iteration 3700: Loss = -12415.501358996813
Iteration 3800: Loss = -12415.50104277614
Iteration 3900: Loss = -12415.500825725785
Iteration 4000: Loss = -12415.500499400814
Iteration 4100: Loss = -12415.500279561486
Iteration 4200: Loss = -12415.50011986964
Iteration 4300: Loss = -12415.499918565854
Iteration 4400: Loss = -12415.499768427531
Iteration 4500: Loss = -12415.499592693663
Iteration 4600: Loss = -12415.499458238748
Iteration 4700: Loss = -12415.499322016196
Iteration 4800: Loss = -12415.49924379516
Iteration 4900: Loss = -12415.4993036767
Iteration 5000: Loss = -12415.499052960256
Iteration 5100: Loss = -12415.498928312965
Iteration 5200: Loss = -12415.499076695813
1
Iteration 5300: Loss = -12415.498722176282
Iteration 5400: Loss = -12415.49868810991
Iteration 5500: Loss = -12415.498922167602
1
Iteration 5600: Loss = -12415.49855719788
Iteration 5700: Loss = -12415.498461859095
Iteration 5800: Loss = -12415.498427106344
Iteration 5900: Loss = -12415.498356905831
Iteration 6000: Loss = -12415.4982811125
Iteration 6100: Loss = -12415.498280239251
Iteration 6200: Loss = -12415.498260963055
Iteration 6300: Loss = -12415.498310138537
Iteration 6400: Loss = -12415.498114925265
Iteration 6500: Loss = -12415.498067072767
Iteration 6600: Loss = -12415.498076884414
Iteration 6700: Loss = -12415.498039275686
Iteration 6800: Loss = -12415.498308486009
1
Iteration 6900: Loss = -12415.498007738763
Iteration 7000: Loss = -12415.498301420346
1
Iteration 7100: Loss = -12415.498923911311
2
Iteration 7200: Loss = -12415.498331858878
3
Iteration 7300: Loss = -12415.497891123203
Iteration 7400: Loss = -12415.499131799843
1
Iteration 7500: Loss = -12415.49788011819
Iteration 7600: Loss = -12415.497913408662
Iteration 7700: Loss = -12415.497898031592
Iteration 7800: Loss = -12415.49800615015
1
Iteration 7900: Loss = -12415.497864116374
Iteration 8000: Loss = -12415.497783668747
Iteration 8100: Loss = -12415.49933894404
1
Iteration 8200: Loss = -12415.497767486248
Iteration 8300: Loss = -12415.98344880009
1
Iteration 8400: Loss = -12415.497784343774
Iteration 8500: Loss = -12415.497788429759
Iteration 8600: Loss = -12415.522201950438
1
Iteration 8700: Loss = -12415.49777261504
Iteration 8800: Loss = -12415.497739879056
Iteration 8900: Loss = -12415.51029534792
1
Iteration 9000: Loss = -12415.497726306008
Iteration 9100: Loss = -12415.49772543766
Iteration 9200: Loss = -12415.602805500957
1
Iteration 9300: Loss = -12415.497720454101
Iteration 9400: Loss = -12415.506727614586
1
Iteration 9500: Loss = -12415.506234892488
2
Iteration 9600: Loss = -12415.49766481769
Iteration 9700: Loss = -12415.498627025907
1
Iteration 9800: Loss = -12415.504752480469
2
Iteration 9900: Loss = -12415.507036829904
3
Iteration 10000: Loss = -12415.503871795829
4
Iteration 10100: Loss = -12415.498420025713
5
Iteration 10200: Loss = -12415.497717764218
Iteration 10300: Loss = -12415.497932353801
1
Iteration 10400: Loss = -12415.502884659873
2
Iteration 10500: Loss = -12415.497703334211
Iteration 10600: Loss = -12415.826715743304
1
Iteration 10700: Loss = -12415.49767557737
Iteration 10800: Loss = -12415.497673509897
Iteration 10900: Loss = -12415.49767926887
Iteration 11000: Loss = -12415.49764709763
Iteration 11100: Loss = -12415.497668554503
Iteration 11200: Loss = -12415.497787299655
1
Iteration 11300: Loss = -12415.497676744484
Iteration 11400: Loss = -12415.501302991663
1
Iteration 11500: Loss = -12415.497951002164
2
Iteration 11600: Loss = -12415.497780717105
3
Iteration 11700: Loss = -12415.49766407373
Iteration 11800: Loss = -12415.497636077735
Iteration 11900: Loss = -12415.497756536011
1
Iteration 12000: Loss = -12415.497697136034
Iteration 12100: Loss = -12415.497723280052
Iteration 12200: Loss = -12415.497753752035
Iteration 12300: Loss = -12415.504176583207
1
Iteration 12400: Loss = -12415.497676088482
Iteration 12500: Loss = -12415.497744290955
Iteration 12600: Loss = -12415.497664292769
Iteration 12700: Loss = -12415.501459540477
1
Iteration 12800: Loss = -12415.497639657413
Iteration 12900: Loss = -12415.498225682326
1
Iteration 13000: Loss = -12415.49773262706
Iteration 13100: Loss = -12415.571011948783
1
Iteration 13200: Loss = -12415.497660551127
Iteration 13300: Loss = -12415.561410578499
1
Iteration 13400: Loss = -12415.497639310115
Iteration 13500: Loss = -12415.497697333305
Iteration 13600: Loss = -12415.497729314058
Iteration 13700: Loss = -12415.49765988851
Iteration 13800: Loss = -12415.497966671037
1
Iteration 13900: Loss = -12415.49907069583
2
Iteration 14000: Loss = -12415.55955643599
3
Iteration 14100: Loss = -12415.50290821636
4
Iteration 14200: Loss = -12415.497938487699
5
Iteration 14300: Loss = -12415.50631187297
6
Iteration 14400: Loss = -12415.49766630949
Iteration 14500: Loss = -12415.498463672668
1
Iteration 14600: Loss = -12415.498898240665
2
Iteration 14700: Loss = -12415.507571283333
3
Iteration 14800: Loss = -12415.497762025272
Iteration 14900: Loss = -12415.501243957588
1
Iteration 15000: Loss = -12415.49765705866
Iteration 15100: Loss = -12415.498261307668
1
Iteration 15200: Loss = -12415.498796718379
2
Iteration 15300: Loss = -12415.497696337856
Iteration 15400: Loss = -12415.522465091763
1
Iteration 15500: Loss = -12415.497694837422
Iteration 15600: Loss = -12415.498556173148
1
Iteration 15700: Loss = -12415.503067320611
2
Iteration 15800: Loss = -12415.498034866087
3
Iteration 15900: Loss = -12415.57196555534
4
Iteration 16000: Loss = -12415.498093939235
5
Iteration 16100: Loss = -12415.499201147448
6
Iteration 16200: Loss = -12415.497727838127
Iteration 16300: Loss = -12415.49775646481
Iteration 16400: Loss = -12415.659972803927
1
Iteration 16500: Loss = -12415.49913315458
2
Iteration 16600: Loss = -12415.499052739751
3
Iteration 16700: Loss = -12415.49815049809
4
Iteration 16800: Loss = -12415.49994892619
5
Iteration 16900: Loss = -12415.498670897
6
Iteration 17000: Loss = -12415.49826836353
7
Iteration 17100: Loss = -12415.82998770443
8
Iteration 17200: Loss = -12415.497651489904
Iteration 17300: Loss = -12415.76261337725
1
Iteration 17400: Loss = -12415.497652339984
Iteration 17500: Loss = -12415.502336117703
1
Iteration 17600: Loss = -12415.4976398298
Iteration 17700: Loss = -12415.780785583822
1
Iteration 17800: Loss = -12415.497665001492
Iteration 17900: Loss = -12415.505626665677
1
Iteration 18000: Loss = -12415.497925608968
2
Iteration 18100: Loss = -12415.497754199963
Iteration 18200: Loss = -12415.526261371822
1
Iteration 18300: Loss = -12415.497740683344
Iteration 18400: Loss = -12415.497716006832
Iteration 18500: Loss = -12415.686329815982
1
Iteration 18600: Loss = -12415.498734765719
2
Iteration 18700: Loss = -12415.498559131576
3
Iteration 18800: Loss = -12415.525482032055
4
Iteration 18900: Loss = -12415.498793471366
5
Iteration 19000: Loss = -12415.498139971105
6
Iteration 19100: Loss = -12415.510229066527
7
Iteration 19200: Loss = -12415.514978222101
8
Iteration 19300: Loss = -12415.497805139037
Iteration 19400: Loss = -12415.497988974575
1
Iteration 19500: Loss = -12415.49764846677
Iteration 19600: Loss = -12415.498496973352
1
Iteration 19700: Loss = -12415.498554217616
2
Iteration 19800: Loss = -12415.518869996282
3
Iteration 19900: Loss = -12415.500542003465
4
pi: tensor([[0.9772, 0.0228],
        [0.9719, 0.0281]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6528, 0.3472], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2121, 0.1032],
         [0.6218, 0.2954]],

        [[0.6845, 0.1228],
         [0.6300, 0.6783]],

        [[0.6399, 0.1899],
         [0.5563, 0.5205]],

        [[0.6843, 0.1554],
         [0.6544, 0.7126]],

        [[0.6906, 0.1553],
         [0.7061, 0.5819]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7021882688074582
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.04118420150444328
Average Adjusted Rand Index: 0.14320177767069894
11976.520096087863
[0.0005836786776225259, 0.04118420150444328] [0.0015228343097195556, 0.14320177767069894] [12450.334015903327, 12415.497707430588]
-------------------------------------
This iteration is 99
True Objective function: Loss = -12077.012180968703
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21451.867695399804
Iteration 100: Loss = -12563.603151010206
Iteration 200: Loss = -12563.079651364154
Iteration 300: Loss = -12562.856033204884
Iteration 400: Loss = -12562.497509888959
Iteration 500: Loss = -12561.38007893732
Iteration 600: Loss = -12561.192945545346
Iteration 700: Loss = -12561.12378838405
Iteration 800: Loss = -12561.086789279403
Iteration 900: Loss = -12561.062929769283
Iteration 1000: Loss = -12561.045798171575
Iteration 1100: Loss = -12561.032576595611
Iteration 1200: Loss = -12561.021894573025
Iteration 1300: Loss = -12561.012896522632
Iteration 1400: Loss = -12561.004981272203
Iteration 1500: Loss = -12560.997876416464
Iteration 1600: Loss = -12560.991488603993
Iteration 1700: Loss = -12560.985686922548
Iteration 1800: Loss = -12560.98046646415
Iteration 1900: Loss = -12560.975898006205
Iteration 2000: Loss = -12560.971809695919
Iteration 2100: Loss = -12560.968271628253
Iteration 2200: Loss = -12560.96511449026
Iteration 2300: Loss = -12560.962315345078
Iteration 2400: Loss = -12560.959839005789
Iteration 2500: Loss = -12560.95753706443
Iteration 2600: Loss = -12560.955478324098
Iteration 2700: Loss = -12560.953627406345
Iteration 2800: Loss = -12560.951938371358
Iteration 2900: Loss = -12560.950347954771
Iteration 3000: Loss = -12560.948908154971
Iteration 3100: Loss = -12560.947542989137
Iteration 3200: Loss = -12560.946258768934
Iteration 3300: Loss = -12560.945053924053
Iteration 3400: Loss = -12560.943848923433
Iteration 3500: Loss = -12560.942685408641
Iteration 3600: Loss = -12560.94145019167
Iteration 3700: Loss = -12560.940023586145
Iteration 3800: Loss = -12560.938089315821
Iteration 3900: Loss = -12560.933932889315
Iteration 4000: Loss = -12560.909208549861
Iteration 4100: Loss = -12560.697105451141
Iteration 4200: Loss = -12560.549122434442
Iteration 4300: Loss = -12560.520041175992
Iteration 4400: Loss = -12560.510392392642
Iteration 4500: Loss = -12560.505762302586
Iteration 4600: Loss = -12560.503108144565
Iteration 4700: Loss = -12560.501404703866
Iteration 4800: Loss = -12560.500240665004
Iteration 4900: Loss = -12560.499396576506
Iteration 5000: Loss = -12560.498767191018
Iteration 5100: Loss = -12560.49829037082
Iteration 5200: Loss = -12560.49787220616
Iteration 5300: Loss = -12560.497529137787
Iteration 5400: Loss = -12560.497211384498
Iteration 5500: Loss = -12560.4969692705
Iteration 5600: Loss = -12560.496754973694
Iteration 5700: Loss = -12560.496566751644
Iteration 5800: Loss = -12560.49642577301
Iteration 5900: Loss = -12560.496248702904
Iteration 6000: Loss = -12560.496129010357
Iteration 6100: Loss = -12560.496006474908
Iteration 6200: Loss = -12560.495892082292
Iteration 6300: Loss = -12560.495758039604
Iteration 6400: Loss = -12560.495671669383
Iteration 6500: Loss = -12560.495576231078
Iteration 6600: Loss = -12560.495547562103
Iteration 6700: Loss = -12560.495434665725
Iteration 6800: Loss = -12560.495465231152
Iteration 6900: Loss = -12560.49524494721
Iteration 7000: Loss = -12560.495907422579
1
Iteration 7100: Loss = -12560.495154729557
Iteration 7200: Loss = -12560.506675382458
1
Iteration 7300: Loss = -12560.497413252773
2
Iteration 7400: Loss = -12560.49621244569
3
Iteration 7500: Loss = -12560.494985624868
Iteration 7600: Loss = -12560.494887285759
Iteration 7700: Loss = -12560.494824470277
Iteration 7800: Loss = -12560.49490244187
Iteration 7900: Loss = -12560.494741818535
Iteration 8000: Loss = -12560.495139239245
1
Iteration 8100: Loss = -12560.49466113223
Iteration 8200: Loss = -12560.494597569996
Iteration 8300: Loss = -12560.495293249258
1
Iteration 8400: Loss = -12560.494616312088
Iteration 8500: Loss = -12560.496125885284
1
Iteration 8600: Loss = -12560.494524099944
Iteration 8700: Loss = -12560.494455430864
Iteration 8800: Loss = -12560.595743284719
1
Iteration 8900: Loss = -12560.49442445609
Iteration 9000: Loss = -12560.494489505174
Iteration 9100: Loss = -12560.49440999492
Iteration 9200: Loss = -12560.494382840223
Iteration 9300: Loss = -12560.494345967934
Iteration 9400: Loss = -12560.494771960479
1
Iteration 9500: Loss = -12560.494274338946
Iteration 9600: Loss = -12561.098409287482
1
Iteration 9700: Loss = -12560.494249537194
Iteration 9800: Loss = -12560.494235785784
Iteration 9900: Loss = -12560.723340982899
1
Iteration 10000: Loss = -12560.494237528916
Iteration 10100: Loss = -12560.494684835132
1
Iteration 10200: Loss = -12560.494145229897
Iteration 10300: Loss = -12560.497731641392
1
Iteration 10400: Loss = -12560.495032509696
2
Iteration 10500: Loss = -12560.494149884005
Iteration 10600: Loss = -12560.49421728924
Iteration 10700: Loss = -12560.494872498313
1
Iteration 10800: Loss = -12560.50508050834
2
Iteration 10900: Loss = -12560.494122914994
Iteration 11000: Loss = -12560.49481945766
1
Iteration 11100: Loss = -12560.494052063123
Iteration 11200: Loss = -12560.494303905785
1
Iteration 11300: Loss = -12560.49883588606
2
Iteration 11400: Loss = -12560.494046040734
Iteration 11500: Loss = -12560.503876653967
1
Iteration 11600: Loss = -12560.494007615442
Iteration 11700: Loss = -12560.494018224357
Iteration 11800: Loss = -12560.501205506092
1
Iteration 11900: Loss = -12560.49407228675
Iteration 12000: Loss = -12560.49413893915
Iteration 12100: Loss = -12560.568628942934
1
Iteration 12200: Loss = -12560.494008838043
Iteration 12300: Loss = -12560.519913859074
1
Iteration 12400: Loss = -12560.49394120818
Iteration 12500: Loss = -12560.503131515792
1
Iteration 12600: Loss = -12560.499110350682
2
Iteration 12700: Loss = -12560.533324409535
3
Iteration 12800: Loss = -12560.493966518692
Iteration 12900: Loss = -12560.526850180915
1
Iteration 13000: Loss = -12560.493980331448
Iteration 13100: Loss = -12560.501160625232
1
Iteration 13200: Loss = -12560.49396089663
Iteration 13300: Loss = -12560.49761676908
1
Iteration 13400: Loss = -12560.493932852823
Iteration 13500: Loss = -12560.4990840944
1
Iteration 13600: Loss = -12560.493901103326
Iteration 13700: Loss = -12560.98845546948
1
Iteration 13800: Loss = -12560.493970309193
Iteration 13900: Loss = -12560.494643961649
1
Iteration 14000: Loss = -12560.49392887832
Iteration 14100: Loss = -12560.493919327891
Iteration 14200: Loss = -12560.493910926207
Iteration 14300: Loss = -12560.495116668248
1
Iteration 14400: Loss = -12560.493947110559
Iteration 14500: Loss = -12560.493899956384
Iteration 14600: Loss = -12560.508048121184
1
Iteration 14700: Loss = -12560.493902461556
Iteration 14800: Loss = -12560.495727827802
1
Iteration 14900: Loss = -12560.494407612248
2
Iteration 15000: Loss = -12560.494452905188
3
Iteration 15100: Loss = -12560.496111668754
4
Iteration 15200: Loss = -12560.494119371117
5
Iteration 15300: Loss = -12560.493903300108
Iteration 15400: Loss = -12560.495654146807
1
Iteration 15500: Loss = -12560.4939118568
Iteration 15600: Loss = -12560.49481200561
1
Iteration 15700: Loss = -12560.493907040434
Iteration 15800: Loss = -12560.501390819905
1
Iteration 15900: Loss = -12560.506746122695
2
Iteration 16000: Loss = -12560.493902273582
Iteration 16100: Loss = -12560.499485035158
1
Iteration 16200: Loss = -12560.493905908115
Iteration 16300: Loss = -12560.493941732579
Iteration 16400: Loss = -12560.496426978183
1
Iteration 16500: Loss = -12560.493878251864
Iteration 16600: Loss = -12560.493900206999
Iteration 16700: Loss = -12560.494173432853
1
Iteration 16800: Loss = -12560.493879120231
Iteration 16900: Loss = -12560.540487391247
1
Iteration 17000: Loss = -12560.493890361586
Iteration 17100: Loss = -12560.50199242324
1
Iteration 17200: Loss = -12560.617875062127
2
Iteration 17300: Loss = -12560.49389120625
Iteration 17400: Loss = -12560.863258686333
1
Iteration 17500: Loss = -12560.493918560687
Iteration 17600: Loss = -12560.604286049675
1
Iteration 17700: Loss = -12560.493861980944
Iteration 17800: Loss = -12560.494073480382
1
Iteration 17900: Loss = -12560.4938932955
Iteration 18000: Loss = -12560.499816432015
1
Iteration 18100: Loss = -12560.49388054354
Iteration 18200: Loss = -12560.493949581349
Iteration 18300: Loss = -12560.685168019942
1
Iteration 18400: Loss = -12560.493862945668
Iteration 18500: Loss = -12560.659826938752
1
Iteration 18600: Loss = -12560.493841666244
Iteration 18700: Loss = -12560.494697095522
1
Iteration 18800: Loss = -12560.494802395422
2
Iteration 18900: Loss = -12560.493867566072
Iteration 19000: Loss = -12560.495465770175
1
Iteration 19100: Loss = -12560.494737072126
2
Iteration 19200: Loss = -12560.494059011578
3
Iteration 19300: Loss = -12560.493887836115
Iteration 19400: Loss = -12560.501511908506
1
Iteration 19500: Loss = -12560.493870628523
Iteration 19600: Loss = -12560.49837841107
1
Iteration 19700: Loss = -12560.493870679898
Iteration 19800: Loss = -12560.877102805582
1
Iteration 19900: Loss = -12560.493861367886
pi: tensor([[9.7952e-01, 2.0482e-02],
        [9.9999e-01, 1.0144e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9928, 0.0072], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2056, 0.0837],
         [0.5531, 0.2958]],

        [[0.7194, 0.1598],
         [0.5171, 0.6914]],

        [[0.5835, 0.2758],
         [0.5683, 0.6415]],

        [[0.5673, 0.3143],
         [0.5683, 0.6457]],

        [[0.5321, 0.1659],
         [0.6057, 0.5858]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -1.0239312731407937e-06
Average Adjusted Rand Index: -0.0007607424547914065
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21688.64019187068
Iteration 100: Loss = -12563.424707054719
Iteration 200: Loss = -12563.077283569588
Iteration 300: Loss = -12562.679573644986
Iteration 400: Loss = -12562.089436744855
Iteration 500: Loss = -12561.744842953754
Iteration 600: Loss = -12561.452304838776
Iteration 700: Loss = -12561.213487828554
Iteration 800: Loss = -12561.072907482729
Iteration 900: Loss = -12560.993142767136
Iteration 1000: Loss = -12560.942024605867
Iteration 1100: Loss = -12560.903793955633
Iteration 1200: Loss = -12560.871302203066
Iteration 1300: Loss = -12560.84085753368
Iteration 1400: Loss = -12560.80973340273
Iteration 1500: Loss = -12560.775953010767
Iteration 1600: Loss = -12560.739505149251
Iteration 1700: Loss = -12560.70460484143
Iteration 1800: Loss = -12560.675724313161
Iteration 1900: Loss = -12560.653145200102
Iteration 2000: Loss = -12560.635141456807
Iteration 2100: Loss = -12560.620397126037
Iteration 2200: Loss = -12560.60801870558
Iteration 2300: Loss = -12560.597477282361
Iteration 2400: Loss = -12560.588511303105
Iteration 2500: Loss = -12560.580769522452
Iteration 2600: Loss = -12560.574082388308
Iteration 2700: Loss = -12560.568175798342
Iteration 2800: Loss = -12560.563082740047
Iteration 2900: Loss = -12560.558564528339
Iteration 3000: Loss = -12560.554603812507
Iteration 3100: Loss = -12560.55101855749
Iteration 3200: Loss = -12560.547806805595
Iteration 3300: Loss = -12560.544913124962
Iteration 3400: Loss = -12560.54230445986
Iteration 3500: Loss = -12560.539893016681
Iteration 3600: Loss = -12560.537682561271
Iteration 3700: Loss = -12560.53568017099
Iteration 3800: Loss = -12560.533838544128
Iteration 3900: Loss = -12560.53216205262
Iteration 4000: Loss = -12560.530538848898
Iteration 4100: Loss = -12560.52906420118
Iteration 4200: Loss = -12560.527719982612
Iteration 4300: Loss = -12560.526474047776
Iteration 4400: Loss = -12560.52539600445
Iteration 4500: Loss = -12560.524212379845
Iteration 4600: Loss = -12560.523204154411
Iteration 4700: Loss = -12560.522265334037
Iteration 4800: Loss = -12560.52135724968
Iteration 4900: Loss = -12560.520592679652
Iteration 5000: Loss = -12560.519761387153
Iteration 5100: Loss = -12560.51912169883
Iteration 5200: Loss = -12560.527175309937
1
Iteration 5300: Loss = -12560.51775360549
Iteration 5400: Loss = -12560.517201986011
Iteration 5500: Loss = -12560.516612133664
Iteration 5600: Loss = -12560.516074757506
Iteration 5700: Loss = -12560.515635556578
Iteration 5800: Loss = -12560.515163915235
Iteration 5900: Loss = -12560.514738136546
Iteration 6000: Loss = -12560.520966617365
1
Iteration 6100: Loss = -12560.513943543983
Iteration 6200: Loss = -12560.513629086961
Iteration 6300: Loss = -12560.513302690728
Iteration 6400: Loss = -12560.512915105783
Iteration 6500: Loss = -12560.532850127163
1
Iteration 6600: Loss = -12560.51240444562
Iteration 6700: Loss = -12560.51392293799
1
Iteration 6800: Loss = -12560.511933399408
Iteration 6900: Loss = -12560.511732528113
Iteration 7000: Loss = -12560.760534004783
1
Iteration 7100: Loss = -12560.511190551437
Iteration 7200: Loss = -12560.892642326116
1
Iteration 7300: Loss = -12560.510792122823
Iteration 7400: Loss = -12560.510644212094
Iteration 7500: Loss = -12560.512556128131
1
Iteration 7600: Loss = -12560.510261528674
Iteration 7700: Loss = -12560.510145097614
Iteration 7800: Loss = -12560.510198965498
Iteration 7900: Loss = -12560.509866690398
Iteration 8000: Loss = -12560.509771637826
Iteration 8100: Loss = -12560.51155884424
1
Iteration 8200: Loss = -12560.509526853766
Iteration 8300: Loss = -12560.509397772023
Iteration 8400: Loss = -12560.565513006302
1
Iteration 8500: Loss = -12560.509185657274
Iteration 8600: Loss = -12560.509133390891
Iteration 8700: Loss = -12560.52868337641
1
Iteration 8800: Loss = -12560.508927645278
Iteration 8900: Loss = -12560.508856437615
Iteration 9000: Loss = -12560.656027254383
1
Iteration 9100: Loss = -12560.508715966342
Iteration 9200: Loss = -12560.508642713714
Iteration 9300: Loss = -12560.748685008233
1
Iteration 9400: Loss = -12560.508537347536
Iteration 9500: Loss = -12560.508473006008
Iteration 9600: Loss = -12560.508415767397
Iteration 9700: Loss = -12560.508708375046
1
Iteration 9800: Loss = -12560.508322988127
Iteration 9900: Loss = -12560.508281673863
Iteration 10000: Loss = -12560.508446592632
1
Iteration 10100: Loss = -12560.508188943935
Iteration 10200: Loss = -12560.508147306244
Iteration 10300: Loss = -12560.508135263739
Iteration 10400: Loss = -12560.50807914537
Iteration 10500: Loss = -12560.5080588041
Iteration 10600: Loss = -12560.508014254123
Iteration 10700: Loss = -12560.50796010895
Iteration 10800: Loss = -12560.507912474488
Iteration 10900: Loss = -12560.509441271577
1
Iteration 11000: Loss = -12560.507946883325
Iteration 11100: Loss = -12560.509877316186
1
Iteration 11200: Loss = -12560.507891775163
Iteration 11300: Loss = -12560.526383164814
1
Iteration 11400: Loss = -12560.508050545073
2
Iteration 11500: Loss = -12560.5185265584
3
Iteration 11600: Loss = -12560.50825060794
4
Iteration 11700: Loss = -12560.541476206577
5
Iteration 11800: Loss = -12560.507895557446
Iteration 11900: Loss = -12560.520751250593
1
Iteration 12000: Loss = -12560.507862404656
Iteration 12100: Loss = -12560.507847366429
Iteration 12200: Loss = -12560.532761040564
1
Iteration 12300: Loss = -12560.507689144568
Iteration 12400: Loss = -12560.576317469766
1
Iteration 12500: Loss = -12560.507632168888
Iteration 12600: Loss = -12560.51140850352
1
Iteration 12700: Loss = -12560.507595230143
Iteration 12800: Loss = -12560.507983873002
1
Iteration 12900: Loss = -12560.511601599002
2
Iteration 13000: Loss = -12560.50768101561
Iteration 13100: Loss = -12560.518379893841
1
Iteration 13200: Loss = -12560.508164595398
2
Iteration 13300: Loss = -12560.507560513022
Iteration 13400: Loss = -12560.50976818278
1
Iteration 13500: Loss = -12560.50895056328
2
Iteration 13600: Loss = -12560.509989157119
3
Iteration 13700: Loss = -12560.507722954973
4
Iteration 13800: Loss = -12560.510343358606
5
Iteration 13900: Loss = -12560.507661919004
6
Iteration 14000: Loss = -12560.507667260405
7
Iteration 14100: Loss = -12560.513965933775
8
Iteration 14200: Loss = -12560.509268121483
9
Iteration 14300: Loss = -12560.507536351555
Iteration 14400: Loss = -12560.51955134566
1
Iteration 14500: Loss = -12560.511976822223
2
Iteration 14600: Loss = -12560.50758428053
Iteration 14700: Loss = -12560.508357323726
1
Iteration 14800: Loss = -12560.50822575749
2
Iteration 14900: Loss = -12560.507442182516
Iteration 15000: Loss = -12560.507520075455
Iteration 15100: Loss = -12560.507505557654
Iteration 15200: Loss = -12560.50746433934
Iteration 15300: Loss = -12560.507408410152
Iteration 15400: Loss = -12560.507501941565
Iteration 15500: Loss = -12560.507450646766
Iteration 15600: Loss = -12560.507502749999
Iteration 15700: Loss = -12560.50778029542
1
Iteration 15800: Loss = -12560.507645864449
2
Iteration 15900: Loss = -12560.555818862911
3
Iteration 16000: Loss = -12560.507393703625
Iteration 16100: Loss = -12560.507960790872
1
Iteration 16200: Loss = -12560.521651155905
2
Iteration 16300: Loss = -12560.508327363239
3
Iteration 16400: Loss = -12560.507672488886
4
Iteration 16500: Loss = -12560.585017629945
5
Iteration 16600: Loss = -12560.507334876691
Iteration 16700: Loss = -12560.519701947527
1
Iteration 16800: Loss = -12560.507768589025
2
Iteration 16900: Loss = -12560.535426178723
3
Iteration 17000: Loss = -12560.521982215176
4
Iteration 17100: Loss = -12560.517198279604
5
Iteration 17200: Loss = -12560.508846411636
6
Iteration 17300: Loss = -12560.5079572847
7
Iteration 17400: Loss = -12560.50736847423
Iteration 17500: Loss = -12560.508104152927
1
Iteration 17600: Loss = -12560.531593838474
2
Iteration 17700: Loss = -12560.507585560095
3
Iteration 17800: Loss = -12560.572519640486
4
Iteration 17900: Loss = -12560.50738482916
Iteration 18000: Loss = -12560.511541206773
1
Iteration 18100: Loss = -12560.509308630435
2
Iteration 18200: Loss = -12560.50777956273
3
Iteration 18300: Loss = -12560.561603058346
4
Iteration 18400: Loss = -12560.507407864761
Iteration 18500: Loss = -12560.50917186467
1
Iteration 18600: Loss = -12560.507373773835
Iteration 18700: Loss = -12560.507835901277
1
Iteration 18800: Loss = -12560.507365357664
Iteration 18900: Loss = -12560.507802762995
1
Iteration 19000: Loss = -12560.590212992885
2
Iteration 19100: Loss = -12560.507374766072
Iteration 19200: Loss = -12560.509189758142
1
Iteration 19300: Loss = -12560.507354141242
Iteration 19400: Loss = -12560.508673917153
1
Iteration 19500: Loss = -12560.529493789503
2
Iteration 19600: Loss = -12560.517137278268
3
Iteration 19700: Loss = -12560.525706838509
4
Iteration 19800: Loss = -12560.549468841762
5
Iteration 19900: Loss = -12560.507428405313
pi: tensor([[1.0329e-06, 1.0000e+00],
        [5.5048e-02, 9.4495e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0021, 0.9979], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3494, 0.2140],
         [0.6345, 0.2007]],

        [[0.6933, 0.2191],
         [0.7012, 0.5908]],

        [[0.5715, 0.2647],
         [0.5919, 0.5356]],

        [[0.5129, 0.2740],
         [0.6706, 0.6121]],

        [[0.6834, 0.2460],
         [0.7032, 0.5911]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0007684665887528472
Average Adjusted Rand Index: -0.0006155917856971096
12077.012180968703
[-1.0239312731407937e-06, 0.0007684665887528472] [-0.0007607424547914065, -0.0006155917856971096] [12560.540439603694, 12560.508157509052]
