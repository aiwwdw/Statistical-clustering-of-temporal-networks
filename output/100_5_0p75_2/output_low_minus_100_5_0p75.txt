nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [21:38<35:42:01, 1298.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [42:05<34:11:57, 1256.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [1:03:47<34:24:41, 1277.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [1:25:25<34:16:54, 1285.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [1:44:07<32:21:59, 1226.52s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [2:05:58<32:46:45, 1255.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [2:27:35<32:46:52, 1268.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [2:39:29<27:54:38, 1092.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [2:59:09<28:18:13, 1119.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [3:21:49<29:51:00, 1194.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [3:43:33<30:20:58, 1227.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [4:01:21<28:49:10, 1178.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [4:24:17<29:55:50, 1238.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [4:45:53<30:00:11, 1255.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [5:07:34<29:58:35, 1269.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [5:30:13<30:14:55, 1296.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [5:50:16<29:14:45, 1268.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [6:09:04<27:55:42, 1226.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [6:30:27<27:58:21, 1243.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [6:50:49<27:29:12, 1236.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [7:12:17<27:28:37, 1252.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [7:33:41<27:20:30, 1261.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [7:55:07<27:08:25, 1268.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [8:16:32<26:53:24, 1273.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [8:37:55<26:35:48, 1276.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [8:55:57<25:02:34, 1218.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [9:17:11<25:02:33, 1234.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [9:35:12<23:46:34, 1188.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [9:49:55<21:38:07, 1097.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|███       | 30/100 [10:11:05<22:20:20, 1148.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 31%|███       | 31/100 [10:32:29<22:47:46, 1189.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 32%|███▏      | 32/100 [10:53:46<22:57:43, 1215.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 33%|███▎      | 33/100 [11:12:18<22:02:53, 1184.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 34%|███▍      | 34/100 [11:26:02<19:44:09, 1076.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 35%|███▌      | 35/100 [11:47:30<20:34:57, 1139.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 36%|███▌      | 36/100 [12:12:24<22:09:20, 1246.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 37%|███▋      | 37/100 [12:32:27<21:34:38, 1232.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 38%|███▊      | 38/100 [12:52:04<20:56:52, 1216.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 39%|███▉      | 39/100 [13:13:24<20:56:02, 1235.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 40%|████      | 40/100 [13:34:52<20:51:21, 1251.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 41%|████      | 41/100 [13:54:47<20:13:48, 1234.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -9889.737807953867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23805.400470465353
Iteration 100: Loss = -9741.471865292173
Iteration 200: Loss = -9740.072511863777
Iteration 300: Loss = -9739.672247792296
Iteration 400: Loss = -9739.448307883447
Iteration 500: Loss = -9739.279580420134
Iteration 600: Loss = -9739.126052578351
Iteration 700: Loss = -9739.02392916766
Iteration 800: Loss = -9738.965982144486
Iteration 900: Loss = -9738.913951135853
Iteration 1000: Loss = -9738.855553159307
Iteration 1100: Loss = -9738.786977529026
Iteration 1200: Loss = -9738.710547826278
Iteration 1300: Loss = -9738.629421990401
Iteration 1400: Loss = -9738.544384996545
Iteration 1500: Loss = -9738.445526435578
Iteration 1600: Loss = -9738.3288902066
Iteration 1700: Loss = -9738.185345254871
Iteration 1800: Loss = -9737.984081696719
Iteration 1900: Loss = -9737.744250134552
Iteration 2000: Loss = -9737.393442773155
Iteration 2100: Loss = -9737.082335207993
Iteration 2200: Loss = -9736.841258663813
Iteration 2300: Loss = -9736.588108387852
Iteration 2400: Loss = -9736.10940963506
Iteration 2500: Loss = -9735.311020041347
Iteration 2600: Loss = -9734.802662995706
Iteration 2700: Loss = -9734.666003882912
Iteration 2800: Loss = -9734.620862747417
Iteration 2900: Loss = -9734.520788230839
Iteration 3000: Loss = -9733.79838037848
Iteration 3100: Loss = -9733.540793563934
Iteration 3200: Loss = -9733.515268210092
Iteration 3300: Loss = -9733.510428472406
Iteration 3400: Loss = -9733.509239569577
Iteration 3500: Loss = -9733.508315238834
Iteration 3600: Loss = -9733.507933789651
Iteration 3700: Loss = -9733.507732762384
Iteration 3800: Loss = -9733.507627954636
Iteration 3900: Loss = -9733.507559809113
Iteration 4000: Loss = -9733.507489732856
Iteration 4100: Loss = -9733.507469725122
Iteration 4200: Loss = -9733.507420617843
Iteration 4300: Loss = -9733.51915120611
1
Iteration 4400: Loss = -9733.507975769555
2
Iteration 4500: Loss = -9733.507372231688
Iteration 4600: Loss = -9733.507469026077
Iteration 4700: Loss = -9733.507366098029
Iteration 4800: Loss = -9733.507365126703
Iteration 4900: Loss = -9733.507369033665
Iteration 5000: Loss = -9733.507504732856
1
Iteration 5100: Loss = -9733.507335658212
Iteration 5200: Loss = -9733.514044567683
1
Iteration 5300: Loss = -9733.507339493377
Iteration 5400: Loss = -9733.507329208112
Iteration 5500: Loss = -9733.507650392221
1
Iteration 5600: Loss = -9733.522688284429
2
Iteration 5700: Loss = -9733.507332797912
Iteration 5800: Loss = -9733.507327853957
Iteration 5900: Loss = -9733.507321516943
Iteration 6000: Loss = -9733.507359929334
Iteration 6100: Loss = -9733.507307510601
Iteration 6200: Loss = -9733.517586888705
1
Iteration 6300: Loss = -9733.5074295274
2
Iteration 6400: Loss = -9733.507352284852
Iteration 6500: Loss = -9733.508038268494
1
Iteration 6600: Loss = -9733.507893473368
2
Iteration 6700: Loss = -9733.507503497074
3
Iteration 6800: Loss = -9733.507472806641
4
Iteration 6900: Loss = -9733.507330551654
Iteration 7000: Loss = -9733.508689070304
1
Iteration 7100: Loss = -9733.507354306343
Iteration 7200: Loss = -9733.531403740648
1
Iteration 7300: Loss = -9733.507339345471
Iteration 7400: Loss = -9733.50864803724
1
Iteration 7500: Loss = -9733.50731629159
Iteration 7600: Loss = -9733.507351110808
Iteration 7700: Loss = -9733.52224981612
1
Iteration 7800: Loss = -9733.507370460087
Iteration 7900: Loss = -9733.50730472678
Iteration 8000: Loss = -9733.511831472899
1
Iteration 8100: Loss = -9733.507347145682
Iteration 8200: Loss = -9733.507319289098
Iteration 8300: Loss = -9733.51063588018
1
Iteration 8400: Loss = -9733.507344344438
Iteration 8500: Loss = -9733.507344985876
Iteration 8600: Loss = -9733.507573450113
1
Iteration 8700: Loss = -9733.507335642536
Iteration 8800: Loss = -9733.50820821332
1
Iteration 8900: Loss = -9733.50736349285
Iteration 9000: Loss = -9733.871283416047
1
Iteration 9100: Loss = -9733.507338924917
Iteration 9200: Loss = -9733.507333336498
Iteration 9300: Loss = -9733.507482977035
1
Iteration 9400: Loss = -9733.507340735818
Iteration 9500: Loss = -9733.50739374448
Iteration 9600: Loss = -9733.507334798252
Iteration 9700: Loss = -9733.507428379648
Iteration 9800: Loss = -9733.507357663335
Iteration 9900: Loss = -9733.58545426827
1
Iteration 10000: Loss = -9733.507334126247
Iteration 10100: Loss = -9733.50747919047
1
Iteration 10200: Loss = -9733.507342877303
Iteration 10300: Loss = -9733.507304061699
Iteration 10400: Loss = -9733.508072792483
1
Iteration 10500: Loss = -9733.507298218303
Iteration 10600: Loss = -9733.52907489698
1
Iteration 10700: Loss = -9733.50735337657
Iteration 10800: Loss = -9733.815322690662
1
Iteration 10900: Loss = -9733.507347740484
Iteration 11000: Loss = -9733.507338509075
Iteration 11100: Loss = -9733.508740528387
1
Iteration 11200: Loss = -9733.507361240989
Iteration 11300: Loss = -9733.539157783003
1
Iteration 11400: Loss = -9733.50735131063
Iteration 11500: Loss = -9733.50843361217
1
Iteration 11600: Loss = -9733.507376411277
Iteration 11700: Loss = -9733.507343365058
Iteration 11800: Loss = -9733.507762370644
1
Iteration 11900: Loss = -9733.50731921667
Iteration 12000: Loss = -9733.509563796135
1
Iteration 12100: Loss = -9733.507337184223
Iteration 12200: Loss = -9733.519212547928
1
Iteration 12300: Loss = -9733.507324771117
Iteration 12400: Loss = -9733.55673555451
1
Iteration 12500: Loss = -9733.507333789405
Iteration 12600: Loss = -9733.507351346674
Iteration 12700: Loss = -9733.507613419693
1
Iteration 12800: Loss = -9733.507314573026
Iteration 12900: Loss = -9733.566985710599
1
Iteration 13000: Loss = -9733.50730726137
Iteration 13100: Loss = -9733.507345915361
Iteration 13200: Loss = -9733.507616590055
1
Iteration 13300: Loss = -9733.507328284524
Iteration 13400: Loss = -9733.51005841636
1
Iteration 13500: Loss = -9733.5073399007
Iteration 13600: Loss = -9733.85657653585
1
Iteration 13700: Loss = -9733.507375920766
Iteration 13800: Loss = -9733.507579070383
1
Iteration 13900: Loss = -9733.507392556316
Iteration 14000: Loss = -9733.507352742508
Iteration 14100: Loss = -9733.614285064441
1
Iteration 14200: Loss = -9733.507294201594
Iteration 14300: Loss = -9733.507366139731
Iteration 14400: Loss = -9733.508351908118
1
Iteration 14500: Loss = -9733.507316463809
Iteration 14600: Loss = -9733.528467934884
1
Iteration 14700: Loss = -9733.507316140935
Iteration 14800: Loss = -9733.509808812943
1
Iteration 14900: Loss = -9733.507321791813
Iteration 15000: Loss = -9733.5516176009
1
Iteration 15100: Loss = -9733.507357812414
Iteration 15200: Loss = -9733.80972511189
1
Iteration 15300: Loss = -9733.507338142084
Iteration 15400: Loss = -9733.521685464972
1
Iteration 15500: Loss = -9733.507354311965
Iteration 15600: Loss = -9733.507349620179
Iteration 15700: Loss = -9733.50747552762
1
Iteration 15800: Loss = -9733.507342742934
Iteration 15900: Loss = -9733.512614940943
1
Iteration 16000: Loss = -9733.50732557112
Iteration 16100: Loss = -9733.52393270202
1
Iteration 16200: Loss = -9733.507314897357
Iteration 16300: Loss = -9733.50731061862
Iteration 16400: Loss = -9733.50734714409
Iteration 16500: Loss = -9733.50733136102
Iteration 16600: Loss = -9733.507515276664
1
Iteration 16700: Loss = -9733.507348702788
Iteration 16800: Loss = -9733.5083604549
1
Iteration 16900: Loss = -9733.507319113818
Iteration 17000: Loss = -9733.508224508267
1
Iteration 17100: Loss = -9733.507328560923
Iteration 17200: Loss = -9733.510098618175
1
Iteration 17300: Loss = -9733.507344274254
Iteration 17400: Loss = -9733.510360705477
1
Iteration 17500: Loss = -9733.507332738216
Iteration 17600: Loss = -9733.511751798156
1
Iteration 17700: Loss = -9733.507307251299
Iteration 17800: Loss = -9733.509470039062
1
Iteration 17900: Loss = -9733.507321495745
Iteration 18000: Loss = -9733.50920327471
1
Iteration 18100: Loss = -9733.507347530896
Iteration 18200: Loss = -9733.5101498407
1
Iteration 18300: Loss = -9733.507349814838
Iteration 18400: Loss = -9733.51772384148
1
Iteration 18500: Loss = -9733.50736991084
Iteration 18600: Loss = -9733.521133258662
1
Iteration 18700: Loss = -9733.507345450975
Iteration 18800: Loss = -9733.525500046862
1
Iteration 18900: Loss = -9733.507366565367
Iteration 19000: Loss = -9733.603364605853
1
Iteration 19100: Loss = -9733.507326307958
Iteration 19200: Loss = -9733.790752808578
1
Iteration 19300: Loss = -9733.507355900454
Iteration 19400: Loss = -9733.60692329541
1
Iteration 19500: Loss = -9733.507339726482
Iteration 19600: Loss = -9733.618752850858
1
Iteration 19700: Loss = -9733.50735069442
Iteration 19800: Loss = -9733.584264262581
1
Iteration 19900: Loss = -9733.507359030169
pi: tensor([[0.8258, 0.1742],
        [0.1497, 0.8503]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1274, 0.8726], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1078, 0.0810],
         [0.7296, 0.1523]],

        [[0.5767, 0.1168],
         [0.5634, 0.5259]],

        [[0.5035, 0.1117],
         [0.6767, 0.5639]],

        [[0.5709, 0.1273],
         [0.5704, 0.5744]],

        [[0.7146, 0.1294],
         [0.6370, 0.6270]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.010862123580924353
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.020913200498678036
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 63
Adjusted Rand Index: 0.060329417513284606
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.03257722892007769
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.059806307118939396
Global Adjusted Rand Index: 0.03879990598530774
Average Adjusted Rand Index: 0.036897655526380815
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22991.22978533629
Iteration 100: Loss = -9742.402823258195
Iteration 200: Loss = -9740.191324057407
Iteration 300: Loss = -9739.588311978609
Iteration 400: Loss = -9739.356081452279
Iteration 500: Loss = -9739.232037999775
Iteration 600: Loss = -9739.143862559758
Iteration 700: Loss = -9739.066247688068
Iteration 800: Loss = -9738.991036043566
Iteration 900: Loss = -9738.921795189313
Iteration 1000: Loss = -9738.86010735992
Iteration 1100: Loss = -9738.798641284191
Iteration 1200: Loss = -9738.729821785038
Iteration 1300: Loss = -9738.647722466125
Iteration 1400: Loss = -9738.54906624858
Iteration 1500: Loss = -9738.434454531338
Iteration 1600: Loss = -9738.30994323959
Iteration 1700: Loss = -9738.189838457643
Iteration 1800: Loss = -9738.08503203257
Iteration 1900: Loss = -9737.998550618511
Iteration 2000: Loss = -9737.925818537335
Iteration 2100: Loss = -9737.862416976903
Iteration 2200: Loss = -9737.807016168375
Iteration 2300: Loss = -9737.754578146876
Iteration 2400: Loss = -9737.699104683812
Iteration 2500: Loss = -9737.63344708562
Iteration 2600: Loss = -9737.534418754765
Iteration 2700: Loss = -9737.3189973962
Iteration 2800: Loss = -9736.9321715219
Iteration 2900: Loss = -9736.52060949867
Iteration 3000: Loss = -9735.943928776524
Iteration 3100: Loss = -9735.18010350442
Iteration 3200: Loss = -9734.80498858269
Iteration 3300: Loss = -9734.678225737087
Iteration 3400: Loss = -9734.636992177684
Iteration 3500: Loss = -9734.551153904791
Iteration 3600: Loss = -9733.883612958634
Iteration 3700: Loss = -9733.545442421786
Iteration 3800: Loss = -9733.515946797595
Iteration 3900: Loss = -9733.510688524064
Iteration 4000: Loss = -9733.509004006106
Iteration 4100: Loss = -9733.508332134921
Iteration 4200: Loss = -9733.507968736889
Iteration 4300: Loss = -9733.507785411597
Iteration 4400: Loss = -9733.507811412048
Iteration 4500: Loss = -9733.507565954571
Iteration 4600: Loss = -9733.507529254812
Iteration 4700: Loss = -9733.5074933616
Iteration 4800: Loss = -9733.507460028019
Iteration 4900: Loss = -9733.508108847629
1
Iteration 5000: Loss = -9733.507400033744
Iteration 5100: Loss = -9733.507410418466
Iteration 5200: Loss = -9733.507414549606
Iteration 5300: Loss = -9733.507361329826
Iteration 5400: Loss = -9733.507474423972
1
Iteration 5500: Loss = -9733.507362581742
Iteration 5600: Loss = -9733.508296881944
1
Iteration 5700: Loss = -9733.507391911022
Iteration 5800: Loss = -9733.507436288837
Iteration 5900: Loss = -9733.507473778569
Iteration 6000: Loss = -9733.507450453697
Iteration 6100: Loss = -9733.507324355889
Iteration 6200: Loss = -9733.50737883511
Iteration 6300: Loss = -9733.52740005725
1
Iteration 6400: Loss = -9733.507440017978
Iteration 6500: Loss = -9733.507310474
Iteration 6600: Loss = -9733.50884216075
1
Iteration 6700: Loss = -9733.507347151426
Iteration 6800: Loss = -9733.50824759674
1
Iteration 6900: Loss = -9733.507299501356
Iteration 7000: Loss = -9733.533913251062
1
Iteration 7100: Loss = -9733.507353593679
Iteration 7200: Loss = -9733.507341552495
Iteration 7300: Loss = -9733.512702034446
1
Iteration 7400: Loss = -9733.507332958681
Iteration 7500: Loss = -9733.507322498917
Iteration 7600: Loss = -9733.520806977069
1
Iteration 7700: Loss = -9733.507317492664
Iteration 7800: Loss = -9733.50733827028
Iteration 7900: Loss = -9733.50777731101
1
Iteration 8000: Loss = -9733.507315492203
Iteration 8100: Loss = -9733.512669158332
1
Iteration 8200: Loss = -9733.507330228667
Iteration 8300: Loss = -9733.50731345351
Iteration 8400: Loss = -9733.517851862702
1
Iteration 8500: Loss = -9733.507337187764
Iteration 8600: Loss = -9733.50731706636
Iteration 8700: Loss = -9733.50823453585
1
Iteration 8800: Loss = -9733.507316972848
Iteration 8900: Loss = -9733.507347841822
Iteration 9000: Loss = -9733.507653968332
1
Iteration 9100: Loss = -9733.507350611682
Iteration 9200: Loss = -9733.507305719444
Iteration 9300: Loss = -9733.507854500087
1
Iteration 9400: Loss = -9733.50734754935
Iteration 9500: Loss = -9733.561039866154
1
Iteration 9600: Loss = -9733.507319224693
Iteration 9700: Loss = -9733.507328480222
Iteration 9800: Loss = -9733.509428115949
1
Iteration 9900: Loss = -9733.507353151892
Iteration 10000: Loss = -9733.50735354457
Iteration 10100: Loss = -9733.510826895259
1
Iteration 10200: Loss = -9733.507330007616
Iteration 10300: Loss = -9733.591738658133
1
Iteration 10400: Loss = -9733.507354602967
Iteration 10500: Loss = -9733.50735052531
Iteration 10600: Loss = -9733.509867566958
1
Iteration 10700: Loss = -9733.507317773934
Iteration 10800: Loss = -9733.50735780875
Iteration 10900: Loss = -9733.507338933432
Iteration 11000: Loss = -9733.507348817686
Iteration 11100: Loss = -9733.517718617513
1
Iteration 11200: Loss = -9733.507327206938
Iteration 11300: Loss = -9733.507318149397
Iteration 11400: Loss = -9733.507616583178
1
Iteration 11500: Loss = -9733.507376723634
Iteration 11600: Loss = -9733.553301580798
1
Iteration 11700: Loss = -9733.507323022992
Iteration 11800: Loss = -9733.507346444456
Iteration 11900: Loss = -9733.507396760968
Iteration 12000: Loss = -9733.507370155912
Iteration 12100: Loss = -9733.52360747558
1
Iteration 12200: Loss = -9733.50734947803
Iteration 12300: Loss = -9733.507331181072
Iteration 12400: Loss = -9733.507414509875
Iteration 12500: Loss = -9733.507359286326
Iteration 12600: Loss = -9733.507408374504
Iteration 12700: Loss = -9733.507447154212
Iteration 12800: Loss = -9733.507356481512
Iteration 12900: Loss = -9733.512749573365
1
Iteration 13000: Loss = -9733.507331112929
Iteration 13100: Loss = -9733.881052888513
1
Iteration 13200: Loss = -9733.507340904202
Iteration 13300: Loss = -9733.507342052222
Iteration 13400: Loss = -9733.507502733062
1
Iteration 13500: Loss = -9733.507347476332
Iteration 13600: Loss = -9733.547810110374
1
Iteration 13700: Loss = -9733.507337022065
Iteration 13800: Loss = -9733.508887501017
1
Iteration 13900: Loss = -9733.507525997547
2
Iteration 14000: Loss = -9733.507360136537
Iteration 14100: Loss = -9733.523278757144
1
Iteration 14200: Loss = -9733.507374644243
Iteration 14300: Loss = -9733.668295013178
1
Iteration 14400: Loss = -9733.507343719639
Iteration 14500: Loss = -9733.61105750503
1
Iteration 14600: Loss = -9733.507331850476
Iteration 14700: Loss = -9733.507354370695
Iteration 14800: Loss = -9733.507723747027
1
Iteration 14900: Loss = -9733.5073452114
Iteration 15000: Loss = -9733.513733578151
1
Iteration 15100: Loss = -9733.507331409823
Iteration 15200: Loss = -9733.516523935174
1
Iteration 15300: Loss = -9733.507331302788
Iteration 15400: Loss = -9733.761166065033
1
Iteration 15500: Loss = -9733.507323724913
Iteration 15600: Loss = -9733.522351212065
1
Iteration 15700: Loss = -9733.507351179918
Iteration 15800: Loss = -9733.511779591718
1
Iteration 15900: Loss = -9733.507336817176
Iteration 16000: Loss = -9733.509415332557
1
Iteration 16100: Loss = -9733.507346396076
Iteration 16200: Loss = -9733.514946032128
1
Iteration 16300: Loss = -9733.507300100418
Iteration 16400: Loss = -9733.578564191603
1
Iteration 16500: Loss = -9733.50733067972
Iteration 16600: Loss = -9733.638479302594
1
Iteration 16700: Loss = -9733.507367296339
Iteration 16800: Loss = -9733.780536137572
1
Iteration 16900: Loss = -9733.507350618078
Iteration 17000: Loss = -9733.892899111304
1
Iteration 17100: Loss = -9733.507339905638
Iteration 17200: Loss = -9733.507340126152
Iteration 17300: Loss = -9733.507558888086
1
Iteration 17400: Loss = -9733.507348760975
Iteration 17500: Loss = -9733.512774217383
1
Iteration 17600: Loss = -9733.507337917861
Iteration 17700: Loss = -9733.58467081706
1
Iteration 17800: Loss = -9733.507292357175
Iteration 17900: Loss = -9733.522694202189
1
Iteration 18000: Loss = -9733.507327963602
Iteration 18100: Loss = -9733.507465127002
1
Iteration 18200: Loss = -9733.507384480325
Iteration 18300: Loss = -9733.5073236771
Iteration 18400: Loss = -9733.507455462326
1
Iteration 18500: Loss = -9733.50733042461
Iteration 18600: Loss = -9733.507494777996
1
Iteration 18700: Loss = -9733.507347497825
Iteration 18800: Loss = -9733.510366390452
1
Iteration 18900: Loss = -9733.507337068837
Iteration 19000: Loss = -9733.518304730445
1
Iteration 19100: Loss = -9733.507353519371
Iteration 19200: Loss = -9733.508541975752
1
Iteration 19300: Loss = -9733.50732993943
Iteration 19400: Loss = -9733.513413005987
1
Iteration 19500: Loss = -9733.507338570993
Iteration 19600: Loss = -9733.533301720303
1
Iteration 19700: Loss = -9733.507320455805
Iteration 19800: Loss = -9733.899545564089
1
Iteration 19900: Loss = -9733.507359940384
pi: tensor([[0.8492, 0.1508],
        [0.1773, 0.8227]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8737, 0.1263], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1507, 0.0806],
         [0.6661, 0.1079]],

        [[0.7298, 0.1170],
         [0.6248, 0.5678]],

        [[0.6593, 0.1120],
         [0.6721, 0.6408]],

        [[0.7060, 0.1273],
         [0.5444, 0.5077]],

        [[0.5087, 0.1294],
         [0.5110, 0.7115]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.010862123580924353
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.027469209953202733
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.07107080494979295
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.040858473632435006
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.059806307118939396
Global Adjusted Rand Index: 0.043722537775146904
Average Adjusted Rand Index: 0.04201338384705889
9889.737807953867
[0.03879990598530774, 0.043722537775146904] [0.036897655526380815, 0.04201338384705889] [9733.876346478088, 9733.507444581972]
-------------------------------------
This iteration is 1
True Objective function: Loss = -9654.098549469172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20633.883357956085
Iteration 100: Loss = -9540.768556545589
Iteration 200: Loss = -9539.959213918373
Iteration 300: Loss = -9539.67138406301
Iteration 400: Loss = -9539.52437153881
Iteration 500: Loss = -9539.438795765054
Iteration 600: Loss = -9539.384393471066
Iteration 700: Loss = -9539.347964761615
Iteration 800: Loss = -9539.322812636929
Iteration 900: Loss = -9539.304706936828
Iteration 1000: Loss = -9539.291014232824
Iteration 1100: Loss = -9539.28004532952
Iteration 1200: Loss = -9539.270611391335
Iteration 1300: Loss = -9539.262033413605
Iteration 1400: Loss = -9539.253851418363
Iteration 1500: Loss = -9539.245725465715
Iteration 1600: Loss = -9539.23757408666
Iteration 1700: Loss = -9539.229144141182
Iteration 1800: Loss = -9539.220199819876
Iteration 1900: Loss = -9539.210409841198
Iteration 2000: Loss = -9539.19925038119
Iteration 2100: Loss = -9539.185879072322
Iteration 2200: Loss = -9539.168917764706
Iteration 2300: Loss = -9539.145748231074
Iteration 2400: Loss = -9539.10870038197
Iteration 2500: Loss = -9539.01294087738
Iteration 2600: Loss = -9538.475946520392
Iteration 2700: Loss = -9538.27153795691
Iteration 2800: Loss = -9538.181522351217
Iteration 2900: Loss = -9538.12398412084
Iteration 3000: Loss = -9538.078734123832
Iteration 3100: Loss = -9538.038741731947
Iteration 3200: Loss = -9538.001647126583
Iteration 3300: Loss = -9537.967995782985
Iteration 3400: Loss = -9537.938436221195
Iteration 3500: Loss = -9537.906735814691
Iteration 3600: Loss = -9537.879149026121
Iteration 3700: Loss = -9537.731811940519
Iteration 3800: Loss = -9537.561829909584
Iteration 3900: Loss = -9537.426422289398
Iteration 4000: Loss = -9537.256442210477
Iteration 4100: Loss = -9537.150886422894
Iteration 4200: Loss = -9537.056686091099
Iteration 4300: Loss = -9536.214481260418
Iteration 4400: Loss = -9536.01894762566
Iteration 4500: Loss = -9535.974919363647
Iteration 4600: Loss = -9535.957424102795
Iteration 4700: Loss = -9535.947933805584
Iteration 4800: Loss = -9535.941904805451
Iteration 4900: Loss = -9535.937775669427
Iteration 5000: Loss = -9535.934761972494
Iteration 5100: Loss = -9535.932425222309
Iteration 5200: Loss = -9535.930611493637
Iteration 5300: Loss = -9535.929218630246
Iteration 5400: Loss = -9535.92791224934
Iteration 5500: Loss = -9535.927703114769
Iteration 5600: Loss = -9535.926008618986
Iteration 5700: Loss = -9535.963334325286
1
Iteration 5800: Loss = -9535.924647472335
Iteration 5900: Loss = -9535.924041833427
Iteration 6000: Loss = -9535.923623364695
Iteration 6100: Loss = -9535.936715688531
1
Iteration 6200: Loss = -9535.922674958725
Iteration 6300: Loss = -9535.92323112259
1
Iteration 6400: Loss = -9535.92201087635
Iteration 6500: Loss = -9535.93451643801
1
Iteration 6600: Loss = -9535.921464012226
Iteration 6700: Loss = -9535.921225674565
Iteration 6800: Loss = -9535.92237845945
1
Iteration 6900: Loss = -9535.920844481072
Iteration 7000: Loss = -9535.920647874838
Iteration 7100: Loss = -9535.920658300312
Iteration 7200: Loss = -9535.920323715165
Iteration 7300: Loss = -9536.171156981016
1
Iteration 7400: Loss = -9535.92006481451
Iteration 7500: Loss = -9535.91993696009
Iteration 7600: Loss = -9535.926152710066
1
Iteration 7700: Loss = -9535.919665555122
Iteration 7800: Loss = -9535.920699493276
1
Iteration 7900: Loss = -9535.9194281281
Iteration 8000: Loss = -9535.922814161126
1
Iteration 8100: Loss = -9535.919253209813
Iteration 8200: Loss = -9535.919191053807
Iteration 8300: Loss = -9535.919161163496
Iteration 8400: Loss = -9535.919035552517
Iteration 8500: Loss = -9535.919038388645
Iteration 8600: Loss = -9535.91892569585
Iteration 8700: Loss = -9535.922285962533
1
Iteration 8800: Loss = -9535.918825262746
Iteration 8900: Loss = -9535.918759670458
Iteration 9000: Loss = -9535.919276976689
1
Iteration 9100: Loss = -9535.918707362238
Iteration 9200: Loss = -9535.958787365891
1
Iteration 9300: Loss = -9535.918632377954
Iteration 9400: Loss = -9535.918738405595
1
Iteration 9500: Loss = -9535.918556401823
Iteration 9600: Loss = -9535.918568787676
Iteration 9700: Loss = -9535.953174815411
1
Iteration 9800: Loss = -9535.918445279141
Iteration 9900: Loss = -9535.957678530189
1
Iteration 10000: Loss = -9535.918417042756
Iteration 10100: Loss = -9535.921537642931
1
Iteration 10200: Loss = -9535.918388948103
Iteration 10300: Loss = -9535.918711061895
1
Iteration 10400: Loss = -9535.922909913335
2
Iteration 10500: Loss = -9535.918283284396
Iteration 10600: Loss = -9536.052806821745
1
Iteration 10700: Loss = -9535.918235049252
Iteration 10800: Loss = -9535.919791898268
1
Iteration 10900: Loss = -9535.936085235522
2
Iteration 11000: Loss = -9535.919012700462
3
Iteration 11100: Loss = -9535.923281467913
4
Iteration 11200: Loss = -9535.943894471633
5
Iteration 11300: Loss = -9535.919086006063
6
Iteration 11400: Loss = -9536.006299938603
7
Iteration 11500: Loss = -9535.946288988818
8
Iteration 11600: Loss = -9535.929002466408
9
Iteration 11700: Loss = -9535.926468620108
10
Iteration 11800: Loss = -9535.919042991305
11
Iteration 11900: Loss = -9535.918070866537
Iteration 12000: Loss = -9535.921231224425
1
Iteration 12100: Loss = -9535.9191932228
2
Iteration 12200: Loss = -9535.918069937412
Iteration 12300: Loss = -9535.91831865584
1
Iteration 12400: Loss = -9535.920004106987
2
Iteration 12500: Loss = -9535.91801488139
Iteration 12600: Loss = -9536.15055439337
1
Iteration 12700: Loss = -9535.917927707656
Iteration 12800: Loss = -9535.941164223486
1
Iteration 12900: Loss = -9535.922930553326
2
Iteration 13000: Loss = -9535.917463220989
Iteration 13100: Loss = -9535.917541001132
Iteration 13200: Loss = -9535.917417036284
Iteration 13300: Loss = -9535.917849911115
1
Iteration 13400: Loss = -9535.91898395697
2
Iteration 13500: Loss = -9535.917419616462
Iteration 13600: Loss = -9535.91787225416
1
Iteration 13700: Loss = -9535.918572799015
2
Iteration 13800: Loss = -9535.917368507337
Iteration 13900: Loss = -9535.917663381902
1
Iteration 14000: Loss = -9535.919317554582
2
Iteration 14100: Loss = -9535.999978092987
3
Iteration 14200: Loss = -9535.917334125365
Iteration 14300: Loss = -9535.919107363072
1
Iteration 14400: Loss = -9535.917374409477
Iteration 14500: Loss = -9535.91746702138
Iteration 14600: Loss = -9535.918289557852
1
Iteration 14700: Loss = -9535.917387678583
Iteration 14800: Loss = -9535.917409081048
Iteration 14900: Loss = -9535.91736430952
Iteration 15000: Loss = -9535.918671509608
1
Iteration 15100: Loss = -9535.917307081127
Iteration 15200: Loss = -9535.917345148482
Iteration 15300: Loss = -9535.917395920258
Iteration 15400: Loss = -9535.917337419623
Iteration 15500: Loss = -9535.918831727808
1
Iteration 15600: Loss = -9535.917315047378
Iteration 15700: Loss = -9535.981769353893
1
Iteration 15800: Loss = -9536.056685204612
2
Iteration 15900: Loss = -9535.91729277972
Iteration 16000: Loss = -9535.917483250789
1
Iteration 16100: Loss = -9535.934431771822
2
Iteration 16200: Loss = -9535.918533933755
3
Iteration 16300: Loss = -9535.917713836918
4
Iteration 16400: Loss = -9535.918253903794
5
Iteration 16500: Loss = -9535.91829747765
6
Iteration 16600: Loss = -9535.917379271381
Iteration 16700: Loss = -9535.917320652503
Iteration 16800: Loss = -9535.919527025133
1
Iteration 16900: Loss = -9536.01150315136
2
Iteration 17000: Loss = -9536.025044010088
3
Iteration 17100: Loss = -9535.917236037469
Iteration 17200: Loss = -9535.91741383639
1
Iteration 17300: Loss = -9536.00681553206
2
Iteration 17400: Loss = -9535.942338733217
3
Iteration 17500: Loss = -9535.917212900484
Iteration 17600: Loss = -9535.91733439978
1
Iteration 17700: Loss = -9535.950322236069
2
Iteration 17800: Loss = -9535.918213973955
3
Iteration 17900: Loss = -9535.917380481575
4
Iteration 18000: Loss = -9535.9176182754
5
Iteration 18100: Loss = -9535.918837202578
6
Iteration 18200: Loss = -9535.930095041755
7
Iteration 18300: Loss = -9536.139842828174
8
Iteration 18400: Loss = -9535.917196116381
Iteration 18500: Loss = -9535.92304722628
1
Iteration 18600: Loss = -9535.917150183861
Iteration 18700: Loss = -9535.91727402956
1
Iteration 18800: Loss = -9535.917107593821
Iteration 18900: Loss = -9535.919254942912
1
Iteration 19000: Loss = -9535.925519261316
2
Iteration 19100: Loss = -9535.91715935571
Iteration 19200: Loss = -9535.951921283073
1
Iteration 19300: Loss = -9535.91713421521
Iteration 19400: Loss = -9535.918507047078
1
Iteration 19500: Loss = -9535.918996668732
2
Iteration 19600: Loss = -9535.917119534617
Iteration 19700: Loss = -9535.91759240676
1
Iteration 19800: Loss = -9535.91710033498
Iteration 19900: Loss = -9535.927443871991
1
pi: tensor([[1.0000e+00, 1.0342e-07],
        [6.1316e-01, 3.8684e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0105, 0.9895], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1345, 0.0410],
         [0.5326, 0.1239]],

        [[0.5779, 0.1267],
         [0.5537, 0.5302]],

        [[0.7053, 0.1273],
         [0.7295, 0.7064]],

        [[0.6918, 0.0736],
         [0.6542, 0.5240]],

        [[0.7025, 0.1580],
         [0.6985, 0.7086]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0028959952356207414
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001823854615464514
Average Adjusted Rand Index: -0.0009892036148026503
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24435.01616889511
Iteration 100: Loss = -9540.569835903332
Iteration 200: Loss = -9539.672646729285
Iteration 300: Loss = -9539.450774827214
Iteration 400: Loss = -9539.325934454775
Iteration 500: Loss = -9539.233991462102
Iteration 600: Loss = -9539.145085479226
Iteration 700: Loss = -9539.035769154672
Iteration 800: Loss = -9538.889911232101
Iteration 900: Loss = -9538.726659017575
Iteration 1000: Loss = -9538.587557743065
Iteration 1100: Loss = -9538.487558315543
Iteration 1200: Loss = -9538.419136731318
Iteration 1300: Loss = -9538.372011692414
Iteration 1400: Loss = -9538.338721181706
Iteration 1500: Loss = -9538.314568715648
Iteration 1600: Loss = -9538.29645146621
Iteration 1700: Loss = -9538.282550233649
Iteration 1800: Loss = -9538.271548699055
Iteration 1900: Loss = -9538.26260591588
Iteration 2000: Loss = -9538.255095355547
Iteration 2100: Loss = -9538.248651309741
Iteration 2200: Loss = -9538.24292158192
Iteration 2300: Loss = -9538.23775753397
Iteration 2400: Loss = -9538.232911970852
Iteration 2500: Loss = -9538.228353955934
Iteration 2600: Loss = -9538.223921781218
Iteration 2700: Loss = -9538.219609168695
Iteration 2800: Loss = -9538.21528182475
Iteration 2900: Loss = -9538.21093547016
Iteration 3000: Loss = -9538.206562966874
Iteration 3100: Loss = -9538.202106640052
Iteration 3200: Loss = -9538.197476580346
Iteration 3300: Loss = -9538.192730847293
Iteration 3400: Loss = -9538.187791498642
Iteration 3500: Loss = -9538.182700838512
Iteration 3600: Loss = -9538.177388844637
Iteration 3700: Loss = -9538.171755111174
Iteration 3800: Loss = -9538.165890762924
Iteration 3900: Loss = -9538.159678693457
Iteration 4000: Loss = -9538.153080139207
Iteration 4100: Loss = -9538.146024064876
Iteration 4200: Loss = -9538.138290129727
Iteration 4300: Loss = -9538.129492361491
Iteration 4400: Loss = -9538.118561532312
Iteration 4500: Loss = -9538.101183236144
Iteration 4600: Loss = -9538.042167062114
Iteration 4700: Loss = -9536.812955842903
Iteration 4800: Loss = -9536.703138794317
Iteration 4900: Loss = -9536.67202042308
Iteration 5000: Loss = -9536.658435546327
Iteration 5100: Loss = -9536.649930695987
Iteration 5200: Loss = -9536.64439616433
Iteration 5300: Loss = -9536.633175274354
Iteration 5400: Loss = -9536.621653345444
Iteration 5500: Loss = -9536.594456977973
Iteration 5600: Loss = -9536.456228830924
Iteration 5700: Loss = -9536.270145859422
Iteration 5800: Loss = -9536.16656158316
Iteration 5900: Loss = -9536.169093340504
1
Iteration 6000: Loss = -9536.166143356648
Iteration 6100: Loss = -9536.166091168181
Iteration 6200: Loss = -9536.171457196202
1
Iteration 6300: Loss = -9536.165902205965
Iteration 6400: Loss = -9536.164615650749
Iteration 6500: Loss = -9536.13320975672
Iteration 6600: Loss = -9536.136827628927
1
Iteration 6700: Loss = -9536.13713395385
2
Iteration 6800: Loss = -9536.132712348175
Iteration 6900: Loss = -9536.132734443121
Iteration 7000: Loss = -9536.132618326445
Iteration 7100: Loss = -9536.134144405665
1
Iteration 7200: Loss = -9536.132653371975
Iteration 7300: Loss = -9536.132609028784
Iteration 7400: Loss = -9536.132546491477
Iteration 7500: Loss = -9536.132467168576
Iteration 7600: Loss = -9536.143434347554
1
Iteration 7700: Loss = -9536.132419123724
Iteration 7800: Loss = -9536.132882367336
1
Iteration 7900: Loss = -9536.164261696471
2
Iteration 8000: Loss = -9536.132405739405
Iteration 8100: Loss = -9536.132355422555
Iteration 8200: Loss = -9536.1443721818
1
Iteration 8300: Loss = -9536.132313670463
Iteration 8400: Loss = -9536.14670934366
1
Iteration 8500: Loss = -9536.132278487223
Iteration 8600: Loss = -9536.134793198607
1
Iteration 8700: Loss = -9536.132223376087
Iteration 8800: Loss = -9536.136450512971
1
Iteration 8900: Loss = -9536.13220884063
Iteration 9000: Loss = -9536.134059834116
1
Iteration 9100: Loss = -9536.132143963077
Iteration 9200: Loss = -9536.132496875514
1
Iteration 9300: Loss = -9536.132158233462
Iteration 9400: Loss = -9536.133301592654
1
Iteration 9500: Loss = -9536.132111510928
Iteration 9600: Loss = -9536.13215571456
Iteration 9700: Loss = -9536.13406117399
1
Iteration 9800: Loss = -9536.14785796168
2
Iteration 9900: Loss = -9536.135483051958
3
Iteration 10000: Loss = -9536.142717002593
4
Iteration 10100: Loss = -9536.13330535389
5
Iteration 10200: Loss = -9536.134165350666
6
Iteration 10300: Loss = -9536.187934977668
7
Iteration 10400: Loss = -9536.139676371824
8
Iteration 10500: Loss = -9536.132107397374
Iteration 10600: Loss = -9536.13209541504
Iteration 10700: Loss = -9536.135205672294
1
Iteration 10800: Loss = -9536.132350070828
2
Iteration 10900: Loss = -9536.13292964382
3
Iteration 11000: Loss = -9536.152387570653
4
Iteration 11100: Loss = -9536.151066368382
5
Iteration 11200: Loss = -9536.137403547536
6
Iteration 11300: Loss = -9536.176631584418
7
Iteration 11400: Loss = -9536.132089374696
Iteration 11500: Loss = -9536.132016213696
Iteration 11600: Loss = -9536.138649413439
1
Iteration 11700: Loss = -9536.136896941603
2
Iteration 11800: Loss = -9536.132961347512
3
Iteration 11900: Loss = -9536.163563925047
4
Iteration 12000: Loss = -9536.15112813876
5
Iteration 12100: Loss = -9536.13399131812
6
Iteration 12200: Loss = -9536.132814865052
7
Iteration 12300: Loss = -9536.226993118016
8
Iteration 12400: Loss = -9536.132698580568
9
Iteration 12500: Loss = -9536.131994655538
Iteration 12600: Loss = -9536.16142626053
1
Iteration 12700: Loss = -9536.344512481573
2
Iteration 12800: Loss = -9536.133160861697
3
Iteration 12900: Loss = -9536.136386868684
4
Iteration 13000: Loss = -9536.135157386467
5
Iteration 13100: Loss = -9536.132237329828
6
Iteration 13200: Loss = -9536.132975529032
7
Iteration 13300: Loss = -9536.132997261147
8
Iteration 13400: Loss = -9536.133216925917
9
Iteration 13500: Loss = -9536.132763926384
10
Iteration 13600: Loss = -9536.139015041557
11
Iteration 13700: Loss = -9536.13377659698
12
Iteration 13800: Loss = -9536.350447999164
13
Iteration 13900: Loss = -9536.131902285144
Iteration 14000: Loss = -9536.13266380744
1
Iteration 14100: Loss = -9536.134590130398
2
Iteration 14200: Loss = -9536.172082625799
3
Iteration 14300: Loss = -9536.136262198881
4
Iteration 14400: Loss = -9536.157082247288
5
Iteration 14500: Loss = -9536.172535171503
6
Iteration 14600: Loss = -9536.14798414908
7
Iteration 14700: Loss = -9536.13600879274
8
Iteration 14800: Loss = -9536.132869901958
9
Iteration 14900: Loss = -9536.134102515627
10
Iteration 15000: Loss = -9536.148704616013
11
Iteration 15100: Loss = -9536.162718953725
12
Iteration 15200: Loss = -9536.131927273691
Iteration 15300: Loss = -9536.13708596917
1
Iteration 15400: Loss = -9536.196026989624
2
Iteration 15500: Loss = -9536.161197671818
3
Iteration 15600: Loss = -9536.142583268333
4
Iteration 15700: Loss = -9536.178778610347
5
Iteration 15800: Loss = -9536.138629557068
6
Iteration 15900: Loss = -9536.13188928699
Iteration 16000: Loss = -9536.13307907641
1
Iteration 16100: Loss = -9536.132277191015
2
Iteration 16200: Loss = -9536.131941743133
Iteration 16300: Loss = -9536.147936462075
1
Iteration 16400: Loss = -9536.132294084538
2
Iteration 16500: Loss = -9536.13713621447
3
Iteration 16600: Loss = -9536.141600356073
4
Iteration 16700: Loss = -9536.139348875044
5
Iteration 16800: Loss = -9536.133880117182
6
Iteration 16900: Loss = -9536.154669567644
7
Iteration 17000: Loss = -9536.137132194292
8
Iteration 17100: Loss = -9536.269664655429
9
Iteration 17200: Loss = -9536.155093161691
10
Iteration 17300: Loss = -9536.155546657983
11
Iteration 17400: Loss = -9536.143621342497
12
Iteration 17500: Loss = -9536.33420831167
13
Iteration 17600: Loss = -9536.155879126763
14
Iteration 17700: Loss = -9536.138173177282
15
Stopping early at iteration 17700 due to no improvement.
pi: tensor([[8.8605e-01, 1.1395e-01],
        [9.9999e-01, 7.0949e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6512, 0.3488], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1365, 0.1173],
         [0.6929, 0.1016]],

        [[0.5280, 0.0861],
         [0.7269, 0.5697]],

        [[0.5923, 0.1246],
         [0.6109, 0.5143]],

        [[0.5816, 0.0882],
         [0.6781, 0.6932]],

        [[0.7289, 0.1266],
         [0.6788, 0.6664]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.012681472519875562
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0020512683807855877
Average Adjusted Rand Index: -0.0030025997618157415
9654.098549469172
[-0.001823854615464514, -0.0020512683807855877] [-0.0009892036148026503, -0.0030025997618157415] [9535.917095502295, 9536.138173177282]
-------------------------------------
This iteration is 2
True Objective function: Loss = -9909.493893091018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23279.23565077584
Iteration 100: Loss = -9769.502573971115
Iteration 200: Loss = -9768.194886152587
Iteration 300: Loss = -9767.727114228695
Iteration 400: Loss = -9767.515339905705
Iteration 500: Loss = -9767.407249945225
Iteration 600: Loss = -9767.347378648854
Iteration 700: Loss = -9767.31194417669
Iteration 800: Loss = -9767.289675950808
Iteration 900: Loss = -9767.274907431647
Iteration 1000: Loss = -9767.264699241694
Iteration 1100: Loss = -9767.257325156424
Iteration 1200: Loss = -9767.251815060552
Iteration 1300: Loss = -9767.247529518821
Iteration 1400: Loss = -9767.244073961007
Iteration 1500: Loss = -9767.241212830037
Iteration 1600: Loss = -9767.23873053417
Iteration 1700: Loss = -9767.2365284079
Iteration 1800: Loss = -9767.23447492777
Iteration 1900: Loss = -9767.232538557862
Iteration 2000: Loss = -9767.23062165406
Iteration 2100: Loss = -9767.228687616747
Iteration 2200: Loss = -9767.226644720493
Iteration 2300: Loss = -9767.224469545929
Iteration 2400: Loss = -9767.222052417093
Iteration 2500: Loss = -9767.219270279747
Iteration 2600: Loss = -9767.216051150826
Iteration 2700: Loss = -9767.212185288769
Iteration 2800: Loss = -9767.207375037622
Iteration 2900: Loss = -9767.201256434544
Iteration 3000: Loss = -9767.193087678967
Iteration 3100: Loss = -9767.181797325034
Iteration 3200: Loss = -9767.16560361061
Iteration 3300: Loss = -9767.14216737281
Iteration 3400: Loss = -9767.110339247196
Iteration 3500: Loss = -9767.070962759026
Iteration 3600: Loss = -9767.02120424084
Iteration 3700: Loss = -9766.955918585212
Iteration 3800: Loss = -9766.876297108045
Iteration 3900: Loss = -9766.806101817492
Iteration 4000: Loss = -9766.77585188801
Iteration 4100: Loss = -9766.75610727312
Iteration 4200: Loss = -9766.746977200883
Iteration 4300: Loss = -9766.741208058267
Iteration 4400: Loss = -9766.740515604608
Iteration 4500: Loss = -9766.740268707677
Iteration 4600: Loss = -9766.740024476716
Iteration 4700: Loss = -9766.73986923805
Iteration 4800: Loss = -9766.739638390856
Iteration 4900: Loss = -9766.7396558206
Iteration 5000: Loss = -9766.73936883164
Iteration 5100: Loss = -9766.739235108364
Iteration 5200: Loss = -9766.739097750802
Iteration 5300: Loss = -9766.738972782241
Iteration 5400: Loss = -9766.73892938253
Iteration 5500: Loss = -9766.738729764977
Iteration 5600: Loss = -9766.74347938962
1
Iteration 5700: Loss = -9766.738534849492
Iteration 5800: Loss = -9766.738471569974
Iteration 5900: Loss = -9766.738356740989
Iteration 6000: Loss = -9766.738279854697
Iteration 6100: Loss = -9766.738763715935
1
Iteration 6200: Loss = -9766.738109762166
Iteration 6300: Loss = -9766.738065088146
Iteration 6400: Loss = -9766.738015062288
Iteration 6500: Loss = -9766.737931139605
Iteration 6600: Loss = -9766.738526145404
1
Iteration 6700: Loss = -9766.737809431947
Iteration 6800: Loss = -9766.738948368416
1
Iteration 6900: Loss = -9766.73772639946
Iteration 7000: Loss = -9766.737674455513
Iteration 7100: Loss = -9766.73767046364
Iteration 7200: Loss = -9766.737601570767
Iteration 7300: Loss = -9766.740444937657
1
Iteration 7400: Loss = -9766.737515631205
Iteration 7500: Loss = -9766.737455417557
Iteration 7600: Loss = -9766.738357700684
1
Iteration 7700: Loss = -9766.737418914407
Iteration 7800: Loss = -9766.73767807268
1
Iteration 7900: Loss = -9766.737361890664
Iteration 8000: Loss = -9766.74064325913
1
Iteration 8100: Loss = -9766.737291752333
Iteration 8200: Loss = -9766.737335549939
Iteration 8300: Loss = -9766.7372459262
Iteration 8400: Loss = -9766.737225889574
Iteration 8500: Loss = -9766.737733050777
1
Iteration 8600: Loss = -9766.737179755908
Iteration 8700: Loss = -9766.739387999662
1
Iteration 8800: Loss = -9766.73713996538
Iteration 8900: Loss = -9766.744679620531
1
Iteration 9000: Loss = -9766.737098689611
Iteration 9100: Loss = -9766.737352860748
1
Iteration 9200: Loss = -9766.821657464883
2
Iteration 9300: Loss = -9766.737078918675
Iteration 9400: Loss = -9766.761876207489
1
Iteration 9500: Loss = -9766.737025583958
Iteration 9600: Loss = -9766.746567844042
1
Iteration 9700: Loss = -9766.737012782793
Iteration 9800: Loss = -9766.737629489378
1
Iteration 9900: Loss = -9766.737008950638
Iteration 10000: Loss = -9766.7369789664
Iteration 10100: Loss = -9766.740303000954
1
Iteration 10200: Loss = -9766.736983984023
Iteration 10300: Loss = -9766.74711645089
1
Iteration 10400: Loss = -9766.737002490887
Iteration 10500: Loss = -9766.74136718612
1
Iteration 10600: Loss = -9766.736934131717
Iteration 10700: Loss = -9766.73826610079
1
Iteration 10800: Loss = -9766.73689908496
Iteration 10900: Loss = -9766.737047838531
1
Iteration 11000: Loss = -9766.770197492884
2
Iteration 11100: Loss = -9766.737324991627
3
Iteration 11200: Loss = -9766.746811913365
4
Iteration 11300: Loss = -9766.736913956896
Iteration 11400: Loss = -9766.739271829823
1
Iteration 11500: Loss = -9766.751057911006
2
Iteration 11600: Loss = -9766.753886182436
3
Iteration 11700: Loss = -9766.785467904461
4
Iteration 11800: Loss = -9766.737574726123
5
Iteration 11900: Loss = -9766.736879424534
Iteration 12000: Loss = -9766.738831850898
1
Iteration 12100: Loss = -9766.73858694167
2
Iteration 12200: Loss = -9766.736825760468
Iteration 12300: Loss = -9766.741617052949
1
Iteration 12400: Loss = -9766.737438057073
2
Iteration 12500: Loss = -9766.736799077624
Iteration 12600: Loss = -9766.737613772126
1
Iteration 12700: Loss = -9766.736792497355
Iteration 12800: Loss = -9766.752395323387
1
Iteration 12900: Loss = -9766.736843239047
Iteration 13000: Loss = -9766.736813778616
Iteration 13100: Loss = -9766.736917026907
1
Iteration 13200: Loss = -9766.736786028803
Iteration 13300: Loss = -9766.926350695874
1
Iteration 13400: Loss = -9766.736842060516
Iteration 13500: Loss = -9766.73680168013
Iteration 13600: Loss = -9766.736787760856
Iteration 13700: Loss = -9766.736804183947
Iteration 13800: Loss = -9766.750678813409
1
Iteration 13900: Loss = -9766.736785420057
Iteration 14000: Loss = -9766.737103599233
1
Iteration 14100: Loss = -9766.736789576576
Iteration 14200: Loss = -9766.736918392451
1
Iteration 14300: Loss = -9766.737294579094
2
Iteration 14400: Loss = -9766.73678677158
Iteration 14500: Loss = -9766.739587043343
1
Iteration 14600: Loss = -9766.736822518718
Iteration 14700: Loss = -9766.791706310998
1
Iteration 14800: Loss = -9766.736793634142
Iteration 14900: Loss = -9766.805843518809
1
Iteration 15000: Loss = -9766.736804697846
Iteration 15100: Loss = -9766.737102085792
1
Iteration 15200: Loss = -9766.737654452452
2
Iteration 15300: Loss = -9766.73675000094
Iteration 15400: Loss = -9766.773773599809
1
Iteration 15500: Loss = -9766.736765995936
Iteration 15600: Loss = -9766.740412880781
1
Iteration 15700: Loss = -9766.737547791903
2
Iteration 15800: Loss = -9766.736790608853
Iteration 15900: Loss = -9766.737629862782
1
Iteration 16000: Loss = -9766.736824753727
Iteration 16100: Loss = -9766.736829999605
Iteration 16200: Loss = -9766.73680546665
Iteration 16300: Loss = -9766.736894266745
Iteration 16400: Loss = -9766.739564209258
1
Iteration 16500: Loss = -9766.736818890871
Iteration 16600: Loss = -9766.73677535314
Iteration 16700: Loss = -9766.736798687778
Iteration 16800: Loss = -9766.739434135914
1
Iteration 16900: Loss = -9766.736735066705
Iteration 17000: Loss = -9766.764945020617
1
Iteration 17100: Loss = -9766.736728494105
Iteration 17200: Loss = -9766.742901968026
1
Iteration 17300: Loss = -9766.736699526404
Iteration 17400: Loss = -9766.750446496417
1
Iteration 17500: Loss = -9766.836188733025
2
Iteration 17600: Loss = -9766.86346939321
3
Iteration 17700: Loss = -9766.737602416035
4
Iteration 17800: Loss = -9766.736771867096
Iteration 17900: Loss = -9766.748957601727
1
Iteration 18000: Loss = -9766.736833584315
Iteration 18100: Loss = -9766.736842517192
Iteration 18200: Loss = -9766.736811658835
Iteration 18300: Loss = -9766.73687335883
Iteration 18400: Loss = -9766.77641140956
1
Iteration 18500: Loss = -9766.736704333325
Iteration 18600: Loss = -9766.739212596438
1
Iteration 18700: Loss = -9766.736751166185
Iteration 18800: Loss = -9766.736754880985
Iteration 18900: Loss = -9766.737265041176
1
Iteration 19000: Loss = -9766.760295643657
2
Iteration 19100: Loss = -9766.80530979207
3
Iteration 19200: Loss = -9766.736747549301
Iteration 19300: Loss = -9766.754385279835
1
Iteration 19400: Loss = -9766.759615124683
2
Iteration 19500: Loss = -9766.736741711584
Iteration 19600: Loss = -9766.737517302008
1
Iteration 19700: Loss = -9766.73681187391
Iteration 19800: Loss = -9766.890404550322
1
Iteration 19900: Loss = -9766.736778505327
pi: tensor([[1.3977e-05, 9.9999e-01],
        [8.5730e-02, 9.1427e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.6965e-05, 9.9995e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1757, 0.1352],
         [0.5574, 0.1319]],

        [[0.5022, 0.1509],
         [0.6760, 0.5148]],

        [[0.7043, 0.1635],
         [0.6485, 0.5868]],

        [[0.6150, 0.1406],
         [0.7215, 0.6531]],

        [[0.5167, 0.1507],
         [0.5046, 0.5837]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23710.313384605655
Iteration 100: Loss = -9769.209291509806
Iteration 200: Loss = -9768.09961441668
Iteration 300: Loss = -9767.702977621275
Iteration 400: Loss = -9767.53265608982
Iteration 500: Loss = -9767.440870838254
Iteration 600: Loss = -9767.383930851784
Iteration 700: Loss = -9767.343955974784
Iteration 800: Loss = -9767.313294873189
Iteration 900: Loss = -9767.288229145264
Iteration 1000: Loss = -9767.26681630875
Iteration 1100: Loss = -9767.247981191476
Iteration 1200: Loss = -9767.23105550711
Iteration 1300: Loss = -9767.215843102511
Iteration 1400: Loss = -9767.201938087088
Iteration 1500: Loss = -9767.189179417133
Iteration 1600: Loss = -9767.177299489267
Iteration 1700: Loss = -9767.166132152044
Iteration 1800: Loss = -9767.155499870843
Iteration 1900: Loss = -9767.145189611369
Iteration 2000: Loss = -9767.135060417082
Iteration 2100: Loss = -9767.124939721498
Iteration 2200: Loss = -9767.114672659287
Iteration 2300: Loss = -9767.103997441094
Iteration 2400: Loss = -9767.092645935907
Iteration 2500: Loss = -9767.080219822183
Iteration 2600: Loss = -9767.066058005394
Iteration 2700: Loss = -9767.048587275853
Iteration 2800: Loss = -9767.023162208507
Iteration 2900: Loss = -9766.972468276119
Iteration 3000: Loss = -9766.839623631182
Iteration 3100: Loss = -9766.644174760257
Iteration 3200: Loss = -9766.531303885848
Iteration 3300: Loss = -9766.47367846
Iteration 3400: Loss = -9766.437869314286
Iteration 3500: Loss = -9766.412495785074
Iteration 3600: Loss = -9766.393459764899
Iteration 3700: Loss = -9766.378716443782
Iteration 3800: Loss = -9766.367160507916
Iteration 3900: Loss = -9766.357904804776
Iteration 4000: Loss = -9766.350471790172
Iteration 4100: Loss = -9766.344370795212
Iteration 4200: Loss = -9766.339259972401
Iteration 4300: Loss = -9766.334809954064
Iteration 4400: Loss = -9766.33083443018
Iteration 4500: Loss = -9766.327207887427
Iteration 4600: Loss = -9766.323737325476
Iteration 4700: Loss = -9766.320315195848
Iteration 4800: Loss = -9766.31683454854
Iteration 4900: Loss = -9766.31337927163
Iteration 5000: Loss = -9766.309993906489
Iteration 5100: Loss = -9766.306901035232
Iteration 5200: Loss = -9766.304218607127
Iteration 5300: Loss = -9766.302076209871
Iteration 5400: Loss = -9766.300511152771
Iteration 5500: Loss = -9766.299227562467
Iteration 5600: Loss = -9766.29811623301
Iteration 5700: Loss = -9766.297160953196
Iteration 5800: Loss = -9766.296308464976
Iteration 5900: Loss = -9766.295520557494
Iteration 6000: Loss = -9766.294697564204
Iteration 6100: Loss = -9766.293995027412
Iteration 6200: Loss = -9766.293375593916
Iteration 6300: Loss = -9766.292675973482
Iteration 6400: Loss = -9766.292043287993
Iteration 6500: Loss = -9766.29156274059
Iteration 6600: Loss = -9766.290891116156
Iteration 6700: Loss = -9766.290381726401
Iteration 6800: Loss = -9766.289886988367
Iteration 6900: Loss = -9766.289414684881
Iteration 7000: Loss = -9766.289794476197
1
Iteration 7100: Loss = -9766.28846321052
Iteration 7200: Loss = -9766.288051076606
Iteration 7300: Loss = -9766.287631908543
Iteration 7400: Loss = -9766.287225884584
Iteration 7500: Loss = -9766.286819092385
Iteration 7600: Loss = -9766.286408924629
Iteration 7700: Loss = -9766.286018147533
Iteration 7800: Loss = -9766.28565680215
Iteration 7900: Loss = -9766.285263853317
Iteration 8000: Loss = -9766.285550174778
1
Iteration 8100: Loss = -9766.284357818175
Iteration 8200: Loss = -9766.283874746632
Iteration 8300: Loss = -9766.283013957058
Iteration 8400: Loss = -9766.282663087919
Iteration 8500: Loss = -9766.279429081487
Iteration 8600: Loss = -9766.272220660816
Iteration 8700: Loss = -9766.262425570108
Iteration 8800: Loss = -9766.271794036715
1
Iteration 8900: Loss = -9766.220249217098
Iteration 9000: Loss = -9766.222436771519
1
Iteration 9100: Loss = -9766.209152545869
Iteration 9200: Loss = -9766.211519080083
1
Iteration 9300: Loss = -9766.205462115799
Iteration 9400: Loss = -9766.204889386952
Iteration 9500: Loss = -9766.204286748398
Iteration 9600: Loss = -9766.203873602453
Iteration 9700: Loss = -9766.20358940135
Iteration 9800: Loss = -9766.203882403957
1
Iteration 9900: Loss = -9766.208766344624
2
Iteration 10000: Loss = -9766.203231943506
Iteration 10100: Loss = -9766.224410616334
1
Iteration 10200: Loss = -9766.202894636519
Iteration 10300: Loss = -9766.204651110098
1
Iteration 10400: Loss = -9766.202763909247
Iteration 10500: Loss = -9766.342111615762
1
Iteration 10600: Loss = -9766.202689016882
Iteration 10700: Loss = -9766.206046802228
1
Iteration 10800: Loss = -9766.202727218893
Iteration 10900: Loss = -9766.215163690655
1
Iteration 11000: Loss = -9766.204095007606
2
Iteration 11100: Loss = -9766.202570786929
Iteration 11200: Loss = -9766.202556214808
Iteration 11300: Loss = -9766.209940699286
1
Iteration 11400: Loss = -9766.202788620125
2
Iteration 11500: Loss = -9766.202507988555
Iteration 11600: Loss = -9766.376925229
1
Iteration 11700: Loss = -9766.202439727893
Iteration 11800: Loss = -9766.519420032317
1
Iteration 11900: Loss = -9766.210316591
2
Iteration 12000: Loss = -9766.20237798018
Iteration 12100: Loss = -9766.202523372718
1
Iteration 12200: Loss = -9766.202315395352
Iteration 12300: Loss = -9766.2023596287
Iteration 12400: Loss = -9766.202305469575
Iteration 12500: Loss = -9766.21696571915
1
Iteration 12600: Loss = -9766.202510611338
2
Iteration 12700: Loss = -9766.20225570622
Iteration 12800: Loss = -9766.204339690921
1
Iteration 12900: Loss = -9766.206790573455
2
Iteration 13000: Loss = -9766.202182836063
Iteration 13100: Loss = -9766.20229041789
1
Iteration 13200: Loss = -9766.202742166559
2
Iteration 13300: Loss = -9766.202351442593
3
Iteration 13400: Loss = -9766.202201981561
Iteration 13500: Loss = -9766.206579676013
1
Iteration 13600: Loss = -9766.20220447022
Iteration 13700: Loss = -9766.20215896179
Iteration 13800: Loss = -9766.202288287137
1
Iteration 13900: Loss = -9766.221544949953
2
Iteration 14000: Loss = -9766.202501106676
3
Iteration 14100: Loss = -9766.202099778931
Iteration 14200: Loss = -9766.202116007376
Iteration 14300: Loss = -9766.202102594076
Iteration 14400: Loss = -9766.209347104039
1
Iteration 14500: Loss = -9766.207817204919
2
Iteration 14600: Loss = -9766.202874447343
3
Iteration 14700: Loss = -9766.202810234394
4
Iteration 14800: Loss = -9766.20226325751
5
Iteration 14900: Loss = -9766.20249907833
6
Iteration 15000: Loss = -9766.202580138257
7
Iteration 15100: Loss = -9766.202725677025
8
Iteration 15200: Loss = -9766.202574804607
9
Iteration 15300: Loss = -9766.202041126378
Iteration 15400: Loss = -9766.202061505772
Iteration 15500: Loss = -9766.202322194293
1
Iteration 15600: Loss = -9766.209279447059
2
Iteration 15700: Loss = -9766.205115554045
3
Iteration 15800: Loss = -9766.201986629327
Iteration 15900: Loss = -9766.20586451807
1
Iteration 16000: Loss = -9766.202124383331
2
Iteration 16100: Loss = -9766.20397761128
3
Iteration 16200: Loss = -9766.201994798432
Iteration 16300: Loss = -9766.203333300418
1
Iteration 16400: Loss = -9766.229985753549
2
Iteration 16500: Loss = -9766.204938977842
3
Iteration 16600: Loss = -9766.202088854525
Iteration 16700: Loss = -9766.213980458731
1
Iteration 16800: Loss = -9766.20199042323
Iteration 16900: Loss = -9766.204646988917
1
Iteration 17000: Loss = -9766.201991030983
Iteration 17100: Loss = -9766.20204067534
Iteration 17200: Loss = -9766.205461177284
1
Iteration 17300: Loss = -9766.205306690597
2
Iteration 17400: Loss = -9766.247377903699
3
Iteration 17500: Loss = -9766.209439143497
4
Iteration 17600: Loss = -9766.212507785182
5
Iteration 17700: Loss = -9766.202471344846
6
Iteration 17800: Loss = -9766.283978959964
7
Iteration 17900: Loss = -9766.204227454487
8
Iteration 18000: Loss = -9766.2072372253
9
Iteration 18100: Loss = -9766.203355710659
10
Iteration 18200: Loss = -9766.230792066015
11
Iteration 18300: Loss = -9766.20448153045
12
Iteration 18400: Loss = -9766.202117259387
Iteration 18500: Loss = -9766.202058334373
Iteration 18600: Loss = -9766.296214521337
1
Iteration 18700: Loss = -9766.201971465971
Iteration 18800: Loss = -9766.202083778664
1
Iteration 18900: Loss = -9766.202031330125
Iteration 19000: Loss = -9766.282679434305
1
Iteration 19100: Loss = -9766.20195703149
Iteration 19200: Loss = -9766.201983114337
Iteration 19300: Loss = -9766.20278529902
1
Iteration 19400: Loss = -9766.202096055533
2
Iteration 19500: Loss = -9766.360214273842
3
Iteration 19600: Loss = -9766.201949792376
Iteration 19700: Loss = -9766.202065121297
1
Iteration 19800: Loss = -9766.204993991068
2
Iteration 19900: Loss = -9766.202236105002
3
pi: tensor([[9.7937e-01, 2.0632e-02],
        [9.9999e-01, 6.8258e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2869, 0.7131], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1351, 0.1330],
         [0.5874, 0.1314]],

        [[0.5798, 0.1505],
         [0.5907, 0.6550]],

        [[0.5224, 0.1749],
         [0.6152, 0.7117]],

        [[0.7185, 0.0594],
         [0.6413, 0.6283]],

        [[0.7060, 0.1391],
         [0.6429, 0.6080]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 61
Adjusted Rand Index: -0.007726325420627255
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0017702829747583116
Average Adjusted Rand Index: -0.001545265084125451
9909.493893091018
[0.0, 0.0017702829747583116] [0.0, -0.001545265084125451] [9766.737513642354, 9766.202340933984]
-------------------------------------
This iteration is 3
True Objective function: Loss = -9887.844520076182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24670.366434130417
Iteration 100: Loss = -9728.43915857619
Iteration 200: Loss = -9727.536947787701
Iteration 300: Loss = -9727.218149726727
Iteration 400: Loss = -9727.068175789816
Iteration 500: Loss = -9726.959976879796
Iteration 600: Loss = -9726.87821732227
Iteration 700: Loss = -9726.818738136493
Iteration 800: Loss = -9726.769823861518
Iteration 900: Loss = -9726.729404270309
Iteration 1000: Loss = -9726.69934769311
Iteration 1100: Loss = -9726.67382993111
Iteration 1200: Loss = -9726.648723545653
Iteration 1300: Loss = -9726.623258889476
Iteration 1400: Loss = -9726.59541638913
Iteration 1500: Loss = -9726.565552404756
Iteration 1600: Loss = -9726.535340404642
Iteration 1700: Loss = -9726.50754581488
Iteration 1800: Loss = -9726.484483540415
Iteration 1900: Loss = -9726.466887897246
Iteration 2000: Loss = -9726.454095580522
Iteration 2100: Loss = -9726.444954723647
Iteration 2200: Loss = -9726.438385358557
Iteration 2300: Loss = -9726.43356026958
Iteration 2400: Loss = -9726.429857600704
Iteration 2500: Loss = -9726.426938140447
Iteration 2600: Loss = -9726.424629512658
Iteration 2700: Loss = -9726.42268781459
Iteration 2800: Loss = -9726.42102711418
Iteration 2900: Loss = -9726.419567526193
Iteration 3000: Loss = -9726.418292845141
Iteration 3100: Loss = -9726.41720519734
Iteration 3200: Loss = -9726.416171489489
Iteration 3300: Loss = -9726.415212996448
Iteration 3400: Loss = -9726.414290323766
Iteration 3500: Loss = -9726.41340054915
Iteration 3600: Loss = -9726.412447927232
Iteration 3700: Loss = -9726.411454912824
Iteration 3800: Loss = -9726.41029148745
Iteration 3900: Loss = -9726.408836817463
Iteration 4000: Loss = -9726.406860135992
Iteration 4100: Loss = -9726.403846647521
Iteration 4200: Loss = -9726.399184515863
Iteration 4300: Loss = -9726.392280735483
Iteration 4400: Loss = -9726.383956202455
Iteration 4500: Loss = -9726.376386752054
Iteration 4600: Loss = -9726.371368537939
Iteration 4700: Loss = -9726.368589762316
Iteration 4800: Loss = -9726.367241257867
Iteration 4900: Loss = -9726.36636382927
Iteration 5000: Loss = -9726.365918556932
Iteration 5100: Loss = -9726.365546715859
Iteration 5200: Loss = -9726.365279370508
Iteration 5300: Loss = -9726.365072401082
Iteration 5400: Loss = -9726.364881385793
Iteration 5500: Loss = -9726.364776592969
Iteration 5600: Loss = -9726.364675186933
Iteration 5700: Loss = -9726.364951083233
1
Iteration 5800: Loss = -9726.364424733929
Iteration 5900: Loss = -9726.369715617859
1
Iteration 6000: Loss = -9726.364297347498
Iteration 6100: Loss = -9726.364226842257
Iteration 6200: Loss = -9726.364217911705
Iteration 6300: Loss = -9726.36412991091
Iteration 6400: Loss = -9726.365261696226
1
Iteration 6500: Loss = -9726.364018148155
Iteration 6600: Loss = -9726.36406623708
Iteration 6700: Loss = -9726.363958444797
Iteration 6800: Loss = -9726.363930703841
Iteration 6900: Loss = -9726.363866823594
Iteration 7000: Loss = -9726.364930311624
1
Iteration 7100: Loss = -9726.363834974974
Iteration 7200: Loss = -9726.363767632822
Iteration 7300: Loss = -9726.363857339686
Iteration 7400: Loss = -9726.36372679149
Iteration 7500: Loss = -9726.36372503602
Iteration 7600: Loss = -9726.363645990627
Iteration 7700: Loss = -9726.3638015386
1
Iteration 7800: Loss = -9726.363637376755
Iteration 7900: Loss = -9726.364025376704
1
Iteration 8000: Loss = -9726.363608865473
Iteration 8100: Loss = -9726.36360298845
Iteration 8200: Loss = -9726.363597938494
Iteration 8300: Loss = -9726.363577477188
Iteration 8400: Loss = -9726.363570961978
Iteration 8500: Loss = -9726.363519247743
Iteration 8600: Loss = -9726.36853210012
1
Iteration 8700: Loss = -9726.363554637514
Iteration 8800: Loss = -9726.363638896872
Iteration 8900: Loss = -9726.366292098215
1
Iteration 9000: Loss = -9726.363478851763
Iteration 9100: Loss = -9726.373955041707
1
Iteration 9200: Loss = -9726.363470873075
Iteration 9300: Loss = -9726.364271504262
1
Iteration 9400: Loss = -9726.363461949253
Iteration 9500: Loss = -9726.363423841143
Iteration 9600: Loss = -9726.394904471581
1
Iteration 9700: Loss = -9726.363420398375
Iteration 9800: Loss = -9726.36605084174
1
Iteration 9900: Loss = -9726.36341090945
Iteration 10000: Loss = -9726.38039309227
1
Iteration 10100: Loss = -9726.363434770628
Iteration 10200: Loss = -9726.363401382623
Iteration 10300: Loss = -9726.389339119907
1
Iteration 10400: Loss = -9726.363393342368
Iteration 10500: Loss = -9726.366473469188
1
Iteration 10600: Loss = -9726.363413180477
Iteration 10700: Loss = -9726.369762514105
1
Iteration 10800: Loss = -9726.363374006814
Iteration 10900: Loss = -9726.363367294358
Iteration 11000: Loss = -9726.363678245278
1
Iteration 11100: Loss = -9726.363988934172
2
Iteration 11200: Loss = -9726.388578983502
3
Iteration 11300: Loss = -9726.36342435113
Iteration 11400: Loss = -9726.384554819588
1
Iteration 11500: Loss = -9726.363465752926
Iteration 11600: Loss = -9726.363424405
Iteration 11700: Loss = -9726.363465595748
Iteration 11800: Loss = -9726.363337265238
Iteration 11900: Loss = -9726.363380421106
Iteration 12000: Loss = -9726.363455499719
Iteration 12100: Loss = -9726.363433579656
Iteration 12200: Loss = -9726.36373319146
1
Iteration 12300: Loss = -9726.478477762405
2
Iteration 12400: Loss = -9726.363912869258
3
Iteration 12500: Loss = -9726.363495787336
Iteration 12600: Loss = -9726.368404677702
1
Iteration 12700: Loss = -9726.389004711487
2
Iteration 12800: Loss = -9726.363344511117
Iteration 12900: Loss = -9726.374574085125
1
Iteration 13000: Loss = -9726.363860631667
2
Iteration 13100: Loss = -9726.393772924404
3
Iteration 13200: Loss = -9726.364872520973
4
Iteration 13300: Loss = -9726.363333094148
Iteration 13400: Loss = -9726.364337959098
1
Iteration 13500: Loss = -9726.36331189376
Iteration 13600: Loss = -9726.363388276595
Iteration 13700: Loss = -9726.363798870247
1
Iteration 13800: Loss = -9726.363323334617
Iteration 13900: Loss = -9726.38046806421
1
Iteration 14000: Loss = -9726.36329613831
Iteration 14100: Loss = -9726.36475722745
1
Iteration 14200: Loss = -9726.36331520841
Iteration 14300: Loss = -9726.36388960864
1
Iteration 14400: Loss = -9726.363282999799
Iteration 14500: Loss = -9726.363430919264
1
Iteration 14600: Loss = -9726.363339232044
Iteration 14700: Loss = -9726.367606788734
1
Iteration 14800: Loss = -9726.36562241918
2
Iteration 14900: Loss = -9726.400177456011
3
Iteration 15000: Loss = -9726.363533343298
4
Iteration 15100: Loss = -9726.36931370118
5
Iteration 15200: Loss = -9726.36516032887
6
Iteration 15300: Loss = -9726.367885379936
7
Iteration 15400: Loss = -9726.372752738744
8
Iteration 15500: Loss = -9726.363300728979
Iteration 15600: Loss = -9726.364001253036
1
Iteration 15700: Loss = -9726.363301075655
Iteration 15800: Loss = -9726.369059186407
1
Iteration 15900: Loss = -9726.363297604023
Iteration 16000: Loss = -9726.364028896698
1
Iteration 16100: Loss = -9726.364023033413
2
Iteration 16200: Loss = -9726.363523255688
3
Iteration 16300: Loss = -9726.363337499542
Iteration 16400: Loss = -9726.364045153407
1
Iteration 16500: Loss = -9726.363343412404
Iteration 16600: Loss = -9726.36337119528
Iteration 16700: Loss = -9726.363972234098
1
Iteration 16800: Loss = -9726.374807243916
2
Iteration 16900: Loss = -9726.372510396173
3
Iteration 17000: Loss = -9726.363305229203
Iteration 17100: Loss = -9726.372975065606
1
Iteration 17200: Loss = -9726.363296253547
Iteration 17300: Loss = -9726.446640181957
1
Iteration 17400: Loss = -9726.363260366852
Iteration 17500: Loss = -9726.363303740096
Iteration 17600: Loss = -9726.363896556844
1
Iteration 17700: Loss = -9726.363279155265
Iteration 17800: Loss = -9726.368883321058
1
Iteration 17900: Loss = -9726.36342076157
2
Iteration 18000: Loss = -9726.365070477072
3
Iteration 18100: Loss = -9726.363317741929
Iteration 18200: Loss = -9726.37809348516
1
Iteration 18300: Loss = -9726.363301031375
Iteration 18400: Loss = -9726.363640181047
1
Iteration 18500: Loss = -9726.369571942989
2
Iteration 18600: Loss = -9726.363356230155
Iteration 18700: Loss = -9726.370756437726
1
Iteration 18800: Loss = -9726.363319008824
Iteration 18900: Loss = -9726.363589514014
1
Iteration 19000: Loss = -9726.363300045277
Iteration 19100: Loss = -9726.365980114257
1
Iteration 19200: Loss = -9726.431753853058
2
Iteration 19300: Loss = -9726.363306789757
Iteration 19400: Loss = -9726.363567936867
1
Iteration 19500: Loss = -9726.363304765937
Iteration 19600: Loss = -9726.36358710256
1
Iteration 19700: Loss = -9726.373057770916
2
Iteration 19800: Loss = -9726.363275568252
Iteration 19900: Loss = -9726.36411006999
1
pi: tensor([[5.2831e-08, 1.0000e+00],
        [8.2913e-03, 9.9171e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0818, 0.9182], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2283, 0.1759],
         [0.7041, 0.1329]],

        [[0.6823, 0.0546],
         [0.5216, 0.6923]],

        [[0.5116, 0.0529],
         [0.5639, 0.5530]],

        [[0.7003, 0.1582],
         [0.6673, 0.5990]],

        [[0.6715, 0.1525],
         [0.5929, 0.6074]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005925715997178844
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21951.180148182597
Iteration 100: Loss = -9729.98000307305
Iteration 200: Loss = -9728.811716753335
Iteration 300: Loss = -9728.397351883632
Iteration 400: Loss = -9728.156684954954
Iteration 500: Loss = -9727.85418461491
Iteration 600: Loss = -9727.298505200415
Iteration 700: Loss = -9727.116274989923
Iteration 800: Loss = -9727.026806010823
Iteration 900: Loss = -9726.961948864104
Iteration 1000: Loss = -9726.91216602561
Iteration 1100: Loss = -9726.87339863498
Iteration 1200: Loss = -9726.843173538988
Iteration 1300: Loss = -9726.81976021417
Iteration 1400: Loss = -9726.80166668729
Iteration 1500: Loss = -9726.787687300606
Iteration 1600: Loss = -9726.776780910543
Iteration 1700: Loss = -9726.768233439654
Iteration 1800: Loss = -9726.761482915472
Iteration 1900: Loss = -9726.756044817947
Iteration 2000: Loss = -9726.751710582885
Iteration 2100: Loss = -9726.748173778667
Iteration 2200: Loss = -9726.745273197985
Iteration 2300: Loss = -9726.742918586273
Iteration 2400: Loss = -9726.74089858648
Iteration 2500: Loss = -9726.739242122234
Iteration 2600: Loss = -9726.737739598944
Iteration 2700: Loss = -9726.736403815883
Iteration 2800: Loss = -9726.735106019203
Iteration 2900: Loss = -9726.733769893148
Iteration 3000: Loss = -9726.732246251613
Iteration 3100: Loss = -9726.730403672325
Iteration 3200: Loss = -9726.727945144588
Iteration 3300: Loss = -9726.724364328882
Iteration 3400: Loss = -9726.718382988653
Iteration 3500: Loss = -9726.705787101717
Iteration 3600: Loss = -9726.658150463822
Iteration 3700: Loss = -9725.972999401465
Iteration 3800: Loss = -9725.431387168906
Iteration 3900: Loss = -9725.263509308012
Iteration 4000: Loss = -9725.180225249147
Iteration 4100: Loss = -9725.131727770036
Iteration 4200: Loss = -9725.100264013881
Iteration 4300: Loss = -9725.079025294219
Iteration 4400: Loss = -9725.06375221192
Iteration 4500: Loss = -9725.052268527554
Iteration 4600: Loss = -9725.043245787705
Iteration 4700: Loss = -9725.035972505832
Iteration 4800: Loss = -9725.030021163257
Iteration 4900: Loss = -9725.025169340506
Iteration 5000: Loss = -9725.021093002555
Iteration 5100: Loss = -9725.017576058559
Iteration 5200: Loss = -9725.014523651198
Iteration 5300: Loss = -9725.011844739822
Iteration 5400: Loss = -9725.009492657506
Iteration 5500: Loss = -9725.007411213435
Iteration 5600: Loss = -9725.005510700423
Iteration 5700: Loss = -9725.004261234726
Iteration 5800: Loss = -9725.00223216443
Iteration 5900: Loss = -9725.002013728159
Iteration 6000: Loss = -9724.999496909953
Iteration 6100: Loss = -9724.998345842401
Iteration 6200: Loss = -9724.997324987758
Iteration 6300: Loss = -9724.996308900845
Iteration 6400: Loss = -9724.99620916123
Iteration 6500: Loss = -9724.99470124345
Iteration 6600: Loss = -9724.993962005301
Iteration 6700: Loss = -9724.993351880024
Iteration 6800: Loss = -9724.9927448101
Iteration 6900: Loss = -9724.992173262403
Iteration 7000: Loss = -9724.991767170504
Iteration 7100: Loss = -9724.991246681217
Iteration 7200: Loss = -9724.990792034036
Iteration 7300: Loss = -9724.990361050526
Iteration 7400: Loss = -9724.989979553473
Iteration 7500: Loss = -9724.989606002591
Iteration 7600: Loss = -9724.98921919185
Iteration 7700: Loss = -9724.99136174508
1
Iteration 7800: Loss = -9724.988570606074
Iteration 7900: Loss = -9724.988296517162
Iteration 8000: Loss = -9724.98806152003
Iteration 8100: Loss = -9724.987848810717
Iteration 8200: Loss = -9724.987609339034
Iteration 8300: Loss = -9724.989150695428
1
Iteration 8400: Loss = -9725.00772811792
2
Iteration 8500: Loss = -9725.019775972358
3
Iteration 8600: Loss = -9724.986937910251
Iteration 8700: Loss = -9724.986671532122
Iteration 8800: Loss = -9724.987417259721
1
Iteration 8900: Loss = -9724.98640181459
Iteration 9000: Loss = -9724.986270027886
Iteration 9100: Loss = -9724.987847876348
1
Iteration 9200: Loss = -9724.986022814366
Iteration 9300: Loss = -9724.985891299293
Iteration 9400: Loss = -9724.985972966884
Iteration 9500: Loss = -9724.98571530896
Iteration 9600: Loss = -9724.986231333862
1
Iteration 9700: Loss = -9724.98562502443
Iteration 9800: Loss = -9724.98545780634
Iteration 9900: Loss = -9724.985651044284
1
Iteration 10000: Loss = -9724.985328531124
Iteration 10100: Loss = -9724.985267335507
Iteration 10200: Loss = -9724.98521084581
Iteration 10300: Loss = -9724.985111659835
Iteration 10400: Loss = -9725.002157068726
1
Iteration 10500: Loss = -9724.984989641349
Iteration 10600: Loss = -9724.984976621532
Iteration 10700: Loss = -9724.988670436416
1
Iteration 10800: Loss = -9724.984864830345
Iteration 10900: Loss = -9724.98715891829
1
Iteration 11000: Loss = -9724.984774124596
Iteration 11100: Loss = -9724.986069594375
1
Iteration 11200: Loss = -9724.984705338959
Iteration 11300: Loss = -9724.984658904139
Iteration 11400: Loss = -9725.334048016352
1
Iteration 11500: Loss = -9724.984620224966
Iteration 11600: Loss = -9724.984538933291
Iteration 11700: Loss = -9724.988301337738
1
Iteration 11800: Loss = -9724.984510153852
Iteration 11900: Loss = -9724.98447843925
Iteration 12000: Loss = -9724.98469451702
1
Iteration 12100: Loss = -9724.984420902418
Iteration 12200: Loss = -9725.054060530882
1
Iteration 12300: Loss = -9724.994009812886
2
Iteration 12400: Loss = -9725.116873282906
3
Iteration 12500: Loss = -9724.984327284024
Iteration 12600: Loss = -9724.984907678287
1
Iteration 12700: Loss = -9724.98430606021
Iteration 12800: Loss = -9724.984332077216
Iteration 12900: Loss = -9725.137417236652
1
Iteration 13000: Loss = -9724.984279609565
Iteration 13100: Loss = -9724.998603615242
1
Iteration 13200: Loss = -9724.98424627959
Iteration 13300: Loss = -9724.999892198406
1
Iteration 13400: Loss = -9724.988522621412
2
Iteration 13500: Loss = -9724.984776393041
3
Iteration 13600: Loss = -9724.98419098384
Iteration 13700: Loss = -9724.984492548703
1
Iteration 13800: Loss = -9724.985596428407
2
Iteration 13900: Loss = -9724.99667665664
3
Iteration 14000: Loss = -9724.984160734082
Iteration 14100: Loss = -9724.987160952856
1
Iteration 14200: Loss = -9724.984295902565
2
Iteration 14300: Loss = -9724.995027175399
3
Iteration 14400: Loss = -9725.050992972052
4
Iteration 14500: Loss = -9724.98541020803
5
Iteration 14600: Loss = -9724.984489125916
6
Iteration 14700: Loss = -9724.99425425874
7
Iteration 14800: Loss = -9724.984092030218
Iteration 14900: Loss = -9724.984290181308
1
Iteration 15000: Loss = -9724.985487885806
2
Iteration 15100: Loss = -9724.984186245209
Iteration 15200: Loss = -9725.196463704606
1
Iteration 15300: Loss = -9724.984060300909
Iteration 15400: Loss = -9724.992105323385
1
Iteration 15500: Loss = -9724.984054468074
Iteration 15600: Loss = -9724.993320470532
1
Iteration 15700: Loss = -9724.984117660459
Iteration 15800: Loss = -9724.984457687395
1
Iteration 15900: Loss = -9724.988039890626
2
Iteration 16000: Loss = -9724.998502802133
3
Iteration 16100: Loss = -9724.984022115912
Iteration 16200: Loss = -9724.984870049395
1
Iteration 16300: Loss = -9725.049986860933
2
Iteration 16400: Loss = -9724.983991050885
Iteration 16500: Loss = -9724.984649853073
1
Iteration 16600: Loss = -9724.992005013555
2
Iteration 16700: Loss = -9724.984369012636
3
Iteration 16800: Loss = -9724.98922032212
4
Iteration 16900: Loss = -9724.984001819575
Iteration 17000: Loss = -9724.984060674773
Iteration 17100: Loss = -9724.99579463687
1
Iteration 17200: Loss = -9724.984034571677
Iteration 17300: Loss = -9724.990044510558
1
Iteration 17400: Loss = -9724.984549098666
2
Iteration 17500: Loss = -9724.98407669285
Iteration 17600: Loss = -9724.988128329584
1
Iteration 17700: Loss = -9724.984795864315
2
Iteration 17800: Loss = -9724.99080689596
3
Iteration 17900: Loss = -9725.140127245786
4
Iteration 18000: Loss = -9724.983992884287
Iteration 18100: Loss = -9724.985222955947
1
Iteration 18200: Loss = -9724.98737976736
2
Iteration 18300: Loss = -9725.001077375135
3
Iteration 18400: Loss = -9724.984075275486
Iteration 18500: Loss = -9724.983964641724
Iteration 18600: Loss = -9724.987699201862
1
Iteration 18700: Loss = -9724.9987879662
2
Iteration 18800: Loss = -9724.985667937552
3
Iteration 18900: Loss = -9724.98401033462
Iteration 19000: Loss = -9724.987951847348
1
Iteration 19100: Loss = -9724.989438324506
2
Iteration 19200: Loss = -9724.984008576106
Iteration 19300: Loss = -9725.08183113723
1
Iteration 19400: Loss = -9724.984100759415
Iteration 19500: Loss = -9725.01894057095
1
Iteration 19600: Loss = -9724.984008618067
Iteration 19700: Loss = -9724.985490759254
1
Iteration 19800: Loss = -9724.987383934149
2
Iteration 19900: Loss = -9724.984016821634
pi: tensor([[9.1805e-01, 8.1949e-02],
        [1.2087e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0598, 0.9402], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1984, 0.1828],
         [0.7114, 0.1310]],

        [[0.6107, 0.1544],
         [0.5317, 0.6685]],

        [[0.6027, 0.1035],
         [0.7167, 0.5937]],

        [[0.7212, 0.1831],
         [0.6672, 0.6006]],

        [[0.6323, 0.1541],
         [0.7213, 0.6490]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.00485601903559462
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.02868762140155424
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
Global Adjusted Rand Index: 0.010592274453531543
Average Adjusted Rand Index: 0.010702951624142174
9887.844520076182
[-0.0005925715997178844, 0.010592274453531543] [0.0, 0.010702951624142174] [9726.381591903582, 9724.9841489816]
-------------------------------------
This iteration is 4
True Objective function: Loss = -9823.019673668823
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23510.81474505088
Iteration 100: Loss = -9718.829650487227
Iteration 200: Loss = -9717.601666108894
Iteration 300: Loss = -9717.189085415052
Iteration 400: Loss = -9717.026550127512
Iteration 500: Loss = -9716.943665726074
Iteration 600: Loss = -9716.893656079852
Iteration 700: Loss = -9716.859767851793
Iteration 800: Loss = -9716.834792406566
Iteration 900: Loss = -9716.814766996498
Iteration 1000: Loss = -9716.79765959215
Iteration 1100: Loss = -9716.782248603362
Iteration 1200: Loss = -9716.767726652712
Iteration 1300: Loss = -9716.753548253593
Iteration 1400: Loss = -9716.739249519345
Iteration 1500: Loss = -9716.724284128944
Iteration 1600: Loss = -9716.708197367016
Iteration 1700: Loss = -9716.690617096005
Iteration 1800: Loss = -9716.67108202965
Iteration 1900: Loss = -9716.649335036851
Iteration 2000: Loss = -9716.625068310645
Iteration 2100: Loss = -9716.59807283793
Iteration 2200: Loss = -9716.56838081144
Iteration 2300: Loss = -9716.536150432887
Iteration 2400: Loss = -9716.501303205705
Iteration 2500: Loss = -9716.463223787057
Iteration 2600: Loss = -9716.420915182993
Iteration 2700: Loss = -9716.373827506717
Iteration 2800: Loss = -9716.322445609498
Iteration 2900: Loss = -9716.268370921316
Iteration 3000: Loss = -9716.214764007418
Iteration 3100: Loss = -9716.165624382964
Iteration 3200: Loss = -9716.12462991487
Iteration 3300: Loss = -9716.093560828045
Iteration 3400: Loss = -9716.071868215811
Iteration 3500: Loss = -9716.057517895619
Iteration 3600: Loss = -9716.047793371139
Iteration 3700: Loss = -9716.040810730081
Iteration 3800: Loss = -9716.035223519792
Iteration 3900: Loss = -9716.030185246673
Iteration 4000: Loss = -9716.025395536268
Iteration 4100: Loss = -9716.02031121481
Iteration 4200: Loss = -9716.015191829872
Iteration 4300: Loss = -9716.009609029432
Iteration 4400: Loss = -9716.004703400704
Iteration 4500: Loss = -9715.997594095765
Iteration 4600: Loss = -9715.98948044276
Iteration 4700: Loss = -9715.977198412556
Iteration 4800: Loss = -9715.954358785984
Iteration 4900: Loss = -9715.902071702498
Iteration 5000: Loss = -9715.770092665054
Iteration 5100: Loss = -9715.548584706146
Iteration 5200: Loss = -9715.376438875017
Iteration 5300: Loss = -9715.288821552602
Iteration 5400: Loss = -9715.245956297656
Iteration 5500: Loss = -9715.222730146888
Iteration 5600: Loss = -9715.208722657924
Iteration 5700: Loss = -9715.199798109606
Iteration 5800: Loss = -9715.193377896869
Iteration 5900: Loss = -9715.188718696152
Iteration 6000: Loss = -9715.185143569595
Iteration 6100: Loss = -9715.18232582866
Iteration 6200: Loss = -9715.180103875215
Iteration 6300: Loss = -9715.17820491409
Iteration 6400: Loss = -9715.178354900398
1
Iteration 6500: Loss = -9715.175367033693
Iteration 6600: Loss = -9715.174222117645
Iteration 6700: Loss = -9715.17323471391
Iteration 6800: Loss = -9715.172373152558
Iteration 6900: Loss = -9715.17167320563
Iteration 7000: Loss = -9715.170973492903
Iteration 7100: Loss = -9715.171837770631
1
Iteration 7200: Loss = -9715.169872115557
Iteration 7300: Loss = -9715.176034054843
1
Iteration 7400: Loss = -9715.168963433069
Iteration 7500: Loss = -9715.169218764393
1
Iteration 7600: Loss = -9715.168194284532
Iteration 7700: Loss = -9715.167879734217
Iteration 7800: Loss = -9715.167626526296
Iteration 7900: Loss = -9715.167359154126
Iteration 8000: Loss = -9715.167178145843
Iteration 8100: Loss = -9715.16685020162
Iteration 8200: Loss = -9715.166697323619
Iteration 8300: Loss = -9715.16647114206
Iteration 8400: Loss = -9715.169342650763
1
Iteration 8500: Loss = -9715.166454652566
Iteration 8600: Loss = -9715.166023034759
Iteration 8700: Loss = -9715.166777242532
1
Iteration 8800: Loss = -9715.184623816143
2
Iteration 8900: Loss = -9715.165542725983
Iteration 9000: Loss = -9715.165494557063
Iteration 9100: Loss = -9715.17826302418
1
Iteration 9200: Loss = -9715.165214232768
Iteration 9300: Loss = -9715.166181588163
1
Iteration 9400: Loss = -9715.165051599706
Iteration 9500: Loss = -9715.165297075098
1
Iteration 9600: Loss = -9715.164869114264
Iteration 9700: Loss = -9715.165675925962
1
Iteration 9800: Loss = -9715.164715675333
Iteration 9900: Loss = -9715.16464028384
Iteration 10000: Loss = -9715.16515013981
1
Iteration 10100: Loss = -9715.164658512685
Iteration 10200: Loss = -9715.164496650676
Iteration 10300: Loss = -9715.169634758824
1
Iteration 10400: Loss = -9715.164386706912
Iteration 10500: Loss = -9715.1691250584
1
Iteration 10600: Loss = -9715.164300473436
Iteration 10700: Loss = -9715.164602643208
1
Iteration 10800: Loss = -9715.16420135751
Iteration 10900: Loss = -9715.164258585337
Iteration 11000: Loss = -9715.166617082014
1
Iteration 11100: Loss = -9715.164121162892
Iteration 11200: Loss = -9715.16501988025
1
Iteration 11300: Loss = -9715.182000132769
2
Iteration 11400: Loss = -9715.16717578241
3
Iteration 11500: Loss = -9715.164085514562
Iteration 11600: Loss = -9715.164119128896
Iteration 11700: Loss = -9715.16419792215
Iteration 11800: Loss = -9715.168735997404
1
Iteration 11900: Loss = -9715.175847385148
2
Iteration 12000: Loss = -9715.176184248156
3
Iteration 12100: Loss = -9715.173118737248
4
Iteration 12200: Loss = -9715.163917009615
Iteration 12300: Loss = -9715.164900332618
1
Iteration 12400: Loss = -9715.197104002089
2
Iteration 12500: Loss = -9715.206342774338
3
Iteration 12600: Loss = -9715.164321799175
4
Iteration 12700: Loss = -9715.164268703222
5
Iteration 12800: Loss = -9715.166554725374
6
Iteration 12900: Loss = -9715.171276634554
7
Iteration 13000: Loss = -9715.163758177494
Iteration 13100: Loss = -9715.166398328629
1
Iteration 13200: Loss = -9715.26665592818
2
Iteration 13300: Loss = -9715.164487660242
3
Iteration 13400: Loss = -9715.241921086415
4
Iteration 13500: Loss = -9715.163716877321
Iteration 13600: Loss = -9715.171789889835
1
Iteration 13700: Loss = -9715.168054548627
2
Iteration 13800: Loss = -9715.19689517633
3
Iteration 13900: Loss = -9715.186925195785
4
Iteration 14000: Loss = -9715.17782553475
5
Iteration 14100: Loss = -9715.179036069097
6
Iteration 14200: Loss = -9715.201211637255
7
Iteration 14300: Loss = -9715.18938016693
8
Iteration 14400: Loss = -9715.175780220707
9
Iteration 14500: Loss = -9715.170156617454
10
Iteration 14600: Loss = -9715.237502275117
11
Iteration 14700: Loss = -9715.19742198891
12
Iteration 14800: Loss = -9715.2405925456
13
Iteration 14900: Loss = -9715.165087035242
14
Iteration 15000: Loss = -9715.172497279058
15
Stopping early at iteration 15000 due to no improvement.
pi: tensor([[9.9997e-01, 3.4641e-05],
        [7.3465e-02, 9.2654e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0586, 0.9414], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1682, 0.1729],
         [0.5937, 0.1267]],

        [[0.7309, 0.1418],
         [0.6048, 0.5211]],

        [[0.6611, 0.1371],
         [0.7283, 0.5061]],

        [[0.7218, 0.1414],
         [0.5511, 0.6449]],

        [[0.7291, 0.1449],
         [0.6027, 0.6430]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.011530202595462608
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.008732110438009916
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.04047959520404796
Global Adjusted Rand Index: 0.01768314012486437
Average Adjusted Rand Index: 0.015401023871536796
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23461.218371448223
Iteration 100: Loss = -9719.975141155208
Iteration 200: Loss = -9718.2071924321
Iteration 300: Loss = -9717.561574170766
Iteration 400: Loss = -9717.24503522731
Iteration 500: Loss = -9717.085216041587
Iteration 600: Loss = -9716.99404668761
Iteration 700: Loss = -9716.937309326288
Iteration 800: Loss = -9716.899913703082
Iteration 900: Loss = -9716.87333640918
Iteration 1000: Loss = -9716.852838448587
Iteration 1100: Loss = -9716.836178397603
Iteration 1200: Loss = -9716.821942466519
Iteration 1300: Loss = -9716.809306236968
Iteration 1400: Loss = -9716.797702300873
Iteration 1500: Loss = -9716.786587170103
Iteration 1600: Loss = -9716.77550228711
Iteration 1700: Loss = -9716.764142245076
Iteration 1800: Loss = -9716.752015855953
Iteration 1900: Loss = -9716.738806777334
Iteration 2000: Loss = -9716.724214608854
Iteration 2100: Loss = -9716.708190557765
Iteration 2200: Loss = -9716.691084497343
Iteration 2300: Loss = -9716.673409528492
Iteration 2400: Loss = -9716.655836114005
Iteration 2500: Loss = -9716.63853236528
Iteration 2600: Loss = -9716.621012565649
Iteration 2700: Loss = -9716.602505402465
Iteration 2800: Loss = -9716.58208357752
Iteration 2900: Loss = -9716.558471178147
Iteration 3000: Loss = -9716.530021777966
Iteration 3100: Loss = -9716.494537502358
Iteration 3200: Loss = -9716.449589252808
Iteration 3300: Loss = -9716.393575478021
Iteration 3400: Loss = -9716.327134641699
Iteration 3500: Loss = -9716.252591868251
Iteration 3600: Loss = -9716.184034036141
Iteration 3700: Loss = -9716.129948019407
Iteration 3800: Loss = -9716.092978973276
Iteration 3900: Loss = -9716.07112312322
Iteration 4000: Loss = -9716.059015239278
Iteration 4100: Loss = -9716.052084982872
Iteration 4200: Loss = -9716.047842726524
Iteration 4300: Loss = -9716.04484224815
Iteration 4400: Loss = -9716.044840263254
Iteration 4500: Loss = -9716.041065591193
Iteration 4600: Loss = -9716.039702047978
Iteration 4700: Loss = -9716.038607368262
Iteration 4800: Loss = -9716.037524963831
Iteration 4900: Loss = -9716.03654828615
Iteration 5000: Loss = -9716.035165874193
Iteration 5100: Loss = -9716.033486590391
Iteration 5200: Loss = -9716.031163430656
Iteration 5300: Loss = -9716.02808504256
Iteration 5400: Loss = -9716.024807776928
Iteration 5500: Loss = -9716.02167412047
Iteration 5600: Loss = -9716.019771229634
Iteration 5700: Loss = -9716.018605648651
Iteration 5800: Loss = -9716.017510951157
Iteration 5900: Loss = -9716.01669417831
Iteration 6000: Loss = -9716.015808142136
Iteration 6100: Loss = -9716.015049842154
Iteration 6200: Loss = -9716.014259391348
Iteration 6300: Loss = -9716.013541771643
Iteration 6400: Loss = -9716.013100195718
Iteration 6500: Loss = -9716.012161494966
Iteration 6600: Loss = -9716.011660867094
Iteration 6700: Loss = -9716.011034586634
Iteration 6800: Loss = -9716.010483350761
Iteration 6900: Loss = -9716.009993709524
Iteration 7000: Loss = -9716.009400852192
Iteration 7100: Loss = -9716.009563344172
1
Iteration 7200: Loss = -9716.007950525507
Iteration 7300: Loss = -9716.006890866547
Iteration 7400: Loss = -9716.005393920115
Iteration 7500: Loss = -9716.002289425254
Iteration 7600: Loss = -9715.995969683736
Iteration 7700: Loss = -9715.97338940831
Iteration 7800: Loss = -9715.729004133718
Iteration 7900: Loss = -9715.257961870899
Iteration 8000: Loss = -9715.20294521073
Iteration 8100: Loss = -9715.187702873647
Iteration 8200: Loss = -9715.180704727425
Iteration 8300: Loss = -9715.17670643192
Iteration 8400: Loss = -9715.174111138409
Iteration 8500: Loss = -9715.17266909735
Iteration 8600: Loss = -9715.176891769961
1
Iteration 8700: Loss = -9715.197703976999
2
Iteration 8800: Loss = -9715.18221181226
3
Iteration 8900: Loss = -9715.168819038196
Iteration 9000: Loss = -9715.167936323473
Iteration 9100: Loss = -9715.168643687928
1
Iteration 9200: Loss = -9715.167236839952
Iteration 9300: Loss = -9715.16679197857
Iteration 9400: Loss = -9715.167182741348
1
Iteration 9500: Loss = -9715.166227654447
Iteration 9600: Loss = -9715.165967645013
Iteration 9700: Loss = -9715.166374786719
1
Iteration 9800: Loss = -9715.165588101549
Iteration 9900: Loss = -9715.16546159233
Iteration 10000: Loss = -9715.16549438827
Iteration 10100: Loss = -9715.166550981057
1
Iteration 10200: Loss = -9715.165134646968
Iteration 10300: Loss = -9715.16511994124
Iteration 10400: Loss = -9715.231047833382
1
Iteration 10500: Loss = -9715.164773206527
Iteration 10600: Loss = -9715.187992979065
1
Iteration 10700: Loss = -9715.164595938259
Iteration 10800: Loss = -9715.171649819085
1
Iteration 10900: Loss = -9715.164513561263
Iteration 11000: Loss = -9715.16461999869
1
Iteration 11100: Loss = -9715.16434106108
Iteration 11200: Loss = -9715.166444060666
1
Iteration 11300: Loss = -9715.164262192135
Iteration 11400: Loss = -9715.165270740881
1
Iteration 11500: Loss = -9715.191541259586
2
Iteration 11600: Loss = -9715.164833585764
3
Iteration 11700: Loss = -9715.167302557586
4
Iteration 11800: Loss = -9715.16618194962
5
Iteration 11900: Loss = -9715.21177971672
6
Iteration 12000: Loss = -9715.188230688305
7
Iteration 12100: Loss = -9715.265764840597
8
Iteration 12200: Loss = -9715.171781165825
9
Iteration 12300: Loss = -9715.17354987584
10
Iteration 12400: Loss = -9715.171130318262
11
Iteration 12500: Loss = -9715.165126125996
12
Iteration 12600: Loss = -9715.181345488121
13
Iteration 12700: Loss = -9715.221084447916
14
Iteration 12800: Loss = -9715.163880895821
Iteration 12900: Loss = -9715.163878508023
Iteration 13000: Loss = -9715.16667708436
1
Iteration 13100: Loss = -9715.166275572703
2
Iteration 13200: Loss = -9715.18165738945
3
Iteration 13300: Loss = -9715.211881501986
4
Iteration 13400: Loss = -9715.163900758062
Iteration 13500: Loss = -9715.163781762449
Iteration 13600: Loss = -9715.183668724298
1
Iteration 13700: Loss = -9715.168877872109
2
Iteration 13800: Loss = -9715.168226278916
3
Iteration 13900: Loss = -9715.16555496274
4
Iteration 14000: Loss = -9715.170292402398
5
Iteration 14100: Loss = -9715.172924199982
6
Iteration 14200: Loss = -9715.180800738055
7
Iteration 14300: Loss = -9715.16751677734
8
Iteration 14400: Loss = -9715.166050177946
9
Iteration 14500: Loss = -9715.195523764258
10
Iteration 14600: Loss = -9715.187531136937
11
Iteration 14700: Loss = -9715.16682931684
12
Iteration 14800: Loss = -9715.163610905098
Iteration 14900: Loss = -9715.165320315044
1
Iteration 15000: Loss = -9715.163681722597
Iteration 15100: Loss = -9715.163710753406
Iteration 15200: Loss = -9715.16407463894
1
Iteration 15300: Loss = -9715.163991272642
2
Iteration 15400: Loss = -9715.164033346733
3
Iteration 15500: Loss = -9715.166567840457
4
Iteration 15600: Loss = -9715.213468984257
5
Iteration 15700: Loss = -9715.172355002724
6
Iteration 15800: Loss = -9715.204577302256
7
Iteration 15900: Loss = -9715.174936978296
8
Iteration 16000: Loss = -9715.18174183911
9
Iteration 16100: Loss = -9715.171155916927
10
Iteration 16200: Loss = -9715.184897508212
11
Iteration 16300: Loss = -9715.165253279943
12
Iteration 16400: Loss = -9715.198822038428
13
Iteration 16500: Loss = -9715.171789885502
14
Iteration 16600: Loss = -9715.189348569329
15
Stopping early at iteration 16600 due to no improvement.
pi: tensor([[9.9998e-01, 1.8544e-05],
        [7.4210e-02, 9.2579e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0595, 0.9405], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1686, 0.1724],
         [0.6369, 0.1263]],

        [[0.5589, 0.1421],
         [0.7188, 0.5983]],

        [[0.5329, 0.1375],
         [0.5825, 0.6099]],

        [[0.5161, 0.1414],
         [0.5425, 0.5957]],

        [[0.5932, 0.1446],
         [0.6585, 0.5207]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.011530202595462608
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.008732110438009916
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.04047959520404796
Global Adjusted Rand Index: 0.01768314012486437
Average Adjusted Rand Index: 0.015401023871536796
9823.019673668823
[0.01768314012486437, 0.01768314012486437] [0.015401023871536796, 0.015401023871536796] [9715.172497279058, 9715.189348569329]
-------------------------------------
This iteration is 5
True Objective function: Loss = -10013.22145267735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23555.388726300163
Iteration 100: Loss = -9886.344825400514
Iteration 200: Loss = -9884.634809245365
Iteration 300: Loss = -9883.69534443719
Iteration 400: Loss = -9883.099940821814
Iteration 500: Loss = -9882.898729544599
Iteration 600: Loss = -9882.80175081839
Iteration 700: Loss = -9882.740910166018
Iteration 800: Loss = -9882.6944513609
Iteration 900: Loss = -9882.654853333117
Iteration 1000: Loss = -9882.621739231266
Iteration 1100: Loss = -9882.592215792134
Iteration 1200: Loss = -9882.565309913021
Iteration 1300: Loss = -9882.540649650173
Iteration 1400: Loss = -9882.518057154906
Iteration 1500: Loss = -9882.497595680956
Iteration 1600: Loss = -9882.4793374647
Iteration 1700: Loss = -9882.463093386641
Iteration 1800: Loss = -9882.448676356658
Iteration 1900: Loss = -9882.435800911766
Iteration 2000: Loss = -9882.424279084096
Iteration 2100: Loss = -9882.41373188846
Iteration 2200: Loss = -9882.403925992634
Iteration 2300: Loss = -9882.394536588077
Iteration 2400: Loss = -9882.385437564966
Iteration 2500: Loss = -9882.376452354856
Iteration 2600: Loss = -9882.367359478674
Iteration 2700: Loss = -9882.357984793098
Iteration 2800: Loss = -9882.348204965778
Iteration 2900: Loss = -9882.337763690899
Iteration 3000: Loss = -9882.32652512827
Iteration 3100: Loss = -9882.314301039525
Iteration 3200: Loss = -9882.300753008705
Iteration 3300: Loss = -9882.285902525364
Iteration 3400: Loss = -9882.269385031463
Iteration 3500: Loss = -9882.251089703264
Iteration 3600: Loss = -9882.230389944903
Iteration 3700: Loss = -9882.20578285179
Iteration 3800: Loss = -9882.171980971223
Iteration 3900: Loss = -9882.097179094284
Iteration 4000: Loss = -9881.321302995833
Iteration 4100: Loss = -9874.800528491061
Iteration 4200: Loss = -9873.96200557441
Iteration 4300: Loss = -9873.830015899168
Iteration 4400: Loss = -9873.784249980945
Iteration 4500: Loss = -9873.761421408792
Iteration 4600: Loss = -9873.74761403954
Iteration 4700: Loss = -9873.73849064413
Iteration 4800: Loss = -9873.732357979163
Iteration 4900: Loss = -9873.727974584368
Iteration 5000: Loss = -9873.724654509864
Iteration 5100: Loss = -9873.722070366131
Iteration 5200: Loss = -9873.71997676692
Iteration 5300: Loss = -9873.718224863018
Iteration 5400: Loss = -9873.716772368216
Iteration 5500: Loss = -9873.71552070077
Iteration 5600: Loss = -9873.714434383246
Iteration 5700: Loss = -9873.71346868076
Iteration 5800: Loss = -9873.712587829967
Iteration 5900: Loss = -9873.711816010467
Iteration 6000: Loss = -9873.711087103755
Iteration 6100: Loss = -9873.710342743414
Iteration 6200: Loss = -9873.709334393208
Iteration 6300: Loss = -9873.707326996804
Iteration 6400: Loss = -9873.705887347267
Iteration 6500: Loss = -9873.705358678348
Iteration 6600: Loss = -9873.704918936673
Iteration 6700: Loss = -9873.704573301085
Iteration 6800: Loss = -9873.704274396892
Iteration 6900: Loss = -9873.703988051266
Iteration 7000: Loss = -9873.703783579382
Iteration 7100: Loss = -9873.704188879623
1
Iteration 7200: Loss = -9873.70340704168
Iteration 7300: Loss = -9873.703242892001
Iteration 7400: Loss = -9873.703536319728
1
Iteration 7500: Loss = -9873.702988921557
Iteration 7600: Loss = -9873.702857359398
Iteration 7700: Loss = -9873.702787714421
Iteration 7800: Loss = -9873.702633393537
Iteration 7900: Loss = -9873.703751223335
1
Iteration 8000: Loss = -9873.702474391328
Iteration 8100: Loss = -9873.702412830102
Iteration 8200: Loss = -9873.702707934768
1
Iteration 8300: Loss = -9873.707043975459
2
Iteration 8400: Loss = -9873.704485309272
3
Iteration 8500: Loss = -9873.706704925944
4
Iteration 8600: Loss = -9873.702080136803
Iteration 8700: Loss = -9873.708906249438
1
Iteration 8800: Loss = -9873.702015792294
Iteration 8900: Loss = -9873.73761834287
1
Iteration 9000: Loss = -9873.70195739121
Iteration 9100: Loss = -9873.702014298096
Iteration 9200: Loss = -9873.701920356849
Iteration 9300: Loss = -9873.702127157718
1
Iteration 9400: Loss = -9873.701808706883
Iteration 9500: Loss = -9873.70230044122
1
Iteration 9600: Loss = -9873.70178947037
Iteration 9700: Loss = -9873.813882588627
1
Iteration 9800: Loss = -9873.70176525388
Iteration 9900: Loss = -9873.701746826959
Iteration 10000: Loss = -9873.73259695002
1
Iteration 10100: Loss = -9873.701754041775
Iteration 10200: Loss = -9873.701651306808
Iteration 10300: Loss = -9873.702314647153
1
Iteration 10400: Loss = -9873.701651883135
Iteration 10500: Loss = -9873.702094663662
1
Iteration 10600: Loss = -9873.701684176189
Iteration 10700: Loss = -9873.701697503902
Iteration 10800: Loss = -9873.702027724501
1
Iteration 10900: Loss = -9873.701588240485
Iteration 11000: Loss = -9873.703657662827
1
Iteration 11100: Loss = -9873.701607454335
Iteration 11200: Loss = -9873.70927421336
1
Iteration 11300: Loss = -9873.701572444754
Iteration 11400: Loss = -9873.702457401609
1
Iteration 11500: Loss = -9873.702655217845
2
Iteration 11600: Loss = -9873.701848532806
3
Iteration 11700: Loss = -9873.764579529843
4
Iteration 11800: Loss = -9873.70154634039
Iteration 11900: Loss = -9873.705928409103
1
Iteration 12000: Loss = -9873.701525547991
Iteration 12100: Loss = -9873.704919390691
1
Iteration 12200: Loss = -9873.701522785686
Iteration 12300: Loss = -9873.820440418905
1
Iteration 12400: Loss = -9873.701683971343
2
Iteration 12500: Loss = -9873.720184179572
3
Iteration 12600: Loss = -9873.809769266045
4
Iteration 12700: Loss = -9873.70296810197
5
Iteration 12800: Loss = -9873.701577665419
Iteration 12900: Loss = -9873.702690922333
1
Iteration 13000: Loss = -9873.703549928154
2
Iteration 13100: Loss = -9873.702011769314
3
Iteration 13200: Loss = -9873.702873953167
4
Iteration 13300: Loss = -9873.701486689475
Iteration 13400: Loss = -9873.702948381362
1
Iteration 13500: Loss = -9873.718642043226
2
Iteration 13600: Loss = -9873.70150242353
Iteration 13700: Loss = -9873.702431428856
1
Iteration 13800: Loss = -9873.712761491184
2
Iteration 13900: Loss = -9873.70155314794
Iteration 14000: Loss = -9873.702095432904
1
Iteration 14100: Loss = -9873.701573369839
Iteration 14200: Loss = -9873.702749194494
1
Iteration 14300: Loss = -9873.702010798857
2
Iteration 14400: Loss = -9873.70505734874
3
Iteration 14500: Loss = -9873.722673275364
4
Iteration 14600: Loss = -9873.701701317275
5
Iteration 14700: Loss = -9873.74914678053
6
Iteration 14800: Loss = -9873.701448476877
Iteration 14900: Loss = -9873.702693710708
1
Iteration 15000: Loss = -9873.701681427636
2
Iteration 15100: Loss = -9873.701564520376
3
Iteration 15200: Loss = -9873.70251819817
4
Iteration 15300: Loss = -9873.727274143368
5
Iteration 15400: Loss = -9873.70158513073
6
Iteration 15500: Loss = -9873.70149993455
Iteration 15600: Loss = -9873.705630383582
1
Iteration 15700: Loss = -9873.867009938547
2
Iteration 15800: Loss = -9873.701481386242
Iteration 15900: Loss = -9873.725973980003
1
Iteration 16000: Loss = -9873.701456476492
Iteration 16100: Loss = -9873.707330876883
1
Iteration 16200: Loss = -9873.701484946128
Iteration 16300: Loss = -9873.709225490114
1
Iteration 16400: Loss = -9873.701478394403
Iteration 16500: Loss = -9873.706977107408
1
Iteration 16600: Loss = -9873.70148813045
Iteration 16700: Loss = -9873.707822227892
1
Iteration 16800: Loss = -9873.701518910535
Iteration 16900: Loss = -9873.717028906296
1
Iteration 17000: Loss = -9873.701558390863
Iteration 17100: Loss = -9873.70490243869
1
Iteration 17200: Loss = -9873.701494168507
Iteration 17300: Loss = -9873.7100997489
1
Iteration 17400: Loss = -9873.701493219674
Iteration 17500: Loss = -9873.7553432953
1
Iteration 17600: Loss = -9873.701481989736
Iteration 17700: Loss = -9873.701593968592
1
Iteration 17800: Loss = -9873.701968011786
2
Iteration 17900: Loss = -9873.701473040757
Iteration 18000: Loss = -9873.702988155586
1
Iteration 18100: Loss = -9873.704525117591
2
Iteration 18200: Loss = -9873.702299243389
3
Iteration 18300: Loss = -9873.704604530963
4
Iteration 18400: Loss = -9873.701559969777
Iteration 18500: Loss = -9873.70697754954
1
Iteration 18600: Loss = -9873.706715845196
2
Iteration 18700: Loss = -9873.701719002178
3
Iteration 18800: Loss = -9873.705411536599
4
Iteration 18900: Loss = -9873.701480557787
Iteration 19000: Loss = -9873.705173938848
1
Iteration 19100: Loss = -9873.713922036972
2
Iteration 19200: Loss = -9873.717623509508
3
Iteration 19300: Loss = -9873.701614013098
4
Iteration 19400: Loss = -9873.743680271104
5
Iteration 19500: Loss = -9873.703033100193
6
Iteration 19600: Loss = -9873.716386627128
7
Iteration 19700: Loss = -9873.701514547454
Iteration 19800: Loss = -9873.701692186565
1
Iteration 19900: Loss = -9873.702024204793
2
pi: tensor([[0.9756, 0.0244],
        [0.1978, 0.8022]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9892, 0.0108], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1332, 0.2670],
         [0.5812, 0.7014]],

        [[0.6972, 0.1432],
         [0.5978, 0.7230]],

        [[0.6074, 0.1568],
         [0.5713, 0.6685]],

        [[0.6827, 0.1489],
         [0.7073, 0.6246]],

        [[0.6573, 0.1719],
         [0.6370, 0.5471]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.03523714791144254
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.012368369840205852
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.003485103334991517
Global Adjusted Rand Index: 0.009979382914979336
Average Adjusted Rand Index: 0.010703543437502006
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22324.477811627086
Iteration 100: Loss = -9884.465002390252
Iteration 200: Loss = -9883.273146368543
Iteration 300: Loss = -9882.999572192892
Iteration 400: Loss = -9882.905788421938
Iteration 500: Loss = -9882.856114262719
Iteration 600: Loss = -9882.825713249053
Iteration 700: Loss = -9882.80510799454
Iteration 800: Loss = -9882.790028633919
Iteration 900: Loss = -9882.778244092087
Iteration 1000: Loss = -9882.768608659006
Iteration 1100: Loss = -9882.760342441408
Iteration 1200: Loss = -9882.752932571006
Iteration 1300: Loss = -9882.745940673993
Iteration 1400: Loss = -9882.738918422154
Iteration 1500: Loss = -9882.731118413416
Iteration 1600: Loss = -9882.720950593615
Iteration 1700: Loss = -9882.704407512127
Iteration 1800: Loss = -9882.667141818587
Iteration 1900: Loss = -9882.556636676767
Iteration 2000: Loss = -9882.412247598848
Iteration 2100: Loss = -9882.348802197666
Iteration 2200: Loss = -9882.276908840904
Iteration 2300: Loss = -9882.08021802224
Iteration 2400: Loss = -9876.206997219426
Iteration 2500: Loss = -9874.019682084456
Iteration 2600: Loss = -9873.792933036237
Iteration 2700: Loss = -9873.745801960029
Iteration 2800: Loss = -9873.72969662672
Iteration 2900: Loss = -9873.721992081177
Iteration 3000: Loss = -9873.71741983736
Iteration 3100: Loss = -9873.714356598734
Iteration 3200: Loss = -9873.71213947783
Iteration 3300: Loss = -9873.710490546257
Iteration 3400: Loss = -9873.709180738942
Iteration 3500: Loss = -9873.708147480094
Iteration 3600: Loss = -9873.707276724528
Iteration 3700: Loss = -9873.706554649538
Iteration 3800: Loss = -9873.70600486525
Iteration 3900: Loss = -9873.705491065573
Iteration 4000: Loss = -9873.705099663111
Iteration 4100: Loss = -9873.704741283149
Iteration 4200: Loss = -9873.704424490099
Iteration 4300: Loss = -9873.7041233653
Iteration 4400: Loss = -9873.703884534594
Iteration 4500: Loss = -9873.70365632126
Iteration 4600: Loss = -9873.703444498977
Iteration 4700: Loss = -9873.703253216825
Iteration 4800: Loss = -9873.703109885615
Iteration 4900: Loss = -9873.702938969682
Iteration 5000: Loss = -9873.70280636461
Iteration 5100: Loss = -9873.70270898107
Iteration 5200: Loss = -9873.702578008233
Iteration 5300: Loss = -9873.702469441992
Iteration 5400: Loss = -9873.702454679415
Iteration 5500: Loss = -9873.702296366086
Iteration 5600: Loss = -9873.70221243529
Iteration 5700: Loss = -9873.702683758145
1
Iteration 5800: Loss = -9873.702301024485
Iteration 5900: Loss = -9873.702040957898
Iteration 6000: Loss = -9873.702021710978
Iteration 6100: Loss = -9873.701951825316
Iteration 6200: Loss = -9873.701922572227
Iteration 6300: Loss = -9873.70188855667
Iteration 6400: Loss = -9873.702367436925
1
Iteration 6500: Loss = -9873.70179558679
Iteration 6600: Loss = -9873.701773700463
Iteration 6700: Loss = -9873.701775500225
Iteration 6800: Loss = -9873.70171802451
Iteration 6900: Loss = -9873.701732150083
Iteration 7000: Loss = -9873.70172004404
Iteration 7100: Loss = -9873.702126038714
1
Iteration 7200: Loss = -9873.701674693488
Iteration 7300: Loss = -9873.701672051771
Iteration 7400: Loss = -9873.70164092706
Iteration 7500: Loss = -9873.701655282177
Iteration 7600: Loss = -9873.702517485965
1
Iteration 7700: Loss = -9873.701599581773
Iteration 7800: Loss = -9873.701603255951
Iteration 7900: Loss = -9873.701594754806
Iteration 8000: Loss = -9873.701568948432
Iteration 8100: Loss = -9873.701845054085
1
Iteration 8200: Loss = -9873.701575084542
Iteration 8300: Loss = -9873.701564767978
Iteration 8400: Loss = -9873.701577866454
Iteration 8500: Loss = -9873.701543370256
Iteration 8600: Loss = -9873.701555604259
Iteration 8700: Loss = -9873.701568033359
Iteration 8800: Loss = -9873.70153694706
Iteration 8900: Loss = -9873.70185348983
1
Iteration 9000: Loss = -9873.708805469563
2
Iteration 9100: Loss = -9873.701524792285
Iteration 9200: Loss = -9873.701660558849
1
Iteration 9300: Loss = -9873.713886912015
2
Iteration 9400: Loss = -9873.701495016836
Iteration 9500: Loss = -9873.70242710905
1
Iteration 9600: Loss = -9873.701547517314
Iteration 9700: Loss = -9873.710149817396
1
Iteration 9800: Loss = -9873.701534456764
Iteration 9900: Loss = -9873.701887498208
1
Iteration 10000: Loss = -9873.708376089668
2
Iteration 10100: Loss = -9873.701555510945
Iteration 10200: Loss = -9873.71422171494
1
Iteration 10300: Loss = -9873.701589757427
Iteration 10400: Loss = -9873.701512895175
Iteration 10500: Loss = -9873.834981438138
1
Iteration 10600: Loss = -9873.70148146656
Iteration 10700: Loss = -9873.70150565366
Iteration 10800: Loss = -9873.705978649514
1
Iteration 10900: Loss = -9873.701519146027
Iteration 11000: Loss = -9873.7014988965
Iteration 11100: Loss = -9873.703519072651
1
Iteration 11200: Loss = -9873.701511170377
Iteration 11300: Loss = -9873.701494657169
Iteration 11400: Loss = -9873.701529897393
Iteration 11500: Loss = -9873.702398743986
1
Iteration 11600: Loss = -9873.70176071477
2
Iteration 11700: Loss = -9873.701996628828
3
Iteration 11800: Loss = -9873.70458813287
4
Iteration 11900: Loss = -9873.705728592557
5
Iteration 12000: Loss = -9873.703621709663
6
Iteration 12100: Loss = -9873.701496063417
Iteration 12200: Loss = -9873.765251494364
1
Iteration 12300: Loss = -9873.701499669829
Iteration 12400: Loss = -9873.702660731582
1
Iteration 12500: Loss = -9873.708857310134
2
Iteration 12600: Loss = -9873.70594972637
3
Iteration 12700: Loss = -9873.702896385123
4
Iteration 12800: Loss = -9873.701641853526
5
Iteration 12900: Loss = -9873.701537659523
Iteration 13000: Loss = -9873.738272792303
1
Iteration 13100: Loss = -9873.71484247014
2
Iteration 13200: Loss = -9873.701614165768
Iteration 13300: Loss = -9873.701873196107
1
Iteration 13400: Loss = -9873.702832554956
2
Iteration 13500: Loss = -9873.701793174734
3
Iteration 13600: Loss = -9873.701523408019
Iteration 13700: Loss = -9873.718818208703
1
Iteration 13800: Loss = -9873.701618960013
Iteration 13900: Loss = -9873.701521375546
Iteration 14000: Loss = -9873.702512179172
1
Iteration 14100: Loss = -9873.701492833643
Iteration 14200: Loss = -9873.702857627955
1
Iteration 14300: Loss = -9873.70148807368
Iteration 14400: Loss = -9873.703703397274
1
Iteration 14500: Loss = -9873.70647254386
2
Iteration 14600: Loss = -9873.702523254678
3
Iteration 14700: Loss = -9873.702486372598
4
Iteration 14800: Loss = -9873.701536775445
Iteration 14900: Loss = -9873.701476776128
Iteration 15000: Loss = -9873.702653745353
1
Iteration 15100: Loss = -9873.701708800281
2
Iteration 15200: Loss = -9873.735052581727
3
Iteration 15300: Loss = -9873.701484673542
Iteration 15400: Loss = -9873.702280504789
1
Iteration 15500: Loss = -9873.701486498034
Iteration 15600: Loss = -9873.710237883855
1
Iteration 15700: Loss = -9873.70148795283
Iteration 15800: Loss = -9873.704981882776
1
Iteration 15900: Loss = -9873.70156034341
Iteration 16000: Loss = -9873.709262181941
1
Iteration 16100: Loss = -9873.706317137381
2
Iteration 16200: Loss = -9873.703422484683
3
Iteration 16300: Loss = -9873.70176960118
4
Iteration 16400: Loss = -9873.706039265644
5
Iteration 16500: Loss = -9873.701508458582
Iteration 16600: Loss = -9873.701859520424
1
Iteration 16700: Loss = -9873.902718359237
2
Iteration 16800: Loss = -9873.70204774499
3
Iteration 16900: Loss = -9873.702215984871
4
Iteration 17000: Loss = -9873.701979158293
5
Iteration 17100: Loss = -9873.709878216192
6
Iteration 17200: Loss = -9873.701498527962
Iteration 17300: Loss = -9873.703421564056
1
Iteration 17400: Loss = -9873.800867019965
2
Iteration 17500: Loss = -9873.7014902657
Iteration 17600: Loss = -9873.701516645586
Iteration 17700: Loss = -9873.702905185803
1
Iteration 17800: Loss = -9873.703662073234
2
Iteration 17900: Loss = -9873.702323732936
3
Iteration 18000: Loss = -9873.701519060183
Iteration 18100: Loss = -9873.703084617378
1
Iteration 18200: Loss = -9873.702716569755
2
Iteration 18300: Loss = -9873.701522946873
Iteration 18400: Loss = -9873.71729153016
1
Iteration 18500: Loss = -9873.701456930825
Iteration 18600: Loss = -9873.702159252727
1
Iteration 18700: Loss = -9873.701500463074
Iteration 18800: Loss = -9873.701587227353
Iteration 18900: Loss = -9873.702997388213
1
Iteration 19000: Loss = -9873.701522476627
Iteration 19100: Loss = -9873.716500841701
1
Iteration 19200: Loss = -9873.701489589064
Iteration 19300: Loss = -9873.702158218603
1
Iteration 19400: Loss = -9873.702846356702
2
Iteration 19500: Loss = -9873.70150314235
Iteration 19600: Loss = -9873.70811892646
1
Iteration 19700: Loss = -9873.702312852027
2
Iteration 19800: Loss = -9873.701727466963
3
Iteration 19900: Loss = -9873.7015099532
pi: tensor([[0.8030, 0.1970],
        [0.0243, 0.9757]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0108, 0.9892], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.7014, 0.2669],
         [0.5394, 0.1330]],

        [[0.5832, 0.1429],
         [0.5752, 0.5851]],

        [[0.5728, 0.1563],
         [0.5536, 0.6132]],

        [[0.6925, 0.1494],
         [0.6880, 0.6578]],

        [[0.6939, 0.1717],
         [0.6696, 0.6984]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.03523714791144254
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.012368369840205852
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.003485103334991517
Global Adjusted Rand Index: 0.009979382914979336
Average Adjusted Rand Index: 0.010703543437502006
10013.22145267735
[0.009979382914979336, 0.009979382914979336] [0.010703543437502006, 0.010703543437502006] [9873.701678257154, 9873.703685069737]
-------------------------------------
This iteration is 6
True Objective function: Loss = -9985.158562724544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21617.738666806017
Iteration 100: Loss = -9839.63298820971
Iteration 200: Loss = -9838.342031601847
Iteration 300: Loss = -9838.049877996009
Iteration 400: Loss = -9837.91710355669
Iteration 500: Loss = -9837.841992336653
Iteration 600: Loss = -9837.790259598214
Iteration 700: Loss = -9837.743481574229
Iteration 800: Loss = -9837.683262704248
Iteration 900: Loss = -9837.582701519881
Iteration 1000: Loss = -9837.450114872106
Iteration 1100: Loss = -9837.348303444753
Iteration 1200: Loss = -9837.281389121445
Iteration 1300: Loss = -9837.231787630242
Iteration 1400: Loss = -9837.189880842743
Iteration 1500: Loss = -9837.15188623179
Iteration 1600: Loss = -9837.116129878325
Iteration 1700: Loss = -9837.081470164667
Iteration 1800: Loss = -9837.047104068866
Iteration 1900: Loss = -9837.012363525999
Iteration 2000: Loss = -9836.976591476076
Iteration 2100: Loss = -9836.93937841917
Iteration 2200: Loss = -9836.899941881722
Iteration 2300: Loss = -9836.85755683284
Iteration 2400: Loss = -9836.811389248329
Iteration 2500: Loss = -9836.76127011958
Iteration 2600: Loss = -9836.708554546958
Iteration 2700: Loss = -9836.656512317659
Iteration 2800: Loss = -9836.60940676453
Iteration 2900: Loss = -9836.570457411279
Iteration 3000: Loss = -9836.54058681645
Iteration 3100: Loss = -9836.518897534224
Iteration 3200: Loss = -9836.503475569707
Iteration 3300: Loss = -9836.492462405151
Iteration 3400: Loss = -9836.48441625382
Iteration 3500: Loss = -9836.47849991932
Iteration 3600: Loss = -9836.47399285429
Iteration 3700: Loss = -9836.47052410753
Iteration 3800: Loss = -9836.467809421662
Iteration 3900: Loss = -9836.465617840562
Iteration 4000: Loss = -9836.463851815757
Iteration 4100: Loss = -9836.462393373971
Iteration 4200: Loss = -9836.461177847053
Iteration 4300: Loss = -9836.460167127401
Iteration 4400: Loss = -9836.459300305736
Iteration 4500: Loss = -9836.458556472819
Iteration 4600: Loss = -9836.45791966635
Iteration 4700: Loss = -9836.457387745952
Iteration 4800: Loss = -9836.456894445291
Iteration 4900: Loss = -9836.456490482164
Iteration 5000: Loss = -9836.456115444145
Iteration 5100: Loss = -9836.455831872643
Iteration 5200: Loss = -9836.455522506383
Iteration 5300: Loss = -9836.45524939712
Iteration 5400: Loss = -9836.4550548836
Iteration 5500: Loss = -9836.45490464981
Iteration 5600: Loss = -9836.45464359769
Iteration 5700: Loss = -9836.454442656106
Iteration 5800: Loss = -9836.454343851496
Iteration 5900: Loss = -9836.454163266078
Iteration 6000: Loss = -9836.453989120258
Iteration 6100: Loss = -9836.453899366212
Iteration 6200: Loss = -9836.453753015003
Iteration 6300: Loss = -9836.454377290767
1
Iteration 6400: Loss = -9836.453545352622
Iteration 6500: Loss = -9836.453428168212
Iteration 6600: Loss = -9836.453310647063
Iteration 6700: Loss = -9836.453227358117
Iteration 6800: Loss = -9836.459973168106
1
Iteration 6900: Loss = -9836.453075004043
Iteration 7000: Loss = -9836.453020074905
Iteration 7100: Loss = -9836.45306981873
Iteration 7200: Loss = -9836.452847528304
Iteration 7300: Loss = -9836.452810887009
Iteration 7400: Loss = -9836.452768241405
Iteration 7500: Loss = -9836.452717672615
Iteration 7600: Loss = -9836.45272668516
Iteration 7700: Loss = -9836.452626605304
Iteration 7800: Loss = -9836.4525993643
Iteration 7900: Loss = -9836.45250908493
Iteration 8000: Loss = -9836.455743234545
1
Iteration 8100: Loss = -9836.452410909673
Iteration 8200: Loss = -9836.452596902505
1
Iteration 8300: Loss = -9836.45948493577
2
Iteration 8400: Loss = -9836.452959872968
3
Iteration 8500: Loss = -9836.452566082724
4
Iteration 8600: Loss = -9836.452258214784
Iteration 8700: Loss = -9836.45225926476
Iteration 8800: Loss = -9836.452192029254
Iteration 8900: Loss = -9836.581272010468
1
Iteration 9000: Loss = -9836.452183319801
Iteration 9100: Loss = -9836.452116195975
Iteration 9200: Loss = -9836.46052401437
1
Iteration 9300: Loss = -9836.452106030729
Iteration 9400: Loss = -9836.452069978559
Iteration 9500: Loss = -9836.454054720381
1
Iteration 9600: Loss = -9836.452028097865
Iteration 9700: Loss = -9836.452023746406
Iteration 9800: Loss = -9836.46896858324
1
Iteration 9900: Loss = -9836.451993184293
Iteration 10000: Loss = -9836.452017757834
Iteration 10100: Loss = -9836.474168873056
1
Iteration 10200: Loss = -9836.451983849049
Iteration 10300: Loss = -9836.45218754151
1
Iteration 10400: Loss = -9836.451987576836
Iteration 10500: Loss = -9836.451960678927
Iteration 10600: Loss = -9836.466630542553
1
Iteration 10700: Loss = -9836.451959151462
Iteration 10800: Loss = -9836.451919092628
Iteration 10900: Loss = -9836.45202589457
1
Iteration 11000: Loss = -9836.451899629306
Iteration 11100: Loss = -9836.568655245364
1
Iteration 11200: Loss = -9836.451904188378
Iteration 11300: Loss = -9836.516963586051
1
Iteration 11400: Loss = -9836.451892147588
Iteration 11500: Loss = -9836.454216119875
1
Iteration 11600: Loss = -9836.455037400843
2
Iteration 11700: Loss = -9836.45229819872
3
Iteration 11800: Loss = -9836.461060969272
4
Iteration 11900: Loss = -9836.451893714364
Iteration 12000: Loss = -9836.452595873623
1
Iteration 12100: Loss = -9836.4518666382
Iteration 12200: Loss = -9836.454869112962
1
Iteration 12300: Loss = -9836.451851327838
Iteration 12400: Loss = -9836.45199490338
1
Iteration 12500: Loss = -9836.657646294076
2
Iteration 12600: Loss = -9836.452218585417
3
Iteration 12700: Loss = -9836.458323758388
4
Iteration 12800: Loss = -9836.45631205451
5
Iteration 12900: Loss = -9836.456342959124
6
Iteration 13000: Loss = -9836.470758091176
7
Iteration 13100: Loss = -9836.451921088943
Iteration 13200: Loss = -9836.452012512114
Iteration 13300: Loss = -9836.45630841563
1
Iteration 13400: Loss = -9836.451973723135
Iteration 13500: Loss = -9836.451998535393
Iteration 13600: Loss = -9836.455081344717
1
Iteration 13700: Loss = -9836.451934753806
Iteration 13800: Loss = -9836.45186610038
Iteration 13900: Loss = -9836.452221273745
1
Iteration 14000: Loss = -9836.464954385921
2
Iteration 14100: Loss = -9836.456032488744
3
Iteration 14200: Loss = -9836.451925681182
Iteration 14300: Loss = -9836.451966089659
Iteration 14400: Loss = -9836.454247187517
1
Iteration 14500: Loss = -9836.461293227709
2
Iteration 14600: Loss = -9836.47661068978
3
Iteration 14700: Loss = -9836.451864118533
Iteration 14800: Loss = -9836.453491660173
1
Iteration 14900: Loss = -9836.451880282902
Iteration 15000: Loss = -9836.451929254295
Iteration 15100: Loss = -9836.45211927568
1
Iteration 15200: Loss = -9836.451892887333
Iteration 15300: Loss = -9836.451901921875
Iteration 15400: Loss = -9836.452615019207
1
Iteration 15500: Loss = -9836.452114712389
2
Iteration 15600: Loss = -9836.451902708492
Iteration 15700: Loss = -9836.45398262722
1
Iteration 15800: Loss = -9836.453126910128
2
Iteration 15900: Loss = -9836.451859424085
Iteration 16000: Loss = -9836.47602701099
1
Iteration 16100: Loss = -9836.451848541097
Iteration 16200: Loss = -9836.452368663371
1
Iteration 16300: Loss = -9836.451899614503
Iteration 16400: Loss = -9836.45236340649
1
Iteration 16500: Loss = -9836.613358312596
2
Iteration 16600: Loss = -9836.451863544826
Iteration 16700: Loss = -9836.453904097096
1
Iteration 16800: Loss = -9836.452380183284
2
Iteration 16900: Loss = -9836.45189866533
Iteration 17000: Loss = -9836.451995065529
Iteration 17100: Loss = -9836.464117557678
1
Iteration 17200: Loss = -9836.544022381635
2
Iteration 17300: Loss = -9836.452190416125
3
Iteration 17400: Loss = -9836.452206320293
4
Iteration 17500: Loss = -9836.452204215828
5
Iteration 17600: Loss = -9836.453407448298
6
Iteration 17700: Loss = -9836.452172809395
7
Iteration 17800: Loss = -9836.452364426741
8
Iteration 17900: Loss = -9836.457356184548
9
Iteration 18000: Loss = -9836.455841162862
10
Iteration 18100: Loss = -9836.537303295741
11
Iteration 18200: Loss = -9836.45188144406
Iteration 18300: Loss = -9836.461393166068
1
Iteration 18400: Loss = -9836.451856735539
Iteration 18500: Loss = -9836.452091323616
1
Iteration 18600: Loss = -9836.4530772281
2
Iteration 18700: Loss = -9836.521621175627
3
Iteration 18800: Loss = -9836.571203740921
4
Iteration 18900: Loss = -9836.452147373602
5
Iteration 19000: Loss = -9836.451931430385
Iteration 19100: Loss = -9836.452416368287
1
Iteration 19200: Loss = -9836.470925938942
2
Iteration 19300: Loss = -9836.451947467343
Iteration 19400: Loss = -9836.452232289106
1
Iteration 19500: Loss = -9836.452578032284
2
Iteration 19600: Loss = -9836.451945643314
Iteration 19700: Loss = -9836.485895849692
1
Iteration 19800: Loss = -9836.474550269559
2
Iteration 19900: Loss = -9836.501441803264
3
pi: tensor([[0.9692, 0.0308],
        [0.9765, 0.0235]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0135, 0.9865], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1318, 0.1384],
         [0.6154, 0.1385]],

        [[0.6859, 0.1653],
         [0.6278, 0.5732]],

        [[0.5954, 0.1524],
         [0.5727, 0.6267]],

        [[0.5084, 0.1723],
         [0.6983, 0.6844]],

        [[0.6756, 0.2215],
         [0.6159, 0.6474]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
Global Adjusted Rand Index: -0.0011686665472957264
Average Adjusted Rand Index: -0.0006490834186426951
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22235.3292477972
Iteration 100: Loss = -9839.554398439728
Iteration 200: Loss = -9838.455798286495
Iteration 300: Loss = -9838.126299044206
Iteration 400: Loss = -9837.977868242226
Iteration 500: Loss = -9837.898045579714
Iteration 600: Loss = -9837.8480419817
Iteration 700: Loss = -9837.810787026308
Iteration 800: Loss = -9837.776226665965
Iteration 900: Loss = -9837.734053643391
Iteration 1000: Loss = -9837.670034572393
Iteration 1100: Loss = -9837.580015425492
Iteration 1200: Loss = -9837.485386272614
Iteration 1300: Loss = -9837.400281034195
Iteration 1400: Loss = -9837.338164057983
Iteration 1500: Loss = -9837.292100232511
Iteration 1600: Loss = -9837.252967306706
Iteration 1700: Loss = -9837.21721490514
Iteration 1800: Loss = -9837.183060666739
Iteration 1900: Loss = -9837.149541389868
Iteration 2000: Loss = -9837.115981271956
Iteration 2100: Loss = -9837.08172966648
Iteration 2200: Loss = -9837.046325536208
Iteration 2300: Loss = -9837.009314990377
Iteration 2400: Loss = -9836.97002149782
Iteration 2500: Loss = -9836.92747306367
Iteration 2600: Loss = -9836.880867690497
Iteration 2700: Loss = -9836.829543212913
Iteration 2800: Loss = -9836.77341803519
Iteration 2900: Loss = -9836.714147757995
Iteration 3000: Loss = -9836.655986217254
Iteration 3100: Loss = -9836.604668481914
Iteration 3200: Loss = -9836.563714841039
Iteration 3300: Loss = -9836.533644087722
Iteration 3400: Loss = -9836.512527800336
Iteration 3500: Loss = -9836.497875030052
Iteration 3600: Loss = -9836.487681556826
Iteration 3700: Loss = -9836.480420326112
Iteration 3800: Loss = -9836.475096458584
Iteration 3900: Loss = -9836.471162672286
Iteration 4000: Loss = -9836.468107829292
Iteration 4100: Loss = -9836.46571768468
Iteration 4200: Loss = -9836.463848631754
Iteration 4300: Loss = -9836.462347190356
Iteration 4400: Loss = -9836.461117108329
Iteration 4500: Loss = -9836.46009333079
Iteration 4600: Loss = -9836.45924653608
Iteration 4700: Loss = -9836.458532883958
Iteration 4800: Loss = -9836.457922300091
Iteration 4900: Loss = -9836.457410735731
Iteration 5000: Loss = -9836.456941128292
Iteration 5100: Loss = -9836.456553058517
Iteration 5200: Loss = -9836.456184593633
Iteration 5300: Loss = -9836.455900616287
Iteration 5400: Loss = -9836.455624437822
Iteration 5500: Loss = -9836.455377141738
Iteration 5600: Loss = -9836.455124598835
Iteration 5700: Loss = -9836.459352409905
1
Iteration 5800: Loss = -9836.454738827411
Iteration 5900: Loss = -9836.454549712606
Iteration 6000: Loss = -9836.456031578682
1
Iteration 6100: Loss = -9836.454211406874
Iteration 6200: Loss = -9836.454048833626
Iteration 6300: Loss = -9836.45396366779
Iteration 6400: Loss = -9836.453773679259
Iteration 6500: Loss = -9836.45496063532
1
Iteration 6600: Loss = -9836.453683919313
Iteration 6700: Loss = -9836.453440371004
Iteration 6800: Loss = -9836.4533442279
Iteration 6900: Loss = -9836.453262043391
Iteration 7000: Loss = -9836.453133599185
Iteration 7100: Loss = -9836.453412372293
1
Iteration 7200: Loss = -9836.452982867795
Iteration 7300: Loss = -9836.452941166832
Iteration 7400: Loss = -9836.452862799866
Iteration 7500: Loss = -9836.455719695692
1
Iteration 7600: Loss = -9836.452734448738
Iteration 7700: Loss = -9836.453409504304
1
Iteration 7800: Loss = -9836.452632806508
Iteration 7900: Loss = -9836.45263861143
Iteration 8000: Loss = -9836.455031150163
1
Iteration 8100: Loss = -9836.452471208582
Iteration 8200: Loss = -9836.452593998829
1
Iteration 8300: Loss = -9836.452397617493
Iteration 8400: Loss = -9836.453394355709
1
Iteration 8500: Loss = -9836.526262202986
2
Iteration 8600: Loss = -9836.452336987077
Iteration 8700: Loss = -9836.45347846279
1
Iteration 8800: Loss = -9836.452229179737
Iteration 8900: Loss = -9836.452389644775
1
Iteration 9000: Loss = -9836.452152845986
Iteration 9100: Loss = -9836.45310324976
1
Iteration 9200: Loss = -9836.452142915958
Iteration 9300: Loss = -9836.474986779838
1
Iteration 9400: Loss = -9836.452057009525
Iteration 9500: Loss = -9836.452061297805
Iteration 9600: Loss = -9836.456373852994
1
Iteration 9700: Loss = -9836.452040044136
Iteration 9800: Loss = -9836.452024881004
Iteration 9900: Loss = -9836.453732386206
1
Iteration 10000: Loss = -9836.45200887671
Iteration 10100: Loss = -9836.451999077344
Iteration 10200: Loss = -9836.452451842546
1
Iteration 10300: Loss = -9836.451986805534
Iteration 10400: Loss = -9836.451948723883
Iteration 10500: Loss = -9836.452206990843
1
Iteration 10600: Loss = -9836.452347354081
2
Iteration 10700: Loss = -9836.45192299777
Iteration 10800: Loss = -9836.452032911815
1
Iteration 10900: Loss = -9836.453445480485
2
Iteration 11000: Loss = -9836.452257987172
3
Iteration 11100: Loss = -9836.473040980973
4
Iteration 11200: Loss = -9836.451902354129
Iteration 11300: Loss = -9836.454007265116
1
Iteration 11400: Loss = -9836.451951917
Iteration 11500: Loss = -9836.463094157334
1
Iteration 11600: Loss = -9836.452538020403
2
Iteration 11700: Loss = -9836.451930886207
Iteration 11800: Loss = -9836.451896881015
Iteration 11900: Loss = -9836.455189332375
1
Iteration 12000: Loss = -9836.451872793774
Iteration 12100: Loss = -9836.451855942047
Iteration 12200: Loss = -9836.451937983704
Iteration 12300: Loss = -9836.452110303791
1
Iteration 12400: Loss = -9836.45915361722
2
Iteration 12500: Loss = -9836.451867468251
Iteration 12600: Loss = -9836.45672158094
1
Iteration 12700: Loss = -9836.462291853763
2
Iteration 12800: Loss = -9836.451871783624
Iteration 12900: Loss = -9836.458005983537
1
Iteration 13000: Loss = -9836.451853150384
Iteration 13100: Loss = -9836.452482640292
1
Iteration 13200: Loss = -9836.45196177544
2
Iteration 13300: Loss = -9836.451948018122
Iteration 13400: Loss = -9836.45482716645
1
Iteration 13500: Loss = -9836.452852794182
2
Iteration 13600: Loss = -9836.451916636404
Iteration 13700: Loss = -9836.480124937012
1
Iteration 13800: Loss = -9836.451834572805
Iteration 13900: Loss = -9836.485971516086
1
Iteration 14000: Loss = -9836.451854744035
Iteration 14100: Loss = -9836.581361706492
1
Iteration 14200: Loss = -9836.4527044688
2
Iteration 14300: Loss = -9836.465659303121
3
Iteration 14400: Loss = -9836.452079369363
4
Iteration 14500: Loss = -9836.451926553123
Iteration 14600: Loss = -9836.458044743516
1
Iteration 14700: Loss = -9836.451854242658
Iteration 14800: Loss = -9836.45202709048
1
Iteration 14900: Loss = -9836.451842739025
Iteration 15000: Loss = -9836.45215029912
1
Iteration 15100: Loss = -9836.502609301566
2
Iteration 15200: Loss = -9836.460501338166
3
Iteration 15300: Loss = -9836.452266422315
4
Iteration 15400: Loss = -9836.452758763271
5
Iteration 15500: Loss = -9836.457158297791
6
Iteration 15600: Loss = -9836.45193729729
Iteration 15700: Loss = -9836.451980813401
Iteration 15800: Loss = -9836.452487463957
1
Iteration 15900: Loss = -9836.553882419677
2
Iteration 16000: Loss = -9836.451875138977
Iteration 16100: Loss = -9836.463343433537
1
Iteration 16200: Loss = -9836.458599970736
2
Iteration 16300: Loss = -9836.454434972682
3
Iteration 16400: Loss = -9836.451930360337
Iteration 16500: Loss = -9836.45269245006
1
Iteration 16600: Loss = -9836.45292589134
2
Iteration 16700: Loss = -9836.451903754185
Iteration 16800: Loss = -9836.452930872012
1
Iteration 16900: Loss = -9836.453523105652
2
Iteration 17000: Loss = -9836.454264749535
3
Iteration 17100: Loss = -9836.45787521036
4
Iteration 17200: Loss = -9836.455667769374
5
Iteration 17300: Loss = -9836.499605708266
6
Iteration 17400: Loss = -9836.451889873273
Iteration 17500: Loss = -9836.452308361804
1
Iteration 17600: Loss = -9836.484223419517
2
Iteration 17700: Loss = -9836.453071211898
3
Iteration 17800: Loss = -9836.457625669193
4
Iteration 17900: Loss = -9836.533016847785
5
Iteration 18000: Loss = -9836.45243269288
6
Iteration 18100: Loss = -9836.454469839837
7
Iteration 18200: Loss = -9836.45293723621
8
Iteration 18300: Loss = -9836.453426264368
9
Iteration 18400: Loss = -9836.453906524855
10
Iteration 18500: Loss = -9836.539305428209
11
Iteration 18600: Loss = -9836.454435523141
12
Iteration 18700: Loss = -9836.470566888205
13
Iteration 18800: Loss = -9836.45195488111
Iteration 18900: Loss = -9836.452489328592
1
Iteration 19000: Loss = -9836.620790955943
2
Iteration 19100: Loss = -9836.453081693693
3
Iteration 19200: Loss = -9836.458229038013
4
Iteration 19300: Loss = -9836.457351583567
5
Iteration 19400: Loss = -9836.458307618157
6
Iteration 19500: Loss = -9836.45193989635
Iteration 19600: Loss = -9836.452289451105
1
Iteration 19700: Loss = -9836.457033998251
2
Iteration 19800: Loss = -9836.47553050293
3
Iteration 19900: Loss = -9836.471104907429
4
pi: tensor([[0.9692, 0.0308],
        [0.9760, 0.0240]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0138, 0.9862], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1324, 0.1377],
         [0.5956, 0.1392]],

        [[0.7286, 0.1652],
         [0.6270, 0.7277]],

        [[0.7265, 0.1519],
         [0.5388, 0.5872]],

        [[0.6545, 0.1715],
         [0.5062, 0.5612]],

        [[0.7268, 0.2211],
         [0.5282, 0.5628]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
Global Adjusted Rand Index: -0.0011686665472957264
Average Adjusted Rand Index: -0.0006490834186426951
9985.158562724544
[-0.0011686665472957264, -0.0011686665472957264] [-0.0006490834186426951, -0.0006490834186426951] [9836.531527062303, 9836.45306117779]
-------------------------------------
This iteration is 7
True Objective function: Loss = -9909.93737560901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21337.934381437415
Iteration 100: Loss = -9806.795755504438
Iteration 200: Loss = -9806.326347648843
Iteration 300: Loss = -9805.98639006538
Iteration 400: Loss = -9805.268084390684
Iteration 500: Loss = -9804.707190061652
Iteration 600: Loss = -9804.44891097256
Iteration 700: Loss = -9804.27049348197
Iteration 800: Loss = -9804.103804685286
Iteration 900: Loss = -9803.939616199392
Iteration 1000: Loss = -9803.782177707222
Iteration 1100: Loss = -9803.635239893656
Iteration 1200: Loss = -9803.501310191234
Iteration 1300: Loss = -9803.382458338903
Iteration 1400: Loss = -9803.280724643517
Iteration 1500: Loss = -9803.197650766993
Iteration 1600: Loss = -9803.134264916265
Iteration 1700: Loss = -9803.08964305834
Iteration 1800: Loss = -9803.059602195537
Iteration 1900: Loss = -9803.039984031317
Iteration 2000: Loss = -9803.027368025038
Iteration 2100: Loss = -9803.019502988724
Iteration 2200: Loss = -9803.014238177868
Iteration 2300: Loss = -9803.010940771186
Iteration 2400: Loss = -9803.009141830044
Iteration 2500: Loss = -9803.007531091003
Iteration 2600: Loss = -9803.00666359915
Iteration 2700: Loss = -9803.006247027226
Iteration 2800: Loss = -9803.0058545154
Iteration 2900: Loss = -9803.005690309577
Iteration 3000: Loss = -9803.005572474583
Iteration 3100: Loss = -9803.005482926636
Iteration 3200: Loss = -9803.008343951304
1
Iteration 3300: Loss = -9803.005441498428
Iteration 3400: Loss = -9803.005411836666
Iteration 3500: Loss = -9803.007425549298
1
Iteration 3600: Loss = -9803.005390303333
Iteration 3700: Loss = -9803.005404464378
Iteration 3800: Loss = -9803.00535973253
Iteration 3900: Loss = -9803.005424824625
Iteration 4000: Loss = -9803.005381001465
Iteration 4100: Loss = -9803.005368429112
Iteration 4200: Loss = -9803.005361330032
Iteration 4300: Loss = -9803.005541672459
1
Iteration 4400: Loss = -9803.005367544518
Iteration 4500: Loss = -9803.032001248603
1
Iteration 4600: Loss = -9803.005338084122
Iteration 4700: Loss = -9803.005354106765
Iteration 4800: Loss = -9803.005436507252
Iteration 4900: Loss = -9803.005355706671
Iteration 5000: Loss = -9803.005362036585
Iteration 5100: Loss = -9803.005539526332
1
Iteration 5200: Loss = -9803.005381815568
Iteration 5300: Loss = -9803.005361073896
Iteration 5400: Loss = -9803.005352677303
Iteration 5500: Loss = -9803.005358598139
Iteration 5600: Loss = -9803.02566582829
1
Iteration 5700: Loss = -9803.005362271204
Iteration 5800: Loss = -9803.005366510253
Iteration 5900: Loss = -9803.00538168003
Iteration 6000: Loss = -9803.005330796284
Iteration 6100: Loss = -9803.008043589418
1
Iteration 6200: Loss = -9803.005388334715
Iteration 6300: Loss = -9803.005350831661
Iteration 6400: Loss = -9803.005386281324
Iteration 6500: Loss = -9803.005365872463
Iteration 6600: Loss = -9803.07737719761
1
Iteration 6700: Loss = -9803.005380412256
Iteration 6800: Loss = -9803.00538165133
Iteration 6900: Loss = -9803.005370400442
Iteration 7000: Loss = -9803.00533379466
Iteration 7100: Loss = -9803.078740965339
1
Iteration 7200: Loss = -9803.00535007546
Iteration 7300: Loss = -9803.005380986968
Iteration 7400: Loss = -9803.00649368016
1
Iteration 7500: Loss = -9803.005394572723
Iteration 7600: Loss = -9803.00535188388
Iteration 7700: Loss = -9803.020527148345
1
Iteration 7800: Loss = -9803.005361729836
Iteration 7900: Loss = -9803.008553926558
1
Iteration 8000: Loss = -9803.005343344816
Iteration 8100: Loss = -9803.00576547999
1
Iteration 8200: Loss = -9803.005376639529
Iteration 8300: Loss = -9803.007363532139
1
Iteration 8400: Loss = -9803.00533475618
Iteration 8500: Loss = -9803.012326609922
1
Iteration 8600: Loss = -9803.005360150266
Iteration 8700: Loss = -9803.015334625356
1
Iteration 8800: Loss = -9803.014303915797
2
Iteration 8900: Loss = -9803.005539571652
3
Iteration 9000: Loss = -9803.005521083654
4
Iteration 9100: Loss = -9803.005389593854
Iteration 9200: Loss = -9803.008732075115
1
Iteration 9300: Loss = -9803.0056290839
2
Iteration 9400: Loss = -9803.006549774002
3
Iteration 9500: Loss = -9803.006071985546
4
Iteration 9600: Loss = -9803.005819407563
5
Iteration 9700: Loss = -9803.007605461315
6
Iteration 9800: Loss = -9803.0189358086
7
Iteration 9900: Loss = -9803.00861549556
8
Iteration 10000: Loss = -9803.006878043869
9
Iteration 10100: Loss = -9803.012682833682
10
Iteration 10200: Loss = -9803.00791184569
11
Iteration 10300: Loss = -9803.01366499019
12
Iteration 10400: Loss = -9803.014255319982
13
Iteration 10500: Loss = -9803.170770867266
14
Iteration 10600: Loss = -9803.04027459971
15
Stopping early at iteration 10600 due to no improvement.
pi: tensor([[0.7275, 0.2725],
        [0.9561, 0.0439]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9891, 0.0109], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1280, 0.1043],
         [0.6905, 0.1723]],

        [[0.7106, 0.1486],
         [0.6157, 0.6127]],

        [[0.6686, 0.1505],
         [0.5339, 0.6777]],

        [[0.5186, 0.1543],
         [0.7217, 0.5750]],

        [[0.5317, 0.1390],
         [0.6931, 0.7265]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.00038912871648324923
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.0041361575108286525
Average Adjusted Rand Index: 0.0021454588397711895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22991.539554595012
Iteration 100: Loss = -9809.086846820375
Iteration 200: Loss = -9806.631316566565
Iteration 300: Loss = -9806.0881184835
Iteration 400: Loss = -9805.868731117052
Iteration 500: Loss = -9805.699968936318
Iteration 600: Loss = -9805.510476500463
Iteration 700: Loss = -9805.271403657249
Iteration 800: Loss = -9805.029970857422
Iteration 900: Loss = -9804.831450227302
Iteration 1000: Loss = -9804.655889194293
Iteration 1100: Loss = -9804.487545278922
Iteration 1200: Loss = -9804.322638931597
Iteration 1300: Loss = -9804.16166688121
Iteration 1400: Loss = -9804.007445449912
Iteration 1500: Loss = -9803.862965362305
Iteration 1600: Loss = -9803.731466403477
Iteration 1700: Loss = -9803.615451702733
Iteration 1800: Loss = -9803.515717730112
Iteration 1900: Loss = -9803.429439655853
Iteration 2000: Loss = -9803.35324374367
Iteration 2100: Loss = -9803.286129000688
Iteration 2200: Loss = -9803.224844735012
Iteration 2300: Loss = -9803.171328306984
Iteration 2400: Loss = -9803.127054114968
Iteration 2500: Loss = -9803.090389674942
Iteration 2600: Loss = -9803.062719481954
Iteration 2700: Loss = -9803.09774059654
1
Iteration 2800: Loss = -9803.031427418817
Iteration 2900: Loss = -9803.022777426271
Iteration 3000: Loss = -9803.049641788484
1
Iteration 3100: Loss = -9803.013149443372
Iteration 3200: Loss = -9803.010577766805
Iteration 3300: Loss = -9803.014855281537
1
Iteration 3400: Loss = -9803.007789986588
Iteration 3500: Loss = -9803.007099418888
Iteration 3600: Loss = -9803.006657406444
Iteration 3700: Loss = -9803.006350232483
Iteration 3800: Loss = -9803.006104654825
Iteration 3900: Loss = -9803.005998648649
Iteration 4000: Loss = -9803.005902278575
Iteration 4100: Loss = -9803.005859488336
Iteration 4200: Loss = -9803.005861175316
Iteration 4300: Loss = -9803.005803287178
Iteration 4400: Loss = -9803.051699530892
1
Iteration 4500: Loss = -9803.005798112259
Iteration 4600: Loss = -9803.005784630177
Iteration 4700: Loss = -9803.006575966347
1
Iteration 4800: Loss = -9803.005767954726
Iteration 4900: Loss = -9803.00573949689
Iteration 5000: Loss = -9803.014320749591
1
Iteration 5100: Loss = -9803.00577048582
Iteration 5200: Loss = -9803.005740076194
Iteration 5300: Loss = -9803.006074556168
1
Iteration 5400: Loss = -9803.005747755726
Iteration 5500: Loss = -9803.009219866482
1
Iteration 5600: Loss = -9803.005726249128
Iteration 5700: Loss = -9803.005740388086
Iteration 5800: Loss = -9803.005904516824
1
Iteration 5900: Loss = -9803.005723452796
Iteration 6000: Loss = -9803.008045961044
1
Iteration 6100: Loss = -9803.005714694456
Iteration 6200: Loss = -9803.005729012611
Iteration 6300: Loss = -9803.005737981986
Iteration 6400: Loss = -9803.005695713311
Iteration 6500: Loss = -9803.006930650094
1
Iteration 6600: Loss = -9803.005696010148
Iteration 6700: Loss = -9803.0056979679
Iteration 6800: Loss = -9803.005682063538
Iteration 6900: Loss = -9803.005692518682
Iteration 7000: Loss = -9803.007730178795
1
Iteration 7100: Loss = -9803.005655547486
Iteration 7200: Loss = -9803.054892952308
1
Iteration 7300: Loss = -9803.005652219374
Iteration 7400: Loss = -9803.005616830422
Iteration 7500: Loss = -9803.00561013655
Iteration 7600: Loss = -9803.005623555386
Iteration 7700: Loss = -9803.007255570787
1
Iteration 7800: Loss = -9803.005619486197
Iteration 7900: Loss = -9803.010589112318
1
Iteration 8000: Loss = -9803.005610628681
Iteration 8100: Loss = -9803.005586430025
Iteration 8200: Loss = -9803.005681277984
Iteration 8300: Loss = -9803.005556709091
Iteration 8400: Loss = -9803.006290036132
1
Iteration 8500: Loss = -9803.005510872365
Iteration 8600: Loss = -9803.011745328477
1
Iteration 8700: Loss = -9803.005479649397
Iteration 8800: Loss = -9803.032654764464
1
Iteration 8900: Loss = -9803.00548129424
Iteration 9000: Loss = -9803.0384528373
1
Iteration 9100: Loss = -9803.005579506469
Iteration 9200: Loss = -9803.031687718969
1
Iteration 9300: Loss = -9803.00543413448
Iteration 9400: Loss = -9803.008363764058
1
Iteration 9500: Loss = -9803.005480399497
Iteration 9600: Loss = -9803.005579194882
Iteration 9700: Loss = -9803.006316394521
1
Iteration 9800: Loss = -9803.00589058812
2
Iteration 9900: Loss = -9803.005508282007
Iteration 10000: Loss = -9803.005820814342
1
Iteration 10100: Loss = -9803.005836757337
2
Iteration 10200: Loss = -9803.078353916235
3
Iteration 10300: Loss = -9803.006779650104
4
Iteration 10400: Loss = -9803.006685528173
5
Iteration 10500: Loss = -9803.005926611799
6
Iteration 10600: Loss = -9803.005629300864
7
Iteration 10700: Loss = -9803.006743300617
8
Iteration 10800: Loss = -9803.026278121897
9
Iteration 10900: Loss = -9803.01167011747
10
Iteration 11000: Loss = -9803.060229602142
11
Iteration 11100: Loss = -9803.007426076141
12
Iteration 11200: Loss = -9803.008957118278
13
Iteration 11300: Loss = -9803.055201348594
14
Iteration 11400: Loss = -9803.007653526434
15
Stopping early at iteration 11400 due to no improvement.
pi: tensor([[0.0422, 0.9578],
        [0.2743, 0.7257]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0110, 0.9890], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1725, 0.1043],
         [0.6214, 0.1275]],

        [[0.6043, 0.1485],
         [0.7233, 0.6204]],

        [[0.6808, 0.1508],
         [0.7142, 0.6146]],

        [[0.6801, 0.1542],
         [0.5906, 0.7062]],

        [[0.6071, 0.1393],
         [0.6020, 0.6600]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.00038912871648324923
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.0041361575108286525
Average Adjusted Rand Index: 0.0021454588397711895
9909.93737560901
[0.0041361575108286525, 0.0041361575108286525] [0.0021454588397711895, 0.0021454588397711895] [9803.04027459971, 9803.007653526434]
-------------------------------------
This iteration is 8
True Objective function: Loss = -10076.955909772678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23481.290159164677
Iteration 100: Loss = -9963.557640058407
Iteration 200: Loss = -9961.678629419699
Iteration 300: Loss = -9961.155681682958
Iteration 400: Loss = -9960.561788634117
Iteration 500: Loss = -9959.889391426144
Iteration 600: Loss = -9959.718096288636
Iteration 700: Loss = -9959.564431930374
Iteration 800: Loss = -9959.374795642772
Iteration 900: Loss = -9959.196273270185
Iteration 1000: Loss = -9959.054682436794
Iteration 1100: Loss = -9958.960209892495
Iteration 1200: Loss = -9958.88056679442
Iteration 1300: Loss = -9958.80035838846
Iteration 1400: Loss = -9958.711798959072
Iteration 1500: Loss = -9958.596950249444
Iteration 1600: Loss = -9958.392575325826
Iteration 1700: Loss = -9957.604618012825
Iteration 1800: Loss = -9951.289275788384
Iteration 1900: Loss = -9950.775291116195
Iteration 2000: Loss = -9949.876791083809
Iteration 2100: Loss = -9948.07675534504
Iteration 2200: Loss = -9947.867711223069
Iteration 2300: Loss = -9947.795618292796
Iteration 2400: Loss = -9947.757846684728
Iteration 2500: Loss = -9947.73449860484
Iteration 2600: Loss = -9947.718646625553
Iteration 2700: Loss = -9947.707142824087
Iteration 2800: Loss = -9947.69842061948
Iteration 2900: Loss = -9947.691600497361
Iteration 3000: Loss = -9947.686161998534
Iteration 3100: Loss = -9947.681691554852
Iteration 3200: Loss = -9947.677938362907
Iteration 3300: Loss = -9947.67476326989
Iteration 3400: Loss = -9947.672083103962
Iteration 3500: Loss = -9947.669726525906
Iteration 3600: Loss = -9947.667659852194
Iteration 3700: Loss = -9947.665858862802
Iteration 3800: Loss = -9947.664249241174
Iteration 3900: Loss = -9947.662802192832
Iteration 4000: Loss = -9947.661526152067
Iteration 4100: Loss = -9947.66035079451
Iteration 4200: Loss = -9947.659327447167
Iteration 4300: Loss = -9947.660565402839
1
Iteration 4400: Loss = -9947.657569159377
Iteration 4500: Loss = -9947.656820908223
Iteration 4600: Loss = -9947.656419579771
Iteration 4700: Loss = -9947.655521429242
Iteration 4800: Loss = -9947.654919352393
Iteration 4900: Loss = -9947.655024566411
1
Iteration 5000: Loss = -9947.65390182176
Iteration 5100: Loss = -9947.653441329789
Iteration 5200: Loss = -9947.660765747436
1
Iteration 5300: Loss = -9947.6526334533
Iteration 5400: Loss = -9947.652277433173
Iteration 5500: Loss = -9947.65193077737
Iteration 5600: Loss = -9947.657847937926
1
Iteration 5700: Loss = -9947.651323072112
Iteration 5800: Loss = -9947.651047894833
Iteration 5900: Loss = -9947.650803246352
Iteration 6000: Loss = -9947.650546570385
Iteration 6100: Loss = -9947.650345152731
Iteration 6200: Loss = -9947.652863869911
1
Iteration 6300: Loss = -9947.649873072585
Iteration 6400: Loss = -9947.652636434934
1
Iteration 6500: Loss = -9947.649542250158
Iteration 6600: Loss = -9947.649620518574
Iteration 6700: Loss = -9947.649193374973
Iteration 6800: Loss = -9947.649715780295
1
Iteration 6900: Loss = -9947.648947449541
Iteration 7000: Loss = -9947.650848450896
1
Iteration 7100: Loss = -9947.648705369862
Iteration 7200: Loss = -9947.649076039317
1
Iteration 7300: Loss = -9947.648445177967
Iteration 7400: Loss = -9947.649304270137
1
Iteration 7500: Loss = -9947.64826293928
Iteration 7600: Loss = -9947.64833028857
Iteration 7700: Loss = -9947.648070924732
Iteration 7800: Loss = -9947.653466537207
1
Iteration 7900: Loss = -9947.647936866511
Iteration 8000: Loss = -9947.648042879518
1
Iteration 8100: Loss = -9947.647808385724
Iteration 8200: Loss = -9947.648724066492
1
Iteration 8300: Loss = -9947.647675857628
Iteration 8400: Loss = -9947.647627760904
Iteration 8500: Loss = -9947.647571241852
Iteration 8600: Loss = -9947.647494950425
Iteration 8700: Loss = -9947.647464931923
Iteration 8800: Loss = -9947.647398625822
Iteration 8900: Loss = -9947.648274344434
1
Iteration 9000: Loss = -9947.647284791206
Iteration 9100: Loss = -9947.648868456548
1
Iteration 9200: Loss = -9947.661384485982
2
Iteration 9300: Loss = -9947.647205993935
Iteration 9400: Loss = -9947.655571962678
1
Iteration 9500: Loss = -9947.766055430475
2
Iteration 9600: Loss = -9947.647134785075
Iteration 9700: Loss = -9947.647306607418
1
Iteration 9800: Loss = -9947.648546134236
2
Iteration 9900: Loss = -9947.660242294718
3
Iteration 10000: Loss = -9947.650129826206
4
Iteration 10100: Loss = -9947.65041064705
5
Iteration 10200: Loss = -9947.64712452941
Iteration 10300: Loss = -9947.647275749821
1
Iteration 10400: Loss = -9947.651981680008
2
Iteration 10500: Loss = -9947.64765627491
3
Iteration 10600: Loss = -9947.659706514534
4
Iteration 10700: Loss = -9947.64690266264
Iteration 10800: Loss = -9947.647676574237
1
Iteration 10900: Loss = -9947.653291489196
2
Iteration 11000: Loss = -9947.677089631798
3
Iteration 11100: Loss = -9947.646852046706
Iteration 11200: Loss = -9947.64850429646
1
Iteration 11300: Loss = -9947.647071590562
2
Iteration 11400: Loss = -9947.646921301028
Iteration 11500: Loss = -9947.65177823727
1
Iteration 11600: Loss = -9947.655556603104
2
Iteration 11700: Loss = -9947.646716305477
Iteration 11800: Loss = -9947.646861860472
1
Iteration 11900: Loss = -9947.719099242348
2
Iteration 12000: Loss = -9947.646696030586
Iteration 12100: Loss = -9947.648200966092
1
Iteration 12200: Loss = -9947.66953152782
2
Iteration 12300: Loss = -9947.646693500214
Iteration 12400: Loss = -9947.646842751787
1
Iteration 12500: Loss = -9947.699762887374
2
Iteration 12600: Loss = -9947.646650700664
Iteration 12700: Loss = -9947.64678369396
1
Iteration 12800: Loss = -9947.650030164352
2
Iteration 12900: Loss = -9947.651547836176
3
Iteration 13000: Loss = -9947.65255037969
4
Iteration 13100: Loss = -9947.647189611418
5
Iteration 13200: Loss = -9947.646665442056
Iteration 13300: Loss = -9947.71786216403
1
Iteration 13400: Loss = -9947.64659514189
Iteration 13500: Loss = -9947.6468281059
1
Iteration 13600: Loss = -9947.64673787658
2
Iteration 13700: Loss = -9947.647083649423
3
Iteration 13800: Loss = -9947.65732143624
4
Iteration 13900: Loss = -9947.646609049116
Iteration 14000: Loss = -9947.648273163448
1
Iteration 14100: Loss = -9947.646999432747
2
Iteration 14200: Loss = -9947.647799652648
3
Iteration 14300: Loss = -9947.648245647737
4
Iteration 14400: Loss = -9947.646566216234
Iteration 14500: Loss = -9947.647005265799
1
Iteration 14600: Loss = -9947.646684789242
2
Iteration 14700: Loss = -9947.64662296728
Iteration 14800: Loss = -9947.646559996163
Iteration 14900: Loss = -9947.646628818247
Iteration 15000: Loss = -9947.646539821744
Iteration 15100: Loss = -9947.648287216654
1
Iteration 15200: Loss = -9947.672057141097
2
Iteration 15300: Loss = -9947.671344040431
3
Iteration 15400: Loss = -9947.79625701012
4
Iteration 15500: Loss = -9947.65811082239
5
Iteration 15600: Loss = -9947.6476515351
6
Iteration 15700: Loss = -9947.655597160254
7
Iteration 15800: Loss = -9947.649797642312
8
Iteration 15900: Loss = -9947.646937802598
9
Iteration 16000: Loss = -9947.659744703584
10
Iteration 16100: Loss = -9947.651162217026
11
Iteration 16200: Loss = -9947.646760864294
12
Iteration 16300: Loss = -9947.65166051917
13
Iteration 16400: Loss = -9947.647491641266
14
Iteration 16500: Loss = -9947.653913375121
15
Stopping early at iteration 16500 due to no improvement.
pi: tensor([[1.0000e+00, 6.9655e-08],
        [2.7397e-01, 7.2603e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7248, 0.2752], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1359, 0.1189],
         [0.6116, 0.3176]],

        [[0.6018, 0.1144],
         [0.6337, 0.5309]],

        [[0.6184, 0.1487],
         [0.6208, 0.6694]],

        [[0.6948, 0.1421],
         [0.6708, 0.5439]],

        [[0.6264, 0.1251],
         [0.7033, 0.5272]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 70
Adjusted Rand Index: 0.1522014868404054
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 67
Adjusted Rand Index: 0.11006623001207509
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.010176754151044456
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.016344996169141524
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0006861048409158402
Global Adjusted Rand Index: 0.04531428201346476
Average Adjusted Rand Index: 0.057895114402716476
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24578.679399978304
Iteration 100: Loss = -9964.997801961586
Iteration 200: Loss = -9959.318003511074
Iteration 300: Loss = -9958.336125639327
Iteration 400: Loss = -9958.079928434823
Iteration 500: Loss = -9957.93978834638
Iteration 600: Loss = -9957.822759422781
Iteration 700: Loss = -9954.487682404602
Iteration 800: Loss = -9950.181791984016
Iteration 900: Loss = -9948.082241853472
Iteration 1000: Loss = -9947.834267209573
Iteration 1100: Loss = -9947.769052543294
Iteration 1200: Loss = -9947.736791014358
Iteration 1300: Loss = -9947.71726808643
Iteration 1400: Loss = -9947.704048629772
Iteration 1500: Loss = -9947.694557950423
Iteration 1600: Loss = -9947.687422978403
Iteration 1700: Loss = -9947.681851232666
Iteration 1800: Loss = -9947.67737767736
Iteration 1900: Loss = -9947.67372122466
Iteration 2000: Loss = -9947.670703603937
Iteration 2100: Loss = -9947.668190096347
Iteration 2200: Loss = -9947.666024815035
Iteration 2300: Loss = -9947.6641713234
Iteration 2400: Loss = -9947.662576553783
Iteration 2500: Loss = -9947.661117029165
Iteration 2600: Loss = -9947.659914630458
Iteration 2700: Loss = -9947.658834751854
Iteration 2800: Loss = -9947.657859369601
Iteration 2900: Loss = -9947.656972491728
Iteration 3000: Loss = -9947.656182981957
Iteration 3100: Loss = -9947.655473044153
Iteration 3200: Loss = -9947.654823634837
Iteration 3300: Loss = -9947.654260420153
Iteration 3400: Loss = -9947.653713412587
Iteration 3500: Loss = -9947.653202779411
Iteration 3600: Loss = -9947.652776529154
Iteration 3700: Loss = -9947.652356581028
Iteration 3800: Loss = -9947.652005869739
Iteration 3900: Loss = -9947.65162213783
Iteration 4000: Loss = -9947.651338197453
Iteration 4100: Loss = -9947.658259226906
1
Iteration 4200: Loss = -9947.650777848017
Iteration 4300: Loss = -9947.65050015317
Iteration 4400: Loss = -9947.650328625168
Iteration 4500: Loss = -9947.650049397444
Iteration 4600: Loss = -9947.650072307586
Iteration 4700: Loss = -9947.649655916775
Iteration 4800: Loss = -9947.649496745555
Iteration 4900: Loss = -9947.650421976916
1
Iteration 5000: Loss = -9947.649152301314
Iteration 5100: Loss = -9947.648996541047
Iteration 5200: Loss = -9947.648861500204
Iteration 5300: Loss = -9947.648756565404
Iteration 5400: Loss = -9947.65002590929
1
Iteration 5500: Loss = -9947.648509308887
Iteration 5600: Loss = -9947.648560008882
Iteration 5700: Loss = -9947.648275923555
Iteration 5800: Loss = -9947.648185058863
Iteration 5900: Loss = -9947.661122194158
1
Iteration 6000: Loss = -9947.648036005097
Iteration 6100: Loss = -9947.647941582874
Iteration 6200: Loss = -9947.647859915513
Iteration 6300: Loss = -9947.64781928514
Iteration 6400: Loss = -9947.647715965035
Iteration 6500: Loss = -9947.64765801911
Iteration 6600: Loss = -9947.647630653797
Iteration 6700: Loss = -9947.648825566868
1
Iteration 6800: Loss = -9947.647493727622
Iteration 6900: Loss = -9947.669536725856
1
Iteration 7000: Loss = -9947.647409162952
Iteration 7100: Loss = -9947.6473693448
Iteration 7200: Loss = -9947.647347743161
Iteration 7300: Loss = -9947.647258171546
Iteration 7400: Loss = -9947.64842775395
1
Iteration 7500: Loss = -9947.647203868917
Iteration 7600: Loss = -9947.647156190058
Iteration 7700: Loss = -9947.647178931516
Iteration 7800: Loss = -9947.647083983802
Iteration 7900: Loss = -9947.648183315963
1
Iteration 8000: Loss = -9947.650023909999
2
Iteration 8100: Loss = -9947.647037328923
Iteration 8200: Loss = -9947.646997612806
Iteration 8300: Loss = -9947.646955543183
Iteration 8400: Loss = -9947.648739974591
1
Iteration 8500: Loss = -9947.646943124182
Iteration 8600: Loss = -9947.647103134812
1
Iteration 8700: Loss = -9947.64688737814
Iteration 8800: Loss = -9947.651053529531
1
Iteration 8900: Loss = -9947.646855107356
Iteration 9000: Loss = -9947.686466535324
1
Iteration 9100: Loss = -9947.647285314446
2
Iteration 9200: Loss = -9947.646837603266
Iteration 9300: Loss = -9947.647733134096
1
Iteration 9400: Loss = -9947.653389358025
2
Iteration 9500: Loss = -9947.647351290476
3
Iteration 9600: Loss = -9947.646820355916
Iteration 9700: Loss = -9947.649189200207
1
Iteration 9800: Loss = -9947.64735112871
2
Iteration 9900: Loss = -9947.64684697648
Iteration 10000: Loss = -9947.646723822527
Iteration 10100: Loss = -9947.64732403959
1
Iteration 10200: Loss = -9947.779868258465
2
Iteration 10300: Loss = -9947.646714683226
Iteration 10400: Loss = -9947.647470373406
1
Iteration 10500: Loss = -9947.646980436422
2
Iteration 10600: Loss = -9947.646849551593
3
Iteration 10700: Loss = -9947.647324478958
4
Iteration 10800: Loss = -9947.64685251125
5
Iteration 10900: Loss = -9947.646689439503
Iteration 11000: Loss = -9947.647598904583
1
Iteration 11100: Loss = -9947.654981551073
2
Iteration 11200: Loss = -9947.646630898731
Iteration 11300: Loss = -9947.646757736546
1
Iteration 11400: Loss = -9947.680274082191
2
Iteration 11500: Loss = -9947.651216887418
3
Iteration 11600: Loss = -9947.783262840278
4
Iteration 11700: Loss = -9947.646585094095
Iteration 11800: Loss = -9947.646771381838
1
Iteration 11900: Loss = -9947.646715423654
2
Iteration 12000: Loss = -9947.646651905981
Iteration 12100: Loss = -9947.682685361131
1
Iteration 12200: Loss = -9947.65855645248
2
Iteration 12300: Loss = -9947.64814900606
3
Iteration 12400: Loss = -9947.656173743457
4
Iteration 12500: Loss = -9947.648787563865
5
Iteration 12600: Loss = -9947.65134422742
6
Iteration 12700: Loss = -9947.661135317856
7
Iteration 12800: Loss = -9947.64690911379
8
Iteration 12900: Loss = -9947.649339059679
9
Iteration 13000: Loss = -9947.65188596074
10
Iteration 13100: Loss = -9947.731787873263
11
Iteration 13200: Loss = -9947.646591879095
Iteration 13300: Loss = -9947.648084630904
1
Iteration 13400: Loss = -9947.646551538752
Iteration 13500: Loss = -9947.651737127138
1
Iteration 13600: Loss = -9947.64655614797
Iteration 13700: Loss = -9947.654889643645
1
Iteration 13800: Loss = -9947.664015399363
2
Iteration 13900: Loss = -9947.683284911192
3
Iteration 14000: Loss = -9947.656916071634
4
Iteration 14100: Loss = -9947.647102634164
5
Iteration 14200: Loss = -9947.64662037052
Iteration 14300: Loss = -9947.647842931307
1
Iteration 14400: Loss = -9947.718317620922
2
Iteration 14500: Loss = -9947.64726443203
3
Iteration 14600: Loss = -9947.683333949572
4
Iteration 14700: Loss = -9947.65131534866
5
Iteration 14800: Loss = -9947.650873433264
6
Iteration 14900: Loss = -9947.64670930303
Iteration 15000: Loss = -9947.655415348285
1
Iteration 15100: Loss = -9947.68124655567
2
Iteration 15200: Loss = -9947.666256567401
3
Iteration 15300: Loss = -9947.671354787677
4
Iteration 15400: Loss = -9947.651716430284
5
Iteration 15500: Loss = -9947.64678135866
Iteration 15600: Loss = -9947.646596089484
Iteration 15700: Loss = -9947.646607185441
Iteration 15800: Loss = -9947.656399594733
1
Iteration 15900: Loss = -9947.646884647527
2
Iteration 16000: Loss = -9947.646588527854
Iteration 16100: Loss = -9947.902562086208
1
Iteration 16200: Loss = -9947.646554501624
Iteration 16300: Loss = -9947.697168300629
1
Iteration 16400: Loss = -9947.646584263915
Iteration 16500: Loss = -9947.647330484811
1
Iteration 16600: Loss = -9947.646932794374
2
Iteration 16700: Loss = -9947.646580046076
Iteration 16800: Loss = -9947.646797894979
1
Iteration 16900: Loss = -9947.646923465887
2
Iteration 17000: Loss = -9947.649256912973
3
Iteration 17100: Loss = -9947.647315308484
4
Iteration 17200: Loss = -9947.646613221268
Iteration 17300: Loss = -9947.648333588662
1
Iteration 17400: Loss = -9947.646518159921
Iteration 17500: Loss = -9947.646590485563
Iteration 17600: Loss = -9947.646622725992
Iteration 17700: Loss = -9947.646543821598
Iteration 17800: Loss = -9947.649307695581
1
Iteration 17900: Loss = -9947.663719092501
2
Iteration 18000: Loss = -9947.646641605092
Iteration 18100: Loss = -9947.646739360285
Iteration 18200: Loss = -9947.654065384102
1
Iteration 18300: Loss = -9947.6466421215
Iteration 18400: Loss = -9947.646580362565
Iteration 18500: Loss = -9947.647399011372
1
Iteration 18600: Loss = -9947.648988460558
2
Iteration 18700: Loss = -9947.649979891341
3
Iteration 18800: Loss = -9947.755866296706
4
Iteration 18900: Loss = -9947.646543712757
Iteration 19000: Loss = -9947.646787189888
1
Iteration 19100: Loss = -9947.646639163051
Iteration 19200: Loss = -9947.648914086078
1
Iteration 19300: Loss = -9947.666998580768
2
Iteration 19400: Loss = -9947.646522216874
Iteration 19500: Loss = -9947.652283010399
1
Iteration 19600: Loss = -9947.651314481207
2
Iteration 19700: Loss = -9947.647090255841
3
Iteration 19800: Loss = -9947.646578839718
Iteration 19900: Loss = -9947.646826990645
1
pi: tensor([[7.2681e-01, 2.7319e-01],
        [5.4337e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2779, 0.7221], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3160, 0.1190],
         [0.6015, 0.1355]],

        [[0.7139, 0.1148],
         [0.6028, 0.6439]],

        [[0.5025, 0.1497],
         [0.6523, 0.6416]],

        [[0.7309, 0.1420],
         [0.6502, 0.5047]],

        [[0.5583, 0.1255],
         [0.6758, 0.6920]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 30
Adjusted Rand Index: 0.1522014868404054
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 33
Adjusted Rand Index: 0.11006623001207509
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.010176754151044456
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.016344996169141524
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0006861048409158402
Global Adjusted Rand Index: 0.04531428201346476
Average Adjusted Rand Index: 0.057895114402716476
10076.955909772678
[0.04531428201346476, 0.04531428201346476] [0.057895114402716476, 0.057895114402716476] [9947.653913375121, 9947.664750682245]
-------------------------------------
This iteration is 9
True Objective function: Loss = -10048.565141105853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22901.333648996955
Iteration 100: Loss = -9956.433383149304
Iteration 200: Loss = -9955.712096394482
Iteration 300: Loss = -9955.566461130278
Iteration 400: Loss = -9955.477975012118
Iteration 500: Loss = -9955.400006207721
Iteration 600: Loss = -9955.309816384077
Iteration 700: Loss = -9954.93340757201
Iteration 800: Loss = -9952.74390853322
Iteration 900: Loss = -9952.516989506377
Iteration 1000: Loss = -9952.392216488128
Iteration 1100: Loss = -9952.300484493897
Iteration 1200: Loss = -9952.2284133956
Iteration 1300: Loss = -9952.169949871637
Iteration 1400: Loss = -9952.120859439117
Iteration 1500: Loss = -9952.07924430722
Iteration 1600: Loss = -9952.047241532553
Iteration 1700: Loss = -9952.023434127139
Iteration 1800: Loss = -9952.004312471046
Iteration 1900: Loss = -9951.988398655263
Iteration 2000: Loss = -9951.976669389947
Iteration 2100: Loss = -9951.968051579495
Iteration 2200: Loss = -9951.96210007837
Iteration 2300: Loss = -9951.957740793816
Iteration 2400: Loss = -9951.954587078753
Iteration 2500: Loss = -9951.952164280025
Iteration 2600: Loss = -9951.950330743855
Iteration 2700: Loss = -9951.948796856634
Iteration 2800: Loss = -9951.947508517243
Iteration 2900: Loss = -9951.946327961814
Iteration 3000: Loss = -9951.945182470827
Iteration 3100: Loss = -9951.943977365607
Iteration 3200: Loss = -9951.942711428546
Iteration 3300: Loss = -9951.941471054883
Iteration 3400: Loss = -9951.940137032752
Iteration 3500: Loss = -9951.938587017743
Iteration 3600: Loss = -9951.936621841776
Iteration 3700: Loss = -9951.934105930139
Iteration 3800: Loss = -9951.930662074166
Iteration 3900: Loss = -9951.925533134947
Iteration 4000: Loss = -9951.917161645095
Iteration 4100: Loss = -9951.901504940817
Iteration 4200: Loss = -9951.867566216866
Iteration 4300: Loss = -9951.787006940816
Iteration 4400: Loss = -9951.427471699317
Iteration 4500: Loss = -9950.865325357607
Iteration 4600: Loss = -9950.85000094828
Iteration 4700: Loss = -9950.844657791706
Iteration 4800: Loss = -9950.8406191734
Iteration 4900: Loss = -9950.838043576974
Iteration 5000: Loss = -9950.836356800597
Iteration 5100: Loss = -9950.835253495601
Iteration 5200: Loss = -9950.834526614844
Iteration 5300: Loss = -9950.834010732857
Iteration 5400: Loss = -9950.833605931613
Iteration 5500: Loss = -9950.838573829455
1
Iteration 5600: Loss = -9950.83308396844
Iteration 5700: Loss = -9950.83292047951
Iteration 5800: Loss = -9950.832832259548
Iteration 5900: Loss = -9950.832625921408
Iteration 6000: Loss = -9950.84154237175
1
Iteration 6100: Loss = -9950.832392035767
Iteration 6200: Loss = -9950.832246590793
Iteration 6300: Loss = -9950.832141926541
Iteration 6400: Loss = -9950.832063194643
Iteration 6500: Loss = -9950.8328694785
1
Iteration 6600: Loss = -9950.831952294686
Iteration 6700: Loss = -9950.831932765801
Iteration 6800: Loss = -9950.8318914462
Iteration 6900: Loss = -9950.83186801717
Iteration 7000: Loss = -9950.831812935145
Iteration 7100: Loss = -9950.83181423323
Iteration 7200: Loss = -9950.831794323218
Iteration 7300: Loss = -9950.832081347577
1
Iteration 7400: Loss = -9950.83174568384
Iteration 7500: Loss = -9950.8336708135
1
Iteration 7600: Loss = -9950.832384014087
2
Iteration 7700: Loss = -9950.831804186515
Iteration 7800: Loss = -9950.831826284642
Iteration 7900: Loss = -9950.831802579843
Iteration 8000: Loss = -9950.831808239716
Iteration 8100: Loss = -9950.831831417247
Iteration 8200: Loss = -9950.832009261445
1
Iteration 8300: Loss = -9950.831702544276
Iteration 8400: Loss = -9950.832392403481
1
Iteration 8500: Loss = -9950.831810243346
2
Iteration 8600: Loss = -9950.831955206322
3
Iteration 8700: Loss = -9950.831937510702
4
Iteration 8800: Loss = -9950.83242914973
5
Iteration 8900: Loss = -9950.83574675243
6
Iteration 9000: Loss = -9950.831698336102
Iteration 9100: Loss = -9950.831837671412
1
Iteration 9200: Loss = -9950.831690032446
Iteration 9300: Loss = -9950.832802061514
1
Iteration 9400: Loss = -9950.831622834934
Iteration 9500: Loss = -9950.831657116272
Iteration 9600: Loss = -9950.831929021453
1
Iteration 9700: Loss = -9950.831677090799
Iteration 9800: Loss = -9950.831651124488
Iteration 9900: Loss = -9950.831678124083
Iteration 10000: Loss = -9950.831663575576
Iteration 10100: Loss = -9950.989949710207
1
Iteration 10200: Loss = -9950.831664715291
Iteration 10300: Loss = -9950.831644077185
Iteration 10400: Loss = -9950.843771457008
1
Iteration 10500: Loss = -9950.83164882215
Iteration 10600: Loss = -9950.831667764573
Iteration 10700: Loss = -9950.831896400061
1
Iteration 10800: Loss = -9950.831639017082
Iteration 10900: Loss = -9951.084563945176
1
Iteration 11000: Loss = -9950.831666371656
Iteration 11100: Loss = -9950.83165848189
Iteration 11200: Loss = -9950.836743667965
1
Iteration 11300: Loss = -9950.831663594668
Iteration 11400: Loss = -9950.831669524425
Iteration 11500: Loss = -9950.831914401233
1
Iteration 11600: Loss = -9950.831640849805
Iteration 11700: Loss = -9950.831680353289
Iteration 11800: Loss = -9950.837796457043
1
Iteration 11900: Loss = -9950.831672379401
Iteration 12000: Loss = -9950.837286520677
1
Iteration 12100: Loss = -9950.831674754938
Iteration 12200: Loss = -9950.831666071346
Iteration 12300: Loss = -9950.835477940323
1
Iteration 12400: Loss = -9950.831654799727
Iteration 12500: Loss = -9950.833327547716
1
Iteration 12600: Loss = -9950.831636451278
Iteration 12700: Loss = -9950.85627099608
1
Iteration 12800: Loss = -9950.83201798472
2
Iteration 12900: Loss = -9950.831688213797
Iteration 13000: Loss = -9951.20049350317
1
Iteration 13100: Loss = -9950.834527862244
2
Iteration 13200: Loss = -9950.831825549378
3
Iteration 13300: Loss = -9950.831993983404
4
Iteration 13400: Loss = -9950.837510726822
5
Iteration 13500: Loss = -9950.831719937907
Iteration 13600: Loss = -9951.10436877642
1
Iteration 13700: Loss = -9950.831700141809
Iteration 13800: Loss = -9950.839164745483
1
Iteration 13900: Loss = -9950.831838693435
2
Iteration 14000: Loss = -9950.8322229812
3
Iteration 14100: Loss = -9950.833050187699
4
Iteration 14200: Loss = -9950.833611354865
5
Iteration 14300: Loss = -9950.83454803704
6
Iteration 14400: Loss = -9950.8450092643
7
Iteration 14500: Loss = -9950.831703297017
Iteration 14600: Loss = -9950.832380389786
1
Iteration 14700: Loss = -9950.896870219902
2
Iteration 14800: Loss = -9950.831706381174
Iteration 14900: Loss = -9950.831960902915
1
Iteration 15000: Loss = -9950.832193900993
2
Iteration 15100: Loss = -9950.831797899478
Iteration 15200: Loss = -9950.831776107656
Iteration 15300: Loss = -9950.888714972965
1
Iteration 15400: Loss = -9950.831715623279
Iteration 15500: Loss = -9950.831833101025
1
Iteration 15600: Loss = -9950.873316319348
2
Iteration 15700: Loss = -9950.831745803816
Iteration 15800: Loss = -9950.862727989977
1
Iteration 15900: Loss = -9950.831718533715
Iteration 16000: Loss = -9950.840438678362
1
Iteration 16100: Loss = -9950.831863714762
2
Iteration 16200: Loss = -9950.831753875595
Iteration 16300: Loss = -9950.831785744385
Iteration 16400: Loss = -9950.831765740519
Iteration 16500: Loss = -9950.831748523662
Iteration 16600: Loss = -9950.833163472338
1
Iteration 16700: Loss = -9950.840371204793
2
Iteration 16800: Loss = -9950.832022947314
3
Iteration 16900: Loss = -9950.831729817954
Iteration 17000: Loss = -9950.8320800215
1
Iteration 17100: Loss = -9950.832466680895
2
Iteration 17200: Loss = -9950.834209367944
3
Iteration 17300: Loss = -9950.831784861175
Iteration 17400: Loss = -9950.832320765341
1
Iteration 17500: Loss = -9950.831746858175
Iteration 17600: Loss = -9950.83536249076
1
Iteration 17700: Loss = -9950.885426237279
2
Iteration 17800: Loss = -9950.832388113236
3
Iteration 17900: Loss = -9950.835517320487
4
Iteration 18000: Loss = -9950.832016402053
5
Iteration 18100: Loss = -9950.83178143114
Iteration 18200: Loss = -9950.841934098846
1
Iteration 18300: Loss = -9950.831912881771
2
Iteration 18400: Loss = -9950.832108225295
3
Iteration 18500: Loss = -9950.83236690752
4
Iteration 18600: Loss = -9950.842094761221
5
Iteration 18700: Loss = -9950.833162791809
6
Iteration 18800: Loss = -9950.832164649699
7
Iteration 18900: Loss = -9950.833232319923
8
Iteration 19000: Loss = -9950.831741153128
Iteration 19100: Loss = -9950.83256628628
1
Iteration 19200: Loss = -9950.835047078259
2
Iteration 19300: Loss = -9950.880984778936
3
Iteration 19400: Loss = -9950.84843033984
4
Iteration 19500: Loss = -9950.832851441015
5
Iteration 19600: Loss = -9950.831757251679
Iteration 19700: Loss = -9950.836528518295
1
Iteration 19800: Loss = -9950.832302852934
2
Iteration 19900: Loss = -9950.832101922024
3
pi: tensor([[0.9399, 0.0601],
        [0.9712, 0.0288]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0101, 0.9899], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1406, 0.1350],
         [0.5586, 0.1313]],

        [[0.6525, 0.2373],
         [0.6232, 0.6597]],

        [[0.6342, 0.1253],
         [0.5235, 0.6001]],

        [[0.5469, 0.0922],
         [0.6776, 0.7229]],

        [[0.6081, 0.1457],
         [0.6284, 0.6394]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012404720808361573
Average Adjusted Rand Index: 0.0008888888888888889
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23253.96242467857
Iteration 100: Loss = -9953.633526885093
Iteration 200: Loss = -9950.625541381014
Iteration 300: Loss = -9948.06151144867
Iteration 400: Loss = -9945.217582940546
Iteration 500: Loss = -9944.853375526323
Iteration 600: Loss = -9944.806687961003
Iteration 700: Loss = -9944.73162663037
Iteration 800: Loss = -9944.371381867304
Iteration 900: Loss = -9941.79770164781
Iteration 1000: Loss = -9937.790681669958
Iteration 1100: Loss = -9937.765210825637
Iteration 1200: Loss = -9937.759440743454
Iteration 1300: Loss = -9937.756704200994
Iteration 1400: Loss = -9937.755996736927
Iteration 1500: Loss = -9937.755404513304
Iteration 1600: Loss = -9937.754840265537
Iteration 1700: Loss = -9937.754346876054
Iteration 1800: Loss = -9937.753935225537
Iteration 1900: Loss = -9937.753925955256
Iteration 2000: Loss = -9937.75373921258
Iteration 2100: Loss = -9937.75388948067
1
Iteration 2200: Loss = -9937.753571223415
Iteration 2300: Loss = -9937.756492640845
1
Iteration 2400: Loss = -9937.753346521593
Iteration 2500: Loss = -9937.753188210045
Iteration 2600: Loss = -9937.753721355704
1
Iteration 2700: Loss = -9937.75307785654
Iteration 2800: Loss = -9937.753042000953
Iteration 2900: Loss = -9937.753277970642
1
Iteration 3000: Loss = -9937.752996350197
Iteration 3100: Loss = -9937.752971830912
Iteration 3200: Loss = -9937.75337891767
1
Iteration 3300: Loss = -9937.753002204236
Iteration 3400: Loss = -9937.752960360043
Iteration 3500: Loss = -9937.753358744478
1
Iteration 3600: Loss = -9937.752971402204
Iteration 3700: Loss = -9937.753213921402
1
Iteration 3800: Loss = -9937.753135699622
2
Iteration 3900: Loss = -9937.752978356471
Iteration 4000: Loss = -9937.752969475909
Iteration 4100: Loss = -9937.769318730458
1
Iteration 4200: Loss = -9937.753553822336
2
Iteration 4300: Loss = -9937.753871753057
3
Iteration 4400: Loss = -9937.752933960717
Iteration 4500: Loss = -9937.753005579278
Iteration 4600: Loss = -9937.752953813531
Iteration 4700: Loss = -9937.75298847216
Iteration 4800: Loss = -9937.752981459314
Iteration 4900: Loss = -9937.753031458928
Iteration 5000: Loss = -9937.755736689269
1
Iteration 5100: Loss = -9937.752931735544
Iteration 5200: Loss = -9937.752986045874
Iteration 5300: Loss = -9937.752990442877
Iteration 5400: Loss = -9937.753059170918
Iteration 5500: Loss = -9937.7533737393
1
Iteration 5600: Loss = -9937.753087232028
Iteration 5700: Loss = -9937.753134634951
Iteration 5800: Loss = -9937.753864608825
1
Iteration 5900: Loss = -9937.752975215313
Iteration 6000: Loss = -9937.753492260616
1
Iteration 6100: Loss = -9937.762531295812
2
Iteration 6200: Loss = -9937.753077860887
3
Iteration 6300: Loss = -9937.755182343974
4
Iteration 6400: Loss = -9937.752958407054
Iteration 6500: Loss = -9937.752954403428
Iteration 6600: Loss = -9937.753002333386
Iteration 6700: Loss = -9937.75309327401
Iteration 6800: Loss = -9937.752967576544
Iteration 6900: Loss = -9937.772926105788
1
Iteration 7000: Loss = -9937.756602263411
2
Iteration 7100: Loss = -9937.75371668766
3
Iteration 7200: Loss = -9937.753389196581
4
Iteration 7300: Loss = -9937.764881581124
5
Iteration 7400: Loss = -9937.766144593657
6
Iteration 7500: Loss = -9937.778473148388
7
Iteration 7600: Loss = -9937.752967882192
Iteration 7700: Loss = -9937.759063063451
1
Iteration 7800: Loss = -9937.75295759131
Iteration 7900: Loss = -9937.752974328543
Iteration 8000: Loss = -9937.75820171895
1
Iteration 8100: Loss = -9937.753223564903
2
Iteration 8200: Loss = -9937.75438751864
3
Iteration 8300: Loss = -9937.756290597534
4
Iteration 8400: Loss = -9937.758794925774
5
Iteration 8500: Loss = -9937.753152268202
6
Iteration 8600: Loss = -9937.757064264275
7
Iteration 8700: Loss = -9937.754770702531
8
Iteration 8800: Loss = -9937.782999910743
9
Iteration 8900: Loss = -9937.753959000222
10
Iteration 9000: Loss = -9937.758770569162
11
Iteration 9100: Loss = -9937.75331755528
12
Iteration 9200: Loss = -9937.752954000405
Iteration 9300: Loss = -9937.78611121799
1
Iteration 9400: Loss = -9937.769697706775
2
Iteration 9500: Loss = -9937.756020199757
3
Iteration 9600: Loss = -9937.75296915049
Iteration 9700: Loss = -9937.754206049465
1
Iteration 9800: Loss = -9937.75293081021
Iteration 9900: Loss = -9937.753318935138
1
Iteration 10000: Loss = -9937.753732614075
2
Iteration 10100: Loss = -9937.778864510981
3
Iteration 10200: Loss = -9937.752930185192
Iteration 10300: Loss = -9937.763119533676
1
Iteration 10400: Loss = -9937.752927284937
Iteration 10500: Loss = -9937.753226384535
1
Iteration 10600: Loss = -9937.752966660164
Iteration 10700: Loss = -9937.752977736542
Iteration 10800: Loss = -9937.752980179595
Iteration 10900: Loss = -9937.753134457693
1
Iteration 11000: Loss = -9937.754104278543
2
Iteration 11100: Loss = -9937.758512943412
3
Iteration 11200: Loss = -9937.75297511347
Iteration 11300: Loss = -9937.752960216758
Iteration 11400: Loss = -9937.75297380044
Iteration 11500: Loss = -9937.753369351434
1
Iteration 11600: Loss = -9937.77469645093
2
Iteration 11700: Loss = -9937.752933566708
Iteration 11800: Loss = -9937.753201323298
1
Iteration 11900: Loss = -9937.752967625944
Iteration 12000: Loss = -9937.7531040458
1
Iteration 12100: Loss = -9937.75854992969
2
Iteration 12200: Loss = -9937.752961154765
Iteration 12300: Loss = -9937.969886689809
1
Iteration 12400: Loss = -9937.752926905356
Iteration 12500: Loss = -9937.753200049874
1
Iteration 12600: Loss = -9937.7529268858
Iteration 12700: Loss = -9937.753921912796
1
Iteration 12800: Loss = -9937.75293852968
Iteration 12900: Loss = -9937.795927235968
1
Iteration 13000: Loss = -9937.752938554348
Iteration 13100: Loss = -9937.753290590917
1
Iteration 13200: Loss = -9937.752976443817
Iteration 13300: Loss = -9937.752917891947
Iteration 13400: Loss = -9937.76796853332
1
Iteration 13500: Loss = -9937.752963679375
Iteration 13600: Loss = -9937.752926932653
Iteration 13700: Loss = -9937.805576058936
1
Iteration 13800: Loss = -9937.752939061174
Iteration 13900: Loss = -9937.752963587169
Iteration 14000: Loss = -9937.753417654507
1
Iteration 14100: Loss = -9937.752987784557
Iteration 14200: Loss = -9937.752949003525
Iteration 14300: Loss = -9937.753315130592
1
Iteration 14400: Loss = -9937.752963620693
Iteration 14500: Loss = -9937.752940120045
Iteration 14600: Loss = -9937.752949616674
Iteration 14700: Loss = -9937.75302087855
Iteration 14800: Loss = -9937.752970955633
Iteration 14900: Loss = -9937.826055108726
1
Iteration 15000: Loss = -9937.752937772919
Iteration 15100: Loss = -9937.752940086028
Iteration 15200: Loss = -9937.75623482482
1
Iteration 15300: Loss = -9937.752945544178
Iteration 15400: Loss = -9937.752935598755
Iteration 15500: Loss = -9937.758606548581
1
Iteration 15600: Loss = -9937.752937489971
Iteration 15700: Loss = -9937.752951419814
Iteration 15800: Loss = -9937.75379040904
1
Iteration 15900: Loss = -9937.75293448253
Iteration 16000: Loss = -9937.752962387854
Iteration 16100: Loss = -9937.753331761665
1
Iteration 16200: Loss = -9937.7529434766
Iteration 16300: Loss = -9937.753144608922
1
Iteration 16400: Loss = -9937.75299976646
Iteration 16500: Loss = -9937.752942110375
Iteration 16600: Loss = -9937.752981757056
Iteration 16700: Loss = -9937.753026922142
Iteration 16800: Loss = -9937.752940193985
Iteration 16900: Loss = -9937.75389852134
1
Iteration 17000: Loss = -9937.75294652748
Iteration 17100: Loss = -9937.794244176406
1
Iteration 17200: Loss = -9937.752953928075
Iteration 17300: Loss = -9937.752927752477
Iteration 17400: Loss = -9937.753958975683
1
Iteration 17500: Loss = -9937.752945799466
Iteration 17600: Loss = -9937.752984212433
Iteration 17700: Loss = -9937.753103789666
1
Iteration 17800: Loss = -9937.752948212363
Iteration 17900: Loss = -9937.776560301492
1
Iteration 18000: Loss = -9937.752938962103
Iteration 18100: Loss = -9937.75372330544
1
Iteration 18200: Loss = -9937.752979110584
Iteration 18300: Loss = -9937.752957729454
Iteration 18400: Loss = -9937.753419968647
1
Iteration 18500: Loss = -9937.752908894698
Iteration 18600: Loss = -9937.794021425527
1
Iteration 18700: Loss = -9937.752932739397
Iteration 18800: Loss = -9937.752935081015
Iteration 18900: Loss = -9937.753052978442
1
Iteration 19000: Loss = -9937.75296541072
Iteration 19100: Loss = -9937.75656655098
1
Iteration 19200: Loss = -9937.752915461499
Iteration 19300: Loss = -9937.777901535404
1
Iteration 19400: Loss = -9937.752959066956
Iteration 19500: Loss = -9937.752938746493
Iteration 19600: Loss = -9937.763064879271
1
Iteration 19700: Loss = -9937.752958004527
Iteration 19800: Loss = -9937.752947549523
Iteration 19900: Loss = -9938.091415149396
1
pi: tensor([[0.8009, 0.1991],
        [0.2138, 0.7862]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5719, 0.4281], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1448, 0.0987],
         [0.7037, 0.2176]],

        [[0.6257, 0.1106],
         [0.6975, 0.6349]],

        [[0.6824, 0.0984],
         [0.5991, 0.6295]],

        [[0.6331, 0.1001],
         [0.6582, 0.6933]],

        [[0.5293, 0.1001],
         [0.6208, 0.7105]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 77
Adjusted Rand Index: 0.2845033135932985
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5733333333333334
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 84
Adjusted Rand Index: 0.45695294179875523
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 88
Adjusted Rand Index: 0.5733155014824413
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7369525201115061
Global Adjusted Rand Index: 0.5174409669665917
Average Adjusted Rand Index: 0.5250115220638669
10048.565141105853
[-0.0012404720808361573, 0.5174409669665917] [0.0008888888888888889, 0.5250115220638669] [9950.83192080556, 9937.752975645752]
-------------------------------------
This iteration is 10
True Objective function: Loss = -10025.912838403518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22512.637552410255
Iteration 100: Loss = -9899.106073203853
Iteration 200: Loss = -9897.201582717686
Iteration 300: Loss = -9896.286472405805
Iteration 400: Loss = -9895.601310936261
Iteration 500: Loss = -9895.28493757101
Iteration 600: Loss = -9895.04122804344
Iteration 700: Loss = -9894.581525377776
Iteration 800: Loss = -9893.59640406083
Iteration 900: Loss = -9892.933125141823
Iteration 1000: Loss = -9891.733782392077
Iteration 1100: Loss = -9890.605171902102
Iteration 1200: Loss = -9890.210967538673
Iteration 1300: Loss = -9889.980491221844
Iteration 1400: Loss = -9889.792962662832
Iteration 1500: Loss = -9889.533537539137
Iteration 1600: Loss = -9889.421819964993
Iteration 1700: Loss = -9889.341295772438
Iteration 1800: Loss = -9889.276316157608
Iteration 1900: Loss = -9889.221024703163
Iteration 2000: Loss = -9889.173298021427
Iteration 2100: Loss = -9889.130432754138
Iteration 2200: Loss = -9889.09733094862
Iteration 2300: Loss = -9889.063009354995
Iteration 2400: Loss = -9889.017764486358
Iteration 2500: Loss = -9888.981569203976
Iteration 2600: Loss = -9888.934524204524
Iteration 2700: Loss = -9888.908179420774
Iteration 2800: Loss = -9888.892861779932
Iteration 2900: Loss = -9888.881166218322
Iteration 3000: Loss = -9888.870775563462
Iteration 3100: Loss = -9888.861245434433
Iteration 3200: Loss = -9888.852211447813
Iteration 3300: Loss = -9888.84281672491
Iteration 3400: Loss = -9888.830958531396
Iteration 3500: Loss = -9888.821624507122
Iteration 3600: Loss = -9888.814671568422
Iteration 3700: Loss = -9888.808549758536
Iteration 3800: Loss = -9888.80308744294
Iteration 3900: Loss = -9888.798276571015
Iteration 4000: Loss = -9888.793779487267
Iteration 4100: Loss = -9888.789655554345
Iteration 4200: Loss = -9888.785858908332
Iteration 4300: Loss = -9888.782266901197
Iteration 4400: Loss = -9888.778706994595
Iteration 4500: Loss = -9888.775066822724
Iteration 4600: Loss = -9888.771582730513
Iteration 4700: Loss = -9888.768852596753
Iteration 4800: Loss = -9888.76643957849
Iteration 4900: Loss = -9888.764059578349
Iteration 5000: Loss = -9888.76159911124
Iteration 5100: Loss = -9888.758973672404
Iteration 5200: Loss = -9888.756754769935
Iteration 5300: Loss = -9888.754710149486
Iteration 5400: Loss = -9888.752696594547
Iteration 5500: Loss = -9888.750623419113
Iteration 5600: Loss = -9888.748450366491
Iteration 5700: Loss = -9888.746119423558
Iteration 5800: Loss = -9888.744085600767
Iteration 5900: Loss = -9888.74257543104
Iteration 6000: Loss = -9888.741244216013
Iteration 6100: Loss = -9888.740071962695
Iteration 6200: Loss = -9888.738926439522
Iteration 6300: Loss = -9888.737793912682
Iteration 6400: Loss = -9888.736734861339
Iteration 6500: Loss = -9888.735782979316
Iteration 6600: Loss = -9888.734858314328
Iteration 6700: Loss = -9888.734036306229
Iteration 6800: Loss = -9888.733313779707
Iteration 6900: Loss = -9888.732433824025
Iteration 7000: Loss = -9888.731720316995
Iteration 7100: Loss = -9888.731789362035
Iteration 7200: Loss = -9888.729366146908
Iteration 7300: Loss = -9888.728864617624
Iteration 7400: Loss = -9888.728089627473
Iteration 7500: Loss = -9888.722742742311
Iteration 7600: Loss = -9888.722205022681
Iteration 7700: Loss = -9888.721880564648
Iteration 7800: Loss = -9888.72141119031
Iteration 7900: Loss = -9888.721142985381
Iteration 8000: Loss = -9888.720672245821
Iteration 8100: Loss = -9888.7203194555
Iteration 8200: Loss = -9888.720641008977
1
Iteration 8300: Loss = -9888.72066799448
2
Iteration 8400: Loss = -9888.719460476048
Iteration 8500: Loss = -9888.72051787941
1
Iteration 8600: Loss = -9888.719091628742
Iteration 8700: Loss = -9888.718585638822
Iteration 8800: Loss = -9888.72072185487
1
Iteration 8900: Loss = -9888.718176884175
Iteration 9000: Loss = -9888.721156947391
1
Iteration 9100: Loss = -9888.717817978219
Iteration 9200: Loss = -9888.717644587228
Iteration 9300: Loss = -9889.050806838592
1
Iteration 9400: Loss = -9888.717365778073
Iteration 9500: Loss = -9888.717272577192
Iteration 9600: Loss = -9888.749841109706
1
Iteration 9700: Loss = -9888.717037800578
Iteration 9800: Loss = -9888.716936875471
Iteration 9900: Loss = -9888.717116388132
1
Iteration 10000: Loss = -9888.7167217906
Iteration 10100: Loss = -9888.716594885058
Iteration 10200: Loss = -9888.718478993103
1
Iteration 10300: Loss = -9888.716403305723
Iteration 10400: Loss = -9888.716310322137
Iteration 10500: Loss = -9888.716401157213
Iteration 10600: Loss = -9888.716152278792
Iteration 10700: Loss = -9888.716057241154
Iteration 10800: Loss = -9888.715900189678
Iteration 10900: Loss = -9888.715690131483
Iteration 11000: Loss = -9888.756678677377
1
Iteration 11100: Loss = -9888.715564438204
Iteration 11200: Loss = -9888.7154563017
Iteration 11300: Loss = -9888.718953237863
1
Iteration 11400: Loss = -9888.715294651394
Iteration 11500: Loss = -9888.715167411105
Iteration 11600: Loss = -9888.71622906573
1
Iteration 11700: Loss = -9888.71501489094
Iteration 11800: Loss = -9888.714930044525
Iteration 11900: Loss = -9888.715575208944
1
Iteration 12000: Loss = -9888.714806458098
Iteration 12100: Loss = -9888.714711815415
Iteration 12200: Loss = -9888.715353017977
1
Iteration 12300: Loss = -9888.714514785452
Iteration 12400: Loss = -9888.71444362481
Iteration 12500: Loss = -9888.714484228658
Iteration 12600: Loss = -9888.714271137711
Iteration 12700: Loss = -9888.718655994135
1
Iteration 12800: Loss = -9888.714200843056
Iteration 12900: Loss = -9888.714145887634
Iteration 13000: Loss = -9888.76500097695
1
Iteration 13100: Loss = -9888.71402836106
Iteration 13200: Loss = -9888.7139787974
Iteration 13300: Loss = -9888.719202629538
1
Iteration 13400: Loss = -9888.713924388883
Iteration 13500: Loss = -9888.713926779861
Iteration 13600: Loss = -9888.720119075986
1
Iteration 13700: Loss = -9888.71390998207
Iteration 13800: Loss = -9888.713886929761
Iteration 13900: Loss = -9888.746708691884
1
Iteration 14000: Loss = -9888.713830962404
Iteration 14100: Loss = -9888.713792367083
Iteration 14200: Loss = -9888.759696296767
1
Iteration 14300: Loss = -9888.71375614961
Iteration 14400: Loss = -9888.713733200613
Iteration 14500: Loss = -9888.773698103172
1
Iteration 14600: Loss = -9888.71367766371
Iteration 14700: Loss = -9888.713671027757
Iteration 14800: Loss = -9888.75042285039
1
Iteration 14900: Loss = -9888.71366587613
Iteration 15000: Loss = -9888.713636506387
Iteration 15100: Loss = -9888.719070889847
1
Iteration 15200: Loss = -9888.713629532625
Iteration 15300: Loss = -9888.713600897501
Iteration 15400: Loss = -9888.718914513558
1
Iteration 15500: Loss = -9888.713535072793
Iteration 15600: Loss = -9888.71354921191
Iteration 15700: Loss = -9888.807137215766
1
Iteration 15800: Loss = -9888.71350212106
Iteration 15900: Loss = -9888.713498706453
Iteration 16000: Loss = -9888.713510841642
Iteration 16100: Loss = -9888.71351916353
Iteration 16200: Loss = -9888.713474513
Iteration 16300: Loss = -9888.713488896514
Iteration 16400: Loss = -9888.713470282622
Iteration 16500: Loss = -9888.713444047069
Iteration 16600: Loss = -9888.71342137181
Iteration 16700: Loss = -9888.724174503492
1
Iteration 16800: Loss = -9888.713422695293
Iteration 16900: Loss = -9888.713414480217
Iteration 17000: Loss = -9889.139913560646
1
Iteration 17100: Loss = -9888.713405176852
Iteration 17200: Loss = -9888.713420467064
Iteration 17300: Loss = -9888.955144757538
1
Iteration 17400: Loss = -9888.713382998023
Iteration 17500: Loss = -9888.713402801675
Iteration 17600: Loss = -9888.713371261856
Iteration 17700: Loss = -9888.713499666725
1
Iteration 17800: Loss = -9888.713380772011
Iteration 17900: Loss = -9888.71339382472
Iteration 18000: Loss = -9888.713416588093
Iteration 18100: Loss = -9888.713440041038
Iteration 18200: Loss = -9888.713407971622
Iteration 18300: Loss = -9888.71339151899
Iteration 18400: Loss = -9888.718832082946
1
Iteration 18500: Loss = -9888.71337387977
Iteration 18600: Loss = -9888.713381510046
Iteration 18700: Loss = -9888.733333672059
1
Iteration 18800: Loss = -9888.71338358468
Iteration 18900: Loss = -9888.71338110108
Iteration 19000: Loss = -9888.714396068583
1
Iteration 19100: Loss = -9888.71339509744
Iteration 19200: Loss = -9888.713383154374
Iteration 19300: Loss = -9888.713831899535
1
Iteration 19400: Loss = -9888.713366361757
Iteration 19500: Loss = -9888.71340762016
Iteration 19600: Loss = -9888.71349975654
Iteration 19700: Loss = -9888.713393541724
Iteration 19800: Loss = -9888.71340392934
Iteration 19900: Loss = -9888.713467653397
pi: tensor([[9.8112e-01, 1.8882e-02],
        [8.1306e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9792, 0.0208], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1324, 0.2477],
         [0.5156, 0.3852]],

        [[0.5770, 0.1441],
         [0.7131, 0.6740]],

        [[0.7206, 0.1739],
         [0.6540, 0.6939]],

        [[0.6904, 0.1656],
         [0.5559, 0.5497]],

        [[0.6574, 0.1688],
         [0.5760, 0.5636]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0019137235069308285
Global Adjusted Rand Index: -0.0015488639572868842
Average Adjusted Rand Index: -0.002389553788017332
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23118.761352396697
Iteration 100: Loss = -9898.174262907994
Iteration 200: Loss = -9896.494590674063
Iteration 300: Loss = -9896.18237775733
Iteration 400: Loss = -9895.982615611209
Iteration 500: Loss = -9895.721090135718
Iteration 600: Loss = -9895.372720688416
Iteration 700: Loss = -9893.908615855507
Iteration 800: Loss = -9891.905106344006
Iteration 900: Loss = -9890.34601608253
Iteration 1000: Loss = -9889.689792507406
Iteration 1100: Loss = -9889.358362644922
Iteration 1200: Loss = -9889.184265771019
Iteration 1300: Loss = -9889.087033811684
Iteration 1400: Loss = -9889.029090066519
Iteration 1500: Loss = -9888.988924294166
Iteration 1600: Loss = -9888.95779537986
Iteration 1700: Loss = -9888.934622916207
Iteration 1800: Loss = -9888.914620550078
Iteration 1900: Loss = -9888.892475623057
Iteration 2000: Loss = -9888.875102179898
Iteration 2100: Loss = -9888.858557032592
Iteration 2200: Loss = -9888.844320370832
Iteration 2300: Loss = -9888.833938776212
Iteration 2400: Loss = -9888.825625958523
Iteration 2500: Loss = -9888.818925207792
Iteration 2600: Loss = -9888.81292964081
Iteration 2700: Loss = -9888.807171218568
Iteration 2800: Loss = -9888.800192983002
Iteration 2900: Loss = -9888.794530896866
Iteration 3000: Loss = -9888.791201886816
Iteration 3100: Loss = -9888.78842596169
Iteration 3200: Loss = -9888.785784602329
Iteration 3300: Loss = -9888.78313367117
Iteration 3400: Loss = -9888.78076615824
Iteration 3500: Loss = -9888.778523385026
Iteration 3600: Loss = -9888.77624391026
Iteration 3700: Loss = -9888.773563183673
Iteration 3800: Loss = -9888.770614984367
Iteration 3900: Loss = -9888.768415405144
Iteration 4000: Loss = -9888.766788219462
Iteration 4100: Loss = -9888.76527997916
Iteration 4200: Loss = -9888.763820155418
Iteration 4300: Loss = -9888.762394142115
Iteration 4400: Loss = -9888.761032911156
Iteration 4500: Loss = -9888.759630364628
Iteration 4600: Loss = -9888.758170946792
Iteration 4700: Loss = -9888.756800735844
Iteration 4800: Loss = -9888.755466723866
Iteration 4900: Loss = -9888.753898800496
Iteration 5000: Loss = -9888.751899808114
Iteration 5100: Loss = -9888.750566498695
Iteration 5200: Loss = -9888.749504448542
Iteration 5300: Loss = -9888.748558573658
Iteration 5400: Loss = -9888.747607850863
Iteration 5500: Loss = -9888.74685130969
Iteration 5600: Loss = -9888.746189025867
Iteration 5700: Loss = -9888.745507802283
Iteration 5800: Loss = -9888.745063084429
Iteration 5900: Loss = -9888.743902086637
Iteration 6000: Loss = -9888.743045073663
Iteration 6100: Loss = -9888.742000247763
Iteration 6200: Loss = -9888.7405126386
Iteration 6300: Loss = -9888.737085459828
Iteration 6400: Loss = -9888.735211308378
Iteration 6500: Loss = -9888.734724541595
Iteration 6600: Loss = -9888.734096481408
Iteration 6700: Loss = -9888.733276280762
Iteration 6800: Loss = -9888.732241487622
Iteration 6900: Loss = -9888.73229660738
Iteration 7000: Loss = -9888.731160168969
Iteration 7100: Loss = -9888.730744098455
Iteration 7200: Loss = -9888.730484400063
Iteration 7300: Loss = -9888.730051706267
Iteration 7400: Loss = -9888.727176183476
Iteration 7500: Loss = -9888.726668380481
Iteration 7600: Loss = -9888.72633325518
Iteration 7700: Loss = -9888.726296998157
Iteration 7800: Loss = -9888.726239397232
Iteration 7900: Loss = -9888.725704309842
Iteration 8000: Loss = -9888.725581870103
Iteration 8100: Loss = -9888.727636486487
1
Iteration 8200: Loss = -9888.724909408813
Iteration 8300: Loss = -9888.724800443391
Iteration 8400: Loss = -9888.724395417566
Iteration 8500: Loss = -9888.724260787005
Iteration 8600: Loss = -9888.724175730391
Iteration 8700: Loss = -9888.723551540295
Iteration 8800: Loss = -9888.72317630915
Iteration 8900: Loss = -9888.722577336179
Iteration 9000: Loss = -9888.722181986444
Iteration 9100: Loss = -9888.725219497017
1
Iteration 9200: Loss = -9888.72121282219
Iteration 9300: Loss = -9888.72083950941
Iteration 9400: Loss = -9888.720228933707
Iteration 9500: Loss = -9888.720224126298
Iteration 9600: Loss = -9888.71970876217
Iteration 9700: Loss = -9888.96105027527
1
Iteration 9800: Loss = -9888.718723365175
Iteration 9900: Loss = -9888.71833803923
Iteration 10000: Loss = -9888.724150876078
1
Iteration 10100: Loss = -9888.717710474331
Iteration 10200: Loss = -9888.717599413441
Iteration 10300: Loss = -9888.71801817811
1
Iteration 10400: Loss = -9888.717475048306
Iteration 10500: Loss = -9888.717449251802
Iteration 10600: Loss = -9888.722198325398
1
Iteration 10700: Loss = -9888.71727979461
Iteration 10800: Loss = -9888.717169193049
Iteration 10900: Loss = -9888.721074007904
1
Iteration 11000: Loss = -9888.716925777526
Iteration 11100: Loss = -9888.716796059602
Iteration 11200: Loss = -9888.717301764322
1
Iteration 11300: Loss = -9888.716631970927
Iteration 11400: Loss = -9888.716437756424
Iteration 11500: Loss = -9888.71911289101
1
Iteration 11600: Loss = -9888.716330018908
Iteration 11700: Loss = -9888.716271946958
Iteration 11800: Loss = -9888.718077971296
1
Iteration 11900: Loss = -9888.716170688627
Iteration 12000: Loss = -9888.716014710131
Iteration 12100: Loss = -9888.71602669484
Iteration 12200: Loss = -9888.715886192766
Iteration 12300: Loss = -9888.715869288708
Iteration 12400: Loss = -9888.716014198264
1
Iteration 12500: Loss = -9888.71581980218
Iteration 12600: Loss = -9888.715790129849
Iteration 12700: Loss = -9888.89830188518
1
Iteration 12800: Loss = -9888.715755884863
Iteration 12900: Loss = -9888.715691187834
Iteration 13000: Loss = -9888.773856142794
1
Iteration 13100: Loss = -9888.715635299523
Iteration 13200: Loss = -9888.715577658373
Iteration 13300: Loss = -9888.728655886835
1
Iteration 13400: Loss = -9888.71534980481
Iteration 13500: Loss = -9888.714856961436
Iteration 13600: Loss = -9888.714910864945
Iteration 13700: Loss = -9888.714689030701
Iteration 13800: Loss = -9888.721572698289
1
Iteration 13900: Loss = -9888.714627297439
Iteration 14000: Loss = -9888.714602732804
Iteration 14100: Loss = -9888.729189503574
1
Iteration 14200: Loss = -9888.71442050539
Iteration 14300: Loss = -9888.714398643026
Iteration 14400: Loss = -9888.832708064483
1
Iteration 14500: Loss = -9888.714206907618
Iteration 14600: Loss = -9888.714189124095
Iteration 14700: Loss = -9888.71419981763
Iteration 14800: Loss = -9888.877597547598
1
Iteration 14900: Loss = -9888.714177214451
Iteration 15000: Loss = -9888.714123607633
Iteration 15100: Loss = -9888.905074158902
1
Iteration 15200: Loss = -9888.714038037258
Iteration 15300: Loss = -9888.714001175687
Iteration 15400: Loss = -9888.940906247897
1
Iteration 15500: Loss = -9888.713993499528
Iteration 15600: Loss = -9888.71399670742
Iteration 15700: Loss = -9888.714001273975
Iteration 15800: Loss = -9888.71423085195
1
Iteration 15900: Loss = -9888.71398141424
Iteration 16000: Loss = -9888.805000141214
1
Iteration 16100: Loss = -9888.713981338982
Iteration 16200: Loss = -9888.713966415222
Iteration 16300: Loss = -9888.714128863023
1
Iteration 16400: Loss = -9888.71397130911
Iteration 16500: Loss = -9888.713890251607
Iteration 16600: Loss = -9888.713889429046
Iteration 16700: Loss = -9888.713963929717
Iteration 16800: Loss = -9888.713876920341
Iteration 16900: Loss = -9889.128906893364
1
Iteration 17000: Loss = -9888.713701334409
Iteration 17100: Loss = -9888.713694564509
Iteration 17200: Loss = -9888.729571695112
1
Iteration 17300: Loss = -9888.71361100925
Iteration 17400: Loss = -9888.71356881069
Iteration 17500: Loss = -9888.71400724947
1
Iteration 17600: Loss = -9888.713568321535
Iteration 17700: Loss = -9888.713530800716
Iteration 17800: Loss = -9888.713693693122
1
Iteration 17900: Loss = -9888.713558086572
Iteration 18000: Loss = -9888.713531350162
Iteration 18100: Loss = -9888.71351034444
Iteration 18200: Loss = -9888.714334544107
1
Iteration 18300: Loss = -9888.713491486953
Iteration 18400: Loss = -9888.714021214537
1
Iteration 18500: Loss = -9888.713576225418
Iteration 18600: Loss = -9888.713522216332
Iteration 18700: Loss = -9889.043455807338
1
Iteration 18800: Loss = -9888.713531737732
Iteration 18900: Loss = -9888.713515650597
Iteration 19000: Loss = -9888.713516320295
Iteration 19100: Loss = -9888.713713366134
1
Iteration 19200: Loss = -9888.713500244394
Iteration 19300: Loss = -9888.713501310798
Iteration 19400: Loss = -9888.723130471632
1
Iteration 19500: Loss = -9888.713487208805
Iteration 19600: Loss = -9888.713467051755
Iteration 19700: Loss = -9888.713539630551
Iteration 19800: Loss = -9888.713467226286
Iteration 19900: Loss = -9888.713459688668
pi: tensor([[9.8109e-01, 1.8908e-02],
        [4.0197e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9790, 0.0210], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1322, 0.2472],
         [0.5990, 0.3850]],

        [[0.6696, 0.1439],
         [0.6730, 0.6822]],

        [[0.6402, 0.1733],
         [0.5094, 0.6456]],

        [[0.5538, 0.1656],
         [0.7184, 0.5644]],

        [[0.6768, 0.1687],
         [0.5864, 0.6145]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0019137235069308285
Global Adjusted Rand Index: -0.0015488639572868842
Average Adjusted Rand Index: -0.002389553788017332
10025.912838403518
[-0.0015488639572868842, -0.0015488639572868842] [-0.002389553788017332, -0.002389553788017332] [9888.71337406787, 9888.71939488445]
-------------------------------------
This iteration is 11
True Objective function: Loss = -9986.422973457491
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24325.74539566966
Iteration 100: Loss = -9891.113925914073
Iteration 200: Loss = -9889.843192277436
Iteration 300: Loss = -9887.965368297853
Iteration 400: Loss = -9886.567379440223
Iteration 500: Loss = -9885.758255779732
Iteration 600: Loss = -9885.189596128346
Iteration 700: Loss = -9884.745650009054
Iteration 800: Loss = -9884.33676212962
Iteration 900: Loss = -9884.002346580499
Iteration 1000: Loss = -9883.795805738358
Iteration 1100: Loss = -9883.686143811488
Iteration 1200: Loss = -9883.628424048618
Iteration 1300: Loss = -9883.595731135301
Iteration 1400: Loss = -9883.57523117899
Iteration 1500: Loss = -9883.561236160473
Iteration 1600: Loss = -9883.5508985778
Iteration 1700: Loss = -9883.543056038803
Iteration 1800: Loss = -9883.536828984148
Iteration 1900: Loss = -9883.531818749105
Iteration 2000: Loss = -9883.527692673428
Iteration 2100: Loss = -9883.524240345618
Iteration 2200: Loss = -9883.521317947225
Iteration 2300: Loss = -9883.518781723007
Iteration 2400: Loss = -9883.516642689066
Iteration 2500: Loss = -9883.514716437614
Iteration 2600: Loss = -9883.513053258032
Iteration 2700: Loss = -9883.511561887819
Iteration 2800: Loss = -9883.510266760939
Iteration 2900: Loss = -9883.509082897614
Iteration 3000: Loss = -9883.508070979433
Iteration 3100: Loss = -9883.50708482352
Iteration 3200: Loss = -9883.506275962432
Iteration 3300: Loss = -9883.505502720674
Iteration 3400: Loss = -9883.504752462448
Iteration 3500: Loss = -9883.504114577267
Iteration 3600: Loss = -9883.503525310723
Iteration 3700: Loss = -9883.503001673127
Iteration 3800: Loss = -9883.50248639795
Iteration 3900: Loss = -9883.502028768426
Iteration 4000: Loss = -9883.501598624058
Iteration 4100: Loss = -9883.501216798917
Iteration 4200: Loss = -9883.50085387079
Iteration 4300: Loss = -9883.500508619716
Iteration 4400: Loss = -9883.500203071511
Iteration 4500: Loss = -9883.499878209848
Iteration 4600: Loss = -9883.499629447202
Iteration 4700: Loss = -9883.499375087233
Iteration 4800: Loss = -9883.49913397961
Iteration 4900: Loss = -9883.49887950487
Iteration 5000: Loss = -9883.498707429144
Iteration 5100: Loss = -9883.498503796418
Iteration 5200: Loss = -9883.498279676427
Iteration 5300: Loss = -9883.498119271835
Iteration 5400: Loss = -9883.497917575844
Iteration 5500: Loss = -9883.497800022584
Iteration 5600: Loss = -9883.497659074554
Iteration 5700: Loss = -9883.497490521178
Iteration 5800: Loss = -9883.497371493779
Iteration 5900: Loss = -9883.497231189578
Iteration 6000: Loss = -9883.497116219081
Iteration 6100: Loss = -9883.49701245576
Iteration 6200: Loss = -9883.4968705496
Iteration 6300: Loss = -9883.496759711614
Iteration 6400: Loss = -9883.496671214096
Iteration 6500: Loss = -9883.496576140813
Iteration 6600: Loss = -9883.496451583609
Iteration 6700: Loss = -9883.496372848678
Iteration 6800: Loss = -9883.496312729922
Iteration 6900: Loss = -9883.496244201147
Iteration 7000: Loss = -9883.496145506522
Iteration 7100: Loss = -9883.496049241381
Iteration 7200: Loss = -9883.495996521751
Iteration 7300: Loss = -9883.495882681706
Iteration 7400: Loss = -9883.495786555874
Iteration 7500: Loss = -9883.495736772245
Iteration 7600: Loss = -9883.495668593121
Iteration 7700: Loss = -9883.495581511661
Iteration 7800: Loss = -9883.495517291783
Iteration 7900: Loss = -9883.495480742256
Iteration 8000: Loss = -9883.495400142421
Iteration 8100: Loss = -9883.49531337717
Iteration 8200: Loss = -9883.495242923442
Iteration 8300: Loss = -9883.495175762864
Iteration 8400: Loss = -9883.503934652756
1
Iteration 8500: Loss = -9883.49503485314
Iteration 8600: Loss = -9883.494987035194
Iteration 8700: Loss = -9883.494895462143
Iteration 8800: Loss = -9883.496364747036
1
Iteration 8900: Loss = -9883.494759068766
Iteration 9000: Loss = -9883.494640366947
Iteration 9100: Loss = -9883.494589092561
Iteration 9200: Loss = -9883.494666697201
Iteration 9300: Loss = -9883.494463065434
Iteration 9400: Loss = -9883.494363149539
Iteration 9500: Loss = -9883.49428392217
Iteration 9600: Loss = -9883.494268934213
Iteration 9700: Loss = -9883.49411661445
Iteration 9800: Loss = -9883.494037595134
Iteration 9900: Loss = -9883.49395575906
Iteration 10000: Loss = -9883.494131394049
1
Iteration 10100: Loss = -9883.49383673635
Iteration 10200: Loss = -9883.493725588298
Iteration 10300: Loss = -9883.496478569043
1
Iteration 10400: Loss = -9883.493554007475
Iteration 10500: Loss = -9883.49346675357
Iteration 10600: Loss = -9883.493388441542
Iteration 10700: Loss = -9883.493435448476
Iteration 10800: Loss = -9883.787259335462
1
Iteration 10900: Loss = -9883.493098270907
Iteration 11000: Loss = -9883.493003998148
Iteration 11100: Loss = -9883.533729853543
1
Iteration 11200: Loss = -9883.492843778626
Iteration 11300: Loss = -9883.492743868803
Iteration 11400: Loss = -9883.502563647871
1
Iteration 11500: Loss = -9883.492572798763
Iteration 11600: Loss = -9883.494267211314
1
Iteration 11700: Loss = -9883.492384939505
Iteration 11800: Loss = -9883.492279668815
Iteration 11900: Loss = -9883.492204013266
Iteration 12000: Loss = -9883.492089047535
Iteration 12100: Loss = -9883.49203612919
Iteration 12200: Loss = -9883.4919702459
Iteration 12300: Loss = -9883.491834702265
Iteration 12400: Loss = -9883.491731356675
Iteration 12500: Loss = -9883.492270031576
1
Iteration 12600: Loss = -9883.491568398164
Iteration 12700: Loss = -9883.491482163308
Iteration 12800: Loss = -9883.633957106369
1
Iteration 12900: Loss = -9883.49130362082
Iteration 13000: Loss = -9883.491239795107
Iteration 13100: Loss = -9883.491169383859
Iteration 13200: Loss = -9883.518985806473
1
Iteration 13300: Loss = -9883.491015157986
Iteration 13400: Loss = -9883.490929694044
Iteration 13500: Loss = -9883.490855482853
Iteration 13600: Loss = -9883.492291156554
1
Iteration 13700: Loss = -9883.49073507893
Iteration 13800: Loss = -9883.490670211753
Iteration 13900: Loss = -9883.490627985555
Iteration 14000: Loss = -9883.490716858088
Iteration 14100: Loss = -9883.49052929478
Iteration 14200: Loss = -9883.49045811952
Iteration 14300: Loss = -9883.522794414526
1
Iteration 14400: Loss = -9883.490383639652
Iteration 14500: Loss = -9883.490326904037
Iteration 14600: Loss = -9883.490296701271
Iteration 14700: Loss = -9883.501929786047
1
Iteration 14800: Loss = -9883.490196988114
Iteration 14900: Loss = -9883.490155451924
Iteration 15000: Loss = -9883.490124067115
Iteration 15100: Loss = -9883.490126827905
Iteration 15200: Loss = -9883.490063763353
Iteration 15300: Loss = -9883.49002680821
Iteration 15400: Loss = -9883.490014148036
Iteration 15500: Loss = -9883.49005415894
Iteration 15600: Loss = -9883.489927909995
Iteration 15700: Loss = -9883.48991154827
Iteration 15800: Loss = -9883.498461788893
1
Iteration 15900: Loss = -9883.489867922095
Iteration 16000: Loss = -9883.507661199978
1
Iteration 16100: Loss = -9883.489837928757
Iteration 16200: Loss = -9883.489850283999
Iteration 16300: Loss = -9883.490515889649
1
Iteration 16400: Loss = -9883.489813601245
Iteration 16500: Loss = -9883.490132303716
1
Iteration 16600: Loss = -9883.490328693317
2
Iteration 16700: Loss = -9883.48975950985
Iteration 16800: Loss = -9883.489737163121
Iteration 16900: Loss = -9883.490710664313
1
Iteration 17000: Loss = -9883.48970868444
Iteration 17100: Loss = -9883.508253488053
1
Iteration 17200: Loss = -9883.489688922518
Iteration 17300: Loss = -9883.490982380947
1
Iteration 17400: Loss = -9883.489667357
Iteration 17500: Loss = -9883.673254430209
1
Iteration 17600: Loss = -9883.489652097216
Iteration 17700: Loss = -9883.489643541989
Iteration 17800: Loss = -9883.490222308528
1
Iteration 17900: Loss = -9883.489638607833
Iteration 18000: Loss = -9883.509109220895
1
Iteration 18100: Loss = -9883.489629613332
Iteration 18200: Loss = -9883.499060906204
1
Iteration 18300: Loss = -9883.490109240884
2
Iteration 18400: Loss = -9883.490476643881
3
Iteration 18500: Loss = -9883.49003854212
4
Iteration 18600: Loss = -9883.59741002946
5
Iteration 18700: Loss = -9883.489591037714
Iteration 18800: Loss = -9883.489591152596
Iteration 18900: Loss = -9883.490040735744
1
Iteration 19000: Loss = -9883.48980470862
2
Iteration 19100: Loss = -9883.490916910849
3
Iteration 19200: Loss = -9883.490281310937
4
Iteration 19300: Loss = -9883.491560953405
5
Iteration 19400: Loss = -9883.489638067698
Iteration 19500: Loss = -9883.491192566882
1
Iteration 19600: Loss = -9883.489545875218
Iteration 19700: Loss = -9883.495359945817
1
Iteration 19800: Loss = -9883.48953746095
Iteration 19900: Loss = -9883.489544480497
pi: tensor([[1.0000e+00, 1.7059e-07],
        [6.5002e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0100, 0.9900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0011, 0.1812],
         [0.6372, 0.1385]],

        [[0.6199, 0.0608],
         [0.6726, 0.6579]],

        [[0.5425, 0.0411],
         [0.6929, 0.6645]],

        [[0.5552, 0.1113],
         [0.7096, 0.6435]],

        [[0.7179, 0.0515],
         [0.7094, 0.6122]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: 0.000259217264300805
Average Adjusted Rand Index: 3.205467927426451e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21004.791204002813
Iteration 100: Loss = -9891.951297754233
Iteration 200: Loss = -9891.375191989428
Iteration 300: Loss = -9891.194528453612
Iteration 400: Loss = -9891.075843009343
Iteration 500: Loss = -9890.96733186547
Iteration 600: Loss = -9890.86650996446
Iteration 700: Loss = -9890.788484515291
Iteration 800: Loss = -9890.727267197673
Iteration 900: Loss = -9890.661367878605
Iteration 1000: Loss = -9890.548021408162
Iteration 1100: Loss = -9890.410988099004
Iteration 1200: Loss = -9890.331639504331
Iteration 1300: Loss = -9890.22521268087
Iteration 1400: Loss = -9890.061297492744
Iteration 1500: Loss = -9889.835129214105
Iteration 1600: Loss = -9889.593069709079
Iteration 1700: Loss = -9889.418337099349
Iteration 1800: Loss = -9889.248913067906
Iteration 1900: Loss = -9889.146390686326
Iteration 2000: Loss = -9889.083745629612
Iteration 2100: Loss = -9889.022766004831
Iteration 2200: Loss = -9888.912355034257
Iteration 2300: Loss = -9888.589471960662
Iteration 2400: Loss = -9887.545794370142
Iteration 2500: Loss = -9886.00456997065
Iteration 2600: Loss = -9885.891973910379
Iteration 2700: Loss = -9885.860585343373
Iteration 2800: Loss = -9885.831295435493
Iteration 2900: Loss = -9885.766403375905
Iteration 3000: Loss = -9882.803602817194
Iteration 3100: Loss = -9882.695083301734
Iteration 3200: Loss = -9882.674515750592
Iteration 3300: Loss = -9882.664434123457
Iteration 3400: Loss = -9882.65834430134
Iteration 3500: Loss = -9882.654292141096
Iteration 3600: Loss = -9882.651309510713
Iteration 3700: Loss = -9882.650438205535
Iteration 3800: Loss = -9882.647399265185
Iteration 3900: Loss = -9882.646034995056
Iteration 4000: Loss = -9882.64487538052
Iteration 4100: Loss = -9882.643936488194
Iteration 4200: Loss = -9882.643866469863
Iteration 4300: Loss = -9882.642420646498
Iteration 4400: Loss = -9882.641811929392
Iteration 4500: Loss = -9882.641302995464
Iteration 4600: Loss = -9882.64085914144
Iteration 4700: Loss = -9882.650934671608
1
Iteration 4800: Loss = -9882.640038637455
Iteration 4900: Loss = -9882.639732987443
Iteration 5000: Loss = -9882.639596085875
Iteration 5100: Loss = -9882.639206605454
Iteration 5200: Loss = -9882.638989681975
Iteration 5300: Loss = -9882.638894817079
Iteration 5400: Loss = -9882.638550696058
Iteration 5500: Loss = -9882.6392288285
1
Iteration 5600: Loss = -9882.638191745318
Iteration 5700: Loss = -9882.639320919117
1
Iteration 5800: Loss = -9882.637944414471
Iteration 5900: Loss = -9882.645419161867
1
Iteration 6000: Loss = -9882.637653240963
Iteration 6100: Loss = -9882.640673066475
1
Iteration 6200: Loss = -9882.637441538738
Iteration 6300: Loss = -9882.637391163034
Iteration 6400: Loss = -9882.637252594794
Iteration 6500: Loss = -9882.637119424686
Iteration 6600: Loss = -9882.637110835129
Iteration 6700: Loss = -9882.637613339239
1
Iteration 6800: Loss = -9882.636947245457
Iteration 6900: Loss = -9882.637032972594
Iteration 7000: Loss = -9882.639097130474
1
Iteration 7100: Loss = -9882.636763533881
Iteration 7200: Loss = -9882.636764101679
Iteration 7300: Loss = -9882.636682975874
Iteration 7400: Loss = -9882.63660531375
Iteration 7500: Loss = -9882.637461642807
1
Iteration 7600: Loss = -9882.636487687629
Iteration 7700: Loss = -9882.6365094762
Iteration 7800: Loss = -9882.6364970445
Iteration 7900: Loss = -9882.636433853302
Iteration 8000: Loss = -9882.636363790236
Iteration 8100: Loss = -9882.636610271806
1
Iteration 8200: Loss = -9882.636310149817
Iteration 8300: Loss = -9882.723820954581
1
Iteration 8400: Loss = -9882.63623652961
Iteration 8500: Loss = -9882.636213857379
Iteration 8600: Loss = -9882.636660868848
1
Iteration 8700: Loss = -9882.63617940329
Iteration 8800: Loss = -9882.637114830959
1
Iteration 8900: Loss = -9882.636183913755
Iteration 9000: Loss = -9882.636120803008
Iteration 9100: Loss = -9882.652603114493
1
Iteration 9200: Loss = -9882.636088609272
Iteration 9300: Loss = -9882.64620080485
1
Iteration 9400: Loss = -9882.636093218489
Iteration 9500: Loss = -9882.636668702735
1
Iteration 9600: Loss = -9882.726039923657
2
Iteration 9700: Loss = -9882.636486002446
3
Iteration 9800: Loss = -9882.636030889636
Iteration 9900: Loss = -9882.636042509519
Iteration 10000: Loss = -9882.636459707515
1
Iteration 10100: Loss = -9882.671611058846
2
Iteration 10200: Loss = -9882.648204599887
3
Iteration 10300: Loss = -9882.64589566985
4
Iteration 10400: Loss = -9882.635984122731
Iteration 10500: Loss = -9882.636466650813
1
Iteration 10600: Loss = -9882.63627592983
2
Iteration 10700: Loss = -9882.63645577958
3
Iteration 10800: Loss = -9882.635972461216
Iteration 10900: Loss = -9882.635996237344
Iteration 11000: Loss = -9882.663956882374
1
Iteration 11100: Loss = -9882.63681174413
2
Iteration 11200: Loss = -9882.642049107353
3
Iteration 11300: Loss = -9882.636872314342
4
Iteration 11400: Loss = -9882.636208339754
5
Iteration 11500: Loss = -9882.641401031231
6
Iteration 11600: Loss = -9882.636772595342
7
Iteration 11700: Loss = -9882.63731821383
8
Iteration 11800: Loss = -9882.636588106208
9
Iteration 11900: Loss = -9882.653921476416
10
Iteration 12000: Loss = -9882.654738033996
11
Iteration 12100: Loss = -9882.641311943555
12
Iteration 12200: Loss = -9882.638566132286
13
Iteration 12300: Loss = -9882.734342244763
14
Iteration 12400: Loss = -9882.672493602127
15
Stopping early at iteration 12400 due to no improvement.
pi: tensor([[1.0000e+00, 1.6766e-06],
        [1.9384e-01, 8.0616e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2197, 0.7803], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1449, 0.0836],
         [0.6992, 0.1770]],

        [[0.6504, 0.1085],
         [0.7247, 0.5164]],

        [[0.6625, 0.1096],
         [0.5135, 0.7101]],

        [[0.5083, 0.1057],
         [0.7165, 0.6349]],

        [[0.5938, 0.1179],
         [0.5934, 0.5322]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 67
Adjusted Rand Index: 0.10963183521056293
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 79
Adjusted Rand Index: 0.3300277108380656
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 85
Adjusted Rand Index: 0.4847980212325664
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 85
Adjusted Rand Index: 0.4849271883524868
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 67
Adjusted Rand Index: 0.10769449772456777
Global Adjusted Rand Index: 0.2815894086255154
Average Adjusted Rand Index: 0.3034158506716499
9986.422973457491
[0.000259217264300805, 0.2815894086255154] [3.205467927426451e-05, 0.3034158506716499] [9883.536857537554, 9882.672493602127]
-------------------------------------
This iteration is 12
True Objective function: Loss = -9963.45983654651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22550.17088206796
Iteration 100: Loss = -9867.370708751223
Iteration 200: Loss = -9858.239295190111
Iteration 300: Loss = -9854.632336366549
Iteration 400: Loss = -9851.883553896108
Iteration 500: Loss = -9850.672508746216
Iteration 600: Loss = -9849.936790769218
Iteration 700: Loss = -9849.648611469984
Iteration 800: Loss = -9849.548138073953
Iteration 900: Loss = -9849.494698846775
Iteration 1000: Loss = -9849.457095650869
Iteration 1100: Loss = -9849.424891261007
Iteration 1200: Loss = -9849.393720057278
Iteration 1300: Loss = -9849.364209411406
Iteration 1400: Loss = -9849.333997542368
Iteration 1500: Loss = -9849.29838358371
Iteration 1600: Loss = -9849.2467167636
Iteration 1700: Loss = -9849.161372150245
Iteration 1800: Loss = -9849.027349070511
Iteration 1900: Loss = -9848.902391242988
Iteration 2000: Loss = -9848.825393999656
Iteration 2100: Loss = -9848.775857845043
Iteration 2200: Loss = -9848.740523319515
Iteration 2300: Loss = -9848.718778220069
Iteration 2400: Loss = -9848.701815203312
Iteration 2500: Loss = -9848.686373117358
Iteration 2600: Loss = -9848.667818074357
Iteration 2700: Loss = -9848.60896865286
Iteration 2800: Loss = -9835.231493151681
Iteration 2900: Loss = -9832.743279551927
Iteration 3000: Loss = -9832.652781970452
Iteration 3100: Loss = -9832.60603037916
Iteration 3200: Loss = -9832.583952199691
Iteration 3300: Loss = -9832.570245493129
Iteration 3400: Loss = -9832.560238867356
Iteration 3500: Loss = -9832.552509376306
Iteration 3600: Loss = -9832.546358111811
Iteration 3700: Loss = -9832.541342603463
Iteration 3800: Loss = -9832.537331658912
Iteration 3900: Loss = -9832.534103762335
Iteration 4000: Loss = -9832.531356182531
Iteration 4100: Loss = -9832.529070322184
Iteration 4200: Loss = -9832.527101007028
Iteration 4300: Loss = -9832.525299810868
Iteration 4400: Loss = -9832.523761167118
Iteration 4500: Loss = -9832.522341584478
Iteration 4600: Loss = -9832.521110357618
Iteration 4700: Loss = -9832.520069428008
Iteration 4800: Loss = -9832.519027151648
Iteration 4900: Loss = -9832.518340098237
Iteration 5000: Loss = -9832.517324062483
Iteration 5100: Loss = -9832.516554176203
Iteration 5200: Loss = -9832.516138833427
Iteration 5300: Loss = -9832.515824450433
Iteration 5400: Loss = -9832.51567610658
Iteration 5500: Loss = -9832.520147401843
1
Iteration 5600: Loss = -9832.513740081647
Iteration 5700: Loss = -9832.516795292124
1
Iteration 5800: Loss = -9832.512854444269
Iteration 5900: Loss = -9832.512707902502
Iteration 6000: Loss = -9832.512285459889
Iteration 6100: Loss = -9832.511805958995
Iteration 6200: Loss = -9832.513030329594
1
Iteration 6300: Loss = -9832.511189625242
Iteration 6400: Loss = -9832.51094467526
Iteration 6500: Loss = -9832.510665665857
Iteration 6600: Loss = -9832.510608422275
Iteration 6700: Loss = -9832.510200398912
Iteration 6800: Loss = -9832.510370394379
1
Iteration 6900: Loss = -9832.5097882584
Iteration 7000: Loss = -9832.509715824777
Iteration 7100: Loss = -9832.509456670823
Iteration 7200: Loss = -9832.50928109432
Iteration 7300: Loss = -9832.509120923303
Iteration 7400: Loss = -9832.509021822734
Iteration 7500: Loss = -9832.508861339447
Iteration 7600: Loss = -9832.508739401606
Iteration 7700: Loss = -9832.508609372195
Iteration 7800: Loss = -9832.508788522524
1
Iteration 7900: Loss = -9832.508402091578
Iteration 8000: Loss = -9832.509829957286
1
Iteration 8100: Loss = -9832.5081827103
Iteration 8200: Loss = -9832.510165159916
1
Iteration 8300: Loss = -9832.50800889265
Iteration 8400: Loss = -9832.520764933253
1
Iteration 8500: Loss = -9832.507837182333
Iteration 8600: Loss = -9832.532091312713
1
Iteration 8700: Loss = -9832.507769573318
Iteration 8800: Loss = -9832.549876324458
1
Iteration 8900: Loss = -9832.507622873492
Iteration 9000: Loss = -9832.507543836538
Iteration 9100: Loss = -9832.507575163903
Iteration 9200: Loss = -9832.507466617595
Iteration 9300: Loss = -9832.50742571705
Iteration 9400: Loss = -9832.50810068143
1
Iteration 9500: Loss = -9832.50732840171
Iteration 9600: Loss = -9832.515688333311
1
Iteration 9700: Loss = -9832.507225614398
Iteration 9800: Loss = -9832.50744970497
1
Iteration 9900: Loss = -9832.50719253948
Iteration 10000: Loss = -9832.50711479744
Iteration 10100: Loss = -9832.507965997871
1
Iteration 10200: Loss = -9832.507063314935
Iteration 10300: Loss = -9832.512275753359
1
Iteration 10400: Loss = -9832.507021303987
Iteration 10500: Loss = -9832.508959396324
1
Iteration 10600: Loss = -9832.507000021336
Iteration 10700: Loss = -9832.506932859695
Iteration 10800: Loss = -9832.549031018802
1
Iteration 10900: Loss = -9832.50690836707
Iteration 11000: Loss = -9832.507451580286
1
Iteration 11100: Loss = -9832.506903592055
Iteration 11200: Loss = -9832.506836493005
Iteration 11300: Loss = -9832.509544871215
1
Iteration 11400: Loss = -9832.506799951518
Iteration 11500: Loss = -9832.506803289985
Iteration 11600: Loss = -9832.506948388793
1
Iteration 11700: Loss = -9832.506768690548
Iteration 11800: Loss = -9832.506761318351
Iteration 11900: Loss = -9832.507610143291
1
Iteration 12000: Loss = -9832.506721647824
Iteration 12100: Loss = -9832.5069378982
1
Iteration 12200: Loss = -9832.506721236332
Iteration 12300: Loss = -9832.50674464181
Iteration 12400: Loss = -9832.50669672161
Iteration 12500: Loss = -9832.536149436288
1
Iteration 12600: Loss = -9832.51058039203
2
Iteration 12700: Loss = -9832.757106567937
3
Iteration 12800: Loss = -9832.506647782533
Iteration 12900: Loss = -9832.50976724978
1
Iteration 13000: Loss = -9832.506626916032
Iteration 13100: Loss = -9832.584414680554
1
Iteration 13200: Loss = -9832.506641263637
Iteration 13300: Loss = -9832.50661946532
Iteration 13400: Loss = -9832.507585529385
1
Iteration 13500: Loss = -9832.506587501886
Iteration 13600: Loss = -9832.512970581272
1
Iteration 13700: Loss = -9832.50660721771
Iteration 13800: Loss = -9832.781001606374
1
Iteration 13900: Loss = -9832.506582252357
Iteration 14000: Loss = -9832.506569460336
Iteration 14100: Loss = -9832.510344588309
1
Iteration 14200: Loss = -9832.506551091281
Iteration 14300: Loss = -9832.507159244726
1
Iteration 14400: Loss = -9832.510350029403
2
Iteration 14500: Loss = -9832.508406451072
3
Iteration 14600: Loss = -9832.506524359173
Iteration 14700: Loss = -9832.555245068108
1
Iteration 14800: Loss = -9832.50653411896
Iteration 14900: Loss = -9832.655667992025
1
Iteration 15000: Loss = -9832.50651965589
Iteration 15100: Loss = -9832.506523295253
Iteration 15200: Loss = -9832.506977210558
1
Iteration 15300: Loss = -9832.506565673697
Iteration 15400: Loss = -9832.549213567445
1
Iteration 15500: Loss = -9832.506507113188
Iteration 15600: Loss = -9832.863996376305
1
Iteration 15700: Loss = -9832.506545486262
Iteration 15800: Loss = -9832.50653572326
Iteration 15900: Loss = -9832.50688013295
1
Iteration 16000: Loss = -9832.506520963609
Iteration 16100: Loss = -9832.508257203974
1
Iteration 16200: Loss = -9832.506551384677
Iteration 16300: Loss = -9832.50654885663
Iteration 16400: Loss = -9832.508666152338
1
Iteration 16500: Loss = -9832.50653206775
Iteration 16600: Loss = -9832.510764675984
1
Iteration 16700: Loss = -9832.50651705517
Iteration 16800: Loss = -9832.64346974373
1
Iteration 16900: Loss = -9832.506533809088
Iteration 17000: Loss = -9832.52914972382
1
Iteration 17100: Loss = -9832.506502400662
Iteration 17200: Loss = -9832.506670888468
1
Iteration 17300: Loss = -9832.543548064374
2
Iteration 17400: Loss = -9832.506517068094
Iteration 17500: Loss = -9832.509238050066
1
Iteration 17600: Loss = -9832.506522247637
Iteration 17700: Loss = -9832.538281056979
1
Iteration 17800: Loss = -9832.506495481477
Iteration 17900: Loss = -9832.50650484209
Iteration 18000: Loss = -9832.50653266548
Iteration 18100: Loss = -9832.506514970459
Iteration 18200: Loss = -9832.528766927368
1
Iteration 18300: Loss = -9832.530100185335
2
Iteration 18400: Loss = -9832.506482346429
Iteration 18500: Loss = -9832.507558432491
1
Iteration 18600: Loss = -9832.506478183022
Iteration 18700: Loss = -9832.506482602415
Iteration 18800: Loss = -9832.509591479835
1
Iteration 18900: Loss = -9832.506623178802
2
Iteration 19000: Loss = -9832.506514202449
Iteration 19100: Loss = -9832.5065134382
Iteration 19200: Loss = -9832.506738212864
1
Iteration 19300: Loss = -9832.506492411292
Iteration 19400: Loss = -9832.546526268185
1
Iteration 19500: Loss = -9832.506513154673
Iteration 19600: Loss = -9832.507098187329
1
Iteration 19700: Loss = -9832.506519760891
Iteration 19800: Loss = -9832.506489949923
Iteration 19900: Loss = -9832.509062000709
1
pi: tensor([[7.0706e-01, 2.9294e-01],
        [3.9161e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5996, 0.4004], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2138, 0.0922],
         [0.6688, 0.1282]],

        [[0.5564, 0.0957],
         [0.6336, 0.7251]],

        [[0.6834, 0.1366],
         [0.6729, 0.6789]],

        [[0.7032, 0.1446],
         [0.5507, 0.7249]],

        [[0.6738, 0.1536],
         [0.7045, 0.6692]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7367892279983828
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 13
Adjusted Rand Index: 0.5430499964984674
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 32
Adjusted Rand Index: 0.12119414207752995
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.07045526565965987
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
Global Adjusted Rand Index: 0.20995811343179624
Average Adjusted Rand Index: 0.2930047971538787
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22277.446888001403
Iteration 100: Loss = -9866.98156525136
Iteration 200: Loss = -9856.566917527389
Iteration 300: Loss = -9852.789766002525
Iteration 400: Loss = -9851.345644856563
Iteration 500: Loss = -9850.513624953312
Iteration 600: Loss = -9850.09733229446
Iteration 700: Loss = -9849.856068594112
Iteration 800: Loss = -9849.6825910816
Iteration 900: Loss = -9849.54290739063
Iteration 1000: Loss = -9849.44360048382
Iteration 1100: Loss = -9849.380003017275
Iteration 1200: Loss = -9849.33204329165
Iteration 1300: Loss = -9849.286444246925
Iteration 1400: Loss = -9849.232178398766
Iteration 1500: Loss = -9849.159902493959
Iteration 1600: Loss = -9849.064496001107
Iteration 1700: Loss = -9848.966955387541
Iteration 1800: Loss = -9848.891400798826
Iteration 1900: Loss = -9848.837120547463
Iteration 2000: Loss = -9848.796883712605
Iteration 2100: Loss = -9848.766510416493
Iteration 2200: Loss = -9848.743120947687
Iteration 2300: Loss = -9848.724516459355
Iteration 2400: Loss = -9848.709382832203
Iteration 2500: Loss = -9848.696814116434
Iteration 2600: Loss = -9848.686174290084
Iteration 2700: Loss = -9848.684546216664
Iteration 2800: Loss = -9848.663154462243
Iteration 2900: Loss = -9848.637141102714
Iteration 3000: Loss = -9848.408769324387
Iteration 3100: Loss = -9833.395308194853
Iteration 3200: Loss = -9832.765949730087
Iteration 3300: Loss = -9832.673468781592
Iteration 3400: Loss = -9832.631950787665
Iteration 3500: Loss = -9832.60647292531
Iteration 3600: Loss = -9832.589373924093
Iteration 3700: Loss = -9832.576976280883
Iteration 3800: Loss = -9832.567204851675
Iteration 3900: Loss = -9832.559575414609
Iteration 4000: Loss = -9832.55339306678
Iteration 4100: Loss = -9832.548427861982
Iteration 4200: Loss = -9832.54392947991
Iteration 4300: Loss = -9832.540318439082
Iteration 4400: Loss = -9832.538346900377
Iteration 4500: Loss = -9832.534486643142
Iteration 4600: Loss = -9832.532110339902
Iteration 4700: Loss = -9832.530034919533
Iteration 4800: Loss = -9832.528193969018
Iteration 4900: Loss = -9832.526524452884
Iteration 5000: Loss = -9832.525026537687
Iteration 5100: Loss = -9832.523673303638
Iteration 5200: Loss = -9832.522633478697
Iteration 5300: Loss = -9832.521349465462
Iteration 5400: Loss = -9832.520304733296
Iteration 5500: Loss = -9832.519495058428
Iteration 5600: Loss = -9832.51855245142
Iteration 5700: Loss = -9832.517922778829
Iteration 5800: Loss = -9832.517895292889
Iteration 5900: Loss = -9832.516542059024
Iteration 6000: Loss = -9832.527871553628
1
Iteration 6100: Loss = -9832.515141853683
Iteration 6200: Loss = -9832.514693880232
Iteration 6300: Loss = -9832.514129149842
Iteration 6400: Loss = -9832.513732692441
Iteration 6500: Loss = -9832.513290218098
Iteration 6600: Loss = -9832.512913083829
Iteration 6700: Loss = -9832.512485524969
Iteration 6800: Loss = -9832.512183909688
Iteration 6900: Loss = -9832.511840952895
Iteration 7000: Loss = -9832.511562545602
Iteration 7100: Loss = -9832.511260478639
Iteration 7200: Loss = -9832.510999416363
Iteration 7300: Loss = -9832.510730411168
Iteration 7400: Loss = -9832.51131570547
1
Iteration 7500: Loss = -9832.510303380463
Iteration 7600: Loss = -9832.522761538172
1
Iteration 7700: Loss = -9832.509856502895
Iteration 7800: Loss = -9832.509721902028
Iteration 7900: Loss = -9832.50965384143
Iteration 8000: Loss = -9832.509391241432
Iteration 8100: Loss = -9832.552751401614
1
Iteration 8200: Loss = -9832.509078207833
Iteration 8300: Loss = -9832.517769704787
1
Iteration 8400: Loss = -9832.508828871769
Iteration 8500: Loss = -9832.524557451603
1
Iteration 8600: Loss = -9832.508603050519
Iteration 8700: Loss = -9832.509221573353
1
Iteration 8800: Loss = -9832.508406941304
Iteration 8900: Loss = -9832.508281109294
Iteration 9000: Loss = -9832.50879832113
1
Iteration 9100: Loss = -9832.508110499608
Iteration 9200: Loss = -9832.508051887218
Iteration 9300: Loss = -9832.507987281784
Iteration 9400: Loss = -9832.507856000577
Iteration 9500: Loss = -9832.53844796401
1
Iteration 9600: Loss = -9832.507725992056
Iteration 9700: Loss = -9832.50940677471
1
Iteration 9800: Loss = -9832.507625247106
Iteration 9900: Loss = -9832.508878959134
1
Iteration 10000: Loss = -9832.507535314302
Iteration 10100: Loss = -9832.507477910573
Iteration 10200: Loss = -9832.50744629346
Iteration 10300: Loss = -9832.507354902791
Iteration 10400: Loss = -9832.522518753645
1
Iteration 10500: Loss = -9832.507281749615
Iteration 10600: Loss = -9832.507580248575
1
Iteration 10700: Loss = -9832.516234484392
2
Iteration 10800: Loss = -9832.507158314655
Iteration 10900: Loss = -9832.511071836047
1
Iteration 11000: Loss = -9832.507125821932
Iteration 11100: Loss = -9832.507053545512
Iteration 11200: Loss = -9832.508686234452
1
Iteration 11300: Loss = -9832.50884119372
2
Iteration 11400: Loss = -9832.61612190246
3
Iteration 11500: Loss = -9832.507000035499
Iteration 11600: Loss = -9832.518618868155
1
Iteration 11700: Loss = -9832.506931377071
Iteration 11800: Loss = -9832.57007152783
1
Iteration 11900: Loss = -9832.506861644848
Iteration 12000: Loss = -9832.515655802034
1
Iteration 12100: Loss = -9832.509067692428
2
Iteration 12200: Loss = -9832.506806349858
Iteration 12300: Loss = -9832.51228005128
1
Iteration 12400: Loss = -9832.50678350995
Iteration 12500: Loss = -9832.510682555148
1
Iteration 12600: Loss = -9832.506810967625
Iteration 12700: Loss = -9832.507599263701
1
Iteration 12800: Loss = -9832.506727272283
Iteration 12900: Loss = -9832.506880610186
1
Iteration 13000: Loss = -9832.506710633717
Iteration 13100: Loss = -9832.507601175084
1
Iteration 13200: Loss = -9832.506669649161
Iteration 13300: Loss = -9832.506892492118
1
Iteration 13400: Loss = -9832.506709048861
Iteration 13500: Loss = -9832.506953680075
1
Iteration 13600: Loss = -9832.50665364367
Iteration 13700: Loss = -9832.543918191124
1
Iteration 13800: Loss = -9832.506645361113
Iteration 13900: Loss = -9832.511337174548
1
Iteration 14000: Loss = -9832.522680088574
2
Iteration 14100: Loss = -9832.506612853656
Iteration 14200: Loss = -9832.50734039603
1
Iteration 14300: Loss = -9832.50659893311
Iteration 14400: Loss = -9832.506965101578
1
Iteration 14500: Loss = -9832.506599287313
Iteration 14600: Loss = -9832.510195483756
1
Iteration 14700: Loss = -9832.50659045475
Iteration 14800: Loss = -9832.525232567668
1
Iteration 14900: Loss = -9832.506555320651
Iteration 15000: Loss = -9832.508946434591
1
Iteration 15100: Loss = -9832.507164502202
2
Iteration 15200: Loss = -9832.506647043561
Iteration 15300: Loss = -9832.506777094812
1
Iteration 15400: Loss = -9832.512206336676
2
Iteration 15500: Loss = -9832.506570058851
Iteration 15600: Loss = -9832.506635929807
Iteration 15700: Loss = -9832.506567684535
Iteration 15800: Loss = -9832.508341700968
1
Iteration 15900: Loss = -9832.506538287735
Iteration 16000: Loss = -9832.619183292265
1
Iteration 16100: Loss = -9832.506564165207
Iteration 16200: Loss = -9832.50692384895
1
Iteration 16300: Loss = -9832.506579592191
Iteration 16400: Loss = -9832.507350260656
1
Iteration 16500: Loss = -9832.506886821937
2
Iteration 16600: Loss = -9832.50659930618
Iteration 16700: Loss = -9832.506982658606
1
Iteration 16800: Loss = -9832.50650037663
Iteration 16900: Loss = -9832.50663884441
1
Iteration 17000: Loss = -9832.506539817718
Iteration 17100: Loss = -9832.635987390457
1
Iteration 17200: Loss = -9832.506527228454
Iteration 17300: Loss = -9832.507318575657
1
Iteration 17400: Loss = -9832.541826664568
2
Iteration 17500: Loss = -9832.506513069902
Iteration 17600: Loss = -9832.50789013599
1
Iteration 17700: Loss = -9832.5064802617
Iteration 17800: Loss = -9832.863028205218
1
Iteration 17900: Loss = -9832.506554036956
Iteration 18000: Loss = -9832.50649412512
Iteration 18100: Loss = -9832.506957709256
1
Iteration 18200: Loss = -9832.506514037881
Iteration 18300: Loss = -9832.568687085515
1
Iteration 18400: Loss = -9832.50724347144
2
Iteration 18500: Loss = -9832.506533849877
Iteration 18600: Loss = -9832.506722256967
1
Iteration 18700: Loss = -9832.521809719223
2
Iteration 18800: Loss = -9832.506478632129
Iteration 18900: Loss = -9832.510507051407
1
Iteration 19000: Loss = -9832.507629267639
2
Iteration 19100: Loss = -9832.60212190432
3
Iteration 19200: Loss = -9832.506483728248
Iteration 19300: Loss = -9832.506905759356
1
Iteration 19400: Loss = -9832.506573079014
Iteration 19500: Loss = -9832.506508047865
Iteration 19600: Loss = -9832.508279420892
1
Iteration 19700: Loss = -9832.506507450937
Iteration 19800: Loss = -9832.597227619535
1
Iteration 19900: Loss = -9832.506476443548
pi: tensor([[7.0708e-01, 2.9292e-01],
        [6.1580e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5997, 0.4003], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2137, 0.0922],
         [0.5700, 0.1282]],

        [[0.6999, 0.0957],
         [0.7133, 0.5431]],

        [[0.5803, 0.1366],
         [0.6350, 0.6601]],

        [[0.7302, 0.1446],
         [0.6003, 0.5699]],

        [[0.6913, 0.1536],
         [0.6588, 0.5765]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7367892279983828
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 13
Adjusted Rand Index: 0.5430499964984674
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 32
Adjusted Rand Index: 0.12119414207752995
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.07045526565965987
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
Global Adjusted Rand Index: 0.20995811343179624
Average Adjusted Rand Index: 0.2930047971538787
9963.45983654651
[0.20995811343179624, 0.20995811343179624] [0.2930047971538787, 0.2930047971538787] [9832.506506772768, 9832.506495248928]
-------------------------------------
This iteration is 13
True Objective function: Loss = -10045.56715580235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23472.49813948406
Iteration 100: Loss = -9913.337005572168
Iteration 200: Loss = -9911.588625656934
Iteration 300: Loss = -9911.170719281006
Iteration 400: Loss = -9910.938987598796
Iteration 500: Loss = -9910.735881197994
Iteration 600: Loss = -9910.39516459051
Iteration 700: Loss = -9909.948312484696
Iteration 800: Loss = -9909.708833245824
Iteration 900: Loss = -9909.566024191583
Iteration 1000: Loss = -9909.475885886883
Iteration 1100: Loss = -9909.411754556226
Iteration 1200: Loss = -9909.362792324535
Iteration 1300: Loss = -9909.323559142043
Iteration 1400: Loss = -9909.291366295158
Iteration 1500: Loss = -9909.264394518148
Iteration 1600: Loss = -9909.241575017888
Iteration 1700: Loss = -9909.221984708838
Iteration 1800: Loss = -9909.205017404469
Iteration 1900: Loss = -9909.190156693177
Iteration 2000: Loss = -9909.17709318292
Iteration 2100: Loss = -9909.165498182225
Iteration 2200: Loss = -9909.155146245408
Iteration 2300: Loss = -9909.145888529754
Iteration 2400: Loss = -9909.137536181732
Iteration 2500: Loss = -9909.129936181836
Iteration 2600: Loss = -9909.122976517425
Iteration 2700: Loss = -9909.116516936696
Iteration 2800: Loss = -9909.110486774674
Iteration 2900: Loss = -9909.10470215516
Iteration 3000: Loss = -9909.099005546215
Iteration 3100: Loss = -9909.093262546294
Iteration 3200: Loss = -9909.086923796081
Iteration 3300: Loss = -9909.079198617528
Iteration 3400: Loss = -9909.067537773917
Iteration 3500: Loss = -9909.041086828825
Iteration 3600: Loss = -9908.880025020528
Iteration 3700: Loss = -9908.222969573924
Iteration 3800: Loss = -9908.053622395691
Iteration 3900: Loss = -9907.972732002392
Iteration 4000: Loss = -9907.922517857432
Iteration 4100: Loss = -9907.886345139183
Iteration 4200: Loss = -9907.857888736566
Iteration 4300: Loss = -9907.834287846363
Iteration 4400: Loss = -9907.814028120269
Iteration 4500: Loss = -9907.796303352085
Iteration 4600: Loss = -9907.780557696857
Iteration 4700: Loss = -9907.766479231022
Iteration 4800: Loss = -9907.75381128508
Iteration 4900: Loss = -9907.742453144707
Iteration 5000: Loss = -9907.732260267207
Iteration 5100: Loss = -9907.723080808073
Iteration 5200: Loss = -9907.714903570595
Iteration 5300: Loss = -9907.707628968225
Iteration 5400: Loss = -9907.701147391092
Iteration 5500: Loss = -9907.695484460635
Iteration 5600: Loss = -9907.690524632886
Iteration 5700: Loss = -9907.686176679197
Iteration 5800: Loss = -9907.682384913804
Iteration 5900: Loss = -9907.679069145635
Iteration 6000: Loss = -9907.676273056679
Iteration 6100: Loss = -9907.67381889086
Iteration 6200: Loss = -9907.671780597237
Iteration 6300: Loss = -9907.669977473102
Iteration 6400: Loss = -9907.66843564524
Iteration 6500: Loss = -9907.667145066202
Iteration 6600: Loss = -9907.666001946985
Iteration 6700: Loss = -9907.664992085272
Iteration 6800: Loss = -9907.664183118728
Iteration 6900: Loss = -9907.663429518683
Iteration 7000: Loss = -9907.662776727766
Iteration 7100: Loss = -9907.662209350237
Iteration 7200: Loss = -9907.661676735159
Iteration 7300: Loss = -9907.66121234532
Iteration 7400: Loss = -9907.660813727578
Iteration 7500: Loss = -9907.660430563
Iteration 7600: Loss = -9907.660079629344
Iteration 7700: Loss = -9907.678031376941
1
Iteration 7800: Loss = -9907.66130828679
2
Iteration 7900: Loss = -9907.66085832123
3
Iteration 8000: Loss = -9907.658993219926
Iteration 8100: Loss = -9907.660869630748
1
Iteration 8200: Loss = -9907.658530990902
Iteration 8300: Loss = -9907.679931932073
1
Iteration 8400: Loss = -9907.658125552385
Iteration 8500: Loss = -9907.657984096395
Iteration 8600: Loss = -9907.658480002703
1
Iteration 8700: Loss = -9907.657656971061
Iteration 8800: Loss = -9907.657499774094
Iteration 8900: Loss = -9907.657344548756
Iteration 9000: Loss = -9907.657251052799
Iteration 9100: Loss = -9907.657084566132
Iteration 9200: Loss = -9907.661758162978
1
Iteration 9300: Loss = -9907.656902681572
Iteration 9400: Loss = -9907.656748537484
Iteration 9500: Loss = -9907.680739461908
1
Iteration 9600: Loss = -9907.656578336846
Iteration 9700: Loss = -9907.656490696349
Iteration 9800: Loss = -9907.656434855891
Iteration 9900: Loss = -9907.656673775127
1
Iteration 10000: Loss = -9907.65622697929
Iteration 10100: Loss = -9907.65618340427
Iteration 10200: Loss = -9907.656117457409
Iteration 10300: Loss = -9907.656131954123
Iteration 10400: Loss = -9907.655993916791
Iteration 10500: Loss = -9907.655923379685
Iteration 10600: Loss = -9907.657775408305
1
Iteration 10700: Loss = -9907.655840169005
Iteration 10800: Loss = -9907.655759537343
Iteration 10900: Loss = -9907.661879644975
1
Iteration 11000: Loss = -9907.655682412833
Iteration 11100: Loss = -9907.655672569417
Iteration 11200: Loss = -9907.655589668917
Iteration 11300: Loss = -9907.656011668085
1
Iteration 11400: Loss = -9907.655537880282
Iteration 11500: Loss = -9907.655510636221
Iteration 11600: Loss = -9907.721898849515
1
Iteration 11700: Loss = -9907.655444995415
Iteration 11800: Loss = -9907.655394104142
Iteration 11900: Loss = -9907.671672138344
1
Iteration 12000: Loss = -9907.65536394838
Iteration 12100: Loss = -9907.655347892147
Iteration 12200: Loss = -9907.655277478698
Iteration 12300: Loss = -9907.655328396248
Iteration 12400: Loss = -9907.655245772778
Iteration 12500: Loss = -9907.655233908832
Iteration 12600: Loss = -9907.698593720093
1
Iteration 12700: Loss = -9907.655214171678
Iteration 12800: Loss = -9907.655177356828
Iteration 12900: Loss = -9907.655307225054
1
Iteration 13000: Loss = -9907.65517848498
Iteration 13100: Loss = -9907.655125676094
Iteration 13200: Loss = -9907.655121165439
Iteration 13300: Loss = -9907.656148987237
1
Iteration 13400: Loss = -9907.655101628023
Iteration 13500: Loss = -9907.655093927884
Iteration 13600: Loss = -9907.655904411498
1
Iteration 13700: Loss = -9907.655054285773
Iteration 13800: Loss = -9907.655032645687
Iteration 13900: Loss = -9907.655600888034
1
Iteration 14000: Loss = -9907.655083145426
Iteration 14100: Loss = -9907.655006082348
Iteration 14200: Loss = -9907.65501880592
Iteration 14300: Loss = -9907.725854133045
1
Iteration 14400: Loss = -9907.654972104474
Iteration 14500: Loss = -9907.654987009315
Iteration 14600: Loss = -9907.65497732104
Iteration 14700: Loss = -9907.65528060339
1
Iteration 14800: Loss = -9907.654957904311
Iteration 14900: Loss = -9907.65496254754
Iteration 15000: Loss = -9907.957860495151
1
Iteration 15100: Loss = -9907.65493052683
Iteration 15200: Loss = -9907.654917740214
Iteration 15300: Loss = -9907.65493211201
Iteration 15400: Loss = -9907.655458349342
1
Iteration 15500: Loss = -9907.654924853827
Iteration 15600: Loss = -9907.654906904148
Iteration 15700: Loss = -9907.65588171478
1
Iteration 15800: Loss = -9907.654888976573
Iteration 15900: Loss = -9907.654904802313
Iteration 16000: Loss = -9907.681824419138
1
Iteration 16100: Loss = -9907.654887937177
Iteration 16200: Loss = -9907.654887669234
Iteration 16300: Loss = -9907.693451089528
1
Iteration 16400: Loss = -9907.654880088745
Iteration 16500: Loss = -9907.654886605116
Iteration 16600: Loss = -9907.654885072932
Iteration 16700: Loss = -9907.654995081783
1
Iteration 16800: Loss = -9907.65487481555
Iteration 16900: Loss = -9907.654859539658
Iteration 17000: Loss = -9907.678271271141
1
Iteration 17100: Loss = -9907.654869787859
Iteration 17200: Loss = -9907.654844529836
Iteration 17300: Loss = -9907.654851380596
Iteration 17400: Loss = -9907.655237090647
1
Iteration 17500: Loss = -9907.6548675668
Iteration 17600: Loss = -9907.65486008789
Iteration 17700: Loss = -9907.708383507113
1
Iteration 17800: Loss = -9907.65488114463
Iteration 17900: Loss = -9907.654857089703
Iteration 18000: Loss = -9907.654866390678
Iteration 18100: Loss = -9907.655171260998
1
Iteration 18200: Loss = -9907.654848786628
Iteration 18300: Loss = -9907.654879256877
Iteration 18400: Loss = -9907.752712215506
1
Iteration 18500: Loss = -9907.65487040098
Iteration 18600: Loss = -9907.654866783849
Iteration 18700: Loss = -9907.654832354357
Iteration 18800: Loss = -9907.654918481192
Iteration 18900: Loss = -9907.654817995024
Iteration 19000: Loss = -9907.654860076173
Iteration 19100: Loss = -9907.726938032316
1
Iteration 19200: Loss = -9907.654847557547
Iteration 19300: Loss = -9907.654806763087
Iteration 19400: Loss = -9907.65841651335
1
Iteration 19500: Loss = -9907.654864477267
Iteration 19600: Loss = -9907.654840874271
Iteration 19700: Loss = -9907.654808225006
Iteration 19800: Loss = -9907.654831860751
Iteration 19900: Loss = -9907.654817970446
pi: tensor([[1.2231e-05, 9.9999e-01],
        [8.8496e-03, 9.9115e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0390, 0.9610], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0345, 0.0643],
         [0.5476, 0.1375]],

        [[0.5619, 0.1528],
         [0.7058, 0.6386]],

        [[0.6180, 0.1917],
         [0.6379, 0.7211]],

        [[0.6128, 0.2651],
         [0.6097, 0.6836]],

        [[0.5339, 0.2636],
         [0.5547, 0.5435]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00020758318605064086
Average Adjusted Rand Index: 1.427637013092014e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21296.971826475386
Iteration 100: Loss = -9912.458724343753
Iteration 200: Loss = -9910.90628058978
Iteration 300: Loss = -9910.042850091577
Iteration 400: Loss = -9909.603750310142
Iteration 500: Loss = -9909.405443410844
Iteration 600: Loss = -9909.295274247483
Iteration 700: Loss = -9909.23200672972
Iteration 800: Loss = -9909.19100728712
Iteration 900: Loss = -9909.1624665032
Iteration 1000: Loss = -9909.141652537082
Iteration 1100: Loss = -9909.125940871198
Iteration 1200: Loss = -9909.113705456757
Iteration 1300: Loss = -9909.104029119268
Iteration 1400: Loss = -9909.09614194075
Iteration 1500: Loss = -9909.089651479255
Iteration 1600: Loss = -9909.084215683568
Iteration 1700: Loss = -9909.07956081718
Iteration 1800: Loss = -9909.075545907674
Iteration 1900: Loss = -9909.071948152534
Iteration 2000: Loss = -9909.068673393915
Iteration 2100: Loss = -9909.065547315831
Iteration 2200: Loss = -9909.062501502458
Iteration 2300: Loss = -9909.059150789604
Iteration 2400: Loss = -9909.055018209227
Iteration 2500: Loss = -9909.048150508555
Iteration 2600: Loss = -9909.028002947072
Iteration 2700: Loss = -9908.61446808744
Iteration 2800: Loss = -9908.023700512356
Iteration 2900: Loss = -9907.951684698337
Iteration 3000: Loss = -9907.909183792297
Iteration 3100: Loss = -9907.876108894621
Iteration 3200: Loss = -9907.848079819843
Iteration 3300: Loss = -9907.823501807852
Iteration 3400: Loss = -9907.80154922708
Iteration 3500: Loss = -9907.781713028906
Iteration 3600: Loss = -9907.763763146471
Iteration 3700: Loss = -9907.747606420726
Iteration 3800: Loss = -9907.733164042427
Iteration 3900: Loss = -9907.720407837709
Iteration 4000: Loss = -9907.709304315105
Iteration 4100: Loss = -9907.699849739729
Iteration 4200: Loss = -9907.691843646044
Iteration 4300: Loss = -9907.685188212105
Iteration 4400: Loss = -9907.679676102583
Iteration 4500: Loss = -9907.675200674783
Iteration 4600: Loss = -9907.671622902191
Iteration 4700: Loss = -9907.668732827578
Iteration 4800: Loss = -9907.66642290537
Iteration 4900: Loss = -9907.664552018863
Iteration 5000: Loss = -9907.663101830696
Iteration 5100: Loss = -9907.66192970808
Iteration 5200: Loss = -9907.66102603029
Iteration 5300: Loss = -9907.660262009682
Iteration 5400: Loss = -9907.659658498122
Iteration 5500: Loss = -9907.659137616589
Iteration 5600: Loss = -9907.65875433119
Iteration 5700: Loss = -9907.658422481014
Iteration 5800: Loss = -9907.658134304304
Iteration 5900: Loss = -9907.657865922165
Iteration 6000: Loss = -9907.657664100843
Iteration 6100: Loss = -9907.657476182112
Iteration 6200: Loss = -9907.6573260931
Iteration 6300: Loss = -9907.657154110231
Iteration 6400: Loss = -9907.657019260501
Iteration 6500: Loss = -9907.656902345841
Iteration 6600: Loss = -9907.656775602127
Iteration 6700: Loss = -9907.6566831566
Iteration 6800: Loss = -9907.656580464376
Iteration 6900: Loss = -9907.656478256658
Iteration 7000: Loss = -9907.656398017618
Iteration 7100: Loss = -9907.656297867186
Iteration 7200: Loss = -9907.657116628085
1
Iteration 7300: Loss = -9907.656155615283
Iteration 7400: Loss = -9907.656132422693
Iteration 7500: Loss = -9907.656024263822
Iteration 7600: Loss = -9907.656382129013
1
Iteration 7700: Loss = -9907.6558854222
Iteration 7800: Loss = -9907.656180876944
1
Iteration 7900: Loss = -9907.655792721085
Iteration 8000: Loss = -9907.655738776963
Iteration 8100: Loss = -9907.655732174679
Iteration 8200: Loss = -9907.656156212863
1
Iteration 8300: Loss = -9907.6556286855
Iteration 8400: Loss = -9907.655606554352
Iteration 8500: Loss = -9907.658845632272
1
Iteration 8600: Loss = -9907.65551168461
Iteration 8700: Loss = -9907.655739041893
1
Iteration 8800: Loss = -9907.655446371195
Iteration 8900: Loss = -9907.65614263069
1
Iteration 9000: Loss = -9907.655380590026
Iteration 9100: Loss = -9907.65536200341
Iteration 9200: Loss = -9907.655324867153
Iteration 9300: Loss = -9907.655303373256
Iteration 9400: Loss = -9907.655255838663
Iteration 9500: Loss = -9907.655875439172
1
Iteration 9600: Loss = -9907.655241788309
Iteration 9700: Loss = -9907.655205688561
Iteration 9800: Loss = -9907.65907811222
1
Iteration 9900: Loss = -9907.655185784783
Iteration 10000: Loss = -9907.655180846547
Iteration 10100: Loss = -9907.655192178361
Iteration 10200: Loss = -9907.655213169613
Iteration 10300: Loss = -9907.655112262699
Iteration 10400: Loss = -9907.655104559954
Iteration 10500: Loss = -9907.65516324452
Iteration 10600: Loss = -9907.65505473274
Iteration 10700: Loss = -9907.655051455447
Iteration 10800: Loss = -9907.729014452123
1
Iteration 10900: Loss = -9907.655053963423
Iteration 11000: Loss = -9907.655027065368
Iteration 11100: Loss = -9907.654988517641
Iteration 11200: Loss = -9907.656031616218
1
Iteration 11300: Loss = -9907.655028432206
Iteration 11400: Loss = -9907.655016122142
Iteration 11500: Loss = -9907.854399652437
1
Iteration 11600: Loss = -9907.654951795956
Iteration 11700: Loss = -9907.654971717435
Iteration 11800: Loss = -9907.654986161864
Iteration 11900: Loss = -9907.65509732464
1
Iteration 12000: Loss = -9907.654964884516
Iteration 12100: Loss = -9907.654943176549
Iteration 12200: Loss = -9907.679560425417
1
Iteration 12300: Loss = -9907.654938995927
Iteration 12400: Loss = -9907.654935958635
Iteration 12500: Loss = -9907.65493778936
Iteration 12600: Loss = -9907.65562112405
1
Iteration 12700: Loss = -9907.654924168697
Iteration 12800: Loss = -9907.65489726362
Iteration 12900: Loss = -9907.666419959138
1
Iteration 13000: Loss = -9907.654887880595
Iteration 13100: Loss = -9907.654897312443
Iteration 13200: Loss = -9907.654883171483
Iteration 13300: Loss = -9907.655402567327
1
Iteration 13400: Loss = -9907.65489281404
Iteration 13500: Loss = -9907.654899640245
Iteration 13600: Loss = -9907.65513943744
1
Iteration 13700: Loss = -9907.65488492318
Iteration 13800: Loss = -9907.654876194298
Iteration 13900: Loss = -9907.71849843489
1
Iteration 14000: Loss = -9907.654869828984
Iteration 14100: Loss = -9907.654854988912
Iteration 14200: Loss = -9907.6548463643
Iteration 14300: Loss = -9907.654899340087
Iteration 14400: Loss = -9907.654875678296
Iteration 14500: Loss = -9907.65484075264
Iteration 14600: Loss = -9907.65488016697
Iteration 14700: Loss = -9907.654841904488
Iteration 14800: Loss = -9907.655378723619
1
Iteration 14900: Loss = -9907.654844972083
Iteration 15000: Loss = -9907.654875586275
Iteration 15100: Loss = -9907.654861505373
Iteration 15200: Loss = -9907.750997154977
1
Iteration 15300: Loss = -9907.654834760506
Iteration 15400: Loss = -9907.654886224327
Iteration 15500: Loss = -9907.654845810575
Iteration 15600: Loss = -9907.659722389912
1
Iteration 15700: Loss = -9907.65484182237
Iteration 15800: Loss = -9907.654830819392
Iteration 15900: Loss = -9907.65482398666
Iteration 16000: Loss = -9907.655976128033
1
Iteration 16100: Loss = -9907.654965194697
2
Iteration 16200: Loss = -9907.654826649072
Iteration 16300: Loss = -9907.66607491341
1
Iteration 16400: Loss = -9907.65485176827
Iteration 16500: Loss = -9907.65696921955
1
Iteration 16600: Loss = -9907.65490883996
Iteration 16700: Loss = -9907.65485450026
Iteration 16800: Loss = -9907.877775908755
1
Iteration 16900: Loss = -9907.654864355329
Iteration 17000: Loss = -9907.654832877975
Iteration 17100: Loss = -9907.80692358493
1
Iteration 17200: Loss = -9907.654874134863
Iteration 17300: Loss = -9907.65486907075
Iteration 17400: Loss = -9907.654847606398
Iteration 17500: Loss = -9907.654841120486
Iteration 17600: Loss = -9907.688389168248
1
Iteration 17700: Loss = -9907.654828633506
Iteration 17800: Loss = -9907.65483579325
Iteration 17900: Loss = -9907.661243592276
1
Iteration 18000: Loss = -9907.654814594664
Iteration 18100: Loss = -9907.654863809785
Iteration 18200: Loss = -9907.654904743813
Iteration 18300: Loss = -9907.654844316916
Iteration 18400: Loss = -9907.692702525568
1
Iteration 18500: Loss = -9907.654829765517
Iteration 18600: Loss = -9907.654839500163
Iteration 18700: Loss = -9907.655038493473
1
Iteration 18800: Loss = -9907.6548262927
Iteration 18900: Loss = -9907.654847346335
Iteration 19000: Loss = -9907.655002987218
1
Iteration 19100: Loss = -9907.654831573893
Iteration 19200: Loss = -9907.654843543092
Iteration 19300: Loss = -9907.654814647933
Iteration 19400: Loss = -9907.655983618068
1
Iteration 19500: Loss = -9907.654811698558
Iteration 19600: Loss = -9907.654827270968
Iteration 19700: Loss = -9907.66993758369
1
Iteration 19800: Loss = -9907.65482944056
Iteration 19900: Loss = -9907.654831455142
pi: tensor([[9.9118e-01, 8.8215e-03],
        [1.0000e+00, 2.7726e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9609, 0.0391], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1377, 0.0644],
         [0.6675, 0.0344]],

        [[0.6432, 0.1527],
         [0.5031, 0.6517]],

        [[0.5537, 0.1916],
         [0.6836, 0.5096]],

        [[0.5055, 0.2652],
         [0.5137, 0.5378]],

        [[0.5979, 0.2638],
         [0.5855, 0.5355]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00020758318605064086
Average Adjusted Rand Index: 1.427637013092014e-05
10045.56715580235
[-0.00020758318605064086, -0.00020758318605064086] [1.427637013092014e-05, 1.427637013092014e-05] [9907.654809545207, 9907.657639020219]
-------------------------------------
This iteration is 14
True Objective function: Loss = -10158.482235912514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21797.260057251213
Iteration 100: Loss = -10050.070460001061
Iteration 200: Loss = -10048.117314122106
Iteration 300: Loss = -10047.592512534738
Iteration 400: Loss = -10047.124802910597
Iteration 500: Loss = -10046.24516757828
Iteration 600: Loss = -10045.274515105686
Iteration 700: Loss = -10044.836923955805
Iteration 800: Loss = -10044.58613865191
Iteration 900: Loss = -10044.401786361892
Iteration 1000: Loss = -10044.248788793331
Iteration 1100: Loss = -10044.120635956016
Iteration 1200: Loss = -10044.008061392027
Iteration 1300: Loss = -10043.906887995487
Iteration 1400: Loss = -10043.815155361035
Iteration 1500: Loss = -10043.73048009597
Iteration 1600: Loss = -10043.64854769197
Iteration 1700: Loss = -10043.558416977527
Iteration 1800: Loss = -10043.4174194578
Iteration 1900: Loss = -10043.054711522771
Iteration 2000: Loss = -10042.856341130668
Iteration 2100: Loss = -10042.703236689058
Iteration 2200: Loss = -10042.527480434408
Iteration 2300: Loss = -10042.27440194019
Iteration 2400: Loss = -10041.857508478772
Iteration 2500: Loss = -10041.307443116255
Iteration 2600: Loss = -10040.920032425152
Iteration 2700: Loss = -10040.697212236058
Iteration 2800: Loss = -10040.551250500646
Iteration 2900: Loss = -10040.450348881099
Iteration 3000: Loss = -10040.375274278707
Iteration 3100: Loss = -10040.29187803755
Iteration 3200: Loss = -10040.198343992402
Iteration 3300: Loss = -10040.101738384
Iteration 3400: Loss = -10040.001875702288
Iteration 3500: Loss = -10039.91167297974
Iteration 3600: Loss = -10039.833282299749
Iteration 3700: Loss = -10039.754592974548
Iteration 3800: Loss = -10039.673247044739
Iteration 3900: Loss = -10039.589084326133
Iteration 4000: Loss = -10039.497144006917
Iteration 4100: Loss = -10039.387919075189
Iteration 4200: Loss = -10039.244534107467
Iteration 4300: Loss = -10039.04533863942
Iteration 4400: Loss = -10038.806828202358
Iteration 4500: Loss = -10038.634192548325
Iteration 4600: Loss = -10038.562188266056
Iteration 4700: Loss = -10038.537585035714
Iteration 4800: Loss = -10038.528476128227
Iteration 4900: Loss = -10038.524617378553
Iteration 5000: Loss = -10038.522674431988
Iteration 5100: Loss = -10038.521575677929
Iteration 5200: Loss = -10038.520924659164
Iteration 5300: Loss = -10038.52050629242
Iteration 5400: Loss = -10038.520201084413
Iteration 5500: Loss = -10038.522440799368
1
Iteration 5600: Loss = -10038.519785124206
Iteration 5700: Loss = -10038.519637196809
Iteration 5800: Loss = -10038.520806570894
1
Iteration 5900: Loss = -10038.519464816603
Iteration 6000: Loss = -10038.51940597458
Iteration 6100: Loss = -10038.51937727816
Iteration 6200: Loss = -10038.519396465617
Iteration 6300: Loss = -10038.519385858363
Iteration 6400: Loss = -10038.57592249089
1
Iteration 6500: Loss = -10038.519196042462
Iteration 6600: Loss = -10038.769754691795
1
Iteration 6700: Loss = -10038.519149523874
Iteration 6800: Loss = -10038.519114877425
Iteration 6900: Loss = -10038.523321590363
1
Iteration 7000: Loss = -10038.519123360347
Iteration 7100: Loss = -10038.519086473883
Iteration 7200: Loss = -10038.637863421047
1
Iteration 7300: Loss = -10038.519056803194
Iteration 7400: Loss = -10038.519060216806
Iteration 7500: Loss = -10038.519021441596
Iteration 7600: Loss = -10038.519308260986
1
Iteration 7700: Loss = -10038.519027829565
Iteration 7800: Loss = -10038.519025773196
Iteration 7900: Loss = -10038.522428629758
1
Iteration 8000: Loss = -10038.519027380793
Iteration 8100: Loss = -10038.519017140485
Iteration 8200: Loss = -10038.520510792228
1
Iteration 8300: Loss = -10038.519019140196
Iteration 8400: Loss = -10038.51899909718
Iteration 8500: Loss = -10038.519685323445
1
Iteration 8600: Loss = -10038.518996497323
Iteration 8700: Loss = -10038.519005801483
Iteration 8800: Loss = -10038.519050795921
Iteration 8900: Loss = -10038.518989521714
Iteration 9000: Loss = -10038.518972194357
Iteration 9100: Loss = -10038.522429862864
1
Iteration 9200: Loss = -10038.518991589875
Iteration 9300: Loss = -10038.518988452794
Iteration 9400: Loss = -10038.568434018469
1
Iteration 9500: Loss = -10038.518975050174
Iteration 9600: Loss = -10038.518964776304
Iteration 9700: Loss = -10038.529140553319
1
Iteration 9800: Loss = -10038.519011150362
Iteration 9900: Loss = -10038.51899499035
Iteration 10000: Loss = -10038.519100751797
1
Iteration 10100: Loss = -10038.519069009179
Iteration 10200: Loss = -10038.518950520647
Iteration 10300: Loss = -10038.590024794717
1
Iteration 10400: Loss = -10038.518960110408
Iteration 10500: Loss = -10038.51897321684
Iteration 10600: Loss = -10038.980017974001
1
Iteration 10700: Loss = -10038.518963597911
Iteration 10800: Loss = -10038.518970692012
Iteration 10900: Loss = -10038.57982437378
1
Iteration 11000: Loss = -10038.518980459688
Iteration 11100: Loss = -10038.518972848356
Iteration 11200: Loss = -10038.540967569394
1
Iteration 11300: Loss = -10038.518963729333
Iteration 11400: Loss = -10038.518994950058
Iteration 11500: Loss = -10038.557771149031
1
Iteration 11600: Loss = -10038.518942738585
Iteration 11700: Loss = -10038.518965964944
Iteration 11800: Loss = -10038.520530961285
1
Iteration 11900: Loss = -10038.518969095629
Iteration 12000: Loss = -10038.518971202326
Iteration 12100: Loss = -10038.603559412279
1
Iteration 12200: Loss = -10038.518969129513
Iteration 12300: Loss = -10038.518966627646
Iteration 12400: Loss = -10038.523821361396
1
Iteration 12500: Loss = -10038.5189995882
Iteration 12600: Loss = -10038.518991714996
Iteration 12700: Loss = -10038.519058365702
Iteration 12800: Loss = -10038.518981588222
Iteration 12900: Loss = -10038.742025086989
1
Iteration 13000: Loss = -10038.518971654814
Iteration 13100: Loss = -10038.518981128973
Iteration 13200: Loss = -10038.519306842187
1
Iteration 13300: Loss = -10038.518992495296
Iteration 13400: Loss = -10038.52302735772
1
Iteration 13500: Loss = -10038.518983202597
Iteration 13600: Loss = -10038.519464230778
1
Iteration 13700: Loss = -10038.519005486123
Iteration 13800: Loss = -10038.518984326474
Iteration 13900: Loss = -10038.523056170123
1
Iteration 14000: Loss = -10038.518976278256
Iteration 14100: Loss = -10038.518958730334
Iteration 14200: Loss = -10038.52025155089
1
Iteration 14300: Loss = -10038.518985706192
Iteration 14400: Loss = -10038.519231027254
1
Iteration 14500: Loss = -10038.518950262216
Iteration 14600: Loss = -10038.520166225606
1
Iteration 14700: Loss = -10038.518989391725
Iteration 14800: Loss = -10038.574950035232
1
Iteration 14900: Loss = -10038.518954543266
Iteration 15000: Loss = -10038.519085299633
1
Iteration 15100: Loss = -10038.51904368975
Iteration 15200: Loss = -10038.528343942111
1
Iteration 15300: Loss = -10038.518986624764
Iteration 15400: Loss = -10038.519086670394
1
Iteration 15500: Loss = -10038.519062387
Iteration 15600: Loss = -10038.519025371137
Iteration 15700: Loss = -10038.519574184638
1
Iteration 15800: Loss = -10038.51893495199
Iteration 15900: Loss = -10038.542692721034
1
Iteration 16000: Loss = -10038.51896491924
Iteration 16100: Loss = -10038.527413841291
1
Iteration 16200: Loss = -10038.518965209976
Iteration 16300: Loss = -10038.52152232218
1
Iteration 16400: Loss = -10038.518953654897
Iteration 16500: Loss = -10038.531671291983
1
Iteration 16600: Loss = -10038.518990111314
Iteration 16700: Loss = -10038.518978981543
Iteration 16800: Loss = -10038.521038102994
1
Iteration 16900: Loss = -10038.51899805415
Iteration 17000: Loss = -10038.715897906726
1
Iteration 17100: Loss = -10038.518990242143
Iteration 17200: Loss = -10038.51902432872
Iteration 17300: Loss = -10038.519035630527
Iteration 17400: Loss = -10038.518960242092
Iteration 17500: Loss = -10038.522678831258
1
Iteration 17600: Loss = -10038.518974738407
Iteration 17700: Loss = -10038.76225401582
1
Iteration 17800: Loss = -10038.51900017934
Iteration 17900: Loss = -10038.518984301216
Iteration 18000: Loss = -10038.538556359306
1
Iteration 18100: Loss = -10038.519201728885
2
Iteration 18200: Loss = -10038.519056231193
Iteration 18300: Loss = -10038.57767762705
1
Iteration 18400: Loss = -10038.518994131082
Iteration 18500: Loss = -10038.541918893667
1
Iteration 18600: Loss = -10038.518964163191
Iteration 18700: Loss = -10038.607971164653
1
Iteration 18800: Loss = -10038.519003926272
Iteration 18900: Loss = -10038.518954138053
Iteration 19000: Loss = -10038.51916487433
1
Iteration 19100: Loss = -10038.522198229484
2
Iteration 19200: Loss = -10038.518975822606
Iteration 19300: Loss = -10038.52423259409
1
Iteration 19400: Loss = -10038.518983865722
Iteration 19500: Loss = -10038.540032623489
1
Iteration 19600: Loss = -10038.518964762276
Iteration 19700: Loss = -10038.551680397199
1
Iteration 19800: Loss = -10038.519004779992
Iteration 19900: Loss = -10038.52143335759
1
pi: tensor([[0.3407, 0.6593],
        [0.0612, 0.9388]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0741, 0.9259], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0767, 0.0653],
         [0.6135, 0.1498]],

        [[0.5067, 0.0993],
         [0.5617, 0.6126]],

        [[0.6551, 0.1071],
         [0.7209, 0.6548]],

        [[0.5273, 0.0982],
         [0.6854, 0.6666]],

        [[0.6273, 0.0867],
         [0.7254, 0.6893]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.02216370547261358
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.015076275178066338
Global Adjusted Rand Index: 0.007518027188339713
Average Adjusted Rand Index: 0.008799188133203477
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20530.032912974882
Iteration 100: Loss = -10048.195024051314
Iteration 200: Loss = -10046.747980858307
Iteration 300: Loss = -10043.90558410893
Iteration 400: Loss = -10043.079262558058
Iteration 500: Loss = -10042.859317720704
Iteration 600: Loss = -10042.743145448516
Iteration 700: Loss = -10042.658419541995
Iteration 800: Loss = -10042.572888508681
Iteration 900: Loss = -10042.439347355232
Iteration 1000: Loss = -10040.388754092051
Iteration 1100: Loss = -10039.393441014037
Iteration 1200: Loss = -10039.307169037758
Iteration 1300: Loss = -10039.102906274828
Iteration 1400: Loss = -10039.00702974784
Iteration 1500: Loss = -10038.778311090338
Iteration 1600: Loss = -10038.613933252589
Iteration 1700: Loss = -10038.506713543438
Iteration 1800: Loss = -10038.468566622434
Iteration 1900: Loss = -10038.38108129132
Iteration 2000: Loss = -10038.370900046577
Iteration 2100: Loss = -10038.361795380908
Iteration 2200: Loss = -10038.34179334601
Iteration 2300: Loss = -10038.327691598683
Iteration 2400: Loss = -10038.318418336627
Iteration 2500: Loss = -10038.301584391907
Iteration 2600: Loss = -10038.286881711238
Iteration 2700: Loss = -10038.255609813883
Iteration 2800: Loss = -10038.237504535256
Iteration 2900: Loss = -10038.206667063874
Iteration 3000: Loss = -10038.194576570564
Iteration 3100: Loss = -10038.180494822916
Iteration 3200: Loss = -10038.168155487685
Iteration 3300: Loss = -10038.153163614123
Iteration 3400: Loss = -10038.140471453902
Iteration 3500: Loss = -10038.135377483364
Iteration 3600: Loss = -10038.093438263619
Iteration 3700: Loss = -10038.09031692044
Iteration 3800: Loss = -10038.086976254284
Iteration 3900: Loss = -10038.082682090073
Iteration 4000: Loss = -10038.078787071765
Iteration 4100: Loss = -10038.076827728508
Iteration 4200: Loss = -10038.076157698591
Iteration 4300: Loss = -10038.075667953954
Iteration 4400: Loss = -10038.07520144388
Iteration 4500: Loss = -10038.074774819464
Iteration 4600: Loss = -10038.074034980982
Iteration 4700: Loss = -10038.07315260989
Iteration 4800: Loss = -10038.072599346246
Iteration 4900: Loss = -10038.07229235615
Iteration 5000: Loss = -10038.071987595858
Iteration 5100: Loss = -10038.071495060067
Iteration 5200: Loss = -10038.070520897903
Iteration 5300: Loss = -10038.070095771494
Iteration 5400: Loss = -10038.069480180338
Iteration 5500: Loss = -10038.068268967538
Iteration 5600: Loss = -10038.067297123676
Iteration 5700: Loss = -10038.067340016742
Iteration 5800: Loss = -10038.06711221266
Iteration 5900: Loss = -10038.067027398714
Iteration 6000: Loss = -10038.066962243567
Iteration 6100: Loss = -10038.066910459425
Iteration 6200: Loss = -10038.066776207881
Iteration 6300: Loss = -10038.076085834706
1
Iteration 6400: Loss = -10038.066595817909
Iteration 6500: Loss = -10038.066502932543
Iteration 6600: Loss = -10038.066483025483
Iteration 6700: Loss = -10038.066416200794
Iteration 6800: Loss = -10038.068213871455
1
Iteration 6900: Loss = -10038.06635002421
Iteration 7000: Loss = -10038.06632108893
Iteration 7100: Loss = -10038.066346145548
Iteration 7200: Loss = -10038.066266566017
Iteration 7300: Loss = -10038.066242821455
Iteration 7400: Loss = -10038.066254662046
Iteration 7500: Loss = -10038.066228403959
Iteration 7600: Loss = -10038.066214141758
Iteration 7700: Loss = -10038.06697656079
1
Iteration 7800: Loss = -10038.066195459416
Iteration 7900: Loss = -10038.066914167346
1
Iteration 8000: Loss = -10038.068237011392
2
Iteration 8100: Loss = -10038.066182045184
Iteration 8200: Loss = -10038.08145351822
1
Iteration 8300: Loss = -10038.066167120433
Iteration 8400: Loss = -10038.066146031755
Iteration 8500: Loss = -10038.06614663218
Iteration 8600: Loss = -10038.066184993773
Iteration 8700: Loss = -10038.082741666298
1
Iteration 8800: Loss = -10038.066112399543
Iteration 8900: Loss = -10038.066125814099
Iteration 9000: Loss = -10038.066243190353
1
Iteration 9100: Loss = -10038.066109458116
Iteration 9200: Loss = -10038.06610470795
Iteration 9300: Loss = -10038.066314896767
1
Iteration 9400: Loss = -10038.066109316029
Iteration 9500: Loss = -10038.06606215975
Iteration 9600: Loss = -10038.066093689662
Iteration 9700: Loss = -10038.066088116948
Iteration 9800: Loss = -10038.06608572829
Iteration 9900: Loss = -10038.06610334307
Iteration 10000: Loss = -10038.06958478522
1
Iteration 10100: Loss = -10038.066063357444
Iteration 10200: Loss = -10038.079160626117
1
Iteration 10300: Loss = -10038.070997363222
2
Iteration 10400: Loss = -10038.06609838414
Iteration 10500: Loss = -10038.067120147127
1
Iteration 10600: Loss = -10038.066065145546
Iteration 10700: Loss = -10038.06740084012
1
Iteration 10800: Loss = -10038.066068580358
Iteration 10900: Loss = -10038.069020381796
1
Iteration 11000: Loss = -10038.066066574549
Iteration 11100: Loss = -10038.073151946532
1
Iteration 11200: Loss = -10038.066135331417
Iteration 11300: Loss = -10038.073302521585
1
Iteration 11400: Loss = -10038.066046762035
Iteration 11500: Loss = -10038.06850369274
1
Iteration 11600: Loss = -10038.067808284806
2
Iteration 11700: Loss = -10038.068785710188
3
Iteration 11800: Loss = -10038.071164894433
4
Iteration 11900: Loss = -10038.074831178203
5
Iteration 12000: Loss = -10038.067364189752
6
Iteration 12100: Loss = -10038.071353515707
7
Iteration 12200: Loss = -10038.066225462853
8
Iteration 12300: Loss = -10038.066216976435
9
Iteration 12400: Loss = -10038.07733184979
10
Iteration 12500: Loss = -10038.068121751228
11
Iteration 12600: Loss = -10038.066066081874
Iteration 12700: Loss = -10038.072363459909
1
Iteration 12800: Loss = -10038.163080000286
2
Iteration 12900: Loss = -10038.06819638472
3
Iteration 13000: Loss = -10038.066083153883
Iteration 13100: Loss = -10038.349601065394
1
Iteration 13200: Loss = -10038.066063779232
Iteration 13300: Loss = -10038.06708671381
1
Iteration 13400: Loss = -10038.06621002242
2
Iteration 13500: Loss = -10038.115529571682
3
Iteration 13600: Loss = -10038.067427749413
4
Iteration 13700: Loss = -10038.267654981455
5
Iteration 13800: Loss = -10038.06604046086
Iteration 13900: Loss = -10038.101444207416
1
Iteration 14000: Loss = -10038.090457696417
2
Iteration 14100: Loss = -10038.074152548174
3
Iteration 14200: Loss = -10038.068950097266
4
Iteration 14300: Loss = -10038.068370209718
5
Iteration 14400: Loss = -10038.070223032557
6
Iteration 14500: Loss = -10038.066348756205
7
Iteration 14600: Loss = -10038.086359083587
8
Iteration 14700: Loss = -10038.073225401793
9
Iteration 14800: Loss = -10038.066079329403
Iteration 14900: Loss = -10038.071645044067
1
Iteration 15000: Loss = -10038.067075950605
2
Iteration 15100: Loss = -10038.066504654584
3
Iteration 15200: Loss = -10038.238874735041
4
Iteration 15300: Loss = -10038.066217203504
5
Iteration 15400: Loss = -10038.07334524124
6
Iteration 15500: Loss = -10038.06669986498
7
Iteration 15600: Loss = -10038.067573373874
8
Iteration 15700: Loss = -10038.066125533844
Iteration 15800: Loss = -10038.067637004646
1
Iteration 15900: Loss = -10038.06605678965
Iteration 16000: Loss = -10038.066222724005
1
Iteration 16100: Loss = -10038.067251854698
2
Iteration 16200: Loss = -10038.066105514396
Iteration 16300: Loss = -10038.066110799566
Iteration 16400: Loss = -10038.158244475178
1
Iteration 16500: Loss = -10038.066991865759
2
Iteration 16600: Loss = -10038.068547625871
3
Iteration 16700: Loss = -10038.066379306969
4
Iteration 16800: Loss = -10038.066063358845
Iteration 16900: Loss = -10038.095993328192
1
Iteration 17000: Loss = -10038.066068343083
Iteration 17100: Loss = -10038.132517233353
1
Iteration 17200: Loss = -10038.066061354835
Iteration 17300: Loss = -10038.06904219206
1
Iteration 17400: Loss = -10038.06625370352
2
Iteration 17500: Loss = -10038.06816201031
3
Iteration 17600: Loss = -10038.066207843985
4
Iteration 17700: Loss = -10038.066124497338
Iteration 17800: Loss = -10038.069280586265
1
Iteration 17900: Loss = -10038.06608402618
Iteration 18000: Loss = -10038.07675183726
1
Iteration 18100: Loss = -10038.06603444288
Iteration 18200: Loss = -10038.068825917182
1
Iteration 18300: Loss = -10038.066229031714
2
Iteration 18400: Loss = -10038.087459643966
3
Iteration 18500: Loss = -10038.11819283557
4
Iteration 18600: Loss = -10038.066216059555
5
Iteration 18700: Loss = -10038.069647251421
6
Iteration 18800: Loss = -10038.066149242148
7
Iteration 18900: Loss = -10038.066070705065
Iteration 19000: Loss = -10038.074813427704
1
Iteration 19100: Loss = -10038.066138066037
Iteration 19200: Loss = -10038.068490192521
1
Iteration 19300: Loss = -10038.08436556781
2
Iteration 19400: Loss = -10038.066288987919
3
Iteration 19500: Loss = -10038.067181801536
4
Iteration 19600: Loss = -10038.068025899896
5
Iteration 19700: Loss = -10038.067604478476
6
Iteration 19800: Loss = -10038.084748990867
7
Iteration 19900: Loss = -10038.086918991832
8
pi: tensor([[0.9750, 0.0250],
        [0.0025, 0.9975]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9764, 0.0236], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1370, 0.0375],
         [0.6287, 0.1966]],

        [[0.6851, 0.1209],
         [0.5299, 0.6704]],

        [[0.5913, 0.1809],
         [0.6897, 0.5340]],

        [[0.6733, 0.1630],
         [0.5079, 0.6999]],

        [[0.5619, 0.1899],
         [0.5225, 0.5470]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
Global Adjusted Rand Index: -0.0009449654794633072
Average Adjusted Rand Index: -0.0008536000155213273
10158.482235912514
[0.007518027188339713, -0.0009449654794633072] [0.008799188133203477, -0.0008536000155213273] [10038.518967126016, 10038.071141930444]
-------------------------------------
This iteration is 15
True Objective function: Loss = -10096.137406730853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23233.79448573547
Iteration 100: Loss = -9968.325817511446
Iteration 200: Loss = -9966.948245186479
Iteration 300: Loss = -9966.654739612519
Iteration 400: Loss = -9966.52526136734
Iteration 500: Loss = -9966.430481319205
Iteration 600: Loss = -9966.30060214901
Iteration 700: Loss = -9966.040406204285
Iteration 800: Loss = -9965.761753074199
Iteration 900: Loss = -9965.48533707759
Iteration 1000: Loss = -9965.169360514874
Iteration 1100: Loss = -9964.863577293068
Iteration 1200: Loss = -9964.61622774619
Iteration 1300: Loss = -9964.430410180674
Iteration 1400: Loss = -9964.281102191115
Iteration 1500: Loss = -9964.147017397358
Iteration 1600: Loss = -9964.054390683612
Iteration 1700: Loss = -9963.985435349272
Iteration 1800: Loss = -9963.928915564797
Iteration 1900: Loss = -9963.881160645997
Iteration 2000: Loss = -9963.839788832289
Iteration 2100: Loss = -9963.803499468764
Iteration 2200: Loss = -9963.77103594693
Iteration 2300: Loss = -9963.740892714128
Iteration 2400: Loss = -9963.709490543084
Iteration 2500: Loss = -9963.611100914144
Iteration 2600: Loss = -9962.698848981894
Iteration 2700: Loss = -9962.10918540288
Iteration 2800: Loss = -9962.0100231416
Iteration 2900: Loss = -9961.966917851685
Iteration 3000: Loss = -9961.940291923287
Iteration 3100: Loss = -9961.922392230894
Iteration 3200: Loss = -9961.909671811134
Iteration 3300: Loss = -9961.89991254919
Iteration 3400: Loss = -9961.891997219998
Iteration 3500: Loss = -9961.885379111733
Iteration 3600: Loss = -9961.879793841515
Iteration 3700: Loss = -9961.875004058405
Iteration 3800: Loss = -9961.870795630008
Iteration 3900: Loss = -9961.867086373508
Iteration 4000: Loss = -9961.863801560912
Iteration 4100: Loss = -9961.860817505038
Iteration 4200: Loss = -9961.858144710142
Iteration 4300: Loss = -9961.85562626203
Iteration 4400: Loss = -9961.85335813611
Iteration 4500: Loss = -9961.85121190014
Iteration 4600: Loss = -9961.849202663556
Iteration 4700: Loss = -9961.847282241777
Iteration 4800: Loss = -9961.845474262063
Iteration 4900: Loss = -9961.843860890758
Iteration 5000: Loss = -9961.8424077726
Iteration 5100: Loss = -9961.841078881458
Iteration 5200: Loss = -9961.839831400925
Iteration 5300: Loss = -9961.838607157199
Iteration 5400: Loss = -9961.837441011416
Iteration 5500: Loss = -9961.836267528348
Iteration 5600: Loss = -9961.835203375744
Iteration 5700: Loss = -9961.834108764078
Iteration 5800: Loss = -9961.83312084553
Iteration 5900: Loss = -9961.832182654569
Iteration 6000: Loss = -9961.830837560472
Iteration 6100: Loss = -9961.828926045302
Iteration 6200: Loss = -9961.826655202816
Iteration 6300: Loss = -9961.825644129558
Iteration 6400: Loss = -9961.824871060271
Iteration 6500: Loss = -9961.824048157136
Iteration 6600: Loss = -9961.823265728575
Iteration 6700: Loss = -9961.822692432073
Iteration 6800: Loss = -9961.822277095396
Iteration 6900: Loss = -9961.821897577296
Iteration 7000: Loss = -9961.821949317928
Iteration 7100: Loss = -9961.82127926304
Iteration 7200: Loss = -9961.820985219365
Iteration 7300: Loss = -9961.820713707273
Iteration 7400: Loss = -9961.820460480529
Iteration 7500: Loss = -9961.820267827901
Iteration 7600: Loss = -9961.819983837877
Iteration 7700: Loss = -9961.819799734252
Iteration 7800: Loss = -9961.819695060643
Iteration 7900: Loss = -9961.819422137387
Iteration 8000: Loss = -9961.819215268992
Iteration 8100: Loss = -9961.819065633517
Iteration 8200: Loss = -9961.818875664896
Iteration 8300: Loss = -9961.818791309315
Iteration 8400: Loss = -9961.818586598358
Iteration 8500: Loss = -9961.818422344924
Iteration 8600: Loss = -9961.818291144169
Iteration 8700: Loss = -9961.818110142452
Iteration 8800: Loss = -9961.819959617998
1
Iteration 8900: Loss = -9961.817698716228
Iteration 9000: Loss = -9961.81734234523
Iteration 9100: Loss = -9961.840042461929
1
Iteration 9200: Loss = -9961.816939945658
Iteration 9300: Loss = -9961.816781949681
Iteration 9400: Loss = -9961.818541581286
1
Iteration 9500: Loss = -9961.816650184679
Iteration 9600: Loss = -9961.816542120076
Iteration 9700: Loss = -9961.816453491354
Iteration 9800: Loss = -9961.817768645886
1
Iteration 9900: Loss = -9961.816254212707
Iteration 10000: Loss = -9961.816190034011
Iteration 10100: Loss = -9961.81801484974
1
Iteration 10200: Loss = -9961.816049557863
Iteration 10300: Loss = -9961.816037936338
Iteration 10400: Loss = -9961.829480166842
1
Iteration 10500: Loss = -9961.815881885621
Iteration 10600: Loss = -9961.81650633678
1
Iteration 10700: Loss = -9961.81582851554
Iteration 10800: Loss = -9961.815825687028
Iteration 10900: Loss = -9961.816022287428
1
Iteration 11000: Loss = -9961.815850197256
Iteration 11100: Loss = -9961.91506965875
1
Iteration 11200: Loss = -9961.815376085544
Iteration 11300: Loss = -9961.815305542794
Iteration 11400: Loss = -9962.094246184555
1
Iteration 11500: Loss = -9961.815246741102
Iteration 11600: Loss = -9961.815225292177
Iteration 11700: Loss = -9961.82146789518
1
Iteration 11800: Loss = -9961.815153394684
Iteration 11900: Loss = -9961.84107918449
1
Iteration 12000: Loss = -9961.81927712977
2
Iteration 12100: Loss = -9961.815071773923
Iteration 12200: Loss = -9961.817441411853
1
Iteration 12300: Loss = -9961.816079791435
2
Iteration 12400: Loss = -9961.815403261618
3
Iteration 12500: Loss = -9961.81502409299
Iteration 12600: Loss = -9961.816746145509
1
Iteration 12700: Loss = -9961.81506195651
Iteration 12800: Loss = -9961.81931573041
1
Iteration 12900: Loss = -9961.814888575102
Iteration 13000: Loss = -9961.820686357698
1
Iteration 13100: Loss = -9961.81487313529
Iteration 13200: Loss = -9961.818827380293
1
Iteration 13300: Loss = -9961.815761286356
2
Iteration 13400: Loss = -9961.815823006847
3
Iteration 13500: Loss = -9961.842387555947
4
Iteration 13600: Loss = -9961.814838535418
Iteration 13700: Loss = -9961.81519899436
1
Iteration 13800: Loss = -9961.81483311225
Iteration 13900: Loss = -9961.815347166365
1
Iteration 14000: Loss = -9961.814780508823
Iteration 14100: Loss = -9961.821349047977
1
Iteration 14200: Loss = -9961.823960030464
2
Iteration 14300: Loss = -9961.81792767842
3
Iteration 14400: Loss = -9961.823699982435
4
Iteration 14500: Loss = -9961.815036661244
5
Iteration 14600: Loss = -9961.81841850904
6
Iteration 14700: Loss = -9961.814706282967
Iteration 14800: Loss = -9961.814922792631
1
Iteration 14900: Loss = -9961.816024440071
2
Iteration 15000: Loss = -9961.815105722952
3
Iteration 15100: Loss = -9961.83058607095
4
Iteration 15200: Loss = -9961.81466437323
Iteration 15300: Loss = -9961.981699015832
1
Iteration 15400: Loss = -9961.814621228292
Iteration 15500: Loss = -9961.819951854512
1
Iteration 15600: Loss = -9961.81458216817
Iteration 15700: Loss = -9961.815148673397
1
Iteration 15800: Loss = -9962.062858806043
2
Iteration 15900: Loss = -9961.822620155266
3
Iteration 16000: Loss = -9961.814641003775
Iteration 16100: Loss = -9961.814359072285
Iteration 16200: Loss = -9961.814453554982
Iteration 16300: Loss = -9961.815795224105
1
Iteration 16400: Loss = -9961.8143369956
Iteration 16500: Loss = -9961.823537339293
1
Iteration 16600: Loss = -9961.81427248389
Iteration 16700: Loss = -9961.822290555932
1
Iteration 16800: Loss = -9961.815943511963
2
Iteration 16900: Loss = -9961.821288262743
3
Iteration 17000: Loss = -9961.82345241154
4
Iteration 17100: Loss = -9961.83833059619
5
Iteration 17200: Loss = -9961.81445679824
6
Iteration 17300: Loss = -9961.82137656328
7
Iteration 17400: Loss = -9961.814388279548
8
Iteration 17500: Loss = -9961.814690088151
9
Iteration 17600: Loss = -9961.814333684997
Iteration 17700: Loss = -9961.815652338117
1
Iteration 17800: Loss = -9961.814249387948
Iteration 17900: Loss = -9961.815223555375
1
Iteration 18000: Loss = -9961.814249785457
Iteration 18100: Loss = -9961.847125913626
1
Iteration 18200: Loss = -9961.81423444514
Iteration 18300: Loss = -9961.833765609523
1
Iteration 18400: Loss = -9961.818707574044
2
Iteration 18500: Loss = -9961.81424054
Iteration 18600: Loss = -9961.814285987715
Iteration 18700: Loss = -9961.821022353779
1
Iteration 18800: Loss = -9961.814215977154
Iteration 18900: Loss = -9961.814382538523
1
Iteration 19000: Loss = -9961.828900995384
2
Iteration 19100: Loss = -9961.814643291606
3
Iteration 19200: Loss = -9961.814208581342
Iteration 19300: Loss = -9961.821623638514
1
Iteration 19400: Loss = -9961.816261433678
2
Iteration 19500: Loss = -9961.814481518632
3
Iteration 19600: Loss = -9961.992930030485
4
Iteration 19700: Loss = -9961.816766247955
5
Iteration 19800: Loss = -9961.815037352548
6
Iteration 19900: Loss = -9961.81425628464
pi: tensor([[1.0000e+00, 7.1913e-09],
        [2.7504e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9668, 0.0332], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1369, 0.1057],
         [0.6902, 0.4362]],

        [[0.6353, 0.1661],
         [0.5655, 0.6521]],

        [[0.5268, 0.1320],
         [0.7251, 0.5578]],

        [[0.6774, 0.2050],
         [0.5398, 0.5325]],

        [[0.7289, 0.1955],
         [0.7131, 0.5223]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: -0.017511416386760764
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.012298440577296121
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
Global Adjusted Rand Index: -0.00018344578536242723
Average Adjusted Rand Index: -0.008851874394339706
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19285.733065577013
Iteration 100: Loss = -9967.158066644704
Iteration 200: Loss = -9966.57504414244
Iteration 300: Loss = -9966.380183887817
Iteration 400: Loss = -9966.20272141629
Iteration 500: Loss = -9965.774740793462
Iteration 600: Loss = -9965.352426891039
Iteration 700: Loss = -9965.054396428623
Iteration 800: Loss = -9964.860475954154
Iteration 900: Loss = -9964.710459908869
Iteration 1000: Loss = -9964.576248802898
Iteration 1100: Loss = -9964.460884381602
Iteration 1200: Loss = -9964.354652557093
Iteration 1300: Loss = -9964.254337423334
Iteration 1400: Loss = -9964.135573410182
Iteration 1500: Loss = -9963.998474637188
Iteration 1600: Loss = -9963.862391556657
Iteration 1700: Loss = -9963.731437541226
Iteration 1800: Loss = -9963.607551356576
Iteration 1900: Loss = -9963.490540155906
Iteration 2000: Loss = -9963.37927150266
Iteration 2100: Loss = -9963.27323271922
Iteration 2200: Loss = -9963.173207419786
Iteration 2300: Loss = -9963.08103601658
Iteration 2400: Loss = -9962.998690202538
Iteration 2500: Loss = -9962.927346330443
Iteration 2600: Loss = -9962.86684811111
Iteration 2700: Loss = -9962.815958758616
Iteration 2800: Loss = -9962.772973906904
Iteration 2900: Loss = -9962.736139942379
Iteration 3000: Loss = -9962.703732609172
Iteration 3100: Loss = -9962.674386815075
Iteration 3200: Loss = -9962.647074379942
Iteration 3300: Loss = -9962.620942833786
Iteration 3400: Loss = -9962.595551416298
Iteration 3500: Loss = -9962.57047930263
Iteration 3600: Loss = -9962.545707472626
Iteration 3700: Loss = -9962.521200603143
Iteration 3800: Loss = -9962.497164478176
Iteration 3900: Loss = -9962.473773745354
Iteration 4000: Loss = -9962.4512953148
Iteration 4100: Loss = -9962.429896477759
Iteration 4200: Loss = -9962.409820093213
Iteration 4300: Loss = -9962.39107341133
Iteration 4400: Loss = -9962.373778752197
Iteration 4500: Loss = -9962.357885618161
Iteration 4600: Loss = -9962.343344080371
Iteration 4700: Loss = -9962.330146166974
Iteration 4800: Loss = -9962.3182077179
Iteration 4900: Loss = -9962.307295119508
Iteration 5000: Loss = -9962.297519087733
Iteration 5100: Loss = -9962.288604238052
Iteration 5200: Loss = -9962.280614270438
Iteration 5300: Loss = -9962.273315897712
Iteration 5400: Loss = -9962.266816633626
Iteration 5500: Loss = -9962.260829768658
Iteration 5600: Loss = -9962.255446239591
Iteration 5700: Loss = -9962.250550753175
Iteration 5800: Loss = -9962.24609068415
Iteration 5900: Loss = -9962.24205735526
Iteration 6000: Loss = -9962.238349944137
Iteration 6100: Loss = -9962.234976103851
Iteration 6200: Loss = -9962.231861799884
Iteration 6300: Loss = -9962.22901942507
Iteration 6400: Loss = -9962.226470644258
Iteration 6500: Loss = -9962.224036156606
Iteration 6600: Loss = -9962.221880484498
Iteration 6700: Loss = -9962.219845304151
Iteration 6800: Loss = -9962.21795819303
Iteration 6900: Loss = -9962.216312751261
Iteration 7000: Loss = -9962.214709827793
Iteration 7100: Loss = -9962.213217146718
Iteration 7200: Loss = -9962.211858409935
Iteration 7300: Loss = -9962.210575409828
Iteration 7400: Loss = -9962.2094361854
Iteration 7500: Loss = -9962.208354689992
Iteration 7600: Loss = -9962.20734299117
Iteration 7700: Loss = -9962.20639681571
Iteration 7800: Loss = -9962.205499782762
Iteration 7900: Loss = -9962.204689407368
Iteration 8000: Loss = -9962.204061201619
Iteration 8100: Loss = -9962.204299419122
1
Iteration 8200: Loss = -9962.202662694805
Iteration 8300: Loss = -9962.202000485218
Iteration 8400: Loss = -9962.295160395131
1
Iteration 8500: Loss = -9962.200840071937
Iteration 8600: Loss = -9962.200374136997
Iteration 8700: Loss = -9962.199943566886
Iteration 8800: Loss = -9962.199526509947
Iteration 8900: Loss = -9962.247725045314
1
Iteration 9000: Loss = -9962.198698009282
Iteration 9100: Loss = -9962.198385545322
Iteration 9200: Loss = -9962.199104948315
1
Iteration 9300: Loss = -9962.197676215916
Iteration 9400: Loss = -9962.19741784393
Iteration 9500: Loss = -9962.197319748937
Iteration 9600: Loss = -9962.196836844872
Iteration 9700: Loss = -9962.204657659013
1
Iteration 9800: Loss = -9962.196403146017
Iteration 9900: Loss = -9962.196191429757
Iteration 10000: Loss = -9962.288349199383
1
Iteration 10100: Loss = -9962.195881152029
Iteration 10200: Loss = -9962.195662937384
Iteration 10300: Loss = -9962.38559077416
1
Iteration 10400: Loss = -9962.195457806823
Iteration 10500: Loss = -9962.195325534902
Iteration 10600: Loss = -9962.195236157053
Iteration 10700: Loss = -9962.195233442042
Iteration 10800: Loss = -9962.195021180833
Iteration 10900: Loss = -9962.195258788113
1
Iteration 11000: Loss = -9962.194849779191
Iteration 11100: Loss = -9962.194680489562
Iteration 11200: Loss = -9962.236799872846
1
Iteration 11300: Loss = -9962.194474121037
Iteration 11400: Loss = -9962.194393929187
Iteration 11500: Loss = -9962.194622353112
1
Iteration 11600: Loss = -9962.194219972262
Iteration 11700: Loss = -9962.198688581877
1
Iteration 11800: Loss = -9962.194105243889
Iteration 11900: Loss = -9962.194216871421
1
Iteration 12000: Loss = -9962.194108238462
Iteration 12100: Loss = -9962.194015571311
Iteration 12200: Loss = -9962.25747868807
1
Iteration 12300: Loss = -9962.194008239512
Iteration 12400: Loss = -9962.193979619144
Iteration 12500: Loss = -9962.197399920547
1
Iteration 12600: Loss = -9962.19390461507
Iteration 12700: Loss = -9962.193881185603
Iteration 12800: Loss = -9962.193859853809
Iteration 12900: Loss = -9962.193822760797
Iteration 13000: Loss = -9962.194770629654
1
Iteration 13100: Loss = -9962.193754295413
Iteration 13200: Loss = -9962.20520088177
1
Iteration 13300: Loss = -9962.193671451922
Iteration 13400: Loss = -9962.193713699611
Iteration 13500: Loss = -9962.193673247777
Iteration 13600: Loss = -9962.193786133292
1
Iteration 13700: Loss = -9962.193663297006
Iteration 13800: Loss = -9962.424707784903
1
Iteration 13900: Loss = -9962.193622952367
Iteration 14000: Loss = -9962.22470090201
1
Iteration 14100: Loss = -9962.193617056188
Iteration 14200: Loss = -9962.221808845814
1
Iteration 14300: Loss = -9962.193838801066
2
Iteration 14400: Loss = -9962.193809787294
3
Iteration 14500: Loss = -9962.239060418342
4
Iteration 14600: Loss = -9962.193607161784
Iteration 14700: Loss = -9962.196544485469
1
Iteration 14800: Loss = -9962.19360049835
Iteration 14900: Loss = -9962.193700743086
1
Iteration 15000: Loss = -9962.195546658939
2
Iteration 15100: Loss = -9962.197886769229
3
Iteration 15200: Loss = -9962.193554671407
Iteration 15300: Loss = -9962.193578085722
Iteration 15400: Loss = -9962.194134441086
1
Iteration 15500: Loss = -9962.205397469375
2
Iteration 15600: Loss = -9962.193502968705
Iteration 15700: Loss = -9962.204743062533
1
Iteration 15800: Loss = -9962.193508575801
Iteration 15900: Loss = -9962.19527262816
1
Iteration 16000: Loss = -9962.19352550583
Iteration 16100: Loss = -9962.193855546637
1
Iteration 16200: Loss = -9962.193831543571
2
Iteration 16300: Loss = -9962.193620923454
Iteration 16400: Loss = -9962.383446090891
1
Iteration 16500: Loss = -9962.19361645065
Iteration 16600: Loss = -9962.278434894462
1
Iteration 16700: Loss = -9962.193501199125
Iteration 16800: Loss = -9962.193582791051
Iteration 16900: Loss = -9962.215217163712
1
Iteration 17000: Loss = -9962.193839373256
2
Iteration 17100: Loss = -9962.193490095997
Iteration 17200: Loss = -9962.194638834215
1
Iteration 17300: Loss = -9962.193550952212
Iteration 17400: Loss = -9962.196095564917
1
Iteration 17500: Loss = -9962.193521143492
Iteration 17600: Loss = -9962.343886834062
1
Iteration 17700: Loss = -9962.193477126899
Iteration 17800: Loss = -9962.19500385961
1
Iteration 17900: Loss = -9962.19621418809
2
Iteration 18000: Loss = -9962.193530212597
Iteration 18100: Loss = -9962.218307246436
1
Iteration 18200: Loss = -9962.193815758086
2
Iteration 18300: Loss = -9962.193599604529
Iteration 18400: Loss = -9962.193482471408
Iteration 18500: Loss = -9962.193872895348
1
Iteration 18600: Loss = -9962.193846499165
2
Iteration 18700: Loss = -9962.193708353414
3
Iteration 18800: Loss = -9962.193554424168
Iteration 18900: Loss = -9962.303489528926
1
Iteration 19000: Loss = -9962.193520395844
Iteration 19100: Loss = -9962.194017642454
1
Iteration 19200: Loss = -9962.193951352832
2
Iteration 19300: Loss = -9962.195355348491
3
Iteration 19400: Loss = -9962.193467358895
Iteration 19500: Loss = -9962.19417947976
1
Iteration 19600: Loss = -9962.195255715584
2
Iteration 19700: Loss = -9962.193456017276
Iteration 19800: Loss = -9962.194830043962
1
Iteration 19900: Loss = -9962.193494825611
pi: tensor([[9.9699e-01, 3.0068e-03],
        [8.2366e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9902, 0.0098], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1376, 0.2323],
         [0.6464, 0.9998]],

        [[0.6573, 0.1465],
         [0.5977, 0.7177]],

        [[0.5248, 0.1160],
         [0.5577, 0.6549]],

        [[0.6381, 0.2092],
         [0.5007, 0.5068]],

        [[0.6041, 0.1786],
         [0.5794, 0.6715]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: -0.006658343736995423
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
Global Adjusted Rand Index: -9.257234463080936e-05
Average Adjusted Rand Index: -0.005868060585888688
10096.137406730853
[-0.00018344578536242723, -9.257234463080936e-05] [-0.008851874394339706, -0.005868060585888688] [9961.814417735994, 9962.200945675644]
-------------------------------------
This iteration is 16
True Objective function: Loss = -9852.588824891836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22383.379864219904
Iteration 100: Loss = -9696.765852436189
Iteration 200: Loss = -9694.926767487885
Iteration 300: Loss = -9693.178387428612
Iteration 400: Loss = -9692.221249216507
Iteration 500: Loss = -9691.891586842954
Iteration 600: Loss = -9691.642720669168
Iteration 700: Loss = -9691.441019066115
Iteration 800: Loss = -9691.261929628996
Iteration 900: Loss = -9691.09320876142
Iteration 1000: Loss = -9690.933880649207
Iteration 1100: Loss = -9690.792616818204
Iteration 1200: Loss = -9690.676186606604
Iteration 1300: Loss = -9690.57851207339
Iteration 1400: Loss = -9690.500086670552
Iteration 1500: Loss = -9690.438743529052
Iteration 1600: Loss = -9690.391183123631
Iteration 1700: Loss = -9690.354016921025
Iteration 1800: Loss = -9690.32440116566
Iteration 1900: Loss = -9690.30027068416
Iteration 2000: Loss = -9690.27981767649
Iteration 2100: Loss = -9690.261430559874
Iteration 2200: Loss = -9690.243537283004
Iteration 2300: Loss = -9690.22451661592
Iteration 2400: Loss = -9690.202775357671
Iteration 2500: Loss = -9690.177538719161
Iteration 2600: Loss = -9690.150554710059
Iteration 2700: Loss = -9690.1257314697
Iteration 2800: Loss = -9690.106341015418
Iteration 2900: Loss = -9690.092731353056
Iteration 3000: Loss = -9690.083435578335
Iteration 3100: Loss = -9690.076988174475
Iteration 3200: Loss = -9690.072318543043
Iteration 3300: Loss = -9690.068795911082
Iteration 3400: Loss = -9690.066003707716
Iteration 3500: Loss = -9690.063759838957
Iteration 3600: Loss = -9690.061866189391
Iteration 3700: Loss = -9690.06026460343
Iteration 3800: Loss = -9690.05881909296
Iteration 3900: Loss = -9690.057605002969
Iteration 4000: Loss = -9690.056476741962
Iteration 4100: Loss = -9690.055437240444
Iteration 4200: Loss = -9690.05453356329
Iteration 4300: Loss = -9690.053690342775
Iteration 4400: Loss = -9690.05292246675
Iteration 4500: Loss = -9690.052195643004
Iteration 4600: Loss = -9690.051520114977
Iteration 4700: Loss = -9690.050899217405
Iteration 4800: Loss = -9690.050332532912
Iteration 4900: Loss = -9690.049776858526
Iteration 5000: Loss = -9690.04930752369
Iteration 5100: Loss = -9690.04881346702
Iteration 5200: Loss = -9690.04834728363
Iteration 5300: Loss = -9690.047949749547
Iteration 5400: Loss = -9690.047552897186
Iteration 5500: Loss = -9690.04715513514
Iteration 5600: Loss = -9690.046820841402
Iteration 5700: Loss = -9690.046478022805
Iteration 5800: Loss = -9690.046167180035
Iteration 5900: Loss = -9690.045854559768
Iteration 6000: Loss = -9690.045548341763
Iteration 6100: Loss = -9690.045441430673
Iteration 6200: Loss = -9690.050066381189
1
Iteration 6300: Loss = -9690.044920165088
Iteration 6400: Loss = -9690.044569692836
Iteration 6500: Loss = -9690.044353792924
Iteration 6600: Loss = -9690.0441832552
Iteration 6700: Loss = -9690.04388796116
Iteration 6800: Loss = -9690.043735620508
Iteration 6900: Loss = -9690.043490398613
Iteration 7000: Loss = -9690.043335383929
Iteration 7100: Loss = -9690.050670722487
1
Iteration 7200: Loss = -9690.043032493191
Iteration 7300: Loss = -9690.043157965169
1
Iteration 7400: Loss = -9690.042730586578
Iteration 7500: Loss = -9690.042705917358
Iteration 7600: Loss = -9690.042453629
Iteration 7700: Loss = -9690.042387075855
Iteration 7800: Loss = -9690.042204055548
Iteration 7900: Loss = -9690.042076177002
Iteration 8000: Loss = -9690.042129559064
Iteration 8100: Loss = -9690.056373953868
1
Iteration 8200: Loss = -9690.09565898259
2
Iteration 8300: Loss = -9690.041669909668
Iteration 8400: Loss = -9690.04158019017
Iteration 8500: Loss = -9690.041496049536
Iteration 8600: Loss = -9690.04144473776
Iteration 8700: Loss = -9690.041330619753
Iteration 8800: Loss = -9690.09390333566
1
Iteration 8900: Loss = -9690.041163245884
Iteration 9000: Loss = -9690.048382013121
1
Iteration 9100: Loss = -9690.189455635467
2
Iteration 9200: Loss = -9690.05335781757
3
Iteration 9300: Loss = -9690.043119063785
4
Iteration 9400: Loss = -9690.044693230075
5
Iteration 9500: Loss = -9690.041428901419
6
Iteration 9600: Loss = -9690.054936295772
7
Iteration 9700: Loss = -9690.073496635265
8
Iteration 9800: Loss = -9690.041472154944
9
Iteration 9900: Loss = -9690.040647315833
Iteration 10000: Loss = -9690.06582265666
1
Iteration 10100: Loss = -9690.192400106767
2
Iteration 10200: Loss = -9690.040922370805
3
Iteration 10300: Loss = -9690.040939559092
4
Iteration 10400: Loss = -9690.040560303056
Iteration 10500: Loss = -9690.040435262701
Iteration 10600: Loss = -9690.040346834816
Iteration 10700: Loss = -9690.043141332608
1
Iteration 10800: Loss = -9690.046399957931
2
Iteration 10900: Loss = -9690.040356941488
Iteration 11000: Loss = -9690.042680258975
1
Iteration 11100: Loss = -9690.048331536134
2
Iteration 11200: Loss = -9690.040355804673
Iteration 11300: Loss = -9690.041470357119
1
Iteration 11400: Loss = -9690.040260027867
Iteration 11500: Loss = -9690.040406389426
1
Iteration 11600: Loss = -9690.104149671239
2
Iteration 11700: Loss = -9690.040174792912
Iteration 11800: Loss = -9690.04003652009
Iteration 11900: Loss = -9690.045938196617
1
Iteration 12000: Loss = -9690.04001175232
Iteration 12100: Loss = -9690.043867626082
1
Iteration 12200: Loss = -9690.044209866692
2
Iteration 12300: Loss = -9690.047795861114
3
Iteration 12400: Loss = -9690.042160128485
4
Iteration 12500: Loss = -9690.093833413883
5
Iteration 12600: Loss = -9690.04109831393
6
Iteration 12700: Loss = -9690.128120221381
7
Iteration 12800: Loss = -9690.040349398994
8
Iteration 12900: Loss = -9690.040900894508
9
Iteration 13000: Loss = -9690.058484564766
10
Iteration 13100: Loss = -9690.10202258049
11
Iteration 13200: Loss = -9690.043164301267
12
Iteration 13300: Loss = -9690.041103421983
13
Iteration 13400: Loss = -9690.041851649334
14
Iteration 13500: Loss = -9690.068415345826
15
Stopping early at iteration 13500 due to no improvement.
pi: tensor([[9.7561e-01, 2.4385e-02],
        [9.9988e-01, 1.2211e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6410, 0.3590], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1286, 0.1464],
         [0.5539, 0.1675]],

        [[0.7115, 0.0900],
         [0.5585, 0.6124]],

        [[0.6487, 0.1918],
         [0.6191, 0.6848]],

        [[0.5590, 0.1641],
         [0.6199, 0.6454]],

        [[0.5514, 0.2230],
         [0.7245, 0.6170]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.013227671505572266
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 67
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 63
Adjusted Rand Index: -0.008724100327153763
Global Adjusted Rand Index: 0.003386933603679927
Average Adjusted Rand Index: 0.0009007142356837007
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23299.547933163474
Iteration 100: Loss = -9695.703382785345
Iteration 200: Loss = -9694.526143318566
Iteration 300: Loss = -9693.862437235997
Iteration 400: Loss = -9693.132379911982
Iteration 500: Loss = -9692.866736944443
Iteration 600: Loss = -9692.747697319897
Iteration 700: Loss = -9692.659783485557
Iteration 800: Loss = -9692.574213643604
Iteration 900: Loss = -9692.47881253596
Iteration 1000: Loss = -9692.35478755958
Iteration 1100: Loss = -9692.0017266492
Iteration 1200: Loss = -9691.328278010535
Iteration 1300: Loss = -9691.10034666369
Iteration 1400: Loss = -9690.94252141706
Iteration 1500: Loss = -9690.807302065352
Iteration 1600: Loss = -9690.686221221637
Iteration 1700: Loss = -9690.579258537176
Iteration 1800: Loss = -9690.489454837234
Iteration 1900: Loss = -9690.418560109549
Iteration 2000: Loss = -9690.364989537746
Iteration 2100: Loss = -9690.327767125198
Iteration 2200: Loss = -9690.302712426823
Iteration 2300: Loss = -9690.285396074783
Iteration 2400: Loss = -9690.272889809374
Iteration 2500: Loss = -9690.263154881713
Iteration 2600: Loss = -9690.254726107427
Iteration 2700: Loss = -9690.246496756628
Iteration 2800: Loss = -9690.237474879626
Iteration 2900: Loss = -9690.226448854275
Iteration 3000: Loss = -9690.211870159932
Iteration 3100: Loss = -9690.192087146708
Iteration 3200: Loss = -9690.166871446661
Iteration 3300: Loss = -9690.139391025326
Iteration 3400: Loss = -9690.115011967313
Iteration 3500: Loss = -9690.096832144558
Iteration 3600: Loss = -9690.084401054246
Iteration 3700: Loss = -9690.07600990592
Iteration 3800: Loss = -9690.070263445448
Iteration 3900: Loss = -9690.066127141841
Iteration 4000: Loss = -9690.063037006958
Iteration 4100: Loss = -9690.060622207302
Iteration 4200: Loss = -9690.058669185413
Iteration 4300: Loss = -9690.057082571011
Iteration 4400: Loss = -9690.055757836919
Iteration 4500: Loss = -9690.05453768118
Iteration 4600: Loss = -9690.053517860322
Iteration 4700: Loss = -9690.052588303395
Iteration 4800: Loss = -9690.051735227551
Iteration 4900: Loss = -9690.05097648359
Iteration 5000: Loss = -9690.050321039167
Iteration 5100: Loss = -9690.049711470554
Iteration 5200: Loss = -9690.049152161077
Iteration 5300: Loss = -9690.048604460268
Iteration 5400: Loss = -9690.048110392589
Iteration 5500: Loss = -9690.047640202341
Iteration 5600: Loss = -9690.047233181172
Iteration 5700: Loss = -9690.04681842994
Iteration 5800: Loss = -9690.046464077526
Iteration 5900: Loss = -9690.046124215862
Iteration 6000: Loss = -9690.045940982915
Iteration 6100: Loss = -9690.045457474904
Iteration 6200: Loss = -9690.045304576175
Iteration 6300: Loss = -9690.045150798915
Iteration 6400: Loss = -9690.044656650169
Iteration 6500: Loss = -9690.04460411859
Iteration 6600: Loss = -9690.044301050268
Iteration 6700: Loss = -9690.043944270754
Iteration 6800: Loss = -9690.04373731896
Iteration 6900: Loss = -9690.04849395064
1
Iteration 7000: Loss = -9690.043895636863
2
Iteration 7100: Loss = -9690.045841894029
3
Iteration 7200: Loss = -9690.043904216494
4
Iteration 7300: Loss = -9690.042847844934
Iteration 7400: Loss = -9690.043237705653
1
Iteration 7500: Loss = -9690.04298764919
2
Iteration 7600: Loss = -9690.04281441877
Iteration 7700: Loss = -9690.042308850816
Iteration 7800: Loss = -9690.04215936579
Iteration 7900: Loss = -9690.0441208721
1
Iteration 8000: Loss = -9690.041943916707
Iteration 8100: Loss = -9690.041831249497
Iteration 8200: Loss = -9690.04687789159
1
Iteration 8300: Loss = -9690.041659779241
Iteration 8400: Loss = -9690.041691916102
Iteration 8500: Loss = -9690.041457647738
Iteration 8600: Loss = -9690.041876988558
1
Iteration 8700: Loss = -9690.084620678355
2
Iteration 8800: Loss = -9690.041218957254
Iteration 8900: Loss = -9690.041208397406
Iteration 9000: Loss = -9690.04111987728
Iteration 9100: Loss = -9690.04098446333
Iteration 9200: Loss = -9690.045049090562
1
Iteration 9300: Loss = -9690.040877981499
Iteration 9400: Loss = -9690.041289181807
1
Iteration 9500: Loss = -9690.040769883972
Iteration 9600: Loss = -9690.04072345153
Iteration 9700: Loss = -9690.066354513814
1
Iteration 9800: Loss = -9690.04060747562
Iteration 9900: Loss = -9690.041668642527
1
Iteration 10000: Loss = -9690.040550789343
Iteration 10100: Loss = -9690.042465391167
1
Iteration 10200: Loss = -9690.040457703266
Iteration 10300: Loss = -9690.040494282866
Iteration 10400: Loss = -9690.136763580556
1
Iteration 10500: Loss = -9690.040336466873
Iteration 10600: Loss = -9690.042845631047
1
Iteration 10700: Loss = -9690.091328759398
2
Iteration 10800: Loss = -9690.040251523713
Iteration 10900: Loss = -9690.077343139676
1
Iteration 11000: Loss = -9690.040197588327
Iteration 11100: Loss = -9690.050389526496
1
Iteration 11200: Loss = -9690.04035971694
2
Iteration 11300: Loss = -9690.044449887026
3
Iteration 11400: Loss = -9690.057499594002
4
Iteration 11500: Loss = -9690.04009329435
Iteration 11600: Loss = -9690.04051056219
1
Iteration 11700: Loss = -9690.059604774613
2
Iteration 11800: Loss = -9690.04200015192
3
Iteration 11900: Loss = -9690.041704902636
4
Iteration 12000: Loss = -9690.047481081203
5
Iteration 12100: Loss = -9690.040008221162
Iteration 12200: Loss = -9690.045909113476
1
Iteration 12300: Loss = -9690.039984563275
Iteration 12400: Loss = -9690.040508624661
1
Iteration 12500: Loss = -9690.041615804696
2
Iteration 12600: Loss = -9690.039904513385
Iteration 12700: Loss = -9690.03996235592
Iteration 12800: Loss = -9690.040066897845
1
Iteration 12900: Loss = -9690.039968067904
Iteration 13000: Loss = -9690.045254248447
1
Iteration 13100: Loss = -9690.042292059454
2
Iteration 13200: Loss = -9690.052487121975
3
Iteration 13300: Loss = -9690.040089762619
4
Iteration 13400: Loss = -9690.039951591509
Iteration 13500: Loss = -9690.063548267119
1
Iteration 13600: Loss = -9690.040118329434
2
Iteration 13700: Loss = -9690.041626822087
3
Iteration 13800: Loss = -9690.04613322401
4
Iteration 13900: Loss = -9690.044054170614
5
Iteration 14000: Loss = -9690.046720655204
6
Iteration 14100: Loss = -9690.040207461805
7
Iteration 14200: Loss = -9690.040084789436
8
Iteration 14300: Loss = -9690.046103938044
9
Iteration 14400: Loss = -9690.040393851134
10
Iteration 14500: Loss = -9690.041766173807
11
Iteration 14600: Loss = -9690.040004507979
Iteration 14700: Loss = -9690.039807238056
Iteration 14800: Loss = -9690.049882278723
1
Iteration 14900: Loss = -9690.04039001328
2
Iteration 15000: Loss = -9690.146660222446
3
Iteration 15100: Loss = -9690.039745736121
Iteration 15200: Loss = -9690.051901612513
1
Iteration 15300: Loss = -9690.060505962994
2
Iteration 15400: Loss = -9690.046975545238
3
Iteration 15500: Loss = -9690.039716976893
Iteration 15600: Loss = -9690.044892649723
1
Iteration 15700: Loss = -9690.048602339977
2
Iteration 15800: Loss = -9690.040486099486
3
Iteration 15900: Loss = -9690.040191906137
4
Iteration 16000: Loss = -9690.039833688481
5
Iteration 16100: Loss = -9690.042756807552
6
Iteration 16200: Loss = -9690.042064019217
7
Iteration 16300: Loss = -9690.040967228362
8
Iteration 16400: Loss = -9690.04791267243
9
Iteration 16500: Loss = -9690.047249314443
10
Iteration 16600: Loss = -9690.041063845127
11
Iteration 16700: Loss = -9690.039799833161
Iteration 16800: Loss = -9690.040240150682
1
Iteration 16900: Loss = -9690.040855732843
2
Iteration 17000: Loss = -9690.050968486867
3
Iteration 17100: Loss = -9690.041708461005
4
Iteration 17200: Loss = -9690.040215313358
5
Iteration 17300: Loss = -9690.059203816503
6
Iteration 17400: Loss = -9690.043126609706
7
Iteration 17500: Loss = -9690.056755011745
8
Iteration 17600: Loss = -9690.064026465938
9
Iteration 17700: Loss = -9690.045988012696
10
Iteration 17800: Loss = -9690.095931987787
11
Iteration 17900: Loss = -9690.110452329222
12
Iteration 18000: Loss = -9690.042351383529
13
Iteration 18100: Loss = -9690.03987794707
Iteration 18200: Loss = -9690.03980073193
Iteration 18300: Loss = -9690.03968535373
Iteration 18400: Loss = -9690.03983057044
1
Iteration 18500: Loss = -9690.050580917516
2
Iteration 18600: Loss = -9690.051762511552
3
Iteration 18700: Loss = -9690.074391455202
4
Iteration 18800: Loss = -9690.04049967927
5
Iteration 18900: Loss = -9690.039888603067
6
Iteration 19000: Loss = -9690.051924133673
7
Iteration 19100: Loss = -9690.039684537616
Iteration 19200: Loss = -9690.040210380372
1
Iteration 19300: Loss = -9690.23695632398
2
Iteration 19400: Loss = -9690.040461711298
3
Iteration 19500: Loss = -9690.044877787077
4
Iteration 19600: Loss = -9690.101819103962
5
Iteration 19700: Loss = -9690.03994894868
6
Iteration 19800: Loss = -9690.039730087248
Iteration 19900: Loss = -9690.042682075999
1
pi: tensor([[9.7559e-01, 2.4414e-02],
        [1.0000e+00, 4.8839e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6433, 0.3567], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1286, 0.1474],
         [0.6853, 0.1681]],

        [[0.7087, 0.0900],
         [0.6900, 0.6849]],

        [[0.6128, 0.1919],
         [0.6108, 0.5915]],

        [[0.7183, 0.1642],
         [0.5884, 0.6290]],

        [[0.6050, 0.2230],
         [0.7300, 0.6271]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.006784526061713093
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 67
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 63
Adjusted Rand Index: -0.008724100327153763
Global Adjusted Rand Index: 0.0020882527984191815
Average Adjusted Rand Index: -0.00038791485308813393
9852.588824891836
[0.003386933603679927, 0.0020882527984191815] [0.0009007142356837007, -0.00038791485308813393] [9690.068415345826, 9690.039697957909]
-------------------------------------
This iteration is 17
True Objective function: Loss = -10127.107071721859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24882.167357783987
Iteration 100: Loss = -9988.017861608982
Iteration 200: Loss = -9987.015471851037
Iteration 300: Loss = -9986.890762859573
Iteration 400: Loss = -9986.867020284533
Iteration 500: Loss = -9986.852574194096
Iteration 600: Loss = -9986.839921035344
Iteration 700: Loss = -9986.82854362091
Iteration 800: Loss = -9986.818323627
Iteration 900: Loss = -9986.809034573407
Iteration 1000: Loss = -9986.800769572474
Iteration 1100: Loss = -9986.79373218931
Iteration 1200: Loss = -9986.787760501595
Iteration 1300: Loss = -9986.783056069384
Iteration 1400: Loss = -9986.779404814153
Iteration 1500: Loss = -9986.776719430527
Iteration 1600: Loss = -9986.774826081957
Iteration 1700: Loss = -9986.77353381763
Iteration 1800: Loss = -9986.772711981384
Iteration 1900: Loss = -9986.772138298555
Iteration 2000: Loss = -9986.771835052738
Iteration 2100: Loss = -9986.771631030691
Iteration 2200: Loss = -9986.771544662532
Iteration 2300: Loss = -9986.771493664088
Iteration 2400: Loss = -9986.771463271816
Iteration 2500: Loss = -9986.771472389926
Iteration 2600: Loss = -9986.771438781483
Iteration 2700: Loss = -9986.771418192151
Iteration 2800: Loss = -9986.771386053091
Iteration 2900: Loss = -9986.771424762293
Iteration 3000: Loss = -9986.771421977319
Iteration 3100: Loss = -9986.771443478794
Iteration 3200: Loss = -9986.77143611478
Iteration 3300: Loss = -9986.771419380564
Iteration 3400: Loss = -9986.77140581699
Iteration 3500: Loss = -9986.771421499878
Iteration 3600: Loss = -9986.771430021461
Iteration 3700: Loss = -9986.77142995684
Iteration 3800: Loss = -9986.77153014755
1
Iteration 3900: Loss = -9986.7714212556
Iteration 4000: Loss = -9986.772346927079
1
Iteration 4100: Loss = -9986.771420167172
Iteration 4200: Loss = -9986.772010023304
1
Iteration 4300: Loss = -9986.771452059129
Iteration 4400: Loss = -9986.7718181422
1
Iteration 4500: Loss = -9986.771419399833
Iteration 4600: Loss = -9986.771417545868
Iteration 4700: Loss = -9986.771445342722
Iteration 4800: Loss = -9986.771439953838
Iteration 4900: Loss = -9986.771552022534
1
Iteration 5000: Loss = -9986.77143734379
Iteration 5100: Loss = -9986.771425262452
Iteration 5200: Loss = -9986.771424872792
Iteration 5300: Loss = -9986.771420064379
Iteration 5400: Loss = -9986.771479007064
Iteration 5500: Loss = -9986.77142862626
Iteration 5600: Loss = -9986.772099033722
1
Iteration 5700: Loss = -9986.771434220725
Iteration 5800: Loss = -9986.776660576552
1
Iteration 5900: Loss = -9986.771384916876
Iteration 6000: Loss = -9986.775790620763
1
Iteration 6100: Loss = -9986.771436010533
Iteration 6200: Loss = -9986.771408120749
Iteration 6300: Loss = -9986.771433913074
Iteration 6400: Loss = -9986.77142608109
Iteration 6500: Loss = -9986.771502373402
Iteration 6600: Loss = -9986.771431345602
Iteration 6700: Loss = -9986.774124621626
1
Iteration 6800: Loss = -9986.77142305458
Iteration 6900: Loss = -9986.771444490623
Iteration 7000: Loss = -9986.771557582679
1
Iteration 7100: Loss = -9986.77158264221
2
Iteration 7200: Loss = -9986.772224827217
3
Iteration 7300: Loss = -9986.780962829784
4
Iteration 7400: Loss = -9986.771948164362
5
Iteration 7500: Loss = -9986.772343097276
6
Iteration 7600: Loss = -9986.773899730515
7
Iteration 7700: Loss = -9986.772306055398
8
Iteration 7800: Loss = -9986.77582160076
9
Iteration 7900: Loss = -9986.771389926962
Iteration 8000: Loss = -9986.77148689666
Iteration 8100: Loss = -9986.771509294134
Iteration 8200: Loss = -9986.773515624942
1
Iteration 8300: Loss = -9986.78308624409
2
Iteration 8400: Loss = -9986.773710699386
3
Iteration 8500: Loss = -9986.774016314164
4
Iteration 8600: Loss = -9986.771484751784
Iteration 8700: Loss = -9986.771464930362
Iteration 8800: Loss = -9986.771410809753
Iteration 8900: Loss = -9986.830662686421
1
Iteration 9000: Loss = -9986.77142432862
Iteration 9100: Loss = -9986.77143176128
Iteration 9200: Loss = -9986.77153270384
1
Iteration 9300: Loss = -9986.771461396862
Iteration 9400: Loss = -9986.771406119497
Iteration 9500: Loss = -9986.810361280366
1
Iteration 9600: Loss = -9986.771441499064
Iteration 9700: Loss = -9987.019972891816
1
Iteration 9800: Loss = -9986.771456172819
Iteration 9900: Loss = -9986.846859684525
1
Iteration 10000: Loss = -9986.771433998676
Iteration 10100: Loss = -9986.89130681356
1
Iteration 10200: Loss = -9986.788823953968
2
Iteration 10300: Loss = -9986.78990328323
3
Iteration 10400: Loss = -9986.784917707424
4
Iteration 10500: Loss = -9986.771456819206
Iteration 10600: Loss = -9986.77184181463
1
Iteration 10700: Loss = -9986.774309687979
2
Iteration 10800: Loss = -9986.771453130796
Iteration 10900: Loss = -9986.771446209887
Iteration 11000: Loss = -9986.771485143736
Iteration 11100: Loss = -9986.771406875718
Iteration 11200: Loss = -9986.774945322048
1
Iteration 11300: Loss = -9986.791983700783
2
Iteration 11400: Loss = -9986.771426751617
Iteration 11500: Loss = -9986.77184359379
1
Iteration 11600: Loss = -9986.772618355672
2
Iteration 11700: Loss = -9986.787278631109
3
Iteration 11800: Loss = -9986.90885141701
4
Iteration 11900: Loss = -9986.771487292172
Iteration 12000: Loss = -9986.772000287703
1
Iteration 12100: Loss = -9986.771396724338
Iteration 12200: Loss = -9986.771840316094
1
Iteration 12300: Loss = -9986.789957954057
2
Iteration 12400: Loss = -9986.771444025342
Iteration 12500: Loss = -9986.774469174794
1
Iteration 12600: Loss = -9986.771714729282
2
Iteration 12700: Loss = -9986.771504814296
Iteration 12800: Loss = -9986.779034898307
1
Iteration 12900: Loss = -9986.771449941098
Iteration 13000: Loss = -9986.771559463705
1
Iteration 13100: Loss = -9986.777609960856
2
Iteration 13200: Loss = -9986.774343376788
3
Iteration 13300: Loss = -9986.771502136955
Iteration 13400: Loss = -9986.771595216369
Iteration 13500: Loss = -9986.77197415806
1
Iteration 13600: Loss = -9986.771837403096
2
Iteration 13700: Loss = -9986.872218116976
3
Iteration 13800: Loss = -9986.796510605096
4
Iteration 13900: Loss = -9986.771942079513
5
Iteration 14000: Loss = -9986.797315610773
6
Iteration 14100: Loss = -9986.789600266047
7
Iteration 14200: Loss = -9986.78108103062
8
Iteration 14300: Loss = -9986.772007886619
9
Iteration 14400: Loss = -9986.771708705937
10
Iteration 14500: Loss = -9986.843608401614
11
Iteration 14600: Loss = -9986.772080559676
12
Iteration 14700: Loss = -9986.772543193612
13
Iteration 14800: Loss = -9986.777735401985
14
Iteration 14900: Loss = -9986.781588499145
15
Stopping early at iteration 14900 due to no improvement.
pi: tensor([[0.8800, 0.1200],
        [0.0599, 0.9401]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5358, 0.4642], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1591, 0.1392],
         [0.6600, 0.1230]],

        [[0.6905, 0.1387],
         [0.5944, 0.6749]],

        [[0.6932, 0.1338],
         [0.6300, 0.6623]],

        [[0.6608, 0.1437],
         [0.6905, 0.7252]],

        [[0.6059, 0.1402],
         [0.5587, 0.6015]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.030303030303030304
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 37
Adjusted Rand Index: 0.058104321530954246
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 29
Adjusted Rand Index: 0.16808080808080808
time is 3
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 34
Adjusted Rand Index: 0.09349182658973124
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 34
Adjusted Rand Index: 0.09405077191395905
Global Adjusted Rand Index: 0.09061911293662193
Average Adjusted Rand Index: 0.08880615168369659
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24901.422163898686
Iteration 100: Loss = -9990.98987319421
Iteration 200: Loss = -9989.333316103764
Iteration 300: Loss = -9988.827409122041
Iteration 400: Loss = -9988.601559496827
Iteration 500: Loss = -9988.485486736694
Iteration 600: Loss = -9988.420327571699
Iteration 700: Loss = -9988.381129825288
Iteration 800: Loss = -9988.356100608387
Iteration 900: Loss = -9988.33927813721
Iteration 1000: Loss = -9988.327516146483
Iteration 1100: Loss = -9988.31896862367
Iteration 1200: Loss = -9988.312535648616
Iteration 1300: Loss = -9988.30763936725
Iteration 1400: Loss = -9988.303699841319
Iteration 1500: Loss = -9988.300565044421
Iteration 1600: Loss = -9988.297900933137
Iteration 1700: Loss = -9988.295696854915
Iteration 1800: Loss = -9988.293722372797
Iteration 1900: Loss = -9988.292031916526
Iteration 2000: Loss = -9988.290427952663
Iteration 2100: Loss = -9988.28898858485
Iteration 2200: Loss = -9988.287610811489
Iteration 2300: Loss = -9988.286205046283
Iteration 2400: Loss = -9988.284772795654
Iteration 2500: Loss = -9988.283322207639
Iteration 2600: Loss = -9988.281662059278
Iteration 2700: Loss = -9988.27983501304
Iteration 2800: Loss = -9988.27765744012
Iteration 2900: Loss = -9988.275040641201
Iteration 3000: Loss = -9988.271680990998
Iteration 3100: Loss = -9988.2671988215
Iteration 3200: Loss = -9988.260972136675
Iteration 3300: Loss = -9988.25208141235
Iteration 3400: Loss = -9988.239139769265
Iteration 3500: Loss = -9988.221223812638
Iteration 3600: Loss = -9988.199371459366
Iteration 3700: Loss = -9988.176798413111
Iteration 3800: Loss = -9988.15536372109
Iteration 3900: Loss = -9988.134224574202
Iteration 4000: Loss = -9988.111962333876
Iteration 4100: Loss = -9988.087201743474
Iteration 4200: Loss = -9988.058121601625
Iteration 4300: Loss = -9988.022575706535
Iteration 4400: Loss = -9987.979172907328
Iteration 4500: Loss = -9987.92982817196
Iteration 4600: Loss = -9987.882731597585
Iteration 4700: Loss = -9987.848577906663
Iteration 4800: Loss = -9987.830188027721
Iteration 4900: Loss = -9987.822482417561
Iteration 5000: Loss = -9987.819516152102
Iteration 5100: Loss = -9987.818174716338
Iteration 5200: Loss = -9987.817322726356
Iteration 5300: Loss = -9987.817360771312
Iteration 5400: Loss = -9987.816198336102
Iteration 5500: Loss = -9987.815733973403
Iteration 5600: Loss = -9987.81603643865
1
Iteration 5700: Loss = -9987.814976326887
Iteration 5800: Loss = -9987.81466579351
Iteration 5900: Loss = -9987.814516816
Iteration 6000: Loss = -9987.814100809044
Iteration 6100: Loss = -9987.813840867035
Iteration 6200: Loss = -9987.817439361523
1
Iteration 6300: Loss = -9987.813359990152
Iteration 6400: Loss = -9987.813173184235
Iteration 6500: Loss = -9987.814514216303
1
Iteration 6600: Loss = -9987.812809719133
Iteration 6700: Loss = -9987.8126540871
Iteration 6800: Loss = -9987.812785525513
1
Iteration 6900: Loss = -9987.8123122734
Iteration 7000: Loss = -9987.812184055558
Iteration 7100: Loss = -9987.812081496599
Iteration 7200: Loss = -9987.811929097701
Iteration 7300: Loss = -9987.812733985975
1
Iteration 7400: Loss = -9987.811687828282
Iteration 7500: Loss = -9987.811623248052
Iteration 7600: Loss = -9987.819492277396
1
Iteration 7700: Loss = -9987.811399274375
Iteration 7800: Loss = -9987.81234615404
1
Iteration 7900: Loss = -9987.811243729675
Iteration 8000: Loss = -9987.811188709502
Iteration 8100: Loss = -9987.811114581247
Iteration 8200: Loss = -9987.811029957715
Iteration 8300: Loss = -9987.81098033827
Iteration 8400: Loss = -9987.810874484523
Iteration 8500: Loss = -9987.810913891693
Iteration 8600: Loss = -9987.810791618505
Iteration 8700: Loss = -9987.810790298407
Iteration 8800: Loss = -9987.810691716331
Iteration 8900: Loss = -9987.810803955153
1
Iteration 9000: Loss = -9987.810746355452
Iteration 9100: Loss = -9987.811036750443
1
Iteration 9200: Loss = -9987.81053960186
Iteration 9300: Loss = -9987.810860006877
1
Iteration 9400: Loss = -9987.810476301478
Iteration 9500: Loss = -9987.847979821052
1
Iteration 9600: Loss = -9987.810413340407
Iteration 9700: Loss = -9987.810362732089
Iteration 9800: Loss = -9987.811228655739
1
Iteration 9900: Loss = -9987.81031118815
Iteration 10000: Loss = -9987.810301958712
Iteration 10100: Loss = -9987.8871275425
1
Iteration 10200: Loss = -9987.810247549232
Iteration 10300: Loss = -9987.810221062067
Iteration 10400: Loss = -9988.086203724424
1
Iteration 10500: Loss = -9987.810211504668
Iteration 10600: Loss = -9987.810173401716
Iteration 10700: Loss = -9987.816266861764
1
Iteration 10800: Loss = -9987.810160022369
Iteration 10900: Loss = -9987.810115345213
Iteration 11000: Loss = -9987.811050843065
1
Iteration 11100: Loss = -9987.810100158551
Iteration 11200: Loss = -9987.810158665852
Iteration 11300: Loss = -9987.810570046095
1
Iteration 11400: Loss = -9987.81004177534
Iteration 11500: Loss = -9987.878823241523
1
Iteration 11600: Loss = -9987.810052858582
Iteration 11700: Loss = -9987.81002616195
Iteration 11800: Loss = -9987.813743489125
1
Iteration 11900: Loss = -9987.810009053188
Iteration 12000: Loss = -9987.811673022847
1
Iteration 12100: Loss = -9987.810021820653
Iteration 12200: Loss = -9987.809976006038
Iteration 12300: Loss = -9987.859087974295
1
Iteration 12400: Loss = -9987.809977444535
Iteration 12500: Loss = -9987.809948908985
Iteration 12600: Loss = -9987.812927563207
1
Iteration 12700: Loss = -9987.809953872715
Iteration 12800: Loss = -9987.810216669583
1
Iteration 12900: Loss = -9987.810010855268
Iteration 13000: Loss = -9987.809913903742
Iteration 13100: Loss = -9987.810046930721
1
Iteration 13200: Loss = -9987.809910571716
Iteration 13300: Loss = -9987.810152174303
1
Iteration 13400: Loss = -9987.809935544476
Iteration 13500: Loss = -9987.809912751722
Iteration 13600: Loss = -9987.80993150406
Iteration 13700: Loss = -9987.809892152285
Iteration 13800: Loss = -9987.823097430137
1
Iteration 13900: Loss = -9987.809910734164
Iteration 14000: Loss = -9987.811122443441
1
Iteration 14100: Loss = -9987.809873276967
Iteration 14200: Loss = -9987.809896410057
Iteration 14300: Loss = -9987.81008241705
1
Iteration 14400: Loss = -9987.809892316127
Iteration 14500: Loss = -9987.810153187846
1
Iteration 14600: Loss = -9987.809892600513
Iteration 14700: Loss = -9987.81377247391
1
Iteration 14800: Loss = -9987.80989196565
Iteration 14900: Loss = -9987.80991075213
Iteration 15000: Loss = -9987.815996205976
1
Iteration 15100: Loss = -9987.810262720759
2
Iteration 15200: Loss = -9987.811213827119
3
Iteration 15300: Loss = -9987.819577228032
4
Iteration 15400: Loss = -9987.809880594077
Iteration 15500: Loss = -9987.810300669147
1
Iteration 15600: Loss = -9987.830572812441
2
Iteration 15700: Loss = -9987.809851834169
Iteration 15800: Loss = -9987.810337471972
1
Iteration 15900: Loss = -9987.809858342753
Iteration 16000: Loss = -9987.811920169466
1
Iteration 16100: Loss = -9987.809867423102
Iteration 16200: Loss = -9987.810662788943
1
Iteration 16300: Loss = -9987.809927185164
Iteration 16400: Loss = -9987.813343971866
1
Iteration 16500: Loss = -9987.812918590134
2
Iteration 16600: Loss = -9987.809989044581
Iteration 16700: Loss = -9987.809932526354
Iteration 16800: Loss = -9987.815083734396
1
Iteration 16900: Loss = -9987.809863605333
Iteration 17000: Loss = -9987.810313038734
1
Iteration 17100: Loss = -9987.809853125947
Iteration 17200: Loss = -9987.81190267245
1
Iteration 17300: Loss = -9987.81043637503
2
Iteration 17400: Loss = -9987.811900059027
3
Iteration 17500: Loss = -9987.810723382056
4
Iteration 17600: Loss = -9987.814312740287
5
Iteration 17700: Loss = -9987.80987925917
Iteration 17800: Loss = -9987.825597954085
1
Iteration 17900: Loss = -9987.809890883764
Iteration 18000: Loss = -9987.809875155182
Iteration 18100: Loss = -9987.829173884624
1
Iteration 18200: Loss = -9987.809842637427
Iteration 18300: Loss = -9987.809894013079
Iteration 18400: Loss = -9988.178224435022
1
Iteration 18500: Loss = -9987.809839196116
Iteration 18600: Loss = -9987.810728482867
1
Iteration 18700: Loss = -9987.810039334267
2
Iteration 18800: Loss = -9987.812149919715
3
Iteration 18900: Loss = -9987.825482847744
4
Iteration 19000: Loss = -9987.813746034377
5
Iteration 19100: Loss = -9987.80983599996
Iteration 19200: Loss = -9988.0465425283
1
Iteration 19300: Loss = -9987.80986383878
Iteration 19400: Loss = -9987.82988140063
1
Iteration 19500: Loss = -9987.811830589697
2
Iteration 19600: Loss = -9987.809824777172
Iteration 19700: Loss = -9987.810705724589
1
Iteration 19800: Loss = -9987.813629406377
2
Iteration 19900: Loss = -9987.809977202694
3
pi: tensor([[9.7021e-01, 2.9792e-02],
        [1.0000e+00, 4.8435e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9790, 0.0210], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1397, 0.1492],
         [0.6764, 0.1285]],

        [[0.5684, 0.1359],
         [0.6663, 0.6976]],

        [[0.5562, 0.0845],
         [0.5192, 0.7174]],

        [[0.6223, 0.1607],
         [0.5971, 0.7119]],

        [[0.7096, 0.1276],
         [0.5043, 0.6149]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
10127.107071721859
[0.09061911293662193, 0.0] [0.08880615168369659, 0.0] [9986.781588499145, 9987.80985911609]
-------------------------------------
This iteration is 18
True Objective function: Loss = -10094.80456310901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24894.934983082632
Iteration 100: Loss = -9980.971514086761
Iteration 200: Loss = -9980.263474288864
Iteration 300: Loss = -9979.920478292142
Iteration 400: Loss = -9979.77616161794
Iteration 500: Loss = -9979.656769110792
Iteration 600: Loss = -9979.544711419328
Iteration 700: Loss = -9979.449984343437
Iteration 800: Loss = -9979.371817112431
Iteration 900: Loss = -9979.30854831021
Iteration 1000: Loss = -9979.264158222306
Iteration 1100: Loss = -9979.23658826133
Iteration 1200: Loss = -9979.218876963248
Iteration 1300: Loss = -9979.206314601175
Iteration 1400: Loss = -9979.196912762196
Iteration 1500: Loss = -9979.189741152046
Iteration 1600: Loss = -9979.184030710241
Iteration 1700: Loss = -9979.179572393803
Iteration 1800: Loss = -9979.176012394668
Iteration 1900: Loss = -9979.173145144212
Iteration 2000: Loss = -9979.170728515011
Iteration 2100: Loss = -9979.168724947991
Iteration 2200: Loss = -9979.167014727234
Iteration 2300: Loss = -9979.165554442643
Iteration 2400: Loss = -9979.164325074935
Iteration 2500: Loss = -9979.163190823147
Iteration 2600: Loss = -9979.16222659111
Iteration 2700: Loss = -9979.161378056788
Iteration 2800: Loss = -9979.160622339205
Iteration 2900: Loss = -9979.159922881552
Iteration 3000: Loss = -9979.159356662967
Iteration 3100: Loss = -9979.158760873908
Iteration 3200: Loss = -9979.15827748421
Iteration 3300: Loss = -9979.157823684696
Iteration 3400: Loss = -9979.157421314309
Iteration 3500: Loss = -9979.157017578014
Iteration 3600: Loss = -9979.156683537254
Iteration 3700: Loss = -9979.156375032328
Iteration 3800: Loss = -9979.156099423351
Iteration 3900: Loss = -9979.155851991747
Iteration 4000: Loss = -9979.155568108823
Iteration 4100: Loss = -9979.155352513035
Iteration 4200: Loss = -9979.155125783387
Iteration 4300: Loss = -9979.154940218248
Iteration 4400: Loss = -9979.154751478525
Iteration 4500: Loss = -9979.15461572771
Iteration 4600: Loss = -9979.154429636603
Iteration 4700: Loss = -9979.154285423656
Iteration 4800: Loss = -9979.154175376547
Iteration 4900: Loss = -9979.154027380326
Iteration 5000: Loss = -9979.153899451301
Iteration 5100: Loss = -9979.153931617999
Iteration 5200: Loss = -9979.153663328856
Iteration 5300: Loss = -9979.153578914988
Iteration 5400: Loss = -9979.153519775986
Iteration 5500: Loss = -9979.153565988932
Iteration 5600: Loss = -9979.153307380779
Iteration 5700: Loss = -9979.153322045982
Iteration 5800: Loss = -9979.153169559568
Iteration 5900: Loss = -9979.153125119055
Iteration 6000: Loss = -9979.153063741871
Iteration 6100: Loss = -9979.15299690489
Iteration 6200: Loss = -9979.152939464168
Iteration 6300: Loss = -9979.152981223007
Iteration 6400: Loss = -9979.152842817546
Iteration 6500: Loss = -9979.1527957136
Iteration 6600: Loss = -9979.152750552175
Iteration 6700: Loss = -9979.152714133885
Iteration 6800: Loss = -9979.152647577668
Iteration 6900: Loss = -9979.15262504124
Iteration 7000: Loss = -9979.152589210935
Iteration 7100: Loss = -9979.152571289143
Iteration 7200: Loss = -9979.152516076356
Iteration 7300: Loss = -9979.15250354235
Iteration 7400: Loss = -9979.15244485254
Iteration 7500: Loss = -9979.152469038507
Iteration 7600: Loss = -9979.152425310036
Iteration 7700: Loss = -9979.152507886738
Iteration 7800: Loss = -9979.152337885094
Iteration 7900: Loss = -9979.152352379253
Iteration 8000: Loss = -9979.172096886161
1
Iteration 8100: Loss = -9979.152292204562
Iteration 8200: Loss = -9979.152319527846
Iteration 8300: Loss = -9979.152773461437
1
Iteration 8400: Loss = -9979.152241696835
Iteration 8500: Loss = -9979.152358935165
1
Iteration 8600: Loss = -9979.152208274088
Iteration 8700: Loss = -9979.152192600213
Iteration 8800: Loss = -9979.153237207789
1
Iteration 8900: Loss = -9979.152168523247
Iteration 9000: Loss = -9979.154934255335
1
Iteration 9100: Loss = -9979.152163802646
Iteration 9200: Loss = -9979.152132785406
Iteration 9300: Loss = -9979.443422198092
1
Iteration 9400: Loss = -9979.152140096929
Iteration 9500: Loss = -9979.152122072903
Iteration 9600: Loss = -9979.152678442772
1
Iteration 9700: Loss = -9979.152096737998
Iteration 9800: Loss = -9979.152066911167
Iteration 9900: Loss = -9979.15209718335
Iteration 10000: Loss = -9979.15215907387
Iteration 10100: Loss = -9979.152044088625
Iteration 10200: Loss = -9979.152903699585
1
Iteration 10300: Loss = -9979.152079885944
Iteration 10400: Loss = -9979.152052559464
Iteration 10500: Loss = -9979.152928868216
1
Iteration 10600: Loss = -9979.152102956228
Iteration 10700: Loss = -9979.152061315013
Iteration 10800: Loss = -9979.191668441808
1
Iteration 10900: Loss = -9979.152047963136
Iteration 11000: Loss = -9979.152010973216
Iteration 11100: Loss = -9979.194144401828
1
Iteration 11200: Loss = -9979.15203355395
Iteration 11300: Loss = -9979.15198425513
Iteration 11400: Loss = -9979.153951497105
1
Iteration 11500: Loss = -9979.152040710273
Iteration 11600: Loss = -9979.151968721524
Iteration 11700: Loss = -9979.153104717268
1
Iteration 11800: Loss = -9979.151975350993
Iteration 11900: Loss = -9979.15197625639
Iteration 12000: Loss = -9979.152105639709
1
Iteration 12100: Loss = -9979.15197534707
Iteration 12200: Loss = -9979.161471571268
1
Iteration 12300: Loss = -9979.15198846955
Iteration 12400: Loss = -9979.151983429161
Iteration 12500: Loss = -9979.177052630814
1
Iteration 12600: Loss = -9979.151956397369
Iteration 12700: Loss = -9979.155425421426
1
Iteration 12800: Loss = -9979.151997149474
Iteration 12900: Loss = -9979.151966836822
Iteration 13000: Loss = -9979.27078957733
1
Iteration 13100: Loss = -9979.151966089243
Iteration 13200: Loss = -9979.151941387188
Iteration 13300: Loss = -9979.156030480483
1
Iteration 13400: Loss = -9979.1519619553
Iteration 13500: Loss = -9979.151959615225
Iteration 13600: Loss = -9979.151940736698
Iteration 13700: Loss = -9979.152001812747
Iteration 13800: Loss = -9979.169352167299
1
Iteration 13900: Loss = -9979.15193444716
Iteration 14000: Loss = -9979.154553183933
1
Iteration 14100: Loss = -9979.151940599095
Iteration 14200: Loss = -9979.154312906945
1
Iteration 14300: Loss = -9979.151957158989
Iteration 14400: Loss = -9979.153024792495
1
Iteration 14500: Loss = -9979.154048403389
2
Iteration 14600: Loss = -9979.364530491563
3
Iteration 14700: Loss = -9979.151958394239
Iteration 14800: Loss = -9979.155675355478
1
Iteration 14900: Loss = -9979.153959287414
2
Iteration 15000: Loss = -9979.156565867568
3
Iteration 15100: Loss = -9979.219366777359
4
Iteration 15200: Loss = -9979.153365531229
5
Iteration 15300: Loss = -9979.152031373686
Iteration 15400: Loss = -9979.152221366568
1
Iteration 15500: Loss = -9979.15721514999
2
Iteration 15600: Loss = -9979.153003404954
3
Iteration 15700: Loss = -9979.151986640634
Iteration 15800: Loss = -9979.152985432114
1
Iteration 15900: Loss = -9979.155501869802
2
Iteration 16000: Loss = -9979.15197961755
Iteration 16100: Loss = -9979.191980056685
1
Iteration 16200: Loss = -9979.173369494332
2
Iteration 16300: Loss = -9979.1918013673
3
Iteration 16400: Loss = -9979.152626185241
4
Iteration 16500: Loss = -9979.15212944272
5
Iteration 16600: Loss = -9979.158517064163
6
Iteration 16700: Loss = -9979.156481608394
7
Iteration 16800: Loss = -9979.153302176916
8
Iteration 16900: Loss = -9979.151942174785
Iteration 17000: Loss = -9979.152177745746
1
Iteration 17100: Loss = -9979.151947677596
Iteration 17200: Loss = -9979.236015107892
1
Iteration 17300: Loss = -9979.153384045008
2
Iteration 17400: Loss = -9979.160780099188
3
Iteration 17500: Loss = -9979.159657704375
4
Iteration 17600: Loss = -9979.152301065855
5
Iteration 17700: Loss = -9979.161516467271
6
Iteration 17800: Loss = -9979.160467092215
7
Iteration 17900: Loss = -9979.153230733828
8
Iteration 18000: Loss = -9979.152810489033
9
Iteration 18100: Loss = -9979.156130016963
10
Iteration 18200: Loss = -9979.156917574415
11
Iteration 18300: Loss = -9979.154714836066
12
Iteration 18400: Loss = -9979.15203287047
Iteration 18500: Loss = -9979.152780609995
1
Iteration 18600: Loss = -9979.20240271161
2
Iteration 18700: Loss = -9979.159817851638
3
Iteration 18800: Loss = -9979.152036046964
Iteration 18900: Loss = -9979.167912397013
1
Iteration 19000: Loss = -9979.164028117353
2
Iteration 19100: Loss = -9979.152540764413
3
Iteration 19200: Loss = -9979.152354515489
4
Iteration 19300: Loss = -9979.151984949292
Iteration 19400: Loss = -9979.152631502868
1
Iteration 19500: Loss = -9979.152817967948
2
Iteration 19600: Loss = -9979.154144944288
3
Iteration 19700: Loss = -9979.156586164261
4
Iteration 19800: Loss = -9979.152354685411
5
Iteration 19900: Loss = -9979.17211291178
6
pi: tensor([[1.0000e+00, 2.1241e-07],
        [1.6147e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0542, 0.9458], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1222, 0.1166],
         [0.6351, 0.1411]],

        [[0.5226, 0.1114],
         [0.6515, 0.5002]],

        [[0.6329, 0.0933],
         [0.5170, 0.5206]],

        [[0.6487, 0.1700],
         [0.7049, 0.6595]],

        [[0.6227, 0.1133],
         [0.6432, 0.6526]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.005431979218977636
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.00485601903559462
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.01717781179455718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.010102533172496984
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: -0.02789997666760923
Global Adjusted Rand Index: -0.008712345221699978
Average Adjusted Rand Index: -0.00897846467601823
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22183.2319788879
Iteration 100: Loss = -9981.658160851253
Iteration 200: Loss = -9980.815927828116
Iteration 300: Loss = -9979.19499369632
Iteration 400: Loss = -9978.421757196136
Iteration 500: Loss = -9978.327421931848
Iteration 600: Loss = -9978.27518555613
Iteration 700: Loss = -9978.233522066337
Iteration 800: Loss = -9978.196229325962
Iteration 900: Loss = -9978.16284373864
Iteration 1000: Loss = -9978.134369036481
Iteration 1100: Loss = -9978.11146691689
Iteration 1200: Loss = -9978.093868435903
Iteration 1300: Loss = -9978.080585548158
Iteration 1400: Loss = -9978.070471858682
Iteration 1500: Loss = -9978.062447736103
Iteration 1600: Loss = -9978.055826065893
Iteration 1700: Loss = -9978.05003830686
Iteration 1800: Loss = -9978.044925550588
Iteration 1900: Loss = -9978.040331995253
Iteration 2000: Loss = -9978.036186912293
Iteration 2100: Loss = -9978.032465581296
Iteration 2200: Loss = -9978.029167906938
Iteration 2300: Loss = -9978.026336309535
Iteration 2400: Loss = -9978.02389271157
Iteration 2500: Loss = -9978.021845803221
Iteration 2600: Loss = -9978.020094354311
Iteration 2700: Loss = -9978.018679490091
Iteration 2800: Loss = -9978.017434504907
Iteration 2900: Loss = -9978.016472011446
Iteration 3000: Loss = -9978.015631889843
Iteration 3100: Loss = -9978.014948013188
Iteration 3200: Loss = -9978.014377057527
Iteration 3300: Loss = -9978.013846398126
Iteration 3400: Loss = -9978.013416482503
Iteration 3500: Loss = -9978.013056567315
Iteration 3600: Loss = -9978.012739296677
Iteration 3700: Loss = -9978.012455483902
Iteration 3800: Loss = -9978.012184661527
Iteration 3900: Loss = -9978.011981964015
Iteration 4000: Loss = -9978.011779157183
Iteration 4100: Loss = -9978.011591761959
Iteration 4200: Loss = -9978.011455220723
Iteration 4300: Loss = -9978.011322421002
Iteration 4400: Loss = -9978.011169667654
Iteration 4500: Loss = -9978.011089834708
Iteration 4600: Loss = -9978.010998429727
Iteration 4700: Loss = -9978.010885849337
Iteration 4800: Loss = -9978.010841890546
Iteration 4900: Loss = -9978.010760900248
Iteration 5000: Loss = -9978.01071772036
Iteration 5100: Loss = -9978.0106288862
Iteration 5200: Loss = -9978.010612913487
Iteration 5300: Loss = -9978.010561365514
Iteration 5400: Loss = -9978.010532096245
Iteration 5500: Loss = -9978.010502949652
Iteration 5600: Loss = -9978.010470902998
Iteration 5700: Loss = -9978.01042812013
Iteration 5800: Loss = -9978.010379560958
Iteration 5900: Loss = -9978.01039021369
Iteration 6000: Loss = -9978.015075346662
1
Iteration 6100: Loss = -9978.010385784562
Iteration 6200: Loss = -9978.01035753329
Iteration 6300: Loss = -9978.011766718428
1
Iteration 6400: Loss = -9978.010381727645
Iteration 6500: Loss = -9978.010347831354
Iteration 6600: Loss = -9978.010650236172
1
Iteration 6700: Loss = -9978.01034636688
Iteration 6800: Loss = -9978.01031089153
Iteration 6900: Loss = -9978.010460043075
1
Iteration 7000: Loss = -9978.010316452619
Iteration 7100: Loss = -9978.010319511224
Iteration 7200: Loss = -9978.010309541296
Iteration 7300: Loss = -9978.01031431976
Iteration 7400: Loss = -9978.010392226803
Iteration 7500: Loss = -9978.010293575488
Iteration 7600: Loss = -9978.010283114061
Iteration 7700: Loss = -9978.010279339129
Iteration 7800: Loss = -9978.010275463424
Iteration 7900: Loss = -9978.011260549029
1
Iteration 8000: Loss = -9978.013000277362
2
Iteration 8100: Loss = -9978.010327732258
Iteration 8200: Loss = -9978.01205102579
1
Iteration 8300: Loss = -9978.01028508318
Iteration 8400: Loss = -9978.011014573383
1
Iteration 8500: Loss = -9978.01028762901
Iteration 8600: Loss = -9978.021330760826
1
Iteration 8700: Loss = -9978.010296034638
Iteration 8800: Loss = -9978.010276206056
Iteration 8900: Loss = -9978.010445716489
1
Iteration 9000: Loss = -9978.010272764941
Iteration 9100: Loss = -9978.01028103317
Iteration 9200: Loss = -9978.011363746515
1
Iteration 9300: Loss = -9978.010298416724
Iteration 9400: Loss = -9978.010269979397
Iteration 9500: Loss = -9978.010309889523
Iteration 9600: Loss = -9978.010848693264
1
Iteration 9700: Loss = -9978.010271985006
Iteration 9800: Loss = -9978.010259759187
Iteration 9900: Loss = -9978.014337880468
1
Iteration 10000: Loss = -9978.01027758979
Iteration 10100: Loss = -9978.010313429944
Iteration 10200: Loss = -9978.102724913497
1
Iteration 10300: Loss = -9978.010307402554
Iteration 10400: Loss = -9978.010271611156
Iteration 10500: Loss = -9978.010309643716
Iteration 10600: Loss = -9978.01087162993
1
Iteration 10700: Loss = -9978.01033182789
Iteration 10800: Loss = -9978.0102970556
Iteration 10900: Loss = -9978.030330110552
1
Iteration 11000: Loss = -9978.010299547008
Iteration 11100: Loss = -9978.010276708845
Iteration 11200: Loss = -9978.066695889225
1
Iteration 11300: Loss = -9978.01028863502
Iteration 11400: Loss = -9978.010293657771
Iteration 11500: Loss = -9978.451860382589
1
Iteration 11600: Loss = -9978.010299095411
Iteration 11700: Loss = -9978.010292725747
Iteration 11800: Loss = -9978.010382739109
Iteration 11900: Loss = -9978.010292539673
Iteration 12000: Loss = -9978.010314922487
Iteration 12100: Loss = -9978.01071989153
1
Iteration 12200: Loss = -9978.010305258726
Iteration 12300: Loss = -9978.010260702717
Iteration 12400: Loss = -9978.010391537326
1
Iteration 12500: Loss = -9978.010319770565
Iteration 12600: Loss = -9978.010247287404
Iteration 12700: Loss = -9978.010288816537
Iteration 12800: Loss = -9978.010322552296
Iteration 12900: Loss = -9978.010278022784
Iteration 13000: Loss = -9978.010658869614
1
Iteration 13100: Loss = -9978.010330887437
Iteration 13200: Loss = -9978.01027180788
Iteration 13300: Loss = -9978.01043384558
1
Iteration 13400: Loss = -9978.01033722553
Iteration 13500: Loss = -9978.010280154142
Iteration 13600: Loss = -9978.01097233243
1
Iteration 13700: Loss = -9978.010301112976
Iteration 13800: Loss = -9978.01029256646
Iteration 13900: Loss = -9978.01032170266
Iteration 14000: Loss = -9978.01027041859
Iteration 14100: Loss = -9978.011049644458
1
Iteration 14200: Loss = -9978.010259194403
Iteration 14300: Loss = -9978.010698244896
1
Iteration 14400: Loss = -9978.018416300229
2
Iteration 14500: Loss = -9978.010731793718
3
Iteration 14600: Loss = -9978.01027457379
Iteration 14700: Loss = -9978.01485387566
1
Iteration 14800: Loss = -9978.010274462997
Iteration 14900: Loss = -9978.011163601177
1
Iteration 15000: Loss = -9978.010344457603
Iteration 15100: Loss = -9978.102937833382
1
Iteration 15200: Loss = -9978.010276371715
Iteration 15300: Loss = -9978.081822695829
1
Iteration 15400: Loss = -9978.010305709855
Iteration 15500: Loss = -9978.015098808006
1
Iteration 15600: Loss = -9978.010264774
Iteration 15700: Loss = -9978.01366352816
1
Iteration 15800: Loss = -9978.010283578653
Iteration 15900: Loss = -9978.010811917755
1
Iteration 16000: Loss = -9978.01032605232
Iteration 16100: Loss = -9978.011382115636
1
Iteration 16200: Loss = -9978.010324006795
Iteration 16300: Loss = -9978.010391432532
Iteration 16400: Loss = -9978.01584675588
1
Iteration 16500: Loss = -9978.010429262857
Iteration 16600: Loss = -9978.011484835613
1
Iteration 16700: Loss = -9978.010433642778
Iteration 16800: Loss = -9978.010593252353
1
Iteration 16900: Loss = -9978.010289321459
Iteration 17000: Loss = -9978.037123400683
1
Iteration 17100: Loss = -9978.010301710612
Iteration 17200: Loss = -9978.010255358256
Iteration 17300: Loss = -9978.010461332795
1
Iteration 17400: Loss = -9978.010264457433
Iteration 17500: Loss = -9978.024405443684
1
Iteration 17600: Loss = -9978.010305020212
Iteration 17700: Loss = -9978.013183410327
1
Iteration 17800: Loss = -9978.010293409427
Iteration 17900: Loss = -9978.019071246878
1
Iteration 18000: Loss = -9978.010271794776
Iteration 18100: Loss = -9978.038131706266
1
Iteration 18200: Loss = -9978.010262638485
Iteration 18300: Loss = -9978.049949782193
1
Iteration 18400: Loss = -9978.010280962226
Iteration 18500: Loss = -9978.010452402634
1
Iteration 18600: Loss = -9978.010324568319
Iteration 18700: Loss = -9978.010544182975
1
Iteration 18800: Loss = -9978.010281156228
Iteration 18900: Loss = -9978.010279087846
Iteration 19000: Loss = -9978.011504682972
1
Iteration 19100: Loss = -9978.010797627148
2
Iteration 19200: Loss = -9978.218942535219
3
Iteration 19300: Loss = -9978.01028702456
Iteration 19400: Loss = -9978.02005233287
1
Iteration 19500: Loss = -9978.01029757229
Iteration 19600: Loss = -9978.0109661666
1
Iteration 19700: Loss = -9978.199541960697
2
Iteration 19800: Loss = -9978.010302097424
Iteration 19900: Loss = -9978.01113133104
1
pi: tensor([[0.9802, 0.0198],
        [0.9551, 0.0449]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9258, 0.0742], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1362, 0.2006],
         [0.6785, 0.2658]],

        [[0.5683, 0.1899],
         [0.7060, 0.6247]],

        [[0.6998, 0.1172],
         [0.5714, 0.7110]],

        [[0.6197, 0.1758],
         [0.7045, 0.5851]],

        [[0.5549, 0.1732],
         [0.5167, 0.5121]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 63
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0031224224336966746
Average Adjusted Rand Index: -0.0008743023574608124
10094.80456310901
[-0.008712345221699978, 0.0031224224336966746] [-0.00897846467601823, -0.0008743023574608124] [9979.153378983203, 9978.010261835598]
-------------------------------------
This iteration is 19
True Objective function: Loss = -10202.101027632372
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23798.349294061365
Iteration 100: Loss = -10075.976784589957
Iteration 200: Loss = -10073.72915473671
Iteration 300: Loss = -10072.442002339503
Iteration 400: Loss = -10070.934773084524
Iteration 500: Loss = -10069.88563408309
Iteration 600: Loss = -10069.152427798686
Iteration 700: Loss = -10068.799246369443
Iteration 800: Loss = -10068.55433790834
Iteration 900: Loss = -10068.311106866568
Iteration 1000: Loss = -10066.541503466608
Iteration 1100: Loss = -10064.525474301152
Iteration 1200: Loss = -10064.237358282242
Iteration 1300: Loss = -10064.112319129448
Iteration 1400: Loss = -10064.02443264845
Iteration 1500: Loss = -10063.978540141172
Iteration 1600: Loss = -10063.96217319572
Iteration 1700: Loss = -10063.953674382334
Iteration 1800: Loss = -10063.947043943217
Iteration 1900: Loss = -10063.94059437661
Iteration 2000: Loss = -10063.93335118367
Iteration 2100: Loss = -10063.924314632248
Iteration 2200: Loss = -10063.909871746017
Iteration 2300: Loss = -10063.856010515781
Iteration 2400: Loss = -10063.786021941318
Iteration 2500: Loss = -10063.747783269775
Iteration 2600: Loss = -10063.740055143706
Iteration 2700: Loss = -10063.737305244356
Iteration 2800: Loss = -10063.735657391491
Iteration 2900: Loss = -10063.7345956816
Iteration 3000: Loss = -10063.733819924226
Iteration 3100: Loss = -10063.733326179377
Iteration 3200: Loss = -10063.733018240337
Iteration 3300: Loss = -10063.73272373571
Iteration 3400: Loss = -10063.732532810744
Iteration 3500: Loss = -10063.732340520108
Iteration 3600: Loss = -10063.732208230263
Iteration 3700: Loss = -10063.732076904267
Iteration 3800: Loss = -10063.731907620744
Iteration 3900: Loss = -10063.731801962975
Iteration 4000: Loss = -10063.731743365312
Iteration 4100: Loss = -10063.73159357043
Iteration 4200: Loss = -10063.73145795073
Iteration 4300: Loss = -10063.73127054427
Iteration 4400: Loss = -10063.73104674529
Iteration 4500: Loss = -10063.730923413636
Iteration 4600: Loss = -10063.730852375778
Iteration 4700: Loss = -10063.736799674556
1
Iteration 4800: Loss = -10063.730745836963
Iteration 4900: Loss = -10063.730764332257
Iteration 5000: Loss = -10063.731505536934
1
Iteration 5100: Loss = -10063.730748243315
Iteration 5200: Loss = -10063.731479528287
1
Iteration 5300: Loss = -10063.730676560268
Iteration 5400: Loss = -10063.731191282082
1
Iteration 5500: Loss = -10063.730641771233
Iteration 5600: Loss = -10063.730754510738
1
Iteration 5700: Loss = -10063.730677725509
Iteration 5800: Loss = -10063.730645867443
Iteration 5900: Loss = -10063.730701204822
Iteration 6000: Loss = -10063.730631585813
Iteration 6100: Loss = -10063.731774755122
1
Iteration 6200: Loss = -10063.730639987383
Iteration 6300: Loss = -10063.730628898138
Iteration 6400: Loss = -10063.73090018801
1
Iteration 6500: Loss = -10063.730638215915
Iteration 6600: Loss = -10063.732190446932
1
Iteration 6700: Loss = -10063.730562700868
Iteration 6800: Loss = -10063.733106522175
1
Iteration 6900: Loss = -10063.730609835095
Iteration 7000: Loss = -10063.730590905052
Iteration 7100: Loss = -10063.73061785387
Iteration 7200: Loss = -10063.730573503442
Iteration 7300: Loss = -10063.731590849598
1
Iteration 7400: Loss = -10063.730616044171
Iteration 7500: Loss = -10063.730603258307
Iteration 7600: Loss = -10063.73062368107
Iteration 7700: Loss = -10063.730627830579
Iteration 7800: Loss = -10063.734334526136
1
Iteration 7900: Loss = -10063.730602216106
Iteration 8000: Loss = -10063.730632407109
Iteration 8100: Loss = -10063.730627186547
Iteration 8200: Loss = -10063.730595023546
Iteration 8300: Loss = -10063.730779259511
1
Iteration 8400: Loss = -10063.730588392786
Iteration 8500: Loss = -10063.734111257856
1
Iteration 8600: Loss = -10063.730589224826
Iteration 8700: Loss = -10063.730624745058
Iteration 8800: Loss = -10063.730619407483
Iteration 8900: Loss = -10063.73116385495
1
Iteration 9000: Loss = -10063.733944585892
2
Iteration 9100: Loss = -10063.731072884713
3
Iteration 9200: Loss = -10063.73148951797
4
Iteration 9300: Loss = -10063.730689678396
Iteration 9400: Loss = -10063.73063751502
Iteration 9500: Loss = -10063.789724801974
1
Iteration 9600: Loss = -10063.730598026797
Iteration 9700: Loss = -10063.732464280953
1
Iteration 9800: Loss = -10063.730587391008
Iteration 9900: Loss = -10063.730628672714
Iteration 10000: Loss = -10063.730624064963
Iteration 10100: Loss = -10063.730760639844
1
Iteration 10200: Loss = -10063.74356655817
2
Iteration 10300: Loss = -10063.743577583944
3
Iteration 10400: Loss = -10063.764505823692
4
Iteration 10500: Loss = -10063.730647416749
Iteration 10600: Loss = -10063.731906368732
1
Iteration 10700: Loss = -10063.797092726514
2
Iteration 10800: Loss = -10063.730619118898
Iteration 10900: Loss = -10063.730626233799
Iteration 11000: Loss = -10063.731164500685
1
Iteration 11100: Loss = -10063.73093846769
2
Iteration 11200: Loss = -10063.795252919004
3
Iteration 11300: Loss = -10063.73534805045
4
Iteration 11400: Loss = -10063.755535858496
5
Iteration 11500: Loss = -10063.79848312759
6
Iteration 11600: Loss = -10063.748503765846
7
Iteration 11700: Loss = -10063.739635860555
8
Iteration 11800: Loss = -10063.735841989046
9
Iteration 11900: Loss = -10063.730798261371
10
Iteration 12000: Loss = -10063.730657841154
Iteration 12100: Loss = -10063.741742767805
1
Iteration 12200: Loss = -10063.74745313332
2
Iteration 12300: Loss = -10063.738803364324
3
Iteration 12400: Loss = -10063.730831228926
4
Iteration 12500: Loss = -10063.730701220791
Iteration 12600: Loss = -10063.745889329588
1
Iteration 12700: Loss = -10063.730594029714
Iteration 12800: Loss = -10063.735913481103
1
Iteration 12900: Loss = -10063.730607548761
Iteration 13000: Loss = -10063.73405839979
1
Iteration 13100: Loss = -10063.738280030828
2
Iteration 13200: Loss = -10063.748136508902
3
Iteration 13300: Loss = -10063.731694852926
4
Iteration 13400: Loss = -10063.74634277269
5
Iteration 13500: Loss = -10063.756598530908
6
Iteration 13600: Loss = -10063.732236401134
7
Iteration 13700: Loss = -10063.73097761459
8
Iteration 13800: Loss = -10063.732438517187
9
Iteration 13900: Loss = -10063.7310897112
10
Iteration 14000: Loss = -10063.73064086271
Iteration 14100: Loss = -10063.7315398949
1
Iteration 14200: Loss = -10063.841044409734
2
Iteration 14300: Loss = -10063.73057675939
Iteration 14400: Loss = -10063.732450855145
1
Iteration 14500: Loss = -10063.730560107273
Iteration 14600: Loss = -10063.745980052927
1
Iteration 14700: Loss = -10063.73057529532
Iteration 14800: Loss = -10063.744980956837
1
Iteration 14900: Loss = -10063.771722790101
2
Iteration 15000: Loss = -10063.739622777219
3
Iteration 15100: Loss = -10063.751255104362
4
Iteration 15200: Loss = -10063.732173581606
5
Iteration 15300: Loss = -10063.737640141337
6
Iteration 15400: Loss = -10063.730678252705
7
Iteration 15500: Loss = -10063.73099850959
8
Iteration 15600: Loss = -10063.743211571697
9
Iteration 15700: Loss = -10063.73158027852
10
Iteration 15800: Loss = -10063.748763348907
11
Iteration 15900: Loss = -10063.74673567845
12
Iteration 16000: Loss = -10063.739760543369
13
Iteration 16100: Loss = -10063.735959919855
14
Iteration 16200: Loss = -10063.730710077372
15
Stopping early at iteration 16200 due to no improvement.
pi: tensor([[0.9377, 0.0623],
        [0.4333, 0.5667]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9135, 0.0865], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1333, 0.1914],
         [0.5677, 0.4492]],

        [[0.5502, 0.1384],
         [0.6491, 0.7087]],

        [[0.6555, 0.1668],
         [0.6327, 0.6956]],

        [[0.6568, 0.1268],
         [0.5366, 0.5439]],

        [[0.7054, 0.1557],
         [0.5577, 0.6145]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.015743731881538
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.003422492374587702
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0531930439865556
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 69
Adjusted Rand Index: 0.12744687447309594
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.033695242120345935
Global Adjusted Rand Index: 0.024880945161016303
Average Adjusted Rand Index: 0.039033787264774356
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22478.88756267066
Iteration 100: Loss = -10074.080350892707
Iteration 200: Loss = -10073.291219219243
Iteration 300: Loss = -10072.501547857582
Iteration 400: Loss = -10071.313324238808
Iteration 500: Loss = -10070.755822941357
Iteration 600: Loss = -10070.210199567768
Iteration 700: Loss = -10069.725407148484
Iteration 800: Loss = -10069.037330260426
Iteration 900: Loss = -10068.340576196626
Iteration 1000: Loss = -10068.124275269
Iteration 1100: Loss = -10068.002656364153
Iteration 1200: Loss = -10067.911219794978
Iteration 1300: Loss = -10067.83433577914
Iteration 1400: Loss = -10067.761564054199
Iteration 1500: Loss = -10067.668708009825
Iteration 1600: Loss = -10067.322966455049
Iteration 1700: Loss = -10065.501947244622
Iteration 1800: Loss = -10064.386041875965
Iteration 1900: Loss = -10064.060338478315
Iteration 2000: Loss = -10063.940811058483
Iteration 2100: Loss = -10063.879537806306
Iteration 2200: Loss = -10063.836155512572
Iteration 2300: Loss = -10063.800251550862
Iteration 2400: Loss = -10063.774732394044
Iteration 2500: Loss = -10063.760189842942
Iteration 2600: Loss = -10063.752795966082
Iteration 2700: Loss = -10063.748049710895
Iteration 2800: Loss = -10063.744232953408
Iteration 2900: Loss = -10063.740878582203
Iteration 3000: Loss = -10063.738120073149
Iteration 3100: Loss = -10063.7363541422
Iteration 3200: Loss = -10063.735253298024
Iteration 3300: Loss = -10063.7345239138
Iteration 3400: Loss = -10063.733913707872
Iteration 3500: Loss = -10063.73348121995
Iteration 3600: Loss = -10063.733108766373
Iteration 3700: Loss = -10063.732757214455
Iteration 3800: Loss = -10063.73249690596
Iteration 3900: Loss = -10063.732302212831
Iteration 4000: Loss = -10063.732132102414
Iteration 4100: Loss = -10063.733595038339
1
Iteration 4200: Loss = -10063.731782780238
Iteration 4300: Loss = -10063.731658971346
Iteration 4400: Loss = -10063.734577788557
1
Iteration 4500: Loss = -10063.731474439031
Iteration 4600: Loss = -10063.731376373318
Iteration 4700: Loss = -10063.732596332702
1
Iteration 4800: Loss = -10063.731192423747
Iteration 4900: Loss = -10063.731170341984
Iteration 5000: Loss = -10063.731869165644
1
Iteration 5100: Loss = -10063.73108039473
Iteration 5200: Loss = -10063.73103182924
Iteration 5300: Loss = -10063.730994295116
Iteration 5400: Loss = -10063.730955104042
Iteration 5500: Loss = -10063.730908963062
Iteration 5600: Loss = -10063.73089213644
Iteration 5700: Loss = -10063.730835359875
Iteration 5800: Loss = -10063.730829581238
Iteration 5900: Loss = -10063.730874521296
Iteration 6000: Loss = -10063.730818424661
Iteration 6100: Loss = -10063.733887098773
1
Iteration 6200: Loss = -10063.7307929205
Iteration 6300: Loss = -10063.730760334585
Iteration 6400: Loss = -10063.730755334253
Iteration 6500: Loss = -10063.730710687552
Iteration 6600: Loss = -10063.733798155252
1
Iteration 6700: Loss = -10063.730725284819
Iteration 6800: Loss = -10063.733721216655
1
Iteration 6900: Loss = -10063.73070697789
Iteration 7000: Loss = -10063.730685542025
Iteration 7100: Loss = -10063.730682493548
Iteration 7200: Loss = -10063.73067687692
Iteration 7300: Loss = -10063.7327064104
1
Iteration 7400: Loss = -10063.730683149543
Iteration 7500: Loss = -10063.73065300347
Iteration 7600: Loss = -10063.730657048334
Iteration 7700: Loss = -10063.73062704873
Iteration 7800: Loss = -10063.730626148064
Iteration 7900: Loss = -10063.731071642434
1
Iteration 8000: Loss = -10063.730652474645
Iteration 8100: Loss = -10063.914346448313
1
Iteration 8200: Loss = -10063.730582969334
Iteration 8300: Loss = -10063.730634376268
Iteration 8400: Loss = -10063.730814040387
1
Iteration 8500: Loss = -10063.730594280461
Iteration 8600: Loss = -10063.730621787761
Iteration 8700: Loss = -10063.731312587657
1
Iteration 8800: Loss = -10063.730615568551
Iteration 8900: Loss = -10063.730621154105
Iteration 9000: Loss = -10063.730922750916
1
Iteration 9100: Loss = -10063.730625933797
Iteration 9200: Loss = -10063.73100386912
1
Iteration 9300: Loss = -10063.73063190468
Iteration 9400: Loss = -10063.73062650243
Iteration 9500: Loss = -10063.730609252707
Iteration 9600: Loss = -10063.730662143034
Iteration 9700: Loss = -10063.730573008346
Iteration 9800: Loss = -10063.730609219221
Iteration 9900: Loss = -10063.730623358593
Iteration 10000: Loss = -10063.730602197184
Iteration 10100: Loss = -10063.73199615428
1
Iteration 10200: Loss = -10063.730614025224
Iteration 10300: Loss = -10063.731047810725
1
Iteration 10400: Loss = -10063.73059840114
Iteration 10500: Loss = -10063.740284430216
1
Iteration 10600: Loss = -10063.73234519207
2
Iteration 10700: Loss = -10063.730617482788
Iteration 10800: Loss = -10063.741333948516
1
Iteration 10900: Loss = -10063.73315207202
2
Iteration 11000: Loss = -10063.730739962899
3
Iteration 11100: Loss = -10063.731811822756
4
Iteration 11200: Loss = -10063.731898869168
5
Iteration 11300: Loss = -10063.733839678625
6
Iteration 11400: Loss = -10063.735052860908
7
Iteration 11500: Loss = -10063.764792810794
8
Iteration 11600: Loss = -10063.785911037843
9
Iteration 11700: Loss = -10063.735430064893
10
Iteration 11800: Loss = -10063.730623813337
Iteration 11900: Loss = -10063.731733753653
1
Iteration 12000: Loss = -10063.731955512438
2
Iteration 12100: Loss = -10063.748870739268
3
Iteration 12200: Loss = -10063.734312342649
4
Iteration 12300: Loss = -10063.74168249253
5
Iteration 12400: Loss = -10063.734683367122
6
Iteration 12500: Loss = -10063.730631353099
Iteration 12600: Loss = -10063.730806164325
1
Iteration 12700: Loss = -10063.731391780448
2
Iteration 12800: Loss = -10063.730758814796
3
Iteration 12900: Loss = -10063.730611641522
Iteration 13000: Loss = -10063.737374991364
1
Iteration 13100: Loss = -10063.803411956029
2
Iteration 13200: Loss = -10063.747179883136
3
Iteration 13300: Loss = -10063.760789923761
4
Iteration 13400: Loss = -10063.733544388357
5
Iteration 13500: Loss = -10063.734235049817
6
Iteration 13600: Loss = -10063.730671021496
Iteration 13700: Loss = -10063.731066327742
1
Iteration 13800: Loss = -10063.732631288578
2
Iteration 13900: Loss = -10063.735295674913
3
Iteration 14000: Loss = -10063.792231639687
4
Iteration 14100: Loss = -10063.733878155364
5
Iteration 14200: Loss = -10063.732530448917
6
Iteration 14300: Loss = -10063.906256993021
7
Iteration 14400: Loss = -10063.730591475605
Iteration 14500: Loss = -10063.731342562623
1
Iteration 14600: Loss = -10063.730745628844
2
Iteration 14700: Loss = -10063.730627066734
Iteration 14800: Loss = -10063.78550778341
1
Iteration 14900: Loss = -10063.736489121173
2
Iteration 15000: Loss = -10063.910830101475
3
Iteration 15100: Loss = -10063.730610489809
Iteration 15200: Loss = -10063.750162161232
1
Iteration 15300: Loss = -10063.731643778041
2
Iteration 15400: Loss = -10063.730595744353
Iteration 15500: Loss = -10063.739672243628
1
Iteration 15600: Loss = -10063.731600662855
2
Iteration 15700: Loss = -10063.730625086706
Iteration 15800: Loss = -10063.757841269595
1
Iteration 15900: Loss = -10063.742129176251
2
Iteration 16000: Loss = -10063.856852809678
3
Iteration 16100: Loss = -10063.730634530546
Iteration 16200: Loss = -10063.730752026187
1
Iteration 16300: Loss = -10063.73063476203
Iteration 16400: Loss = -10063.730671124764
Iteration 16500: Loss = -10063.73069720095
Iteration 16600: Loss = -10063.732799593829
1
Iteration 16700: Loss = -10063.730852712879
2
Iteration 16800: Loss = -10063.737766022794
3
Iteration 16900: Loss = -10063.733627597067
4
Iteration 17000: Loss = -10063.735269468378
5
Iteration 17100: Loss = -10063.73227497939
6
Iteration 17200: Loss = -10063.736589556489
7
Iteration 17300: Loss = -10063.736112752726
8
Iteration 17400: Loss = -10063.810768618398
9
Iteration 17500: Loss = -10063.876031869868
10
Iteration 17600: Loss = -10063.730596954492
Iteration 17700: Loss = -10063.73136818989
1
Iteration 17800: Loss = -10063.730885226187
2
Iteration 17900: Loss = -10063.730624165193
Iteration 18000: Loss = -10063.731200680539
1
Iteration 18100: Loss = -10063.927603534994
2
Iteration 18200: Loss = -10063.730606597
Iteration 18300: Loss = -10063.730866849948
1
Iteration 18400: Loss = -10063.730599297183
Iteration 18500: Loss = -10063.730589105906
Iteration 18600: Loss = -10063.730699536662
1
Iteration 18700: Loss = -10063.732487452338
2
Iteration 18800: Loss = -10063.733058180287
3
Iteration 18900: Loss = -10063.732269626853
4
Iteration 19000: Loss = -10063.732702088178
5
Iteration 19100: Loss = -10063.755410216727
6
Iteration 19200: Loss = -10063.766251908373
7
Iteration 19300: Loss = -10063.742725560167
8
Iteration 19400: Loss = -10063.731955275996
9
Iteration 19500: Loss = -10063.732551099101
10
Iteration 19600: Loss = -10063.733597038019
11
Iteration 19700: Loss = -10063.731067849192
12
Iteration 19800: Loss = -10063.731415230537
13
Iteration 19900: Loss = -10063.856854347563
14
pi: tensor([[0.5666, 0.4334],
        [0.0623, 0.9377]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0866, 0.9134], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4492, 0.1914],
         [0.6609, 0.1333]],

        [[0.6611, 0.1384],
         [0.5626, 0.5489]],

        [[0.6101, 0.1668],
         [0.6891, 0.7290]],

        [[0.6170, 0.1268],
         [0.6576, 0.7000]],

        [[0.6663, 0.1558],
         [0.6028, 0.6580]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.015743731881538
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.003422492374587702
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0531930439865556
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 31
Adjusted Rand Index: 0.12744687447309594
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.033695242120345935
Global Adjusted Rand Index: 0.024880945161016303
Average Adjusted Rand Index: 0.039033787264774356
10202.101027632372
[0.024880945161016303, 0.024880945161016303] [0.039033787264774356, 0.039033787264774356] [10063.730710077372, 10063.730705814722]
-------------------------------------
This iteration is 20
True Objective function: Loss = -10126.457678183191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22577.671563284708
Iteration 100: Loss = -10033.401958677587
Iteration 200: Loss = -10027.826720558034
Iteration 300: Loss = -10027.143485181869
Iteration 400: Loss = -10026.918791605816
Iteration 500: Loss = -10026.815097711027
Iteration 600: Loss = -10026.755620017731
Iteration 700: Loss = -10026.717151467428
Iteration 800: Loss = -10026.69106456254
Iteration 900: Loss = -10026.672416368601
Iteration 1000: Loss = -10026.658289125699
Iteration 1100: Loss = -10026.646955113956
Iteration 1200: Loss = -10026.636835261601
Iteration 1300: Loss = -10026.626799760734
Iteration 1400: Loss = -10026.615649075307
Iteration 1500: Loss = -10026.602304712884
Iteration 1600: Loss = -10026.586186334298
Iteration 1700: Loss = -10026.56793441036
Iteration 1800: Loss = -10026.549264590474
Iteration 1900: Loss = -10026.532067825965
Iteration 2000: Loss = -10026.517130519265
Iteration 2100: Loss = -10026.504255383568
Iteration 2200: Loss = -10026.493052582784
Iteration 2300: Loss = -10026.483127777452
Iteration 2400: Loss = -10026.474344394488
Iteration 2500: Loss = -10026.466439020161
Iteration 2600: Loss = -10026.459213421329
Iteration 2700: Loss = -10026.452509441284
Iteration 2800: Loss = -10026.44618640305
Iteration 2900: Loss = -10026.440003415473
Iteration 3000: Loss = -10026.433709652683
Iteration 3100: Loss = -10026.427105656514
Iteration 3200: Loss = -10026.419755912868
Iteration 3300: Loss = -10026.411132671978
Iteration 3400: Loss = -10026.40052037852
Iteration 3500: Loss = -10026.387307158824
Iteration 3600: Loss = -10026.372001606913
Iteration 3700: Loss = -10026.356968651411
Iteration 3800: Loss = -10026.345149771689
Iteration 3900: Loss = -10026.337168616523
Iteration 4000: Loss = -10026.331608385415
Iteration 4100: Loss = -10026.3273741586
Iteration 4200: Loss = -10026.323918024707
Iteration 4300: Loss = -10026.320979774871
Iteration 4400: Loss = -10026.318550145572
Iteration 4500: Loss = -10026.31637245928
Iteration 4600: Loss = -10026.314453982552
Iteration 4700: Loss = -10026.312724146232
Iteration 4800: Loss = -10026.311231308842
Iteration 4900: Loss = -10026.309764924805
Iteration 5000: Loss = -10026.30853784629
Iteration 5100: Loss = -10026.307341102567
Iteration 5200: Loss = -10026.30627609977
Iteration 5300: Loss = -10026.305284093361
Iteration 5400: Loss = -10026.304419441032
Iteration 5500: Loss = -10026.303672707092
Iteration 5600: Loss = -10026.302997410789
Iteration 5700: Loss = -10026.30239186774
Iteration 5800: Loss = -10026.301872848117
Iteration 5900: Loss = -10026.30134633261
Iteration 6000: Loss = -10026.300903260953
Iteration 6100: Loss = -10026.300464876456
Iteration 6200: Loss = -10026.300056197866
Iteration 6300: Loss = -10026.29966459532
Iteration 6400: Loss = -10026.2993352915
Iteration 6500: Loss = -10026.298973492207
Iteration 6600: Loss = -10026.299716022739
1
Iteration 6700: Loss = -10026.2987399544
Iteration 6800: Loss = -10026.29833480458
Iteration 6900: Loss = -10026.29780894815
Iteration 7000: Loss = -10026.297607424302
Iteration 7100: Loss = -10026.297369608721
Iteration 7200: Loss = -10026.29713188746
Iteration 7300: Loss = -10026.296914365988
Iteration 7400: Loss = -10026.296791623081
Iteration 7500: Loss = -10026.296519369045
Iteration 7600: Loss = -10026.29640601309
Iteration 7700: Loss = -10026.296212537389
Iteration 7800: Loss = -10026.296066327977
Iteration 7900: Loss = -10026.295910985133
Iteration 8000: Loss = -10026.295757229173
Iteration 8100: Loss = -10026.295719376207
Iteration 8200: Loss = -10026.295519846839
Iteration 8300: Loss = -10026.29567015105
1
Iteration 8400: Loss = -10026.295524372223
Iteration 8500: Loss = -10026.29519302951
Iteration 8600: Loss = -10026.295547623076
1
Iteration 8700: Loss = -10026.295396763275
2
Iteration 8800: Loss = -10026.29684895818
3
Iteration 8900: Loss = -10026.29482720034
Iteration 9000: Loss = -10026.29601420737
1
Iteration 9100: Loss = -10026.294651745606
Iteration 9200: Loss = -10026.297180572774
1
Iteration 9300: Loss = -10026.294502468008
Iteration 9400: Loss = -10026.375439153404
1
Iteration 9500: Loss = -10026.294423096238
Iteration 9600: Loss = -10026.294323672462
Iteration 9700: Loss = -10026.295392925229
1
Iteration 9800: Loss = -10026.294226728942
Iteration 9900: Loss = -10026.294354539908
1
Iteration 10000: Loss = -10026.294110748615
Iteration 10100: Loss = -10026.294074527244
Iteration 10200: Loss = -10026.298565986488
1
Iteration 10300: Loss = -10026.294011747517
Iteration 10400: Loss = -10026.294026148082
Iteration 10500: Loss = -10026.293929598165
Iteration 10600: Loss = -10026.293956885642
Iteration 10700: Loss = -10026.293831195671
Iteration 10800: Loss = -10026.294681749567
1
Iteration 10900: Loss = -10026.29380127711
Iteration 11000: Loss = -10026.295323211829
1
Iteration 11100: Loss = -10026.293732972736
Iteration 11200: Loss = -10026.29369526253
Iteration 11300: Loss = -10026.363469360867
1
Iteration 11400: Loss = -10026.293843484442
2
Iteration 11500: Loss = -10026.293632140729
Iteration 11600: Loss = -10026.29513284326
1
Iteration 11700: Loss = -10026.293573025454
Iteration 11800: Loss = -10026.293553079535
Iteration 11900: Loss = -10026.29364373823
Iteration 12000: Loss = -10026.293862528226
1
Iteration 12100: Loss = -10026.293515484442
Iteration 12200: Loss = -10026.293501516113
Iteration 12300: Loss = -10026.293550404162
Iteration 12400: Loss = -10026.293470458635
Iteration 12500: Loss = -10026.293491796967
Iteration 12600: Loss = -10026.294346041639
1
Iteration 12700: Loss = -10026.293469080905
Iteration 12800: Loss = -10026.293881447227
1
Iteration 12900: Loss = -10026.29337685259
Iteration 13000: Loss = -10026.441381845385
1
Iteration 13100: Loss = -10026.293413528709
Iteration 13200: Loss = -10026.293389732982
Iteration 13300: Loss = -10026.293542719246
1
Iteration 13400: Loss = -10026.293359820671
Iteration 13500: Loss = -10026.299313704108
1
Iteration 13600: Loss = -10026.293338734942
Iteration 13700: Loss = -10026.304224910624
1
Iteration 13800: Loss = -10026.293323224903
Iteration 13900: Loss = -10026.564626778549
1
Iteration 14000: Loss = -10026.29334317892
Iteration 14100: Loss = -10026.345863484801
1
Iteration 14200: Loss = -10026.293329518652
Iteration 14300: Loss = -10026.293333937108
Iteration 14400: Loss = -10026.293353164063
Iteration 14500: Loss = -10026.293622274312
1
Iteration 14600: Loss = -10026.293536966963
2
Iteration 14700: Loss = -10026.298685092348
3
Iteration 14800: Loss = -10026.293262188772
Iteration 14900: Loss = -10026.293266741002
Iteration 15000: Loss = -10026.30646293116
1
Iteration 15100: Loss = -10026.293294445446
Iteration 15200: Loss = -10026.293280031758
Iteration 15300: Loss = -10026.293295867925
Iteration 15400: Loss = -10026.293264764558
Iteration 15500: Loss = -10026.293250078174
Iteration 15600: Loss = -10026.300007086578
1
Iteration 15700: Loss = -10026.296521718716
2
Iteration 15800: Loss = -10026.293754435199
3
Iteration 15900: Loss = -10026.293265312894
Iteration 16000: Loss = -10026.295408867358
1
Iteration 16100: Loss = -10026.293289025583
Iteration 16200: Loss = -10026.362260419335
1
Iteration 16300: Loss = -10026.29322038945
Iteration 16400: Loss = -10026.293279947002
Iteration 16500: Loss = -10026.293659532288
1
Iteration 16600: Loss = -10026.298214798038
2
Iteration 16700: Loss = -10026.293242634445
Iteration 16800: Loss = -10026.293607499058
1
Iteration 16900: Loss = -10026.293268768199
Iteration 17000: Loss = -10026.293327394009
Iteration 17100: Loss = -10026.294586264861
1
Iteration 17200: Loss = -10026.293372269245
Iteration 17300: Loss = -10026.300378225978
1
Iteration 17400: Loss = -10026.29404345843
2
Iteration 17500: Loss = -10026.29413977408
3
Iteration 17600: Loss = -10026.293426636337
Iteration 17700: Loss = -10026.29333578026
Iteration 17800: Loss = -10026.303400885763
1
Iteration 17900: Loss = -10026.294898814504
2
Iteration 18000: Loss = -10026.293355866123
Iteration 18100: Loss = -10026.2956433597
1
Iteration 18200: Loss = -10026.296496837742
2
Iteration 18300: Loss = -10026.293224246601
Iteration 18400: Loss = -10026.301095050429
1
Iteration 18500: Loss = -10026.29320680846
Iteration 18600: Loss = -10026.297063283595
1
Iteration 18700: Loss = -10026.293191788503
Iteration 18800: Loss = -10026.293402480786
1
Iteration 18900: Loss = -10026.293199928286
Iteration 19000: Loss = -10026.297044062127
1
Iteration 19100: Loss = -10026.293207872126
Iteration 19200: Loss = -10026.330566413153
1
Iteration 19300: Loss = -10026.293217922332
Iteration 19400: Loss = -10026.321079929148
1
Iteration 19500: Loss = -10026.293245977135
Iteration 19600: Loss = -10026.29394508196
1
Iteration 19700: Loss = -10026.29575171673
2
Iteration 19800: Loss = -10026.293202101779
Iteration 19900: Loss = -10026.300041225459
1
pi: tensor([[1.0000e+00, 7.2685e-08],
        [6.8884e-01, 3.1116e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8984, 0.1016], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1368, 0.2065],
         [0.7203, 0.3035]],

        [[0.6835, 0.1539],
         [0.6481, 0.5478]],

        [[0.6746, 0.2222],
         [0.7300, 0.6008]],

        [[0.7137, 0.1426],
         [0.5755, 0.6852]],

        [[0.6768, 0.0931],
         [0.7112, 0.5087]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.024360446076493753
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.004060309377102018
Average Adjusted Rand Index: -0.004673015611342496
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23593.101714962842
Iteration 100: Loss = -10029.271359495668
Iteration 200: Loss = -10028.064214877772
Iteration 300: Loss = -10027.711558239165
Iteration 400: Loss = -10027.453277004306
Iteration 500: Loss = -10027.207296501543
Iteration 600: Loss = -10026.972343551197
Iteration 700: Loss = -10026.869131270309
Iteration 800: Loss = -10026.815697832812
Iteration 900: Loss = -10026.783398932439
Iteration 1000: Loss = -10026.762557611122
Iteration 1100: Loss = -10026.750985488774
Iteration 1200: Loss = -10026.74353252196
Iteration 1300: Loss = -10026.738468719603
Iteration 1400: Loss = -10026.73492843822
Iteration 1500: Loss = -10026.732283061494
Iteration 1600: Loss = -10026.730240817693
Iteration 1700: Loss = -10026.728534021102
Iteration 1800: Loss = -10026.726967534967
Iteration 1900: Loss = -10026.725129848775
Iteration 2000: Loss = -10026.721208604602
Iteration 2100: Loss = -10026.717807980185
Iteration 2200: Loss = -10026.716745506614
Iteration 2300: Loss = -10026.715649339916
Iteration 2400: Loss = -10026.714522810496
Iteration 2500: Loss = -10026.71321376682
Iteration 2600: Loss = -10026.71167373311
Iteration 2700: Loss = -10026.709694892523
Iteration 2800: Loss = -10026.706971374793
Iteration 2900: Loss = -10026.702960713614
Iteration 3000: Loss = -10026.69641186204
Iteration 3100: Loss = -10026.684636551345
Iteration 3200: Loss = -10026.661692831547
Iteration 3300: Loss = -10026.627354237095
Iteration 3400: Loss = -10026.603983349754
Iteration 3500: Loss = -10026.590135633374
Iteration 3600: Loss = -10026.580047654425
Iteration 3700: Loss = -10026.572818477436
Iteration 3800: Loss = -10026.567923929942
Iteration 3900: Loss = -10026.564923169226
Iteration 4000: Loss = -10026.563157336153
Iteration 4100: Loss = -10026.562179278777
Iteration 4200: Loss = -10026.561607284244
Iteration 4300: Loss = -10026.561193329746
Iteration 4400: Loss = -10026.560864254729
Iteration 4500: Loss = -10026.56060384317
Iteration 4600: Loss = -10026.56036194036
Iteration 4700: Loss = -10026.560161094285
Iteration 4800: Loss = -10026.55995860889
Iteration 4900: Loss = -10026.559767323626
Iteration 5000: Loss = -10026.560080742118
1
Iteration 5100: Loss = -10026.55947709465
Iteration 5200: Loss = -10026.559354070318
Iteration 5300: Loss = -10026.55920906692
Iteration 5400: Loss = -10026.559116645773
Iteration 5500: Loss = -10026.560197009578
1
Iteration 5600: Loss = -10026.558910905218
Iteration 5700: Loss = -10026.559204944706
1
Iteration 5800: Loss = -10026.558767968972
Iteration 5900: Loss = -10026.558726458312
Iteration 6000: Loss = -10026.559738882985
1
Iteration 6100: Loss = -10026.558549074556
Iteration 6200: Loss = -10026.558509578657
Iteration 6300: Loss = -10026.5584273722
Iteration 6400: Loss = -10026.564833518307
1
Iteration 6500: Loss = -10026.558312709205
Iteration 6600: Loss = -10026.558283259983
Iteration 6700: Loss = -10026.55889282412
1
Iteration 6800: Loss = -10026.558206392456
Iteration 6900: Loss = -10026.558318036616
1
Iteration 7000: Loss = -10026.558133532522
Iteration 7100: Loss = -10026.558072692225
Iteration 7200: Loss = -10026.558081250172
Iteration 7300: Loss = -10026.558070078809
Iteration 7400: Loss = -10026.558262224084
1
Iteration 7500: Loss = -10026.55978351029
2
Iteration 7600: Loss = -10026.557949428452
Iteration 7700: Loss = -10026.557937499007
Iteration 7800: Loss = -10026.557896613647
Iteration 7900: Loss = -10026.557863916089
Iteration 8000: Loss = -10026.557915422769
Iteration 8100: Loss = -10026.557843570661
Iteration 8200: Loss = -10026.557839536383
Iteration 8300: Loss = -10026.557821733188
Iteration 8400: Loss = -10026.557802390935
Iteration 8500: Loss = -10026.557820289298
Iteration 8600: Loss = -10026.557751319018
Iteration 8700: Loss = -10026.557980050364
1
Iteration 8800: Loss = -10026.560148826371
2
Iteration 8900: Loss = -10026.557757344044
Iteration 9000: Loss = -10026.644589544183
1
Iteration 9100: Loss = -10026.557716390329
Iteration 9200: Loss = -10026.67975313299
1
Iteration 9300: Loss = -10026.557694339783
Iteration 9400: Loss = -10026.558479306319
1
Iteration 9500: Loss = -10026.557722516927
Iteration 9600: Loss = -10026.55766249265
Iteration 9700: Loss = -10026.56072714841
1
Iteration 9800: Loss = -10026.55762357643
Iteration 9900: Loss = -10026.55762776033
Iteration 10000: Loss = -10026.56866953292
1
Iteration 10100: Loss = -10026.557626414005
Iteration 10200: Loss = -10026.584147927035
1
Iteration 10300: Loss = -10026.557566139276
Iteration 10400: Loss = -10026.557605386439
Iteration 10500: Loss = -10026.557713787857
1
Iteration 10600: Loss = -10026.557596762008
Iteration 10700: Loss = -10026.55760960007
Iteration 10800: Loss = -10026.559181932378
1
Iteration 10900: Loss = -10026.557564340812
Iteration 11000: Loss = -10026.557568519198
Iteration 11100: Loss = -10026.557560607604
Iteration 11200: Loss = -10026.557797304411
1
Iteration 11300: Loss = -10026.557536750825
Iteration 11400: Loss = -10026.557557802975
Iteration 11500: Loss = -10026.558956553637
1
Iteration 11600: Loss = -10026.557546854116
Iteration 11700: Loss = -10026.559396028495
1
Iteration 11800: Loss = -10026.557577224246
Iteration 11900: Loss = -10026.751324479561
1
Iteration 12000: Loss = -10026.557551030515
Iteration 12100: Loss = -10026.557572920767
Iteration 12200: Loss = -10026.557827877636
1
Iteration 12300: Loss = -10026.557537162376
Iteration 12400: Loss = -10026.785933958796
1
Iteration 12500: Loss = -10026.557533633792
Iteration 12600: Loss = -10026.557552213246
Iteration 12700: Loss = -10026.769612682108
1
Iteration 12800: Loss = -10026.557532269922
Iteration 12900: Loss = -10026.56393067907
1
Iteration 13000: Loss = -10026.557529065803
Iteration 13100: Loss = -10026.557520889146
Iteration 13200: Loss = -10026.560571379056
1
Iteration 13300: Loss = -10026.557532894527
Iteration 13400: Loss = -10026.557535153814
Iteration 13500: Loss = -10026.559225935023
1
Iteration 13600: Loss = -10026.557522540585
Iteration 13700: Loss = -10026.55876297767
1
Iteration 13800: Loss = -10026.557689804791
2
Iteration 13900: Loss = -10026.747509015113
3
Iteration 14000: Loss = -10026.557531913084
Iteration 14100: Loss = -10026.562716882125
1
Iteration 14200: Loss = -10026.557513968326
Iteration 14300: Loss = -10026.574228911111
1
Iteration 14400: Loss = -10026.557501192685
Iteration 14500: Loss = -10026.566483672004
1
Iteration 14600: Loss = -10026.557491343141
Iteration 14700: Loss = -10026.55750957595
Iteration 14800: Loss = -10026.55764668166
1
Iteration 14900: Loss = -10026.557544957257
Iteration 15000: Loss = -10026.557557585771
Iteration 15100: Loss = -10026.557519979762
Iteration 15200: Loss = -10026.567955428693
1
Iteration 15300: Loss = -10026.557525109572
Iteration 15400: Loss = -10026.55821239163
1
Iteration 15500: Loss = -10026.557516002285
Iteration 15600: Loss = -10026.558832801833
1
Iteration 15700: Loss = -10026.557529832366
Iteration 15800: Loss = -10026.557533387278
Iteration 15900: Loss = -10026.557961740138
1
Iteration 16000: Loss = -10026.55766989169
2
Iteration 16100: Loss = -10026.715445723983
3
Iteration 16200: Loss = -10026.557542769013
Iteration 16300: Loss = -10026.557506910802
Iteration 16400: Loss = -10026.559055779686
1
Iteration 16500: Loss = -10026.560206342325
2
Iteration 16600: Loss = -10026.557502722204
Iteration 16700: Loss = -10026.558351075393
1
Iteration 16800: Loss = -10026.557541712195
Iteration 16900: Loss = -10026.557517352585
Iteration 17000: Loss = -10026.562801032269
1
Iteration 17100: Loss = -10026.557530684582
Iteration 17200: Loss = -10026.557493402808
Iteration 17300: Loss = -10026.87060746274
1
Iteration 17400: Loss = -10026.557528030464
Iteration 17500: Loss = -10026.557477443892
Iteration 17600: Loss = -10026.5575906942
1
Iteration 17700: Loss = -10026.557531991648
Iteration 17800: Loss = -10026.736066155943
1
Iteration 17900: Loss = -10026.55798292001
2
Iteration 18000: Loss = -10026.557497259202
Iteration 18100: Loss = -10026.558904222624
1
Iteration 18200: Loss = -10026.557513020598
Iteration 18300: Loss = -10026.55754446691
Iteration 18400: Loss = -10026.56846503572
1
Iteration 18500: Loss = -10026.557512987092
Iteration 18600: Loss = -10026.557488641543
Iteration 18700: Loss = -10026.557925039056
1
Iteration 18800: Loss = -10026.55749931736
Iteration 18900: Loss = -10026.723117275411
1
Iteration 19000: Loss = -10026.557509894054
Iteration 19100: Loss = -10026.571261515757
1
Iteration 19200: Loss = -10026.55752029415
Iteration 19300: Loss = -10026.695482306575
1
Iteration 19400: Loss = -10026.55751080296
Iteration 19500: Loss = -10026.557519076365
Iteration 19600: Loss = -10026.557496701382
Iteration 19700: Loss = -10026.58189112747
1
Iteration 19800: Loss = -10026.557527939818
Iteration 19900: Loss = -10026.557632968346
1
pi: tensor([[9.9308e-01, 6.9242e-03],
        [1.0000e+00, 3.9715e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8851, 0.1149], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1372, 0.1993],
         [0.5503, 0.3158]],

        [[0.7103, 0.0624],
         [0.6993, 0.5607]],

        [[0.5502, 0.1445],
         [0.5469, 0.6996]],

        [[0.6304, 0.1801],
         [0.7081, 0.7082]],

        [[0.6407, 0.1673],
         [0.5696, 0.7183]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.025586367749811115
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0036651041706055824
Average Adjusted Rand Index: -0.005117273549962223
10126.457678183191
[-0.004060309377102018, -0.0036651041706055824] [-0.004673015611342496, -0.005117273549962223] [10026.293423620193, 10026.55759542621]
-------------------------------------
This iteration is 21
True Objective function: Loss = -10082.24358523354
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20474.94167192805
Iteration 100: Loss = -9951.25650952825
Iteration 200: Loss = -9948.037571645722
Iteration 300: Loss = -9946.638885869606
Iteration 400: Loss = -9945.66096679962
Iteration 500: Loss = -9944.623644277177
Iteration 600: Loss = -9944.33151758512
Iteration 700: Loss = -9944.253307808116
Iteration 800: Loss = -9944.216744001133
Iteration 900: Loss = -9944.192564978675
Iteration 1000: Loss = -9944.173198142424
Iteration 1100: Loss = -9944.153785600522
Iteration 1200: Loss = -9944.126795861886
Iteration 1300: Loss = -9944.079282113978
Iteration 1400: Loss = -9944.004341982858
Iteration 1500: Loss = -9943.92098118327
Iteration 1600: Loss = -9943.85025307644
Iteration 1700: Loss = -9943.797274882045
Iteration 1800: Loss = -9943.762043505854
Iteration 1900: Loss = -9943.737204174742
Iteration 2000: Loss = -9943.719121873672
Iteration 2100: Loss = -9943.707637119052
Iteration 2200: Loss = -9943.700338571442
Iteration 2300: Loss = -9943.695583080838
Iteration 2400: Loss = -9943.692287341866
Iteration 2500: Loss = -9943.689741556542
Iteration 2600: Loss = -9943.6880065514
Iteration 2700: Loss = -9943.686808437658
Iteration 2800: Loss = -9943.685920936918
Iteration 2900: Loss = -9943.685130530546
Iteration 3000: Loss = -9943.684355845884
Iteration 3100: Loss = -9943.683409034873
Iteration 3200: Loss = -9943.681579874003
Iteration 3300: Loss = -9943.67454924926
Iteration 3400: Loss = -9943.45261873748
Iteration 3500: Loss = -9942.806990733176
Iteration 3600: Loss = -9942.750657757299
Iteration 3700: Loss = -9942.732945751633
Iteration 3800: Loss = -9942.724042782213
Iteration 3900: Loss = -9942.718629697682
Iteration 4000: Loss = -9942.714968057186
Iteration 4100: Loss = -9942.7123547109
Iteration 4200: Loss = -9942.710388121364
Iteration 4300: Loss = -9942.708854758393
Iteration 4400: Loss = -9942.707559478744
Iteration 4500: Loss = -9942.707025739988
Iteration 4600: Loss = -9942.705721227103
Iteration 4700: Loss = -9942.705525830088
Iteration 4800: Loss = -9942.704376221047
Iteration 4900: Loss = -9942.70385293625
Iteration 5000: Loss = -9942.703477801108
Iteration 5100: Loss = -9942.702971314051
Iteration 5200: Loss = -9942.702625838321
Iteration 5300: Loss = -9942.702297887103
Iteration 5400: Loss = -9942.702273572788
Iteration 5500: Loss = -9942.701736932326
Iteration 5600: Loss = -9942.701520627972
Iteration 5700: Loss = -9942.701273231669
Iteration 5800: Loss = -9942.701134016403
Iteration 5900: Loss = -9942.700900426424
Iteration 6000: Loss = -9942.701164621185
1
Iteration 6100: Loss = -9942.700623580524
Iteration 6200: Loss = -9942.701490098272
1
Iteration 6300: Loss = -9942.700346722117
Iteration 6400: Loss = -9942.700219889934
Iteration 6500: Loss = -9942.700466509366
1
Iteration 6600: Loss = -9942.700005005325
Iteration 6700: Loss = -9942.69994858311
Iteration 6800: Loss = -9942.699832138354
Iteration 6900: Loss = -9942.699734562992
Iteration 7000: Loss = -9942.700203279881
1
Iteration 7100: Loss = -9942.699601467823
Iteration 7200: Loss = -9942.703633194815
1
Iteration 7300: Loss = -9942.699457339768
Iteration 7400: Loss = -9942.704180289857
1
Iteration 7500: Loss = -9942.699369326978
Iteration 7600: Loss = -9942.70781353655
1
Iteration 7700: Loss = -9942.699252893668
Iteration 7800: Loss = -9942.710079373184
1
Iteration 7900: Loss = -9942.699183644929
Iteration 8000: Loss = -9942.71178113786
1
Iteration 8100: Loss = -9942.699079046108
Iteration 8200: Loss = -9942.705156285374
1
Iteration 8300: Loss = -9942.69905215172
Iteration 8400: Loss = -9942.706634320011
1
Iteration 8500: Loss = -9942.698992446736
Iteration 8600: Loss = -9942.698999706032
Iteration 8700: Loss = -9942.70189580364
1
Iteration 8800: Loss = -9942.69892165449
Iteration 8900: Loss = -9942.699198068503
1
Iteration 9000: Loss = -9942.698882263501
Iteration 9100: Loss = -9942.698875616481
Iteration 9200: Loss = -9942.698800757003
Iteration 9300: Loss = -9942.699656407582
1
Iteration 9400: Loss = -9942.69877485828
Iteration 9500: Loss = -9942.77994548316
1
Iteration 9600: Loss = -9942.698751938908
Iteration 9700: Loss = -9942.698785182192
Iteration 9800: Loss = -9942.698752474684
Iteration 9900: Loss = -9942.698706353292
Iteration 10000: Loss = -9942.737850704838
1
Iteration 10100: Loss = -9942.698699245237
Iteration 10200: Loss = -9942.6986452166
Iteration 10300: Loss = -9942.702517502326
1
Iteration 10400: Loss = -9942.69861293027
Iteration 10500: Loss = -9942.698779145201
1
Iteration 10600: Loss = -9942.698677035209
Iteration 10700: Loss = -9942.698625458412
Iteration 10800: Loss = -9942.708109584026
1
Iteration 10900: Loss = -9942.698662480616
Iteration 11000: Loss = -9942.698598103836
Iteration 11100: Loss = -9942.698583685144
Iteration 11200: Loss = -9942.70003195537
1
Iteration 11300: Loss = -9942.698576229339
Iteration 11400: Loss = -9942.729155426003
1
Iteration 11500: Loss = -9942.698565408678
Iteration 11600: Loss = -9942.69856106299
Iteration 11700: Loss = -9942.699198070604
1
Iteration 11800: Loss = -9942.698557736228
Iteration 11900: Loss = -9942.73203728945
1
Iteration 12000: Loss = -9942.698524254896
Iteration 12100: Loss = -9942.698534098741
Iteration 12200: Loss = -9942.699733344669
1
Iteration 12300: Loss = -9942.698539990843
Iteration 12400: Loss = -9942.7073984765
1
Iteration 12500: Loss = -9942.698535444772
Iteration 12600: Loss = -9942.69853874933
Iteration 12700: Loss = -9942.714805129066
1
Iteration 12800: Loss = -9942.698516803128
Iteration 12900: Loss = -9942.711844281628
1
Iteration 13000: Loss = -9942.698541353722
Iteration 13100: Loss = -9942.6985172652
Iteration 13200: Loss = -9942.699700950096
1
Iteration 13300: Loss = -9942.698511985336
Iteration 13400: Loss = -9942.717885804019
1
Iteration 13500: Loss = -9942.69850187271
Iteration 13600: Loss = -9942.698485779538
Iteration 13700: Loss = -9942.698916181658
1
Iteration 13800: Loss = -9942.698513119216
Iteration 13900: Loss = -9942.707508006915
1
Iteration 14000: Loss = -9942.698482154561
Iteration 14100: Loss = -9942.698480381676
Iteration 14200: Loss = -9942.698696848958
1
Iteration 14300: Loss = -9942.698487963029
Iteration 14400: Loss = -9942.701532172465
1
Iteration 14500: Loss = -9942.69845500495
Iteration 14600: Loss = -9942.698462330885
Iteration 14700: Loss = -9942.698643447315
1
Iteration 14800: Loss = -9942.698480442627
Iteration 14900: Loss = -9942.734332067452
1
Iteration 15000: Loss = -9942.698489830125
Iteration 15100: Loss = -9942.698607070703
1
Iteration 15200: Loss = -9942.698502419136
Iteration 15300: Loss = -9942.698471039415
Iteration 15400: Loss = -9942.699843364418
1
Iteration 15500: Loss = -9942.698470089153
Iteration 15600: Loss = -9942.754040168644
1
Iteration 15700: Loss = -9942.698482153377
Iteration 15800: Loss = -9942.698467817938
Iteration 15900: Loss = -9942.700162697853
1
Iteration 16000: Loss = -9942.698459536467
Iteration 16100: Loss = -9942.69845564307
Iteration 16200: Loss = -9942.698744953062
1
Iteration 16300: Loss = -9942.698448026356
Iteration 16400: Loss = -9942.703751956542
1
Iteration 16500: Loss = -9942.698465813266
Iteration 16600: Loss = -9943.096110246943
1
Iteration 16700: Loss = -9942.698477934204
Iteration 16800: Loss = -9942.69845263171
Iteration 16900: Loss = -9942.699923668839
1
Iteration 17000: Loss = -9942.698464181367
Iteration 17100: Loss = -9942.736199025263
1
Iteration 17200: Loss = -9942.698464402654
Iteration 17300: Loss = -9942.698460375632
Iteration 17400: Loss = -9942.698713157775
1
Iteration 17500: Loss = -9942.698474354096
Iteration 17600: Loss = -9942.700661744908
1
Iteration 17700: Loss = -9942.698446317148
Iteration 17800: Loss = -9942.720996566552
1
Iteration 17900: Loss = -9942.698437181469
Iteration 18000: Loss = -9942.70534328089
1
Iteration 18100: Loss = -9942.698460900636
Iteration 18200: Loss = -9942.698460737862
Iteration 18300: Loss = -9942.699656241208
1
Iteration 18400: Loss = -9942.69848273687
Iteration 18500: Loss = -9942.707310571466
1
Iteration 18600: Loss = -9942.698473434424
Iteration 18700: Loss = -9942.69846995282
Iteration 18800: Loss = -9942.699068224894
1
Iteration 18900: Loss = -9942.698468023502
Iteration 19000: Loss = -9942.698461599239
Iteration 19100: Loss = -9942.698553045524
Iteration 19200: Loss = -9942.698449076086
Iteration 19300: Loss = -9942.714609449838
1
Iteration 19400: Loss = -9942.69843976509
Iteration 19500: Loss = -9943.079156398011
1
Iteration 19600: Loss = -9942.698450728802
Iteration 19700: Loss = -9942.698465594392
Iteration 19800: Loss = -9942.698672729528
1
Iteration 19900: Loss = -9942.698468366385
pi: tensor([[1.0000e+00, 1.6572e-07],
        [1.0790e-01, 8.9210e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7433, 0.2567], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1267, 0.1667],
         [0.5452, 0.2028]],

        [[0.5481, 0.1446],
         [0.5278, 0.6922]],

        [[0.6286, 0.1461],
         [0.6203, 0.6853]],

        [[0.5105, 0.1603],
         [0.6868, 0.6984]],

        [[0.5818, 0.1677],
         [0.7093, 0.5335]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.008063686668175179
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.012274307921588627
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.02685207384320307
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0023411493580129494
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0007177784756179624
Global Adjusted Rand Index: 0.009650426768014385
Average Adjusted Rand Index: 0.005887864842844306
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22815.750138892527
Iteration 100: Loss = -9951.643507933602
Iteration 200: Loss = -9948.07439701093
Iteration 300: Loss = -9946.969587162112
Iteration 400: Loss = -9945.492801648783
Iteration 500: Loss = -9944.59343490104
Iteration 600: Loss = -9944.344518964475
Iteration 700: Loss = -9944.240609982857
Iteration 800: Loss = -9944.178149424346
Iteration 900: Loss = -9944.123595232988
Iteration 1000: Loss = -9944.067539357604
Iteration 1100: Loss = -9944.00526142258
Iteration 1200: Loss = -9943.938290975935
Iteration 1300: Loss = -9943.875608757973
Iteration 1400: Loss = -9943.824026004379
Iteration 1500: Loss = -9943.784997891322
Iteration 1600: Loss = -9943.756654787156
Iteration 1700: Loss = -9943.735960667094
Iteration 1800: Loss = -9943.72078559037
Iteration 1900: Loss = -9943.709785766789
Iteration 2000: Loss = -9943.70216304041
Iteration 2100: Loss = -9943.696973196746
Iteration 2200: Loss = -9943.69332505507
Iteration 2300: Loss = -9943.690778956705
Iteration 2400: Loss = -9943.6890143822
Iteration 2500: Loss = -9943.687680474008
Iteration 2600: Loss = -9943.68669280367
Iteration 2700: Loss = -9943.685952305026
Iteration 2800: Loss = -9943.685305189963
Iteration 2900: Loss = -9943.684676909372
Iteration 3000: Loss = -9943.684037427158
Iteration 3100: Loss = -9943.68323168623
Iteration 3200: Loss = -9943.68186428775
Iteration 3300: Loss = -9943.678267724616
Iteration 3400: Loss = -9943.649910040858
Iteration 3500: Loss = -9942.989637783927
Iteration 3600: Loss = -9942.780509364145
Iteration 3700: Loss = -9942.746287914462
Iteration 3800: Loss = -9942.731886184842
Iteration 3900: Loss = -9942.724024047557
Iteration 4000: Loss = -9942.719025527123
Iteration 4100: Loss = -9942.715524868665
Iteration 4200: Loss = -9942.712959163122
Iteration 4300: Loss = -9942.71096344466
Iteration 4400: Loss = -9942.709411958667
Iteration 4500: Loss = -9942.708113886967
Iteration 4600: Loss = -9942.707073795373
Iteration 4700: Loss = -9942.706310008385
Iteration 4800: Loss = -9942.705440644475
Iteration 4900: Loss = -9942.704979194175
Iteration 5000: Loss = -9942.704242183601
Iteration 5100: Loss = -9942.703737844127
Iteration 5200: Loss = -9942.70331615451
Iteration 5300: Loss = -9942.70293315189
Iteration 5400: Loss = -9942.702580963156
Iteration 5500: Loss = -9942.702289533458
Iteration 5600: Loss = -9942.702031975403
Iteration 5700: Loss = -9942.701759385878
Iteration 5800: Loss = -9942.705299519383
1
Iteration 5900: Loss = -9942.701319175321
Iteration 6000: Loss = -9942.712915217266
1
Iteration 6100: Loss = -9942.700976645996
Iteration 6200: Loss = -9942.703268981846
1
Iteration 6300: Loss = -9942.70066000592
Iteration 6400: Loss = -9942.700488878461
Iteration 6500: Loss = -9942.700389312713
Iteration 6600: Loss = -9942.700272121701
Iteration 6700: Loss = -9942.70015130991
Iteration 6800: Loss = -9942.700040655174
Iteration 6900: Loss = -9942.700019676347
Iteration 7000: Loss = -9942.69986894843
Iteration 7100: Loss = -9942.699791386765
Iteration 7200: Loss = -9942.699698755243
Iteration 7300: Loss = -9942.699631528547
Iteration 7400: Loss = -9942.699662485276
Iteration 7500: Loss = -9942.699497658909
Iteration 7600: Loss = -9942.700083445949
1
Iteration 7700: Loss = -9942.699386955614
Iteration 7800: Loss = -9942.711247776882
1
Iteration 7900: Loss = -9942.69952944743
2
Iteration 8000: Loss = -9942.70122797428
3
Iteration 8100: Loss = -9942.699241233222
Iteration 8200: Loss = -9942.699422967054
1
Iteration 8300: Loss = -9942.69942318976
2
Iteration 8400: Loss = -9942.700082136464
3
Iteration 8500: Loss = -9942.699533836236
4
Iteration 8600: Loss = -9942.703050034357
5
Iteration 8700: Loss = -9942.69898991112
Iteration 8800: Loss = -9942.699000158567
Iteration 8900: Loss = -9942.698954834066
Iteration 9000: Loss = -9942.699018766409
Iteration 9100: Loss = -9942.698872156
Iteration 9200: Loss = -9942.698890567193
Iteration 9300: Loss = -9942.69886683594
Iteration 9400: Loss = -9942.698886728953
Iteration 9500: Loss = -9942.699235750555
1
Iteration 9600: Loss = -9942.698786153716
Iteration 9700: Loss = -9942.698736720444
Iteration 9800: Loss = -9942.69916894837
1
Iteration 9900: Loss = -9942.698727760258
Iteration 10000: Loss = -9942.700282166737
1
Iteration 10100: Loss = -9942.698703354772
Iteration 10200: Loss = -9942.698699227303
Iteration 10300: Loss = -9942.699153578815
1
Iteration 10400: Loss = -9942.698695847208
Iteration 10500: Loss = -9942.878928462354
1
Iteration 10600: Loss = -9942.698651351035
Iteration 10700: Loss = -9942.698644659806
Iteration 10800: Loss = -9942.69889537083
1
Iteration 10900: Loss = -9942.698623090202
Iteration 11000: Loss = -9943.13531827247
1
Iteration 11100: Loss = -9942.69860368235
Iteration 11200: Loss = -9942.698572781934
Iteration 11300: Loss = -9942.704953518738
1
Iteration 11400: Loss = -9942.698573764936
Iteration 11500: Loss = -9942.698567073354
Iteration 11600: Loss = -9942.69913632502
1
Iteration 11700: Loss = -9942.698561911277
Iteration 11800: Loss = -9942.74404594212
1
Iteration 11900: Loss = -9942.698570361277
Iteration 12000: Loss = -9942.698755110596
1
Iteration 12100: Loss = -9942.698532262253
Iteration 12200: Loss = -9942.698540740901
Iteration 12300: Loss = -9942.699779421004
1
Iteration 12400: Loss = -9942.698555022413
Iteration 12500: Loss = -9942.87034303709
1
Iteration 12600: Loss = -9942.698533109095
Iteration 12700: Loss = -9942.69851384397
Iteration 12800: Loss = -9942.69895398087
1
Iteration 12900: Loss = -9942.698532839695
Iteration 13000: Loss = -9942.698520474709
Iteration 13100: Loss = -9942.698695729046
1
Iteration 13200: Loss = -9942.698494915718
Iteration 13300: Loss = -9942.716753590308
1
Iteration 13400: Loss = -9942.69847847526
Iteration 13500: Loss = -9942.700814029025
1
Iteration 13600: Loss = -9942.69848806654
Iteration 13700: Loss = -9942.69851434837
Iteration 13800: Loss = -9942.700064119537
1
Iteration 13900: Loss = -9942.69848091198
Iteration 14000: Loss = -9942.69849876403
Iteration 14100: Loss = -9942.699002354364
1
Iteration 14200: Loss = -9942.698454500294
Iteration 14300: Loss = -9942.728278197443
1
Iteration 14400: Loss = -9942.698524242367
Iteration 14500: Loss = -9942.698481396224
Iteration 14600: Loss = -9942.701178546504
1
Iteration 14700: Loss = -9942.698475073903
Iteration 14800: Loss = -9943.071161164738
1
Iteration 14900: Loss = -9942.698498439324
Iteration 15000: Loss = -9942.698477040618
Iteration 15100: Loss = -9942.699018340765
1
Iteration 15200: Loss = -9942.698481060817
Iteration 15300: Loss = -9942.711222384049
1
Iteration 15400: Loss = -9942.698466688435
Iteration 15500: Loss = -9942.718265553629
1
Iteration 15600: Loss = -9942.69850728117
Iteration 15700: Loss = -9942.698500967505
Iteration 15800: Loss = -9942.699504793769
1
Iteration 15900: Loss = -9942.698459754349
Iteration 16000: Loss = -9942.69845896934
Iteration 16100: Loss = -9942.699172571001
1
Iteration 16200: Loss = -9942.698458567145
Iteration 16300: Loss = -9942.698953997668
1
Iteration 16400: Loss = -9942.69853230737
Iteration 16500: Loss = -9942.698452838578
Iteration 16600: Loss = -9942.698779728222
1
Iteration 16700: Loss = -9942.698459896217
Iteration 16800: Loss = -9942.707617082791
1
Iteration 16900: Loss = -9942.698481220545
Iteration 17000: Loss = -9942.698449303129
Iteration 17100: Loss = -9942.69852913819
Iteration 17200: Loss = -9942.698465537309
Iteration 17300: Loss = -9942.698807485122
1
Iteration 17400: Loss = -9942.698441962406
Iteration 17500: Loss = -9942.698494909842
Iteration 17600: Loss = -9942.69851078208
Iteration 17700: Loss = -9942.698457658693
Iteration 17800: Loss = -9942.737508910359
1
Iteration 17900: Loss = -9942.69845078698
Iteration 18000: Loss = -9942.698462858867
Iteration 18100: Loss = -9942.699856031795
1
Iteration 18200: Loss = -9942.69845899323
Iteration 18300: Loss = -9942.841996182875
1
Iteration 18400: Loss = -9942.698473983677
Iteration 18500: Loss = -9942.698541854892
Iteration 18600: Loss = -9942.6985559724
Iteration 18700: Loss = -9942.69844968297
Iteration 18800: Loss = -9942.699273677992
1
Iteration 18900: Loss = -9942.69845898489
Iteration 19000: Loss = -9942.7917072641
1
Iteration 19100: Loss = -9942.698461268319
Iteration 19200: Loss = -9942.69845675911
Iteration 19300: Loss = -9942.698907313477
1
Iteration 19400: Loss = -9942.698456947659
Iteration 19500: Loss = -9942.69846038286
Iteration 19600: Loss = -9942.69881268327
1
Iteration 19700: Loss = -9942.698454651008
Iteration 19800: Loss = -9942.71453640804
1
Iteration 19900: Loss = -9942.698485810351
pi: tensor([[8.9445e-01, 1.0555e-01],
        [1.9278e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2642, 0.7358], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.1644],
         [0.6499, 0.1250]],

        [[0.5911, 0.1427],
         [0.7185, 0.6806]],

        [[0.5969, 0.1441],
         [0.5304, 0.5641]],

        [[0.6754, 0.1582],
         [0.5295, 0.6598]],

        [[0.5234, 0.1655],
         [0.7083, 0.5986]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.008063686668175179
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.012274307921588627
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.02685207384320307
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0023411493580129494
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0007177784756179624
Global Adjusted Rand Index: 0.009650426768014385
Average Adjusted Rand Index: 0.005887864842844306
10082.24358523354
[0.009650426768014385, 0.009650426768014385] [0.005887864842844306, 0.005887864842844306] [9943.120662001165, 9942.69845072581]
-------------------------------------
This iteration is 22
True Objective function: Loss = -9982.652075692185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24344.944569390555
Iteration 100: Loss = -9827.763509467151
Iteration 200: Loss = -9826.829124239479
Iteration 300: Loss = -9826.487230757333
Iteration 400: Loss = -9825.738078359509
Iteration 500: Loss = -9825.113674732898
Iteration 600: Loss = -9821.566588740614
Iteration 700: Loss = -9818.024649785006
Iteration 800: Loss = -9817.487273468585
Iteration 900: Loss = -9817.294435101949
Iteration 1000: Loss = -9817.200602022698
Iteration 1100: Loss = -9817.120167353727
Iteration 1200: Loss = -9816.346463798773
Iteration 1300: Loss = -9815.196291590484
Iteration 1400: Loss = -9814.603516409788
Iteration 1500: Loss = -9814.385757419383
Iteration 1600: Loss = -9814.297009408938
Iteration 1700: Loss = -9814.259780877012
Iteration 1800: Loss = -9814.243565430703
Iteration 1900: Loss = -9814.236145061674
Iteration 2000: Loss = -9814.232358884788
Iteration 2100: Loss = -9814.230312427153
Iteration 2200: Loss = -9814.228988491433
Iteration 2300: Loss = -9814.228156959562
Iteration 2400: Loss = -9814.22750958816
Iteration 2500: Loss = -9814.227040466583
Iteration 2600: Loss = -9814.226653365107
Iteration 2700: Loss = -9814.226352032605
Iteration 2800: Loss = -9814.226084256483
Iteration 2900: Loss = -9814.225854833028
Iteration 3000: Loss = -9814.22568245163
Iteration 3100: Loss = -9814.225549361901
Iteration 3200: Loss = -9814.225431819728
Iteration 3300: Loss = -9814.225316106631
Iteration 3400: Loss = -9814.225206623529
Iteration 3500: Loss = -9814.225112180926
Iteration 3600: Loss = -9814.225008393478
Iteration 3700: Loss = -9814.224922086345
Iteration 3800: Loss = -9814.224892454502
Iteration 3900: Loss = -9814.224826739983
Iteration 4000: Loss = -9814.22478627985
Iteration 4100: Loss = -9814.224732427063
Iteration 4200: Loss = -9814.224680410172
Iteration 4300: Loss = -9814.22465543657
Iteration 4400: Loss = -9814.224606035965
Iteration 4500: Loss = -9814.224603771325
Iteration 4600: Loss = -9814.224581552968
Iteration 4700: Loss = -9814.224513228673
Iteration 4800: Loss = -9814.224518838964
Iteration 4900: Loss = -9814.224516796168
Iteration 5000: Loss = -9814.224471403888
Iteration 5100: Loss = -9814.224481006198
Iteration 5200: Loss = -9814.224455311576
Iteration 5300: Loss = -9814.224416749841
Iteration 5400: Loss = -9814.224423862634
Iteration 5500: Loss = -9814.224390172662
Iteration 5600: Loss = -9814.224395897434
Iteration 5700: Loss = -9814.224352627649
Iteration 5800: Loss = -9814.224375718095
Iteration 5900: Loss = -9814.224341422741
Iteration 6000: Loss = -9814.224344459493
Iteration 6100: Loss = -9814.224341516656
Iteration 6200: Loss = -9814.224296103659
Iteration 6300: Loss = -9814.224339304441
Iteration 6400: Loss = -9814.224326923224
Iteration 6500: Loss = -9814.22431378888
Iteration 6600: Loss = -9814.224355040784
Iteration 6700: Loss = -9814.22431335557
Iteration 6800: Loss = -9814.224271523963
Iteration 6900: Loss = -9814.224250436186
Iteration 7000: Loss = -9814.224300039668
Iteration 7100: Loss = -9814.22435482316
Iteration 7200: Loss = -9814.22427021665
Iteration 7300: Loss = -9814.224283423853
Iteration 7400: Loss = -9814.224269578697
Iteration 7500: Loss = -9814.234047280446
1
Iteration 7600: Loss = -9814.224233720017
Iteration 7700: Loss = -9814.224260328474
Iteration 7800: Loss = -9814.22427763096
Iteration 7900: Loss = -9814.224179086892
Iteration 8000: Loss = -9814.22432510456
1
Iteration 8100: Loss = -9814.224190911717
Iteration 8200: Loss = -9814.224185835381
Iteration 8300: Loss = -9814.224203998465
Iteration 8400: Loss = -9814.224204120697
Iteration 8500: Loss = -9814.22429124809
Iteration 8600: Loss = -9814.224231805096
Iteration 8700: Loss = -9814.224471216772
1
Iteration 8800: Loss = -9814.224208781372
Iteration 8900: Loss = -9814.224982103362
1
Iteration 9000: Loss = -9814.224165416186
Iteration 9100: Loss = -9814.224208433137
Iteration 9200: Loss = -9814.224234859228
Iteration 9300: Loss = -9814.224170141058
Iteration 9400: Loss = -9814.225922495634
1
Iteration 9500: Loss = -9814.224211120656
Iteration 9600: Loss = -9814.224200506771
Iteration 9700: Loss = -9814.224183075328
Iteration 9800: Loss = -9814.258421365534
1
Iteration 9900: Loss = -9814.22416235135
Iteration 10000: Loss = -9814.224166525468
Iteration 10100: Loss = -9814.224233231233
Iteration 10200: Loss = -9814.22423207339
Iteration 10300: Loss = -9814.23201009478
1
Iteration 10400: Loss = -9814.224671425058
2
Iteration 10500: Loss = -9814.477142426245
3
Iteration 10600: Loss = -9814.22416967466
Iteration 10700: Loss = -9814.224172129527
Iteration 10800: Loss = -9814.241312097836
1
Iteration 10900: Loss = -9814.224189425275
Iteration 11000: Loss = -9814.224154404848
Iteration 11100: Loss = -9814.298338183828
1
Iteration 11200: Loss = -9814.224180446407
Iteration 11300: Loss = -9814.224206241695
Iteration 11400: Loss = -9814.266000697336
1
Iteration 11500: Loss = -9814.224183252169
Iteration 11600: Loss = -9814.224315049856
1
Iteration 11700: Loss = -9814.224277161273
Iteration 11800: Loss = -9814.224267389189
Iteration 11900: Loss = -9814.224284398806
Iteration 12000: Loss = -9814.239700407757
1
Iteration 12100: Loss = -9814.224451932974
2
Iteration 12200: Loss = -9814.226048867353
3
Iteration 12300: Loss = -9814.225049037055
4
Iteration 12400: Loss = -9814.224660803586
5
Iteration 12500: Loss = -9814.224180981808
Iteration 12600: Loss = -9814.232483108874
1
Iteration 12700: Loss = -9814.224165675609
Iteration 12800: Loss = -9814.238412667026
1
Iteration 12900: Loss = -9814.2241809364
Iteration 13000: Loss = -9814.224151587261
Iteration 13100: Loss = -9814.227135373727
1
Iteration 13200: Loss = -9814.224185848178
Iteration 13300: Loss = -9814.288381655826
1
Iteration 13400: Loss = -9814.22417282951
Iteration 13500: Loss = -9814.264125084204
1
Iteration 13600: Loss = -9814.248770468641
2
Iteration 13700: Loss = -9814.22433562149
3
Iteration 13800: Loss = -9814.224240375876
Iteration 13900: Loss = -9814.233324722098
1
Iteration 14000: Loss = -9814.224851480802
2
Iteration 14100: Loss = -9814.225208542826
3
Iteration 14200: Loss = -9814.224668199526
4
Iteration 14300: Loss = -9814.224545487348
5
Iteration 14400: Loss = -9814.22930901748
6
Iteration 14500: Loss = -9814.224173679895
Iteration 14600: Loss = -9814.263259424248
1
Iteration 14700: Loss = -9814.224144338565
Iteration 14800: Loss = -9814.251110622556
1
Iteration 14900: Loss = -9814.22420972954
Iteration 15000: Loss = -9814.224193619735
Iteration 15100: Loss = -9814.224584221458
1
Iteration 15200: Loss = -9814.233959110326
2
Iteration 15300: Loss = -9814.22418259764
Iteration 15400: Loss = -9814.224396027319
1
Iteration 15500: Loss = -9814.22421575
Iteration 15600: Loss = -9814.224255019304
Iteration 15700: Loss = -9814.261128222686
1
Iteration 15800: Loss = -9814.224175653004
Iteration 15900: Loss = -9814.226839444496
1
Iteration 16000: Loss = -9814.22421425553
Iteration 16100: Loss = -9814.224248921742
Iteration 16200: Loss = -9814.298318804622
1
Iteration 16300: Loss = -9814.261291646495
2
Iteration 16400: Loss = -9814.224199936732
Iteration 16500: Loss = -9814.224269158902
Iteration 16600: Loss = -9814.46487009259
1
Iteration 16700: Loss = -9814.224264383476
Iteration 16800: Loss = -9814.229730056177
1
Iteration 16900: Loss = -9814.224258452723
Iteration 17000: Loss = -9814.224244288529
Iteration 17100: Loss = -9814.22486050703
1
Iteration 17200: Loss = -9814.243958145657
2
Iteration 17300: Loss = -9814.227978015055
3
Iteration 17400: Loss = -9814.245089045398
4
Iteration 17500: Loss = -9814.224254948052
Iteration 17600: Loss = -9814.224245562691
Iteration 17700: Loss = -9814.229103679072
1
Iteration 17800: Loss = -9814.224227503564
Iteration 17900: Loss = -9814.224316891512
Iteration 18000: Loss = -9814.249578449248
1
Iteration 18100: Loss = -9814.224181044348
Iteration 18200: Loss = -9814.226800330653
1
Iteration 18300: Loss = -9814.293098611666
2
Iteration 18400: Loss = -9814.22418382021
Iteration 18500: Loss = -9814.224209097403
Iteration 18600: Loss = -9814.226391525095
1
Iteration 18700: Loss = -9814.22419009363
Iteration 18800: Loss = -9814.224517153436
1
Iteration 18900: Loss = -9814.224215789132
Iteration 19000: Loss = -9814.232079521798
1
Iteration 19100: Loss = -9814.224543708722
2
Iteration 19200: Loss = -9814.22421226648
Iteration 19300: Loss = -9814.230871681559
1
Iteration 19400: Loss = -9814.224180833176
Iteration 19500: Loss = -9814.224893289846
1
Iteration 19600: Loss = -9814.24300178135
2
Iteration 19700: Loss = -9814.227715725932
3
Iteration 19800: Loss = -9814.225255336018
4
Iteration 19900: Loss = -9814.224266606398
pi: tensor([[0.9956, 0.0044],
        [0.3628, 0.6372]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9156, 0.0844], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1341, 0.1723],
         [0.5111, 0.4366]],

        [[0.5430, 0.1496],
         [0.7158, 0.7118]],

        [[0.6950, 0.0598],
         [0.6987, 0.6194]],

        [[0.5601, 0.0837],
         [0.6074, 0.6043]],

        [[0.7089, 0.2555],
         [0.5720, 0.5288]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004130624939255516
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.007272876449007818
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.0035158395898187145
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002322994018404594
Average Adjusted Rand Index: -0.002076041093391039
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22857.92619518303
Iteration 100: Loss = -9827.58589611212
Iteration 200: Loss = -9825.47743749177
Iteration 300: Loss = -9824.110035884227
Iteration 400: Loss = -9823.289961187762
Iteration 500: Loss = -9822.892271604573
Iteration 600: Loss = -9822.093934992477
Iteration 700: Loss = -9821.568062994711
Iteration 800: Loss = -9821.338910582956
Iteration 900: Loss = -9821.172906481348
Iteration 1000: Loss = -9821.0222872563
Iteration 1100: Loss = -9820.858676906688
Iteration 1200: Loss = -9820.68584666451
Iteration 1300: Loss = -9820.349842354557
Iteration 1400: Loss = -9820.174366675108
Iteration 1500: Loss = -9819.991064683383
Iteration 1600: Loss = -9819.763576271649
Iteration 1700: Loss = -9819.557308728175
Iteration 1800: Loss = -9819.239373882318
Iteration 1900: Loss = -9817.484314326324
Iteration 2000: Loss = -9815.667925623935
Iteration 2100: Loss = -9814.756825182976
Iteration 2200: Loss = -9814.53456879289
Iteration 2300: Loss = -9814.432827014234
Iteration 2400: Loss = -9814.332008325651
Iteration 2500: Loss = -9814.304696639736
Iteration 2600: Loss = -9814.287112562337
Iteration 2700: Loss = -9814.27553063095
Iteration 2800: Loss = -9814.268019154642
Iteration 2900: Loss = -9814.26246170594
Iteration 3000: Loss = -9814.258114607313
Iteration 3100: Loss = -9814.25459775859
Iteration 3200: Loss = -9814.251761479276
Iteration 3300: Loss = -9814.249429068723
Iteration 3400: Loss = -9814.247474327874
Iteration 3500: Loss = -9814.245807090141
Iteration 3600: Loss = -9814.244360253097
Iteration 3700: Loss = -9814.243115102634
Iteration 3800: Loss = -9814.241943858573
Iteration 3900: Loss = -9814.24093999807
Iteration 4000: Loss = -9814.240051746767
Iteration 4100: Loss = -9814.239166807456
Iteration 4200: Loss = -9814.238407612676
Iteration 4300: Loss = -9814.237678261536
Iteration 4400: Loss = -9814.237098246493
Iteration 4500: Loss = -9814.236570964524
Iteration 4600: Loss = -9814.236041863636
Iteration 4700: Loss = -9814.235646845073
Iteration 4800: Loss = -9814.235276834894
Iteration 4900: Loss = -9814.234944418846
Iteration 5000: Loss = -9814.234639644003
Iteration 5100: Loss = -9814.234351246641
Iteration 5200: Loss = -9814.23411633228
Iteration 5300: Loss = -9814.233866311773
Iteration 5400: Loss = -9814.233622547692
Iteration 5500: Loss = -9814.23340907287
Iteration 5600: Loss = -9814.233269783763
Iteration 5700: Loss = -9814.233021373655
Iteration 5800: Loss = -9814.232887114344
Iteration 5900: Loss = -9814.232718976924
Iteration 6000: Loss = -9814.234370326978
1
Iteration 6100: Loss = -9814.232401414634
Iteration 6200: Loss = -9814.23229851676
Iteration 6300: Loss = -9814.232165240148
Iteration 6400: Loss = -9814.23203168394
Iteration 6500: Loss = -9814.233298003004
1
Iteration 6600: Loss = -9814.231861902163
Iteration 6700: Loss = -9814.231803663926
Iteration 6800: Loss = -9814.231786922792
Iteration 6900: Loss = -9814.231617677113
Iteration 7000: Loss = -9814.232032480679
1
Iteration 7100: Loss = -9814.231537791255
Iteration 7200: Loss = -9814.231472760075
Iteration 7300: Loss = -9814.23148850436
Iteration 7400: Loss = -9814.23139387599
Iteration 7500: Loss = -9814.231362060711
Iteration 7600: Loss = -9814.231548412718
1
Iteration 7700: Loss = -9814.231291622073
Iteration 7800: Loss = -9814.231247821424
Iteration 7900: Loss = -9814.231208021527
Iteration 8000: Loss = -9814.23116775808
Iteration 8100: Loss = -9814.234564133132
1
Iteration 8200: Loss = -9814.230929316587
Iteration 8300: Loss = -9814.22475132982
Iteration 8400: Loss = -9814.225028723207
1
Iteration 8500: Loss = -9814.224564932363
Iteration 8600: Loss = -9814.224605317868
Iteration 8700: Loss = -9814.226890464342
1
Iteration 8800: Loss = -9814.224525743033
Iteration 8900: Loss = -9814.228697387682
1
Iteration 9000: Loss = -9814.224464719271
Iteration 9100: Loss = -9814.224447874203
Iteration 9200: Loss = -9814.224634895587
1
Iteration 9300: Loss = -9814.224434535168
Iteration 9400: Loss = -9814.22441131585
Iteration 9500: Loss = -9814.228109451977
1
Iteration 9600: Loss = -9814.224393617696
Iteration 9700: Loss = -9814.224378177685
Iteration 9800: Loss = -9814.224395775133
Iteration 9900: Loss = -9814.224431654273
Iteration 10000: Loss = -9814.224316425367
Iteration 10100: Loss = -9814.224405696239
Iteration 10200: Loss = -9814.224463058108
Iteration 10300: Loss = -9814.224302019262
Iteration 10400: Loss = -9814.227212441752
1
Iteration 10500: Loss = -9814.224257974918
Iteration 10600: Loss = -9814.22428149569
Iteration 10700: Loss = -9814.224435556405
1
Iteration 10800: Loss = -9814.224252444834
Iteration 10900: Loss = -9814.224237215687
Iteration 11000: Loss = -9814.224456038737
1
Iteration 11100: Loss = -9814.224212096067
Iteration 11200: Loss = -9814.224202780619
Iteration 11300: Loss = -9814.224214504999
Iteration 11400: Loss = -9814.224782256455
1
Iteration 11500: Loss = -9814.224211162178
Iteration 11600: Loss = -9814.224218264475
Iteration 11700: Loss = -9814.226409128534
1
Iteration 11800: Loss = -9814.224186569729
Iteration 11900: Loss = -9814.224192156047
Iteration 12000: Loss = -9814.274511263266
1
Iteration 12100: Loss = -9814.224190265017
Iteration 12200: Loss = -9814.224195842764
Iteration 12300: Loss = -9814.224213384967
Iteration 12400: Loss = -9814.224398138824
1
Iteration 12500: Loss = -9814.224204762142
Iteration 12600: Loss = -9814.22419074267
Iteration 12700: Loss = -9814.228046886528
1
Iteration 12800: Loss = -9814.224194681414
Iteration 12900: Loss = -9814.22418433363
Iteration 13000: Loss = -9814.550280741092
1
Iteration 13100: Loss = -9814.22420212404
Iteration 13200: Loss = -9814.224183153154
Iteration 13300: Loss = -9814.22450331599
1
Iteration 13400: Loss = -9814.224184367518
Iteration 13500: Loss = -9814.224174087403
Iteration 13600: Loss = -9814.236629381616
1
Iteration 13700: Loss = -9814.224444448186
2
Iteration 13800: Loss = -9814.22418419377
Iteration 13900: Loss = -9814.226611890725
1
Iteration 14000: Loss = -9814.224214913183
Iteration 14100: Loss = -9814.228181605731
1
Iteration 14200: Loss = -9814.236737017602
2
Iteration 14300: Loss = -9814.22418330219
Iteration 14400: Loss = -9814.224453283618
1
Iteration 14500: Loss = -9814.224207998217
Iteration 14600: Loss = -9814.22420729161
Iteration 14700: Loss = -9814.224194620496
Iteration 14800: Loss = -9814.224341536803
1
Iteration 14900: Loss = -9814.224231582122
Iteration 15000: Loss = -9814.22422224771
Iteration 15100: Loss = -9814.224286849274
Iteration 15200: Loss = -9814.224603598514
1
Iteration 15300: Loss = -9814.22443287004
2
Iteration 15400: Loss = -9814.25759524226
3
Iteration 15500: Loss = -9814.224801703685
4
Iteration 15600: Loss = -9814.235079177131
5
Iteration 15700: Loss = -9814.250387637945
6
Iteration 15800: Loss = -9814.225939558613
7
Iteration 15900: Loss = -9814.224172193704
Iteration 16000: Loss = -9814.230318160478
1
Iteration 16100: Loss = -9814.22463834361
2
Iteration 16200: Loss = -9814.227238450601
3
Iteration 16300: Loss = -9814.224156584349
Iteration 16400: Loss = -9814.224555163284
1
Iteration 16500: Loss = -9814.224872947385
2
Iteration 16600: Loss = -9814.224240612886
Iteration 16700: Loss = -9814.22770962473
1
Iteration 16800: Loss = -9814.224166459382
Iteration 16900: Loss = -9814.226272549065
1
Iteration 17000: Loss = -9814.225452754868
2
Iteration 17100: Loss = -9814.224510751781
3
Iteration 17200: Loss = -9814.345767238165
4
Iteration 17300: Loss = -9814.22433159957
5
Iteration 17400: Loss = -9814.224274138025
6
Iteration 17500: Loss = -9814.228340838332
7
Iteration 17600: Loss = -9814.22435912589
8
Iteration 17700: Loss = -9814.225093997731
9
Iteration 17800: Loss = -9814.344423036853
10
Iteration 17900: Loss = -9814.22421467151
Iteration 18000: Loss = -9814.22421166398
Iteration 18100: Loss = -9814.224365385458
1
Iteration 18200: Loss = -9814.225409329878
2
Iteration 18300: Loss = -9814.224689862847
3
Iteration 18400: Loss = -9814.262889406136
4
Iteration 18500: Loss = -9814.224255535273
Iteration 18600: Loss = -9814.459848066032
1
Iteration 18700: Loss = -9814.224154010404
Iteration 18800: Loss = -9814.244852221265
1
Iteration 18900: Loss = -9814.224245308407
Iteration 19000: Loss = -9814.225756274172
1
Iteration 19100: Loss = -9814.229176985404
2
Iteration 19200: Loss = -9814.23612382797
3
Iteration 19300: Loss = -9814.22705780167
4
Iteration 19400: Loss = -9814.365078414918
5
Iteration 19500: Loss = -9814.224184985324
Iteration 19600: Loss = -9814.22447391295
1
Iteration 19700: Loss = -9814.229827980856
2
Iteration 19800: Loss = -9814.224189762073
Iteration 19900: Loss = -9814.446931699678
1
pi: tensor([[0.6374, 0.3626],
        [0.0044, 0.9956]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0844, 0.9156], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4367, 0.1723],
         [0.6552, 0.1341]],

        [[0.6792, 0.1496],
         [0.5632, 0.6984]],

        [[0.7273, 0.0599],
         [0.6265, 0.5272]],

        [[0.6982, 0.0837],
         [0.7284, 0.6540]],

        [[0.6417, 0.2554],
         [0.5636, 0.5249]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004130624939255516
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.007272876449007818
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0035158395898187145
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002322994018404594
Average Adjusted Rand Index: -0.002076041093391039
9982.652075692185
[-0.0002322994018404594, -0.0002322994018404594] [-0.002076041093391039, -0.002076041093391039] [9814.224208287003, 9814.224185004128]
-------------------------------------
This iteration is 23
True Objective function: Loss = -9961.488887135527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21547.775212437544
Iteration 100: Loss = -9830.22706454006
Iteration 200: Loss = -9828.905816765251
Iteration 300: Loss = -9827.081488860027
Iteration 400: Loss = -9825.818237517851
Iteration 500: Loss = -9825.20841587401
Iteration 600: Loss = -9824.809939946808
Iteration 700: Loss = -9824.533078297125
Iteration 800: Loss = -9824.337489243084
Iteration 900: Loss = -9824.191065018778
Iteration 1000: Loss = -9824.071080283564
Iteration 1100: Loss = -9823.98835231413
Iteration 1200: Loss = -9823.929683814358
Iteration 1300: Loss = -9823.883559932276
Iteration 1400: Loss = -9823.846343024386
Iteration 1500: Loss = -9823.815874531723
Iteration 1600: Loss = -9823.790590613426
Iteration 1700: Loss = -9823.769347319345
Iteration 1800: Loss = -9823.751413037404
Iteration 1900: Loss = -9823.736032181643
Iteration 2000: Loss = -9823.722555070659
Iteration 2100: Loss = -9823.710681461269
Iteration 2200: Loss = -9823.7004124984
Iteration 2300: Loss = -9823.69145449677
Iteration 2400: Loss = -9823.683489810055
Iteration 2500: Loss = -9823.676294220279
Iteration 2600: Loss = -9823.669727670449
Iteration 2700: Loss = -9823.663672126384
Iteration 2800: Loss = -9823.658070317457
Iteration 2900: Loss = -9823.652835536006
Iteration 3000: Loss = -9823.647855737188
Iteration 3100: Loss = -9823.643142761552
Iteration 3200: Loss = -9823.638560588037
Iteration 3300: Loss = -9823.634119459071
Iteration 3400: Loss = -9823.629696374584
Iteration 3500: Loss = -9823.625247453027
Iteration 3600: Loss = -9823.620626789681
Iteration 3700: Loss = -9823.616055154607
Iteration 3800: Loss = -9823.611683522675
Iteration 3900: Loss = -9823.607144144684
Iteration 4000: Loss = -9823.60207000809
Iteration 4100: Loss = -9823.59729473003
Iteration 4200: Loss = -9823.592502042538
Iteration 4300: Loss = -9823.587565545313
Iteration 4400: Loss = -9823.582295663253
Iteration 4500: Loss = -9823.576707022456
Iteration 4600: Loss = -9823.570634829015
Iteration 4700: Loss = -9823.564059991078
Iteration 4800: Loss = -9823.55682636514
Iteration 4900: Loss = -9823.548737696947
Iteration 5000: Loss = -9823.539561563795
Iteration 5100: Loss = -9823.52890700419
Iteration 5200: Loss = -9823.516447648877
Iteration 5300: Loss = -9823.501277544061
Iteration 5400: Loss = -9823.481157191432
Iteration 5500: Loss = -9823.44378108974
Iteration 5600: Loss = -9823.362544224328
Iteration 5700: Loss = -9823.323926933517
Iteration 5800: Loss = -9823.298371840827
Iteration 5900: Loss = -9823.279727846028
Iteration 6000: Loss = -9823.266710175536
Iteration 6100: Loss = -9823.255851151544
Iteration 6200: Loss = -9823.248018039478
Iteration 6300: Loss = -9823.24236769214
Iteration 6400: Loss = -9823.237653464423
Iteration 6500: Loss = -9823.232639224772
Iteration 6600: Loss = -9823.220346315218
Iteration 6700: Loss = -9823.217308614801
Iteration 6800: Loss = -9823.214806877411
Iteration 6900: Loss = -9823.212689574484
Iteration 7000: Loss = -9823.210860176338
Iteration 7100: Loss = -9823.209179626805
Iteration 7200: Loss = -9823.207545233527
Iteration 7300: Loss = -9823.20619139418
Iteration 7400: Loss = -9823.204936741819
Iteration 7500: Loss = -9823.203406807694
Iteration 7600: Loss = -9823.201491470318
Iteration 7700: Loss = -9823.200596665349
Iteration 7800: Loss = -9823.189501132754
Iteration 7900: Loss = -9823.177864518482
Iteration 8000: Loss = -9823.177331168326
Iteration 8100: Loss = -9823.176911479048
Iteration 8200: Loss = -9823.176683003616
Iteration 8300: Loss = -9823.176378501375
Iteration 8400: Loss = -9823.176346895818
Iteration 8500: Loss = -9823.176098830487
Iteration 8600: Loss = -9823.175816762301
Iteration 8700: Loss = -9823.177689136955
1
Iteration 8800: Loss = -9823.175433485925
Iteration 8900: Loss = -9823.179021998309
1
Iteration 9000: Loss = -9823.155323900146
Iteration 9100: Loss = -9823.154473110635
Iteration 9200: Loss = -9823.153632477897
Iteration 9300: Loss = -9823.153405632298
Iteration 9400: Loss = -9823.153299930587
Iteration 9500: Loss = -9823.153525986043
1
Iteration 9600: Loss = -9823.152622147576
Iteration 9700: Loss = -9823.13677698505
Iteration 9800: Loss = -9823.137598669213
1
Iteration 9900: Loss = -9823.136608936351
Iteration 10000: Loss = -9823.136894987912
1
Iteration 10100: Loss = -9823.13620217368
Iteration 10200: Loss = -9823.136084285548
Iteration 10300: Loss = -9823.523590710132
1
Iteration 10400: Loss = -9823.133408310556
Iteration 10500: Loss = -9823.133210281188
Iteration 10600: Loss = -9823.214153404373
1
Iteration 10700: Loss = -9823.096794194631
Iteration 10800: Loss = -9823.096380714225
Iteration 10900: Loss = -9823.11127558054
1
Iteration 11000: Loss = -9823.09622966936
Iteration 11100: Loss = -9823.096117366045
Iteration 11200: Loss = -9823.094991743374
Iteration 11300: Loss = -9823.09562129864
1
Iteration 11400: Loss = -9823.094888635644
Iteration 11500: Loss = -9823.094807557696
Iteration 11600: Loss = -9823.10024504076
1
Iteration 11700: Loss = -9823.0937397973
Iteration 11800: Loss = -9823.093707976162
Iteration 11900: Loss = -9823.093664806232
Iteration 12000: Loss = -9823.094084812148
1
Iteration 12100: Loss = -9823.093205980422
Iteration 12200: Loss = -9823.092901821514
Iteration 12300: Loss = -9823.112629428913
1
Iteration 12400: Loss = -9823.092731342209
Iteration 12500: Loss = -9823.092630404843
Iteration 12600: Loss = -9823.10253485076
1
Iteration 12700: Loss = -9823.09250563137
Iteration 12800: Loss = -9823.09244389652
Iteration 12900: Loss = -9823.095455539935
1
Iteration 13000: Loss = -9823.091960848662
Iteration 13100: Loss = -9823.091941647386
Iteration 13200: Loss = -9823.102499478113
1
Iteration 13300: Loss = -9823.091905246789
Iteration 13400: Loss = -9823.091837410553
Iteration 13500: Loss = -9823.09310124753
1
Iteration 13600: Loss = -9823.093356974478
2
Iteration 13700: Loss = -9823.09654757316
3
Iteration 13800: Loss = -9823.087769733093
Iteration 13900: Loss = -9823.127740263333
1
Iteration 14000: Loss = -9823.085550373566
Iteration 14100: Loss = -9823.119835985615
1
Iteration 14200: Loss = -9823.104111109142
2
Iteration 14300: Loss = -9823.085262390454
Iteration 14400: Loss = -9823.085294644867
Iteration 14500: Loss = -9823.090934220148
1
Iteration 14600: Loss = -9823.085215162062
Iteration 14700: Loss = -9823.085544794621
1
Iteration 14800: Loss = -9823.085076732386
Iteration 14900: Loss = -9823.116790405225
1
Iteration 15000: Loss = -9823.085057681423
Iteration 15100: Loss = -9823.08624138323
1
Iteration 15200: Loss = -9823.085595502425
2
Iteration 15300: Loss = -9823.08611966185
3
Iteration 15400: Loss = -9823.0850086729
Iteration 15500: Loss = -9823.086534622267
1
Iteration 15600: Loss = -9823.085013214084
Iteration 15700: Loss = -9823.114814069064
1
Iteration 15800: Loss = -9823.085185814925
2
Iteration 15900: Loss = -9823.085194508594
3
Iteration 16000: Loss = -9823.084895695089
Iteration 16100: Loss = -9823.217937918525
1
Iteration 16200: Loss = -9823.08486163547
Iteration 16300: Loss = -9823.086878659133
1
Iteration 16400: Loss = -9823.114066036716
2
Iteration 16500: Loss = -9823.084593573745
Iteration 16600: Loss = -9823.095260827891
1
Iteration 16700: Loss = -9823.084575405897
Iteration 16800: Loss = -9823.090413946264
1
Iteration 16900: Loss = -9823.084969676422
2
Iteration 17000: Loss = -9823.092153646969
3
Iteration 17100: Loss = -9823.085084305514
4
Iteration 17200: Loss = -9823.08458325229
Iteration 17300: Loss = -9823.084504887831
Iteration 17400: Loss = -9823.090494448717
1
Iteration 17500: Loss = -9823.08910298672
2
Iteration 17600: Loss = -9823.084498575063
Iteration 17700: Loss = -9823.084330402835
Iteration 17800: Loss = -9823.104468490912
1
Iteration 17900: Loss = -9823.085323742336
2
Iteration 18000: Loss = -9823.084317220102
Iteration 18100: Loss = -9823.087676856007
1
Iteration 18200: Loss = -9823.088730642803
2
Iteration 18300: Loss = -9823.084244014617
Iteration 18400: Loss = -9823.084529706168
1
Iteration 18500: Loss = -9823.08420499437
Iteration 18600: Loss = -9823.084441036088
1
Iteration 18700: Loss = -9823.084554290055
2
Iteration 18800: Loss = -9823.084248880163
Iteration 18900: Loss = -9823.09672389573
1
Iteration 19000: Loss = -9823.09382879918
2
Iteration 19100: Loss = -9823.084185326503
Iteration 19200: Loss = -9823.084666652374
1
Iteration 19300: Loss = -9823.23786829466
2
Iteration 19400: Loss = -9823.08416510767
Iteration 19500: Loss = -9823.092733225049
1
Iteration 19600: Loss = -9823.084162717729
Iteration 19700: Loss = -9823.084689592566
1
Iteration 19800: Loss = -9823.084166111561
Iteration 19900: Loss = -9823.086961652878
1
pi: tensor([[9.9723e-01, 2.7666e-03],
        [2.1339e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9820, 0.0180], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1335, 0.2271],
         [0.7248, 0.0036]],

        [[0.5942, 0.1906],
         [0.5648, 0.6709]],

        [[0.5712, 0.1263],
         [0.6537, 0.7185]],

        [[0.6997, 0.2216],
         [0.6329, 0.5511]],

        [[0.5463, 0.1519],
         [0.6362, 0.6053]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.008844375641612072
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: -0.004267232452421997
Global Adjusted Rand Index: 0.00526032388196233
Average Adjusted Rand Index: 0.004101146220298821
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24950.450514314132
Iteration 100: Loss = -9830.69685190371
Iteration 200: Loss = -9829.37305732696
Iteration 300: Loss = -9828.996545013772
Iteration 400: Loss = -9828.812898228938
Iteration 500: Loss = -9828.706150314012
Iteration 600: Loss = -9828.633056067174
Iteration 700: Loss = -9828.568696672675
Iteration 800: Loss = -9828.484057665775
Iteration 900: Loss = -9828.393833847982
Iteration 1000: Loss = -9828.364819965347
Iteration 1100: Loss = -9828.345014615441
Iteration 1200: Loss = -9828.32669168071
Iteration 1300: Loss = -9828.302565013228
Iteration 1400: Loss = -9828.221229070563
Iteration 1500: Loss = -9827.819666855425
Iteration 1600: Loss = -9827.617380101154
Iteration 1700: Loss = -9827.451168755992
Iteration 1800: Loss = -9827.178444214935
Iteration 1900: Loss = -9824.19377304472
Iteration 2000: Loss = -9823.959271958613
Iteration 2100: Loss = -9823.8654048656
Iteration 2200: Loss = -9823.813023831744
Iteration 2300: Loss = -9823.77932898565
Iteration 2400: Loss = -9823.75590593213
Iteration 2500: Loss = -9823.73861759645
Iteration 2600: Loss = -9823.725344165756
Iteration 2700: Loss = -9823.714917852762
Iteration 2800: Loss = -9823.706477896643
Iteration 2900: Loss = -9823.699510369524
Iteration 3000: Loss = -9823.693742479238
Iteration 3100: Loss = -9823.688782135298
Iteration 3200: Loss = -9823.684572899952
Iteration 3300: Loss = -9823.680946796756
Iteration 3400: Loss = -9823.677761649082
Iteration 3500: Loss = -9823.674989191326
Iteration 3600: Loss = -9823.672519764352
Iteration 3700: Loss = -9823.670333523976
Iteration 3800: Loss = -9823.668396947518
Iteration 3900: Loss = -9823.666654862749
Iteration 4000: Loss = -9823.6650645275
Iteration 4100: Loss = -9823.663643126074
Iteration 4200: Loss = -9823.662302468312
Iteration 4300: Loss = -9823.661112926347
Iteration 4400: Loss = -9823.66009596355
Iteration 4500: Loss = -9823.6590966037
Iteration 4600: Loss = -9823.658206017555
Iteration 4700: Loss = -9823.657393989051
Iteration 4800: Loss = -9823.65660709089
Iteration 4900: Loss = -9823.655905670259
Iteration 5000: Loss = -9823.655237916928
Iteration 5100: Loss = -9823.654676521453
Iteration 5200: Loss = -9823.654082170562
Iteration 5300: Loss = -9823.65353527609
Iteration 5400: Loss = -9823.65306874726
Iteration 5500: Loss = -9823.652611342673
Iteration 5600: Loss = -9823.652243967215
Iteration 5700: Loss = -9823.651835694089
Iteration 5800: Loss = -9823.651471702724
Iteration 5900: Loss = -9823.651133932499
Iteration 6000: Loss = -9823.65080611304
Iteration 6100: Loss = -9823.650509513885
Iteration 6200: Loss = -9823.650224416087
Iteration 6300: Loss = -9823.649938462551
Iteration 6400: Loss = -9823.649701297794
Iteration 6500: Loss = -9823.64947925805
Iteration 6600: Loss = -9823.649269395775
Iteration 6700: Loss = -9823.649068768935
Iteration 6800: Loss = -9823.64885792306
Iteration 6900: Loss = -9823.648671732235
Iteration 7000: Loss = -9823.648517105235
Iteration 7100: Loss = -9823.648355836905
Iteration 7200: Loss = -9823.648183672129
Iteration 7300: Loss = -9823.648054909441
Iteration 7400: Loss = -9823.647888887623
Iteration 7500: Loss = -9823.64780520242
Iteration 7600: Loss = -9823.647706224054
Iteration 7700: Loss = -9823.64755683115
Iteration 7800: Loss = -9823.6474407504
Iteration 7900: Loss = -9823.647332885932
Iteration 8000: Loss = -9823.64727436991
Iteration 8100: Loss = -9823.647146362564
Iteration 8200: Loss = -9823.647059396051
Iteration 8300: Loss = -9823.646995034738
Iteration 8400: Loss = -9823.646898159097
Iteration 8500: Loss = -9823.646817836145
Iteration 8600: Loss = -9823.646774216322
Iteration 8700: Loss = -9823.646723341213
Iteration 8800: Loss = -9823.646645330698
Iteration 8900: Loss = -9823.684375247196
1
Iteration 9000: Loss = -9823.646534431813
Iteration 9100: Loss = -9823.646441540377
Iteration 9200: Loss = -9823.646384059068
Iteration 9300: Loss = -9823.65885902008
1
Iteration 9400: Loss = -9823.646035880889
Iteration 9500: Loss = -9823.64586929021
Iteration 9600: Loss = -9823.645860754688
Iteration 9700: Loss = -9823.842188451186
1
Iteration 9800: Loss = -9823.645786125398
Iteration 9900: Loss = -9823.645712240783
Iteration 10000: Loss = -9823.645663721196
Iteration 10100: Loss = -9823.75646519182
1
Iteration 10200: Loss = -9823.645577521225
Iteration 10300: Loss = -9823.645569396911
Iteration 10400: Loss = -9823.645533731242
Iteration 10500: Loss = -9823.72819874842
1
Iteration 10600: Loss = -9823.64553116907
Iteration 10700: Loss = -9823.64551724689
Iteration 10800: Loss = -9823.64548855412
Iteration 10900: Loss = -9824.111473787383
1
Iteration 11000: Loss = -9823.645598163186
2
Iteration 11100: Loss = -9823.645423334125
Iteration 11200: Loss = -9823.64545891887
Iteration 11300: Loss = -9823.652397403233
1
Iteration 11400: Loss = -9823.645535729875
Iteration 11500: Loss = -9823.645301274551
Iteration 11600: Loss = -9823.645294393771
Iteration 11700: Loss = -9823.645453525374
1
Iteration 11800: Loss = -9823.645225863844
Iteration 11900: Loss = -9823.645224481896
Iteration 12000: Loss = -9823.806628104043
1
Iteration 12100: Loss = -9823.645179618203
Iteration 12200: Loss = -9823.645136258803
Iteration 12300: Loss = -9823.645176528078
Iteration 12400: Loss = -9823.645282784437
1
Iteration 12500: Loss = -9823.64530872191
2
Iteration 12600: Loss = -9823.645277721385
3
Iteration 12700: Loss = -9823.647707379432
4
Iteration 12800: Loss = -9823.645268894008
Iteration 12900: Loss = -9823.645263420161
Iteration 13000: Loss = -9823.645261032769
Iteration 13100: Loss = -9823.645416390562
1
Iteration 13200: Loss = -9823.645314236586
Iteration 13300: Loss = -9823.645400838775
Iteration 13400: Loss = -9823.647714231256
1
Iteration 13500: Loss = -9823.645281389645
Iteration 13600: Loss = -9823.64527765491
Iteration 13700: Loss = -9823.645376634542
Iteration 13800: Loss = -9823.645225545644
Iteration 13900: Loss = -9823.64518670284
Iteration 14000: Loss = -9823.645567452777
1
Iteration 14100: Loss = -9823.645158339088
Iteration 14200: Loss = -9823.645154439595
Iteration 14300: Loss = -9823.657210891153
1
Iteration 14400: Loss = -9823.37734027378
Iteration 14500: Loss = -9823.348997066736
Iteration 14600: Loss = -9823.346286112468
Iteration 14700: Loss = -9823.343453683627
Iteration 14800: Loss = -9823.344067882246
1
Iteration 14900: Loss = -9823.341953687877
Iteration 15000: Loss = -9823.341553422037
Iteration 15100: Loss = -9823.341355461855
Iteration 15200: Loss = -9823.341022954033
Iteration 15300: Loss = -9823.403785907474
1
Iteration 15400: Loss = -9823.34070461933
Iteration 15500: Loss = -9823.340572306519
Iteration 15600: Loss = -9823.35262703922
1
Iteration 15700: Loss = -9823.340395298203
Iteration 15800: Loss = -9823.340326299323
Iteration 15900: Loss = -9823.44293690906
1
Iteration 16000: Loss = -9823.34022833021
Iteration 16100: Loss = -9823.340152219766
Iteration 16200: Loss = -9823.340098762126
Iteration 16300: Loss = -9823.343498013579
1
Iteration 16400: Loss = -9823.340039521487
Iteration 16500: Loss = -9823.339995227689
Iteration 16600: Loss = -9823.350687070793
1
Iteration 16700: Loss = -9823.339938810352
Iteration 16800: Loss = -9823.33990694633
Iteration 16900: Loss = -9823.341142504716
1
Iteration 17000: Loss = -9823.339872969684
Iteration 17100: Loss = -9823.339860158116
Iteration 17200: Loss = -9823.474486699623
1
Iteration 17300: Loss = -9823.339818285487
Iteration 17400: Loss = -9823.339802146718
Iteration 17500: Loss = -9823.3397784535
Iteration 17600: Loss = -9823.339818216306
Iteration 17700: Loss = -9823.339761327461
Iteration 17800: Loss = -9823.341270057514
1
Iteration 17900: Loss = -9823.33973663219
Iteration 18000: Loss = -9823.403738144209
1
Iteration 18100: Loss = -9823.339720836346
Iteration 18200: Loss = -9823.33991295757
1
Iteration 18300: Loss = -9823.33969123769
Iteration 18400: Loss = -9823.339794196958
1
Iteration 18500: Loss = -9823.339809374118
2
Iteration 18600: Loss = -9823.340282256871
3
Iteration 18700: Loss = -9823.339668849338
Iteration 18800: Loss = -9823.34011681591
1
Iteration 18900: Loss = -9823.345927504524
2
Iteration 19000: Loss = -9823.339650013962
Iteration 19100: Loss = -9823.339833477316
1
Iteration 19200: Loss = -9823.339635068882
Iteration 19300: Loss = -9823.34536660714
1
Iteration 19400: Loss = -9823.339654946862
Iteration 19500: Loss = -9823.339621586485
Iteration 19600: Loss = -9823.51624550933
1
Iteration 19700: Loss = -9823.339624841814
Iteration 19800: Loss = -9823.339598419414
Iteration 19900: Loss = -9823.367331443533
1
pi: tensor([[1.0000e+00, 1.1786e-07],
        [1.0355e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0160, 0.9840], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.6496e-05, 2.3159e-01],
         [6.0603e-01, 1.3405e-01]],

        [[7.1041e-01, 1.9725e-01],
         [6.1534e-01, 5.0646e-01]],

        [[5.5965e-01, 1.2975e-01],
         [6.9431e-01, 6.7955e-01]],

        [[6.8496e-01, 2.2012e-01],
         [5.0521e-01, 5.3778e-01]],

        [[6.9913e-01, 1.5773e-01],
         [5.1211e-01, 6.3291e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.016025857647765086
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0012662455124815629
Global Adjusted Rand Index: 0.008128634764757336
Average Adjusted Rand Index: 0.007118380159204611
9961.488887135527
[0.00526032388196233, 0.008128634764757336] [0.004101146220298821, 0.007118380159204611] [9823.08407627704, 9823.339596833828]
-------------------------------------
This iteration is 24
True Objective function: Loss = -9940.99000736485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21863.281909706926
Iteration 100: Loss = -9815.65295863541
Iteration 200: Loss = -9814.247187737996
Iteration 300: Loss = -9813.859481682503
Iteration 400: Loss = -9813.672325874797
Iteration 500: Loss = -9813.540888088204
Iteration 600: Loss = -9813.367197537627
Iteration 700: Loss = -9813.117727312572
Iteration 800: Loss = -9812.93469124456
Iteration 900: Loss = -9812.790813901527
Iteration 1000: Loss = -9812.63871584093
Iteration 1100: Loss = -9812.48000419332
Iteration 1200: Loss = -9812.327151618601
Iteration 1300: Loss = -9812.191472525748
Iteration 1400: Loss = -9812.087363360784
Iteration 1500: Loss = -9812.002721172224
Iteration 1600: Loss = -9811.935532547379
Iteration 1700: Loss = -9811.886978180471
Iteration 1800: Loss = -9811.849143205316
Iteration 1900: Loss = -9811.819237180542
Iteration 2000: Loss = -9811.794728678251
Iteration 2100: Loss = -9811.775470520453
Iteration 2200: Loss = -9811.761341177473
Iteration 2300: Loss = -9811.751408216383
Iteration 2400: Loss = -9811.744473234909
Iteration 2500: Loss = -9811.73986523858
Iteration 2600: Loss = -9811.736824474934
Iteration 2700: Loss = -9811.734958386269
Iteration 2800: Loss = -9811.73376447818
Iteration 2900: Loss = -9811.733010966484
Iteration 3000: Loss = -9811.732607236749
Iteration 3100: Loss = -9811.732309740346
Iteration 3200: Loss = -9811.732140850305
Iteration 3300: Loss = -9811.732007293256
Iteration 3400: Loss = -9811.731915882237
Iteration 3500: Loss = -9811.73188543803
Iteration 3600: Loss = -9811.731809660343
Iteration 3700: Loss = -9811.731796672075
Iteration 3800: Loss = -9811.732763717857
1
Iteration 3900: Loss = -9811.731769224923
Iteration 4000: Loss = -9811.734153081656
1
Iteration 4100: Loss = -9811.731759121034
Iteration 4200: Loss = -9811.73172543228
Iteration 4300: Loss = -9811.731709062053
Iteration 4400: Loss = -9811.731744221212
Iteration 4500: Loss = -9811.731763823056
Iteration 4600: Loss = -9811.731725159729
Iteration 4700: Loss = -9811.731727689745
Iteration 4800: Loss = -9811.731668654531
Iteration 4900: Loss = -9811.731690378374
Iteration 5000: Loss = -9811.732636883578
1
Iteration 5100: Loss = -9811.731669039118
Iteration 5200: Loss = -9811.731732798497
Iteration 5300: Loss = -9811.731689007236
Iteration 5400: Loss = -9811.73170691637
Iteration 5500: Loss = -9811.731694620828
Iteration 5600: Loss = -9811.731662416154
Iteration 5700: Loss = -9811.731701042065
Iteration 5800: Loss = -9811.73163352506
Iteration 5900: Loss = -9811.731844693344
1
Iteration 6000: Loss = -9811.73166494839
Iteration 6100: Loss = -9811.731648623474
Iteration 6200: Loss = -9811.732488070213
1
Iteration 6300: Loss = -9811.731617110268
Iteration 6400: Loss = -9811.731760777402
1
Iteration 6500: Loss = -9811.731587795059
Iteration 6600: Loss = -9811.731591870184
Iteration 6700: Loss = -9811.7316090582
Iteration 6800: Loss = -9811.731510758935
Iteration 6900: Loss = -9811.731663727254
1
Iteration 7000: Loss = -9811.731428054853
Iteration 7100: Loss = -9811.73134240741
Iteration 7200: Loss = -9811.738614324779
1
Iteration 7300: Loss = -9811.731141471008
Iteration 7400: Loss = -9811.734231742701
1
Iteration 7500: Loss = -9811.730466895438
Iteration 7600: Loss = -9811.734731785698
1
Iteration 7700: Loss = -9811.727086763494
Iteration 7800: Loss = -9811.705478293024
Iteration 7900: Loss = -9795.863804941699
Iteration 8000: Loss = -9795.655879619842
Iteration 8100: Loss = -9795.610339822591
Iteration 8200: Loss = -9795.60119198121
Iteration 8300: Loss = -9795.583068548258
Iteration 8400: Loss = -9795.580463099077
Iteration 8500: Loss = -9795.577701450868
Iteration 8600: Loss = -9795.573955378552
Iteration 8700: Loss = -9795.572808599509
Iteration 8800: Loss = -9795.571925919074
Iteration 8900: Loss = -9795.571222152894
Iteration 9000: Loss = -9795.570527564669
Iteration 9100: Loss = -9795.57012657192
Iteration 9200: Loss = -9795.569285361962
Iteration 9300: Loss = -9795.565050958276
Iteration 9400: Loss = -9795.56435430281
Iteration 9500: Loss = -9795.564016275137
Iteration 9600: Loss = -9795.563899956293
Iteration 9700: Loss = -9795.561176116289
Iteration 9800: Loss = -9795.560900382807
Iteration 9900: Loss = -9795.562181343565
1
Iteration 10000: Loss = -9795.560494935378
Iteration 10100: Loss = -9795.56192784934
1
Iteration 10200: Loss = -9795.560164750996
Iteration 10300: Loss = -9795.56045494955
1
Iteration 10400: Loss = -9795.559935803223
Iteration 10500: Loss = -9795.559801001446
Iteration 10600: Loss = -9795.573655776441
1
Iteration 10700: Loss = -9795.571144358075
2
Iteration 10800: Loss = -9795.61479078465
3
Iteration 10900: Loss = -9795.608924111912
4
Iteration 11000: Loss = -9795.582062662106
5
Iteration 11100: Loss = -9795.55974150594
Iteration 11200: Loss = -9795.558427532798
Iteration 11300: Loss = -9795.630764403139
1
Iteration 11400: Loss = -9795.558288915276
Iteration 11500: Loss = -9795.60259077049
1
Iteration 11600: Loss = -9795.559078667655
2
Iteration 11700: Loss = -9795.558205836021
Iteration 11800: Loss = -9795.55906429318
1
Iteration 11900: Loss = -9795.557849461218
Iteration 12000: Loss = -9795.557638485723
Iteration 12100: Loss = -9795.558257466091
1
Iteration 12200: Loss = -9795.557566189344
Iteration 12300: Loss = -9795.55756292265
Iteration 12400: Loss = -9795.557513181873
Iteration 12500: Loss = -9795.557508393033
Iteration 12600: Loss = -9795.557483123332
Iteration 12700: Loss = -9795.557430938798
Iteration 12800: Loss = -9795.55765561793
1
Iteration 12900: Loss = -9795.554288234609
Iteration 13000: Loss = -9795.688312943059
1
Iteration 13100: Loss = -9795.553356859968
Iteration 13200: Loss = -9795.559615693974
1
Iteration 13300: Loss = -9795.573473688266
2
Iteration 13400: Loss = -9795.556945077069
3
Iteration 13500: Loss = -9795.567720928138
4
Iteration 13600: Loss = -9795.644295191578
5
Iteration 13700: Loss = -9795.556861342217
6
Iteration 13800: Loss = -9795.554036031339
7
Iteration 13900: Loss = -9795.55341919125
Iteration 14000: Loss = -9795.554141551878
1
Iteration 14100: Loss = -9795.55372504083
2
Iteration 14200: Loss = -9795.673688248778
3
Iteration 14300: Loss = -9795.552791652224
Iteration 14400: Loss = -9795.552929529667
1
Iteration 14500: Loss = -9795.613824512715
2
Iteration 14600: Loss = -9795.55277479701
Iteration 14700: Loss = -9795.5535504489
1
Iteration 14800: Loss = -9795.552785286196
Iteration 14900: Loss = -9795.552808323477
Iteration 15000: Loss = -9795.568239555343
1
Iteration 15100: Loss = -9795.552729612926
Iteration 15200: Loss = -9795.556433123033
1
Iteration 15300: Loss = -9795.552886861651
2
Iteration 15400: Loss = -9795.553299778769
3
Iteration 15500: Loss = -9795.553724379519
4
Iteration 15600: Loss = -9795.555724675418
5
Iteration 15700: Loss = -9795.554803112218
6
Iteration 15800: Loss = -9795.553332945412
7
Iteration 15900: Loss = -9795.570761458019
8
Iteration 16000: Loss = -9795.55818238207
9
Iteration 16100: Loss = -9795.662696047706
10
Iteration 16200: Loss = -9795.552684890696
Iteration 16300: Loss = -9795.552811530493
1
Iteration 16400: Loss = -9795.553260736071
2
Iteration 16500: Loss = -9795.553497137133
3
Iteration 16600: Loss = -9795.551207130478
Iteration 16700: Loss = -9795.551985533588
1
Iteration 16800: Loss = -9795.551219042372
Iteration 16900: Loss = -9795.82551569284
1
Iteration 17000: Loss = -9795.551233686248
Iteration 17100: Loss = -9795.551222591535
Iteration 17200: Loss = -9795.551548474814
1
Iteration 17300: Loss = -9795.552385048182
2
Iteration 17400: Loss = -9795.551412656441
3
Iteration 17500: Loss = -9795.580569568207
4
Iteration 17600: Loss = -9795.551193225287
Iteration 17700: Loss = -9795.55244285109
1
Iteration 17800: Loss = -9795.551218238114
Iteration 17900: Loss = -9795.551298018365
Iteration 18000: Loss = -9795.551193707732
Iteration 18100: Loss = -9795.551443834582
1
Iteration 18200: Loss = -9795.551188103718
Iteration 18300: Loss = -9795.551412637646
1
Iteration 18400: Loss = -9795.555475655097
2
Iteration 18500: Loss = -9795.55121088512
Iteration 18600: Loss = -9795.551886108406
1
Iteration 18700: Loss = -9795.551000841886
Iteration 18800: Loss = -9795.551115257585
1
Iteration 18900: Loss = -9795.557320527358
2
Iteration 19000: Loss = -9795.566032771721
3
Iteration 19100: Loss = -9795.665980781052
4
Iteration 19200: Loss = -9795.55086815772
Iteration 19300: Loss = -9795.551848649078
1
Iteration 19400: Loss = -9795.550835741904
Iteration 19500: Loss = -9795.551237636919
1
Iteration 19600: Loss = -9795.550833469171
Iteration 19700: Loss = -9795.550922448523
Iteration 19800: Loss = -9795.552078620678
1
Iteration 19900: Loss = -9795.550881828622
pi: tensor([[1.0000e+00, 2.1035e-07],
        [8.0530e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4699, 0.5301], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1821, 0.1306],
         [0.7018, 0.1373]],

        [[0.6445, 0.1066],
         [0.7293, 0.6834]],

        [[0.7301, 0.1101],
         [0.5419, 0.5912]],

        [[0.6719, 0.1021],
         [0.6682, 0.7146]],

        [[0.6031, 0.1207],
         [0.5816, 0.5211]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01571644091972255
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 28
Adjusted Rand Index: 0.18540348045081
time is 2
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 32
Adjusted Rand Index: 0.12075296302627113
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 29
Adjusted Rand Index: 0.1680351735240574
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 35
Adjusted Rand Index: 0.0807576589447453
Global Adjusted Rand Index: 0.11112088667858254
Average Adjusted Rand Index: 0.11413314337312128
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21639.52786256005
Iteration 100: Loss = -9814.290388382571
Iteration 200: Loss = -9813.777690634115
Iteration 300: Loss = -9813.552351561766
Iteration 400: Loss = -9813.343451408413
Iteration 500: Loss = -9813.107357667363
Iteration 600: Loss = -9812.77621875143
Iteration 700: Loss = -9812.41894710582
Iteration 800: Loss = -9812.216494790673
Iteration 900: Loss = -9812.077622201223
Iteration 1000: Loss = -9811.983946525123
Iteration 1100: Loss = -9811.922074569284
Iteration 1200: Loss = -9811.88089405701
Iteration 1300: Loss = -9811.850638941049
Iteration 1400: Loss = -9811.826568507591
Iteration 1500: Loss = -9811.80698495157
Iteration 1600: Loss = -9811.79088947041
Iteration 1700: Loss = -9811.77778753191
Iteration 1800: Loss = -9811.767150778975
Iteration 1900: Loss = -9811.758371713773
Iteration 2000: Loss = -9811.751227497345
Iteration 2100: Loss = -9811.745605114984
Iteration 2200: Loss = -9811.741321130185
Iteration 2300: Loss = -9811.738204137819
Iteration 2400: Loss = -9811.735928429363
Iteration 2500: Loss = -9811.734272583162
Iteration 2600: Loss = -9811.73316969439
Iteration 2700: Loss = -9811.732364762876
Iteration 2800: Loss = -9811.731839462867
Iteration 2900: Loss = -9811.731438924007
Iteration 3000: Loss = -9811.731215885675
Iteration 3100: Loss = -9811.731036709249
Iteration 3200: Loss = -9811.730901129265
Iteration 3300: Loss = -9811.730834769754
Iteration 3400: Loss = -9811.730734874982
Iteration 3500: Loss = -9811.73069410317
Iteration 3600: Loss = -9811.730612462075
Iteration 3700: Loss = -9811.740601225849
1
Iteration 3800: Loss = -9811.730458142707
Iteration 3900: Loss = -9811.730415011592
Iteration 4000: Loss = -9811.730685976683
1
Iteration 4100: Loss = -9811.730278315961
Iteration 4200: Loss = -9811.730190897173
Iteration 4300: Loss = -9811.73008977178
Iteration 4400: Loss = -9811.729967945888
Iteration 4500: Loss = -9811.729863612578
Iteration 4600: Loss = -9811.72968086636
Iteration 4700: Loss = -9811.74556652944
1
Iteration 4800: Loss = -9811.729250777089
Iteration 4900: Loss = -9811.729014000339
Iteration 5000: Loss = -9811.728674690425
Iteration 5100: Loss = -9811.728211696409
Iteration 5200: Loss = -9811.727629627727
Iteration 5300: Loss = -9811.726736731807
Iteration 5400: Loss = -9811.736502309383
1
Iteration 5500: Loss = -9811.723418712594
Iteration 5600: Loss = -9811.719708438248
Iteration 5700: Loss = -9811.711578436612
Iteration 5800: Loss = -9811.684689192121
Iteration 5900: Loss = -9811.234461901086
Iteration 6000: Loss = -9795.711189293626
Iteration 6100: Loss = -9795.626660046739
Iteration 6200: Loss = -9795.602944130296
Iteration 6300: Loss = -9795.591661534832
Iteration 6400: Loss = -9795.584652247138
Iteration 6500: Loss = -9795.578381918047
Iteration 6600: Loss = -9795.57468500182
Iteration 6700: Loss = -9795.568180721884
Iteration 6800: Loss = -9795.56532515803
Iteration 6900: Loss = -9795.56367919936
Iteration 7000: Loss = -9795.56151537445
Iteration 7100: Loss = -9795.558747379106
Iteration 7200: Loss = -9795.557182595516
Iteration 7300: Loss = -9795.556104996505
Iteration 7400: Loss = -9795.555262493142
Iteration 7500: Loss = -9795.554581764101
Iteration 7600: Loss = -9795.553945648115
Iteration 7700: Loss = -9795.553444086278
Iteration 7800: Loss = -9795.554041044108
1
Iteration 7900: Loss = -9795.55265342065
Iteration 8000: Loss = -9795.5523350559
Iteration 8100: Loss = -9795.567277344662
1
Iteration 8200: Loss = -9795.551750060671
Iteration 8300: Loss = -9795.551491154085
Iteration 8400: Loss = -9795.551807024816
1
Iteration 8500: Loss = -9795.551095834424
Iteration 8600: Loss = -9795.55089190482
Iteration 8700: Loss = -9795.550879419516
Iteration 8800: Loss = -9795.550549490537
Iteration 8900: Loss = -9795.550662831174
1
Iteration 9000: Loss = -9795.550230576418
Iteration 9100: Loss = -9795.55062215352
1
Iteration 9200: Loss = -9795.546368934974
Iteration 9300: Loss = -9795.580064888687
1
Iteration 9400: Loss = -9795.546073810361
Iteration 9500: Loss = -9795.5459596863
Iteration 9600: Loss = -9795.546636846953
1
Iteration 9700: Loss = -9795.545763884624
Iteration 9800: Loss = -9795.6728337315
1
Iteration 9900: Loss = -9795.546031423788
2
Iteration 10000: Loss = -9795.546041206562
3
Iteration 10100: Loss = -9795.544827814676
Iteration 10200: Loss = -9795.547207288448
1
Iteration 10300: Loss = -9795.5476212138
2
Iteration 10400: Loss = -9795.546296908133
3
Iteration 10500: Loss = -9795.54490173392
Iteration 10600: Loss = -9795.544578662315
Iteration 10700: Loss = -9795.544750507503
1
Iteration 10800: Loss = -9795.55447255987
2
Iteration 10900: Loss = -9795.544115380271
Iteration 11000: Loss = -9795.803486656629
1
Iteration 11100: Loss = -9795.54332861274
Iteration 11200: Loss = -9795.554379415027
1
Iteration 11300: Loss = -9795.543217562683
Iteration 11400: Loss = -9795.543193858719
Iteration 11500: Loss = -9795.54318826147
Iteration 11600: Loss = -9795.543094151832
Iteration 11700: Loss = -9795.54322598788
1
Iteration 11800: Loss = -9795.543035165556
Iteration 11900: Loss = -9795.543132332201
Iteration 12000: Loss = -9795.56184487472
1
Iteration 12100: Loss = -9795.542982557696
Iteration 12200: Loss = -9795.550621830353
1
Iteration 12300: Loss = -9795.563905504714
2
Iteration 12400: Loss = -9795.544494297123
3
Iteration 12500: Loss = -9795.543035901313
Iteration 12600: Loss = -9795.545921248822
1
Iteration 12700: Loss = -9795.54245482589
Iteration 12800: Loss = -9795.542414958662
Iteration 12900: Loss = -9795.54239525746
Iteration 13000: Loss = -9795.542406924127
Iteration 13100: Loss = -9795.588726774564
1
Iteration 13200: Loss = -9795.549210463583
2
Iteration 13300: Loss = -9795.625365217102
3
Iteration 13400: Loss = -9795.5800173095
4
Iteration 13500: Loss = -9795.54678320341
5
Iteration 13600: Loss = -9795.592024445741
6
Iteration 13700: Loss = -9795.55727200566
7
Iteration 13800: Loss = -9795.603840545866
8
Iteration 13900: Loss = -9795.554657549068
9
Iteration 14000: Loss = -9795.54417693112
10
Iteration 14100: Loss = -9795.54224367801
Iteration 14200: Loss = -9795.544609767021
1
Iteration 14300: Loss = -9795.552814748815
2
Iteration 14400: Loss = -9795.5423056587
Iteration 14500: Loss = -9795.542227167305
Iteration 14600: Loss = -9795.55186979764
1
Iteration 14700: Loss = -9795.5417973585
Iteration 14800: Loss = -9795.541825432283
Iteration 14900: Loss = -9795.542330060342
1
Iteration 15000: Loss = -9795.541761894854
Iteration 15100: Loss = -9795.541747224435
Iteration 15200: Loss = -9795.541804732526
Iteration 15300: Loss = -9795.643134678896
1
Iteration 15400: Loss = -9795.541728178987
Iteration 15500: Loss = -9795.541894123826
1
Iteration 15600: Loss = -9795.544806529362
2
Iteration 15700: Loss = -9795.541714425653
Iteration 15800: Loss = -9795.541707791821
Iteration 15900: Loss = -9795.545543617694
1
Iteration 16000: Loss = -9795.54169587912
Iteration 16100: Loss = -9795.627539897154
1
Iteration 16200: Loss = -9795.541677335039
Iteration 16300: Loss = -9795.542402808709
1
Iteration 16400: Loss = -9795.54160889597
Iteration 16500: Loss = -9795.541676759769
Iteration 16600: Loss = -9795.541509768946
Iteration 16700: Loss = -9795.541423996738
Iteration 16800: Loss = -9795.542812137966
1
Iteration 16900: Loss = -9795.541397075653
Iteration 17000: Loss = -9795.542357334767
1
Iteration 17100: Loss = -9795.55015311092
2
Iteration 17200: Loss = -9795.541392494808
Iteration 17300: Loss = -9795.542252171497
1
Iteration 17400: Loss = -9795.543782260253
2
Iteration 17500: Loss = -9795.544384954332
3
Iteration 17600: Loss = -9795.54376572714
4
Iteration 17700: Loss = -9795.587400758823
5
Iteration 17800: Loss = -9795.543437756702
6
Iteration 17900: Loss = -9795.54317298369
7
Iteration 18000: Loss = -9795.548436757255
8
Iteration 18100: Loss = -9795.550482884386
9
Iteration 18200: Loss = -9795.541377413654
Iteration 18300: Loss = -9795.543598133689
1
Iteration 18400: Loss = -9795.541329347985
Iteration 18500: Loss = -9795.541422735865
Iteration 18600: Loss = -9795.541338586647
Iteration 18700: Loss = -9795.541433160271
Iteration 18800: Loss = -9795.541368535447
Iteration 18900: Loss = -9795.529495837913
Iteration 19000: Loss = -9795.530873246018
1
Iteration 19100: Loss = -9795.53899713681
2
Iteration 19200: Loss = -9795.529468446866
Iteration 19300: Loss = -9795.529427539479
Iteration 19400: Loss = -9795.529649480835
1
Iteration 19500: Loss = -9795.529525426793
Iteration 19600: Loss = -9795.52938926265
Iteration 19700: Loss = -9795.529756106318
1
Iteration 19800: Loss = -9795.569263009907
2
Iteration 19900: Loss = -9795.530018167816
3
pi: tensor([[1.0000e+00, 3.5274e-07],
        [1.4055e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5301, 0.4699], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1373, 0.1306],
         [0.5533, 0.1820]],

        [[0.7239, 0.1066],
         [0.7112, 0.7259]],

        [[0.7131, 0.1101],
         [0.6891, 0.7247]],

        [[0.5529, 0.1021],
         [0.6523, 0.5802]],

        [[0.5757, 0.1207],
         [0.6185, 0.5584]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.01571644091972255
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 72
Adjusted Rand Index: 0.18540348045081
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.12075296302627113
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 71
Adjusted Rand Index: 0.1680351735240574
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 65
Adjusted Rand Index: 0.0807576589447453
Global Adjusted Rand Index: 0.11112088667858254
Average Adjusted Rand Index: 0.11413314337312128
9940.99000736485
[0.11112088667858254, 0.11112088667858254] [0.11413314337312128, 0.11413314337312128] [9795.551374857227, 9795.5294109536]
-------------------------------------
This iteration is 25
True Objective function: Loss = -10027.797480885527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23583.508076633327
Iteration 100: Loss = -9927.050263384228
Iteration 200: Loss = -9925.41735708011
Iteration 300: Loss = -9925.022804461332
Iteration 400: Loss = -9924.821545892437
Iteration 500: Loss = -9924.663828672734
Iteration 600: Loss = -9924.520883815278
Iteration 700: Loss = -9924.384994480568
Iteration 800: Loss = -9924.249379664323
Iteration 900: Loss = -9924.108147529232
Iteration 1000: Loss = -9923.95757694257
Iteration 1100: Loss = -9923.796548207287
Iteration 1200: Loss = -9923.629355968325
Iteration 1300: Loss = -9923.465231326427
Iteration 1400: Loss = -9923.310351455792
Iteration 1500: Loss = -9923.172338508208
Iteration 1600: Loss = -9923.059043283693
Iteration 1700: Loss = -9922.9736473872
Iteration 1800: Loss = -9922.911975421292
Iteration 1900: Loss = -9922.865860264155
Iteration 2000: Loss = -9922.828233249906
Iteration 2100: Loss = -9922.791195237929
Iteration 2200: Loss = -9922.733754805811
Iteration 2300: Loss = -9922.549784947549
Iteration 2400: Loss = -9921.986749808979
Iteration 2500: Loss = -9921.461298860722
Iteration 2600: Loss = -9921.144669661668
Iteration 2700: Loss = -9920.909124164531
Iteration 2800: Loss = -9920.656176877963
Iteration 2900: Loss = -9920.379903964775
Iteration 3000: Loss = -9919.912748696788
Iteration 3100: Loss = -9918.93320498277
Iteration 3200: Loss = -9918.408286007603
Iteration 3300: Loss = -9918.212665515992
Iteration 3400: Loss = -9918.124553449446
Iteration 3500: Loss = -9918.07952603512
Iteration 3600: Loss = -9918.055427503108
Iteration 3700: Loss = -9918.038786106945
Iteration 3800: Loss = -9918.026484337162
Iteration 3900: Loss = -9918.016924563419
Iteration 4000: Loss = -9918.009323645174
Iteration 4100: Loss = -9918.003256137301
Iteration 4200: Loss = -9917.99819963947
Iteration 4300: Loss = -9917.993955626558
Iteration 4400: Loss = -9917.990377164364
Iteration 4500: Loss = -9917.987233637023
Iteration 4600: Loss = -9917.984540359485
Iteration 4700: Loss = -9917.982183584909
Iteration 4800: Loss = -9917.98015016839
Iteration 4900: Loss = -9917.97832068651
Iteration 5000: Loss = -9917.976763812538
Iteration 5100: Loss = -9917.975311181526
Iteration 5200: Loss = -9917.974007537265
Iteration 5300: Loss = -9917.972848360017
Iteration 5400: Loss = -9917.971753090089
Iteration 5500: Loss = -9917.9718352542
Iteration 5600: Loss = -9917.970187736797
Iteration 5700: Loss = -9917.969150852901
Iteration 5800: Loss = -9917.968445077264
Iteration 5900: Loss = -9917.967826585282
Iteration 6000: Loss = -9917.967200087322
Iteration 6100: Loss = -9917.966776511257
Iteration 6200: Loss = -9917.966137811638
Iteration 6300: Loss = -9917.965681322401
Iteration 6400: Loss = -9917.965254698152
Iteration 6500: Loss = -9917.964884594738
Iteration 6600: Loss = -9917.965485994731
1
Iteration 6700: Loss = -9917.964119272627
Iteration 6800: Loss = -9917.963830676992
Iteration 6900: Loss = -9917.96347266432
Iteration 7000: Loss = -9917.963845499331
1
Iteration 7100: Loss = -9917.96290261328
Iteration 7200: Loss = -9917.962697615598
Iteration 7300: Loss = -9917.962435068688
Iteration 7400: Loss = -9917.962235343635
Iteration 7500: Loss = -9917.962082640715
Iteration 7600: Loss = -9917.961792892345
Iteration 7700: Loss = -9917.965219926336
1
Iteration 7800: Loss = -9917.961481758613
Iteration 7900: Loss = -9917.969851521959
1
Iteration 8000: Loss = -9917.961170943738
Iteration 8100: Loss = -9917.965586842949
1
Iteration 8200: Loss = -9917.960881613159
Iteration 8300: Loss = -9917.96673471878
1
Iteration 8400: Loss = -9917.960624586742
Iteration 8500: Loss = -9917.960586914485
Iteration 8600: Loss = -9917.961004635728
1
Iteration 8700: Loss = -9918.133551869016
2
Iteration 8800: Loss = -9917.960223065356
Iteration 8900: Loss = -9917.967216798697
1
Iteration 9000: Loss = -9917.960056313015
Iteration 9100: Loss = -9917.960710963807
1
Iteration 9200: Loss = -9917.959902016806
Iteration 9300: Loss = -9917.959825762235
Iteration 9400: Loss = -9917.960571513271
1
Iteration 9500: Loss = -9917.9597046395
Iteration 9600: Loss = -9917.959613334862
Iteration 9700: Loss = -9917.959778082039
1
Iteration 9800: Loss = -9917.959503435246
Iteration 9900: Loss = -9917.959449065529
Iteration 10000: Loss = -9917.959601726066
1
Iteration 10100: Loss = -9917.959390707521
Iteration 10200: Loss = -9917.965978783712
1
Iteration 10300: Loss = -9917.959270514577
Iteration 10400: Loss = -9917.959250056687
Iteration 10500: Loss = -9918.084008466294
1
Iteration 10600: Loss = -9917.95919566714
Iteration 10700: Loss = -9917.959115980548
Iteration 10800: Loss = -9918.00379299278
1
Iteration 10900: Loss = -9917.959054421799
Iteration 11000: Loss = -9917.9590535633
Iteration 11100: Loss = -9917.972349615247
1
Iteration 11200: Loss = -9917.958990365853
Iteration 11300: Loss = -9917.958988230375
Iteration 11400: Loss = -9917.95903361585
Iteration 11500: Loss = -9917.961257238108
1
Iteration 11600: Loss = -9917.974636860812
2
Iteration 11700: Loss = -9917.960088060763
3
Iteration 11800: Loss = -9917.966402089245
4
Iteration 11900: Loss = -9917.958939061007
Iteration 12000: Loss = -9917.959070591434
1
Iteration 12100: Loss = -9918.007603934655
2
Iteration 12200: Loss = -9917.95879933915
Iteration 12300: Loss = -9917.959641056446
1
Iteration 12400: Loss = -9917.958783939963
Iteration 12500: Loss = -9917.963170296813
1
Iteration 12600: Loss = -9917.998284917405
2
Iteration 12700: Loss = -9917.966014288333
3
Iteration 12800: Loss = -9917.95878323405
Iteration 12900: Loss = -9917.960256797442
1
Iteration 13000: Loss = -9917.96261935889
2
Iteration 13100: Loss = -9917.95873642496
Iteration 13200: Loss = -9917.959214765397
1
Iteration 13300: Loss = -9917.95973160598
2
Iteration 13400: Loss = -9917.981713161238
3
Iteration 13500: Loss = -9918.017384738847
4
Iteration 13600: Loss = -9917.959038498351
5
Iteration 13700: Loss = -9917.95867402164
Iteration 13800: Loss = -9917.961938572364
1
Iteration 13900: Loss = -9917.960413428826
2
Iteration 14000: Loss = -9917.969939549255
3
Iteration 14100: Loss = -9917.960004645367
4
Iteration 14200: Loss = -9917.972174159087
5
Iteration 14300: Loss = -9917.96099723038
6
Iteration 14400: Loss = -9917.964551049552
7
Iteration 14500: Loss = -9917.959975562018
8
Iteration 14600: Loss = -9917.962170980521
9
Iteration 14700: Loss = -9917.961361718553
10
Iteration 14800: Loss = -9917.960729756309
11
Iteration 14900: Loss = -9917.969501869247
12
Iteration 15000: Loss = -9917.963757074991
13
Iteration 15100: Loss = -9917.982392408014
14
Iteration 15200: Loss = -9917.969664345514
15
Stopping early at iteration 15200 due to no improvement.
pi: tensor([[1.0000e+00, 1.2318e-06],
        [2.8741e-01, 7.1259e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8445, 0.1555], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1409, 0.1308],
         [0.7214, 0.1176]],

        [[0.7186, 0.1002],
         [0.6879, 0.6495]],

        [[0.5981, 0.0937],
         [0.5740, 0.6886]],

        [[0.7244, 0.1069],
         [0.5095, 0.5518]],

        [[0.6000, 0.2291],
         [0.5790, 0.6398]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.006214218236452756
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.012184899471542247
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.010091437982433116
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.004614384364187652
Average Adjusted Rand Index: 0.00668869045106628
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24188.9970904662
Iteration 100: Loss = -9927.54435552762
Iteration 200: Loss = -9926.672709635775
Iteration 300: Loss = -9926.43757412532
Iteration 400: Loss = -9926.32608547313
Iteration 500: Loss = -9926.264684062682
Iteration 600: Loss = -9926.22690040536
Iteration 700: Loss = -9926.201422792017
Iteration 800: Loss = -9926.182522497596
Iteration 900: Loss = -9926.16677592206
Iteration 1000: Loss = -9926.151300908059
Iteration 1100: Loss = -9926.131495539974
Iteration 1200: Loss = -9926.090489272427
Iteration 1300: Loss = -9925.745721706933
Iteration 1400: Loss = -9923.446512078874
Iteration 1500: Loss = -9923.214453656148
Iteration 1600: Loss = -9923.007656531401
Iteration 1700: Loss = -9922.839825916584
Iteration 1800: Loss = -9922.332786189365
Iteration 1900: Loss = -9921.8498946443
Iteration 2000: Loss = -9921.796465825417
Iteration 2100: Loss = -9921.792534863866
Iteration 2200: Loss = -9921.790525128134
Iteration 2300: Loss = -9921.788915268105
Iteration 2400: Loss = -9921.787711945693
Iteration 2500: Loss = -9921.786660004102
Iteration 2600: Loss = -9921.785685177521
Iteration 2700: Loss = -9921.784809352337
Iteration 2800: Loss = -9921.78395388995
Iteration 2900: Loss = -9921.783094072143
Iteration 3000: Loss = -9921.782166645966
Iteration 3100: Loss = -9921.781016711646
Iteration 3200: Loss = -9921.779590166194
Iteration 3300: Loss = -9921.777536520527
Iteration 3400: Loss = -9921.774254694992
Iteration 3500: Loss = -9921.768066116994
Iteration 3600: Loss = -9921.75384538402
Iteration 3700: Loss = -9921.715228879319
Iteration 3800: Loss = -9921.674728550428
Iteration 3900: Loss = -9921.663616178459
Iteration 4000: Loss = -9921.65570567823
Iteration 4100: Loss = -9921.650300114592
Iteration 4200: Loss = -9921.64468485285
Iteration 4300: Loss = -9921.637359715101
Iteration 4400: Loss = -9921.605676471787
Iteration 4500: Loss = -9921.41187337256
Iteration 4600: Loss = -9920.689430682147
Iteration 4700: Loss = -9920.65265935034
Iteration 4800: Loss = -9920.644506761655
Iteration 4900: Loss = -9920.640715349267
Iteration 5000: Loss = -9920.638163641548
Iteration 5100: Loss = -9920.649680673718
1
Iteration 5200: Loss = -9920.635085616252
Iteration 5300: Loss = -9920.63377913873
Iteration 5400: Loss = -9920.632399909458
Iteration 5500: Loss = -9920.630728184397
Iteration 5600: Loss = -9920.62926108534
Iteration 5700: Loss = -9920.62464985966
Iteration 5800: Loss = -9920.616750969053
Iteration 5900: Loss = -9920.593880900557
Iteration 6000: Loss = -9920.501859476411
Iteration 6100: Loss = -9917.975935594306
Iteration 6200: Loss = -9917.96110918887
Iteration 6300: Loss = -9917.960623550249
Iteration 6400: Loss = -9917.96029539686
Iteration 6500: Loss = -9917.960098677931
Iteration 6600: Loss = -9917.959913023893
Iteration 6700: Loss = -9917.959772314198
Iteration 6800: Loss = -9917.95964771733
Iteration 6900: Loss = -9917.959516086785
Iteration 7000: Loss = -9917.959399021658
Iteration 7100: Loss = -9917.95936301448
Iteration 7200: Loss = -9917.959259599967
Iteration 7300: Loss = -9917.959466159577
1
Iteration 7400: Loss = -9917.959216093863
Iteration 7500: Loss = -9917.959133229246
Iteration 7600: Loss = -9917.959094853208
Iteration 7700: Loss = -9917.959047805232
Iteration 7800: Loss = -9917.959153839356
1
Iteration 7900: Loss = -9917.973566360573
2
Iteration 8000: Loss = -9917.95894795827
Iteration 8100: Loss = -9917.958904764075
Iteration 8200: Loss = -9917.97142285811
1
Iteration 8300: Loss = -9917.958821326636
Iteration 8400: Loss = -9917.960266134382
1
Iteration 8500: Loss = -9917.958824131292
Iteration 8600: Loss = -9917.967525920098
1
Iteration 8700: Loss = -9917.958761823034
Iteration 8800: Loss = -9917.958980122481
1
Iteration 8900: Loss = -9917.958748852512
Iteration 9000: Loss = -9917.95871189459
Iteration 9100: Loss = -9918.005018029799
1
Iteration 9200: Loss = -9917.958697232527
Iteration 9300: Loss = -9917.958672496043
Iteration 9400: Loss = -9917.964678100943
1
Iteration 9500: Loss = -9917.958658378131
Iteration 9600: Loss = -9917.958642509173
Iteration 9700: Loss = -9917.958849152723
1
Iteration 9800: Loss = -9917.958620213216
Iteration 9900: Loss = -9917.963580073665
1
Iteration 10000: Loss = -9917.958634110895
Iteration 10100: Loss = -9917.958696282682
Iteration 10200: Loss = -9917.959904778785
1
Iteration 10300: Loss = -9917.958737996718
Iteration 10400: Loss = -9917.95860111844
Iteration 10500: Loss = -9917.958680385187
Iteration 10600: Loss = -9917.95860973793
Iteration 10700: Loss = -9917.989785772417
1
Iteration 10800: Loss = -9917.958780510251
2
Iteration 10900: Loss = -9917.958695482097
Iteration 11000: Loss = -9917.994794020127
1
Iteration 11100: Loss = -9917.959215227203
2
Iteration 11200: Loss = -9917.958750587668
Iteration 11300: Loss = -9917.962136406726
1
Iteration 11400: Loss = -9917.960469571666
2
Iteration 11500: Loss = -9917.958578061984
Iteration 11600: Loss = -9917.96812201967
1
Iteration 11700: Loss = -9917.9592890962
2
Iteration 11800: Loss = -9918.01752809902
3
Iteration 11900: Loss = -9917.958539664089
Iteration 12000: Loss = -9917.959084801127
1
Iteration 12100: Loss = -9917.960527169222
2
Iteration 12200: Loss = -9917.97486582974
3
Iteration 12300: Loss = -9917.994837263252
4
Iteration 12400: Loss = -9917.964686696632
5
Iteration 12500: Loss = -9917.958550203955
Iteration 12600: Loss = -9918.013290319117
1
Iteration 12700: Loss = -9917.958821474775
2
Iteration 12800: Loss = -9917.95963794269
3
Iteration 12900: Loss = -9917.962819953751
4
Iteration 13000: Loss = -9917.958794075073
5
Iteration 13100: Loss = -9917.958606730974
Iteration 13200: Loss = -9917.965968877354
1
Iteration 13300: Loss = -9917.962213955307
2
Iteration 13400: Loss = -9917.963232866914
3
Iteration 13500: Loss = -9917.95867254537
Iteration 13600: Loss = -9917.967973248173
1
Iteration 13700: Loss = -9917.974898389371
2
Iteration 13800: Loss = -9917.962550418399
3
Iteration 13900: Loss = -9918.01674217275
4
Iteration 14000: Loss = -9917.96090118832
5
Iteration 14100: Loss = -9917.959371660501
6
Iteration 14200: Loss = -9917.96408477067
7
Iteration 14300: Loss = -9917.959689613877
8
Iteration 14400: Loss = -9917.959191121343
9
Iteration 14500: Loss = -9917.960578203925
10
Iteration 14600: Loss = -9917.958718162721
Iteration 14700: Loss = -9917.96171014645
1
Iteration 14800: Loss = -9917.959661356776
2
Iteration 14900: Loss = -9917.958660539207
Iteration 15000: Loss = -9917.96013009887
1
Iteration 15100: Loss = -9917.962031151912
2
Iteration 15200: Loss = -9917.958508594216
Iteration 15300: Loss = -9917.958622937625
1
Iteration 15400: Loss = -9917.96743980565
2
Iteration 15500: Loss = -9917.970542174404
3
Iteration 15600: Loss = -9917.959371958927
4
Iteration 15700: Loss = -9917.964944936271
5
Iteration 15800: Loss = -9917.963746860383
6
Iteration 15900: Loss = -9917.971861006527
7
Iteration 16000: Loss = -9917.9587805005
8
Iteration 16100: Loss = -9917.96147078768
9
Iteration 16200: Loss = -9917.961326299002
10
Iteration 16300: Loss = -9917.97770739617
11
Iteration 16400: Loss = -9917.958493319446
Iteration 16500: Loss = -9917.958621595897
1
Iteration 16600: Loss = -9917.967359573197
2
Iteration 16700: Loss = -9917.958518743326
Iteration 16800: Loss = -9917.958518399788
Iteration 16900: Loss = -9917.965065154793
1
Iteration 17000: Loss = -9917.961427898717
2
Iteration 17100: Loss = -9917.95850846754
Iteration 17200: Loss = -9917.960557405366
1
Iteration 17300: Loss = -9917.960034625765
2
Iteration 17400: Loss = -9917.962912194938
3
Iteration 17500: Loss = -9917.958819407928
4
Iteration 17600: Loss = -9917.958632094402
5
Iteration 17700: Loss = -9917.973269467764
6
Iteration 17800: Loss = -9918.007066217999
7
Iteration 17900: Loss = -9917.96287237176
8
Iteration 18000: Loss = -9917.959836467488
9
Iteration 18100: Loss = -9917.95896347008
10
Iteration 18200: Loss = -9918.108748932586
11
Iteration 18300: Loss = -9917.958777502896
12
Iteration 18400: Loss = -9917.959696466402
13
Iteration 18500: Loss = -9917.991590412808
14
Iteration 18600: Loss = -9918.027887174203
15
Stopping early at iteration 18600 due to no improvement.
pi: tensor([[1.0000e+00, 3.0799e-08],
        [2.8019e-01, 7.1981e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8484, 0.1516], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1403, 0.1301],
         [0.6756, 0.1186]],

        [[0.5975, 0.0997],
         [0.5165, 0.5140]],

        [[0.5943, 0.0949],
         [0.6425, 0.5305]],

        [[0.7065, 0.1061],
         [0.6187, 0.7268]],

        [[0.6293, 0.2289],
         [0.6549, 0.6707]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.006214218236452756
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.012184899471542247
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.010091437982433116
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.004614384364187652
Average Adjusted Rand Index: 0.00668869045106628
10027.797480885527
[0.004614384364187652, 0.004614384364187652] [0.00668869045106628, 0.00668869045106628] [9917.969664345514, 9918.027887174203]
-------------------------------------
This iteration is 26
True Objective function: Loss = -9867.808325692185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20067.0537714839
Iteration 100: Loss = -9726.867233636056
Iteration 200: Loss = -9726.418102690568
Iteration 300: Loss = -9726.28929217801
Iteration 400: Loss = -9726.23417564832
Iteration 500: Loss = -9726.202204673426
Iteration 600: Loss = -9726.180762428981
Iteration 700: Loss = -9726.165063205934
Iteration 800: Loss = -9726.152944835285
Iteration 900: Loss = -9726.143046540148
Iteration 1000: Loss = -9726.134639875507
Iteration 1100: Loss = -9726.127273397968
Iteration 1200: Loss = -9726.12059340848
Iteration 1300: Loss = -9726.114353618903
Iteration 1400: Loss = -9726.108365015823
Iteration 1500: Loss = -9726.102372625344
Iteration 1600: Loss = -9726.096214154339
Iteration 1700: Loss = -9726.089707003423
Iteration 1800: Loss = -9726.082585659247
Iteration 1900: Loss = -9726.074610258183
Iteration 2000: Loss = -9726.065395948115
Iteration 2100: Loss = -9726.054535194968
Iteration 2200: Loss = -9726.041178237585
Iteration 2300: Loss = -9726.024514611894
Iteration 2400: Loss = -9726.00297074574
Iteration 2500: Loss = -9725.974512676796
Iteration 2600: Loss = -9725.936368521143
Iteration 2700: Loss = -9725.886064931114
Iteration 2800: Loss = -9725.825165255155
Iteration 2900: Loss = -9725.764282746584
Iteration 3000: Loss = -9725.715660589221
Iteration 3100: Loss = -9725.68161842938
Iteration 3200: Loss = -9725.659136796952
Iteration 3300: Loss = -9725.652325740999
Iteration 3400: Loss = -9725.634661394619
Iteration 3500: Loss = -9725.627436111696
Iteration 3600: Loss = -9725.622537295563
Iteration 3700: Loss = -9725.618518954692
Iteration 3800: Loss = -9725.615722774946
Iteration 3900: Loss = -9725.61301840087
Iteration 4000: Loss = -9725.610838347038
Iteration 4100: Loss = -9725.60905224591
Iteration 4200: Loss = -9725.607208721956
Iteration 4300: Loss = -9725.60543093085
Iteration 4400: Loss = -9725.604160636778
Iteration 4500: Loss = -9725.601455940841
Iteration 4600: Loss = -9725.599149061813
Iteration 4700: Loss = -9725.596207287079
Iteration 4800: Loss = -9725.591222047167
Iteration 4900: Loss = -9725.58462031655
Iteration 5000: Loss = -9725.573964469368
Iteration 5100: Loss = -9725.554039185821
Iteration 5200: Loss = -9725.523018529224
Iteration 5300: Loss = -9725.488152803418
Iteration 5400: Loss = -9725.461656160087
Iteration 5500: Loss = -9725.445336171779
Iteration 5600: Loss = -9725.43568542898
Iteration 5700: Loss = -9725.429721027955
Iteration 5800: Loss = -9725.425717846967
Iteration 5900: Loss = -9725.42294343465
Iteration 6000: Loss = -9725.420840677632
Iteration 6100: Loss = -9725.419197003886
Iteration 6200: Loss = -9725.417958643444
Iteration 6300: Loss = -9725.416883393023
Iteration 6400: Loss = -9725.416013544798
Iteration 6500: Loss = -9725.415314802609
Iteration 6600: Loss = -9725.414684877473
Iteration 6700: Loss = -9725.414190693326
Iteration 6800: Loss = -9725.413677302553
Iteration 6900: Loss = -9725.413435743129
Iteration 7000: Loss = -9725.412865937085
Iteration 7100: Loss = -9725.412971921816
1
Iteration 7200: Loss = -9725.412280275194
Iteration 7300: Loss = -9725.447881114209
1
Iteration 7400: Loss = -9725.411793635942
Iteration 7500: Loss = -9725.41153150261
Iteration 7600: Loss = -9725.411392654727
Iteration 7700: Loss = -9725.411147551395
Iteration 7800: Loss = -9725.411361959294
1
Iteration 7900: Loss = -9725.410867974231
Iteration 8000: Loss = -9725.454854927382
1
Iteration 8100: Loss = -9725.410593901232
Iteration 8200: Loss = -9725.41048735165
Iteration 8300: Loss = -9725.41035888755
Iteration 8400: Loss = -9725.410232460094
Iteration 8500: Loss = -9725.41029424177
Iteration 8600: Loss = -9725.410044409751
Iteration 8700: Loss = -9725.410465573032
1
Iteration 8800: Loss = -9725.409873478335
Iteration 8900: Loss = -9725.409900086544
Iteration 9000: Loss = -9725.409717034112
Iteration 9100: Loss = -9725.409833698639
1
Iteration 9200: Loss = -9725.588709000785
2
Iteration 9300: Loss = -9725.409553587739
Iteration 9400: Loss = -9725.437132433792
1
Iteration 9500: Loss = -9725.409419015123
Iteration 9600: Loss = -9725.4100163201
1
Iteration 9700: Loss = -9725.409325415383
Iteration 9800: Loss = -9725.409497231894
1
Iteration 9900: Loss = -9725.44091568182
2
Iteration 10000: Loss = -9725.409160354528
Iteration 10100: Loss = -9725.410205560634
1
Iteration 10200: Loss = -9725.419181024725
2
Iteration 10300: Loss = -9725.409059453856
Iteration 10400: Loss = -9725.409480080156
1
Iteration 10500: Loss = -9725.423803529073
2
Iteration 10600: Loss = -9725.408975904156
Iteration 10700: Loss = -9725.409112202055
1
Iteration 10800: Loss = -9725.410806749882
2
Iteration 10900: Loss = -9725.421915273022
3
Iteration 11000: Loss = -9725.408847137754
Iteration 11100: Loss = -9725.409924549143
1
Iteration 11200: Loss = -9725.549816873758
2
Iteration 11300: Loss = -9725.408779551757
Iteration 11400: Loss = -9725.408892716925
1
Iteration 11500: Loss = -9725.411377916542
2
Iteration 11600: Loss = -9725.46559873016
3
Iteration 11700: Loss = -9725.416324838237
4
Iteration 11800: Loss = -9725.409395261417
5
Iteration 11900: Loss = -9725.408709356636
Iteration 12000: Loss = -9725.408875686588
1
Iteration 12100: Loss = -9725.409189571223
2
Iteration 12200: Loss = -9725.432951326744
3
Iteration 12300: Loss = -9725.43806138186
4
Iteration 12400: Loss = -9725.408610908567
Iteration 12500: Loss = -9725.409733251572
1
Iteration 12600: Loss = -9725.468273038145
2
Iteration 12700: Loss = -9725.408584199118
Iteration 12800: Loss = -9725.758097926877
1
Iteration 12900: Loss = -9725.408504343717
Iteration 13000: Loss = -9725.557976337508
1
Iteration 13100: Loss = -9725.408523645016
Iteration 13200: Loss = -9725.408760966173
1
Iteration 13300: Loss = -9725.416937613234
2
Iteration 13400: Loss = -9725.40854565499
Iteration 13500: Loss = -9725.40997221048
1
Iteration 13600: Loss = -9725.422661457795
2
Iteration 13700: Loss = -9725.408518349537
Iteration 13800: Loss = -9725.408493906623
Iteration 13900: Loss = -9725.409446783991
1
Iteration 14000: Loss = -9725.409343331736
2
Iteration 14100: Loss = -9725.410448734052
3
Iteration 14200: Loss = -9725.45893265601
4
Iteration 14300: Loss = -9725.408444507986
Iteration 14400: Loss = -9725.408599280225
1
Iteration 14500: Loss = -9725.439443818585
2
Iteration 14600: Loss = -9725.40841755459
Iteration 14700: Loss = -9725.408386398009
Iteration 14800: Loss = -9725.408514932524
1
Iteration 14900: Loss = -9725.416167234443
2
Iteration 15000: Loss = -9725.409185762504
3
Iteration 15100: Loss = -9725.408420365733
Iteration 15200: Loss = -9725.408880275665
1
Iteration 15300: Loss = -9725.408455034447
Iteration 15400: Loss = -9725.411683538498
1
Iteration 15500: Loss = -9725.408378767188
Iteration 15600: Loss = -9725.409605237921
1
Iteration 15700: Loss = -9725.431620346304
2
Iteration 15800: Loss = -9725.416239778013
3
Iteration 15900: Loss = -9725.578923589825
4
Iteration 16000: Loss = -9725.408412368117
Iteration 16100: Loss = -9725.411144680356
1
Iteration 16200: Loss = -9725.411520841611
2
Iteration 16300: Loss = -9725.41009001954
3
Iteration 16400: Loss = -9725.446655593722
4
Iteration 16500: Loss = -9725.418910166973
5
Iteration 16600: Loss = -9725.417762281735
6
Iteration 16700: Loss = -9725.41307365028
7
Iteration 16800: Loss = -9725.41231962066
8
Iteration 16900: Loss = -9725.409218279841
9
Iteration 17000: Loss = -9725.40859155989
10
Iteration 17100: Loss = -9725.408673476857
11
Iteration 17200: Loss = -9725.408751454768
12
Iteration 17300: Loss = -9725.413418302089
13
Iteration 17400: Loss = -9725.409024234312
14
Iteration 17500: Loss = -9725.408386285391
Iteration 17600: Loss = -9725.408558469995
1
Iteration 17700: Loss = -9725.40912795334
2
Iteration 17800: Loss = -9725.413069716282
3
Iteration 17900: Loss = -9725.437524397024
4
Iteration 18000: Loss = -9725.409023816117
5
Iteration 18100: Loss = -9725.408685052458
6
Iteration 18200: Loss = -9725.439601013864
7
Iteration 18300: Loss = -9725.447125017383
8
Iteration 18400: Loss = -9725.41087868426
9
Iteration 18500: Loss = -9725.409393268423
10
Iteration 18600: Loss = -9725.49584704632
11
Iteration 18700: Loss = -9725.420615724352
12
Iteration 18800: Loss = -9725.522205838899
13
Iteration 18900: Loss = -9725.408336466142
Iteration 19000: Loss = -9725.408741913108
1
Iteration 19100: Loss = -9725.408371568063
Iteration 19200: Loss = -9725.408453834574
Iteration 19300: Loss = -9725.408350358011
Iteration 19400: Loss = -9725.40935719492
1
Iteration 19500: Loss = -9725.408360621286
Iteration 19600: Loss = -9725.7603669566
1
Iteration 19700: Loss = -9725.408325521947
Iteration 19800: Loss = -9725.408325086897
Iteration 19900: Loss = -9725.408552462357
1
pi: tensor([[9.9999e-01, 6.8251e-06],
        [1.1539e-01, 8.8461e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([8.3082e-06, 9.9999e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1429, 0.1296],
         [0.6856, 0.1313]],

        [[0.5312, 0.1354],
         [0.7262, 0.5902]],

        [[0.7149, 0.1342],
         [0.6623, 0.5170]],

        [[0.6165, 0.1331],
         [0.6743, 0.5706]],

        [[0.6478, 0.1415],
         [0.7116, 0.5379]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: 0.00020085494627357869
Average Adjusted Rand Index: -0.0001522251028708703
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20924.107887263523
Iteration 100: Loss = -9727.31725091759
Iteration 200: Loss = -9726.51935415652
Iteration 300: Loss = -9726.331129467639
Iteration 400: Loss = -9726.26454322798
Iteration 500: Loss = -9726.227759418725
Iteration 600: Loss = -9726.202796262025
Iteration 700: Loss = -9726.184219054861
Iteration 800: Loss = -9726.169677136857
Iteration 900: Loss = -9726.1580295553
Iteration 1000: Loss = -9726.148393101974
Iteration 1100: Loss = -9726.140198944933
Iteration 1200: Loss = -9726.133056278155
Iteration 1300: Loss = -9726.126717954801
Iteration 1400: Loss = -9726.120885622147
Iteration 1500: Loss = -9726.11540125304
Iteration 1600: Loss = -9726.11007945016
Iteration 1700: Loss = -9726.10484098987
Iteration 1800: Loss = -9726.09954724021
Iteration 1900: Loss = -9726.093990888248
Iteration 2000: Loss = -9726.088086674525
Iteration 2100: Loss = -9726.081688880275
Iteration 2200: Loss = -9726.074477903005
Iteration 2300: Loss = -9726.066329942665
Iteration 2400: Loss = -9726.056812983132
Iteration 2500: Loss = -9726.045496919056
Iteration 2600: Loss = -9726.03166932178
Iteration 2700: Loss = -9726.014513184153
Iteration 2800: Loss = -9725.992643092532
Iteration 2900: Loss = -9725.96425228625
Iteration 3000: Loss = -9725.92712331194
Iteration 3100: Loss = -9725.879597829611
Iteration 3200: Loss = -9725.823490526745
Iteration 3300: Loss = -9725.767332753574
Iteration 3400: Loss = -9725.720909190328
Iteration 3500: Loss = -9725.686864185043
Iteration 3600: Loss = -9725.665551857213
Iteration 3700: Loss = -9725.647898282028
Iteration 3800: Loss = -9725.652496050778
1
Iteration 3900: Loss = -9725.62900910922
Iteration 4000: Loss = -9725.623222205446
Iteration 4100: Loss = -9725.618760135334
Iteration 4200: Loss = -9725.61517746106
Iteration 4300: Loss = -9725.612469914513
Iteration 4400: Loss = -9725.609980553256
Iteration 4500: Loss = -9725.611465211141
1
Iteration 4600: Loss = -9725.605952628366
Iteration 4700: Loss = -9725.60404472612
Iteration 4800: Loss = -9725.602296726056
Iteration 4900: Loss = -9725.600217038294
Iteration 5000: Loss = -9725.598151056502
Iteration 5100: Loss = -9725.595223628949
Iteration 5200: Loss = -9725.591908305378
Iteration 5300: Loss = -9725.589529899611
Iteration 5400: Loss = -9725.580773053158
Iteration 5500: Loss = -9725.570979324177
Iteration 5600: Loss = -9725.555556791693
Iteration 5700: Loss = -9725.53253455215
Iteration 5800: Loss = -9725.504365225497
Iteration 5900: Loss = -9725.478414102363
Iteration 6000: Loss = -9725.45936870632
Iteration 6100: Loss = -9725.446690433377
Iteration 6200: Loss = -9725.438333388007
Iteration 6300: Loss = -9725.432581004125
Iteration 6400: Loss = -9725.428530549983
Iteration 6500: Loss = -9725.425535848972
Iteration 6600: Loss = -9725.423362984702
Iteration 6700: Loss = -9725.42137374745
Iteration 6800: Loss = -9725.42192183316
1
Iteration 6900: Loss = -9725.41867928876
Iteration 7000: Loss = -9725.417645852254
Iteration 7100: Loss = -9725.41679031598
Iteration 7200: Loss = -9725.416042025563
Iteration 7300: Loss = -9725.415464731337
Iteration 7400: Loss = -9725.414819218313
Iteration 7500: Loss = -9725.414349362873
Iteration 7600: Loss = -9725.413873691015
Iteration 7700: Loss = -9725.413512205501
Iteration 7800: Loss = -9725.413255798248
Iteration 7900: Loss = -9725.412789484733
Iteration 8000: Loss = -9725.440225628174
1
Iteration 8100: Loss = -9725.412260418752
Iteration 8200: Loss = -9725.414010280994
1
Iteration 8300: Loss = -9725.411787867897
Iteration 8400: Loss = -9725.487456213992
1
Iteration 8500: Loss = -9725.41138808932
Iteration 8600: Loss = -9725.411819337123
1
Iteration 8700: Loss = -9725.411076555858
Iteration 8800: Loss = -9725.411015560427
Iteration 8900: Loss = -9725.410962444772
Iteration 9000: Loss = -9725.410665597336
Iteration 9100: Loss = -9725.412375948274
1
Iteration 9200: Loss = -9725.410564103444
Iteration 9300: Loss = -9725.411259964561
1
Iteration 9400: Loss = -9725.411328922853
2
Iteration 9500: Loss = -9725.43783000707
3
Iteration 9600: Loss = -9725.41120127987
4
Iteration 9700: Loss = -9725.431291227358
5
Iteration 9800: Loss = -9725.415146855052
6
Iteration 9900: Loss = -9725.44640666976
7
Iteration 10000: Loss = -9725.412074892085
8
Iteration 10100: Loss = -9725.422886475564
9
Iteration 10200: Loss = -9725.424574248971
10
Iteration 10300: Loss = -9725.409506765785
Iteration 10400: Loss = -9725.418023434222
1
Iteration 10500: Loss = -9725.454345154867
2
Iteration 10600: Loss = -9725.416185395101
3
Iteration 10700: Loss = -9725.433306167628
4
Iteration 10800: Loss = -9725.409236068026
Iteration 10900: Loss = -9725.409220735237
Iteration 11000: Loss = -9725.40925786788
Iteration 11100: Loss = -9725.409147138549
Iteration 11200: Loss = -9725.409373617753
1
Iteration 11300: Loss = -9725.40905320418
Iteration 11400: Loss = -9725.446759882292
1
Iteration 11500: Loss = -9725.408993818031
Iteration 11600: Loss = -9725.444101448762
1
Iteration 11700: Loss = -9725.408905756653
Iteration 11800: Loss = -9725.431140902543
1
Iteration 11900: Loss = -9725.41061804472
2
Iteration 12000: Loss = -9725.44550488477
3
Iteration 12100: Loss = -9725.408980774197
Iteration 12200: Loss = -9725.412567010591
1
Iteration 12300: Loss = -9725.492366444487
2
Iteration 12400: Loss = -9725.536414383836
3
Iteration 12500: Loss = -9725.41059600481
4
Iteration 12600: Loss = -9725.409126999986
5
Iteration 12700: Loss = -9725.408820970799
Iteration 12800: Loss = -9725.410012978775
1
Iteration 12900: Loss = -9725.408810612445
Iteration 13000: Loss = -9725.523761187487
1
Iteration 13100: Loss = -9725.408618291785
Iteration 13200: Loss = -9725.4092065992
1
Iteration 13300: Loss = -9725.566457803881
2
Iteration 13400: Loss = -9725.408598121443
Iteration 13500: Loss = -9725.460130595502
1
Iteration 13600: Loss = -9725.409383775304
2
Iteration 13700: Loss = -9725.40858448096
Iteration 13800: Loss = -9725.408899646072
1
Iteration 13900: Loss = -9725.42472005592
2
Iteration 14000: Loss = -9725.41057998156
3
Iteration 14100: Loss = -9725.409757526753
4
Iteration 14200: Loss = -9725.408534205406
Iteration 14300: Loss = -9725.414625316564
1
Iteration 14400: Loss = -9725.40913993604
2
Iteration 14500: Loss = -9725.613656060084
3
Iteration 14600: Loss = -9725.408486554612
Iteration 14700: Loss = -9725.409699513
1
Iteration 14800: Loss = -9725.411756026324
2
Iteration 14900: Loss = -9725.416356737398
3
Iteration 15000: Loss = -9725.408437611628
Iteration 15100: Loss = -9725.437106130545
1
Iteration 15200: Loss = -9725.429856406376
2
Iteration 15300: Loss = -9725.410113942016
3
Iteration 15400: Loss = -9725.410725963808
4
Iteration 15500: Loss = -9725.408436006366
Iteration 15600: Loss = -9725.40877610854
1
Iteration 15700: Loss = -9725.408386076457
Iteration 15800: Loss = -9725.414125163188
1
Iteration 15900: Loss = -9725.408403513055
Iteration 16000: Loss = -9725.483925195324
1
Iteration 16100: Loss = -9725.40839321708
Iteration 16200: Loss = -9725.777623851147
1
Iteration 16300: Loss = -9725.408401095074
Iteration 16400: Loss = -9725.409407125091
1
Iteration 16500: Loss = -9725.409550324603
2
Iteration 16600: Loss = -9725.448607082126
3
Iteration 16700: Loss = -9725.421921504716
4
Iteration 16800: Loss = -9725.476161134526
5
Iteration 16900: Loss = -9725.4084162852
Iteration 17000: Loss = -9725.408578223749
1
Iteration 17100: Loss = -9725.409795460531
2
Iteration 17200: Loss = -9725.411981143498
3
Iteration 17300: Loss = -9725.427777306655
4
Iteration 17400: Loss = -9725.453669035767
5
Iteration 17500: Loss = -9725.41279494047
6
Iteration 17600: Loss = -9725.4086321146
7
Iteration 17700: Loss = -9725.416675769558
8
Iteration 17800: Loss = -9725.409285198182
9
Iteration 17900: Loss = -9725.40941059724
10
Iteration 18000: Loss = -9725.409365988891
11
Iteration 18100: Loss = -9725.408462643432
Iteration 18200: Loss = -9725.408372367297
Iteration 18300: Loss = -9725.419572184866
1
Iteration 18400: Loss = -9725.408378114194
Iteration 18500: Loss = -9725.408501458998
1
Iteration 18600: Loss = -9725.410995229158
2
Iteration 18700: Loss = -9725.408473576344
Iteration 18800: Loss = -9725.424312504074
1
Iteration 18900: Loss = -9725.442406571883
2
Iteration 19000: Loss = -9725.408404075526
Iteration 19100: Loss = -9725.41191053575
1
Iteration 19200: Loss = -9725.473411690326
2
Iteration 19300: Loss = -9725.408382125348
Iteration 19400: Loss = -9725.408603352435
1
Iteration 19500: Loss = -9725.431842370508
2
Iteration 19600: Loss = -9725.408419120324
Iteration 19700: Loss = -9725.40843449858
Iteration 19800: Loss = -9725.433124235038
1
Iteration 19900: Loss = -9725.41047100362
2
pi: tensor([[8.8480e-01, 1.1520e-01],
        [1.1800e-05, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 1.2553e-05], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1313, 0.1295],
         [0.5948, 0.1429]],

        [[0.6755, 0.1354],
         [0.6643, 0.5313]],

        [[0.6728, 0.1342],
         [0.5695, 0.7015]],

        [[0.6346, 0.1331],
         [0.6165, 0.6662]],

        [[0.6818, 0.1415],
         [0.6239, 0.6424]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: 0.00020085494627357869
Average Adjusted Rand Index: -0.0001522251028708703
9867.808325692185
[0.00020085494627357869, 0.00020085494627357869] [-0.0001522251028708703, -0.0001522251028708703] [9725.408341203802, 9725.40835490604]
-------------------------------------
This iteration is 27
True Objective function: Loss = -10034.945147061346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20002.673486055723
Iteration 100: Loss = -9922.909748706174
Iteration 200: Loss = -9920.211633255554
Iteration 300: Loss = -9919.145010015613
Iteration 400: Loss = -9918.592564563567
Iteration 500: Loss = -9918.267399489423
Iteration 600: Loss = -9918.027454813393
Iteration 700: Loss = -9917.861438172784
Iteration 800: Loss = -9917.748693043883
Iteration 900: Loss = -9917.655433794362
Iteration 1000: Loss = -9917.576938551108
Iteration 1100: Loss = -9917.51171278757
Iteration 1200: Loss = -9917.45815025519
Iteration 1300: Loss = -9917.415772490529
Iteration 1400: Loss = -9917.381868475426
Iteration 1500: Loss = -9917.356011551123
Iteration 1600: Loss = -9917.337131080903
Iteration 1700: Loss = -9917.322819790319
Iteration 1800: Loss = -9917.311425414247
Iteration 1900: Loss = -9917.302000884156
Iteration 2000: Loss = -9917.293874916923
Iteration 2100: Loss = -9917.286665163127
Iteration 2200: Loss = -9917.279997430427
Iteration 2300: Loss = -9917.27410923081
Iteration 2400: Loss = -9917.2690855484
Iteration 2500: Loss = -9917.26470339989
Iteration 2600: Loss = -9917.260740577092
Iteration 2700: Loss = -9917.257183450056
Iteration 2800: Loss = -9917.253880166501
Iteration 2900: Loss = -9917.25074495517
Iteration 3000: Loss = -9917.24778397667
Iteration 3100: Loss = -9917.244976548465
Iteration 3200: Loss = -9917.242223083233
Iteration 3300: Loss = -9917.239527584916
Iteration 3400: Loss = -9917.236880015535
Iteration 3500: Loss = -9917.2342202405
Iteration 3600: Loss = -9917.23164578931
Iteration 3700: Loss = -9917.229046881968
Iteration 3800: Loss = -9917.226557895285
Iteration 3900: Loss = -9917.224087594657
Iteration 4000: Loss = -9917.221744084856
Iteration 4100: Loss = -9917.219587874466
Iteration 4200: Loss = -9917.217596403725
Iteration 4300: Loss = -9917.220690180111
1
Iteration 4400: Loss = -9917.214365082318
Iteration 4500: Loss = -9917.213090126053
Iteration 4600: Loss = -9917.212180748078
Iteration 4700: Loss = -9917.211411822902
Iteration 4800: Loss = -9917.210854736662
Iteration 4900: Loss = -9917.210435073956
Iteration 5000: Loss = -9917.210140716701
Iteration 5100: Loss = -9917.209990720763
Iteration 5200: Loss = -9917.209854003164
Iteration 5300: Loss = -9917.20984838216
Iteration 5400: Loss = -9917.209716744783
Iteration 5500: Loss = -9917.210321895627
1
Iteration 5600: Loss = -9917.209765243359
Iteration 5700: Loss = -9917.209597818586
Iteration 5800: Loss = -9917.210328705498
1
Iteration 5900: Loss = -9917.20958938378
Iteration 6000: Loss = -9917.209630840882
Iteration 6100: Loss = -9917.20978242376
1
Iteration 6200: Loss = -9917.209574769999
Iteration 6300: Loss = -9917.20956319739
Iteration 6400: Loss = -9917.209564221612
Iteration 6500: Loss = -9917.209557867265
Iteration 6600: Loss = -9917.209648846978
Iteration 6700: Loss = -9917.212347321665
1
Iteration 6800: Loss = -9917.209686407148
Iteration 6900: Loss = -9917.20957706297
Iteration 7000: Loss = -9917.228409388528
1
Iteration 7100: Loss = -9917.209549170224
Iteration 7200: Loss = -9917.210159115351
1
Iteration 7300: Loss = -9917.209562994285
Iteration 7400: Loss = -9917.209901997116
1
Iteration 7500: Loss = -9917.209519898995
Iteration 7600: Loss = -9917.210110843796
1
Iteration 7700: Loss = -9917.209553227052
Iteration 7800: Loss = -9917.215106339298
1
Iteration 7900: Loss = -9917.209525132728
Iteration 8000: Loss = -9917.219040864285
1
Iteration 8100: Loss = -9917.209509947528
Iteration 8200: Loss = -9917.20982892448
1
Iteration 8300: Loss = -9917.209545489199
Iteration 8400: Loss = -9917.210039811509
1
Iteration 8500: Loss = -9917.20952465587
Iteration 8600: Loss = -9917.213470765286
1
Iteration 8700: Loss = -9917.209542469223
Iteration 8800: Loss = -9917.211745297516
1
Iteration 8900: Loss = -9917.209566762644
Iteration 9000: Loss = -9917.209543301335
Iteration 9100: Loss = -9917.210575373827
1
Iteration 9200: Loss = -9917.209513794003
Iteration 9300: Loss = -9917.209513810905
Iteration 9400: Loss = -9917.20955072064
Iteration 9500: Loss = -9917.209537898732
Iteration 9600: Loss = -9917.20956048452
Iteration 9700: Loss = -9917.209704357261
1
Iteration 9800: Loss = -9917.209530492564
Iteration 9900: Loss = -9917.209531600785
Iteration 10000: Loss = -9917.209558793464
Iteration 10100: Loss = -9917.209532200162
Iteration 10200: Loss = -9917.339673240842
1
Iteration 10300: Loss = -9917.209586997371
Iteration 10400: Loss = -9917.209538142382
Iteration 10500: Loss = -9917.209674190812
1
Iteration 10600: Loss = -9917.21318932986
2
Iteration 10700: Loss = -9917.210290919784
3
Iteration 10800: Loss = -9917.209776833155
4
Iteration 10900: Loss = -9917.336499864372
5
Iteration 11000: Loss = -9917.233415957151
6
Iteration 11100: Loss = -9917.295432818331
7
Iteration 11200: Loss = -9917.214595354797
8
Iteration 11300: Loss = -9917.209589767444
Iteration 11400: Loss = -9917.226209812186
1
Iteration 11500: Loss = -9917.209577018972
Iteration 11600: Loss = -9917.209588524212
Iteration 11700: Loss = -9917.215147853529
1
Iteration 11800: Loss = -9917.420996763234
2
Iteration 11900: Loss = -9917.20958804667
Iteration 12000: Loss = -9917.214621070552
1
Iteration 12100: Loss = -9917.209579617716
Iteration 12200: Loss = -9917.209638889486
Iteration 12300: Loss = -9917.2101688425
1
Iteration 12400: Loss = -9917.209904861775
2
Iteration 12500: Loss = -9917.214292630077
3
Iteration 12600: Loss = -9917.213997302551
4
Iteration 12700: Loss = -9917.23779229949
5
Iteration 12800: Loss = -9917.236122379927
6
Iteration 12900: Loss = -9917.252298850397
7
Iteration 13000: Loss = -9917.20960220336
Iteration 13100: Loss = -9917.209590741088
Iteration 13200: Loss = -9917.251991351299
1
Iteration 13300: Loss = -9917.209550258027
Iteration 13400: Loss = -9917.223735871255
1
Iteration 13500: Loss = -9917.260773219778
2
Iteration 13600: Loss = -9917.209535600243
Iteration 13700: Loss = -9917.209735652506
1
Iteration 13800: Loss = -9917.209554677083
Iteration 13900: Loss = -9917.209816313618
1
Iteration 14000: Loss = -9917.345952885737
2
Iteration 14100: Loss = -9917.209544473919
Iteration 14200: Loss = -9917.21169849272
1
Iteration 14300: Loss = -9917.21257698097
2
Iteration 14400: Loss = -9917.209574991432
Iteration 14500: Loss = -9917.209772373597
1
Iteration 14600: Loss = -9917.211586395693
2
Iteration 14700: Loss = -9917.209548023133
Iteration 14800: Loss = -9917.20962713962
Iteration 14900: Loss = -9917.209924307277
1
Iteration 15000: Loss = -9917.212708493322
2
Iteration 15100: Loss = -9917.209543569019
Iteration 15200: Loss = -9917.209630895497
Iteration 15300: Loss = -9917.231955740042
1
Iteration 15400: Loss = -9917.217763091247
2
Iteration 15500: Loss = -9917.210870755549
3
Iteration 15600: Loss = -9917.211083974278
4
Iteration 15700: Loss = -9917.2148230404
5
Iteration 15800: Loss = -9917.212940726118
6
Iteration 15900: Loss = -9917.215214559157
7
Iteration 16000: Loss = -9917.352899763378
8
Iteration 16100: Loss = -9917.20956741539
Iteration 16200: Loss = -9917.209753501604
1
Iteration 16300: Loss = -9917.210105559705
2
Iteration 16400: Loss = -9917.211652003234
3
Iteration 16500: Loss = -9917.209538594065
Iteration 16600: Loss = -9917.210165616665
1
Iteration 16700: Loss = -9917.20965977894
2
Iteration 16800: Loss = -9917.209547141778
Iteration 16900: Loss = -9917.2626756082
1
Iteration 17000: Loss = -9917.212478348034
2
Iteration 17100: Loss = -9917.212485790029
3
Iteration 17200: Loss = -9917.225924160319
4
Iteration 17300: Loss = -9917.23658276227
5
Iteration 17400: Loss = -9917.210255669797
6
Iteration 17500: Loss = -9917.22778185188
7
Iteration 17600: Loss = -9917.222817988632
8
Iteration 17700: Loss = -9917.23555493811
9
Iteration 17800: Loss = -9917.232096489606
10
Iteration 17900: Loss = -9917.209858707825
11
Iteration 18000: Loss = -9917.383435586786
12
Iteration 18100: Loss = -9917.209551664699
Iteration 18200: Loss = -9917.209594894946
Iteration 18300: Loss = -9917.249884778315
1
Iteration 18400: Loss = -9917.210016040262
2
Iteration 18500: Loss = -9917.21715788631
3
Iteration 18600: Loss = -9917.210280668736
4
Iteration 18700: Loss = -9917.211725012843
5
Iteration 18800: Loss = -9917.217322183773
6
Iteration 18900: Loss = -9917.209713273242
7
Iteration 19000: Loss = -9917.209592153034
Iteration 19100: Loss = -9917.217981516265
1
Iteration 19200: Loss = -9917.220362179902
2
Iteration 19300: Loss = -9917.20953329737
Iteration 19400: Loss = -9917.211575664203
1
Iteration 19500: Loss = -9917.22610717277
2
Iteration 19600: Loss = -9917.209526906012
Iteration 19700: Loss = -9917.210428592944
1
Iteration 19800: Loss = -9917.210730275732
2
Iteration 19900: Loss = -9917.20962197838
pi: tensor([[0.2977, 0.7023],
        [0.1029, 0.8971]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0052, 0.9948], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2300, 0.0979],
         [0.6300, 0.1303]],

        [[0.7118, 0.1629],
         [0.6718, 0.7023]],

        [[0.6943, 0.1831],
         [0.5719, 0.7055]],

        [[0.6640, 0.1664],
         [0.5633, 0.7182]],

        [[0.6057, 0.1601],
         [0.5278, 0.7169]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.022328462449853457
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.006467401572035518
Global Adjusted Rand Index: -0.0049928028915088825
Average Adjusted Rand Index: -0.007226153649864247
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22989.50971462866
Iteration 100: Loss = -9923.302336182205
Iteration 200: Loss = -9921.024926303706
Iteration 300: Loss = -9920.197857193323
Iteration 400: Loss = -9919.68011884108
Iteration 500: Loss = -9919.34176448449
Iteration 600: Loss = -9919.094818563504
Iteration 700: Loss = -9918.793887129415
Iteration 800: Loss = -9918.020483584758
Iteration 900: Loss = -9917.688170305642
Iteration 1000: Loss = -9917.572420948147
Iteration 1100: Loss = -9917.498665705267
Iteration 1200: Loss = -9917.44513327901
Iteration 1300: Loss = -9917.405662372987
Iteration 1400: Loss = -9917.37555214176
Iteration 1500: Loss = -9917.352185398475
Iteration 1600: Loss = -9917.334025834367
Iteration 1700: Loss = -9917.319628578787
Iteration 1800: Loss = -9917.307897796562
Iteration 1900: Loss = -9917.298201474921
Iteration 2000: Loss = -9917.290007041649
Iteration 2100: Loss = -9917.282972395398
Iteration 2200: Loss = -9917.276930503309
Iteration 2300: Loss = -9917.271533593925
Iteration 2400: Loss = -9917.26678661044
Iteration 2500: Loss = -9917.262543399149
Iteration 2600: Loss = -9917.258688157892
Iteration 2700: Loss = -9917.25516427858
Iteration 2800: Loss = -9917.251927375677
Iteration 2900: Loss = -9917.248916711636
Iteration 3000: Loss = -9917.246071740377
Iteration 3100: Loss = -9917.243406437388
Iteration 3200: Loss = -9917.240850895547
Iteration 3300: Loss = -9917.23840924056
Iteration 3400: Loss = -9917.236051428263
Iteration 3500: Loss = -9917.233883636703
Iteration 3600: Loss = -9917.231566670143
Iteration 3700: Loss = -9917.229356742837
Iteration 3800: Loss = -9917.22735221564
Iteration 3900: Loss = -9917.225281017829
Iteration 4000: Loss = -9917.22333203349
Iteration 4100: Loss = -9917.221515791854
Iteration 4200: Loss = -9917.219812564063
Iteration 4300: Loss = -9917.218229185593
Iteration 4400: Loss = -9917.216809011526
Iteration 4500: Loss = -9917.2155043026
Iteration 4600: Loss = -9917.21453258406
Iteration 4700: Loss = -9917.213487046372
Iteration 4800: Loss = -9917.212669442024
Iteration 4900: Loss = -9917.2121654522
Iteration 5000: Loss = -9917.211498583096
Iteration 5100: Loss = -9917.211079339131
Iteration 5200: Loss = -9917.210770040869
Iteration 5300: Loss = -9917.21055989214
Iteration 5400: Loss = -9917.210354269908
Iteration 5500: Loss = -9917.21019490633
Iteration 5600: Loss = -9917.226117587037
1
Iteration 5700: Loss = -9917.210239111944
Iteration 5800: Loss = -9917.209928830573
Iteration 5900: Loss = -9917.209878267926
Iteration 6000: Loss = -9917.210753698006
1
Iteration 6100: Loss = -9917.211179513793
2
Iteration 6200: Loss = -9917.209817830331
Iteration 6300: Loss = -9917.209836152992
Iteration 6400: Loss = -9917.20968957534
Iteration 6500: Loss = -9917.210105218886
1
Iteration 6600: Loss = -9917.209647520462
Iteration 6700: Loss = -9917.222068959292
1
Iteration 6800: Loss = -9917.209610944738
Iteration 6900: Loss = -9917.209668931448
Iteration 7000: Loss = -9917.214347320092
1
Iteration 7100: Loss = -9917.209595791963
Iteration 7200: Loss = -9917.215377525972
1
Iteration 7300: Loss = -9917.209599578002
Iteration 7400: Loss = -9917.209605603193
Iteration 7500: Loss = -9917.209567461927
Iteration 7600: Loss = -9917.209575414237
Iteration 7700: Loss = -9917.20957790386
Iteration 7800: Loss = -9917.209544836503
Iteration 7900: Loss = -9917.209563828961
Iteration 8000: Loss = -9917.209547285242
Iteration 8100: Loss = -9917.209565263393
Iteration 8200: Loss = -9917.287560226274
1
Iteration 8300: Loss = -9917.209543422123
Iteration 8400: Loss = -9917.209612586978
Iteration 8500: Loss = -9917.209642650789
Iteration 8600: Loss = -9917.209569468814
Iteration 8700: Loss = -9917.212902017598
1
Iteration 8800: Loss = -9917.209542016259
Iteration 8900: Loss = -9917.374489372443
1
Iteration 9000: Loss = -9917.209541388425
Iteration 9100: Loss = -9917.209531885344
Iteration 9200: Loss = -9917.209643453405
1
Iteration 9300: Loss = -9917.209536852974
Iteration 9400: Loss = -9917.244382102279
1
Iteration 9500: Loss = -9917.209534839034
Iteration 9600: Loss = -9917.20954849506
Iteration 9700: Loss = -9917.210245933133
1
Iteration 9800: Loss = -9917.20952772027
Iteration 9900: Loss = -9917.234914285355
1
Iteration 10000: Loss = -9917.20957052883
Iteration 10100: Loss = -9917.209543563598
Iteration 10200: Loss = -9917.20954547505
Iteration 10300: Loss = -9917.216394437775
1
Iteration 10400: Loss = -9917.209526144847
Iteration 10500: Loss = -9917.209711218575
1
Iteration 10600: Loss = -9917.259500770899
2
Iteration 10700: Loss = -9917.209537956962
Iteration 10800: Loss = -9917.210409360743
1
Iteration 10900: Loss = -9917.209551126056
Iteration 11000: Loss = -9917.209603182318
Iteration 11100: Loss = -9917.209542495955
Iteration 11200: Loss = -9917.210211907633
1
Iteration 11300: Loss = -9917.209562339396
Iteration 11400: Loss = -9917.210320098653
1
Iteration 11500: Loss = -9917.231448678445
2
Iteration 11600: Loss = -9917.209549039035
Iteration 11700: Loss = -9917.21609053442
1
Iteration 11800: Loss = -9917.226505426494
2
Iteration 11900: Loss = -9917.209764122113
3
Iteration 12000: Loss = -9917.243760360852
4
Iteration 12100: Loss = -9917.209565142359
Iteration 12200: Loss = -9917.209615689164
Iteration 12300: Loss = -9917.291764777327
1
Iteration 12400: Loss = -9917.209518482146
Iteration 12500: Loss = -9917.210637214013
1
Iteration 12600: Loss = -9917.209706674139
2
Iteration 12700: Loss = -9917.20962236897
3
Iteration 12800: Loss = -9917.20962441009
4
Iteration 12900: Loss = -9917.266280616877
5
Iteration 13000: Loss = -9917.21113816171
6
Iteration 13100: Loss = -9917.209698656927
7
Iteration 13200: Loss = -9917.20963835513
8
Iteration 13300: Loss = -9917.24724718283
9
Iteration 13400: Loss = -9917.306517579214
10
Iteration 13500: Loss = -9917.309130526148
11
Iteration 13600: Loss = -9917.23089403614
12
Iteration 13700: Loss = -9917.312525998032
13
Iteration 13800: Loss = -9917.210618531713
14
Iteration 13900: Loss = -9917.209725164517
15
Stopping early at iteration 13900 due to no improvement.
pi: tensor([[0.8966, 0.1034],
        [0.7007, 0.2993]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9948, 0.0052], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1301, 0.0981],
         [0.5739, 0.2297]],

        [[0.6429, 0.1629],
         [0.6329, 0.7180]],

        [[0.5327, 0.1828],
         [0.5817, 0.5260]],

        [[0.5871, 0.1662],
         [0.5956, 0.6570]],

        [[0.7044, 0.1599],
         [0.7017, 0.7135]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.022328462449853457
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
Global Adjusted Rand Index: -0.0049928028915088825
Average Adjusted Rand Index: -0.007226153649864247
10034.945147061346
[-0.0049928028915088825, -0.0049928028915088825] [-0.007226153649864247, -0.007226153649864247] [9917.2151890204, 9917.209725164517]
-------------------------------------
This iteration is 28
True Objective function: Loss = -10139.155066951182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24131.171456610606
Iteration 100: Loss = -10038.666910592872
Iteration 200: Loss = -10036.856889894552
Iteration 300: Loss = -10035.947988153755
Iteration 400: Loss = -10035.32434741429
Iteration 500: Loss = -10032.762803164458
Iteration 600: Loss = -10031.29853469887
Iteration 700: Loss = -10030.332406223048
Iteration 800: Loss = -10029.34258146627
Iteration 900: Loss = -10028.06085787904
Iteration 1000: Loss = -10026.44106980708
Iteration 1100: Loss = -10022.96119919509
Iteration 1200: Loss = -10021.217116341399
Iteration 1300: Loss = -10021.098338643227
Iteration 1400: Loss = -10021.067751633505
Iteration 1500: Loss = -10021.05018781739
Iteration 1600: Loss = -10021.037924922804
Iteration 1700: Loss = -10021.029752515862
Iteration 1800: Loss = -10021.021588328176
Iteration 1900: Loss = -10021.015244870856
Iteration 2000: Loss = -10021.009968284876
Iteration 2100: Loss = -10021.005694989903
Iteration 2200: Loss = -10021.004412387178
Iteration 2300: Loss = -10021.00378380603
Iteration 2400: Loss = -10021.003422564276
Iteration 2500: Loss = -10021.003202411051
Iteration 2600: Loss = -10021.003090166834
Iteration 2700: Loss = -10021.00301645815
Iteration 2800: Loss = -10021.003008441048
Iteration 2900: Loss = -10021.002919526622
Iteration 3000: Loss = -10021.002913882241
Iteration 3100: Loss = -10021.002897752074
Iteration 3200: Loss = -10021.002844108627
Iteration 3300: Loss = -10021.00283129341
Iteration 3400: Loss = -10021.002809429132
Iteration 3500: Loss = -10021.002749399422
Iteration 3600: Loss = -10021.002698340233
Iteration 3700: Loss = -10021.00267863636
Iteration 3800: Loss = -10021.002592424024
Iteration 3900: Loss = -10021.002589197056
Iteration 4000: Loss = -10021.013361007948
1
Iteration 4100: Loss = -10021.002569150345
Iteration 4200: Loss = -10021.002531730082
Iteration 4300: Loss = -10021.002663455482
1
Iteration 4400: Loss = -10021.002549687317
Iteration 4500: Loss = -10021.00255396261
Iteration 4600: Loss = -10021.002521197583
Iteration 4700: Loss = -10021.002530903208
Iteration 4800: Loss = -10021.008850076705
1
Iteration 4900: Loss = -10021.002798204734
2
Iteration 5000: Loss = -10021.002513473524
Iteration 5100: Loss = -10021.0025340501
Iteration 5200: Loss = -10021.002764539235
1
Iteration 5300: Loss = -10021.002647630346
2
Iteration 5400: Loss = -10021.003902345545
3
Iteration 5500: Loss = -10021.002548896968
Iteration 5600: Loss = -10021.002515462387
Iteration 5700: Loss = -10021.003444750448
1
Iteration 5800: Loss = -10021.002559581493
Iteration 5900: Loss = -10021.002620117437
Iteration 6000: Loss = -10021.003118690434
1
Iteration 6100: Loss = -10021.007840217539
2
Iteration 6200: Loss = -10021.004000679319
3
Iteration 6300: Loss = -10021.014765767843
4
Iteration 6400: Loss = -10021.002545636447
Iteration 6500: Loss = -10021.002538817102
Iteration 6600: Loss = -10021.002583601668
Iteration 6700: Loss = -10021.00372701774
1
Iteration 6800: Loss = -10021.008085096648
2
Iteration 6900: Loss = -10021.00260517906
Iteration 7000: Loss = -10021.00266614673
Iteration 7100: Loss = -10021.002536949843
Iteration 7200: Loss = -10021.002543273076
Iteration 7300: Loss = -10021.002576588513
Iteration 7400: Loss = -10021.002736177054
1
Iteration 7500: Loss = -10021.002744931619
2
Iteration 7600: Loss = -10021.002547385615
Iteration 7700: Loss = -10021.002602760336
Iteration 7800: Loss = -10021.002548337154
Iteration 7900: Loss = -10021.011268026163
1
Iteration 8000: Loss = -10021.002508740092
Iteration 8100: Loss = -10021.003359681368
1
Iteration 8200: Loss = -10021.002503839114
Iteration 8300: Loss = -10021.002781355473
1
Iteration 8400: Loss = -10021.00251207337
Iteration 8500: Loss = -10021.004153046999
1
Iteration 8600: Loss = -10021.002503226995
Iteration 8700: Loss = -10021.005165621922
1
Iteration 8800: Loss = -10021.00251685273
Iteration 8900: Loss = -10021.006645147863
1
Iteration 9000: Loss = -10021.002564299462
Iteration 9100: Loss = -10021.00272610402
1
Iteration 9200: Loss = -10021.011356787576
2
Iteration 9300: Loss = -10021.004712235299
3
Iteration 9400: Loss = -10021.004743872876
4
Iteration 9500: Loss = -10021.002547298562
Iteration 9600: Loss = -10021.02456195889
1
Iteration 9700: Loss = -10021.002899135925
2
Iteration 9800: Loss = -10021.002529069721
Iteration 9900: Loss = -10021.00273422933
1
Iteration 10000: Loss = -10021.002538688279
Iteration 10100: Loss = -10021.006475709966
1
Iteration 10200: Loss = -10021.015995807531
2
Iteration 10300: Loss = -10021.00256724825
Iteration 10400: Loss = -10021.002492123229
Iteration 10500: Loss = -10021.04954621525
1
Iteration 10600: Loss = -10021.002538044064
Iteration 10700: Loss = -10021.00365745139
1
Iteration 10800: Loss = -10021.00256156479
Iteration 10900: Loss = -10021.00301573052
1
Iteration 11000: Loss = -10021.002510377586
Iteration 11100: Loss = -10021.002734961448
1
Iteration 11200: Loss = -10021.0025385826
Iteration 11300: Loss = -10021.002575148517
Iteration 11400: Loss = -10021.002525642669
Iteration 11500: Loss = -10021.002845078105
1
Iteration 11600: Loss = -10021.002530460799
Iteration 11700: Loss = -10021.018320873896
1
Iteration 11800: Loss = -10021.002542035554
Iteration 11900: Loss = -10021.00666996408
1
Iteration 12000: Loss = -10021.002539372916
Iteration 12100: Loss = -10021.002782148385
1
Iteration 12200: Loss = -10021.002512194602
Iteration 12300: Loss = -10021.0027302103
1
Iteration 12400: Loss = -10021.074431162324
2
Iteration 12500: Loss = -10021.002525792952
Iteration 12600: Loss = -10021.004812742409
1
Iteration 12700: Loss = -10021.002553838327
Iteration 12800: Loss = -10021.002570613666
Iteration 12900: Loss = -10021.129969521182
1
Iteration 13000: Loss = -10021.002534426216
Iteration 13100: Loss = -10021.038170782194
1
Iteration 13200: Loss = -10021.002517193865
Iteration 13300: Loss = -10021.002924235336
1
Iteration 13400: Loss = -10021.002664844185
2
Iteration 13500: Loss = -10021.002566696658
Iteration 13600: Loss = -10021.00260669318
Iteration 13700: Loss = -10021.002529586998
Iteration 13800: Loss = -10021.011736911369
1
Iteration 13900: Loss = -10021.002509088066
Iteration 14000: Loss = -10021.036107699529
1
Iteration 14100: Loss = -10021.002534517138
Iteration 14200: Loss = -10021.02594147792
1
Iteration 14300: Loss = -10021.00252196055
Iteration 14400: Loss = -10021.005026390054
1
Iteration 14500: Loss = -10021.00253718022
Iteration 14600: Loss = -10021.003059802313
1
Iteration 14700: Loss = -10021.002521375796
Iteration 14800: Loss = -10021.002596299792
Iteration 14900: Loss = -10021.002536925924
Iteration 15000: Loss = -10021.003213050944
1
Iteration 15100: Loss = -10021.002487682226
Iteration 15200: Loss = -10021.00283991277
1
Iteration 15300: Loss = -10021.05561505127
2
Iteration 15400: Loss = -10021.002534640387
Iteration 15500: Loss = -10021.031670096127
1
Iteration 15600: Loss = -10021.002524870803
Iteration 15700: Loss = -10021.002495495446
Iteration 15800: Loss = -10021.002827666938
1
Iteration 15900: Loss = -10021.002501672341
Iteration 16000: Loss = -10021.00490560176
1
Iteration 16100: Loss = -10021.002563811797
Iteration 16200: Loss = -10021.151860355278
1
Iteration 16300: Loss = -10021.002525118594
Iteration 16400: Loss = -10021.009274993583
1
Iteration 16500: Loss = -10021.002532436538
Iteration 16600: Loss = -10021.007243613902
1
Iteration 16700: Loss = -10021.002550028408
Iteration 16800: Loss = -10021.06155148216
1
Iteration 16900: Loss = -10021.002528520086
Iteration 17000: Loss = -10021.002563063115
Iteration 17100: Loss = -10021.002573813146
Iteration 17200: Loss = -10021.004095448736
1
Iteration 17300: Loss = -10021.00254672885
Iteration 17400: Loss = -10021.002688027176
1
Iteration 17500: Loss = -10021.04622878775
2
Iteration 17600: Loss = -10021.00252942578
Iteration 17700: Loss = -10021.004131089181
1
Iteration 17800: Loss = -10021.002871558778
2
Iteration 17900: Loss = -10021.002522469169
Iteration 18000: Loss = -10021.034986961358
1
Iteration 18100: Loss = -10021.002522495564
Iteration 18200: Loss = -10021.003226903615
1
Iteration 18300: Loss = -10021.017240937384
2
Iteration 18400: Loss = -10021.00251582826
Iteration 18500: Loss = -10021.002539642064
Iteration 18600: Loss = -10021.035154286126
1
Iteration 18700: Loss = -10021.002522595625
Iteration 18800: Loss = -10021.002970754322
1
Iteration 18900: Loss = -10021.00269028624
2
Iteration 19000: Loss = -10021.002576801808
Iteration 19100: Loss = -10021.003249733667
1
Iteration 19200: Loss = -10021.252976538924
2
Iteration 19300: Loss = -10021.002516234092
Iteration 19400: Loss = -10021.011038508415
1
Iteration 19500: Loss = -10021.002516733157
Iteration 19600: Loss = -10021.002837588965
1
Iteration 19700: Loss = -10021.177983113592
2
Iteration 19800: Loss = -10021.002524222744
Iteration 19900: Loss = -10021.058892489142
1
pi: tensor([[0.9604, 0.0396],
        [0.2275, 0.7725]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6793, 0.3207], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1265, 0.1183],
         [0.6782, 0.2537]],

        [[0.6816, 0.1385],
         [0.5758, 0.6249]],

        [[0.6454, 0.1456],
         [0.6887, 0.6735]],

        [[0.6087, 0.1573],
         [0.7032, 0.5744]],

        [[0.5152, 0.1614],
         [0.5472, 0.5847]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 70
Adjusted Rand Index: 0.1528117359413203
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 69
Adjusted Rand Index: 0.13639707015851624
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 65
Adjusted Rand Index: 0.08190877860298984
time is 3
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.030303030303030304
time is 4
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.013296098272453878
Global Adjusted Rand Index: 0.07837845189157505
Average Adjusted Rand Index: 0.08294334265566211
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23484.43447004367
Iteration 100: Loss = -10038.528951873981
Iteration 200: Loss = -10037.198097924787
Iteration 300: Loss = -10036.217265874488
Iteration 400: Loss = -10035.826461733917
Iteration 500: Loss = -10035.594026911322
Iteration 600: Loss = -10035.392855685674
Iteration 700: Loss = -10035.109708436468
Iteration 800: Loss = -10032.19335332448
Iteration 900: Loss = -10031.20895305375
Iteration 1000: Loss = -10030.7842982706
Iteration 1100: Loss = -10030.131751022638
Iteration 1200: Loss = -10029.4532984428
Iteration 1300: Loss = -10028.93691004744
Iteration 1400: Loss = -10028.534103962334
Iteration 1500: Loss = -10028.283764797377
Iteration 1600: Loss = -10027.249397364423
Iteration 1700: Loss = -10021.64501791327
Iteration 1800: Loss = -10021.188774953149
Iteration 1900: Loss = -10021.12464089889
Iteration 2000: Loss = -10021.067079006081
Iteration 2100: Loss = -10021.053044239257
Iteration 2200: Loss = -10021.042187798996
Iteration 2300: Loss = -10021.034521075764
Iteration 2400: Loss = -10021.028316845692
Iteration 2500: Loss = -10021.023676466904
Iteration 2600: Loss = -10021.019568861126
Iteration 2700: Loss = -10021.014901609708
Iteration 2800: Loss = -10021.01209128274
Iteration 2900: Loss = -10021.009180765204
Iteration 3000: Loss = -10021.008444989913
Iteration 3100: Loss = -10021.006887417077
Iteration 3200: Loss = -10021.005302739068
Iteration 3300: Loss = -10021.00583732778
1
Iteration 3400: Loss = -10021.00476527326
Iteration 3500: Loss = -10021.00437867932
Iteration 3600: Loss = -10021.004040987056
Iteration 3700: Loss = -10021.003756099846
Iteration 3800: Loss = -10021.011249860687
1
Iteration 3900: Loss = -10021.003508270136
Iteration 4000: Loss = -10021.00336842861
Iteration 4100: Loss = -10021.003430887311
Iteration 4200: Loss = -10021.002823208177
Iteration 4300: Loss = -10021.002706928826
Iteration 4400: Loss = -10021.002656336655
Iteration 4500: Loss = -10021.002577644798
Iteration 4600: Loss = -10021.002594125308
Iteration 4700: Loss = -10021.002537366783
Iteration 4800: Loss = -10021.002844948223
1
Iteration 4900: Loss = -10021.002529014551
Iteration 5000: Loss = -10021.004877238427
1
Iteration 5100: Loss = -10021.007819952414
2
Iteration 5200: Loss = -10021.002894992063
3
Iteration 5300: Loss = -10021.005435805426
4
Iteration 5400: Loss = -10021.002708511533
5
Iteration 5500: Loss = -10021.00309193217
6
Iteration 5600: Loss = -10021.002613869005
Iteration 5700: Loss = -10021.004945395509
1
Iteration 5800: Loss = -10021.002538731556
Iteration 5900: Loss = -10021.00255769628
Iteration 6000: Loss = -10021.002651691946
Iteration 6100: Loss = -10021.002547026068
Iteration 6200: Loss = -10021.00253454998
Iteration 6300: Loss = -10021.003656297906
1
Iteration 6400: Loss = -10021.003058872397
2
Iteration 6500: Loss = -10021.00385645975
3
Iteration 6600: Loss = -10021.008183781714
4
Iteration 6700: Loss = -10021.003272496779
5
Iteration 6800: Loss = -10021.002679863199
6
Iteration 6900: Loss = -10021.002762029528
7
Iteration 7000: Loss = -10021.002707537657
8
Iteration 7100: Loss = -10021.002733220595
9
Iteration 7200: Loss = -10021.004517086709
10
Iteration 7300: Loss = -10021.002654728863
11
Iteration 7400: Loss = -10021.003967573233
12
Iteration 7500: Loss = -10021.00450286561
13
Iteration 7600: Loss = -10021.004607215484
14
Iteration 7700: Loss = -10021.007634298414
15
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7747, 0.2253],
        [0.0392, 0.9608]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3194, 0.6806], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2537, 0.1183],
         [0.7167, 0.1265]],

        [[0.5915, 0.1385],
         [0.6135, 0.6372]],

        [[0.5125, 0.1456],
         [0.7086, 0.6803]],

        [[0.6298, 0.1573],
         [0.6176, 0.6986]],

        [[0.6804, 0.1613],
         [0.5155, 0.5776]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 30
Adjusted Rand Index: 0.1528117359413203
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 31
Adjusted Rand Index: 0.13639707015851624
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.0702861335289802
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.030303030303030304
time is 4
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.013296098272453878
Global Adjusted Rand Index: 0.07611037711484671
Average Adjusted Rand Index: 0.08061881364086018
10139.155066951182
[0.07837845189157505, 0.07611037711484671] [0.08294334265566211, 0.08061881364086018] [10021.002536849415, 10021.007634298414]
-------------------------------------
This iteration is 29
True Objective function: Loss = -10027.401037558191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23161.62981539483
Iteration 100: Loss = -9893.363128238081
Iteration 200: Loss = -9891.972284704325
Iteration 300: Loss = -9891.45982103944
Iteration 400: Loss = -9891.125381267339
Iteration 500: Loss = -9890.819364643463
Iteration 600: Loss = -9890.441475557749
Iteration 700: Loss = -9889.331100536867
Iteration 800: Loss = -9888.568258051675
Iteration 900: Loss = -9888.26374857022
Iteration 1000: Loss = -9888.131410045115
Iteration 1100: Loss = -9888.039689106019
Iteration 1200: Loss = -9887.942364576087
Iteration 1300: Loss = -9887.890029972566
Iteration 1400: Loss = -9887.856011542448
Iteration 1500: Loss = -9887.828642190918
Iteration 1600: Loss = -9887.70943544696
Iteration 1700: Loss = -9887.67943031004
Iteration 1800: Loss = -9887.653334566594
Iteration 1900: Loss = -9887.63583022963
Iteration 2000: Loss = -9887.62124162228
Iteration 2100: Loss = -9887.60697768662
Iteration 2200: Loss = -9887.595289491863
Iteration 2300: Loss = -9887.55167718928
Iteration 2400: Loss = -9887.544010462037
Iteration 2500: Loss = -9887.534933977411
Iteration 2600: Loss = -9887.527357976982
Iteration 2700: Loss = -9887.520546836759
Iteration 2800: Loss = -9887.5147429489
Iteration 2900: Loss = -9887.509717554072
Iteration 3000: Loss = -9887.504179081247
Iteration 3100: Loss = -9887.497964052463
Iteration 3200: Loss = -9887.492554837143
Iteration 3300: Loss = -9887.490430642985
Iteration 3400: Loss = -9887.488497533228
Iteration 3500: Loss = -9887.486572620837
Iteration 3600: Loss = -9887.484754616558
Iteration 3700: Loss = -9887.48179799522
Iteration 3800: Loss = -9887.477686912818
Iteration 3900: Loss = -9887.474284709075
Iteration 4000: Loss = -9887.471351416236
Iteration 4100: Loss = -9887.468971565648
Iteration 4200: Loss = -9887.467350285626
Iteration 4300: Loss = -9887.46632662346
Iteration 4400: Loss = -9887.465380121348
Iteration 4500: Loss = -9887.463862193605
Iteration 4600: Loss = -9887.4617714328
Iteration 4700: Loss = -9887.460257943056
Iteration 4800: Loss = -9887.459731687633
Iteration 4900: Loss = -9887.45929016942
Iteration 5000: Loss = -9887.458892205868
Iteration 5100: Loss = -9887.458466354035
Iteration 5200: Loss = -9887.458025334594
Iteration 5300: Loss = -9887.45698103459
Iteration 5400: Loss = -9887.456378888786
Iteration 5500: Loss = -9887.455917202196
Iteration 5600: Loss = -9887.45550204352
Iteration 5700: Loss = -9887.455012128947
Iteration 5800: Loss = -9887.4543001348
Iteration 5900: Loss = -9887.453565355227
Iteration 6000: Loss = -9887.451967238587
Iteration 6100: Loss = -9887.45104932819
Iteration 6200: Loss = -9887.450257095756
Iteration 6300: Loss = -9887.449468506118
Iteration 6400: Loss = -9887.449140158962
Iteration 6500: Loss = -9887.448426132143
Iteration 6600: Loss = -9887.445796398706
Iteration 6700: Loss = -9887.440654649481
Iteration 6800: Loss = -9887.440218910506
Iteration 6900: Loss = -9887.439869665412
Iteration 7000: Loss = -9887.439241743308
Iteration 7100: Loss = -9887.409302713051
Iteration 7200: Loss = -9887.40502539637
Iteration 7300: Loss = -9887.404355710425
Iteration 7400: Loss = -9887.403452382963
Iteration 7500: Loss = -9887.403279846812
Iteration 7600: Loss = -9887.401897053103
Iteration 7700: Loss = -9887.401422902996
Iteration 7800: Loss = -9887.40099881866
Iteration 7900: Loss = -9887.400388022916
Iteration 8000: Loss = -9887.400185009497
Iteration 8100: Loss = -9887.399167150652
Iteration 8200: Loss = -9887.394921779503
Iteration 8300: Loss = -9887.394812911756
Iteration 8400: Loss = -9887.394751735108
Iteration 8500: Loss = -9887.394313806606
Iteration 8600: Loss = -9887.391826463592
Iteration 8700: Loss = -9887.389852022645
Iteration 8800: Loss = -9887.390779551473
1
Iteration 8900: Loss = -9887.389781007725
Iteration 9000: Loss = -9887.38944400965
Iteration 9100: Loss = -9887.539909206625
1
Iteration 9200: Loss = -9887.389151083957
Iteration 9300: Loss = -9887.38716643722
Iteration 9400: Loss = -9887.382540317489
Iteration 9500: Loss = -9887.332946984463
Iteration 9600: Loss = -9887.330164800629
Iteration 9700: Loss = -9887.326985712896
Iteration 9800: Loss = -9887.324180395384
Iteration 9900: Loss = -9887.323795021553
Iteration 10000: Loss = -9887.32339636279
Iteration 10100: Loss = -9887.320990356742
Iteration 10200: Loss = -9887.320883872577
Iteration 10300: Loss = -9887.68125743233
1
Iteration 10400: Loss = -9887.319390580322
Iteration 10500: Loss = -9887.31717237982
Iteration 10600: Loss = -9887.332868246922
1
Iteration 10700: Loss = -9887.312457081445
Iteration 10800: Loss = -9887.311337203419
Iteration 10900: Loss = -9887.405773298473
1
Iteration 11000: Loss = -9887.305163567446
Iteration 11100: Loss = -9887.304749122488
Iteration 11200: Loss = -9887.304473057322
Iteration 11300: Loss = -9887.30476658086
1
Iteration 11400: Loss = -9887.304241558671
Iteration 11500: Loss = -9887.304144967024
Iteration 11600: Loss = -9887.304405982135
1
Iteration 11700: Loss = -9887.303752949314
Iteration 11800: Loss = -9887.303852414649
Iteration 11900: Loss = -9887.3030451439
Iteration 12000: Loss = -9887.302406480727
Iteration 12100: Loss = -9887.302509876463
1
Iteration 12200: Loss = -9887.302202981991
Iteration 12300: Loss = -9887.302167981634
Iteration 12400: Loss = -9887.302728889772
1
Iteration 12500: Loss = -9887.302158462657
Iteration 12600: Loss = -9887.301286886812
Iteration 12700: Loss = -9887.302213916466
1
Iteration 12800: Loss = -9887.300975190403
Iteration 12900: Loss = -9887.30084824942
Iteration 13000: Loss = -9887.301988266487
1
Iteration 13100: Loss = -9887.300765113196
Iteration 13200: Loss = -9887.30074141428
Iteration 13300: Loss = -9887.324173620827
1
Iteration 13400: Loss = -9887.300675373599
Iteration 13500: Loss = -9887.300630060452
Iteration 13600: Loss = -9887.310010444511
1
Iteration 13700: Loss = -9887.296365093644
Iteration 13800: Loss = -9887.295342347825
Iteration 13900: Loss = -9887.299318900563
1
Iteration 14000: Loss = -9887.295169065634
Iteration 14100: Loss = -9887.2929219036
Iteration 14200: Loss = -9887.293337255385
1
Iteration 14300: Loss = -9887.292529191633
Iteration 14400: Loss = -9887.292479549991
Iteration 14500: Loss = -9887.291807663596
Iteration 14600: Loss = -9887.291180469836
Iteration 14700: Loss = -9887.290769058745
Iteration 14800: Loss = -9887.290706008082
Iteration 14900: Loss = -9887.29052470154
Iteration 15000: Loss = -9887.509752323738
1
Iteration 15100: Loss = -9887.290422103031
Iteration 15200: Loss = -9887.290091024435
Iteration 15300: Loss = -9887.293053912921
1
Iteration 15400: Loss = -9887.290057772056
Iteration 15500: Loss = -9887.600287426954
1
Iteration 15600: Loss = -9887.290074908939
Iteration 15700: Loss = -9887.29000123882
Iteration 15800: Loss = -9887.292392487401
1
Iteration 15900: Loss = -9887.289955702738
Iteration 16000: Loss = -9887.28995223391
Iteration 16100: Loss = -9887.290483905077
1
Iteration 16200: Loss = -9887.289994228677
Iteration 16300: Loss = -9887.303084741854
1
Iteration 16400: Loss = -9887.28834255439
Iteration 16500: Loss = -9887.294164117022
1
Iteration 16600: Loss = -9887.286630781768
Iteration 16700: Loss = -9887.286495810496
Iteration 16800: Loss = -9887.286396896934
Iteration 16900: Loss = -9887.286186154965
Iteration 17000: Loss = -9887.28671482166
1
Iteration 17100: Loss = -9887.286028881023
Iteration 17200: Loss = -9887.286870165286
1
Iteration 17300: Loss = -9887.285978366092
Iteration 17400: Loss = -9887.287038128803
1
Iteration 17500: Loss = -9887.285947893328
Iteration 17600: Loss = -9887.285923864734
Iteration 17700: Loss = -9887.360730270442
1
Iteration 17800: Loss = -9887.285950113142
Iteration 17900: Loss = -9887.285996228149
Iteration 18000: Loss = -9887.286386512807
1
Iteration 18100: Loss = -9887.286312188156
2
Iteration 18200: Loss = -9887.286165559133
3
Iteration 18300: Loss = -9887.29614612328
4
Iteration 18400: Loss = -9887.285846399182
Iteration 18500: Loss = -9887.290919325531
1
Iteration 18600: Loss = -9887.285862162016
Iteration 18700: Loss = -9887.28602473361
1
Iteration 18800: Loss = -9887.285892178526
Iteration 18900: Loss = -9887.28583011328
Iteration 19000: Loss = -9887.285867810071
Iteration 19100: Loss = -9887.285930482934
Iteration 19200: Loss = -9887.287299100553
1
Iteration 19300: Loss = -9887.28583207903
Iteration 19400: Loss = -9887.305236910808
1
Iteration 19500: Loss = -9887.285849209855
Iteration 19600: Loss = -9887.285826644309
Iteration 19700: Loss = -9887.285910073313
Iteration 19800: Loss = -9887.285916716064
Iteration 19900: Loss = -9887.285809017596
pi: tensor([[1.0000e+00, 2.9627e-07],
        [1.5373e-02, 9.8463e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0179, 0.9821], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2846, 0.1490],
         [0.6806, 0.1332]],

        [[0.7120, 0.2095],
         [0.6134, 0.5821]],

        [[0.6204, 0.1523],
         [0.5392, 0.6392]],

        [[0.6823, 0.1641],
         [0.5690, 0.7293]],

        [[0.5389, 0.1886],
         [0.6822, 0.5284]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.008125898971292667
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.015076275178066338
Global Adjusted Rand Index: 0.006533713624604089
Average Adjusted Rand Index: 0.005377426612698991
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23435.542657738264
Iteration 100: Loss = -9893.07660083218
Iteration 200: Loss = -9892.129693934117
Iteration 300: Loss = -9891.95160900268
Iteration 400: Loss = -9891.769666219514
Iteration 500: Loss = -9891.477995239773
Iteration 600: Loss = -9890.92157260417
Iteration 700: Loss = -9889.459392296018
Iteration 800: Loss = -9888.62392802961
Iteration 900: Loss = -9888.009355281512
Iteration 1000: Loss = -9887.716457417622
Iteration 1100: Loss = -9887.595747297748
Iteration 1200: Loss = -9887.530127438233
Iteration 1300: Loss = -9887.48648947392
Iteration 1400: Loss = -9887.453718591745
Iteration 1500: Loss = -9887.427219476804
Iteration 1600: Loss = -9887.405133386468
Iteration 1700: Loss = -9887.38647375095
Iteration 1800: Loss = -9887.370919645795
Iteration 1900: Loss = -9887.358038242444
Iteration 2000: Loss = -9887.347455157491
Iteration 2100: Loss = -9887.338717058925
Iteration 2200: Loss = -9887.3315681842
Iteration 2300: Loss = -9887.325732007588
Iteration 2400: Loss = -9887.320878046152
Iteration 2500: Loss = -9887.31687342174
Iteration 2600: Loss = -9887.313483512244
Iteration 2700: Loss = -9887.310687674797
Iteration 2800: Loss = -9887.308277531243
Iteration 2900: Loss = -9887.306189704297
Iteration 3000: Loss = -9887.304392406188
Iteration 3100: Loss = -9887.302807515878
Iteration 3200: Loss = -9887.301405525342
Iteration 3300: Loss = -9887.300215768344
Iteration 3400: Loss = -9887.299074997123
Iteration 3500: Loss = -9887.298106227849
Iteration 3600: Loss = -9887.2971836312
Iteration 3700: Loss = -9887.296361447174
Iteration 3800: Loss = -9887.295620603303
Iteration 3900: Loss = -9887.294942486513
Iteration 4000: Loss = -9887.294303432098
Iteration 4100: Loss = -9887.293758768255
Iteration 4200: Loss = -9887.293200723383
Iteration 4300: Loss = -9887.292730503914
Iteration 4400: Loss = -9887.292280524538
Iteration 4500: Loss = -9887.291863607285
Iteration 4600: Loss = -9887.29146685483
Iteration 4700: Loss = -9887.291086682291
Iteration 4800: Loss = -9887.290761870318
Iteration 4900: Loss = -9887.290478925224
Iteration 5000: Loss = -9887.290133015065
Iteration 5100: Loss = -9887.289826852935
Iteration 5200: Loss = -9887.289551730137
Iteration 5300: Loss = -9887.289298683314
Iteration 5400: Loss = -9887.289021781078
Iteration 5500: Loss = -9887.288822536002
Iteration 5600: Loss = -9887.288616836435
Iteration 5700: Loss = -9887.288417928316
Iteration 5800: Loss = -9887.288223662354
Iteration 5900: Loss = -9887.288044163846
Iteration 6000: Loss = -9887.287867352816
Iteration 6100: Loss = -9887.28773529692
Iteration 6200: Loss = -9887.287583174812
Iteration 6300: Loss = -9887.287423941565
Iteration 6400: Loss = -9887.287427033621
Iteration 6500: Loss = -9887.28716194804
Iteration 6600: Loss = -9887.287514449896
1
Iteration 6700: Loss = -9887.286931161538
Iteration 6800: Loss = -9887.286858839901
Iteration 6900: Loss = -9887.286748102246
Iteration 7000: Loss = -9887.28724944295
1
Iteration 7100: Loss = -9887.286581338021
Iteration 7200: Loss = -9887.286620613939
Iteration 7300: Loss = -9887.28645104296
Iteration 7400: Loss = -9887.28799798056
1
Iteration 7500: Loss = -9887.286300316275
Iteration 7600: Loss = -9887.286321111751
Iteration 7700: Loss = -9887.286186872812
Iteration 7800: Loss = -9887.286126180092
Iteration 7900: Loss = -9887.286083097284
Iteration 8000: Loss = -9887.285991387926
Iteration 8100: Loss = -9887.285956207354
Iteration 8200: Loss = -9887.285903735396
Iteration 8300: Loss = -9887.285865079502
Iteration 8400: Loss = -9887.285807973407
Iteration 8500: Loss = -9887.28612676184
1
Iteration 8600: Loss = -9887.286859419226
2
Iteration 8700: Loss = -9887.28573208815
Iteration 8800: Loss = -9887.28571110456
Iteration 8900: Loss = -9887.451032011066
1
Iteration 9000: Loss = -9887.285637539693
Iteration 9100: Loss = -9887.294295749069
1
Iteration 9200: Loss = -9887.285591050771
Iteration 9300: Loss = -9887.28552650125
Iteration 9400: Loss = -9887.285528686245
Iteration 9500: Loss = -9887.285497463965
Iteration 9600: Loss = -9887.3582411331
1
Iteration 9700: Loss = -9887.28544828027
Iteration 9800: Loss = -9887.28543841863
Iteration 9900: Loss = -9887.289403414745
1
Iteration 10000: Loss = -9887.285399941355
Iteration 10100: Loss = -9887.285342432766
Iteration 10200: Loss = -9887.285664310422
1
Iteration 10300: Loss = -9887.285387512205
Iteration 10400: Loss = -9887.285290022224
Iteration 10500: Loss = -9887.287242224269
1
Iteration 10600: Loss = -9887.285269740256
Iteration 10700: Loss = -9887.285241163596
Iteration 10800: Loss = -9887.29495108349
1
Iteration 10900: Loss = -9887.285147844183
Iteration 11000: Loss = -9887.285157547547
Iteration 11100: Loss = -9887.285152046152
Iteration 11200: Loss = -9887.286082479763
1
Iteration 11300: Loss = -9887.28514383121
Iteration 11400: Loss = -9887.285114690907
Iteration 11500: Loss = -9887.285135248676
Iteration 11600: Loss = -9887.285080840562
Iteration 11700: Loss = -9887.285090464109
Iteration 11800: Loss = -9887.285357869012
1
Iteration 11900: Loss = -9887.285076598018
Iteration 12000: Loss = -9887.28507577675
Iteration 12100: Loss = -9887.285124414837
Iteration 12200: Loss = -9887.285066814571
Iteration 12300: Loss = -9887.285071957422
Iteration 12400: Loss = -9887.285103560458
Iteration 12500: Loss = -9887.285042679632
Iteration 12600: Loss = -9887.285050009841
Iteration 12700: Loss = -9887.2860482362
1
Iteration 12800: Loss = -9887.285015254853
Iteration 12900: Loss = -9887.285025481184
Iteration 13000: Loss = -9887.285023132717
Iteration 13100: Loss = -9887.285021581854
Iteration 13200: Loss = -9887.285024291155
Iteration 13300: Loss = -9887.288730206801
1
Iteration 13400: Loss = -9887.285016971964
Iteration 13500: Loss = -9887.285035873683
Iteration 13600: Loss = -9887.285336039473
1
Iteration 13700: Loss = -9887.284998837265
Iteration 13800: Loss = -9887.290485063282
1
Iteration 13900: Loss = -9887.284994158908
Iteration 14000: Loss = -9887.284980073906
Iteration 14100: Loss = -9887.293444940624
1
Iteration 14200: Loss = -9887.28499032781
Iteration 14300: Loss = -9887.284983876641
Iteration 14400: Loss = -9887.285031943573
Iteration 14500: Loss = -9887.284968670438
Iteration 14600: Loss = -9887.286252506568
1
Iteration 14700: Loss = -9887.28505789981
Iteration 14800: Loss = -9887.284989675927
Iteration 14900: Loss = -9887.285227643397
1
Iteration 15000: Loss = -9887.284980202752
Iteration 15100: Loss = -9887.298567257816
1
Iteration 15200: Loss = -9887.284965994168
Iteration 15300: Loss = -9887.2854717703
1
Iteration 15400: Loss = -9887.285125975177
2
Iteration 15500: Loss = -9887.28500861036
Iteration 15600: Loss = -9887.285494771726
1
Iteration 15700: Loss = -9887.284989428723
Iteration 15800: Loss = -9887.29824852485
1
Iteration 15900: Loss = -9887.285006398597
Iteration 16000: Loss = -9887.291153997468
1
Iteration 16100: Loss = -9887.284977125038
Iteration 16200: Loss = -9887.28500443242
Iteration 16300: Loss = -9887.287710611137
1
Iteration 16400: Loss = -9887.284980977729
Iteration 16500: Loss = -9887.284992905077
Iteration 16600: Loss = -9887.501690480405
1
Iteration 16700: Loss = -9887.284965263043
Iteration 16800: Loss = -9887.284946127962
Iteration 16900: Loss = -9887.28718670332
1
Iteration 17000: Loss = -9887.284961344834
Iteration 17100: Loss = -9887.286431641525
1
Iteration 17200: Loss = -9887.2850820236
2
Iteration 17300: Loss = -9887.28497662477
Iteration 17400: Loss = -9887.313568111917
1
Iteration 17500: Loss = -9887.284961619873
Iteration 17600: Loss = -9887.284971437071
Iteration 17700: Loss = -9887.286524613182
1
Iteration 17800: Loss = -9887.28496485713
Iteration 17900: Loss = -9887.285235266054
1
Iteration 18000: Loss = -9887.284970242512
Iteration 18100: Loss = -9887.284960662108
Iteration 18200: Loss = -9887.28833576468
1
Iteration 18300: Loss = -9887.463707668354
2
Iteration 18400: Loss = -9887.284993535042
Iteration 18500: Loss = -9887.285452912856
1
Iteration 18600: Loss = -9887.284956686435
Iteration 18700: Loss = -9887.28973254204
1
Iteration 18800: Loss = -9887.284977670075
Iteration 18900: Loss = -9887.307029001555
1
Iteration 19000: Loss = -9887.28497151839
Iteration 19100: Loss = -9887.284974310358
Iteration 19200: Loss = -9887.285358429137
1
Iteration 19300: Loss = -9887.284979289765
Iteration 19400: Loss = -9887.287983286706
1
Iteration 19500: Loss = -9887.284974336262
Iteration 19600: Loss = -9887.284964687267
Iteration 19700: Loss = -9887.298688493007
1
Iteration 19800: Loss = -9887.284946679876
Iteration 19900: Loss = -9887.285841494431
1
pi: tensor([[1.0000e+00, 3.6110e-07],
        [1.5437e-02, 9.8456e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0179, 0.9821], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2842, 0.1477],
         [0.6635, 0.1331]],

        [[0.6611, 0.2091],
         [0.7120, 0.6366]],

        [[0.6107, 0.1518],
         [0.5572, 0.5072]],

        [[0.6674, 0.1637],
         [0.6101, 0.6143]],

        [[0.6678, 0.1884],
         [0.6449, 0.5126]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.008125898971292667
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.015076275178066338
Global Adjusted Rand Index: 0.006533713624604089
Average Adjusted Rand Index: 0.005377426612698991
10027.401037558191
[0.006533713624604089, 0.006533713624604089] [0.005377426612698991, 0.005377426612698991] [9887.286562448691, 9887.286246713209]
-------------------------------------
This iteration is 30
True Objective function: Loss = -9858.563270076182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22265.207600112222
Iteration 100: Loss = -9828.259989012155
Iteration 200: Loss = -9730.583943381893
Iteration 300: Loss = -9727.038252669463
Iteration 400: Loss = -9726.7007705884
Iteration 500: Loss = -9726.538999070868
Iteration 600: Loss = -9726.440637048738
Iteration 700: Loss = -9726.377953724314
Iteration 800: Loss = -9726.334982047902
Iteration 900: Loss = -9726.303724390173
Iteration 1000: Loss = -9726.280056273137
Iteration 1100: Loss = -9726.261500316263
Iteration 1200: Loss = -9726.246175934775
Iteration 1300: Loss = -9726.233516034823
Iteration 1400: Loss = -9726.223090218688
Iteration 1500: Loss = -9726.214211494196
Iteration 1600: Loss = -9726.206170056386
Iteration 1700: Loss = -9726.198669058402
Iteration 1800: Loss = -9726.191653046715
Iteration 1900: Loss = -9726.185003342025
Iteration 2000: Loss = -9726.178348628358
Iteration 2100: Loss = -9726.171238169465
Iteration 2200: Loss = -9726.162887035567
Iteration 2300: Loss = -9726.151743557333
Iteration 2400: Loss = -9726.134017669912
Iteration 2500: Loss = -9726.096655968422
Iteration 2600: Loss = -9726.010530748856
Iteration 2700: Loss = -9725.915083511718
Iteration 2800: Loss = -9725.83273621464
Iteration 2900: Loss = -9725.730943077211
Iteration 3000: Loss = -9725.51888270785
Iteration 3100: Loss = -9724.284306315161
Iteration 3200: Loss = -9724.107480405544
Iteration 3300: Loss = -9724.044980004974
Iteration 3400: Loss = -9724.008641723514
Iteration 3500: Loss = -9723.98440353175
Iteration 3600: Loss = -9723.968089312622
Iteration 3700: Loss = -9723.9547911293
Iteration 3800: Loss = -9723.944933925699
Iteration 3900: Loss = -9723.936790169702
Iteration 4000: Loss = -9723.930704463948
Iteration 4100: Loss = -9723.926087969196
Iteration 4200: Loss = -9723.921774677581
Iteration 4300: Loss = -9723.916728363625
Iteration 4400: Loss = -9723.909959161694
Iteration 4500: Loss = -9723.90457259203
Iteration 4600: Loss = -9723.896853267017
Iteration 4700: Loss = -9723.891989021758
Iteration 4800: Loss = -9723.887750988517
Iteration 4900: Loss = -9723.884929258798
Iteration 5000: Loss = -9723.883152017524
Iteration 5100: Loss = -9723.87995107327
Iteration 5200: Loss = -9723.877432945344
Iteration 5300: Loss = -9723.874248961049
Iteration 5400: Loss = -9723.870659698425
Iteration 5500: Loss = -9723.863911384527
Iteration 5600: Loss = -9723.859407068381
Iteration 5700: Loss = -9723.857456788464
Iteration 5800: Loss = -9723.857138954643
Iteration 5900: Loss = -9723.856846761777
Iteration 6000: Loss = -9723.856804382483
Iteration 6100: Loss = -9723.857022440816
1
Iteration 6200: Loss = -9723.856814124536
Iteration 6300: Loss = -9723.857468762053
1
Iteration 6400: Loss = -9723.856771601702
Iteration 6500: Loss = -9723.857149933563
1
Iteration 6600: Loss = -9723.8567355269
Iteration 6700: Loss = -9723.856904720265
1
Iteration 6800: Loss = -9723.856740825806
Iteration 6900: Loss = -9723.857017999573
1
Iteration 7000: Loss = -9723.856735636402
Iteration 7100: Loss = -9723.85708763487
1
Iteration 7200: Loss = -9723.857062310015
2
Iteration 7300: Loss = -9723.856763495625
Iteration 7400: Loss = -9723.85675473778
Iteration 7500: Loss = -9723.856820691793
Iteration 7600: Loss = -9723.856925485228
1
Iteration 7700: Loss = -9723.856761044395
Iteration 7800: Loss = -9723.856975737133
1
Iteration 7900: Loss = -9723.856743212214
Iteration 8000: Loss = -9723.856772668774
Iteration 8100: Loss = -9723.862716109756
1
Iteration 8200: Loss = -9723.856733744395
Iteration 8300: Loss = -9723.856761445966
Iteration 8400: Loss = -9723.859234061845
1
Iteration 8500: Loss = -9723.856750004257
Iteration 8600: Loss = -9723.856856495458
1
Iteration 8700: Loss = -9723.856745146608
Iteration 8800: Loss = -9723.856799593352
Iteration 8900: Loss = -9723.856735310532
Iteration 9000: Loss = -9723.86126511688
1
Iteration 9100: Loss = -9723.85672835062
Iteration 9200: Loss = -9723.856733171922
Iteration 9300: Loss = -9723.856727940241
Iteration 9400: Loss = -9723.856740795261
Iteration 9500: Loss = -9723.856701571334
Iteration 9600: Loss = -9723.856760459907
Iteration 9700: Loss = -9723.856713429153
Iteration 9800: Loss = -9723.861233501195
1
Iteration 9900: Loss = -9723.885454646748
2
Iteration 10000: Loss = -9723.870411905404
3
Iteration 10100: Loss = -9723.856717940484
Iteration 10200: Loss = -9723.86211221443
1
Iteration 10300: Loss = -9723.856751110603
Iteration 10400: Loss = -9723.871656888328
1
Iteration 10500: Loss = -9723.85674032527
Iteration 10600: Loss = -9723.856731900405
Iteration 10700: Loss = -9723.856761969118
Iteration 10800: Loss = -9723.856738773178
Iteration 10900: Loss = -9723.856722323068
Iteration 11000: Loss = -9723.856829349996
1
Iteration 11100: Loss = -9723.856719525545
Iteration 11200: Loss = -9723.871973239675
1
Iteration 11300: Loss = -9723.85673069343
Iteration 11400: Loss = -9723.85869146152
1
Iteration 11500: Loss = -9723.86679833356
2
Iteration 11600: Loss = -9723.857826579828
3
Iteration 11700: Loss = -9723.857083100704
4
Iteration 11800: Loss = -9723.865595797237
5
Iteration 11900: Loss = -9723.856748314207
Iteration 12000: Loss = -9723.856884487923
1
Iteration 12100: Loss = -9723.88707965472
2
Iteration 12200: Loss = -9723.856733191495
Iteration 12300: Loss = -9723.875900839195
1
Iteration 12400: Loss = -9723.856716165017
Iteration 12500: Loss = -9723.873650140713
1
Iteration 12600: Loss = -9723.864075736328
2
Iteration 12700: Loss = -9723.856780791428
Iteration 12800: Loss = -9723.85679966464
Iteration 12900: Loss = -9723.887913833796
1
Iteration 13000: Loss = -9723.871734805514
2
Iteration 13100: Loss = -9723.880636489212
3
Iteration 13200: Loss = -9723.856770583538
Iteration 13300: Loss = -9723.857391336442
1
Iteration 13400: Loss = -9723.85696715429
2
Iteration 13500: Loss = -9723.856914139244
3
Iteration 13600: Loss = -9723.857032107095
4
Iteration 13700: Loss = -9723.856740958177
Iteration 13800: Loss = -9723.856802569608
Iteration 13900: Loss = -9723.94569901112
1
Iteration 14000: Loss = -9723.856747496511
Iteration 14100: Loss = -9723.863607204199
1
Iteration 14200: Loss = -9723.858715923918
2
Iteration 14300: Loss = -9723.864545954506
3
Iteration 14400: Loss = -9723.959961873952
4
Iteration 14500: Loss = -9723.856929345526
5
Iteration 14600: Loss = -9723.856775101214
Iteration 14700: Loss = -9723.860704507706
1
Iteration 14800: Loss = -9723.85688963882
2
Iteration 14900: Loss = -9723.856754820828
Iteration 15000: Loss = -9723.871340748738
1
Iteration 15100: Loss = -9723.85675012135
Iteration 15200: Loss = -9723.859883652885
1
Iteration 15300: Loss = -9723.915105146136
2
Iteration 15400: Loss = -9723.857262929021
3
Iteration 15500: Loss = -9723.856748186212
Iteration 15600: Loss = -9723.860836715736
1
Iteration 15700: Loss = -9723.85675398956
Iteration 15800: Loss = -9723.856789823785
Iteration 15900: Loss = -9723.860160929491
1
Iteration 16000: Loss = -9723.8806461657
2
Iteration 16100: Loss = -9723.856915956278
3
Iteration 16200: Loss = -9723.856757270913
Iteration 16300: Loss = -9723.86252082481
1
Iteration 16400: Loss = -9723.856741734378
Iteration 16500: Loss = -9723.857469822146
1
Iteration 16600: Loss = -9723.873520599613
2
Iteration 16700: Loss = -9723.983531194654
3
Iteration 16800: Loss = -9723.856945363863
4
Iteration 16900: Loss = -9723.856739086574
Iteration 17000: Loss = -9723.856789433046
Iteration 17100: Loss = -9723.860736662273
1
Iteration 17200: Loss = -9723.856768959724
Iteration 17300: Loss = -9723.872081401541
1
Iteration 17400: Loss = -9723.856846885932
Iteration 17500: Loss = -9723.856857317109
Iteration 17600: Loss = -9723.857675790143
1
Iteration 17700: Loss = -9723.856871022472
Iteration 17800: Loss = -9723.856796790498
Iteration 17900: Loss = -9723.86353720426
1
Iteration 18000: Loss = -9723.85714619933
2
Iteration 18100: Loss = -9723.856791916674
Iteration 18200: Loss = -9723.864375442035
1
Iteration 18300: Loss = -9723.85672799984
Iteration 18400: Loss = -9723.85726332221
1
Iteration 18500: Loss = -9723.879855857094
2
Iteration 18600: Loss = -9723.85780008963
3
Iteration 18700: Loss = -9723.857047560936
4
Iteration 18800: Loss = -9723.856864509946
5
Iteration 18900: Loss = -9723.857135937862
6
Iteration 19000: Loss = -9723.921486046738
7
Iteration 19100: Loss = -9723.856751895686
Iteration 19200: Loss = -9723.857945369533
1
Iteration 19300: Loss = -9723.85967508061
2
Iteration 19400: Loss = -9723.856840284578
Iteration 19500: Loss = -9723.856841494666
Iteration 19600: Loss = -9723.857578057075
1
Iteration 19700: Loss = -9723.856958197539
2
Iteration 19800: Loss = -9723.856785956998
Iteration 19900: Loss = -9723.875065077513
1
pi: tensor([[0.9800, 0.0200],
        [0.9224, 0.0776]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0107, 0.9893], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1335, 0.1461],
         [0.5659, 0.1417]],

        [[0.6829, 0.1161],
         [0.6913, 0.6599]],

        [[0.6911, 0.0952],
         [0.5611, 0.6502]],

        [[0.6776, 0.1128],
         [0.6539, 0.5686]],

        [[0.7006, 0.0635],
         [0.6134, 0.6567]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: -0.0017111993612375532
Average Adjusted Rand Index: -0.00015692302765368048
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23955.354763138308
Iteration 100: Loss = -9729.110337626167
Iteration 200: Loss = -9727.365399375652
Iteration 300: Loss = -9726.653114848494
Iteration 400: Loss = -9726.276069348265
Iteration 500: Loss = -9726.013379109674
Iteration 600: Loss = -9725.804080363807
Iteration 700: Loss = -9725.659648092264
Iteration 800: Loss = -9725.564433078065
Iteration 900: Loss = -9725.488040155597
Iteration 1000: Loss = -9725.417392592552
Iteration 1100: Loss = -9725.347644872061
Iteration 1200: Loss = -9725.27591901191
Iteration 1300: Loss = -9725.200035759046
Iteration 1400: Loss = -9725.11825395919
Iteration 1500: Loss = -9725.028973709985
Iteration 1600: Loss = -9724.93045387711
Iteration 1700: Loss = -9724.820172132664
Iteration 1800: Loss = -9724.694120404232
Iteration 1900: Loss = -9724.548340806237
Iteration 2000: Loss = -9724.383928433597
Iteration 2100: Loss = -9724.211649653009
Iteration 2200: Loss = -9724.042051212891
Iteration 2300: Loss = -9723.882124751035
Iteration 2400: Loss = -9723.737402868062
Iteration 2500: Loss = -9723.612306409555
Iteration 2600: Loss = -9723.511982350052
Iteration 2700: Loss = -9723.43591588154
Iteration 2800: Loss = -9723.380995516443
Iteration 2900: Loss = -9723.344245550144
Iteration 3000: Loss = -9723.320733745632
Iteration 3100: Loss = -9723.305475084213
Iteration 3200: Loss = -9723.2960076078
Iteration 3300: Loss = -9723.290192805642
Iteration 3400: Loss = -9723.286211033972
Iteration 3500: Loss = -9723.283312996775
Iteration 3600: Loss = -9723.281046922299
Iteration 3700: Loss = -9723.279256138017
Iteration 3800: Loss = -9723.277752083082
Iteration 3900: Loss = -9723.276508121784
Iteration 4000: Loss = -9723.275376519494
Iteration 4100: Loss = -9723.274370858548
Iteration 4200: Loss = -9723.273418225019
Iteration 4300: Loss = -9723.272516824478
Iteration 4400: Loss = -9723.271622557131
Iteration 4500: Loss = -9723.270792968733
Iteration 4600: Loss = -9723.269980131034
Iteration 4700: Loss = -9723.269187778698
Iteration 4800: Loss = -9723.268394338333
Iteration 4900: Loss = -9723.269476547848
1
Iteration 5000: Loss = -9723.26687300553
Iteration 5100: Loss = -9723.266133624471
Iteration 5200: Loss = -9723.265485408987
Iteration 5300: Loss = -9723.264624049274
Iteration 5400: Loss = -9723.26397173903
Iteration 5500: Loss = -9723.263210350322
Iteration 5600: Loss = -9723.262480525402
Iteration 5700: Loss = -9723.262408635286
Iteration 5800: Loss = -9723.261057670506
Iteration 5900: Loss = -9723.260364179421
Iteration 6000: Loss = -9723.259610020896
Iteration 6100: Loss = -9723.25889037907
Iteration 6200: Loss = -9723.260012538094
1
Iteration 6300: Loss = -9723.25752055311
Iteration 6400: Loss = -9723.256951548374
Iteration 6500: Loss = -9723.256244659584
Iteration 6600: Loss = -9723.255340235817
Iteration 6700: Loss = -9723.254772762184
Iteration 6800: Loss = -9723.253999483479
Iteration 6900: Loss = -9723.253843318971
Iteration 7000: Loss = -9723.25261084644
Iteration 7100: Loss = -9723.2520153714
Iteration 7200: Loss = -9723.251333910266
Iteration 7300: Loss = -9723.250935354114
Iteration 7400: Loss = -9723.25013120463
Iteration 7500: Loss = -9723.2497529277
Iteration 7600: Loss = -9723.249055551081
Iteration 7700: Loss = -9723.248714843574
Iteration 7800: Loss = -9723.248029894607
Iteration 7900: Loss = -9723.249349942911
1
Iteration 8000: Loss = -9723.24717189594
Iteration 8100: Loss = -9723.246711775853
Iteration 8200: Loss = -9723.24685453373
1
Iteration 8300: Loss = -9723.245974498297
Iteration 8400: Loss = -9723.250816907977
1
Iteration 8500: Loss = -9723.248853493858
2
Iteration 8600: Loss = -9723.244974217407
Iteration 8700: Loss = -9723.244999506915
Iteration 8800: Loss = -9723.246027995312
1
Iteration 8900: Loss = -9723.25286847554
2
Iteration 9000: Loss = -9723.244386939203
Iteration 9100: Loss = -9723.243792939567
Iteration 9200: Loss = -9723.466362010828
1
Iteration 9300: Loss = -9723.2433831668
Iteration 9400: Loss = -9723.243197872107
Iteration 9500: Loss = -9723.248245411622
1
Iteration 9600: Loss = -9723.242879229523
Iteration 9700: Loss = -9723.242731458458
Iteration 9800: Loss = -9723.243446322409
1
Iteration 9900: Loss = -9723.242497264344
Iteration 10000: Loss = -9723.242584977866
Iteration 10100: Loss = -9723.242284797754
Iteration 10200: Loss = -9723.253169299987
1
Iteration 10300: Loss = -9723.242065913579
Iteration 10400: Loss = -9723.242017074237
Iteration 10500: Loss = -9723.254767027354
1
Iteration 10600: Loss = -9723.241967398899
Iteration 10700: Loss = -9723.24235888095
1
Iteration 10800: Loss = -9723.242177413491
2
Iteration 10900: Loss = -9723.24944573903
3
Iteration 11000: Loss = -9723.241617585338
Iteration 11100: Loss = -9723.272354235984
1
Iteration 11200: Loss = -9723.285719940737
2
Iteration 11300: Loss = -9723.24138745157
Iteration 11400: Loss = -9723.241789702326
1
Iteration 11500: Loss = -9723.241413779224
Iteration 11600: Loss = -9723.243385583653
1
Iteration 11700: Loss = -9723.241851360099
2
Iteration 11800: Loss = -9723.241232912751
Iteration 11900: Loss = -9723.241188248492
Iteration 12000: Loss = -9723.241221688442
Iteration 12100: Loss = -9723.28179711849
1
Iteration 12200: Loss = -9723.241066448256
Iteration 12300: Loss = -9723.269852758274
1
Iteration 12400: Loss = -9723.375552024601
2
Iteration 12500: Loss = -9723.241534386712
3
Iteration 12600: Loss = -9723.240953646713
Iteration 12700: Loss = -9723.246801795573
1
Iteration 12800: Loss = -9723.240997315133
Iteration 12900: Loss = -9723.242935661085
1
Iteration 13000: Loss = -9723.319640434582
2
Iteration 13100: Loss = -9723.28886901906
3
Iteration 13200: Loss = -9723.24196115173
4
Iteration 13300: Loss = -9723.240987996534
Iteration 13400: Loss = -9723.246932958029
1
Iteration 13500: Loss = -9723.265945125677
2
Iteration 13600: Loss = -9723.240771971556
Iteration 13700: Loss = -9723.24126512537
1
Iteration 13800: Loss = -9723.25816370914
2
Iteration 13900: Loss = -9723.251290471464
3
Iteration 14000: Loss = -9723.24219395709
4
Iteration 14100: Loss = -9723.241259441897
5
Iteration 14200: Loss = -9723.240771774928
Iteration 14300: Loss = -9723.243946783758
1
Iteration 14400: Loss = -9723.24076585933
Iteration 14500: Loss = -9723.241461295695
1
Iteration 14600: Loss = -9723.240963901026
2
Iteration 14700: Loss = -9723.241118356958
3
Iteration 14800: Loss = -9723.490826011415
4
Iteration 14900: Loss = -9723.240662191489
Iteration 15000: Loss = -9723.263465932961
1
Iteration 15100: Loss = -9723.277191340238
2
Iteration 15200: Loss = -9723.243073438667
3
Iteration 15300: Loss = -9723.240715137454
Iteration 15400: Loss = -9723.2411521633
1
Iteration 15500: Loss = -9723.245378625865
2
Iteration 15600: Loss = -9723.24080154042
Iteration 15700: Loss = -9723.241805331361
1
Iteration 15800: Loss = -9723.240693113408
Iteration 15900: Loss = -9723.24108041109
1
Iteration 16000: Loss = -9723.240620471004
Iteration 16100: Loss = -9723.240914997614
1
Iteration 16200: Loss = -9723.24869171652
2
Iteration 16300: Loss = -9723.242892345066
3
Iteration 16400: Loss = -9723.24941548865
4
Iteration 16500: Loss = -9723.240796216225
5
Iteration 16600: Loss = -9723.247917244573
6
Iteration 16700: Loss = -9723.365239839792
7
Iteration 16800: Loss = -9723.240557437559
Iteration 16900: Loss = -9723.24084442697
1
Iteration 17000: Loss = -9723.3891717509
2
Iteration 17100: Loss = -9723.240785765867
3
Iteration 17200: Loss = -9723.240600950861
Iteration 17300: Loss = -9723.247391163959
1
Iteration 17400: Loss = -9723.253774258968
2
Iteration 17500: Loss = -9723.24061754705
Iteration 17600: Loss = -9723.258258219783
1
Iteration 17700: Loss = -9723.240748229588
2
Iteration 17800: Loss = -9723.240769325848
3
Iteration 17900: Loss = -9723.24133757053
4
Iteration 18000: Loss = -9723.240583290833
Iteration 18100: Loss = -9723.3172236826
1
Iteration 18200: Loss = -9723.241318635555
2
Iteration 18300: Loss = -9723.284054810554
3
Iteration 18400: Loss = -9723.245993608298
4
Iteration 18500: Loss = -9723.242094078034
5
Iteration 18600: Loss = -9723.26917628283
6
Iteration 18700: Loss = -9723.29893937159
7
Iteration 18800: Loss = -9723.240588724946
Iteration 18900: Loss = -9723.241075244818
1
Iteration 19000: Loss = -9723.28714313102
2
Iteration 19100: Loss = -9723.243431611443
3
Iteration 19200: Loss = -9723.240653230427
Iteration 19300: Loss = -9723.240702164188
Iteration 19400: Loss = -9723.240743204604
Iteration 19500: Loss = -9723.240579459114
Iteration 19600: Loss = -9723.24064156706
Iteration 19700: Loss = -9723.281112767956
1
Iteration 19800: Loss = -9723.24052555475
Iteration 19900: Loss = -9723.252906464344
1
pi: tensor([[6.2843e-06, 9.9999e-01],
        [6.1942e-02, 9.3806e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6880, 0.3120], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1486, 0.1382],
         [0.6900, 0.1290]],

        [[0.5668, 0.1123],
         [0.6522, 0.5422]],

        [[0.6108, 0.1500],
         [0.5713, 0.5730]],

        [[0.5193, 0.1371],
         [0.6662, 0.5217]],

        [[0.7240, 0.1867],
         [0.5680, 0.5300]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: -0.0010844293124436077
Average Adjusted Rand Index: 0.000488518314825395
9858.563270076182
[-0.0017111993612375532, -0.0010844293124436077] [-0.00015692302765368048, 0.000488518314825395] [9723.856732458004, 9723.240494674867]
-------------------------------------
This iteration is 31
True Objective function: Loss = -9957.769222144521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24008.924842661545
Iteration 100: Loss = -9830.764921304803
Iteration 200: Loss = -9829.044916488847
Iteration 300: Loss = -9828.620432398786
Iteration 400: Loss = -9828.400964991093
Iteration 500: Loss = -9828.245259677165
Iteration 600: Loss = -9828.11438390094
Iteration 700: Loss = -9828.00098907475
Iteration 800: Loss = -9827.902811719
Iteration 900: Loss = -9827.814705691258
Iteration 1000: Loss = -9827.728937072432
Iteration 1100: Loss = -9827.642723635965
Iteration 1200: Loss = -9827.5685263533
Iteration 1300: Loss = -9827.517010664353
Iteration 1400: Loss = -9827.479963128664
Iteration 1500: Loss = -9827.449869250378
Iteration 1600: Loss = -9827.42522168368
Iteration 1700: Loss = -9827.405736812727
Iteration 1800: Loss = -9827.390511736123
Iteration 1900: Loss = -9827.379295298551
Iteration 2000: Loss = -9827.371075132221
Iteration 2100: Loss = -9827.363094688364
Iteration 2200: Loss = -9827.353412496683
Iteration 2300: Loss = -9827.34068056414
Iteration 2400: Loss = -9827.320599667726
Iteration 2500: Loss = -9827.285775163882
Iteration 2600: Loss = -9827.221575487665
Iteration 2700: Loss = -9827.098686310392
Iteration 2800: Loss = -9826.892735031272
Iteration 2900: Loss = -9826.751763670705
Iteration 3000: Loss = -9826.726328415192
Iteration 3100: Loss = -9826.721659291956
Iteration 3200: Loss = -9826.720424523384
Iteration 3300: Loss = -9826.72049168777
Iteration 3400: Loss = -9826.719655493605
Iteration 3500: Loss = -9826.719431903868
Iteration 3600: Loss = -9826.719201862907
Iteration 3700: Loss = -9826.7189720283
Iteration 3800: Loss = -9826.718804868626
Iteration 3900: Loss = -9826.718547043585
Iteration 4000: Loss = -9826.718274692912
Iteration 4100: Loss = -9826.718148350665
Iteration 4200: Loss = -9826.717655052118
Iteration 4300: Loss = -9826.717272335809
Iteration 4400: Loss = -9826.716717172165
Iteration 4500: Loss = -9826.716037954042
Iteration 4600: Loss = -9826.715608981382
Iteration 4700: Loss = -9826.714045495542
Iteration 4800: Loss = -9826.712420547969
Iteration 4900: Loss = -9826.710054529509
Iteration 5000: Loss = -9826.705260333938
Iteration 5100: Loss = -9826.695075811218
Iteration 5200: Loss = -9826.655926546766
Iteration 5300: Loss = -9826.444820942319
Iteration 5400: Loss = -9826.31843587749
Iteration 5500: Loss = -9826.284435227375
Iteration 5600: Loss = -9826.274646600672
Iteration 5700: Loss = -9826.268641402043
Iteration 5800: Loss = -9826.264079168837
Iteration 5900: Loss = -9826.261822329834
Iteration 6000: Loss = -9826.259684102395
Iteration 6100: Loss = -9826.258082847793
Iteration 6200: Loss = -9826.257150092611
Iteration 6300: Loss = -9826.256347898083
Iteration 6400: Loss = -9826.255497187056
Iteration 6500: Loss = -9826.254476442868
Iteration 6600: Loss = -9826.253987065398
Iteration 6700: Loss = -9826.253604761756
Iteration 6800: Loss = -9826.253341905223
Iteration 6900: Loss = -9826.253069064282
Iteration 7000: Loss = -9826.253057669444
Iteration 7100: Loss = -9826.252922774791
Iteration 7200: Loss = -9826.252438648282
Iteration 7300: Loss = -9826.25356157361
1
Iteration 7400: Loss = -9826.252121624062
Iteration 7500: Loss = -9826.25241723896
1
Iteration 7600: Loss = -9826.251825948038
Iteration 7700: Loss = -9826.251721823006
Iteration 7800: Loss = -9826.251602499462
Iteration 7900: Loss = -9826.251527313858
Iteration 8000: Loss = -9826.251441755516
Iteration 8100: Loss = -9826.251521192029
Iteration 8200: Loss = -9826.251219687485
Iteration 8300: Loss = -9826.25109980105
Iteration 8400: Loss = -9826.251750552861
1
Iteration 8500: Loss = -9826.2510035158
Iteration 8600: Loss = -9826.251914710514
1
Iteration 8700: Loss = -9826.251161495378
2
Iteration 8800: Loss = -9826.250702869225
Iteration 8900: Loss = -9826.25164606986
1
Iteration 9000: Loss = -9826.250117762665
Iteration 9100: Loss = -9826.25173023784
1
Iteration 9200: Loss = -9826.250059479258
Iteration 9300: Loss = -9826.556565663393
1
Iteration 9400: Loss = -9826.249980457495
Iteration 9500: Loss = -9826.250106133772
1
Iteration 9600: Loss = -9826.250840013858
2
Iteration 9700: Loss = -9826.273693182853
3
Iteration 9800: Loss = -9826.257851736053
4
Iteration 9900: Loss = -9826.276276110028
5
Iteration 10000: Loss = -9826.277698457232
6
Iteration 10100: Loss = -9826.267178996577
7
Iteration 10200: Loss = -9826.270643445054
8
Iteration 10300: Loss = -9826.356359572444
9
Iteration 10400: Loss = -9826.265409835854
10
Iteration 10500: Loss = -9826.248833130043
Iteration 10600: Loss = -9826.252065228255
1
Iteration 10700: Loss = -9826.249012403963
2
Iteration 10800: Loss = -9826.248518953582
Iteration 10900: Loss = -9826.249692911852
1
Iteration 11000: Loss = -9826.264409848476
2
Iteration 11100: Loss = -9826.248039051867
Iteration 11200: Loss = -9826.268230945709
1
Iteration 11300: Loss = -9826.247995504986
Iteration 11400: Loss = -9826.248262481718
1
Iteration 11500: Loss = -9826.29068782739
2
Iteration 11600: Loss = -9826.247190700373
Iteration 11700: Loss = -9826.248816818488
1
Iteration 11800: Loss = -9826.247077798102
Iteration 11900: Loss = -9826.242439167978
Iteration 12000: Loss = -9826.422704192515
1
Iteration 12100: Loss = -9826.241995659242
Iteration 12200: Loss = -9826.248256221837
1
Iteration 12300: Loss = -9826.299339561441
2
Iteration 12400: Loss = -9826.251515076407
3
Iteration 12500: Loss = -9826.251109943978
4
Iteration 12600: Loss = -9826.28379165268
5
Iteration 12700: Loss = -9826.240842828987
Iteration 12800: Loss = -9826.24229730584
1
Iteration 12900: Loss = -9826.241308727973
2
Iteration 13000: Loss = -9826.240850187154
Iteration 13100: Loss = -9826.278955566788
1
Iteration 13200: Loss = -9826.24080319972
Iteration 13300: Loss = -9826.242229343805
1
Iteration 13400: Loss = -9826.526888715423
2
Iteration 13500: Loss = -9826.24079767022
Iteration 13600: Loss = -9826.240816388816
Iteration 13700: Loss = -9826.241480873388
1
Iteration 13800: Loss = -9826.284470469715
2
Iteration 13900: Loss = -9826.24093013801
3
Iteration 14000: Loss = -9826.247811422369
4
Iteration 14100: Loss = -9826.262520950357
5
Iteration 14200: Loss = -9826.243206919022
6
Iteration 14300: Loss = -9826.255359467903
7
Iteration 14400: Loss = -9826.256112309065
8
Iteration 14500: Loss = -9826.241929187392
9
Iteration 14600: Loss = -9826.259718180307
10
Iteration 14700: Loss = -9826.2398562789
Iteration 14800: Loss = -9826.23869380353
Iteration 14900: Loss = -9826.244344547336
1
Iteration 15000: Loss = -9826.348614328697
2
Iteration 15100: Loss = -9826.247600756653
3
Iteration 15200: Loss = -9826.23876878261
Iteration 15300: Loss = -9826.239900409246
1
Iteration 15400: Loss = -9826.243344885384
2
Iteration 15500: Loss = -9826.247521758823
3
Iteration 15600: Loss = -9826.238173422
Iteration 15700: Loss = -9826.238375595161
1
Iteration 15800: Loss = -9826.24503699164
2
Iteration 15900: Loss = -9826.241758554717
3
Iteration 16000: Loss = -9826.24366502694
4
Iteration 16100: Loss = -9826.23815412323
Iteration 16200: Loss = -9826.239745624085
1
Iteration 16300: Loss = -9826.249887919348
2
Iteration 16400: Loss = -9826.247487815
3
Iteration 16500: Loss = -9826.26139512713
4
Iteration 16600: Loss = -9826.236375997156
Iteration 16700: Loss = -9826.238363703384
1
Iteration 16800: Loss = -9826.270747249599
2
Iteration 16900: Loss = -9826.27670092328
3
Iteration 17000: Loss = -9826.236406949805
Iteration 17100: Loss = -9826.236492116897
Iteration 17200: Loss = -9826.23664959751
1
Iteration 17300: Loss = -9826.236410746384
Iteration 17400: Loss = -9826.236519354668
1
Iteration 17500: Loss = -9826.23940798124
2
Iteration 17600: Loss = -9826.235128527309
Iteration 17700: Loss = -9826.235149783575
Iteration 17800: Loss = -9826.235376802813
1
Iteration 17900: Loss = -9826.235451726898
2
Iteration 18000: Loss = -9826.297007299878
3
Iteration 18100: Loss = -9826.282953721451
4
Iteration 18200: Loss = -9826.245293690765
5
Iteration 18300: Loss = -9826.235174152296
Iteration 18400: Loss = -9826.235434425995
1
Iteration 18500: Loss = -9826.235647321086
2
Iteration 18600: Loss = -9826.34081832517
3
Iteration 18700: Loss = -9826.254156284052
4
Iteration 18800: Loss = -9826.23478674782
Iteration 18900: Loss = -9826.23482163158
Iteration 19000: Loss = -9826.240527431684
1
Iteration 19100: Loss = -9826.241103542725
2
Iteration 19200: Loss = -9826.234235354497
Iteration 19300: Loss = -9826.233835363038
Iteration 19400: Loss = -9826.23379199923
Iteration 19500: Loss = -9826.235153383757
1
Iteration 19600: Loss = -9826.233680366631
Iteration 19700: Loss = -9826.252475206009
1
Iteration 19800: Loss = -9826.432972393524
2
Iteration 19900: Loss = -9826.233517559702
pi: tensor([[1.0000e+00, 2.3684e-06],
        [6.1266e-02, 9.3873e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0324, 0.9676], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1593, 0.2097],
         [0.6967, 0.1312]],

        [[0.5348, 0.1378],
         [0.5891, 0.6471]],

        [[0.5505, 0.1593],
         [0.7144, 0.7165]],

        [[0.5417, 0.1349],
         [0.5978, 0.6794]],

        [[0.7011, 0.1427],
         [0.5606, 0.5516]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: -0.004063414211148451
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.004400601925053371
Average Adjusted Rand Index: 0.0010484000091984
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21336.016706436803
Iteration 100: Loss = -9829.631735471827
Iteration 200: Loss = -9828.82033314978
Iteration 300: Loss = -9828.520450520467
Iteration 400: Loss = -9828.174966135573
Iteration 500: Loss = -9827.629012297877
Iteration 600: Loss = -9827.559134256415
Iteration 700: Loss = -9827.517084203186
Iteration 800: Loss = -9827.482876868295
Iteration 900: Loss = -9827.45494874295
Iteration 1000: Loss = -9827.432577127523
Iteration 1100: Loss = -9827.415271819887
Iteration 1200: Loss = -9827.402556865065
Iteration 1300: Loss = -9827.393696749801
Iteration 1400: Loss = -9827.387690165106
Iteration 1500: Loss = -9827.383614357745
Iteration 1600: Loss = -9827.380815271033
Iteration 1700: Loss = -9827.378793366726
Iteration 1800: Loss = -9827.377298751675
Iteration 1900: Loss = -9827.376226031934
Iteration 2000: Loss = -9827.375436836734
Iteration 2100: Loss = -9827.37479113395
Iteration 2200: Loss = -9827.374278308593
Iteration 2300: Loss = -9827.373917454093
Iteration 2400: Loss = -9827.373651969478
Iteration 2500: Loss = -9827.373388212178
Iteration 2600: Loss = -9827.373162043681
Iteration 2700: Loss = -9827.372953165484
Iteration 2800: Loss = -9827.37278906547
Iteration 2900: Loss = -9827.372624113199
Iteration 3000: Loss = -9827.372453984564
Iteration 3100: Loss = -9827.372365472456
Iteration 3200: Loss = -9827.372221994849
Iteration 3300: Loss = -9827.372093801421
Iteration 3400: Loss = -9827.371951541129
Iteration 3500: Loss = -9827.371817686666
Iteration 3600: Loss = -9827.371667244957
Iteration 3700: Loss = -9827.371554003259
Iteration 3800: Loss = -9827.371371906245
Iteration 3900: Loss = -9827.371223141481
Iteration 4000: Loss = -9827.371012335949
Iteration 4100: Loss = -9827.370738086709
Iteration 4200: Loss = -9827.370403931523
Iteration 4300: Loss = -9827.369960922837
Iteration 4400: Loss = -9827.369286067795
Iteration 4500: Loss = -9827.368263847427
Iteration 4600: Loss = -9827.36638440897
Iteration 4700: Loss = -9827.36227500273
Iteration 4800: Loss = -9827.34971526986
Iteration 4900: Loss = -9827.237143424909
Iteration 5000: Loss = -9824.457733519284
Iteration 5100: Loss = -9824.289238089239
Iteration 5200: Loss = -9824.2533180061
Iteration 5300: Loss = -9824.234416566289
Iteration 5400: Loss = -9824.222787819666
Iteration 5500: Loss = -9824.177982244499
Iteration 5600: Loss = -9824.174193280178
Iteration 5700: Loss = -9824.17144235297
Iteration 5800: Loss = -9824.169405319763
Iteration 5900: Loss = -9824.16756829154
Iteration 6000: Loss = -9824.16580300304
Iteration 6100: Loss = -9824.164198709268
Iteration 6200: Loss = -9824.162937966998
Iteration 6300: Loss = -9824.16208462869
Iteration 6400: Loss = -9824.161228883162
Iteration 6500: Loss = -9824.160394301953
Iteration 6600: Loss = -9824.16018607334
Iteration 6700: Loss = -9824.158802079117
Iteration 6800: Loss = -9824.158344424328
Iteration 6900: Loss = -9824.157785163168
Iteration 7000: Loss = -9824.157465099828
Iteration 7100: Loss = -9824.158100927158
1
Iteration 7200: Loss = -9824.15688720064
Iteration 7300: Loss = -9824.15806112226
1
Iteration 7400: Loss = -9824.15645100923
Iteration 7500: Loss = -9824.163308472625
1
Iteration 7600: Loss = -9824.156053041515
Iteration 7700: Loss = -9824.15628685447
1
Iteration 7800: Loss = -9824.155738668243
Iteration 7900: Loss = -9824.155560316025
Iteration 8000: Loss = -9824.155410012558
Iteration 8100: Loss = -9824.15526255883
Iteration 8200: Loss = -9824.15528248265
Iteration 8300: Loss = -9824.155036117507
Iteration 8400: Loss = -9824.156908593382
1
Iteration 8500: Loss = -9824.154819666795
Iteration 8600: Loss = -9824.165539904048
1
Iteration 8700: Loss = -9824.17971442041
2
Iteration 8800: Loss = -9824.154571315952
Iteration 8900: Loss = -9824.157222945254
1
Iteration 9000: Loss = -9824.15440316388
Iteration 9100: Loss = -9824.155129638562
1
Iteration 9200: Loss = -9824.15427854144
Iteration 9300: Loss = -9824.154570549374
1
Iteration 9400: Loss = -9824.154177969847
Iteration 9500: Loss = -9824.154150740404
Iteration 9600: Loss = -9824.154108076165
Iteration 9700: Loss = -9824.15403253158
Iteration 9800: Loss = -9824.213055625607
1
Iteration 9900: Loss = -9824.153956118473
Iteration 10000: Loss = -9824.153920034636
Iteration 10100: Loss = -9824.154536955566
1
Iteration 10200: Loss = -9824.15386880058
Iteration 10300: Loss = -9824.159425603391
1
Iteration 10400: Loss = -9824.15382832008
Iteration 10500: Loss = -9824.153783901886
Iteration 10600: Loss = -9824.153955779046
1
Iteration 10700: Loss = -9824.153885978023
2
Iteration 10800: Loss = -9824.15433985267
3
Iteration 10900: Loss = -9824.154064622562
4
Iteration 11000: Loss = -9824.16765496085
5
Iteration 11100: Loss = -9824.1536475396
Iteration 11200: Loss = -9824.154556305832
1
Iteration 11300: Loss = -9824.18706262496
2
Iteration 11400: Loss = -9824.155539576906
3
Iteration 11500: Loss = -9824.158599085495
4
Iteration 11600: Loss = -9824.187114888073
5
Iteration 11700: Loss = -9824.153750273485
6
Iteration 11800: Loss = -9824.153585269834
Iteration 11900: Loss = -9824.15393185947
1
Iteration 12000: Loss = -9824.157065301953
2
Iteration 12100: Loss = -9824.15349990685
Iteration 12200: Loss = -9824.153704361963
1
Iteration 12300: Loss = -9824.15348859392
Iteration 12400: Loss = -9824.172471762136
1
Iteration 12500: Loss = -9824.153452313238
Iteration 12600: Loss = -9824.153450847527
Iteration 12700: Loss = -9824.156077771386
1
Iteration 12800: Loss = -9824.153454066429
Iteration 12900: Loss = -9824.255599573602
1
Iteration 13000: Loss = -9824.153407952508
Iteration 13100: Loss = -9824.153420574654
Iteration 13200: Loss = -9824.154219791972
1
Iteration 13300: Loss = -9824.153407694652
Iteration 13400: Loss = -9824.154785780964
1
Iteration 13500: Loss = -9824.153393826264
Iteration 13600: Loss = -9824.153399166971
Iteration 13700: Loss = -9824.153378680321
Iteration 13800: Loss = -9824.153427719306
Iteration 13900: Loss = -9824.15334723925
Iteration 14000: Loss = -9824.15734490756
1
Iteration 14100: Loss = -9824.153349530507
Iteration 14200: Loss = -9824.153355728808
Iteration 14300: Loss = -9824.153429050255
Iteration 14400: Loss = -9824.154168745843
1
Iteration 14500: Loss = -9824.167845508717
2
Iteration 14600: Loss = -9824.153353972299
Iteration 14700: Loss = -9824.16286463962
1
Iteration 14800: Loss = -9824.153349831284
Iteration 14900: Loss = -9824.159035338267
1
Iteration 15000: Loss = -9824.153318442362
Iteration 15100: Loss = -9824.153331800197
Iteration 15200: Loss = -9824.153429742317
Iteration 15300: Loss = -9824.15332699911
Iteration 15400: Loss = -9824.15330268534
Iteration 15500: Loss = -9824.153508650987
1
Iteration 15600: Loss = -9824.153303924388
Iteration 15700: Loss = -9824.182679876769
1
Iteration 15800: Loss = -9824.153333670301
Iteration 15900: Loss = -9824.15419122179
1
Iteration 16000: Loss = -9824.15330564875
Iteration 16100: Loss = -9824.153554614511
1
Iteration 16200: Loss = -9824.20776745448
2
Iteration 16300: Loss = -9824.153337924225
Iteration 16400: Loss = -9824.15407201195
1
Iteration 16500: Loss = -9824.15332362488
Iteration 16600: Loss = -9824.153303849513
Iteration 16700: Loss = -9824.153339946579
Iteration 16800: Loss = -9824.17978246565
1
Iteration 16900: Loss = -9824.153308198192
Iteration 17000: Loss = -9824.155540465152
1
Iteration 17100: Loss = -9824.153312551616
Iteration 17200: Loss = -9824.15330834554
Iteration 17300: Loss = -9824.153521608887
1
Iteration 17400: Loss = -9824.153386828984
Iteration 17500: Loss = -9824.153386104417
Iteration 17600: Loss = -9824.153295490833
Iteration 17700: Loss = -9824.153524474568
1
Iteration 17800: Loss = -9824.153312146751
Iteration 17900: Loss = -9824.153388068802
Iteration 18000: Loss = -9824.154511912697
1
Iteration 18100: Loss = -9824.19538753282
2
Iteration 18200: Loss = -9824.153314400903
Iteration 18300: Loss = -9824.202489951402
1
Iteration 18400: Loss = -9824.153310481093
Iteration 18500: Loss = -9824.192368711363
1
Iteration 18600: Loss = -9824.153295965529
Iteration 18700: Loss = -9824.153335961239
Iteration 18800: Loss = -9824.153314126583
Iteration 18900: Loss = -9824.153894641106
1
Iteration 19000: Loss = -9824.17241053575
2
Iteration 19100: Loss = -9824.153296533827
Iteration 19200: Loss = -9824.153754497951
1
Iteration 19300: Loss = -9824.217297111196
2
Iteration 19400: Loss = -9824.153296935252
Iteration 19500: Loss = -9824.15422518393
1
Iteration 19600: Loss = -9824.15330253608
Iteration 19700: Loss = -9824.153307238628
Iteration 19800: Loss = -9824.240570476546
1
Iteration 19900: Loss = -9824.153293304125
pi: tensor([[8.6732e-01, 1.3268e-01],
        [3.5692e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1486, 0.8514], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2874, 0.1613],
         [0.5851, 0.1306]],

        [[0.5372, 0.1278],
         [0.7028, 0.6998]],

        [[0.7050, 0.1560],
         [0.5403, 0.6155]],

        [[0.5834, 0.1365],
         [0.6213, 0.6579]],

        [[0.5394, 0.1575],
         [0.7070, 0.7120]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 34
Adjusted Rand Index: 0.09726678538618627
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.027108887462139547
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.052689140504374676
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.02522714359005478
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.044678991113540255
Average Adjusted Rand Index: 0.041347280277439946
9957.769222144521
[0.004400601925053371, 0.044678991113540255] [0.0010484000091984, 0.041347280277439946] [9826.23419311314, 9824.174494628018]
-------------------------------------
This iteration is 32
True Objective function: Loss = -10070.65504642735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19781.56285678603
Iteration 100: Loss = -9947.577767152048
Iteration 200: Loss = -9944.775555694903
Iteration 300: Loss = -9941.207111455034
Iteration 400: Loss = -9939.864781493812
Iteration 500: Loss = -9939.509141090897
Iteration 600: Loss = -9939.292796023012
Iteration 700: Loss = -9939.145486837117
Iteration 800: Loss = -9939.034352118668
Iteration 900: Loss = -9938.945330067885
Iteration 1000: Loss = -9938.873561712036
Iteration 1100: Loss = -9938.817602457177
Iteration 1200: Loss = -9938.776369485036
Iteration 1300: Loss = -9938.746369740293
Iteration 1400: Loss = -9938.724480153456
Iteration 1500: Loss = -9938.708464097368
Iteration 1600: Loss = -9938.696670951946
Iteration 1700: Loss = -9938.687886236989
Iteration 1800: Loss = -9938.68124972865
Iteration 1900: Loss = -9938.676109005557
Iteration 2000: Loss = -9938.672073511942
Iteration 2100: Loss = -9938.668793440354
Iteration 2200: Loss = -9938.6660987497
Iteration 2300: Loss = -9938.663765749345
Iteration 2400: Loss = -9938.661661970384
Iteration 2500: Loss = -9938.659698436471
Iteration 2600: Loss = -9938.65779141875
Iteration 2700: Loss = -9938.655781455258
Iteration 2800: Loss = -9938.656627155126
1
Iteration 2900: Loss = -9938.651015294778
Iteration 3000: Loss = -9938.647522654821
Iteration 3100: Loss = -9938.642342448582
Iteration 3200: Loss = -9938.632679631317
Iteration 3300: Loss = -9938.612433734514
Iteration 3400: Loss = -9938.476649639406
Iteration 3500: Loss = -9935.890703367208
Iteration 3600: Loss = -9934.918080502539
Iteration 3700: Loss = -9934.903468039984
Iteration 3800: Loss = -9934.899390986693
Iteration 3900: Loss = -9934.89814935302
Iteration 4000: Loss = -9934.89764570934
Iteration 4100: Loss = -9934.897333610072
Iteration 4200: Loss = -9934.897466881195
1
Iteration 4300: Loss = -9934.897131503236
Iteration 4400: Loss = -9934.909611941432
1
Iteration 4500: Loss = -9934.897002095337
Iteration 4600: Loss = -9934.89701388278
Iteration 4700: Loss = -9934.897063630566
Iteration 4800: Loss = -9934.896969877462
Iteration 4900: Loss = -9934.897864907674
1
Iteration 5000: Loss = -9934.896930138884
Iteration 5100: Loss = -9934.896956711476
Iteration 5200: Loss = -9934.896912109787
Iteration 5300: Loss = -9934.89692368767
Iteration 5400: Loss = -9934.896928016977
Iteration 5500: Loss = -9934.89691933653
Iteration 5600: Loss = -9934.896904296555
Iteration 5700: Loss = -9934.896977533725
Iteration 5800: Loss = -9934.896892732426
Iteration 5900: Loss = -9934.896914117122
Iteration 6000: Loss = -9934.897994586492
1
Iteration 6100: Loss = -9934.896908821609
Iteration 6200: Loss = -9934.896920919873
Iteration 6300: Loss = -9934.897606880326
1
Iteration 6400: Loss = -9934.896914670806
Iteration 6500: Loss = -9934.896921048974
Iteration 6600: Loss = -9934.896977993016
Iteration 6700: Loss = -9934.89714147173
1
Iteration 6800: Loss = -9934.896906617452
Iteration 6900: Loss = -9934.89734987627
1
Iteration 7000: Loss = -9934.896928042564
Iteration 7100: Loss = -9934.89711010911
1
Iteration 7200: Loss = -9934.896902924327
Iteration 7300: Loss = -9934.9131754531
1
Iteration 7400: Loss = -9934.89692012916
Iteration 7500: Loss = -9934.897148343822
1
Iteration 7600: Loss = -9934.89690483019
Iteration 7700: Loss = -9934.897165480523
1
Iteration 7800: Loss = -9935.057965335573
2
Iteration 7900: Loss = -9934.896914848072
Iteration 8000: Loss = -9934.89902273765
1
Iteration 8100: Loss = -9935.02825306618
2
Iteration 8200: Loss = -9934.897109003166
3
Iteration 8300: Loss = -9934.90671409017
4
Iteration 8400: Loss = -9934.897080575982
5
Iteration 8500: Loss = -9934.897551867894
6
Iteration 8600: Loss = -9934.898128317547
7
Iteration 8700: Loss = -9934.897225609016
8
Iteration 8800: Loss = -9934.897866926016
9
Iteration 8900: Loss = -9934.899314474449
10
Iteration 9000: Loss = -9934.901986074283
11
Iteration 9100: Loss = -9934.897407293955
12
Iteration 9200: Loss = -9934.897463214278
13
Iteration 9300: Loss = -9934.896941879504
Iteration 9400: Loss = -9934.904999024297
1
Iteration 9500: Loss = -9934.897100254742
2
Iteration 9600: Loss = -9934.896997809707
Iteration 9700: Loss = -9934.89774597859
1
Iteration 9800: Loss = -9934.897127649763
2
Iteration 9900: Loss = -9934.897164667003
3
Iteration 10000: Loss = -9934.896963340534
Iteration 10100: Loss = -9934.97576173775
1
Iteration 10200: Loss = -9934.934053058822
2
Iteration 10300: Loss = -9934.89702044807
Iteration 10400: Loss = -9934.8985647236
1
Iteration 10500: Loss = -9934.897252930572
2
Iteration 10600: Loss = -9934.905160088454
3
Iteration 10700: Loss = -9934.898319695714
4
Iteration 10800: Loss = -9934.89884051584
5
Iteration 10900: Loss = -9934.899394374128
6
Iteration 11000: Loss = -9934.903414443583
7
Iteration 11100: Loss = -9934.898273719824
8
Iteration 11200: Loss = -9934.897047674043
Iteration 11300: Loss = -9934.916167566469
1
Iteration 11400: Loss = -9934.907990902906
2
Iteration 11500: Loss = -9934.909593428361
3
Iteration 11600: Loss = -9934.8970685673
Iteration 11700: Loss = -9934.900649357145
1
Iteration 11800: Loss = -9934.900226654561
2
Iteration 11900: Loss = -9934.914750407808
3
Iteration 12000: Loss = -9934.911616367306
4
Iteration 12100: Loss = -9934.900427302502
5
Iteration 12200: Loss = -9934.89823256665
6
Iteration 12300: Loss = -9934.897640271385
7
Iteration 12400: Loss = -9934.897095887918
Iteration 12500: Loss = -9934.897014995682
Iteration 12600: Loss = -9934.898847771758
1
Iteration 12700: Loss = -9934.979109477163
2
Iteration 12800: Loss = -9934.899328390366
3
Iteration 12900: Loss = -9934.898470007258
4
Iteration 13000: Loss = -9934.897741482331
5
Iteration 13100: Loss = -9934.897623823264
6
Iteration 13200: Loss = -9934.896989923916
Iteration 13300: Loss = -9934.907762571871
1
Iteration 13400: Loss = -9934.897768411047
2
Iteration 13500: Loss = -9934.900620147037
3
Iteration 13600: Loss = -9934.96197042568
4
Iteration 13700: Loss = -9934.931777872342
5
Iteration 13800: Loss = -9934.914428609804
6
Iteration 13900: Loss = -9934.9003508442
7
Iteration 14000: Loss = -9934.90391583325
8
Iteration 14100: Loss = -9934.898048151501
9
Iteration 14200: Loss = -9934.897651549605
10
Iteration 14300: Loss = -9934.900724485642
11
Iteration 14400: Loss = -9934.901885544883
12
Iteration 14500: Loss = -9934.903691270001
13
Iteration 14600: Loss = -9934.900087722735
14
Iteration 14700: Loss = -9934.89997330575
15
Stopping early at iteration 14700 due to no improvement.
pi: tensor([[0.9870, 0.0130],
        [0.2258, 0.7742]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4907, 0.5093], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1613, 0.1446],
         [0.5566, 0.1286]],

        [[0.7278, 0.1280],
         [0.6708, 0.6349]],

        [[0.5135, 0.0898],
         [0.6802, 0.5757]],

        [[0.5318, 0.0931],
         [0.6131, 0.6637]],

        [[0.6090, 0.1208],
         [0.6095, 0.6710]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.04843871396029556
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 37
Adjusted Rand Index: 0.05947155415897392
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 22
Adjusted Rand Index: 0.3079630444346678
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 25
Adjusted Rand Index: 0.2432070262677796
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 33
Adjusted Rand Index: 0.1075897253749657
Global Adjusted Rand Index: 0.1426955412374019
Average Adjusted Rand Index: 0.1533340128393365
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20251.888128400395
Iteration 100: Loss = -9948.713081181913
Iteration 200: Loss = -9948.24131762012
Iteration 300: Loss = -9947.965376839593
Iteration 400: Loss = -9947.10821935011
Iteration 500: Loss = -9946.535983642823
Iteration 600: Loss = -9945.573465992742
Iteration 700: Loss = -9943.047579796597
Iteration 800: Loss = -9942.448103861618
Iteration 900: Loss = -9941.87976272621
Iteration 1000: Loss = -9941.642394110411
Iteration 1100: Loss = -9941.497704443702
Iteration 1200: Loss = -9941.473900426052
Iteration 1300: Loss = -9941.460268967896
Iteration 1400: Loss = -9941.445678993847
Iteration 1500: Loss = -9941.434516446032
Iteration 1600: Loss = -9941.423748681595
Iteration 1700: Loss = -9941.41751790175
Iteration 1800: Loss = -9941.412685458568
Iteration 1900: Loss = -9941.407409246644
Iteration 2000: Loss = -9941.40391246642
Iteration 2100: Loss = -9941.401119677445
Iteration 2200: Loss = -9941.39834558211
Iteration 2300: Loss = -9941.395418962158
Iteration 2400: Loss = -9941.39261712427
Iteration 2500: Loss = -9941.387875542188
Iteration 2600: Loss = -9941.38278035358
Iteration 2700: Loss = -9941.381177632735
Iteration 2800: Loss = -9941.37997736675
Iteration 2900: Loss = -9941.37890261741
Iteration 3000: Loss = -9941.377903245733
Iteration 3100: Loss = -9941.37691853162
Iteration 3200: Loss = -9941.375779628639
Iteration 3300: Loss = -9941.374299470837
Iteration 3400: Loss = -9941.372938667972
Iteration 3500: Loss = -9941.371353763097
Iteration 3600: Loss = -9941.370486715092
Iteration 3700: Loss = -9941.36962152699
Iteration 3800: Loss = -9941.368783832973
Iteration 3900: Loss = -9941.36819268485
Iteration 4000: Loss = -9941.367703431364
Iteration 4100: Loss = -9941.367353628711
Iteration 4200: Loss = -9941.367048979992
Iteration 4300: Loss = -9941.366778419893
Iteration 4400: Loss = -9941.366534996081
Iteration 4500: Loss = -9941.366314408344
Iteration 4600: Loss = -9941.366053906364
Iteration 4700: Loss = -9941.365700131586
Iteration 4800: Loss = -9941.365010570606
Iteration 4900: Loss = -9941.364264185117
Iteration 5000: Loss = -9941.364036449198
Iteration 5100: Loss = -9941.363757233794
Iteration 5200: Loss = -9941.363365726385
Iteration 5300: Loss = -9941.362968648615
Iteration 5400: Loss = -9941.36275705834
Iteration 5500: Loss = -9941.362531142
Iteration 5600: Loss = -9941.362336093438
Iteration 5700: Loss = -9941.361986581682
Iteration 5800: Loss = -9941.360686359174
Iteration 5900: Loss = -9941.360225942122
Iteration 6000: Loss = -9941.359801901292
Iteration 6100: Loss = -9941.35934593335
Iteration 6200: Loss = -9941.35908454164
Iteration 6300: Loss = -9941.358989081627
Iteration 6400: Loss = -9941.358938609168
Iteration 6500: Loss = -9941.358834855337
Iteration 6600: Loss = -9941.35874336504
Iteration 6700: Loss = -9941.358723239082
Iteration 6800: Loss = -9941.35869431576
Iteration 6900: Loss = -9941.359539332076
1
Iteration 7000: Loss = -9941.35857513832
Iteration 7100: Loss = -9941.358595574398
Iteration 7200: Loss = -9941.35852499041
Iteration 7300: Loss = -9941.35849459777
Iteration 7400: Loss = -9941.358473147744
Iteration 7500: Loss = -9941.358426669154
Iteration 7600: Loss = -9941.358393162309
Iteration 7700: Loss = -9941.358588442385
1
Iteration 7800: Loss = -9941.35833354904
Iteration 7900: Loss = -9941.358196646548
Iteration 8000: Loss = -9941.358122919835
Iteration 8100: Loss = -9941.358128520389
Iteration 8200: Loss = -9941.359564731383
1
Iteration 8300: Loss = -9941.358099359877
Iteration 8400: Loss = -9941.358088817911
Iteration 8500: Loss = -9941.358040243891
Iteration 8600: Loss = -9941.368774984127
1
Iteration 8700: Loss = -9941.357845772578
Iteration 8800: Loss = -9941.357780399785
Iteration 8900: Loss = -9941.357471824485
Iteration 9000: Loss = -9941.362066629597
1
Iteration 9100: Loss = -9941.357382794391
Iteration 9200: Loss = -9941.357270055592
Iteration 9300: Loss = -9941.357220084195
Iteration 9400: Loss = -9941.357200065804
Iteration 9500: Loss = -9941.384875097507
1
Iteration 9600: Loss = -9941.357101470976
Iteration 9700: Loss = -9941.357109313441
Iteration 9800: Loss = -9941.390309304037
1
Iteration 9900: Loss = -9941.357082098108
Iteration 10000: Loss = -9941.357092405362
Iteration 10100: Loss = -9941.704110334096
1
Iteration 10200: Loss = -9941.357062423152
Iteration 10300: Loss = -9941.35701993264
Iteration 10400: Loss = -9941.357019518098
Iteration 10500: Loss = -9941.358885592388
1
Iteration 10600: Loss = -9941.35700311536
Iteration 10700: Loss = -9941.356944878124
Iteration 10800: Loss = -9941.366613983728
1
Iteration 10900: Loss = -9941.356707153791
Iteration 11000: Loss = -9941.356533129603
Iteration 11100: Loss = -9941.38211146143
1
Iteration 11200: Loss = -9941.356462405509
Iteration 11300: Loss = -9941.356417750643
Iteration 11400: Loss = -9941.356562755938
1
Iteration 11500: Loss = -9941.356448897222
Iteration 11600: Loss = -9941.356355936427
Iteration 11700: Loss = -9941.356535254185
1
Iteration 11800: Loss = -9941.356097222018
Iteration 11900: Loss = -9941.355495169682
Iteration 12000: Loss = -9941.361942992758
1
Iteration 12100: Loss = -9941.35540201297
Iteration 12200: Loss = -9941.363024348784
1
Iteration 12300: Loss = -9941.356185586965
2
Iteration 12400: Loss = -9941.442939454893
3
Iteration 12500: Loss = -9941.35523874834
Iteration 12600: Loss = -9941.358572738714
1
Iteration 12700: Loss = -9941.355238036858
Iteration 12800: Loss = -9941.35555905216
1
Iteration 12900: Loss = -9941.355341307259
2
Iteration 13000: Loss = -9941.371625695212
3
Iteration 13100: Loss = -9941.35525199694
Iteration 13200: Loss = -9941.369057162952
1
Iteration 13300: Loss = -9941.355331347773
Iteration 13400: Loss = -9941.3564392873
1
Iteration 13500: Loss = -9941.366399121413
2
Iteration 13600: Loss = -9941.355651148517
3
Iteration 13700: Loss = -9941.35764070533
4
Iteration 13800: Loss = -9941.355349636935
Iteration 13900: Loss = -9941.355217609022
Iteration 14000: Loss = -9941.356080199708
1
Iteration 14100: Loss = -9941.355248776144
Iteration 14200: Loss = -9941.360507509648
1
Iteration 14300: Loss = -9941.357052528247
2
Iteration 14400: Loss = -9941.35946536637
3
Iteration 14500: Loss = -9941.422160551647
4
Iteration 14600: Loss = -9941.355168017775
Iteration 14700: Loss = -9941.358111928415
1
Iteration 14800: Loss = -9941.356038422817
2
Iteration 14900: Loss = -9941.356267057308
3
Iteration 15000: Loss = -9941.355764017208
4
Iteration 15100: Loss = -9941.355397713993
5
Iteration 15200: Loss = -9941.355594407165
6
Iteration 15300: Loss = -9941.359567739111
7
Iteration 15400: Loss = -9941.356050936978
8
Iteration 15500: Loss = -9941.355416837912
9
Iteration 15600: Loss = -9941.357048720827
10
Iteration 15700: Loss = -9941.474568680069
11
Iteration 15800: Loss = -9941.35696772472
12
Iteration 15900: Loss = -9941.355803229899
13
Iteration 16000: Loss = -9941.359259450415
14
Iteration 16100: Loss = -9941.355184264683
Iteration 16200: Loss = -9941.357469431632
1
Iteration 16300: Loss = -9941.387325890097
2
Iteration 16400: Loss = -9941.355110755572
Iteration 16500: Loss = -9941.358482946154
1
Iteration 16600: Loss = -9941.355440148964
2
Iteration 16700: Loss = -9941.35578665293
3
Iteration 16800: Loss = -9941.35533084515
4
Iteration 16900: Loss = -9941.35551239787
5
Iteration 17000: Loss = -9941.355676754449
6
Iteration 17100: Loss = -9941.538412806074
7
Iteration 17200: Loss = -9941.355029110033
Iteration 17300: Loss = -9941.356423225263
1
Iteration 17400: Loss = -9941.35505353032
Iteration 17500: Loss = -9941.355749785178
1
Iteration 17600: Loss = -9941.35601143575
2
Iteration 17700: Loss = -9941.359630947494
3
Iteration 17800: Loss = -9941.355138737668
Iteration 17900: Loss = -9941.355201275848
Iteration 18000: Loss = -9941.357801240723
1
Iteration 18100: Loss = -9941.38013196546
2
Iteration 18200: Loss = -9941.357024686527
3
Iteration 18300: Loss = -9941.355032492947
Iteration 18400: Loss = -9941.355831132007
1
Iteration 18500: Loss = -9941.354039631535
Iteration 18600: Loss = -9941.354426397842
1
Iteration 18700: Loss = -9941.358205522298
2
Iteration 18800: Loss = -9941.354142407175
3
Iteration 18900: Loss = -9941.35417930438
4
Iteration 19000: Loss = -9941.35558867596
5
Iteration 19100: Loss = -9941.35624293524
6
Iteration 19200: Loss = -9941.356699943275
7
Iteration 19300: Loss = -9941.354604609767
8
Iteration 19400: Loss = -9941.354210178228
9
Iteration 19500: Loss = -9941.380814208698
10
Iteration 19600: Loss = -9941.353982833747
Iteration 19700: Loss = -9941.354964071465
1
Iteration 19800: Loss = -9941.364847645287
2
Iteration 19900: Loss = -9941.354040585296
pi: tensor([[1.0000e+00, 4.2661e-07],
        [5.9733e-03, 9.9403e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0364, 0.9636], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1611, 0.2062],
         [0.5812, 0.1349]],

        [[0.5263, 0.2020],
         [0.5474, 0.5772]],

        [[0.6986, 0.1231],
         [0.6863, 0.5716]],

        [[0.6760, 0.1318],
         [0.6080, 0.7038]],

        [[0.7279, 0.2055],
         [0.5326, 0.5848]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0015539260913902783
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0005624593535232805
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.005396094422863973
Global Adjusted Rand Index: -0.0011631642686010769
Average Adjusted Rand Index: -0.0022107335977000686
10070.65504642735
[0.1426955412374019, -0.0011631642686010769] [0.1533340128393365, -0.0022107335977000686] [9934.89997330575, 9941.354165277431]
-------------------------------------
This iteration is 33
True Objective function: Loss = -10090.426674469172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21904.970630694523
Iteration 100: Loss = -9975.147011134432
Iteration 200: Loss = -9974.083277643496
Iteration 300: Loss = -9973.792959567912
Iteration 400: Loss = -9973.60808553835
Iteration 500: Loss = -9973.34382500689
Iteration 600: Loss = -9972.670826035755
Iteration 700: Loss = -9971.345270348642
Iteration 800: Loss = -9970.29448701516
Iteration 900: Loss = -9969.62696754424
Iteration 1000: Loss = -9968.977886591834
Iteration 1100: Loss = -9968.093828820318
Iteration 1200: Loss = -9964.549150129491
Iteration 1300: Loss = -9958.364588574354
Iteration 1400: Loss = -9957.878937909167
Iteration 1500: Loss = -9957.767829660017
Iteration 1600: Loss = -9957.724077838522
Iteration 1700: Loss = -9957.701226247264
Iteration 1800: Loss = -9957.6870322868
Iteration 1900: Loss = -9957.677765604387
Iteration 2000: Loss = -9957.671770243865
Iteration 2100: Loss = -9957.66760741907
Iteration 2200: Loss = -9957.664486866244
Iteration 2300: Loss = -9957.662067150664
Iteration 2400: Loss = -9957.660158657685
Iteration 2500: Loss = -9957.658665915913
Iteration 2600: Loss = -9957.657550950773
Iteration 2700: Loss = -9957.656600665907
Iteration 2800: Loss = -9957.655820246937
Iteration 2900: Loss = -9957.655172602474
Iteration 3000: Loss = -9957.654660009282
Iteration 3100: Loss = -9957.654178732257
Iteration 3200: Loss = -9957.653808980705
Iteration 3300: Loss = -9957.65346565159
Iteration 3400: Loss = -9957.65314411012
Iteration 3500: Loss = -9957.652881922673
Iteration 3600: Loss = -9957.652664783953
Iteration 3700: Loss = -9957.652439864369
Iteration 3800: Loss = -9957.652251232745
Iteration 3900: Loss = -9957.65210638656
Iteration 4000: Loss = -9957.65194540072
Iteration 4100: Loss = -9957.651823163314
Iteration 4200: Loss = -9957.65167747016
Iteration 4300: Loss = -9957.651558241196
Iteration 4400: Loss = -9957.651462214828
Iteration 4500: Loss = -9957.651376934225
Iteration 4600: Loss = -9957.65128873423
Iteration 4700: Loss = -9957.657742116697
1
Iteration 4800: Loss = -9957.651097684013
Iteration 4900: Loss = -9957.651036663694
Iteration 5000: Loss = -9957.650961087755
Iteration 5100: Loss = -9957.650895045788
Iteration 5200: Loss = -9957.650866809003
Iteration 5300: Loss = -9957.650709813097
Iteration 5400: Loss = -9957.650754841417
Iteration 5500: Loss = -9957.650527149992
Iteration 5600: Loss = -9957.653608341168
1
Iteration 5700: Loss = -9957.650388365286
Iteration 5800: Loss = -9957.650318752061
Iteration 5900: Loss = -9957.650440440812
1
Iteration 6000: Loss = -9957.650303819466
Iteration 6100: Loss = -9957.654754536043
1
Iteration 6200: Loss = -9957.650250711775
Iteration 6300: Loss = -9957.650220023506
Iteration 6400: Loss = -9957.65022768738
Iteration 6500: Loss = -9957.65016559698
Iteration 6600: Loss = -9957.650470699062
1
Iteration 6700: Loss = -9957.650140713691
Iteration 6800: Loss = -9957.653857739439
1
Iteration 6900: Loss = -9957.650085094936
Iteration 7000: Loss = -9957.650178985628
Iteration 7100: Loss = -9957.650410711427
1
Iteration 7200: Loss = -9957.652855309045
2
Iteration 7300: Loss = -9957.650115588845
Iteration 7400: Loss = -9957.650073097606
Iteration 7500: Loss = -9957.650311147298
1
Iteration 7600: Loss = -9957.650051610273
Iteration 7700: Loss = -9957.650060017866
Iteration 7800: Loss = -9957.650025239105
Iteration 7900: Loss = -9957.65001194309
Iteration 8000: Loss = -9957.649985593482
Iteration 8100: Loss = -9957.65002980896
Iteration 8200: Loss = -9957.649991371154
Iteration 8300: Loss = -9957.650020409163
Iteration 8400: Loss = -9957.656816602053
1
Iteration 8500: Loss = -9957.649993302452
Iteration 8600: Loss = -9957.654032264154
1
Iteration 8700: Loss = -9957.649954754781
Iteration 8800: Loss = -9957.680716381075
1
Iteration 8900: Loss = -9957.650380454723
2
Iteration 9000: Loss = -9957.653656951208
3
Iteration 9100: Loss = -9957.649988703264
Iteration 9200: Loss = -9957.686569598283
1
Iteration 9300: Loss = -9957.650901649684
2
Iteration 9400: Loss = -9957.65041068612
3
Iteration 9500: Loss = -9957.65734767302
4
Iteration 9600: Loss = -9957.6527724108
5
Iteration 9700: Loss = -9957.701624401569
6
Iteration 9800: Loss = -9957.651156221384
7
Iteration 9900: Loss = -9957.650025535795
Iteration 10000: Loss = -9957.650305205243
1
Iteration 10100: Loss = -9957.722073728602
2
Iteration 10200: Loss = -9957.651771875127
3
Iteration 10300: Loss = -9957.85639653661
4
Iteration 10400: Loss = -9957.650342724277
5
Iteration 10500: Loss = -9957.666879488432
6
Iteration 10600: Loss = -9957.652963780889
7
Iteration 10700: Loss = -9957.650525526204
8
Iteration 10800: Loss = -9957.666271731981
9
Iteration 10900: Loss = -9957.662651770186
10
Iteration 11000: Loss = -9957.657302716294
11
Iteration 11100: Loss = -9957.650510665375
12
Iteration 11200: Loss = -9957.712000821908
13
Iteration 11300: Loss = -9957.702370223287
14
Iteration 11400: Loss = -9957.655770194324
15
Stopping early at iteration 11400 due to no improvement.
pi: tensor([[0.4857, 0.5143],
        [0.0574, 0.9426]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1842, 0.8158], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4436, 0.1291],
         [0.7205, 0.1311]],

        [[0.6920, 0.1365],
         [0.6857, 0.7191]],

        [[0.5693, 0.1426],
         [0.6092, 0.5966]],

        [[0.5563, 0.1299],
         [0.5725, 0.5794]],

        [[0.6519, 0.1733],
         [0.5531, 0.6304]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.036947811396319974
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.004740841076001609
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.003677385461885144
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.006214218236452756
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.01644068827141728
Global Adjusted Rand Index: 0.017235651374700803
Average Adjusted Rand Index: 0.012133234703661294
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23087.63725522198
Iteration 100: Loss = -9977.622783459065
Iteration 200: Loss = -9975.184897127374
Iteration 300: Loss = -9974.213842427012
Iteration 400: Loss = -9973.866283625355
Iteration 500: Loss = -9973.706301366363
Iteration 600: Loss = -9973.597233163668
Iteration 700: Loss = -9973.473041756313
Iteration 800: Loss = -9973.144906972238
Iteration 900: Loss = -9971.752675999489
Iteration 1000: Loss = -9970.975605594413
Iteration 1100: Loss = -9970.58100492098
Iteration 1200: Loss = -9970.066161225905
Iteration 1300: Loss = -9969.58391018262
Iteration 1400: Loss = -9968.823617925555
Iteration 1500: Loss = -9965.336253912952
Iteration 1600: Loss = -9958.040407549679
Iteration 1700: Loss = -9957.820791767284
Iteration 1800: Loss = -9957.761297118424
Iteration 1900: Loss = -9957.729629932506
Iteration 2000: Loss = -9957.708347540432
Iteration 2100: Loss = -9957.695486310706
Iteration 2200: Loss = -9957.688565245653
Iteration 2300: Loss = -9957.68389035546
Iteration 2400: Loss = -9957.680503008529
Iteration 2500: Loss = -9957.67782723065
Iteration 2600: Loss = -9957.675646299525
Iteration 2700: Loss = -9957.673735798004
Iteration 2800: Loss = -9957.672021535902
Iteration 2900: Loss = -9957.670475311788
Iteration 3000: Loss = -9957.669084891813
Iteration 3100: Loss = -9957.667934229808
Iteration 3200: Loss = -9957.66684081617
Iteration 3300: Loss = -9957.665812292746
Iteration 3400: Loss = -9957.664965364142
Iteration 3500: Loss = -9957.664379060525
Iteration 3600: Loss = -9957.66394267719
Iteration 3700: Loss = -9957.663577267384
Iteration 3800: Loss = -9957.663341980855
Iteration 3900: Loss = -9957.66307874072
Iteration 4000: Loss = -9957.662867622268
Iteration 4100: Loss = -9957.662645946559
Iteration 4200: Loss = -9957.662445404198
Iteration 4300: Loss = -9957.662291214603
Iteration 4400: Loss = -9957.662097788856
Iteration 4500: Loss = -9957.661858935902
Iteration 4600: Loss = -9957.66147610027
Iteration 4700: Loss = -9957.660843730373
Iteration 4800: Loss = -9957.656207647124
Iteration 4900: Loss = -9957.654943991833
Iteration 5000: Loss = -9957.654755287344
Iteration 5100: Loss = -9957.654768270828
Iteration 5200: Loss = -9957.6545915123
Iteration 5300: Loss = -9957.654684550795
Iteration 5400: Loss = -9957.65401920851
Iteration 5500: Loss = -9957.653985355159
Iteration 5600: Loss = -9957.65384012277
Iteration 5700: Loss = -9957.65393581846
Iteration 5800: Loss = -9957.651118979247
Iteration 5900: Loss = -9957.650884536359
Iteration 6000: Loss = -9957.658046359906
1
Iteration 6100: Loss = -9957.650739638964
Iteration 6200: Loss = -9957.650862443465
1
Iteration 6300: Loss = -9957.650669436109
Iteration 6400: Loss = -9957.650633611995
Iteration 6500: Loss = -9957.65122160947
1
Iteration 6600: Loss = -9957.650585389714
Iteration 6700: Loss = -9957.650532578444
Iteration 6800: Loss = -9957.650669411621
1
Iteration 6900: Loss = -9957.65206083012
2
Iteration 7000: Loss = -9957.650479252026
Iteration 7100: Loss = -9957.650606759606
1
Iteration 7200: Loss = -9957.650443275152
Iteration 7300: Loss = -9957.652418886517
1
Iteration 7400: Loss = -9957.650400350552
Iteration 7500: Loss = -9957.650364196581
Iteration 7600: Loss = -9957.650323670634
Iteration 7700: Loss = -9957.650295832047
Iteration 7800: Loss = -9957.650733281249
1
Iteration 7900: Loss = -9957.650174233548
Iteration 8000: Loss = -9957.652769752152
1
Iteration 8100: Loss = -9957.649960531371
Iteration 8200: Loss = -9957.64997304312
Iteration 8300: Loss = -9957.649998108043
Iteration 8400: Loss = -9957.649976066028
Iteration 8500: Loss = -9957.649981353836
Iteration 8600: Loss = -9957.650471251749
1
Iteration 8700: Loss = -9957.649954110322
Iteration 8800: Loss = -9957.650065249169
1
Iteration 8900: Loss = -9957.772241064727
2
Iteration 9000: Loss = -9957.649953888154
Iteration 9100: Loss = -9957.65087323385
1
Iteration 9200: Loss = -9957.655792665915
2
Iteration 9300: Loss = -9957.650186978546
3
Iteration 9400: Loss = -9957.717124677074
4
Iteration 9500: Loss = -9957.649945069043
Iteration 9600: Loss = -9957.654104123014
1
Iteration 9700: Loss = -9957.650136738408
2
Iteration 9800: Loss = -9957.651773857408
3
Iteration 9900: Loss = -9957.65145400052
4
Iteration 10000: Loss = -9957.65000335231
Iteration 10100: Loss = -9957.650768512818
1
Iteration 10200: Loss = -9957.658767682993
2
Iteration 10300: Loss = -9957.650064364998
Iteration 10400: Loss = -9957.654265598261
1
Iteration 10500: Loss = -9957.742896764381
2
Iteration 10600: Loss = -9957.650469760796
3
Iteration 10700: Loss = -9957.652231816533
4
Iteration 10800: Loss = -9957.65454238887
5
Iteration 10900: Loss = -9957.659340100934
6
Iteration 11000: Loss = -9957.653417672413
7
Iteration 11100: Loss = -9957.651274712622
8
Iteration 11200: Loss = -9957.651357431805
9
Iteration 11300: Loss = -9957.650981328941
10
Iteration 11400: Loss = -9957.654498759868
11
Iteration 11500: Loss = -9957.651782820276
12
Iteration 11600: Loss = -9957.658250370776
13
Iteration 11700: Loss = -9957.652039186953
14
Iteration 11800: Loss = -9957.650797845205
15
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[0.9426, 0.0574],
        [0.5167, 0.4833]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8174, 0.1826], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1313, 0.1295],
         [0.6649, 0.4436]],

        [[0.5403, 0.1365],
         [0.7249, 0.6440]],

        [[0.5707, 0.1427],
         [0.6424, 0.5939]],

        [[0.5971, 0.1302],
         [0.6135, 0.5877]],

        [[0.5587, 0.1735],
         [0.7219, 0.5169]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.036947811396319974
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.004740841076001609
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.003677385461885144
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.006214218236452756
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.01644068827141728
Global Adjusted Rand Index: 0.017235651374700803
Average Adjusted Rand Index: 0.012133234703661294
10090.426674469172
[0.017235651374700803, 0.017235651374700803] [0.012133234703661294, 0.012133234703661294] [9957.655770194324, 9957.650797845205]
-------------------------------------
This iteration is 34
True Objective function: Loss = -10104.20582767735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21567.81148274407
Iteration 100: Loss = -10017.969097686822
Iteration 200: Loss = -10017.196845382761
Iteration 300: Loss = -10014.300430776288
Iteration 400: Loss = -10013.912572790552
Iteration 500: Loss = -10013.756655427522
Iteration 600: Loss = -10013.557750961454
Iteration 700: Loss = -10013.265013075441
Iteration 800: Loss = -10013.000736685932
Iteration 900: Loss = -10012.713429792508
Iteration 1000: Loss = -10012.507056559785
Iteration 1100: Loss = -10012.418637562203
Iteration 1200: Loss = -10012.365443673338
Iteration 1300: Loss = -10012.32721993556
Iteration 1400: Loss = -10012.297447311797
Iteration 1500: Loss = -10012.273368871965
Iteration 1600: Loss = -10012.253464860332
Iteration 1700: Loss = -10012.23670102233
Iteration 1800: Loss = -10012.222484086442
Iteration 1900: Loss = -10012.210216763775
Iteration 2000: Loss = -10012.199626793124
Iteration 2100: Loss = -10012.190393740839
Iteration 2200: Loss = -10012.182325172544
Iteration 2300: Loss = -10012.175229559676
Iteration 2400: Loss = -10012.168961767713
Iteration 2500: Loss = -10012.163460629381
Iteration 2600: Loss = -10012.158571982329
Iteration 2700: Loss = -10012.154237682207
Iteration 2800: Loss = -10012.150355549977
Iteration 2900: Loss = -10012.146906395259
Iteration 3000: Loss = -10012.14381404177
Iteration 3100: Loss = -10012.141070471696
Iteration 3200: Loss = -10012.138592984547
Iteration 3300: Loss = -10012.136343154814
Iteration 3400: Loss = -10012.134315860247
Iteration 3500: Loss = -10012.132516466932
Iteration 3600: Loss = -10012.130864577239
Iteration 3700: Loss = -10012.12936690762
Iteration 3800: Loss = -10012.127990227205
Iteration 3900: Loss = -10012.126780726834
Iteration 4000: Loss = -10012.12560878374
Iteration 4100: Loss = -10012.12456740668
Iteration 4200: Loss = -10012.123589763265
Iteration 4300: Loss = -10012.122737819138
Iteration 4400: Loss = -10012.121866203779
Iteration 4500: Loss = -10012.121117681525
Iteration 4600: Loss = -10012.12039817784
Iteration 4700: Loss = -10012.119758881572
Iteration 4800: Loss = -10012.119129733497
Iteration 4900: Loss = -10012.118542396869
Iteration 5000: Loss = -10012.117980130826
Iteration 5100: Loss = -10012.117456578817
Iteration 5200: Loss = -10012.117002994948
Iteration 5300: Loss = -10012.116572785919
Iteration 5400: Loss = -10012.116111415615
Iteration 5500: Loss = -10012.115731637597
Iteration 5600: Loss = -10012.115353085235
Iteration 5700: Loss = -10012.114989299582
Iteration 5800: Loss = -10012.117937336563
1
Iteration 5900: Loss = -10012.11434776241
Iteration 6000: Loss = -10012.114021455649
Iteration 6100: Loss = -10012.113784068433
Iteration 6200: Loss = -10012.113486429886
Iteration 6300: Loss = -10012.113248140824
Iteration 6400: Loss = -10012.11296382279
Iteration 6500: Loss = -10012.11275653454
Iteration 6600: Loss = -10012.112590854234
Iteration 6700: Loss = -10012.11238056619
Iteration 6800: Loss = -10012.112247211458
Iteration 6900: Loss = -10012.11196777521
Iteration 7000: Loss = -10012.111808723825
Iteration 7100: Loss = -10012.111639455114
Iteration 7200: Loss = -10012.111513258096
Iteration 7300: Loss = -10012.111357415017
Iteration 7400: Loss = -10012.111165751554
Iteration 7500: Loss = -10012.111851680143
1
Iteration 7600: Loss = -10012.110921712641
Iteration 7700: Loss = -10012.110941107476
Iteration 7800: Loss = -10012.110740661517
Iteration 7900: Loss = -10012.110661981125
Iteration 8000: Loss = -10012.203006431499
1
Iteration 8100: Loss = -10012.110418525264
Iteration 8200: Loss = -10012.111010492315
1
Iteration 8300: Loss = -10012.110248541816
Iteration 8400: Loss = -10012.110145710998
Iteration 8500: Loss = -10012.110086707975
Iteration 8600: Loss = -10012.119867344541
1
Iteration 8700: Loss = -10012.109909017814
Iteration 8800: Loss = -10012.109866029072
Iteration 8900: Loss = -10012.109838053304
Iteration 9000: Loss = -10012.10974001328
Iteration 9100: Loss = -10012.125117000573
1
Iteration 9200: Loss = -10012.109633940045
Iteration 9300: Loss = -10012.10954524126
Iteration 9400: Loss = -10012.109482636193
Iteration 9500: Loss = -10012.110778342763
1
Iteration 9600: Loss = -10012.109420268476
Iteration 9700: Loss = -10012.109406094523
Iteration 9800: Loss = -10012.109534712026
1
Iteration 9900: Loss = -10012.109325659563
Iteration 10000: Loss = -10012.109259395353
Iteration 10100: Loss = -10012.344052285349
1
Iteration 10200: Loss = -10012.109229404432
Iteration 10300: Loss = -10012.109168150748
Iteration 10400: Loss = -10012.110132634987
1
Iteration 10500: Loss = -10012.109174655694
Iteration 10600: Loss = -10012.109103403303
Iteration 10700: Loss = -10012.111464240574
1
Iteration 10800: Loss = -10012.10908870902
Iteration 10900: Loss = -10012.109017502658
Iteration 11000: Loss = -10012.108955919795
Iteration 11100: Loss = -10012.11030706895
1
Iteration 11200: Loss = -10012.108949652533
Iteration 11300: Loss = -10012.108925830267
Iteration 11400: Loss = -10012.542779681078
1
Iteration 11500: Loss = -10012.108880061804
Iteration 11600: Loss = -10012.108880450534
Iteration 11700: Loss = -10012.136502462326
1
Iteration 11800: Loss = -10012.108841293495
Iteration 11900: Loss = -10012.108838884596
Iteration 12000: Loss = -10012.10881386412
Iteration 12100: Loss = -10012.108939209396
1
Iteration 12200: Loss = -10012.108771101788
Iteration 12300: Loss = -10012.108770471956
Iteration 12400: Loss = -10012.108813041055
Iteration 12500: Loss = -10012.108755721732
Iteration 12600: Loss = -10012.108759186196
Iteration 12700: Loss = -10012.109455683478
1
Iteration 12800: Loss = -10012.108769546157
Iteration 12900: Loss = -10012.108736138533
Iteration 13000: Loss = -10012.108866960485
1
Iteration 13100: Loss = -10012.213656645637
2
Iteration 13200: Loss = -10012.108731792407
Iteration 13300: Loss = -10012.181872098592
1
Iteration 13400: Loss = -10012.108681858123
Iteration 13500: Loss = -10012.116400935052
1
Iteration 13600: Loss = -10012.108841170138
2
Iteration 13700: Loss = -10012.108731640392
Iteration 13800: Loss = -10012.112783254075
1
Iteration 13900: Loss = -10012.11333414227
2
Iteration 14000: Loss = -10012.131067139206
3
Iteration 14100: Loss = -10012.108688146569
Iteration 14200: Loss = -10012.109907436243
1
Iteration 14300: Loss = -10012.115585339625
2
Iteration 14400: Loss = -10012.108784421349
Iteration 14500: Loss = -10012.108693884944
Iteration 14600: Loss = -10012.111184556146
1
Iteration 14700: Loss = -10012.108625027142
Iteration 14800: Loss = -10012.111635850151
1
Iteration 14900: Loss = -10012.13376431185
2
Iteration 15000: Loss = -10012.108675259738
Iteration 15100: Loss = -10012.10916553033
1
Iteration 15200: Loss = -10012.108629671848
Iteration 15300: Loss = -10012.333234189951
1
Iteration 15400: Loss = -10012.108622974609
Iteration 15500: Loss = -10012.129967358705
1
Iteration 15600: Loss = -10012.108605027628
Iteration 15700: Loss = -10012.111520831813
1
Iteration 15800: Loss = -10012.10861920699
Iteration 15900: Loss = -10012.10878587596
1
Iteration 16000: Loss = -10012.112391609822
2
Iteration 16100: Loss = -10012.132497701861
3
Iteration 16200: Loss = -10012.112840094647
4
Iteration 16300: Loss = -10012.10868993938
Iteration 16400: Loss = -10012.108946904083
1
Iteration 16500: Loss = -10012.136575807372
2
Iteration 16600: Loss = -10012.108819901383
3
Iteration 16700: Loss = -10012.108641386005
Iteration 16800: Loss = -10012.110973620516
1
Iteration 16900: Loss = -10012.111579102373
2
Iteration 17000: Loss = -10012.108632760373
Iteration 17100: Loss = -10012.16133095992
1
Iteration 17200: Loss = -10012.108562265168
Iteration 17300: Loss = -10012.127235132582
1
Iteration 17400: Loss = -10012.108582349298
Iteration 17500: Loss = -10012.108924597802
1
Iteration 17600: Loss = -10012.108626430698
Iteration 17700: Loss = -10012.108563649614
Iteration 17800: Loss = -10012.110120438409
1
Iteration 17900: Loss = -10012.108581099059
Iteration 18000: Loss = -10012.108670646601
Iteration 18100: Loss = -10012.204940365398
1
Iteration 18200: Loss = -10012.161842864509
2
Iteration 18300: Loss = -10012.108613725126
Iteration 18400: Loss = -10012.108803917397
1
Iteration 18500: Loss = -10012.126518739464
2
Iteration 18600: Loss = -10012.108578255706
Iteration 18700: Loss = -10012.108639525222
Iteration 18800: Loss = -10012.108695987405
Iteration 18900: Loss = -10012.11120963334
1
Iteration 19000: Loss = -10012.15140672739
2
Iteration 19100: Loss = -10012.109464643985
3
Iteration 19200: Loss = -10012.118752375269
4
Iteration 19300: Loss = -10012.108588074165
Iteration 19400: Loss = -10012.178076503133
1
Iteration 19500: Loss = -10012.10877154906
2
Iteration 19600: Loss = -10012.108657288803
Iteration 19700: Loss = -10012.117016980492
1
Iteration 19800: Loss = -10012.108569460484
Iteration 19900: Loss = -10012.109473877252
1
pi: tensor([[9.7462e-01, 2.5383e-02],
        [1.0000e+00, 5.1068e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9074, 0.0926], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1349, 0.1946],
         [0.7107, 0.3897]],

        [[0.7294, 0.2065],
         [0.5329, 0.6903]],

        [[0.7224, 0.2062],
         [0.7301, 0.6423]],

        [[0.6659, 0.2035],
         [0.5671, 0.6173]],

        [[0.5041, 0.1768],
         [0.5876, 0.6155]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0037746410354473374
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0018801826829044942
Average Adjusted Rand Index: -0.001184278645924542
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21978.83968448709
Iteration 100: Loss = -10019.137522511694
Iteration 200: Loss = -10016.506878775586
Iteration 300: Loss = -10013.944004126111
Iteration 400: Loss = -10013.742853669317
Iteration 500: Loss = -10013.604701853754
Iteration 600: Loss = -10013.483310140231
Iteration 700: Loss = -10013.347232936158
Iteration 800: Loss = -10013.15876524788
Iteration 900: Loss = -10012.935692120154
Iteration 1000: Loss = -10012.781631160182
Iteration 1100: Loss = -10012.609542473581
Iteration 1200: Loss = -10012.382533786478
Iteration 1300: Loss = -10012.282407538158
Iteration 1400: Loss = -10012.237720550762
Iteration 1500: Loss = -10012.211974995285
Iteration 1600: Loss = -10012.19498816245
Iteration 1700: Loss = -10012.18286582861
Iteration 1800: Loss = -10012.173617054203
Iteration 1900: Loss = -10012.166286473775
Iteration 2000: Loss = -10012.16033970999
Iteration 2100: Loss = -10012.15530835398
Iteration 2200: Loss = -10012.151025718465
Iteration 2300: Loss = -10012.147276426831
Iteration 2400: Loss = -10012.14408245721
Iteration 2500: Loss = -10012.141221799886
Iteration 2600: Loss = -10012.138646946287
Iteration 2700: Loss = -10012.136351128067
Iteration 2800: Loss = -10012.13425826851
Iteration 2900: Loss = -10012.132420438822
Iteration 3000: Loss = -10012.130693291443
Iteration 3100: Loss = -10012.12914895327
Iteration 3200: Loss = -10012.127725114102
Iteration 3300: Loss = -10012.126459384359
Iteration 3400: Loss = -10012.12522621194
Iteration 3500: Loss = -10012.124151444763
Iteration 3600: Loss = -10012.123170825058
Iteration 3700: Loss = -10012.122177570876
Iteration 3800: Loss = -10012.121373545884
Iteration 3900: Loss = -10012.12057133991
Iteration 4000: Loss = -10012.119811203438
Iteration 4100: Loss = -10012.119163353402
Iteration 4200: Loss = -10012.118535770662
Iteration 4300: Loss = -10012.117938552088
Iteration 4400: Loss = -10012.117394563867
Iteration 4500: Loss = -10012.116902578384
Iteration 4600: Loss = -10012.116413222191
Iteration 4700: Loss = -10012.115970776254
Iteration 4800: Loss = -10012.115547064232
Iteration 4900: Loss = -10012.115142768294
Iteration 5000: Loss = -10012.11479202751
Iteration 5100: Loss = -10012.114443708766
Iteration 5200: Loss = -10012.114135136526
Iteration 5300: Loss = -10012.113845260257
Iteration 5400: Loss = -10012.113551578073
Iteration 5500: Loss = -10012.11327807035
Iteration 5600: Loss = -10012.113047082497
Iteration 5700: Loss = -10012.113495817606
1
Iteration 5800: Loss = -10012.112552067822
Iteration 5900: Loss = -10012.11235866058
Iteration 6000: Loss = -10012.1128145307
1
Iteration 6100: Loss = -10012.113247771564
2
Iteration 6200: Loss = -10012.111814689079
Iteration 6300: Loss = -10012.111620148453
Iteration 6400: Loss = -10012.116455916454
1
Iteration 6500: Loss = -10012.111326001092
Iteration 6600: Loss = -10012.11120315811
Iteration 6700: Loss = -10012.113594447905
1
Iteration 6800: Loss = -10012.111219321927
Iteration 6900: Loss = -10012.110787751684
Iteration 7000: Loss = -10012.110695882018
Iteration 7100: Loss = -10012.112848419763
1
Iteration 7200: Loss = -10012.110446642562
Iteration 7300: Loss = -10012.118579638405
1
Iteration 7400: Loss = -10012.110278168388
Iteration 7500: Loss = -10012.110267996226
Iteration 7600: Loss = -10012.110126280366
Iteration 7700: Loss = -10012.11109083412
1
Iteration 7800: Loss = -10012.109940972412
Iteration 7900: Loss = -10012.109902356524
Iteration 8000: Loss = -10012.109847919854
Iteration 8100: Loss = -10012.109766596772
Iteration 8200: Loss = -10012.121659806453
1
Iteration 8300: Loss = -10012.109653784786
Iteration 8400: Loss = -10012.109610439447
Iteration 8500: Loss = -10012.109572538251
Iteration 8600: Loss = -10012.11657550939
1
Iteration 8700: Loss = -10012.10944474873
Iteration 8800: Loss = -10012.109501214969
Iteration 8900: Loss = -10012.10985768623
1
Iteration 9000: Loss = -10012.109351953026
Iteration 9100: Loss = -10012.109292582818
Iteration 9200: Loss = -10012.109902141392
1
Iteration 9300: Loss = -10012.109221162751
Iteration 9400: Loss = -10012.109461669328
1
Iteration 9500: Loss = -10012.109210584727
Iteration 9600: Loss = -10012.109128235523
Iteration 9700: Loss = -10012.42863303456
1
Iteration 9800: Loss = -10012.109102031309
Iteration 9900: Loss = -10012.109026335209
Iteration 10000: Loss = -10012.469448787198
1
Iteration 10100: Loss = -10012.109003917638
Iteration 10200: Loss = -10012.108991438574
Iteration 10300: Loss = -10012.218862498818
1
Iteration 10400: Loss = -10012.108944854821
Iteration 10500: Loss = -10012.108894231871
Iteration 10600: Loss = -10012.162223834644
1
Iteration 10700: Loss = -10012.108895636715
Iteration 10800: Loss = -10012.10888275004
Iteration 10900: Loss = -10012.114706557635
1
Iteration 11000: Loss = -10012.10884084473
Iteration 11100: Loss = -10012.108818904686
Iteration 11200: Loss = -10012.108831052185
Iteration 11300: Loss = -10012.108812533213
Iteration 11400: Loss = -10012.396950871549
1
Iteration 11500: Loss = -10012.108781977096
Iteration 11600: Loss = -10012.10876578249
Iteration 11700: Loss = -10012.109039487576
1
Iteration 11800: Loss = -10012.108762918995
Iteration 11900: Loss = -10012.108711169205
Iteration 12000: Loss = -10012.146019191421
1
Iteration 12100: Loss = -10012.108729115582
Iteration 12200: Loss = -10012.10872018013
Iteration 12300: Loss = -10012.109528095296
1
Iteration 12400: Loss = -10012.108712326479
Iteration 12500: Loss = -10012.10867588844
Iteration 12600: Loss = -10012.108728915957
Iteration 12700: Loss = -10012.108714970847
Iteration 12800: Loss = -10012.108695550285
Iteration 12900: Loss = -10012.21517499905
1
Iteration 13000: Loss = -10012.108647697787
Iteration 13100: Loss = -10012.108677766066
Iteration 13200: Loss = -10012.109141204977
1
Iteration 13300: Loss = -10012.11003486639
2
Iteration 13400: Loss = -10012.161276546249
3
Iteration 13500: Loss = -10012.108673383962
Iteration 13600: Loss = -10012.108916478786
1
Iteration 13700: Loss = -10012.304239677565
2
Iteration 13800: Loss = -10012.10865816384
Iteration 13900: Loss = -10012.118481338008
1
Iteration 14000: Loss = -10012.108624427521
Iteration 14100: Loss = -10012.109267289348
1
Iteration 14200: Loss = -10012.10861540551
Iteration 14300: Loss = -10012.109643003321
1
Iteration 14400: Loss = -10012.134286767363
2
Iteration 14500: Loss = -10012.108631893021
Iteration 14600: Loss = -10012.109047948452
1
Iteration 14700: Loss = -10012.108919725662
2
Iteration 14800: Loss = -10012.108629845468
Iteration 14900: Loss = -10012.138682815674
1
Iteration 15000: Loss = -10012.123828050579
2
Iteration 15100: Loss = -10012.108815771184
3
Iteration 15200: Loss = -10012.109545335219
4
Iteration 15300: Loss = -10012.10882275378
5
Iteration 15400: Loss = -10012.108591578562
Iteration 15500: Loss = -10012.113082385238
1
Iteration 15600: Loss = -10012.11281128511
2
Iteration 15700: Loss = -10012.108601625592
Iteration 15800: Loss = -10012.118761270725
1
Iteration 15900: Loss = -10012.108578764843
Iteration 16000: Loss = -10012.10875647545
1
Iteration 16100: Loss = -10012.109359520311
2
Iteration 16200: Loss = -10012.158943189286
3
Iteration 16300: Loss = -10012.10865123369
Iteration 16400: Loss = -10012.109195600393
1
Iteration 16500: Loss = -10012.23633984298
2
Iteration 16600: Loss = -10012.10860848501
Iteration 16700: Loss = -10012.109473663477
1
Iteration 16800: Loss = -10012.109605837615
2
Iteration 16900: Loss = -10012.109403085098
3
Iteration 17000: Loss = -10012.277160659523
4
Iteration 17100: Loss = -10012.108583636289
Iteration 17200: Loss = -10012.113182853831
1
Iteration 17300: Loss = -10012.112451660536
2
Iteration 17400: Loss = -10012.109713741453
3
Iteration 17500: Loss = -10012.223676577181
4
Iteration 17600: Loss = -10012.1086106902
Iteration 17700: Loss = -10012.10878593042
1
Iteration 17800: Loss = -10012.109007283974
2
Iteration 17900: Loss = -10012.108600841895
Iteration 18000: Loss = -10012.221130319007
1
Iteration 18100: Loss = -10012.1091089105
2
Iteration 18200: Loss = -10012.10857430417
Iteration 18300: Loss = -10012.112311922483
1
Iteration 18400: Loss = -10012.108603368268
Iteration 18500: Loss = -10012.108830900052
1
Iteration 18600: Loss = -10012.109447808103
2
Iteration 18700: Loss = -10012.156058358467
3
Iteration 18800: Loss = -10012.108596196362
Iteration 18900: Loss = -10012.10875746757
1
Iteration 19000: Loss = -10012.110058701324
2
Iteration 19100: Loss = -10012.108592580917
Iteration 19200: Loss = -10012.108756370722
1
Iteration 19300: Loss = -10012.108571009338
Iteration 19400: Loss = -10012.110523622827
1
Iteration 19500: Loss = -10012.108566903275
Iteration 19600: Loss = -10012.126758043894
1
Iteration 19700: Loss = -10012.108563072288
Iteration 19800: Loss = -10012.115576821558
1
Iteration 19900: Loss = -10012.108559044373
pi: tensor([[3.3900e-07, 1.0000e+00],
        [2.5335e-02, 9.7467e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0925, 0.9075], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3900, 0.1948],
         [0.7220, 0.1349]],

        [[0.6520, 0.2079],
         [0.5558, 0.5681]],

        [[0.5229, 0.2064],
         [0.6115, 0.5952]],

        [[0.5800, 0.2038],
         [0.5763, 0.5729]],

        [[0.7159, 0.1781],
         [0.6939, 0.6032]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0037746410354473374
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0018801826829044942
Average Adjusted Rand Index: -0.001184278645924542
10104.20582767735
[-0.0018801826829044942, -0.0018801826829044942] [-0.001184278645924542, -0.001184278645924542] [10012.11258603481, 10012.108978500963]
-------------------------------------
This iteration is 35
True Objective function: Loss = -10088.981300397678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20056.95796151703
Iteration 100: Loss = -10041.194818668668
Iteration 200: Loss = -10002.583045589294
Iteration 300: Loss = -9990.644585920223
Iteration 400: Loss = -9990.377969003634
Iteration 500: Loss = -9990.301314826778
Iteration 600: Loss = -9990.262869396796
Iteration 700: Loss = -9990.239405716173
Iteration 800: Loss = -9990.223121194693
Iteration 900: Loss = -9990.210930156356
Iteration 1000: Loss = -9990.201295743835
Iteration 1100: Loss = -9990.19324007644
Iteration 1200: Loss = -9990.193195305965
Iteration 1300: Loss = -9990.180611052505
Iteration 1400: Loss = -9990.175475126158
Iteration 1500: Loss = -9990.171554446128
Iteration 1600: Loss = -9990.166884582173
Iteration 1700: Loss = -9990.163276319023
Iteration 1800: Loss = -9990.160075607546
Iteration 1900: Loss = -9990.157038837835
Iteration 2000: Loss = -9990.15437651057
Iteration 2100: Loss = -9990.151905747722
Iteration 2200: Loss = -9990.149669431701
Iteration 2300: Loss = -9990.148023398635
Iteration 2400: Loss = -9990.145723353009
Iteration 2500: Loss = -9990.143995616576
Iteration 2600: Loss = -9990.14242531323
Iteration 2700: Loss = -9990.140977389003
Iteration 2800: Loss = -9990.14014608059
Iteration 2900: Loss = -9990.138402122091
Iteration 3000: Loss = -9990.137268762926
Iteration 3100: Loss = -9990.136417148722
Iteration 3200: Loss = -9990.135224702413
Iteration 3300: Loss = -9990.134344438346
Iteration 3400: Loss = -9990.133435376787
Iteration 3500: Loss = -9990.13267558365
Iteration 3600: Loss = -9990.132527580114
Iteration 3700: Loss = -9990.131225943396
Iteration 3800: Loss = -9990.130603680136
Iteration 3900: Loss = -9990.130018798502
Iteration 4000: Loss = -9990.129462977655
Iteration 4100: Loss = -9990.129282766395
Iteration 4200: Loss = -9990.128458001036
Iteration 4300: Loss = -9990.156108676967
1
Iteration 4400: Loss = -9990.127574810964
Iteration 4500: Loss = -9990.12714451423
Iteration 4600: Loss = -9990.12683944918
Iteration 4700: Loss = -9990.126429102816
Iteration 4800: Loss = -9990.126231174969
Iteration 4900: Loss = -9990.125792903227
Iteration 5000: Loss = -9990.125504371055
Iteration 5100: Loss = -9990.125210820175
Iteration 5200: Loss = -9990.124952624894
Iteration 5300: Loss = -9990.124669412044
Iteration 5400: Loss = -9990.124691858678
Iteration 5500: Loss = -9990.124260900622
Iteration 5600: Loss = -9990.13305514043
1
Iteration 5700: Loss = -9990.12388480883
Iteration 5800: Loss = -9990.123680006503
Iteration 5900: Loss = -9990.123537272575
Iteration 6000: Loss = -9990.123326220517
Iteration 6100: Loss = -9990.123862189741
1
Iteration 6200: Loss = -9990.12304898244
Iteration 6300: Loss = -9990.12548964597
1
Iteration 6400: Loss = -9990.122758021587
Iteration 6500: Loss = -9990.122778308716
Iteration 6600: Loss = -9990.122566068776
Iteration 6700: Loss = -9990.12245272535
Iteration 6800: Loss = -9990.122329848196
Iteration 6900: Loss = -9990.122229890827
Iteration 7000: Loss = -9990.122177115203
Iteration 7100: Loss = -9990.122013244301
Iteration 7200: Loss = -9990.129081819228
1
Iteration 7300: Loss = -9990.121852219387
Iteration 7400: Loss = -9990.121817138071
Iteration 7500: Loss = -9990.121747196818
Iteration 7600: Loss = -9990.121676383027
Iteration 7700: Loss = -9990.122132807872
1
Iteration 7800: Loss = -9990.121536818278
Iteration 7900: Loss = -9990.121771291477
1
Iteration 8000: Loss = -9990.121432279757
Iteration 8100: Loss = -9990.121358463395
Iteration 8200: Loss = -9990.12130292223
Iteration 8300: Loss = -9990.124003789319
1
Iteration 8400: Loss = -9990.121225141938
Iteration 8500: Loss = -9990.121314579486
Iteration 8600: Loss = -9990.121240011256
Iteration 8700: Loss = -9990.133505520536
1
Iteration 8800: Loss = -9990.121070644884
Iteration 8900: Loss = -9990.121440893798
1
Iteration 9000: Loss = -9990.120998137625
Iteration 9100: Loss = -9990.130909824522
1
Iteration 9200: Loss = -9990.120917493894
Iteration 9300: Loss = -9990.12157920994
1
Iteration 9400: Loss = -9990.120887154084
Iteration 9500: Loss = -9990.197065165943
1
Iteration 9600: Loss = -9990.12083714128
Iteration 9700: Loss = -9990.147334809344
1
Iteration 9800: Loss = -9990.12081510372
Iteration 9900: Loss = -9990.128238042202
1
Iteration 10000: Loss = -9990.120804771428
Iteration 10100: Loss = -9990.120854779483
Iteration 10200: Loss = -9990.404777159803
1
Iteration 10300: Loss = -9990.120724869723
Iteration 10400: Loss = -9990.123901836389
1
Iteration 10500: Loss = -9990.259295314832
2
Iteration 10600: Loss = -9990.12066388845
Iteration 10700: Loss = -9990.156798165537
1
Iteration 10800: Loss = -9990.196688158014
2
Iteration 10900: Loss = -9990.120596761573
Iteration 11000: Loss = -9990.121806325831
1
Iteration 11100: Loss = -9990.120680835656
Iteration 11200: Loss = -9990.120985765321
1
Iteration 11300: Loss = -9990.226641201665
2
Iteration 11400: Loss = -9990.12050272061
Iteration 11500: Loss = -9990.193327449751
1
Iteration 11600: Loss = -9990.120523125208
Iteration 11700: Loss = -9990.161957155447
1
Iteration 11800: Loss = -9990.123103363234
2
Iteration 11900: Loss = -9990.123239980676
3
Iteration 12000: Loss = -9990.123668432363
4
Iteration 12100: Loss = -9990.120907460285
5
Iteration 12200: Loss = -9990.12437672557
6
Iteration 12300: Loss = -9990.121557647459
7
Iteration 12400: Loss = -9990.187768887017
8
Iteration 12500: Loss = -9990.120963110397
9
Iteration 12600: Loss = -9990.120678675572
10
Iteration 12700: Loss = -9990.124560184457
11
Iteration 12800: Loss = -9990.120712730475
12
Iteration 12900: Loss = -9990.133355798727
13
Iteration 13000: Loss = -9990.12056928836
Iteration 13100: Loss = -9990.120509136104
Iteration 13200: Loss = -9990.121082292524
1
Iteration 13300: Loss = -9990.120629942008
2
Iteration 13400: Loss = -9990.123251355855
3
Iteration 13500: Loss = -9990.12075064948
4
Iteration 13600: Loss = -9990.12150202886
5
Iteration 13700: Loss = -9990.157095323935
6
Iteration 13800: Loss = -9990.120764080193
7
Iteration 13900: Loss = -9990.124868569183
8
Iteration 14000: Loss = -9990.18783999685
9
Iteration 14100: Loss = -9990.133329939505
10
Iteration 14200: Loss = -9990.12613815468
11
Iteration 14300: Loss = -9990.120867531452
12
Iteration 14400: Loss = -9990.120594898946
Iteration 14500: Loss = -9990.12130392857
1
Iteration 14600: Loss = -9990.120878490987
2
Iteration 14700: Loss = -9990.120984914522
3
Iteration 14800: Loss = -9990.121201722188
4
Iteration 14900: Loss = -9990.121179357438
5
Iteration 15000: Loss = -9990.134295319063
6
Iteration 15100: Loss = -9990.120529317892
Iteration 15200: Loss = -9990.17453707604
1
Iteration 15300: Loss = -9990.1607375333
2
Iteration 15400: Loss = -9990.161571383454
3
Iteration 15500: Loss = -9990.12253247008
4
Iteration 15600: Loss = -9990.123549790105
5
Iteration 15700: Loss = -9990.126246969226
6
Iteration 15800: Loss = -9990.120458828045
Iteration 15900: Loss = -9990.142799185438
1
Iteration 16000: Loss = -9990.160472181366
2
Iteration 16100: Loss = -9990.120811683406
3
Iteration 16200: Loss = -9990.121179038666
4
Iteration 16300: Loss = -9990.120445337192
Iteration 16400: Loss = -9990.120860436926
1
Iteration 16500: Loss = -9990.141785867658
2
Iteration 16600: Loss = -9990.163761120193
3
Iteration 16700: Loss = -9990.126303805064
4
Iteration 16800: Loss = -9990.120992033393
5
Iteration 16900: Loss = -9990.129533976602
6
Iteration 17000: Loss = -9990.120417803115
Iteration 17100: Loss = -9990.12103551529
1
Iteration 17200: Loss = -9990.120630213367
2
Iteration 17300: Loss = -9990.12357831187
3
Iteration 17400: Loss = -9990.211231430818
4
Iteration 17500: Loss = -9990.126698565115
5
Iteration 17600: Loss = -9990.134783758325
6
Iteration 17700: Loss = -9990.124034540413
7
Iteration 17800: Loss = -9990.120407090868
Iteration 17900: Loss = -9990.12365628714
1
Iteration 18000: Loss = -9990.125161010757
2
Iteration 18100: Loss = -9990.121731597948
3
Iteration 18200: Loss = -9990.23626900369
4
Iteration 18300: Loss = -9990.12192205943
5
Iteration 18400: Loss = -9990.120439145847
Iteration 18500: Loss = -9990.123010598523
1
Iteration 18600: Loss = -9990.122222866003
2
Iteration 18700: Loss = -9990.12102981014
3
Iteration 18800: Loss = -9990.311973451002
4
Iteration 18900: Loss = -9990.120381009054
Iteration 19000: Loss = -9990.123707930035
1
Iteration 19100: Loss = -9990.121034689995
2
Iteration 19200: Loss = -9990.127429611486
3
Iteration 19300: Loss = -9990.12060264896
4
Iteration 19400: Loss = -9990.121282939199
5
Iteration 19500: Loss = -9990.14004013495
6
Iteration 19600: Loss = -9990.24839777
7
Iteration 19700: Loss = -9990.12038504257
Iteration 19800: Loss = -9990.120741923323
1
Iteration 19900: Loss = -9990.12040136013
pi: tensor([[1.0000e+00, 1.1567e-07],
        [5.7874e-01, 4.2126e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.7143e-09, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1356, 0.1200],
         [0.6405, 0.1394]],

        [[0.6182, 0.1406],
         [0.6735, 0.6820]],

        [[0.5877, 0.1379],
         [0.6564, 0.5836]],

        [[0.6678, 0.1803],
         [0.5937, 0.7134]],

        [[0.6216, 0.2045],
         [0.5790, 0.7110]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0007830659075602383
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
Global Adjusted Rand Index: -0.00101657768243428
Average Adjusted Rand Index: -0.0020964607669646555
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24146.49057094897
Iteration 100: Loss = -9992.130904753523
Iteration 200: Loss = -9990.321384111876
Iteration 300: Loss = -9988.638521568944
Iteration 400: Loss = -9986.48702460306
Iteration 500: Loss = -9983.881221961414
Iteration 600: Loss = -9982.8655431228
Iteration 700: Loss = -9981.103597729933
Iteration 800: Loss = -9975.174610688477
Iteration 900: Loss = -9973.617220566157
Iteration 1000: Loss = -9970.571017758888
Iteration 1100: Loss = -9969.019864253038
Iteration 1200: Loss = -9968.063809560688
Iteration 1300: Loss = -9966.269087634639
Iteration 1400: Loss = -9965.975708055421
Iteration 1500: Loss = -9965.781908003213
Iteration 1600: Loss = -9965.381653841365
Iteration 1700: Loss = -9965.092415629377
Iteration 1800: Loss = -9964.887018843281
Iteration 1900: Loss = -9964.851985291716
Iteration 2000: Loss = -9964.845516848978
Iteration 2100: Loss = -9964.844254552572
Iteration 2200: Loss = -9964.843687495177
Iteration 2300: Loss = -9964.843369189319
Iteration 2400: Loss = -9964.843132425796
Iteration 2500: Loss = -9964.842948880716
Iteration 2600: Loss = -9964.842842639122
Iteration 2700: Loss = -9964.842728493213
Iteration 2800: Loss = -9964.842593462741
Iteration 2900: Loss = -9964.842638112708
Iteration 3000: Loss = -9964.842501937339
Iteration 3100: Loss = -9964.842462138487
Iteration 3200: Loss = -9964.842403805033
Iteration 3300: Loss = -9964.842334972951
Iteration 3400: Loss = -9964.842272645721
Iteration 3500: Loss = -9964.8421655665
Iteration 3600: Loss = -9964.842109359366
Iteration 3700: Loss = -9964.84203304469
Iteration 3800: Loss = -9964.846558695832
1
Iteration 3900: Loss = -9964.841703242619
Iteration 4000: Loss = -9964.841378931085
Iteration 4100: Loss = -9964.841026923652
Iteration 4200: Loss = -9964.840677976188
Iteration 4300: Loss = -9964.840565163282
Iteration 4400: Loss = -9964.840516001246
Iteration 4500: Loss = -9964.840488177142
Iteration 4600: Loss = -9964.8409283154
1
Iteration 4700: Loss = -9964.840456326421
Iteration 4800: Loss = -9964.840539888879
Iteration 4900: Loss = -9964.840433337333
Iteration 5000: Loss = -9964.842814303574
1
Iteration 5100: Loss = -9964.840448338799
Iteration 5200: Loss = -9964.840457546936
Iteration 5300: Loss = -9964.840415599245
Iteration 5400: Loss = -9964.840409476488
Iteration 5500: Loss = -9964.84039558679
Iteration 5600: Loss = -9964.840430488062
Iteration 5700: Loss = -9964.840405621628
Iteration 5800: Loss = -9964.840402952894
Iteration 5900: Loss = -9964.840469099858
Iteration 6000: Loss = -9964.840407832344
Iteration 6100: Loss = -9964.840396202046
Iteration 6200: Loss = -9964.844464975735
1
Iteration 6300: Loss = -9964.840404767028
Iteration 6400: Loss = -9964.84039761021
Iteration 6500: Loss = -9964.840737872972
1
Iteration 6600: Loss = -9964.840415412971
Iteration 6700: Loss = -9964.840403391852
Iteration 6800: Loss = -9964.84038654422
Iteration 6900: Loss = -9964.840386990478
Iteration 7000: Loss = -9964.841157246987
1
Iteration 7100: Loss = -9964.840387233275
Iteration 7200: Loss = -9964.840382540468
Iteration 7300: Loss = -9964.84039782696
Iteration 7400: Loss = -9964.840386095379
Iteration 7500: Loss = -9964.894075733435
1
Iteration 7600: Loss = -9964.840385774383
Iteration 7700: Loss = -9964.840420509216
Iteration 7800: Loss = -9964.840443383257
Iteration 7900: Loss = -9964.840436273023
Iteration 8000: Loss = -9964.840938979909
1
Iteration 8100: Loss = -9964.840416198813
Iteration 8200: Loss = -9964.848140006356
1
Iteration 8300: Loss = -9964.840998847767
2
Iteration 8400: Loss = -9964.842998847562
3
Iteration 8500: Loss = -9964.855857364026
4
Iteration 8600: Loss = -9964.840404355837
Iteration 8700: Loss = -9964.840961332799
1
Iteration 8800: Loss = -9964.84193502521
2
Iteration 8900: Loss = -9964.84059883212
3
Iteration 9000: Loss = -9964.840371626953
Iteration 9100: Loss = -9964.841280231258
1
Iteration 9200: Loss = -9964.840905700528
2
Iteration 9300: Loss = -9964.841865142205
3
Iteration 9400: Loss = -9964.840699096718
4
Iteration 9500: Loss = -9964.841378941526
5
Iteration 9600: Loss = -9964.8585851643
6
Iteration 9700: Loss = -9964.840739494106
7
Iteration 9800: Loss = -9964.880533315698
8
Iteration 9900: Loss = -9964.84038814808
Iteration 10000: Loss = -9964.841866471668
1
Iteration 10100: Loss = -9964.887152433588
2
Iteration 10200: Loss = -9964.840397914642
Iteration 10300: Loss = -9964.931873353855
1
Iteration 10400: Loss = -9964.840350720655
Iteration 10500: Loss = -9964.850105213578
1
Iteration 10600: Loss = -9964.84039335059
Iteration 10700: Loss = -9964.85773201458
1
Iteration 10800: Loss = -9964.840385279414
Iteration 10900: Loss = -9964.889036513418
1
Iteration 11000: Loss = -9964.84039388316
Iteration 11100: Loss = -9964.840377895756
Iteration 11200: Loss = -9964.84079670796
1
Iteration 11300: Loss = -9964.840370189439
Iteration 11400: Loss = -9964.841526808123
1
Iteration 11500: Loss = -9964.840382600762
Iteration 11600: Loss = -9964.895071030702
1
Iteration 11700: Loss = -9964.840407126989
Iteration 11800: Loss = -9964.8479713036
1
Iteration 11900: Loss = -9964.840372089182
Iteration 12000: Loss = -9964.854407060653
1
Iteration 12100: Loss = -9964.840413731363
Iteration 12200: Loss = -9964.840400765834
Iteration 12300: Loss = -9964.886735208202
1
Iteration 12400: Loss = -9964.857773069463
2
Iteration 12500: Loss = -9964.84063039079
3
Iteration 12600: Loss = -9964.840769395301
4
Iteration 12700: Loss = -9964.842161749368
5
Iteration 12800: Loss = -9964.84954720125
6
Iteration 12900: Loss = -9964.847151456253
7
Iteration 13000: Loss = -9964.840409242704
Iteration 13100: Loss = -9964.841065310482
1
Iteration 13200: Loss = -9964.848887988004
2
Iteration 13300: Loss = -9964.867555059462
3
Iteration 13400: Loss = -9964.84114426549
4
Iteration 13500: Loss = -9964.85296121603
5
Iteration 13600: Loss = -9964.844289465032
6
Iteration 13700: Loss = -9964.841398378396
7
Iteration 13800: Loss = -9964.843496874153
8
Iteration 13900: Loss = -9964.84858273149
9
Iteration 14000: Loss = -9964.866725582515
10
Iteration 14100: Loss = -9964.909418936373
11
Iteration 14200: Loss = -9964.863416655304
12
Iteration 14300: Loss = -9964.840402485648
Iteration 14400: Loss = -9964.840506950508
1
Iteration 14500: Loss = -9964.845005892546
2
Iteration 14600: Loss = -9964.840381663313
Iteration 14700: Loss = -9964.892938033114
1
Iteration 14800: Loss = -9964.84041048439
Iteration 14900: Loss = -9964.840567394362
1
Iteration 15000: Loss = -9964.973714812337
2
Iteration 15100: Loss = -9964.840376840066
Iteration 15200: Loss = -9964.841092550021
1
Iteration 15300: Loss = -9964.861507631113
2
Iteration 15400: Loss = -9964.84869575806
3
Iteration 15500: Loss = -9964.840571740675
4
Iteration 15600: Loss = -9964.840435431277
Iteration 15700: Loss = -9964.841782711475
1
Iteration 15800: Loss = -9964.842567244606
2
Iteration 15900: Loss = -9964.865424415602
3
Iteration 16000: Loss = -9964.840823100087
4
Iteration 16100: Loss = -9964.84156030524
5
Iteration 16200: Loss = -9964.84044029782
Iteration 16300: Loss = -9964.843968143296
1
Iteration 16400: Loss = -9964.840364765209
Iteration 16500: Loss = -9964.842551151998
1
Iteration 16600: Loss = -9964.912900530144
2
Iteration 16700: Loss = -9964.84038784279
Iteration 16800: Loss = -9964.84077426967
1
Iteration 16900: Loss = -9964.905935306633
2
Iteration 17000: Loss = -9964.840853841304
3
Iteration 17100: Loss = -9964.963074553942
4
Iteration 17200: Loss = -9964.840386894635
Iteration 17300: Loss = -9964.846766017805
1
Iteration 17400: Loss = -9964.849890990496
2
Iteration 17500: Loss = -9964.840382464175
Iteration 17600: Loss = -9964.906889634965
1
Iteration 17700: Loss = -9964.840367108904
Iteration 17800: Loss = -9964.841139713395
1
Iteration 17900: Loss = -9964.840761881595
2
Iteration 18000: Loss = -9964.840438051897
Iteration 18100: Loss = -9964.843827066576
1
Iteration 18200: Loss = -9964.866385347956
2
Iteration 18300: Loss = -9964.840437976012
Iteration 18400: Loss = -9964.840502564428
Iteration 18500: Loss = -9964.842808192849
1
Iteration 18600: Loss = -9964.85671526413
2
Iteration 18700: Loss = -9964.840585622924
Iteration 18800: Loss = -9964.84058564829
Iteration 18900: Loss = -9964.941058347506
1
Iteration 19000: Loss = -9964.840388818258
Iteration 19100: Loss = -9964.840596275328
1
Iteration 19200: Loss = -9964.84041866745
Iteration 19300: Loss = -9964.842950632132
1
Iteration 19400: Loss = -9964.840381758757
Iteration 19500: Loss = -9964.840681233138
1
Iteration 19600: Loss = -9964.844113053627
2
Iteration 19700: Loss = -9964.842926897854
3
Iteration 19800: Loss = -9964.842398573905
4
Iteration 19900: Loss = -9964.845052148881
5
pi: tensor([[0.8030, 0.1970],
        [0.0929, 0.9071]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7191, 0.2809], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1387, 0.1255],
         [0.5001, 0.2101]],

        [[0.6082, 0.1156],
         [0.5546, 0.6108]],

        [[0.5429, 0.1065],
         [0.6488, 0.7293]],

        [[0.6601, 0.0987],
         [0.5482, 0.6978]],

        [[0.5580, 0.0964],
         [0.6545, 0.5353]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 69
Adjusted Rand Index: 0.1375316154099923
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 78
Adjusted Rand Index: 0.3068448883666275
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 85
Adjusted Rand Index: 0.4848303190856317
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 85
Adjusted Rand Index: 0.4848706857078213
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7368923834368226
Global Adjusted Rand Index: 0.40841953654722823
Average Adjusted Rand Index: 0.4301939784013791
10088.981300397678
[-0.00101657768243428, 0.40841953654722823] [-0.0020964607669646555, 0.4301939784013791] [9990.124676772002, 9964.858680845477]
-------------------------------------
This iteration is 36
True Objective function: Loss = -9981.457698707021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23883.055700536108
Iteration 100: Loss = -9871.59591396787
Iteration 200: Loss = -9870.015457326475
Iteration 300: Loss = -9869.597646060738
Iteration 400: Loss = -9869.394325812158
Iteration 500: Loss = -9869.2782783002
Iteration 600: Loss = -9869.20318986275
Iteration 700: Loss = -9869.143223975323
Iteration 800: Loss = -9869.062442903774
Iteration 900: Loss = -9868.620656118717
Iteration 1000: Loss = -9866.090811643337
Iteration 1100: Loss = -9865.35397558874
Iteration 1200: Loss = -9865.110574283388
Iteration 1300: Loss = -9864.913445581702
Iteration 1400: Loss = -9864.04406587509
Iteration 1500: Loss = -9862.105762042607
Iteration 1600: Loss = -9861.916915190579
Iteration 1700: Loss = -9861.823219068441
Iteration 1800: Loss = -9861.746712701146
Iteration 1900: Loss = -9861.667961702691
Iteration 2000: Loss = -9861.577281290374
Iteration 2100: Loss = -9861.467744927151
Iteration 2200: Loss = -9861.315772147842
Iteration 2300: Loss = -9860.694747835716
Iteration 2400: Loss = -9858.62302439867
Iteration 2500: Loss = -9857.904149873473
Iteration 2600: Loss = -9857.781032643754
Iteration 2700: Loss = -9857.705072597346
Iteration 2800: Loss = -9857.577811525345
Iteration 2900: Loss = -9857.197580048476
Iteration 3000: Loss = -9857.13794923093
Iteration 3100: Loss = -9857.1278595347
Iteration 3200: Loss = -9857.123315644836
Iteration 3300: Loss = -9857.120529421494
Iteration 3400: Loss = -9857.11846850929
Iteration 3500: Loss = -9857.116911960302
Iteration 3600: Loss = -9857.115619426164
Iteration 3700: Loss = -9857.114556254093
Iteration 3800: Loss = -9857.113830402724
Iteration 3900: Loss = -9857.112854028994
Iteration 4000: Loss = -9857.11220716086
Iteration 4100: Loss = -9857.111654396214
Iteration 4200: Loss = -9857.111143346068
Iteration 4300: Loss = -9857.111938423472
1
Iteration 4400: Loss = -9857.110257475153
Iteration 4500: Loss = -9857.109895895024
Iteration 4600: Loss = -9857.109597369095
Iteration 4700: Loss = -9857.109266286237
Iteration 4800: Loss = -9857.110936289813
1
Iteration 4900: Loss = -9857.108739660132
Iteration 5000: Loss = -9857.108518855044
Iteration 5100: Loss = -9857.108786735138
1
Iteration 5200: Loss = -9857.108119124365
Iteration 5300: Loss = -9857.116801246328
1
Iteration 5400: Loss = -9857.107754212035
Iteration 5500: Loss = -9857.107597904022
Iteration 5600: Loss = -9857.107450654828
Iteration 5700: Loss = -9857.107324259636
Iteration 5800: Loss = -9857.118198560556
1
Iteration 5900: Loss = -9857.107106608417
Iteration 6000: Loss = -9857.106963685497
Iteration 6100: Loss = -9857.106989397433
Iteration 6200: Loss = -9857.10678028726
Iteration 6300: Loss = -9857.106683468646
Iteration 6400: Loss = -9857.10659271711
Iteration 6500: Loss = -9857.10650102022
Iteration 6600: Loss = -9857.106921020795
1
Iteration 6700: Loss = -9857.10640125178
Iteration 6800: Loss = -9857.106335087188
Iteration 6900: Loss = -9857.106268466763
Iteration 7000: Loss = -9857.106183152975
Iteration 7100: Loss = -9857.107083978755
1
Iteration 7200: Loss = -9857.106070705537
Iteration 7300: Loss = -9857.106027329255
Iteration 7400: Loss = -9857.105966654739
Iteration 7500: Loss = -9857.105947824528
Iteration 7600: Loss = -9857.10686061592
1
Iteration 7700: Loss = -9857.10588296254
Iteration 7800: Loss = -9857.122300846338
1
Iteration 7900: Loss = -9857.105803797744
Iteration 8000: Loss = -9857.105775473088
Iteration 8100: Loss = -9857.112769982034
1
Iteration 8200: Loss = -9857.105667761847
Iteration 8300: Loss = -9857.105640848664
Iteration 8400: Loss = -9857.107281797173
1
Iteration 8500: Loss = -9857.105585343597
Iteration 8600: Loss = -9857.15842008814
1
Iteration 8700: Loss = -9857.105568408535
Iteration 8800: Loss = -9857.10554589236
Iteration 8900: Loss = -9857.105519131906
Iteration 9000: Loss = -9857.115245673282
1
Iteration 9100: Loss = -9857.105498670493
Iteration 9200: Loss = -9857.10556292955
Iteration 9300: Loss = -9857.105474487124
Iteration 9400: Loss = -9857.107302003082
1
Iteration 9500: Loss = -9857.113443187707
2
Iteration 9600: Loss = -9857.105587172084
3
Iteration 9700: Loss = -9857.105376561918
Iteration 9800: Loss = -9857.135744274954
1
Iteration 9900: Loss = -9857.10535677835
Iteration 10000: Loss = -9857.107139421314
1
Iteration 10100: Loss = -9857.106692008867
2
Iteration 10200: Loss = -9857.251321270247
3
Iteration 10300: Loss = -9857.105318378526
Iteration 10400: Loss = -9857.105356733799
Iteration 10500: Loss = -9857.108768334647
1
Iteration 10600: Loss = -9857.105550480232
2
Iteration 10700: Loss = -9857.105635312853
3
Iteration 10800: Loss = -9857.105965644594
4
Iteration 10900: Loss = -9857.108432327666
5
Iteration 11000: Loss = -9857.105583570725
6
Iteration 11100: Loss = -9857.105767385385
7
Iteration 11200: Loss = -9857.11908559386
8
Iteration 11300: Loss = -9857.105197878685
Iteration 11400: Loss = -9857.105813875633
1
Iteration 11500: Loss = -9857.105658755074
2
Iteration 11600: Loss = -9857.105629979294
3
Iteration 11700: Loss = -9857.10880144678
4
Iteration 11800: Loss = -9857.105878572878
5
Iteration 11900: Loss = -9857.13649730524
6
Iteration 12000: Loss = -9857.105995355358
7
Iteration 12100: Loss = -9857.105185024364
Iteration 12200: Loss = -9857.11606602323
1
Iteration 12300: Loss = -9857.105528357739
2
Iteration 12400: Loss = -9857.105238211048
Iteration 12500: Loss = -9857.105405112818
1
Iteration 12600: Loss = -9857.1085981998
2
Iteration 12700: Loss = -9857.105138757039
Iteration 12800: Loss = -9857.105326364797
1
Iteration 12900: Loss = -9857.105149134142
Iteration 13000: Loss = -9857.305248820565
1
Iteration 13100: Loss = -9857.105138181514
Iteration 13200: Loss = -9857.105717680532
1
Iteration 13300: Loss = -9857.105835289143
2
Iteration 13400: Loss = -9857.105912401905
3
Iteration 13500: Loss = -9857.106843647865
4
Iteration 13600: Loss = -9857.172722254863
5
Iteration 13700: Loss = -9857.105327591718
6
Iteration 13800: Loss = -9857.105121900417
Iteration 13900: Loss = -9857.105303150724
1
Iteration 14000: Loss = -9857.105885147821
2
Iteration 14100: Loss = -9857.109815403282
3
Iteration 14200: Loss = -9857.105296288975
4
Iteration 14300: Loss = -9857.105394284401
5
Iteration 14400: Loss = -9857.105210865684
Iteration 14500: Loss = -9857.105402346233
1
Iteration 14600: Loss = -9857.114725488094
2
Iteration 14700: Loss = -9857.26419567579
3
Iteration 14800: Loss = -9857.105142871216
Iteration 14900: Loss = -9857.105163969949
Iteration 15000: Loss = -9857.10584500268
1
Iteration 15100: Loss = -9857.150164142844
2
Iteration 15200: Loss = -9857.105105597062
Iteration 15300: Loss = -9857.105085757565
Iteration 15400: Loss = -9857.105324454713
1
Iteration 15500: Loss = -9857.105476359027
2
Iteration 15600: Loss = -9857.110537536975
3
Iteration 15700: Loss = -9857.152466156362
4
Iteration 15800: Loss = -9857.133052753796
5
Iteration 15900: Loss = -9857.108685913692
6
Iteration 16000: Loss = -9857.107516257936
7
Iteration 16100: Loss = -9857.114794462179
8
Iteration 16200: Loss = -9857.105751389698
9
Iteration 16300: Loss = -9857.115075258185
10
Iteration 16400: Loss = -9857.108580625872
11
Iteration 16500: Loss = -9857.110467804405
12
Iteration 16600: Loss = -9857.10971428983
13
Iteration 16700: Loss = -9857.186449667604
14
Iteration 16800: Loss = -9857.118236557844
15
Stopping early at iteration 16800 due to no improvement.
pi: tensor([[1.6400e-07, 1.0000e+00],
        [3.0017e-02, 9.6998e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5792, 0.4208], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1226, 0.1336],
         [0.5991, 0.1409]],

        [[0.6097, 0.2807],
         [0.7258, 0.7128]],

        [[0.5900, 0.0377],
         [0.5713, 0.5565]],

        [[0.6900, 0.0498],
         [0.6676, 0.6912]],

        [[0.6899, 0.0897],
         [0.5747, 0.6414]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.006767238045575733
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.021933873838361234
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.005643926093476274
Average Adjusted Rand Index: 0.005309288138753666
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23170.278298918212
Iteration 100: Loss = -9870.479948932398
Iteration 200: Loss = -9869.549998427447
Iteration 300: Loss = -9869.350961367027
Iteration 400: Loss = -9869.250495996914
Iteration 500: Loss = -9869.189918136553
Iteration 600: Loss = -9869.148239776314
Iteration 700: Loss = -9869.115947035116
Iteration 800: Loss = -9869.086579375518
Iteration 900: Loss = -9869.050307600823
Iteration 1000: Loss = -9868.963938507593
Iteration 1100: Loss = -9867.252426110746
Iteration 1200: Loss = -9862.881308075654
Iteration 1300: Loss = -9862.111907851548
Iteration 1400: Loss = -9861.92644743574
Iteration 1500: Loss = -9861.796493637126
Iteration 1600: Loss = -9861.660574524862
Iteration 1700: Loss = -9861.482837435242
Iteration 1800: Loss = -9861.164916650165
Iteration 1900: Loss = -9860.62402090456
Iteration 2000: Loss = -9860.360607686165
Iteration 2100: Loss = -9860.080340243689
Iteration 2200: Loss = -9859.121911225597
Iteration 2300: Loss = -9857.292845459095
Iteration 2400: Loss = -9857.166643598774
Iteration 2500: Loss = -9857.135671943126
Iteration 2600: Loss = -9857.1246796637
Iteration 2700: Loss = -9857.119544517418
Iteration 2800: Loss = -9857.11665574247
Iteration 2900: Loss = -9857.114726590862
Iteration 3000: Loss = -9857.11336608989
Iteration 3100: Loss = -9857.112291478754
Iteration 3200: Loss = -9857.111490670528
Iteration 3300: Loss = -9857.110797988149
Iteration 3400: Loss = -9857.112711347854
1
Iteration 3500: Loss = -9857.109763468066
Iteration 3600: Loss = -9857.109356930176
Iteration 3700: Loss = -9857.109041693247
Iteration 3800: Loss = -9857.108714485352
Iteration 3900: Loss = -9857.11010770814
1
Iteration 4000: Loss = -9857.108167164286
Iteration 4100: Loss = -9857.107949350977
Iteration 4200: Loss = -9857.107811206604
Iteration 4300: Loss = -9857.10756644507
Iteration 4400: Loss = -9857.108868301642
1
Iteration 4500: Loss = -9857.107291292901
Iteration 4600: Loss = -9857.107105074725
Iteration 4700: Loss = -9857.109475418765
1
Iteration 4800: Loss = -9857.106884249491
Iteration 4900: Loss = -9857.106775599344
Iteration 5000: Loss = -9857.106726856955
Iteration 5100: Loss = -9857.10660254461
Iteration 5200: Loss = -9857.121546481612
1
Iteration 5300: Loss = -9857.106406855451
Iteration 5400: Loss = -9857.106359601581
Iteration 5500: Loss = -9857.106389538732
Iteration 5600: Loss = -9857.106223723897
Iteration 5700: Loss = -9857.110633573871
1
Iteration 5800: Loss = -9857.106113335489
Iteration 5900: Loss = -9857.106060337343
Iteration 6000: Loss = -9857.105999646765
Iteration 6100: Loss = -9857.10594587163
Iteration 6200: Loss = -9857.105884184439
Iteration 6300: Loss = -9857.105843597466
Iteration 6400: Loss = -9857.105814491095
Iteration 6500: Loss = -9857.105758925145
Iteration 6600: Loss = -9857.105730518922
Iteration 6700: Loss = -9857.10574280089
Iteration 6800: Loss = -9857.105659721583
Iteration 6900: Loss = -9857.112964995722
1
Iteration 7000: Loss = -9857.10556731259
Iteration 7100: Loss = -9857.105598832826
Iteration 7200: Loss = -9857.105581799293
Iteration 7300: Loss = -9857.105513485112
Iteration 7400: Loss = -9857.105549915095
Iteration 7500: Loss = -9857.105492140006
Iteration 7600: Loss = -9857.108074879994
1
Iteration 7700: Loss = -9857.105473207186
Iteration 7800: Loss = -9857.107489349273
1
Iteration 7900: Loss = -9857.105425194406
Iteration 8000: Loss = -9857.105379183478
Iteration 8100: Loss = -9857.106077247507
1
Iteration 8200: Loss = -9857.105366569553
Iteration 8300: Loss = -9857.109576425797
1
Iteration 8400: Loss = -9857.10532108138
Iteration 8500: Loss = -9857.16380641499
1
Iteration 8600: Loss = -9857.105352205224
Iteration 8700: Loss = -9857.105455013036
1
Iteration 8800: Loss = -9857.107653834555
2
Iteration 8900: Loss = -9857.105240645522
Iteration 9000: Loss = -9857.105575338259
1
Iteration 9100: Loss = -9857.10526859808
Iteration 9200: Loss = -9857.105287506783
Iteration 9300: Loss = -9857.142461604259
1
Iteration 9400: Loss = -9857.105238179971
Iteration 9500: Loss = -9857.109144408696
1
Iteration 9600: Loss = -9857.182728137974
2
Iteration 9700: Loss = -9857.10542240055
3
Iteration 9800: Loss = -9857.10530624078
Iteration 9900: Loss = -9857.1104980564
1
Iteration 10000: Loss = -9857.10513689241
Iteration 10100: Loss = -9857.105819438142
1
Iteration 10200: Loss = -9857.105199924099
Iteration 10300: Loss = -9857.105203114483
Iteration 10400: Loss = -9857.107686844844
1
Iteration 10500: Loss = -9857.112987383136
2
Iteration 10600: Loss = -9857.10731414731
3
Iteration 10700: Loss = -9857.108012699839
4
Iteration 10800: Loss = -9857.105401683446
5
Iteration 10900: Loss = -9857.105226764912
Iteration 11000: Loss = -9857.107989949958
1
Iteration 11100: Loss = -9857.106695319497
2
Iteration 11200: Loss = -9857.105238103843
Iteration 11300: Loss = -9857.10516210032
Iteration 11400: Loss = -9857.106586255233
1
Iteration 11500: Loss = -9857.105767249446
2
Iteration 11600: Loss = -9857.105213971889
Iteration 11700: Loss = -9857.10745894169
1
Iteration 11800: Loss = -9857.105219694078
Iteration 11900: Loss = -9857.10526749213
Iteration 12000: Loss = -9857.106412799207
1
Iteration 12100: Loss = -9857.10544910762
2
Iteration 12200: Loss = -9857.105256118171
Iteration 12300: Loss = -9857.111846058846
1
Iteration 12400: Loss = -9857.171704133623
2
Iteration 12500: Loss = -9857.105879754628
3
Iteration 12600: Loss = -9857.111009913835
4
Iteration 12700: Loss = -9857.169690956754
5
Iteration 12800: Loss = -9857.105765573879
6
Iteration 12900: Loss = -9857.105200189533
Iteration 13000: Loss = -9857.105134881089
Iteration 13100: Loss = -9857.340885064872
1
Iteration 13200: Loss = -9857.105083968534
Iteration 13300: Loss = -9857.124455798541
1
Iteration 13400: Loss = -9857.115911089224
2
Iteration 13500: Loss = -9857.107178150973
3
Iteration 13600: Loss = -9857.107845856912
4
Iteration 13700: Loss = -9857.120051094433
5
Iteration 13800: Loss = -9857.117672475923
6
Iteration 13900: Loss = -9857.183806446928
7
Iteration 14000: Loss = -9857.118183096443
8
Iteration 14100: Loss = -9857.157901892517
9
Iteration 14200: Loss = -9857.132627256704
10
Iteration 14300: Loss = -9857.204714473588
11
Iteration 14400: Loss = -9857.236116747077
12
Iteration 14500: Loss = -9857.105609870405
13
Iteration 14600: Loss = -9857.10517942313
Iteration 14700: Loss = -9857.105712547498
1
Iteration 14800: Loss = -9857.107912951744
2
Iteration 14900: Loss = -9857.105157181159
Iteration 15000: Loss = -9857.105172767115
Iteration 15100: Loss = -9857.105200933582
Iteration 15200: Loss = -9857.105436025384
1
Iteration 15300: Loss = -9857.106812477763
2
Iteration 15400: Loss = -9857.105137274097
Iteration 15500: Loss = -9857.106746736348
1
Iteration 15600: Loss = -9857.111069459004
2
Iteration 15700: Loss = -9857.122121707402
3
Iteration 15800: Loss = -9857.10584045454
4
Iteration 15900: Loss = -9857.112816504528
5
Iteration 16000: Loss = -9857.10558774224
6
Iteration 16100: Loss = -9857.107454660803
7
Iteration 16200: Loss = -9857.105513691915
8
Iteration 16300: Loss = -9857.111397184375
9
Iteration 16400: Loss = -9857.106235020363
10
Iteration 16500: Loss = -9857.11802078652
11
Iteration 16600: Loss = -9857.105117190285
Iteration 16700: Loss = -9857.118550683717
1
Iteration 16800: Loss = -9857.105116323808
Iteration 16900: Loss = -9857.114301477659
1
Iteration 17000: Loss = -9857.106914174428
2
Iteration 17100: Loss = -9857.105124196738
Iteration 17200: Loss = -9857.121999861643
1
Iteration 17300: Loss = -9857.105658954859
2
Iteration 17400: Loss = -9857.109922300924
3
Iteration 17500: Loss = -9857.128946741255
4
Iteration 17600: Loss = -9857.105682925032
5
Iteration 17700: Loss = -9857.107152874676
6
Iteration 17800: Loss = -9857.13288751907
7
Iteration 17900: Loss = -9857.10507391357
Iteration 18000: Loss = -9857.116636854389
1
Iteration 18100: Loss = -9857.105129881948
Iteration 18200: Loss = -9857.105178196767
Iteration 18300: Loss = -9857.160905030887
1
Iteration 18400: Loss = -9857.106997573263
2
Iteration 18500: Loss = -9857.133309587767
3
Iteration 18600: Loss = -9857.109904284373
4
Iteration 18700: Loss = -9857.105052201443
Iteration 18800: Loss = -9857.128852889466
1
Iteration 18900: Loss = -9857.161460565729
2
Iteration 19000: Loss = -9857.10526325067
3
Iteration 19100: Loss = -9857.109643720782
4
Iteration 19200: Loss = -9857.110630381732
5
Iteration 19300: Loss = -9857.108057224626
6
Iteration 19400: Loss = -9857.105067966682
Iteration 19500: Loss = -9857.1178431666
1
Iteration 19600: Loss = -9857.10507828532
Iteration 19700: Loss = -9857.107679851624
1
Iteration 19800: Loss = -9857.105077851353
Iteration 19900: Loss = -9857.110579536824
1
pi: tensor([[1.6296e-08, 1.0000e+00],
        [2.9992e-02, 9.7001e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5762, 0.4238], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1226, 0.1332],
         [0.7098, 0.1409]],

        [[0.5887, 0.2799],
         [0.6990, 0.6425]],

        [[0.7195, 0.0377],
         [0.6412, 0.6562]],

        [[0.7205, 0.0498],
         [0.6649, 0.7199]],

        [[0.7125, 0.0900],
         [0.5062, 0.5075]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.006767238045575733
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.021933873838361234
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.005643926093476274
Average Adjusted Rand Index: 0.005309288138753666
9981.457698707021
[0.005643926093476274, 0.005643926093476274] [0.005309288138753666, 0.005309288138753666] [9857.118236557844, 9857.107309880072]
-------------------------------------
This iteration is 37
True Objective function: Loss = -9859.087540471859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25628.104008170267
Iteration 100: Loss = -9722.580467483898
Iteration 200: Loss = -9721.45848414256
Iteration 300: Loss = -9721.077583224145
Iteration 400: Loss = -9720.894872693158
Iteration 500: Loss = -9720.793813611563
Iteration 600: Loss = -9720.733545584479
Iteration 700: Loss = -9720.694540443554
Iteration 800: Loss = -9720.667513317569
Iteration 900: Loss = -9720.647975040885
Iteration 1000: Loss = -9720.633427396004
Iteration 1100: Loss = -9720.622311753963
Iteration 1200: Loss = -9720.613644150902
Iteration 1300: Loss = -9720.606775968301
Iteration 1400: Loss = -9720.60119547067
Iteration 1500: Loss = -9720.596590565023
Iteration 1600: Loss = -9720.592704007055
Iteration 1700: Loss = -9720.589457445205
Iteration 1800: Loss = -9720.586722885138
Iteration 1900: Loss = -9720.584343165148
Iteration 2000: Loss = -9720.582305349266
Iteration 2100: Loss = -9720.580528764269
Iteration 2200: Loss = -9720.578992570348
Iteration 2300: Loss = -9720.577572264658
Iteration 2400: Loss = -9720.576388555874
Iteration 2500: Loss = -9720.575257800327
Iteration 2600: Loss = -9720.574255687383
Iteration 2700: Loss = -9720.57338513131
Iteration 2800: Loss = -9720.57256165736
Iteration 2900: Loss = -9720.571801781498
Iteration 3000: Loss = -9720.571127699566
Iteration 3100: Loss = -9720.570494058773
Iteration 3200: Loss = -9720.569940882877
Iteration 3300: Loss = -9720.56942843336
Iteration 3400: Loss = -9720.568933125844
Iteration 3500: Loss = -9720.56846943689
Iteration 3600: Loss = -9720.56804813125
Iteration 3700: Loss = -9720.56763347653
Iteration 3800: Loss = -9720.5672387034
Iteration 3900: Loss = -9720.566892992782
Iteration 4000: Loss = -9720.566523604315
Iteration 4100: Loss = -9720.566189087309
Iteration 4200: Loss = -9720.565803283398
Iteration 4300: Loss = -9720.56545392164
Iteration 4400: Loss = -9720.565063436563
Iteration 4500: Loss = -9720.564708814996
Iteration 4600: Loss = -9720.564218709473
Iteration 4700: Loss = -9720.56368301806
Iteration 4800: Loss = -9720.56306862727
Iteration 4900: Loss = -9720.562213191555
Iteration 5000: Loss = -9720.560752452357
Iteration 5100: Loss = -9720.557346630669
Iteration 5200: Loss = -9720.521659772063
Iteration 5300: Loss = -9718.041648850047
Iteration 5400: Loss = -9717.929433955474
Iteration 5500: Loss = -9717.90788395411
Iteration 5600: Loss = -9717.898171365832
Iteration 5700: Loss = -9717.892548824871
Iteration 5800: Loss = -9717.888906763446
Iteration 5900: Loss = -9717.886387574878
Iteration 6000: Loss = -9717.884517783761
Iteration 6100: Loss = -9717.883044937744
Iteration 6200: Loss = -9717.881908456247
Iteration 6300: Loss = -9717.880948927324
Iteration 6400: Loss = -9717.880175357543
Iteration 6500: Loss = -9717.879544009245
Iteration 6600: Loss = -9717.878958962745
Iteration 6700: Loss = -9717.87853399647
Iteration 6800: Loss = -9717.878093146219
Iteration 6900: Loss = -9717.877761376914
Iteration 7000: Loss = -9717.877423373444
Iteration 7100: Loss = -9717.877116212065
Iteration 7200: Loss = -9717.876909466
Iteration 7300: Loss = -9717.876710036486
Iteration 7400: Loss = -9717.876484481014
Iteration 7500: Loss = -9717.876268305547
Iteration 7600: Loss = -9717.876131295116
Iteration 7700: Loss = -9717.875972570695
Iteration 7800: Loss = -9717.875873755127
Iteration 7900: Loss = -9717.87573402268
Iteration 8000: Loss = -9717.875607730772
Iteration 8100: Loss = -9717.875500088157
Iteration 8200: Loss = -9717.875398185237
Iteration 8300: Loss = -9717.875326780482
Iteration 8400: Loss = -9717.875248546216
Iteration 8500: Loss = -9717.875173725246
Iteration 8600: Loss = -9717.875070077742
Iteration 8700: Loss = -9717.875027417655
Iteration 8800: Loss = -9717.874976156956
Iteration 8900: Loss = -9717.874903035186
Iteration 9000: Loss = -9717.87484545065
Iteration 9100: Loss = -9717.87478306012
Iteration 9200: Loss = -9717.874848272631
Iteration 9300: Loss = -9717.874717676135
Iteration 9400: Loss = -9717.87466473607
Iteration 9500: Loss = -9717.874604036986
Iteration 9600: Loss = -9717.874591540936
Iteration 9700: Loss = -9717.917526207979
1
Iteration 9800: Loss = -9717.87452431397
Iteration 9900: Loss = -9717.87448789872
Iteration 10000: Loss = -9717.874477287976
Iteration 10100: Loss = -9717.874442993047
Iteration 10200: Loss = -9717.874601910848
1
Iteration 10300: Loss = -9717.874408281852
Iteration 10400: Loss = -9717.874368495492
Iteration 10500: Loss = -9717.874346136838
Iteration 10600: Loss = -9717.874437227601
Iteration 10700: Loss = -9717.87425475336
Iteration 10800: Loss = -9717.874198934667
Iteration 10900: Loss = -9717.874169134957
Iteration 11000: Loss = -9717.876297948513
1
Iteration 11100: Loss = -9717.874210378055
Iteration 11200: Loss = -9717.874223550209
Iteration 11300: Loss = -9717.87422436975
Iteration 11400: Loss = -9717.874479827444
1
Iteration 11500: Loss = -9717.874224902307
Iteration 11600: Loss = -9717.874214413498
Iteration 11700: Loss = -9717.882187308398
1
Iteration 11800: Loss = -9717.87421584604
Iteration 11900: Loss = -9717.87422928764
Iteration 12000: Loss = -9717.874209404938
Iteration 12100: Loss = -9717.89775784853
1
Iteration 12200: Loss = -9717.874237744538
Iteration 12300: Loss = -9717.874215071966
Iteration 12400: Loss = -9717.874204168993
Iteration 12500: Loss = -9717.878150792592
1
Iteration 12600: Loss = -9717.874199386182
Iteration 12700: Loss = -9717.874191548295
Iteration 12800: Loss = -9717.87421165482
Iteration 12900: Loss = -9717.89674002059
1
Iteration 13000: Loss = -9717.874242356651
Iteration 13100: Loss = -9717.874248679114
Iteration 13200: Loss = -9717.87424990371
Iteration 13300: Loss = -9717.874716927283
1
Iteration 13400: Loss = -9717.874258162105
Iteration 13500: Loss = -9717.874258132942
Iteration 13600: Loss = -9717.874236293985
Iteration 13700: Loss = -9717.981926446935
1
Iteration 13800: Loss = -9717.874219437192
Iteration 13900: Loss = -9717.874202380532
Iteration 14000: Loss = -9717.874207069975
Iteration 14100: Loss = -9717.874196991686
Iteration 14200: Loss = -9717.874275965798
Iteration 14300: Loss = -9717.874179711023
Iteration 14400: Loss = -9717.87416980285
Iteration 14500: Loss = -9717.874156909187
Iteration 14600: Loss = -9717.896143850221
1
Iteration 14700: Loss = -9717.874136824077
Iteration 14800: Loss = -9717.874114964143
Iteration 14900: Loss = -9717.874101941417
Iteration 15000: Loss = -9718.124198293817
1
Iteration 15100: Loss = -9717.874078419234
Iteration 15200: Loss = -9717.874058951342
Iteration 15300: Loss = -9717.874046744902
Iteration 15400: Loss = -9717.874045309032
Iteration 15500: Loss = -9717.87412529817
Iteration 15600: Loss = -9717.874055141632
Iteration 15700: Loss = -9717.874069524281
Iteration 15800: Loss = -9717.874055767657
Iteration 15900: Loss = -9717.874134756812
Iteration 16000: Loss = -9717.874075501111
Iteration 16100: Loss = -9717.875109264813
1
Iteration 16200: Loss = -9717.877963088138
2
Iteration 16300: Loss = -9717.874197268487
3
Iteration 16400: Loss = -9717.874095013189
Iteration 16500: Loss = -9717.874097135185
Iteration 16600: Loss = -9717.877015258524
1
Iteration 16700: Loss = -9717.874189674782
Iteration 16800: Loss = -9717.874106320938
Iteration 16900: Loss = -9717.87410510187
Iteration 17000: Loss = -9717.875143803174
1
Iteration 17100: Loss = -9717.874769632626
2
Iteration 17200: Loss = -9717.874098579076
Iteration 17300: Loss = -9717.874091286934
Iteration 17400: Loss = -9717.874949628358
1
Iteration 17500: Loss = -9717.876527893848
2
Iteration 17600: Loss = -9717.874086339027
Iteration 17700: Loss = -9717.874076509883
Iteration 17800: Loss = -9717.883271751003
1
Iteration 17900: Loss = -9717.874066800661
Iteration 18000: Loss = -9717.874067607732
Iteration 18100: Loss = -9717.874063528947
Iteration 18200: Loss = -9717.876013059491
1
Iteration 18300: Loss = -9717.874054305323
Iteration 18400: Loss = -9717.875255099505
1
Iteration 18500: Loss = -9717.874046793839
Iteration 18600: Loss = -9717.878684214456
1
Iteration 18700: Loss = -9717.874041901141
Iteration 18800: Loss = -9717.874037264213
Iteration 18900: Loss = -9717.874039661785
Iteration 19000: Loss = -9717.87418131031
1
Iteration 19100: Loss = -9717.874323779477
2
Iteration 19200: Loss = -9717.874026144555
Iteration 19300: Loss = -9717.87440671397
1
Iteration 19400: Loss = -9717.874320622772
2
Iteration 19500: Loss = -9717.874020749641
Iteration 19600: Loss = -9717.874392252937
1
Iteration 19700: Loss = -9718.127072967554
2
Iteration 19800: Loss = -9717.874039056269
Iteration 19900: Loss = -9717.874065871976
pi: tensor([[1.0000e+00, 2.5643e-07],
        [1.7601e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0097, 0.9903], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2414, 0.0303],
         [0.5789, 0.1336]],

        [[0.5947, 0.2020],
         [0.7199, 0.6203]],

        [[0.5744, 0.1212],
         [0.5322, 0.6873]],

        [[0.5064, 0.1313],
         [0.7034, 0.5736]],

        [[0.5046, 0.1212],
         [0.6001, 0.7205]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 35
Adjusted Rand Index: -0.009657204736290852
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 0.00186969706031488
Average Adjusted Rand Index: -0.0010582697668140913
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22331.047431030216
Iteration 100: Loss = -9722.224789695498
Iteration 200: Loss = -9721.242217367682
Iteration 300: Loss = -9720.910279312242
Iteration 400: Loss = -9720.761481254754
Iteration 500: Loss = -9720.687165869054
Iteration 600: Loss = -9720.645241240109
Iteration 700: Loss = -9720.618730297574
Iteration 800: Loss = -9720.600139340333
Iteration 900: Loss = -9720.585886618448
Iteration 1000: Loss = -9720.574136567182
Iteration 1100: Loss = -9720.563794484733
Iteration 1200: Loss = -9720.554078058485
Iteration 1300: Loss = -9720.544520005411
Iteration 1400: Loss = -9720.534519105722
Iteration 1500: Loss = -9720.523536405206
Iteration 1600: Loss = -9720.510905644784
Iteration 1700: Loss = -9720.495804325627
Iteration 1800: Loss = -9720.477207415135
Iteration 1900: Loss = -9720.454352153793
Iteration 2000: Loss = -9720.427161404235
Iteration 2100: Loss = -9720.397061608866
Iteration 2200: Loss = -9720.363849213993
Iteration 2300: Loss = -9720.322211289693
Iteration 2400: Loss = -9720.263001347283
Iteration 2500: Loss = -9720.177620320408
Iteration 2600: Loss = -9720.070518436765
Iteration 2700: Loss = -9719.958032436753
Iteration 2800: Loss = -9719.845864045621
Iteration 2900: Loss = -9719.733977302141
Iteration 3000: Loss = -9719.610272089385
Iteration 3100: Loss = -9719.470432388738
Iteration 3200: Loss = -9719.335149853909
Iteration 3300: Loss = -9719.26219513001
Iteration 3400: Loss = -9719.230824280334
Iteration 3500: Loss = -9719.215337803847
Iteration 3600: Loss = -9719.214524820129
Iteration 3700: Loss = -9719.198870023298
Iteration 3800: Loss = -9719.193831658698
Iteration 3900: Loss = -9719.190512242436
Iteration 4000: Loss = -9719.186840278644
Iteration 4100: Loss = -9719.184363136743
Iteration 4200: Loss = -9719.182344822508
Iteration 4300: Loss = -9719.180543771232
Iteration 4400: Loss = -9719.185509768864
1
Iteration 4500: Loss = -9719.17778189616
Iteration 4600: Loss = -9719.176684223787
Iteration 4700: Loss = -9719.175723977303
Iteration 4800: Loss = -9719.174851070433
Iteration 4900: Loss = -9719.17408747475
Iteration 5000: Loss = -9719.173381262111
Iteration 5100: Loss = -9719.172761299358
Iteration 5200: Loss = -9719.172656958948
Iteration 5300: Loss = -9719.171730775635
Iteration 5400: Loss = -9719.171261909712
Iteration 5500: Loss = -9719.17084286028
Iteration 5600: Loss = -9719.170503727273
Iteration 5700: Loss = -9719.170499991387
Iteration 5800: Loss = -9719.16985543073
Iteration 5900: Loss = -9719.16999600841
1
Iteration 6000: Loss = -9719.169296988015
Iteration 6100: Loss = -9719.169634556914
1
Iteration 6200: Loss = -9719.168817989725
Iteration 6300: Loss = -9719.168682874215
Iteration 6400: Loss = -9719.168468277087
Iteration 6500: Loss = -9719.16835057618
Iteration 6600: Loss = -9719.168115961953
Iteration 6700: Loss = -9719.168093648108
Iteration 6800: Loss = -9719.16782090844
Iteration 6900: Loss = -9719.167676320012
Iteration 7000: Loss = -9719.167552264167
Iteration 7100: Loss = -9719.167474251337
Iteration 7200: Loss = -9719.167319353412
Iteration 7300: Loss = -9719.167233689781
Iteration 7400: Loss = -9719.167114095373
Iteration 7500: Loss = -9719.167263722464
1
Iteration 7600: Loss = -9719.16691862191
Iteration 7700: Loss = -9719.196049377084
1
Iteration 7800: Loss = -9719.166783442999
Iteration 7900: Loss = -9719.167496594104
1
Iteration 8000: Loss = -9719.166666573787
Iteration 8100: Loss = -9719.166584211842
Iteration 8200: Loss = -9719.166579171359
Iteration 8300: Loss = -9719.166450833867
Iteration 8400: Loss = -9719.16665607041
1
Iteration 8500: Loss = -9719.166354566378
Iteration 8600: Loss = -9719.166564213294
1
Iteration 8700: Loss = -9719.166261415061
Iteration 8800: Loss = -9719.2173915329
1
Iteration 8900: Loss = -9719.166188717105
Iteration 9000: Loss = -9719.16622141319
Iteration 9100: Loss = -9719.166501411211
1
Iteration 9200: Loss = -9719.170712632867
2
Iteration 9300: Loss = -9719.169784913449
3
Iteration 9400: Loss = -9719.190667828687
4
Iteration 9500: Loss = -9719.166052323559
Iteration 9600: Loss = -9719.22947894884
1
Iteration 9700: Loss = -9719.166701943004
2
Iteration 9800: Loss = -9719.165921680498
Iteration 9900: Loss = -9719.175147310098
1
Iteration 10000: Loss = -9719.172928874794
2
Iteration 10100: Loss = -9719.168734326387
3
Iteration 10200: Loss = -9719.240065116684
4
Iteration 10300: Loss = -9719.165739460086
Iteration 10400: Loss = -9719.197330628333
1
Iteration 10500: Loss = -9719.165744790243
Iteration 10600: Loss = -9719.165777245262
Iteration 10700: Loss = -9719.165761494341
Iteration 10800: Loss = -9719.169419421893
1
Iteration 10900: Loss = -9719.165743701917
Iteration 11000: Loss = -9719.167011740967
1
Iteration 11100: Loss = -9719.173962813096
2
Iteration 11200: Loss = -9719.16619638182
3
Iteration 11300: Loss = -9719.165675100214
Iteration 11400: Loss = -9719.381601819912
1
Iteration 11500: Loss = -9719.165606237506
Iteration 11600: Loss = -9719.220342699393
1
Iteration 11700: Loss = -9719.165607584015
Iteration 11800: Loss = -9719.314266013711
1
Iteration 11900: Loss = -9719.237120349419
2
Iteration 12000: Loss = -9719.167354008581
3
Iteration 12100: Loss = -9719.182916336193
4
Iteration 12200: Loss = -9719.165587864392
Iteration 12300: Loss = -9719.167401623232
1
Iteration 12400: Loss = -9719.166453317286
2
Iteration 12500: Loss = -9719.165633104962
Iteration 12600: Loss = -9719.165604826781
Iteration 12700: Loss = -9719.165640580999
Iteration 12800: Loss = -9719.167286756017
1
Iteration 12900: Loss = -9719.17580709044
2
Iteration 13000: Loss = -9719.165536540078
Iteration 13100: Loss = -9719.166169435332
1
Iteration 13200: Loss = -9719.166418635821
2
Iteration 13300: Loss = -9719.165428224429
Iteration 13400: Loss = -9719.171415611685
1
Iteration 13500: Loss = -9719.167150521296
2
Iteration 13600: Loss = -9719.194394583308
3
Iteration 13700: Loss = -9719.165439416198
Iteration 13800: Loss = -9719.165587766303
1
Iteration 13900: Loss = -9719.172536302447
2
Iteration 14000: Loss = -9719.17998677877
3
Iteration 14100: Loss = -9719.166278806952
4
Iteration 14200: Loss = -9719.1695804984
5
Iteration 14300: Loss = -9719.16565739075
6
Iteration 14400: Loss = -9719.16538856322
Iteration 14500: Loss = -9719.165464324024
Iteration 14600: Loss = -9719.165495316112
Iteration 14700: Loss = -9719.166172889618
1
Iteration 14800: Loss = -9719.168019798703
2
Iteration 14900: Loss = -9719.171322772825
3
Iteration 15000: Loss = -9719.205356728055
4
Iteration 15100: Loss = -9719.168288541428
5
Iteration 15200: Loss = -9719.165385664191
Iteration 15300: Loss = -9719.166207624143
1
Iteration 15400: Loss = -9719.203734802077
2
Iteration 15500: Loss = -9719.169962913693
3
Iteration 15600: Loss = -9719.165824209487
4
Iteration 15700: Loss = -9719.168183399017
5
Iteration 15800: Loss = -9719.168267117293
6
Iteration 15900: Loss = -9719.169692552578
7
Iteration 16000: Loss = -9719.166238463358
8
Iteration 16100: Loss = -9719.189833108963
9
Iteration 16200: Loss = -9719.165849550023
10
Iteration 16300: Loss = -9719.168758868922
11
Iteration 16400: Loss = -9719.172872294515
12
Iteration 16500: Loss = -9719.184968359881
13
Iteration 16600: Loss = -9719.178896441459
14
Iteration 16700: Loss = -9719.222981573907
15
Stopping early at iteration 16700 due to no improvement.
pi: tensor([[8.6669e-01, 1.3331e-01],
        [3.5953e-05, 9.9996e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9993e-01, 6.8514e-05], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1289, 0.1376],
         [0.6208, 0.1487]],

        [[0.6964, 0.1293],
         [0.6387, 0.5489]],

        [[0.6684, 0.1419],
         [0.5058, 0.5532]],

        [[0.6475, 0.1369],
         [0.6005, 0.7060]],

        [[0.6991, 0.1379],
         [0.6070, 0.5142]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 66
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0049666994303792225
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.07205759804101049
Global Adjusted Rand Index: 0.02616027124101986
Average Adjusted Rand Index: 0.016706675184582843
9859.087540471859
[0.00186969706031488, 0.02616027124101986] [-0.0010582697668140913, 0.016706675184582843] [9717.87401688562, 9719.222981573907]
-------------------------------------
This iteration is 38
True Objective function: Loss = -9956.67437243903
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27015.595246972265
Iteration 100: Loss = -9849.506849841862
Iteration 200: Loss = -9846.7902430272
Iteration 300: Loss = -9845.915253089557
Iteration 400: Loss = -9845.313212876732
Iteration 500: Loss = -9844.857471783258
Iteration 600: Loss = -9844.464760962823
Iteration 700: Loss = -9844.089151185688
Iteration 800: Loss = -9843.715170979065
Iteration 900: Loss = -9843.279587417459
Iteration 1000: Loss = -9842.71707736015
Iteration 1100: Loss = -9842.250907087528
Iteration 1200: Loss = -9841.819761332205
Iteration 1300: Loss = -9841.424813302552
Iteration 1400: Loss = -9841.09407734045
Iteration 1500: Loss = -9840.848043215308
Iteration 1600: Loss = -9840.70056158866
Iteration 1700: Loss = -9840.597063298646
Iteration 1800: Loss = -9840.522211325291
Iteration 1900: Loss = -9840.471677502472
Iteration 2000: Loss = -9840.438597775334
Iteration 2100: Loss = -9840.419486487535
Iteration 2200: Loss = -9840.409431862365
Iteration 2300: Loss = -9840.403127538846
Iteration 2400: Loss = -9840.398701209984
Iteration 2500: Loss = -9840.395653016047
Iteration 2600: Loss = -9840.393727869518
Iteration 2700: Loss = -9840.392334729255
Iteration 2800: Loss = -9840.427798114846
1
Iteration 2900: Loss = -9840.389869097637
Iteration 3000: Loss = -9840.389084548022
Iteration 3100: Loss = -9840.38862681741
Iteration 3200: Loss = -9840.388243920841
Iteration 3300: Loss = -9840.387985460968
Iteration 3400: Loss = -9840.387779105155
Iteration 3500: Loss = -9840.387648479249
Iteration 3600: Loss = -9840.387459564421
Iteration 3700: Loss = -9840.387330145819
Iteration 3800: Loss = -9840.387190733778
Iteration 3900: Loss = -9840.387082977059
Iteration 4000: Loss = -9840.38794692885
1
Iteration 4100: Loss = -9840.386917304388
Iteration 4200: Loss = -9840.386836523636
Iteration 4300: Loss = -9840.386821941333
Iteration 4400: Loss = -9840.386689244739
Iteration 4500: Loss = -9840.400578063285
1
Iteration 4600: Loss = -9840.386513913349
Iteration 4700: Loss = -9840.386448882027
Iteration 4800: Loss = -9840.38640032488
Iteration 4900: Loss = -9840.386263907352
Iteration 5000: Loss = -9840.38621952738
Iteration 5100: Loss = -9840.386378505842
1
Iteration 5200: Loss = -9840.385990491122
Iteration 5300: Loss = -9840.391205802734
1
Iteration 5400: Loss = -9840.38583941379
Iteration 5500: Loss = -9840.385734412486
Iteration 5600: Loss = -9840.385657585175
Iteration 5700: Loss = -9840.385598586156
Iteration 5800: Loss = -9840.385527002774
Iteration 5900: Loss = -9840.385403955579
Iteration 6000: Loss = -9840.385337514852
Iteration 6100: Loss = -9840.385217007479
Iteration 6200: Loss = -9840.385142354084
Iteration 6300: Loss = -9840.385064777367
Iteration 6400: Loss = -9840.384972926917
Iteration 6500: Loss = -9840.384848376256
Iteration 6600: Loss = -9840.3847636031
Iteration 6700: Loss = -9840.391362106297
1
Iteration 6800: Loss = -9840.384555769455
Iteration 6900: Loss = -9840.384515209262
Iteration 7000: Loss = -9840.384411609886
Iteration 7100: Loss = -9840.384319877621
Iteration 7200: Loss = -9840.38789135438
1
Iteration 7300: Loss = -9840.38414045294
Iteration 7400: Loss = -9840.384068529966
Iteration 7500: Loss = -9840.384324602808
1
Iteration 7600: Loss = -9840.383930158427
Iteration 7700: Loss = -9840.389966035
1
Iteration 7800: Loss = -9840.383705911543
Iteration 7900: Loss = -9840.383679994055
Iteration 8000: Loss = -9840.383585545478
Iteration 8100: Loss = -9840.383512908656
Iteration 8200: Loss = -9840.385048698172
1
Iteration 8300: Loss = -9840.383376993914
Iteration 8400: Loss = -9840.388285962057
1
Iteration 8500: Loss = -9840.383221542583
Iteration 8600: Loss = -9840.386706738536
1
Iteration 8700: Loss = -9840.383106374047
Iteration 8800: Loss = -9840.387042355142
1
Iteration 8900: Loss = -9840.383003738973
Iteration 9000: Loss = -9840.401694971564
1
Iteration 9100: Loss = -9840.382902154834
Iteration 9200: Loss = -9840.382842921543
Iteration 9300: Loss = -9840.382869431432
Iteration 9400: Loss = -9840.382763938467
Iteration 9500: Loss = -9840.382794493122
Iteration 9600: Loss = -9840.382691731304
Iteration 9700: Loss = -9840.382762251023
Iteration 9800: Loss = -9840.382560123597
Iteration 9900: Loss = -9840.382564401905
Iteration 10000: Loss = -9840.399340668277
1
Iteration 10100: Loss = -9840.382496578248
Iteration 10200: Loss = -9840.388885493232
1
Iteration 10300: Loss = -9840.38332601387
2
Iteration 10400: Loss = -9840.483179855073
3
Iteration 10500: Loss = -9840.426638150348
4
Iteration 10600: Loss = -9840.38241582178
Iteration 10700: Loss = -9840.382460679757
Iteration 10800: Loss = -9840.411102222099
1
Iteration 10900: Loss = -9840.382645102036
2
Iteration 11000: Loss = -9840.38628884146
3
Iteration 11100: Loss = -9840.38265435771
4
Iteration 11200: Loss = -9840.382589885097
5
Iteration 11300: Loss = -9840.38464543307
6
Iteration 11400: Loss = -9840.38535229722
7
Iteration 11500: Loss = -9840.388150499386
8
Iteration 11600: Loss = -9840.384057694635
9
Iteration 11700: Loss = -9840.423211938172
10
Iteration 11800: Loss = -9840.382454478486
Iteration 11900: Loss = -9840.38215098889
Iteration 12000: Loss = -9840.383118208978
1
Iteration 12100: Loss = -9840.387483349876
2
Iteration 12200: Loss = -9840.384426450766
3
Iteration 12300: Loss = -9840.383569563854
4
Iteration 12400: Loss = -9840.56648983927
5
Iteration 12500: Loss = -9840.384033376344
6
Iteration 12600: Loss = -9840.38521965162
7
Iteration 12700: Loss = -9840.383394904275
8
Iteration 12800: Loss = -9840.382210979724
Iteration 12900: Loss = -9840.38202784525
Iteration 13000: Loss = -9840.38323849794
1
Iteration 13100: Loss = -9840.3856164461
2
Iteration 13200: Loss = -9840.3828008761
3
Iteration 13300: Loss = -9840.382485043421
4
Iteration 13400: Loss = -9840.381968206842
Iteration 13500: Loss = -9840.393560151568
1
Iteration 13600: Loss = -9840.388036490838
2
Iteration 13700: Loss = -9840.382111925966
3
Iteration 13800: Loss = -9840.381973803269
Iteration 13900: Loss = -9840.382085260951
1
Iteration 14000: Loss = -9840.38537688314
2
Iteration 14100: Loss = -9840.388079335631
3
Iteration 14200: Loss = -9840.383564688776
4
Iteration 14300: Loss = -9840.414811406827
5
Iteration 14400: Loss = -9840.3819057998
Iteration 14500: Loss = -9840.381938691866
Iteration 14600: Loss = -9840.382004330368
Iteration 14700: Loss = -9840.389494537765
1
Iteration 14800: Loss = -9840.459417276528
2
Iteration 14900: Loss = -9840.455444818717
3
Iteration 15000: Loss = -9840.388252271347
4
Iteration 15100: Loss = -9840.39059582979
5
Iteration 15200: Loss = -9840.384833703882
6
Iteration 15300: Loss = -9840.384373872874
7
Iteration 15400: Loss = -9840.38803199396
8
Iteration 15500: Loss = -9840.38815514184
9
Iteration 15600: Loss = -9840.43003287887
10
Iteration 15700: Loss = -9840.383857080611
11
Iteration 15800: Loss = -9840.384597328575
12
Iteration 15900: Loss = -9840.390940043371
13
Iteration 16000: Loss = -9840.395265041898
14
Iteration 16100: Loss = -9840.381927471431
Iteration 16200: Loss = -9840.403068207885
1
Iteration 16300: Loss = -9840.41739508461
2
Iteration 16400: Loss = -9840.42817582333
3
Iteration 16500: Loss = -9840.419740683816
4
Iteration 16600: Loss = -9840.393355904382
5
Iteration 16700: Loss = -9840.385427493582
6
Iteration 16800: Loss = -9840.396636342632
7
Iteration 16900: Loss = -9840.548221577907
8
Iteration 17000: Loss = -9840.383586671018
9
Iteration 17100: Loss = -9840.395949796304
10
Iteration 17200: Loss = -9840.386784459557
11
Iteration 17300: Loss = -9840.381987824925
Iteration 17400: Loss = -9840.382102444239
1
Iteration 17500: Loss = -9840.405645692368
2
Iteration 17600: Loss = -9840.383143778401
3
Iteration 17700: Loss = -9840.384916021912
4
Iteration 17800: Loss = -9840.382335522989
5
Iteration 17900: Loss = -9840.382206750233
6
Iteration 18000: Loss = -9840.384447994666
7
Iteration 18100: Loss = -9840.381857462802
Iteration 18200: Loss = -9840.382212166518
1
Iteration 18300: Loss = -9840.39121332399
2
Iteration 18400: Loss = -9840.382616017001
3
Iteration 18500: Loss = -9840.429825618063
4
Iteration 18600: Loss = -9840.381836284994
Iteration 18700: Loss = -9840.382344078122
1
Iteration 18800: Loss = -9840.4763390739
2
Iteration 18900: Loss = -9840.382677951218
3
Iteration 19000: Loss = -9840.423959267502
4
Iteration 19100: Loss = -9840.381934014807
Iteration 19200: Loss = -9840.382070926813
1
Iteration 19300: Loss = -9840.386105865638
2
Iteration 19400: Loss = -9840.391319335278
3
Iteration 19500: Loss = -9840.388693710243
4
Iteration 19600: Loss = -9840.38190700904
Iteration 19700: Loss = -9840.384319929297
1
Iteration 19800: Loss = -9840.38183551739
Iteration 19900: Loss = -9840.382014860787
1
pi: tensor([[8.2309e-01, 1.7691e-01],
        [9.9999e-01, 9.7536e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9319, 0.0681], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1236, 0.1869],
         [0.5762, 0.2107]],

        [[0.6237, 0.1679],
         [0.6238, 0.5364]],

        [[0.5065, 0.1819],
         [0.6353, 0.7146]],

        [[0.6093, 0.1585],
         [0.6441, 0.6778]],

        [[0.5038, 0.1591],
         [0.6984, 0.5372]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.02704564545834377
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.011643993543924371
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
Global Adjusted Rand Index: -0.00017896995033451357
Average Adjusted Rand Index: 0.0022876434797991633
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22818.11725457254
Iteration 100: Loss = -9845.723449321327
Iteration 200: Loss = -9844.129428431766
Iteration 300: Loss = -9843.480996162914
Iteration 400: Loss = -9842.60595132875
Iteration 500: Loss = -9842.346774514785
Iteration 600: Loss = -9842.240478193962
Iteration 700: Loss = -9842.18159103882
Iteration 800: Loss = -9842.147050114796
Iteration 900: Loss = -9842.125464471243
Iteration 1000: Loss = -9842.111157305355
Iteration 1100: Loss = -9842.100975525804
Iteration 1200: Loss = -9842.09324889933
Iteration 1300: Loss = -9842.087051258395
Iteration 1400: Loss = -9842.081963395594
Iteration 1500: Loss = -9842.077610468328
Iteration 1600: Loss = -9842.0738856786
Iteration 1700: Loss = -9842.070659745494
Iteration 1800: Loss = -9842.06781512957
Iteration 1900: Loss = -9842.065306424978
Iteration 2000: Loss = -9842.063045377712
Iteration 2100: Loss = -9842.061062506778
Iteration 2200: Loss = -9842.059312392837
Iteration 2300: Loss = -9842.057705666546
Iteration 2400: Loss = -9842.056273554768
Iteration 2500: Loss = -9842.054956193128
Iteration 2600: Loss = -9842.05379338922
Iteration 2700: Loss = -9842.052693301966
Iteration 2800: Loss = -9842.05174300339
Iteration 2900: Loss = -9842.05085603756
Iteration 3000: Loss = -9842.05002836252
Iteration 3100: Loss = -9842.049258890618
Iteration 3200: Loss = -9842.048582818277
Iteration 3300: Loss = -9842.047966746024
Iteration 3400: Loss = -9842.047363536598
Iteration 3500: Loss = -9842.046817384271
Iteration 3600: Loss = -9842.046327061613
Iteration 3700: Loss = -9842.045833344628
Iteration 3800: Loss = -9842.047605983376
1
Iteration 3900: Loss = -9842.045001419872
Iteration 4000: Loss = -9842.044682111704
Iteration 4100: Loss = -9842.044258868424
Iteration 4200: Loss = -9842.043933589732
Iteration 4300: Loss = -9842.043894010008
Iteration 4400: Loss = -9842.043361445585
Iteration 4500: Loss = -9842.043965453588
1
Iteration 4600: Loss = -9842.042834859412
Iteration 4700: Loss = -9842.042599128365
Iteration 4800: Loss = -9842.042405262491
Iteration 4900: Loss = -9842.042187663183
Iteration 5000: Loss = -9842.041996793907
Iteration 5100: Loss = -9842.041788590059
Iteration 5200: Loss = -9842.04164800166
Iteration 5300: Loss = -9842.041471560817
Iteration 5400: Loss = -9842.041296040485
Iteration 5500: Loss = -9842.041494109826
1
Iteration 5600: Loss = -9842.041050474994
Iteration 5700: Loss = -9842.040899126934
Iteration 5800: Loss = -9842.040795287077
Iteration 5900: Loss = -9842.04067133195
Iteration 6000: Loss = -9842.041839139483
1
Iteration 6100: Loss = -9842.040444882132
Iteration 6200: Loss = -9842.068013173935
1
Iteration 6300: Loss = -9842.04026926021
Iteration 6400: Loss = -9842.04018603107
Iteration 6500: Loss = -9842.040096718734
Iteration 6600: Loss = -9842.040068001064
Iteration 6700: Loss = -9842.040191008668
1
Iteration 6800: Loss = -9842.03986708202
Iteration 6900: Loss = -9842.040354195882
1
Iteration 7000: Loss = -9842.039753924902
Iteration 7100: Loss = -9842.041305423125
1
Iteration 7200: Loss = -9842.039639808467
Iteration 7300: Loss = -9842.061664166637
1
Iteration 7400: Loss = -9842.03952691046
Iteration 7500: Loss = -9842.03974184394
1
Iteration 7600: Loss = -9842.039466268876
Iteration 7700: Loss = -9842.039420756164
Iteration 7800: Loss = -9842.03937146063
Iteration 7900: Loss = -9842.039338651659
Iteration 8000: Loss = -9842.039511366629
1
Iteration 8100: Loss = -9842.039247465653
Iteration 8200: Loss = -9842.040063594619
1
Iteration 8300: Loss = -9842.039196102236
Iteration 8400: Loss = -9842.043513813578
1
Iteration 8500: Loss = -9842.039167297507
Iteration 8600: Loss = -9842.055777970305
1
Iteration 8700: Loss = -9842.039098255125
Iteration 8800: Loss = -9842.045490726256
1
Iteration 8900: Loss = -9842.039040023847
Iteration 9000: Loss = -9842.040254242258
1
Iteration 9100: Loss = -9842.03901008178
Iteration 9200: Loss = -9842.039823515383
1
Iteration 9300: Loss = -9842.038978338458
Iteration 9400: Loss = -9842.04736221793
1
Iteration 9500: Loss = -9842.038956495579
Iteration 9600: Loss = -9842.038937361665
Iteration 9700: Loss = -9842.039356218655
1
Iteration 9800: Loss = -9842.038905263904
Iteration 9900: Loss = -9842.42109407501
1
Iteration 10000: Loss = -9842.0388792873
Iteration 10100: Loss = -9842.03886662816
Iteration 10200: Loss = -9842.039010763361
1
Iteration 10300: Loss = -9842.03884968338
Iteration 10400: Loss = -9842.03900011809
1
Iteration 10500: Loss = -9842.038820850152
Iteration 10600: Loss = -9842.204129657157
1
Iteration 10700: Loss = -9842.04167324258
2
Iteration 10800: Loss = -9842.039743752857
3
Iteration 10900: Loss = -9842.042845829435
4
Iteration 11000: Loss = -9842.038835217481
Iteration 11100: Loss = -9842.282709418852
1
Iteration 11200: Loss = -9842.03874798404
Iteration 11300: Loss = -9842.040370656463
1
Iteration 11400: Loss = -9842.03872428083
Iteration 11500: Loss = -9842.045699211185
1
Iteration 11600: Loss = -9842.04454553529
2
Iteration 11700: Loss = -9842.040246808318
3
Iteration 11800: Loss = -9842.065948046127
4
Iteration 11900: Loss = -9842.065656330555
5
Iteration 12000: Loss = -9842.04344057934
6
Iteration 12100: Loss = -9842.041728803513
7
Iteration 12200: Loss = -9842.03877377411
Iteration 12300: Loss = -9842.039660365781
1
Iteration 12400: Loss = -9842.03874726315
Iteration 12500: Loss = -9842.082154483463
1
Iteration 12600: Loss = -9842.03874339084
Iteration 12700: Loss = -9842.038843733879
1
Iteration 12800: Loss = -9842.043605602883
2
Iteration 12900: Loss = -9842.038688711733
Iteration 13000: Loss = -9842.038940718412
1
Iteration 13100: Loss = -9842.039165996901
2
Iteration 13200: Loss = -9842.038853420314
3
Iteration 13300: Loss = -9842.038788918357
4
Iteration 13400: Loss = -9842.038723021538
Iteration 13500: Loss = -9842.039013354333
1
Iteration 13600: Loss = -9842.040451849083
2
Iteration 13700: Loss = -9842.038729836577
Iteration 13800: Loss = -9842.039719918164
1
Iteration 13900: Loss = -9842.039074572513
2
Iteration 14000: Loss = -9842.039606606195
3
Iteration 14100: Loss = -9842.038686400196
Iteration 14200: Loss = -9842.044936841214
1
Iteration 14300: Loss = -9844.756258972331
2
Iteration 14400: Loss = -9840.38199335945
Iteration 14500: Loss = -9840.381846151633
Iteration 14600: Loss = -9840.381862269425
Iteration 14700: Loss = -9840.386387353468
1
Iteration 14800: Loss = -9840.53790950581
2
Iteration 14900: Loss = -9840.381884807986
Iteration 15000: Loss = -9840.388448337268
1
Iteration 15100: Loss = -9840.3818277916
Iteration 15200: Loss = -9840.458565973433
1
Iteration 15300: Loss = -9840.381858715433
Iteration 15400: Loss = -9840.381844177546
Iteration 15500: Loss = -9840.3822246211
1
Iteration 15600: Loss = -9840.381834586424
Iteration 15700: Loss = -9840.381916069873
Iteration 15800: Loss = -9840.38189834762
Iteration 15900: Loss = -9840.381845688762
Iteration 16000: Loss = -9840.381873695482
Iteration 16100: Loss = -9840.384430215056
1
Iteration 16200: Loss = -9840.527220929122
2
Iteration 16300: Loss = -9840.381866659058
Iteration 16400: Loss = -9840.38191905924
Iteration 16500: Loss = -9840.38230064958
1
Iteration 16600: Loss = -9840.591752726627
2
Iteration 16700: Loss = -9840.38205316613
3
Iteration 16800: Loss = -9840.414537776456
4
Iteration 16900: Loss = -9840.381858353456
Iteration 17000: Loss = -9840.384088831082
1
Iteration 17100: Loss = -9840.381851834605
Iteration 17200: Loss = -9840.382476230689
1
Iteration 17300: Loss = -9840.615311616293
2
Iteration 17400: Loss = -9840.406934521683
3
Iteration 17500: Loss = -9840.457462597693
4
Iteration 17600: Loss = -9840.383130241253
5
Iteration 17700: Loss = -9840.385399054036
6
Iteration 17800: Loss = -9840.404450467695
7
Iteration 17900: Loss = -9840.382600884874
8
Iteration 18000: Loss = -9840.396930927027
9
Iteration 18100: Loss = -9840.381998031899
10
Iteration 18200: Loss = -9840.383595275422
11
Iteration 18300: Loss = -9840.416511892055
12
Iteration 18400: Loss = -9840.381888709926
Iteration 18500: Loss = -9840.397758734327
1
Iteration 18600: Loss = -9840.38659158483
2
Iteration 18700: Loss = -9840.381934681835
Iteration 18800: Loss = -9840.382219542405
1
Iteration 18900: Loss = -9840.437491327542
2
Iteration 19000: Loss = -9840.381819025975
Iteration 19100: Loss = -9840.383217255976
1
Iteration 19200: Loss = -9840.381944852435
2
Iteration 19300: Loss = -9840.386443949594
3
Iteration 19400: Loss = -9840.382809753119
4
Iteration 19500: Loss = -9840.536427686804
5
Iteration 19600: Loss = -9840.381839075533
Iteration 19700: Loss = -9840.384359536414
1
Iteration 19800: Loss = -9840.443974078362
2
Iteration 19900: Loss = -9840.389801955465
3
pi: tensor([[2.6459e-06, 1.0000e+00],
        [1.7609e-01, 8.2391e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0679, 0.9321], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2108, 0.1864],
         [0.5020, 0.1239]],

        [[0.6574, 0.1678],
         [0.5081, 0.6126]],

        [[0.5832, 0.1823],
         [0.5390, 0.5474]],

        [[0.5220, 0.1585],
         [0.5849, 0.6814]],

        [[0.5720, 0.1593],
         [0.6118, 0.5181]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.02704564545834377
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.011643993543924371
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
Global Adjusted Rand Index: -0.00017896995033451357
Average Adjusted Rand Index: 0.0022876434797991633
9956.67437243903
[-0.00017896995033451357, -0.00017896995033451357] [0.0022876434797991633, 0.0022876434797991633] [9840.397226524537, 9840.390999916948]
-------------------------------------
This iteration is 39
True Objective function: Loss = -9966.080437724544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23438.946682682395
Iteration 100: Loss = -9812.954990365344
Iteration 200: Loss = -9811.303634046177
Iteration 300: Loss = -9810.660677609485
Iteration 400: Loss = -9810.376524761417
Iteration 500: Loss = -9810.234335129095
Iteration 600: Loss = -9810.15492354352
Iteration 700: Loss = -9810.107408888724
Iteration 800: Loss = -9810.077498995437
Iteration 900: Loss = -9810.057804842341
Iteration 1000: Loss = -9810.044369784047
Iteration 1100: Loss = -9810.034820604378
Iteration 1200: Loss = -9810.027862774417
Iteration 1300: Loss = -9810.022604785143
Iteration 1400: Loss = -9810.018506867378
Iteration 1500: Loss = -9810.015280388063
Iteration 1600: Loss = -9810.012653263304
Iteration 1700: Loss = -9810.010490330045
Iteration 1800: Loss = -9810.008652663637
Iteration 1900: Loss = -9810.007053979227
Iteration 2000: Loss = -9810.005691677829
Iteration 2100: Loss = -9810.004512178897
Iteration 2200: Loss = -9810.003425476365
Iteration 2300: Loss = -9810.002491759522
Iteration 2400: Loss = -9810.001595709602
Iteration 2500: Loss = -9810.000767199164
Iteration 2600: Loss = -9810.000097776248
Iteration 2700: Loss = -9809.999415880371
Iteration 2800: Loss = -9809.998789103953
Iteration 2900: Loss = -9809.998195923905
Iteration 3000: Loss = -9809.997637705994
Iteration 3100: Loss = -9809.997138101777
Iteration 3200: Loss = -9809.996671902178
Iteration 3300: Loss = -9809.99620351654
Iteration 3400: Loss = -9809.995787266493
Iteration 3500: Loss = -9809.99534401916
Iteration 3600: Loss = -9809.994948174926
Iteration 3700: Loss = -9809.994557157424
Iteration 3800: Loss = -9809.994171839677
Iteration 3900: Loss = -9809.993819575204
Iteration 4000: Loss = -9809.9934527771
Iteration 4100: Loss = -9809.993100680891
Iteration 4200: Loss = -9809.992808459863
Iteration 4300: Loss = -9809.992453472216
Iteration 4400: Loss = -9809.992109521281
Iteration 4500: Loss = -9809.991756198824
Iteration 4600: Loss = -9809.99143674515
Iteration 4700: Loss = -9809.991117465028
Iteration 4800: Loss = -9809.99069809827
Iteration 4900: Loss = -9809.990273907008
Iteration 5000: Loss = -9809.989877967022
Iteration 5100: Loss = -9809.989413723462
Iteration 5200: Loss = -9809.988840423743
Iteration 5300: Loss = -9809.988106087072
Iteration 5400: Loss = -9809.987161575526
Iteration 5500: Loss = -9809.985783192154
Iteration 5600: Loss = -9809.983471711386
Iteration 5700: Loss = -9809.978901587743
Iteration 5800: Loss = -9809.966511595296
Iteration 5900: Loss = -9809.91275933611
Iteration 6000: Loss = -9809.60399030225
Iteration 6100: Loss = -9808.801797693934
Iteration 6200: Loss = -9808.748849333353
Iteration 6300: Loss = -9808.742308714149
Iteration 6400: Loss = -9808.738690119144
Iteration 6500: Loss = -9808.736012520574
Iteration 6600: Loss = -9808.733804820467
Iteration 6700: Loss = -9808.731567151477
Iteration 6800: Loss = -9808.734648232079
1
Iteration 6900: Loss = -9808.724041469393
Iteration 7000: Loss = -9808.700645850295
Iteration 7100: Loss = -9806.960346309417
Iteration 7200: Loss = -9806.94132488293
Iteration 7300: Loss = -9806.943324613925
1
Iteration 7400: Loss = -9806.93954311819
Iteration 7500: Loss = -9806.938845163893
Iteration 7600: Loss = -9806.938105084873
Iteration 7700: Loss = -9806.936932418055
Iteration 7800: Loss = -9806.935499943176
Iteration 7900: Loss = -9806.93402726708
Iteration 8000: Loss = -9806.931565368226
Iteration 8100: Loss = -9806.929191478079
Iteration 8200: Loss = -9806.940098666704
1
Iteration 8300: Loss = -9806.925447457857
Iteration 8400: Loss = -9806.926083639257
1
Iteration 8500: Loss = -9806.923750781185
Iteration 8600: Loss = -9806.923591461975
Iteration 8700: Loss = -9806.923428673825
Iteration 8800: Loss = -9806.923367897785
Iteration 8900: Loss = -9806.923342372364
Iteration 9000: Loss = -9807.021140184557
1
Iteration 9100: Loss = -9806.923302033088
Iteration 9200: Loss = -9806.923274846986
Iteration 9300: Loss = -9806.925235646317
1
Iteration 9400: Loss = -9806.923242746641
Iteration 9500: Loss = -9806.923252630993
Iteration 9600: Loss = -9806.944801085845
1
Iteration 9700: Loss = -9806.923235777425
Iteration 9800: Loss = -9806.92322698113
Iteration 9900: Loss = -9806.927880583755
1
Iteration 10000: Loss = -9806.923252115448
Iteration 10100: Loss = -9806.92323168614
Iteration 10200: Loss = -9806.92344528504
1
Iteration 10300: Loss = -9806.923192177786
Iteration 10400: Loss = -9806.923275551022
Iteration 10500: Loss = -9806.923206883994
Iteration 10600: Loss = -9806.92320851941
Iteration 10700: Loss = -9806.96708257581
1
Iteration 10800: Loss = -9806.923240604763
Iteration 10900: Loss = -9806.923193465534
Iteration 11000: Loss = -9806.96899385789
1
Iteration 11100: Loss = -9806.923189054058
Iteration 11200: Loss = -9806.923223108055
Iteration 11300: Loss = -9806.930310647585
1
Iteration 11400: Loss = -9806.923187146589
Iteration 11500: Loss = -9806.923196037738
Iteration 11600: Loss = -9806.926157275011
1
Iteration 11700: Loss = -9806.92320126473
Iteration 11800: Loss = -9806.923197834254
Iteration 11900: Loss = -9806.923284617314
Iteration 12000: Loss = -9806.923221468864
Iteration 12100: Loss = -9806.926629565229
1
Iteration 12200: Loss = -9806.92322321096
Iteration 12300: Loss = -9806.923507046791
1
Iteration 12400: Loss = -9806.923220402794
Iteration 12500: Loss = -9806.923371430526
1
Iteration 12600: Loss = -9806.923191750633
Iteration 12700: Loss = -9806.923446668216
1
Iteration 12800: Loss = -9806.94411251654
2
Iteration 12900: Loss = -9806.923214935854
Iteration 13000: Loss = -9806.925269475314
1
Iteration 13100: Loss = -9806.923167162513
Iteration 13200: Loss = -9806.92666326413
1
Iteration 13300: Loss = -9806.923204508526
Iteration 13400: Loss = -9806.923215324643
Iteration 13500: Loss = -9806.92327846271
Iteration 13600: Loss = -9806.923221000805
Iteration 13700: Loss = -9807.145331800899
1
Iteration 13800: Loss = -9806.923236850787
Iteration 13900: Loss = -9806.923188378776
Iteration 14000: Loss = -9806.923509362772
1
Iteration 14100: Loss = -9806.923174639944
Iteration 14200: Loss = -9807.019040008414
1
Iteration 14300: Loss = -9806.9232168888
Iteration 14400: Loss = -9806.923216703206
Iteration 14500: Loss = -9806.935403028765
1
Iteration 14600: Loss = -9806.923202823753
Iteration 14700: Loss = -9807.123997808023
1
Iteration 14800: Loss = -9806.923209441342
Iteration 14900: Loss = -9806.923200958505
Iteration 15000: Loss = -9806.924108975165
1
Iteration 15100: Loss = -9806.92320439385
Iteration 15200: Loss = -9807.0092967283
1
Iteration 15300: Loss = -9806.923225319631
Iteration 15400: Loss = -9806.923222078784
Iteration 15500: Loss = -9806.923202267099
Iteration 15600: Loss = -9806.923646044206
1
Iteration 15700: Loss = -9806.924172461273
2
Iteration 15800: Loss = -9806.962718007659
3
Iteration 15900: Loss = -9806.923237601626
Iteration 16000: Loss = -9806.923830715563
1
Iteration 16100: Loss = -9807.105132392217
2
Iteration 16200: Loss = -9806.923200807827
Iteration 16300: Loss = -9806.93071222195
1
Iteration 16400: Loss = -9806.92322070179
Iteration 16500: Loss = -9806.926130630782
1
Iteration 16600: Loss = -9806.927728316805
2
Iteration 16700: Loss = -9806.923207665768
Iteration 16800: Loss = -9806.978238538544
1
Iteration 16900: Loss = -9806.923186541193
Iteration 17000: Loss = -9806.945643886174
1
Iteration 17100: Loss = -9806.923407461374
2
Iteration 17200: Loss = -9806.923268381297
Iteration 17300: Loss = -9806.95727493157
1
Iteration 17400: Loss = -9806.923193311231
Iteration 17500: Loss = -9806.92517374878
1
Iteration 17600: Loss = -9806.923254849871
Iteration 17700: Loss = -9806.923326720664
Iteration 17800: Loss = -9807.151471597934
1
Iteration 17900: Loss = -9806.92320611094
Iteration 18000: Loss = -9806.929666117252
1
Iteration 18100: Loss = -9806.928994512904
2
Iteration 18200: Loss = -9806.92323651937
Iteration 18300: Loss = -9807.088039270417
1
Iteration 18400: Loss = -9806.92324686256
Iteration 18500: Loss = -9806.94526668539
1
Iteration 18600: Loss = -9806.923235808674
Iteration 18700: Loss = -9806.923215986024
Iteration 18800: Loss = -9806.927552722489
1
Iteration 18900: Loss = -9806.92320093831
Iteration 19000: Loss = -9806.925693863666
1
Iteration 19100: Loss = -9806.923229163855
Iteration 19200: Loss = -9806.92343008294
1
Iteration 19300: Loss = -9806.923237495786
Iteration 19400: Loss = -9806.923195075806
Iteration 19500: Loss = -9806.923251250413
Iteration 19600: Loss = -9806.923205742389
Iteration 19700: Loss = -9806.926735312958
1
Iteration 19800: Loss = -9807.065466206412
2
Iteration 19900: Loss = -9806.923219123968
pi: tensor([[0.9363, 0.0637],
        [0.8093, 0.1907]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9475, 0.0525], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1394, 0.0719],
         [0.6145, 0.1066]],

        [[0.5064, 0.1176],
         [0.7125, 0.5772]],

        [[0.5531, 0.1247],
         [0.7226, 0.5486]],

        [[0.6250, 0.0764],
         [0.5784, 0.5078]],

        [[0.7225, 0.1408],
         [0.5796, 0.6381]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.02216370547261358
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002574852870843536
Average Adjusted Rand Index: 0.0039940787230917695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24931.22035093533
Iteration 100: Loss = -9812.032958125843
Iteration 200: Loss = -9810.727759060723
Iteration 300: Loss = -9810.409325948292
Iteration 400: Loss = -9810.26318588095
Iteration 500: Loss = -9810.180458360732
Iteration 600: Loss = -9810.128785334693
Iteration 700: Loss = -9810.095198789246
Iteration 800: Loss = -9810.072544618624
Iteration 900: Loss = -9810.056391970149
Iteration 1000: Loss = -9810.044275920007
Iteration 1100: Loss = -9810.035058722473
Iteration 1200: Loss = -9810.02803885671
Iteration 1300: Loss = -9810.02265105905
Iteration 1400: Loss = -9810.01829291086
Iteration 1500: Loss = -9810.01477966503
Iteration 1600: Loss = -9810.0118527959
Iteration 1700: Loss = -9810.009425880915
Iteration 1800: Loss = -9810.007359090947
Iteration 1900: Loss = -9810.005579392455
Iteration 2000: Loss = -9810.004041724877
Iteration 2100: Loss = -9810.002670027445
Iteration 2200: Loss = -9810.00147510349
Iteration 2300: Loss = -9810.000355304088
Iteration 2400: Loss = -9809.999313303988
Iteration 2500: Loss = -9809.998345280808
Iteration 2600: Loss = -9809.99737647435
Iteration 2700: Loss = -9809.99650384055
Iteration 2800: Loss = -9809.995611538567
Iteration 2900: Loss = -9809.99475564593
Iteration 3000: Loss = -9809.993835690208
Iteration 3100: Loss = -9809.992832462482
Iteration 3200: Loss = -9809.991734529514
Iteration 3300: Loss = -9809.990405292401
Iteration 3400: Loss = -9809.988697248888
Iteration 3500: Loss = -9809.986434304888
Iteration 3600: Loss = -9809.98303995605
Iteration 3700: Loss = -9809.97742097752
Iteration 3800: Loss = -9809.966335108056
Iteration 3900: Loss = -9809.93852943509
Iteration 4000: Loss = -9809.828254348475
Iteration 4100: Loss = -9809.42488565171
Iteration 4200: Loss = -9808.935270417152
Iteration 4300: Loss = -9808.84162465235
Iteration 4400: Loss = -9808.822432758856
Iteration 4500: Loss = -9808.808009900495
Iteration 4600: Loss = -9808.794029826217
Iteration 4700: Loss = -9808.778375825701
Iteration 4800: Loss = -9808.759762986485
Iteration 4900: Loss = -9808.741697490404
Iteration 5000: Loss = -9808.730341561357
Iteration 5100: Loss = -9808.724810338948
Iteration 5200: Loss = -9808.721876392412
Iteration 5300: Loss = -9808.719942403714
Iteration 5400: Loss = -9808.718595379669
Iteration 5500: Loss = -9808.71756077998
Iteration 5600: Loss = -9808.716729596976
Iteration 5700: Loss = -9808.716090081998
Iteration 5800: Loss = -9808.715514208554
Iteration 5900: Loss = -9808.71508682268
Iteration 6000: Loss = -9808.714683243279
Iteration 6100: Loss = -9808.71432823568
Iteration 6200: Loss = -9808.713986482759
Iteration 6300: Loss = -9808.713772813397
Iteration 6400: Loss = -9808.713576944443
Iteration 6500: Loss = -9808.713341939647
Iteration 6600: Loss = -9808.71314908023
Iteration 6700: Loss = -9808.714193446092
1
Iteration 6800: Loss = -9808.712805240864
Iteration 6900: Loss = -9808.71267152569
Iteration 7000: Loss = -9808.712673265047
Iteration 7100: Loss = -9808.712435963847
Iteration 7200: Loss = -9808.712449481754
Iteration 7300: Loss = -9808.712244718592
Iteration 7400: Loss = -9808.712155915064
Iteration 7500: Loss = -9808.712061474562
Iteration 7600: Loss = -9808.711961225197
Iteration 7700: Loss = -9808.711952723352
Iteration 7800: Loss = -9808.711866866539
Iteration 7900: Loss = -9808.712263535113
1
Iteration 8000: Loss = -9808.71175515229
Iteration 8100: Loss = -9808.711753519901
Iteration 8200: Loss = -9808.711648908082
Iteration 8300: Loss = -9808.711641079839
Iteration 8400: Loss = -9808.711586059742
Iteration 8500: Loss = -9808.711549388017
Iteration 8600: Loss = -9808.711504565672
Iteration 8700: Loss = -9808.73806435887
1
Iteration 8800: Loss = -9808.716782124933
2
Iteration 8900: Loss = -9808.76926968829
3
Iteration 9000: Loss = -9808.711393335001
Iteration 9100: Loss = -9808.711334947906
Iteration 9200: Loss = -9808.723099132445
1
Iteration 9300: Loss = -9808.71123862601
Iteration 9400: Loss = -9808.71234918725
1
Iteration 9500: Loss = -9808.7112739996
Iteration 9600: Loss = -9808.711228124315
Iteration 9700: Loss = -9808.796814556628
1
Iteration 9800: Loss = -9808.711207889794
Iteration 9900: Loss = -9808.711202474511
Iteration 10000: Loss = -9808.73761300408
1
Iteration 10100: Loss = -9808.711162846084
Iteration 10200: Loss = -9808.71113912786
Iteration 10300: Loss = -9808.717999424258
1
Iteration 10400: Loss = -9808.711048011899
Iteration 10500: Loss = -9808.711024143659
Iteration 10600: Loss = -9808.743478267226
1
Iteration 10700: Loss = -9808.711017774254
Iteration 10800: Loss = -9808.7109898039
Iteration 10900: Loss = -9808.717911872469
1
Iteration 11000: Loss = -9808.711008794935
Iteration 11100: Loss = -9808.71097806852
Iteration 11200: Loss = -9808.711015643685
Iteration 11300: Loss = -9808.711087305577
Iteration 11400: Loss = -9808.710971225804
Iteration 11500: Loss = -9808.710976474307
Iteration 11600: Loss = -9808.711086510912
1
Iteration 11700: Loss = -9808.710977683706
Iteration 11800: Loss = -9808.712065014332
1
Iteration 11900: Loss = -9808.71097504773
Iteration 12000: Loss = -9808.714750032046
1
Iteration 12100: Loss = -9808.71098229547
Iteration 12200: Loss = -9808.71095867868
Iteration 12300: Loss = -9808.711015867737
Iteration 12400: Loss = -9808.710953909755
Iteration 12500: Loss = -9808.730041193223
1
Iteration 12600: Loss = -9808.71095965185
Iteration 12700: Loss = -9808.71092042773
Iteration 12800: Loss = -9808.711298192187
1
Iteration 12900: Loss = -9808.710981365351
Iteration 13000: Loss = -9808.725953101692
1
Iteration 13100: Loss = -9808.710915948854
Iteration 13200: Loss = -9808.710929782119
Iteration 13300: Loss = -9808.730624771839
1
Iteration 13400: Loss = -9808.710911362485
Iteration 13500: Loss = -9808.710909235346
Iteration 13600: Loss = -9808.711288063792
1
Iteration 13700: Loss = -9808.710916850081
Iteration 13800: Loss = -9808.753294375769
1
Iteration 13900: Loss = -9808.710921509168
Iteration 14000: Loss = -9808.716272594935
1
Iteration 14100: Loss = -9808.71086037044
Iteration 14200: Loss = -9808.715694162074
1
Iteration 14300: Loss = -9808.710861219315
Iteration 14400: Loss = -9808.953987664258
1
Iteration 14500: Loss = -9808.710835757685
Iteration 14600: Loss = -9808.710843052602
Iteration 14700: Loss = -9808.711015577568
1
Iteration 14800: Loss = -9808.712030793555
2
Iteration 14900: Loss = -9808.724117813123
3
Iteration 15000: Loss = -9808.710833335448
Iteration 15100: Loss = -9808.710843580042
Iteration 15200: Loss = -9808.711015852066
1
Iteration 15300: Loss = -9808.710935887873
Iteration 15400: Loss = -9808.710847583967
Iteration 15500: Loss = -9808.821178220718
1
Iteration 15600: Loss = -9808.711473343852
2
Iteration 15700: Loss = -9808.72125575606
3
Iteration 15800: Loss = -9808.710841738603
Iteration 15900: Loss = -9808.757288739089
1
Iteration 16000: Loss = -9808.710872653695
Iteration 16100: Loss = -9809.074062488828
1
Iteration 16200: Loss = -9808.710810216484
Iteration 16300: Loss = -9808.710886489258
Iteration 16400: Loss = -9808.71323450074
1
Iteration 16500: Loss = -9808.711714828894
2
Iteration 16600: Loss = -9808.710807300593
Iteration 16700: Loss = -9808.710808170279
Iteration 16800: Loss = -9808.71531877387
1
Iteration 16900: Loss = -9808.71080944407
Iteration 17000: Loss = -9808.721061672484
1
Iteration 17100: Loss = -9808.710829842035
Iteration 17200: Loss = -9808.711966140847
1
Iteration 17300: Loss = -9808.714566798997
2
Iteration 17400: Loss = -9808.710818855281
Iteration 17500: Loss = -9808.7312744373
1
Iteration 17600: Loss = -9808.710804345734
Iteration 17700: Loss = -9808.750348785634
1
Iteration 17800: Loss = -9808.710807951858
Iteration 17900: Loss = -9808.713146570231
1
Iteration 18000: Loss = -9808.71123323321
2
Iteration 18100: Loss = -9808.71356170527
3
Iteration 18200: Loss = -9808.711109428843
4
Iteration 18300: Loss = -9808.754020279137
5
Iteration 18400: Loss = -9808.710776367257
Iteration 18500: Loss = -9808.71166795823
1
Iteration 18600: Loss = -9808.72900348638
2
Iteration 18700: Loss = -9808.710789214258
Iteration 18800: Loss = -9808.711006068404
1
Iteration 18900: Loss = -9808.711697598063
2
Iteration 19000: Loss = -9808.711759020789
3
Iteration 19100: Loss = -9808.710886662646
Iteration 19200: Loss = -9808.712166323776
1
Iteration 19300: Loss = -9808.715180955634
2
Iteration 19400: Loss = -9808.710765748476
Iteration 19500: Loss = -9808.921635471126
1
Iteration 19600: Loss = -9808.710807345815
Iteration 19700: Loss = -9808.71642967375
1
Iteration 19800: Loss = -9808.710906963042
Iteration 19900: Loss = -9808.710905708653
pi: tensor([[1.0000e+00, 4.6748e-07],
        [1.9472e-02, 9.8053e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.7133e-09, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2052, 0.1426],
         [0.6001, 0.1351]],

        [[0.6357, 0.1354],
         [0.6542, 0.6795]],

        [[0.5551, 0.1504],
         [0.6591, 0.5373]],

        [[0.6571, 0.0654],
         [0.6346, 0.7149]],

        [[0.5924, 0.1375],
         [0.6989, 0.6008]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005396094422863973
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.01644068827141728
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.00038912871648324923
Global Adjusted Rand Index: -0.0004307789589913587
Average Adjusted Rand Index: 0.002286744513007311
9966.080437724544
[-0.0002574852870843536, -0.0004307789589913587] [0.0039940787230917695, 0.002286744513007311] [9806.9509417201, 9808.9058004593]
-------------------------------------
This iteration is 40
True Objective function: Loss = -9931.89538342151
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21975.641528367723
Iteration 100: Loss = -9793.817333135506
Iteration 200: Loss = -9792.587025839839
Iteration 300: Loss = -9791.98789124627
Iteration 400: Loss = -9791.268087266091
Iteration 500: Loss = -9790.166037006886
Iteration 600: Loss = -9789.856168821834
Iteration 700: Loss = -9789.678311690219
Iteration 800: Loss = -9789.582023785657
Iteration 900: Loss = -9789.52639395837
Iteration 1000: Loss = -9789.488327252486
Iteration 1100: Loss = -9789.459291486184
Iteration 1200: Loss = -9789.435645315443
Iteration 1300: Loss = -9789.415717962955
Iteration 1400: Loss = -9789.398597508716
Iteration 1500: Loss = -9789.383608133425
Iteration 1600: Loss = -9789.370478132425
Iteration 1700: Loss = -9789.358790358327
Iteration 1800: Loss = -9789.348393166065
Iteration 1900: Loss = -9789.339041436007
Iteration 2000: Loss = -9789.330677960193
Iteration 2100: Loss = -9789.323150038948
Iteration 2200: Loss = -9789.31632034501
Iteration 2300: Loss = -9789.310202934397
Iteration 2400: Loss = -9789.30470816042
Iteration 2500: Loss = -9789.299688606232
Iteration 2600: Loss = -9789.295174803925
Iteration 2700: Loss = -9789.29107942514
Iteration 2800: Loss = -9789.287379730127
Iteration 2900: Loss = -9789.284020873869
Iteration 3000: Loss = -9789.281018743332
Iteration 3100: Loss = -9789.278234056233
Iteration 3200: Loss = -9789.275747899248
Iteration 3300: Loss = -9789.273468030577
Iteration 3400: Loss = -9789.27136312076
Iteration 3500: Loss = -9789.269427410436
Iteration 3600: Loss = -9789.26766165088
Iteration 3700: Loss = -9789.266031202988
Iteration 3800: Loss = -9789.26449976551
Iteration 3900: Loss = -9789.263123184717
Iteration 4000: Loss = -9789.261849402545
Iteration 4100: Loss = -9789.260610026633
Iteration 4200: Loss = -9789.259476164292
Iteration 4300: Loss = -9789.25843169755
Iteration 4400: Loss = -9789.257452056821
Iteration 4500: Loss = -9789.25654492712
Iteration 4600: Loss = -9789.256142435934
Iteration 4700: Loss = -9789.25487878138
Iteration 4800: Loss = -9789.254104970043
Iteration 4900: Loss = -9789.253428119906
Iteration 5000: Loss = -9789.252757886607
Iteration 5100: Loss = -9789.252505131415
Iteration 5200: Loss = -9789.251520714308
Iteration 5300: Loss = -9789.250965301226
Iteration 5400: Loss = -9789.251207312896
1
Iteration 5500: Loss = -9789.249961160709
Iteration 5600: Loss = -9789.249489007045
Iteration 5700: Loss = -9789.249031331548
Iteration 5800: Loss = -9789.248646675127
Iteration 5900: Loss = -9789.248247683305
Iteration 6000: Loss = -9789.24787181802
Iteration 6100: Loss = -9789.247517991475
Iteration 6200: Loss = -9789.247199839687
Iteration 6300: Loss = -9789.246912135555
Iteration 6400: Loss = -9789.248945343566
1
Iteration 6500: Loss = -9789.246328305022
Iteration 6600: Loss = -9789.246091449035
Iteration 6700: Loss = -9789.245831930471
Iteration 6800: Loss = -9789.245574464989
Iteration 6900: Loss = -9789.246329799978
1
Iteration 7000: Loss = -9789.245157824103
Iteration 7100: Loss = -9789.252932591118
1
Iteration 7200: Loss = -9789.24477623724
Iteration 7300: Loss = -9789.244599244024
Iteration 7400: Loss = -9789.24446323792
Iteration 7500: Loss = -9789.24515442635
1
Iteration 7600: Loss = -9789.244403061486
Iteration 7700: Loss = -9789.243997365433
Iteration 7800: Loss = -9789.243886291357
Iteration 7900: Loss = -9789.24381400315
Iteration 8000: Loss = -9789.257549129317
1
Iteration 8100: Loss = -9789.243487532985
Iteration 8200: Loss = -9789.243412940927
Iteration 8300: Loss = -9789.300603743624
1
Iteration 8400: Loss = -9789.24316966591
Iteration 8500: Loss = -9789.244110241447
1
Iteration 8600: Loss = -9789.242991832749
Iteration 8700: Loss = -9789.242907395248
Iteration 8800: Loss = -9789.27300597822
1
Iteration 8900: Loss = -9789.242773415956
Iteration 9000: Loss = -9789.242667751052
Iteration 9100: Loss = -9789.253972819277
1
Iteration 9200: Loss = -9789.24253582748
Iteration 9300: Loss = -9789.24249766807
Iteration 9400: Loss = -9789.252861673001
1
Iteration 9500: Loss = -9789.242346342284
Iteration 9600: Loss = -9789.242319502551
Iteration 9700: Loss = -9789.24572446456
1
Iteration 9800: Loss = -9789.242263632517
Iteration 9900: Loss = -9789.242188266562
Iteration 10000: Loss = -9789.243218373851
1
Iteration 10100: Loss = -9789.242143761701
Iteration 10200: Loss = -9789.24207277998
Iteration 10300: Loss = -9789.256758982929
1
Iteration 10400: Loss = -9789.241984174107
Iteration 10500: Loss = -9789.241964512948
Iteration 10600: Loss = -9789.241911477051
Iteration 10700: Loss = -9789.241917163703
Iteration 10800: Loss = -9789.241989308537
Iteration 10900: Loss = -9789.24183847835
Iteration 11000: Loss = -9789.241786706525
Iteration 11100: Loss = -9789.250849133634
1
Iteration 11200: Loss = -9789.241820113268
Iteration 11300: Loss = -9789.241728013478
Iteration 11400: Loss = -9789.294855091044
1
Iteration 11500: Loss = -9789.241680824867
Iteration 11600: Loss = -9789.241717389164
Iteration 11700: Loss = -9789.242116435958
1
Iteration 11800: Loss = -9789.241659931464
Iteration 11900: Loss = -9789.495950037173
1
Iteration 12000: Loss = -9789.244629933704
2
Iteration 12100: Loss = -9789.490365773454
3
Iteration 12200: Loss = -9789.245353315257
4
Iteration 12300: Loss = -9789.243513477095
5
Iteration 12400: Loss = -9789.242135484907
6
Iteration 12500: Loss = -9789.258690663817
7
Iteration 12600: Loss = -9789.242417155627
8
Iteration 12700: Loss = -9789.242307318313
9
Iteration 12800: Loss = -9789.46744311892
10
Iteration 12900: Loss = -9789.241675668172
Iteration 13000: Loss = -9789.25346597788
1
Iteration 13100: Loss = -9789.24478660062
2
Iteration 13200: Loss = -9789.242259324095
3
Iteration 13300: Loss = -9789.24152033411
Iteration 13400: Loss = -9789.25721133324
1
Iteration 13500: Loss = -9789.241446883243
Iteration 13600: Loss = -9789.241449786412
Iteration 13700: Loss = -9789.244898854931
1
Iteration 13800: Loss = -9789.241468402715
Iteration 13900: Loss = -9789.2416444507
1
Iteration 14000: Loss = -9789.241414473172
Iteration 14100: Loss = -9789.24663239709
1
Iteration 14200: Loss = -9789.244933063883
2
Iteration 14300: Loss = -9789.241417741789
Iteration 14400: Loss = -9789.259805701005
1
Iteration 14500: Loss = -9789.241391527623
Iteration 14600: Loss = -9789.2469299653
1
Iteration 14700: Loss = -9789.241419245343
Iteration 14800: Loss = -9789.246035927563
1
Iteration 14900: Loss = -9789.241396909116
Iteration 15000: Loss = -9789.244156011722
1
Iteration 15100: Loss = -9789.241561074252
2
Iteration 15200: Loss = -9789.24287928208
3
Iteration 15300: Loss = -9789.30052023442
4
Iteration 15400: Loss = -9789.241473446022
Iteration 15500: Loss = -9789.42528362701
1
Iteration 15600: Loss = -9789.241367906005
Iteration 15700: Loss = -9789.253821949222
1
Iteration 15800: Loss = -9789.433770167325
2
Iteration 15900: Loss = -9789.24331394809
3
Iteration 16000: Loss = -9789.245510744611
4
Iteration 16100: Loss = -9789.24157597562
5
Iteration 16200: Loss = -9789.241624930733
6
Iteration 16300: Loss = -9789.246605361448
7
Iteration 16400: Loss = -9789.256669332337
8
Iteration 16500: Loss = -9789.242118494076
9
Iteration 16600: Loss = -9789.260532388114
10
Iteration 16700: Loss = -9789.241584961761
11
Iteration 16800: Loss = -9789.241748045642
12
Iteration 16900: Loss = -9789.282357673943
13
Iteration 17000: Loss = -9789.242403791792
14
Iteration 17100: Loss = -9789.275336708863
15
Stopping early at iteration 17100 due to no improvement.
pi: tensor([[9.5064e-01, 4.9356e-02],
        [9.9999e-01, 5.9295e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9073, 0.0927], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1299, 0.1748],
         [0.5412, 0.2816]],

        [[0.7256, 0.1742],
         [0.7165, 0.6875]],

        [[0.7244, 0.1474],
         [0.6247, 0.7005]],

        [[0.5800, 0.1769],
         [0.6349, 0.5021]],

        [[0.5216, 0.2063],
         [0.5634, 0.6752]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.018772080923839488
Global Adjusted Rand Index: 0.008004824907565605
Average Adjusted Rand Index: 0.004838588123056506
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22095.356950322803
Iteration 100: Loss = -9794.290802686977
Iteration 200: Loss = -9792.2049790921
Iteration 300: Loss = -9791.405579885002
Iteration 400: Loss = -9790.795244843195
Iteration 500: Loss = -9790.460313437869
Iteration 600: Loss = -9790.241099763429
Iteration 700: Loss = -9790.086660777917
Iteration 800: Loss = -9789.968839326562
Iteration 900: Loss = -9789.848188918215
Iteration 1000: Loss = -9789.699746438762
Iteration 1100: Loss = -9789.58982363156
Iteration 1200: Loss = -9789.531112879116
Iteration 1300: Loss = -9789.493011944835
Iteration 1400: Loss = -9789.464736123711
Iteration 1500: Loss = -9789.442023209103
Iteration 1600: Loss = -9789.422930467546
Iteration 1700: Loss = -9789.406404889782
Iteration 1800: Loss = -9789.391811231462
Iteration 1900: Loss = -9789.378821170107
Iteration 2000: Loss = -9789.367118436568
Iteration 2100: Loss = -9789.35652363643
Iteration 2200: Loss = -9789.346875726074
Iteration 2300: Loss = -9789.338147336523
Iteration 2400: Loss = -9789.330175903022
Iteration 2500: Loss = -9789.322939373107
Iteration 2600: Loss = -9789.31630640177
Iteration 2700: Loss = -9789.31028451748
Iteration 2800: Loss = -9789.304801838643
Iteration 2900: Loss = -9789.299825342056
Iteration 3000: Loss = -9789.29532472784
Iteration 3100: Loss = -9789.291234231137
Iteration 3200: Loss = -9789.287547410164
Iteration 3300: Loss = -9789.284179910203
Iteration 3400: Loss = -9789.281177852425
Iteration 3500: Loss = -9789.278461384449
Iteration 3600: Loss = -9789.275933390078
Iteration 3700: Loss = -9789.27367232166
Iteration 3800: Loss = -9789.271586415936
Iteration 3900: Loss = -9789.269717186127
Iteration 4000: Loss = -9789.267965147437
Iteration 4100: Loss = -9789.266398988575
Iteration 4200: Loss = -9789.26491792326
Iteration 4300: Loss = -9789.26349893678
Iteration 4400: Loss = -9789.262219924603
Iteration 4500: Loss = -9789.261040610278
Iteration 4600: Loss = -9789.259964213386
Iteration 4700: Loss = -9789.26193290498
1
Iteration 4800: Loss = -9789.257900853016
Iteration 4900: Loss = -9789.256989685397
Iteration 5000: Loss = -9789.256357629434
Iteration 5100: Loss = -9789.255338675488
Iteration 5200: Loss = -9789.254558794148
Iteration 5300: Loss = -9789.253833081104
Iteration 5400: Loss = -9789.253178876292
Iteration 5500: Loss = -9789.252528317036
Iteration 5600: Loss = -9789.25196216583
Iteration 5700: Loss = -9789.25136867414
Iteration 5800: Loss = -9789.254282849666
1
Iteration 5900: Loss = -9789.25033631458
Iteration 6000: Loss = -9789.249863258281
Iteration 6100: Loss = -9789.24943106536
Iteration 6200: Loss = -9789.249004197369
Iteration 6300: Loss = -9789.253143405836
1
Iteration 6400: Loss = -9789.248214371684
Iteration 6500: Loss = -9789.247845893688
Iteration 6600: Loss = -9789.24762484473
Iteration 6700: Loss = -9789.247188078472
Iteration 6800: Loss = -9789.246906629876
Iteration 6900: Loss = -9789.246616904675
Iteration 7000: Loss = -9789.246336832999
Iteration 7100: Loss = -9789.248089881734
1
Iteration 7200: Loss = -9789.245835641503
Iteration 7300: Loss = -9789.245586298126
Iteration 7400: Loss = -9789.245378685677
Iteration 7500: Loss = -9789.245172777226
Iteration 7600: Loss = -9789.24502775664
Iteration 7700: Loss = -9789.24476433775
Iteration 7800: Loss = -9789.24467095823
Iteration 7900: Loss = -9789.250283519596
1
Iteration 8000: Loss = -9789.244851900256
2
Iteration 8100: Loss = -9789.244145983977
Iteration 8200: Loss = -9789.244035463504
Iteration 8300: Loss = -9789.243878966443
Iteration 8400: Loss = -9789.24372540047
Iteration 8500: Loss = -9789.25007857683
1
Iteration 8600: Loss = -9789.24351396836
Iteration 8700: Loss = -9789.243397651591
Iteration 8800: Loss = -9789.494846105308
1
Iteration 8900: Loss = -9789.243205764731
Iteration 9000: Loss = -9789.243075917337
Iteration 9100: Loss = -9789.256652659622
1
Iteration 9200: Loss = -9789.242927345806
Iteration 9300: Loss = -9789.242842406778
Iteration 9400: Loss = -9789.242769382161
Iteration 9500: Loss = -9789.24269906094
Iteration 9600: Loss = -9789.242634573302
Iteration 9700: Loss = -9789.242767177793
1
Iteration 9800: Loss = -9789.242502878225
Iteration 9900: Loss = -9789.242434272997
Iteration 10000: Loss = -9789.242429196014
Iteration 10100: Loss = -9789.242312703545
Iteration 10200: Loss = -9789.242426889932
1
Iteration 10300: Loss = -9789.242295093853
Iteration 10400: Loss = -9789.242203403646
Iteration 10500: Loss = -9789.24215778461
Iteration 10600: Loss = -9789.242744913921
1
Iteration 10700: Loss = -9789.242035710262
Iteration 10800: Loss = -9789.242196067236
1
Iteration 10900: Loss = -9789.242024352528
Iteration 11000: Loss = -9789.241953685902
Iteration 11100: Loss = -9789.413504891303
1
Iteration 11200: Loss = -9789.241893590917
Iteration 11300: Loss = -9789.24187087132
Iteration 11400: Loss = -9789.268549937258
1
Iteration 11500: Loss = -9789.242507643543
2
Iteration 11600: Loss = -9789.241803015178
Iteration 11700: Loss = -9789.327638153263
1
Iteration 11800: Loss = -9789.24173350904
Iteration 11900: Loss = -9789.24173202043
Iteration 12000: Loss = -9789.242077073426
1
Iteration 12100: Loss = -9789.241668712528
Iteration 12200: Loss = -9789.241710170854
Iteration 12300: Loss = -9789.242648841308
1
Iteration 12400: Loss = -9789.241842997046
2
Iteration 12500: Loss = -9789.241631105358
Iteration 12600: Loss = -9789.272267239114
1
Iteration 12700: Loss = -9789.241593394392
Iteration 12800: Loss = -9789.241935167209
1
Iteration 12900: Loss = -9789.241644926316
Iteration 13000: Loss = -9789.241552654752
Iteration 13100: Loss = -9789.242971386126
1
Iteration 13200: Loss = -9789.244122997177
2
Iteration 13300: Loss = -9789.242528879771
3
Iteration 13400: Loss = -9789.242645086255
4
Iteration 13500: Loss = -9789.241489139098
Iteration 13600: Loss = -9789.245708329361
1
Iteration 13700: Loss = -9789.241802763949
2
Iteration 13800: Loss = -9789.241508147908
Iteration 13900: Loss = -9789.423606883805
1
Iteration 14000: Loss = -9789.24147014001
Iteration 14100: Loss = -9789.245868755626
1
Iteration 14200: Loss = -9789.241422312496
Iteration 14300: Loss = -9789.24183288641
1
Iteration 14400: Loss = -9789.241640301005
2
Iteration 14500: Loss = -9789.24194979927
3
Iteration 14600: Loss = -9789.241461940348
Iteration 14700: Loss = -9789.256620971786
1
Iteration 14800: Loss = -9789.242615497778
2
Iteration 14900: Loss = -9789.279285627497
3
Iteration 15000: Loss = -9789.241622149146
4
Iteration 15100: Loss = -9789.2414181315
Iteration 15200: Loss = -9789.244712185578
1
Iteration 15300: Loss = -9789.25345909681
2
Iteration 15400: Loss = -9789.241391038671
Iteration 15500: Loss = -9789.24159682532
1
Iteration 15600: Loss = -9789.310086196503
2
Iteration 15700: Loss = -9789.241688051226
3
Iteration 15800: Loss = -9789.242388499733
4
Iteration 15900: Loss = -9789.241325995134
Iteration 16000: Loss = -9789.242494126855
1
Iteration 16100: Loss = -9789.293349341293
2
Iteration 16200: Loss = -9789.242506611863
3
Iteration 16300: Loss = -9789.242460571064
4
Iteration 16400: Loss = -9789.350445264357
5
Iteration 16500: Loss = -9789.241328149166
Iteration 16600: Loss = -9789.249831124385
1
Iteration 16700: Loss = -9789.263315425958
2
Iteration 16800: Loss = -9789.241356843093
Iteration 16900: Loss = -9789.248306045498
1
Iteration 17000: Loss = -9789.24429604978
2
Iteration 17100: Loss = -9789.241362679673
Iteration 17200: Loss = -9789.322879871945
1
Iteration 17300: Loss = -9789.268579310672
2
Iteration 17400: Loss = -9789.244341956724
3
Iteration 17500: Loss = -9789.241365397336
Iteration 17600: Loss = -9789.253247380295
1
Iteration 17700: Loss = -9789.24131259091
Iteration 17800: Loss = -9789.244308908646
1
Iteration 17900: Loss = -9789.242591323697
2
Iteration 18000: Loss = -9789.243706841702
3
Iteration 18100: Loss = -9789.241466148542
4
Iteration 18200: Loss = -9789.241474956043
5
Iteration 18300: Loss = -9789.364827376456
6
Iteration 18400: Loss = -9789.241500613474
7
Iteration 18500: Loss = -9789.241986926892
8
Iteration 18600: Loss = -9789.260336478617
9
Iteration 18700: Loss = -9789.241358572666
Iteration 18800: Loss = -9789.247096490675
1
Iteration 18900: Loss = -9789.241322908163
Iteration 19000: Loss = -9789.256773579347
1
Iteration 19100: Loss = -9789.241300395952
Iteration 19200: Loss = -9789.244377399124
1
Iteration 19300: Loss = -9789.241749194649
2
Iteration 19400: Loss = -9789.301693320142
3
Iteration 19500: Loss = -9789.241326951349
Iteration 19600: Loss = -9789.259181644094
1
Iteration 19700: Loss = -9789.24230404709
2
Iteration 19800: Loss = -9789.247207634762
3
Iteration 19900: Loss = -9789.247648703713
4
pi: tensor([[9.4987e-01, 5.0133e-02],
        [1.0000e+00, 1.7929e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9057, 0.0943], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1294, 0.1741],
         [0.5609, 0.2804]],

        [[0.6970, 0.1739],
         [0.6439, 0.6886]],

        [[0.7118, 0.1484],
         [0.6652, 0.5272]],

        [[0.6672, 0.1769],
         [0.6994, 0.7041]],

        [[0.6243, 0.2054],
         [0.5533, 0.6398]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.018772080923839488
Global Adjusted Rand Index: 0.008004824907565605
Average Adjusted Rand Index: 0.004838588123056506
9931.89538342151
[0.008004824907565605, 0.008004824907565605] [0.004838588123056506, 0.004838588123056506] [9789.275336708863, 9789.241440961976]
-------------------------------------
This iteration is 41
True Objective function: Loss = -10126.179542585178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22804.28317952219
Iteration 100: Loss = -10008.269144279315
Iteration 200: Loss = -10006.320047941852
Iteration 300: Loss = -10005.47273508101
Iteration 400: Loss = -10005.074942358528
Iteration 500: Loss = -10004.878895320591
Iteration 600: Loss = -10004.774377238396
Iteration 700: Loss = -10004.713458826023
Iteration 800: Loss = -10004.674788279728
Iteration 900: Loss = -10004.648319099328
Iteration 1000: Loss = -10004.628980130667
Iteration 1100: Loss = -10004.613922700784
Iteration 1200: Loss = -10004.601140281271
Iteration 1300: Loss = -10004.589494227605
Iteration 1400: Loss = -10004.577938509727
Iteration 1500: Loss = -10004.56587379593
Iteration 1600: Loss = -10004.55250184919
Iteration 1700: Loss = -10004.537268941727
Iteration 1800: Loss = -10004.519409933931
Iteration 1900: Loss = -10004.49801922828
Iteration 2000: Loss = -10004.472240466166
Iteration 2100: Loss = -10004.440903089135
Iteration 2200: Loss = -10004.40335425898
Iteration 2300: Loss = -10004.359581684957
Iteration 2400: Loss = -10004.311561350087
Iteration 2500: Loss = -10004.26093873319
Iteration 2600: Loss = -10004.212716641608
Iteration 2700: Loss = -10004.166234108527
Iteration 2800: Loss = -10004.127796412595
Iteration 2900: Loss = -10004.096333920508
Iteration 3000: Loss = -10004.070837465206
Iteration 3100: Loss = -10004.051968261827
Iteration 3200: Loss = -10004.03098123547
Iteration 3300: Loss = -10004.01396147474
Iteration 3400: Loss = -10003.99576960001
Iteration 3500: Loss = -10003.974484049231
Iteration 3600: Loss = -10003.94157543708
Iteration 3700: Loss = -10003.84762160546
Iteration 3800: Loss = -10003.428179858429
Iteration 3900: Loss = -10002.731728332656
Iteration 4000: Loss = -10001.540426051086
Iteration 4100: Loss = -10000.657995088019
Iteration 4200: Loss = -9994.82768209825
Iteration 4300: Loss = -9994.672190462887
Iteration 4400: Loss = -9994.642716284963
Iteration 4500: Loss = -9994.629755055215
Iteration 4600: Loss = -9994.622350310245
Iteration 4700: Loss = -9994.617504150228
Iteration 4800: Loss = -9994.614044437825
Iteration 4900: Loss = -9994.611537155151
Iteration 5000: Loss = -9994.609616162958
Iteration 5100: Loss = -9994.607972299993
Iteration 5200: Loss = -9994.606646892651
Iteration 5300: Loss = -9994.605570394202
Iteration 5400: Loss = -9994.604659403189
Iteration 5500: Loss = -9994.603877923952
Iteration 5600: Loss = -9994.60326431872
Iteration 5700: Loss = -9994.602700674406
Iteration 5800: Loss = -9994.6022175489
Iteration 5900: Loss = -9994.601782153
Iteration 6000: Loss = -9994.60138574832
Iteration 6100: Loss = -9994.601059684808
Iteration 6200: Loss = -9994.601738798921
1
Iteration 6300: Loss = -9994.600449279002
Iteration 6400: Loss = -9994.6002140974
Iteration 6500: Loss = -9994.600208339663
Iteration 6600: Loss = -9994.599763715745
Iteration 6700: Loss = -9994.59955529762
Iteration 6800: Loss = -9994.599516101975
Iteration 6900: Loss = -9994.690751526183
1
Iteration 7000: Loss = -9994.599014306246
Iteration 7100: Loss = -9994.641934795683
1
Iteration 7200: Loss = -9994.5986931403
Iteration 7300: Loss = -9994.59857261959
Iteration 7400: Loss = -9994.59948272013
1
Iteration 7500: Loss = -9994.598293848918
Iteration 7600: Loss = -9994.598184151106
Iteration 7700: Loss = -9994.60141251574
1
Iteration 7800: Loss = -9994.597997534733
Iteration 7900: Loss = -9994.597929189053
Iteration 8000: Loss = -9994.597846917106
Iteration 8100: Loss = -9994.598075163784
1
Iteration 8200: Loss = -9994.597686073603
Iteration 8300: Loss = -9994.597627608995
Iteration 8400: Loss = -9994.59757419465
Iteration 8500: Loss = -9994.597483368336
Iteration 8600: Loss = -9994.597409612297
Iteration 8700: Loss = -9994.597366891474
Iteration 8800: Loss = -9994.597301230553
Iteration 8900: Loss = -9994.597516366091
1
Iteration 9000: Loss = -9994.597222962631
Iteration 9100: Loss = -9994.597184698932
Iteration 9200: Loss = -9994.598798111334
1
Iteration 9300: Loss = -9994.597140572714
Iteration 9400: Loss = -9994.59707207512
Iteration 9500: Loss = -9994.597048784828
Iteration 9600: Loss = -9994.59706234443
Iteration 9700: Loss = -9994.59698000113
Iteration 9800: Loss = -9994.59694870712
Iteration 9900: Loss = -9994.600743572786
1
Iteration 10000: Loss = -9994.596887962241
Iteration 10100: Loss = -9994.597056884573
1
Iteration 10200: Loss = -9994.696832187185
2
Iteration 10300: Loss = -9994.596798583321
Iteration 10400: Loss = -9994.597801937383
1
Iteration 10500: Loss = -9994.596833100639
Iteration 10600: Loss = -9994.596935503667
1
Iteration 10700: Loss = -9994.604180375343
2
Iteration 10800: Loss = -9994.596705854277
Iteration 10900: Loss = -9994.597539314267
1
Iteration 11000: Loss = -9994.59976907301
2
Iteration 11100: Loss = -9994.596884738632
3
Iteration 11200: Loss = -9994.597203687475
4
Iteration 11300: Loss = -9994.60592357344
5
Iteration 11400: Loss = -9994.613558483983
6
Iteration 11500: Loss = -9994.59662023636
Iteration 11600: Loss = -9994.596604456437
Iteration 11700: Loss = -9994.60301522956
1
Iteration 11800: Loss = -9994.596898301432
2
Iteration 11900: Loss = -9994.596936472564
3
Iteration 12000: Loss = -9994.59841912005
4
Iteration 12100: Loss = -9994.597637735746
5
Iteration 12200: Loss = -9994.596539969076
Iteration 12300: Loss = -9994.612235883356
1
Iteration 12400: Loss = -9994.603248971198
2
Iteration 12500: Loss = -9994.59730361617
3
Iteration 12600: Loss = -9994.607960636877
4
Iteration 12700: Loss = -9994.59651263857
Iteration 12800: Loss = -9994.596676964184
1
Iteration 12900: Loss = -9994.59771787359
2
Iteration 13000: Loss = -9994.596479614134
Iteration 13100: Loss = -9994.597383554526
1
Iteration 13200: Loss = -9994.632498258112
2
Iteration 13300: Loss = -9994.59648280328
Iteration 13400: Loss = -9994.59717765208
1
Iteration 13500: Loss = -9994.604782037915
2
Iteration 13600: Loss = -9994.606949097733
3
Iteration 13700: Loss = -9994.596751582541
4
Iteration 13800: Loss = -9994.59664524031
5
Iteration 13900: Loss = -9994.626452817309
6
Iteration 14000: Loss = -9994.596383137223
Iteration 14100: Loss = -9994.597217077704
1
Iteration 14200: Loss = -9994.598634609123
2
Iteration 14300: Loss = -9994.597100283732
3
Iteration 14400: Loss = -9994.613261842254
4
Iteration 14500: Loss = -9994.617850083616
5
Iteration 14600: Loss = -9994.596423750521
Iteration 14700: Loss = -9994.598154054225
1
Iteration 14800: Loss = -9994.628876964887
2
Iteration 14900: Loss = -9994.597534583747
3
Iteration 15000: Loss = -9994.597466559848
4
Iteration 15100: Loss = -9994.614847284398
5
Iteration 15200: Loss = -9994.596380133356
Iteration 15300: Loss = -9994.596405895474
Iteration 15400: Loss = -9994.626817661045
1
Iteration 15500: Loss = -9994.61547386577
2
Iteration 15600: Loss = -9994.598913622578
3
Iteration 15700: Loss = -9994.751239610476
4
Iteration 15800: Loss = -9994.60117486901
5
Iteration 15900: Loss = -9994.596513255334
6
Iteration 16000: Loss = -9994.7962067576
7
Iteration 16100: Loss = -9994.596410976208
Iteration 16200: Loss = -9994.604940771767
1
Iteration 16300: Loss = -9994.596917737277
2
Iteration 16400: Loss = -9994.596563898227
3
Iteration 16500: Loss = -9994.754595035305
4
Iteration 16600: Loss = -9994.596340352244
Iteration 16700: Loss = -9994.596738395521
1
Iteration 16800: Loss = -9994.599239963876
2
Iteration 16900: Loss = -9994.625968645965
3
Iteration 17000: Loss = -9994.596365790632
Iteration 17100: Loss = -9994.596517018133
1
Iteration 17200: Loss = -9994.597528291271
2
Iteration 17300: Loss = -9994.59727898303
3
Iteration 17400: Loss = -9994.596345673237
Iteration 17500: Loss = -9994.644479463277
1
Iteration 17600: Loss = -9994.654652841253
2
Iteration 17700: Loss = -9994.63475192918
3
Iteration 17800: Loss = -9994.597081573258
4
Iteration 17900: Loss = -9994.596569971116
5
Iteration 18000: Loss = -9994.657892402864
6
Iteration 18100: Loss = -9994.596428539333
Iteration 18200: Loss = -9994.596790747906
1
Iteration 18300: Loss = -9994.59735402992
2
Iteration 18400: Loss = -9994.596641013828
3
Iteration 18500: Loss = -9994.597171784533
4
Iteration 18600: Loss = -9994.597885258834
5
Iteration 18700: Loss = -9994.678525377845
6
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 42%|████▏     | 42/100 [14:15:23<19:53:30, 1234.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 43%|████▎     | 43/100 [14:36:53<19:48:52, 1251.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 44%|████▍     | 44/100 [14:58:17<19:37:01, 1261.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 45%|████▌     | 45/100 [15:19:43<19:22:50, 1268.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 46%|████▌     | 46/100 [15:31:55<16:36:49, 1107.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 47%|████▋     | 47/100 [15:45:45<15:04:50, 1024.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 48%|████▊     | 48/100 [16:06:58<15:52:32, 1099.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 49%|████▉     | 49/100 [16:28:39<16:25:37, 1159.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 50%|█████     | 50/100 [16:50:31<16:44:22, 1205.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 51%|█████     | 51/100 [17:10:30<16:22:39, 1203.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 52%|█████▏    | 52/100 [17:31:56<16:22:31, 1228.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 53%|█████▎    | 53/100 [17:53:28<16:17:07, 1247.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 54%|█████▍    | 54/100 [18:15:00<16:06:28, 1260.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 55%|█████▌    | 55/100 [18:36:24<15:50:45, 1267.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 56%|█████▌    | 56/100 [18:51:03<14:04:11, 1151.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 57%|█████▋    | 57/100 [19:07:18<13:07:10, 1098.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 58%|█████▊    | 58/100 [19:28:40<13:27:27, 1153.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 59%|█████▉    | 59/100 [19:50:09<13:35:56, 1194.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 60%|██████    | 60/100 [20:11:58<13:38:56, 1228.40s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 61%|██████    | 61/100 [20:33:16<13:28:18, 1243.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 62%|██████▏   | 62/100 [20:54:43<13:15:44, 1256.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 63%|██████▎   | 63/100 [21:16:05<12:59:31, 1264.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 64%|██████▍   | 64/100 [21:33:48<12:02:22, 1203.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 65%|██████▌   | 65/100 [21:50:16<11:04:26, 1139.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 66%|██████▌   | 66/100 [22:11:41<11:10:19, 1182.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 67%|██████▋   | 67/100 [22:34:46<11:23:50, 1243.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 68%|██████▊   | 68/100 [22:56:14<11:10:19, 1256.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 69%|██████▉   | 69/100 [23:17:40<10:53:49, 1265.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 70%|███████   | 70/100 [23:39:04<10:35:35, 1271.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 71%|███████   | 71/100 [24:00:31<10:16:41, 1275.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 72%|███████▏  | 72/100 [24:22:03<9:57:35, 1280.55s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 73%|███████▎  | 73/100 [24:40:04<9:09:25, 1220.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 74%|███████▍  | 74/100 [25:01:35<8:58:04, 1241.71s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 75%|███████▌  | 75/100 [25:18:56<8:12:23, 1181.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 76%|███████▌  | 76/100 [25:40:23<8:05:16, 1213.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 77%|███████▋  | 77/100 [25:58:05<7:27:41, 1167.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 78%|███████▊  | 78/100 [26:19:34<7:21:30, 1204.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 79%|███████▉  | 79/100 [26:40:59<7:09:56, 1228.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 80%|████████  | 80/100 [27:02:32<6:55:54, 1247.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 81%|████████  | 81/100 [27:23:56<6:38:37, 1258.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 82%|████████▏ | 82/100 [27:45:25<6:20:22, 1267.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
Iteration 18800: Loss = -9994.596383201344
Iteration 18900: Loss = -9994.618936271596
1
Iteration 19000: Loss = -9994.603765002905
2
Iteration 19100: Loss = -9994.596415208327
Iteration 19200: Loss = -9994.598192503428
1
Iteration 19300: Loss = -9994.633466491518
2
Iteration 19400: Loss = -9994.659145949392
3
Iteration 19500: Loss = -9994.596421769667
Iteration 19600: Loss = -9994.596404072445
Iteration 19700: Loss = -9994.596534391918
1
Iteration 19800: Loss = -9994.596546711935
2
Iteration 19900: Loss = -9994.596430830814
pi: tensor([[9.8547e-01, 1.4533e-02],
        [3.1169e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7503, 0.2497], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1583, 0.1319],
         [0.6235, 0.1518]],

        [[0.6781, 0.1018],
         [0.5473, 0.5646]],

        [[0.6886, 0.1155],
         [0.5774, 0.6107]],

        [[0.5976, 0.1148],
         [0.7028, 0.5626]],

        [[0.6851, 0.0990],
         [0.7205, 0.5191]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.040760969172684285
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 31
Adjusted Rand Index: 0.1375316154099923
time is 2
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.07054753493148173
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.07117959409714304
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 30
Adjusted Rand Index: 0.15263065995031702
Global Adjusted Rand Index: 0.09596374861255103
Average Adjusted Rand Index: 0.09453007471232369
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25733.75087570211
Iteration 100: Loss = -10006.428909563283
Iteration 200: Loss = -10005.42560361676
Iteration 300: Loss = -10005.08394480573
Iteration 400: Loss = -10004.917254445121
Iteration 500: Loss = -10004.822043107702
Iteration 600: Loss = -10004.761367493202
Iteration 700: Loss = -10004.719463965715
Iteration 800: Loss = -10004.689222374225
Iteration 900: Loss = -10004.667372308453
Iteration 1000: Loss = -10004.65242961684
Iteration 1100: Loss = -10004.642313872315
Iteration 1200: Loss = -10004.634807807546
Iteration 1300: Loss = -10004.628532583336
Iteration 1400: Loss = -10004.622662912247
Iteration 1500: Loss = -10004.616838033136
Iteration 1600: Loss = -10004.610668735719
Iteration 1700: Loss = -10004.603883196853
Iteration 1800: Loss = -10004.595939499797
Iteration 1900: Loss = -10004.586314788954
Iteration 2000: Loss = -10004.573943903415
Iteration 2100: Loss = -10004.55748378249
Iteration 2200: Loss = -10004.534575712263
Iteration 2300: Loss = -10004.501152387762
Iteration 2400: Loss = -10004.449829868217
Iteration 2500: Loss = -10004.367941327246
Iteration 2600: Loss = -10004.243857731577
Iteration 2700: Loss = -10004.106013967335
Iteration 2800: Loss = -10004.010232586692
Iteration 2900: Loss = -10003.899378641217
Iteration 3000: Loss = -10003.165764220583
Iteration 3100: Loss = -10001.54007086639
Iteration 3200: Loss = -9995.530565985344
Iteration 3300: Loss = -9994.629648612912
Iteration 3400: Loss = -9994.61502525351
Iteration 3500: Loss = -9994.60952869192
Iteration 3600: Loss = -9994.606546177514
Iteration 3700: Loss = -9994.60462021924
Iteration 3800: Loss = -9994.603313305888
Iteration 3900: Loss = -9994.602357855945
Iteration 4000: Loss = -9994.601574424085
Iteration 4100: Loss = -9994.604704686233
1
Iteration 4200: Loss = -9994.600468165436
Iteration 4300: Loss = -9994.600069674547
Iteration 4400: Loss = -9994.59975738122
Iteration 4500: Loss = -9994.599422761774
Iteration 4600: Loss = -9994.599126196397
Iteration 4700: Loss = -9994.598944265243
Iteration 4800: Loss = -9994.598699296008
Iteration 4900: Loss = -9994.608332618676
1
Iteration 5000: Loss = -9994.598346118748
Iteration 5100: Loss = -9994.598191073905
Iteration 5200: Loss = -9994.598301732898
1
Iteration 5300: Loss = -9994.597960737605
Iteration 5400: Loss = -9994.597890429051
Iteration 5500: Loss = -9994.597772412757
Iteration 5600: Loss = -9994.597670400779
Iteration 5700: Loss = -9994.598283747751
1
Iteration 5800: Loss = -9994.597498232328
Iteration 5900: Loss = -9994.597455803083
Iteration 6000: Loss = -9994.597519236026
Iteration 6100: Loss = -9994.597331348741
Iteration 6200: Loss = -9994.597270303253
Iteration 6300: Loss = -9994.597219516681
Iteration 6400: Loss = -9994.597147377213
Iteration 6500: Loss = -9994.597132240358
Iteration 6600: Loss = -9994.598835967647
1
Iteration 6700: Loss = -9994.597074604791
Iteration 6800: Loss = -9994.597036657853
Iteration 6900: Loss = -9994.596989908938
Iteration 7000: Loss = -9994.596948288738
Iteration 7100: Loss = -9994.599865675174
1
Iteration 7200: Loss = -9994.67468995991
2
Iteration 7300: Loss = -9994.596887883536
Iteration 7400: Loss = -9994.597602198122
1
Iteration 7500: Loss = -9994.596849842073
Iteration 7600: Loss = -9994.598057447829
1
Iteration 7700: Loss = -9994.596763067653
Iteration 7800: Loss = -9994.640667224965
1
Iteration 7900: Loss = -9994.596743691764
Iteration 8000: Loss = -9994.59672258368
Iteration 8100: Loss = -9994.610175662689
1
Iteration 8200: Loss = -9994.59669063292
Iteration 8300: Loss = -9994.596642866598
Iteration 8400: Loss = -9994.608774059072
1
Iteration 8500: Loss = -9994.596650164212
Iteration 8600: Loss = -9994.596601142508
Iteration 8700: Loss = -9994.596958647673
1
Iteration 8800: Loss = -9994.596586400417
Iteration 8900: Loss = -9994.59659771518
Iteration 9000: Loss = -9994.60259956331
1
Iteration 9100: Loss = -9994.596554173959
Iteration 9200: Loss = -9994.596544056918
Iteration 9300: Loss = -9994.597417686708
1
Iteration 9400: Loss = -9994.59650006247
Iteration 9500: Loss = -9994.596548280191
Iteration 9600: Loss = -9994.59654315108
Iteration 9700: Loss = -9994.5964995305
Iteration 9800: Loss = -9994.602901812597
1
Iteration 9900: Loss = -9994.59649459743
Iteration 10000: Loss = -9994.622371309573
1
Iteration 10100: Loss = -9994.596474249516
Iteration 10200: Loss = -9994.636135467215
1
Iteration 10300: Loss = -9994.596469993894
Iteration 10400: Loss = -9994.769174101939
1
Iteration 10500: Loss = -9994.607017244785
2
Iteration 10600: Loss = -9994.603579806146
3
Iteration 10700: Loss = -9994.609637829386
4
Iteration 10800: Loss = -9994.607628261087
5
Iteration 10900: Loss = -9994.609205617258
6
Iteration 11000: Loss = -9994.628376247336
7
Iteration 11100: Loss = -9994.596509972338
Iteration 11200: Loss = -9994.613004308549
1
Iteration 11300: Loss = -9994.614875030475
2
Iteration 11400: Loss = -9994.611152101628
3
Iteration 11500: Loss = -9994.596423424357
Iteration 11600: Loss = -9994.597174478697
1
Iteration 11700: Loss = -9994.603224041844
2
Iteration 11800: Loss = -9994.60083241015
3
Iteration 11900: Loss = -9994.597880992158
4
Iteration 12000: Loss = -9994.602891961298
5
Iteration 12100: Loss = -9994.596385217188
Iteration 12200: Loss = -9994.597224102878
1
Iteration 12300: Loss = -9994.599652528148
2
Iteration 12400: Loss = -9994.596399729122
Iteration 12500: Loss = -9994.597411622282
1
Iteration 12600: Loss = -9994.596437821208
Iteration 12700: Loss = -9994.596912297804
1
Iteration 12800: Loss = -9994.598593086865
2
Iteration 12900: Loss = -9994.597718590127
3
Iteration 13000: Loss = -9994.599714431726
4
Iteration 13100: Loss = -9994.596481776782
Iteration 13200: Loss = -9994.597640797292
1
Iteration 13300: Loss = -9994.596467447001
Iteration 13400: Loss = -9994.598304699504
1
Iteration 13500: Loss = -9994.601062890093
2
Iteration 13600: Loss = -9994.599398680395
3
Iteration 13700: Loss = -9994.598765159026
4
Iteration 13800: Loss = -9994.598624351871
5
Iteration 13900: Loss = -9994.597331683608
6
Iteration 14000: Loss = -9994.59818268431
7
Iteration 14100: Loss = -9994.697538695233
8
Iteration 14200: Loss = -9994.59635356754
Iteration 14300: Loss = -9994.59705454454
1
Iteration 14400: Loss = -9994.780366563316
2
Iteration 14500: Loss = -9994.59690282784
3
Iteration 14600: Loss = -9994.597270518043
4
Iteration 14700: Loss = -9994.599293923226
5
Iteration 14800: Loss = -9994.679531784057
6
Iteration 14900: Loss = -9994.603272971372
7
Iteration 15000: Loss = -9994.59821716815
8
Iteration 15100: Loss = -9994.598802677348
9
Iteration 15200: Loss = -9994.596425976159
Iteration 15300: Loss = -9994.61099274664
1
Iteration 15400: Loss = -9994.610403360619
2
Iteration 15500: Loss = -9994.603305542927
3
Iteration 15600: Loss = -9994.658637194172
4
Iteration 15700: Loss = -9994.596351228329
Iteration 15800: Loss = -9994.596733960068
1
Iteration 15900: Loss = -9994.596989863921
2
Iteration 16000: Loss = -9994.597508687735
3
Iteration 16100: Loss = -9994.687946245222
4
Iteration 16200: Loss = -9994.598563004007
5
Iteration 16300: Loss = -9994.596353163199
Iteration 16400: Loss = -9994.596596227182
1
Iteration 16500: Loss = -9994.627188223576
2
Iteration 16600: Loss = -9994.596359891068
Iteration 16700: Loss = -9994.602847668684
1
Iteration 16800: Loss = -9994.59635526198
Iteration 16900: Loss = -9994.600943049969
1
Iteration 17000: Loss = -9994.596342685489
Iteration 17100: Loss = -9994.598668445782
1
Iteration 17200: Loss = -9994.597160384608
2
Iteration 17300: Loss = -9994.64679575302
3
Iteration 17400: Loss = -9994.603509168517
4
Iteration 17500: Loss = -9994.601041051961
5
Iteration 17600: Loss = -9994.597570244943
6
Iteration 17700: Loss = -9994.597372393
7
Iteration 17800: Loss = -9994.600364845586
8
Iteration 17900: Loss = -9994.617387170994
9
Iteration 18000: Loss = -9994.60553748573
10
Iteration 18100: Loss = -9994.609924156151
11
Iteration 18200: Loss = -9994.601309501331
12
Iteration 18300: Loss = -9994.596622148129
13
Iteration 18400: Loss = -9994.604324182346
14
Iteration 18500: Loss = -9994.608622738278
15
Stopping early at iteration 18500 due to no improvement.
pi: tensor([[1.0000e+00, 1.3371e-06],
        [1.4557e-02, 9.8544e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2507, 0.7493], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1515, 0.1320],
         [0.7004, 0.1587]],

        [[0.5397, 0.1020],
         [0.5167, 0.5796]],

        [[0.6869, 0.1157],
         [0.7038, 0.5821]],

        [[0.6827, 0.1151],
         [0.7174, 0.6582]],

        [[0.6419, 0.0989],
         [0.6901, 0.6437]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.040760969172684285
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 69
Adjusted Rand Index: 0.1375316154099923
time is 2
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.07054753493148173
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.07117959409714304
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 70
Adjusted Rand Index: 0.15263065995031702
Global Adjusted Rand Index: 0.09596374861255103
Average Adjusted Rand Index: 0.09453007471232369
10126.179542585178
[0.09596374861255103, 0.09596374861255103] [0.09453007471232369, 0.09453007471232369] [9994.596751491677, 9994.608622738278]
-------------------------------------
This iteration is 42
True Objective function: Loss = -9967.869077805504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23925.064825144094
Iteration 100: Loss = -9861.434018974289
Iteration 200: Loss = -9859.854164707569
Iteration 300: Loss = -9858.605495346597
Iteration 400: Loss = -9857.493420885423
Iteration 500: Loss = -9856.705966242522
Iteration 600: Loss = -9855.715275912082
Iteration 700: Loss = -9854.925364518616
Iteration 800: Loss = -9854.361131475338
Iteration 900: Loss = -9853.862197593851
Iteration 1000: Loss = -9853.570517111948
Iteration 1100: Loss = -9853.41238579871
Iteration 1200: Loss = -9853.30716804022
Iteration 1300: Loss = -9853.223964446252
Iteration 1400: Loss = -9853.146476404898
Iteration 1500: Loss = -9853.070223337249
Iteration 1600: Loss = -9852.996833101446
Iteration 1700: Loss = -9852.930137393516
Iteration 1800: Loss = -9852.872628448657
Iteration 1900: Loss = -9852.824554811876
Iteration 2000: Loss = -9852.785120475944
Iteration 2100: Loss = -9852.7531518015
Iteration 2200: Loss = -9852.727400422382
Iteration 2300: Loss = -9852.706630714656
Iteration 2400: Loss = -9852.689726378085
Iteration 2500: Loss = -9852.675808517064
Iteration 2600: Loss = -9852.66426377456
Iteration 2700: Loss = -9852.654556318499
Iteration 2800: Loss = -9852.646259375693
Iteration 2900: Loss = -9852.639198248557
Iteration 3000: Loss = -9852.633081755373
Iteration 3100: Loss = -9852.627706755247
Iteration 3200: Loss = -9852.623014056333
Iteration 3300: Loss = -9852.6188354267
Iteration 3400: Loss = -9852.615140807648
Iteration 3500: Loss = -9852.611855800284
Iteration 3600: Loss = -9852.608901652346
Iteration 3700: Loss = -9852.606187556721
Iteration 3800: Loss = -9852.603742786834
Iteration 3900: Loss = -9852.601532890934
Iteration 4000: Loss = -9852.599521485568
Iteration 4100: Loss = -9852.597677639957
Iteration 4200: Loss = -9852.595989758172
Iteration 4300: Loss = -9852.594420836413
Iteration 4400: Loss = -9852.592971355254
Iteration 4500: Loss = -9852.591674437597
Iteration 4600: Loss = -9852.590469893372
Iteration 4700: Loss = -9852.58928775375
Iteration 4800: Loss = -9852.588249734168
Iteration 4900: Loss = -9852.587276669301
Iteration 5000: Loss = -9852.586366734104
Iteration 5100: Loss = -9852.585501561798
Iteration 5200: Loss = -9852.584696214977
Iteration 5300: Loss = -9852.583975959838
Iteration 5400: Loss = -9852.583287047944
Iteration 5500: Loss = -9852.582670905473
Iteration 5600: Loss = -9852.582048775475
Iteration 5700: Loss = -9852.581478212061
Iteration 5800: Loss = -9852.580939628855
Iteration 5900: Loss = -9852.580478893122
Iteration 6000: Loss = -9852.580007486245
Iteration 6100: Loss = -9852.579578011286
Iteration 6200: Loss = -9852.579220797277
Iteration 6300: Loss = -9852.57879057331
Iteration 6400: Loss = -9852.578430286005
Iteration 6500: Loss = -9852.578093129625
Iteration 6600: Loss = -9852.577758331796
Iteration 6700: Loss = -9852.577464792312
Iteration 6800: Loss = -9852.577222061644
Iteration 6900: Loss = -9852.57692575807
Iteration 7000: Loss = -9852.584928915345
1
Iteration 7100: Loss = -9852.576416961256
Iteration 7200: Loss = -9852.57620694841
Iteration 7300: Loss = -9852.576775598633
1
Iteration 7400: Loss = -9852.575760613325
Iteration 7500: Loss = -9852.57558051079
Iteration 7600: Loss = -9852.575439983255
Iteration 7700: Loss = -9852.575243461783
Iteration 7800: Loss = -9852.575343570215
1
Iteration 7900: Loss = -9852.574929849165
Iteration 8000: Loss = -9852.574881300923
Iteration 8100: Loss = -9852.574649647919
Iteration 8200: Loss = -9852.574477490805
Iteration 8300: Loss = -9852.574459081332
Iteration 8400: Loss = -9852.574246242424
Iteration 8500: Loss = -9852.574276972404
Iteration 8600: Loss = -9852.57411200388
Iteration 8700: Loss = -9852.573976747419
Iteration 8800: Loss = -9852.578477703613
1
Iteration 8900: Loss = -9852.573769711493
Iteration 9000: Loss = -9852.575016723284
1
Iteration 9100: Loss = -9852.573639899536
Iteration 9200: Loss = -9852.582534210806
1
Iteration 9300: Loss = -9852.573461450073
Iteration 9400: Loss = -9852.573386216733
Iteration 9500: Loss = -9852.57722198399
1
Iteration 9600: Loss = -9852.573242799002
Iteration 9700: Loss = -9852.573201267323
Iteration 9800: Loss = -9852.575001984575
1
Iteration 9900: Loss = -9852.573110560535
Iteration 10000: Loss = -9852.57317547776
Iteration 10100: Loss = -9852.578204806092
1
Iteration 10200: Loss = -9852.572948369603
Iteration 10300: Loss = -9852.572904388688
Iteration 10400: Loss = -9852.573383353922
1
Iteration 10500: Loss = -9852.57281873566
Iteration 10600: Loss = -9852.574779131895
1
Iteration 10700: Loss = -9852.572769620374
Iteration 10800: Loss = -9852.574208843645
1
Iteration 10900: Loss = -9852.586679845746
2
Iteration 11000: Loss = -9852.572679528663
Iteration 11100: Loss = -9852.572661757458
Iteration 11200: Loss = -9852.573205836074
1
Iteration 11300: Loss = -9852.572588188645
Iteration 11400: Loss = -9852.580158197448
1
Iteration 11500: Loss = -9852.572536988002
Iteration 11600: Loss = -9852.575426813815
1
Iteration 11700: Loss = -9852.572492000854
Iteration 11800: Loss = -9852.572464754663
Iteration 11900: Loss = -9852.572633168447
1
Iteration 12000: Loss = -9852.572422170913
Iteration 12100: Loss = -9852.578141953963
1
Iteration 12200: Loss = -9852.572429295476
Iteration 12300: Loss = -9852.572405573066
Iteration 12400: Loss = -9852.57368240981
1
Iteration 12500: Loss = -9852.572894077894
2
Iteration 12600: Loss = -9852.573204019991
3
Iteration 12700: Loss = -9852.575153319813
4
Iteration 12800: Loss = -9852.572319356272
Iteration 12900: Loss = -9852.584489104704
1
Iteration 13000: Loss = -9852.590370042824
2
Iteration 13100: Loss = -9852.603971983477
3
Iteration 13200: Loss = -9852.574636194979
4
Iteration 13300: Loss = -9852.572406371972
Iteration 13400: Loss = -9852.581521618366
1
Iteration 13500: Loss = -9852.57230571782
Iteration 13600: Loss = -9852.57230325498
Iteration 13700: Loss = -9852.575062414237
1
Iteration 13800: Loss = -9852.572257263851
Iteration 13900: Loss = -9852.572350119199
Iteration 14000: Loss = -9852.572235711486
Iteration 14100: Loss = -9852.575950397433
1
Iteration 14200: Loss = -9852.572252429898
Iteration 14300: Loss = -9852.572371792383
1
Iteration 14400: Loss = -9852.572390443926
2
Iteration 14500: Loss = -9852.572400665626
3
Iteration 14600: Loss = -9852.573841611307
4
Iteration 14700: Loss = -9852.581060314647
5
Iteration 14800: Loss = -9852.572156479247
Iteration 14900: Loss = -9852.58398877624
1
Iteration 15000: Loss = -9852.572183425571
Iteration 15100: Loss = -9852.578780994412
1
Iteration 15200: Loss = -9852.572149480762
Iteration 15300: Loss = -9852.58025998358
1
Iteration 15400: Loss = -9852.57216082776
Iteration 15500: Loss = -9852.59861082613
1
Iteration 15600: Loss = -9852.57215223857
Iteration 15700: Loss = -9852.581678351866
1
Iteration 15800: Loss = -9852.57216372377
Iteration 15900: Loss = -9852.572139267313
Iteration 16000: Loss = -9852.572269906897
1
Iteration 16100: Loss = -9852.572125539922
Iteration 16200: Loss = -9852.732998886644
1
Iteration 16300: Loss = -9852.57211582794
Iteration 16400: Loss = -9852.575798478312
1
Iteration 16500: Loss = -9852.579810701358
2
Iteration 16600: Loss = -9852.574049883347
3
Iteration 16700: Loss = -9852.57305905354
4
Iteration 16800: Loss = -9852.572176566353
Iteration 16900: Loss = -9852.783010979747
1
Iteration 17000: Loss = -9852.572113126887
Iteration 17100: Loss = -9852.572103337776
Iteration 17200: Loss = -9852.572262514814
1
Iteration 17300: Loss = -9852.572135761873
Iteration 17400: Loss = -9852.57211049441
Iteration 17500: Loss = -9852.572125630657
Iteration 17600: Loss = -9852.572084400133
Iteration 17700: Loss = -9852.575918273204
1
Iteration 17800: Loss = -9852.572127906831
Iteration 17900: Loss = -9852.576557716238
1
Iteration 18000: Loss = -9852.572130546037
Iteration 18100: Loss = -9852.696024016066
1
Iteration 18200: Loss = -9852.572079853579
Iteration 18300: Loss = -9852.579736457423
1
Iteration 18400: Loss = -9852.572108188871
Iteration 18500: Loss = -9852.572179391816
Iteration 18600: Loss = -9852.572137174255
Iteration 18700: Loss = -9852.57212833296
Iteration 18800: Loss = -9852.572155270529
Iteration 18900: Loss = -9852.57208732177
Iteration 19000: Loss = -9852.572259200459
1
Iteration 19100: Loss = -9852.572068856685
Iteration 19200: Loss = -9852.572221375032
1
Iteration 19300: Loss = -9852.572081080229
Iteration 19400: Loss = -9852.573598159595
1
Iteration 19500: Loss = -9852.76276623274
2
Iteration 19600: Loss = -9852.572081430895
Iteration 19700: Loss = -9852.583671699389
1
Iteration 19800: Loss = -9852.572083686347
Iteration 19900: Loss = -9852.70066358483
1
pi: tensor([[4.9650e-01, 5.0350e-01],
        [3.2305e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1325, 0.8675], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4613, 0.1546],
         [0.6815, 0.1328]],

        [[0.5899, 0.1652],
         [0.7068, 0.5760]],

        [[0.6668, 0.1803],
         [0.6907, 0.6312]],

        [[0.5320, 0.1154],
         [0.5633, 0.5858]],

        [[0.6911, 0.0743],
         [0.5575, 0.5771]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.009553639649501185
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.003485103334991517
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.009201769803340854
Average Adjusted Rand Index: 0.0012598235225581172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25419.249245055766
Iteration 100: Loss = -9863.264277938588
Iteration 200: Loss = -9860.746811461566
Iteration 300: Loss = -9858.378438318701
Iteration 400: Loss = -9856.516765341708
Iteration 500: Loss = -9854.928337577157
Iteration 600: Loss = -9854.119252487399
Iteration 700: Loss = -9853.823406559797
Iteration 800: Loss = -9853.6499210962
Iteration 900: Loss = -9853.5244050276
Iteration 1000: Loss = -9853.426747781721
Iteration 1100: Loss = -9853.345991349845
Iteration 1200: Loss = -9853.274692824036
Iteration 1300: Loss = -9853.205810272042
Iteration 1400: Loss = -9853.124592850183
Iteration 1500: Loss = -9853.023052216844
Iteration 1600: Loss = -9852.915464609167
Iteration 1700: Loss = -9852.828638267369
Iteration 1800: Loss = -9852.768628744436
Iteration 1900: Loss = -9852.732832350768
Iteration 2000: Loss = -9852.709940197688
Iteration 2100: Loss = -9852.69302775768
Iteration 2200: Loss = -9852.679601364125
Iteration 2300: Loss = -9852.66857998462
Iteration 2400: Loss = -9852.659289514953
Iteration 2500: Loss = -9852.651252457548
Iteration 2600: Loss = -9852.644269185843
Iteration 2700: Loss = -9852.638070610908
Iteration 2800: Loss = -9852.63258407755
Iteration 2900: Loss = -9852.627749587937
Iteration 3000: Loss = -9852.623514230241
Iteration 3100: Loss = -9852.619749647836
Iteration 3200: Loss = -9852.616373939274
Iteration 3300: Loss = -9852.613292390812
Iteration 3400: Loss = -9852.610463092848
Iteration 3500: Loss = -9852.607874102858
Iteration 3600: Loss = -9852.605564028141
Iteration 3700: Loss = -9852.60340189067
Iteration 3800: Loss = -9852.601461121581
Iteration 3900: Loss = -9852.59963995233
Iteration 4000: Loss = -9852.59800125508
Iteration 4100: Loss = -9852.596441075731
Iteration 4200: Loss = -9852.594987584542
Iteration 4300: Loss = -9852.593670961425
Iteration 4400: Loss = -9852.592445022885
Iteration 4500: Loss = -9852.591268341805
Iteration 4600: Loss = -9852.590193545964
Iteration 4700: Loss = -9852.589237563025
Iteration 4800: Loss = -9852.588292370408
Iteration 4900: Loss = -9852.587376593083
Iteration 5000: Loss = -9852.586574086463
Iteration 5100: Loss = -9852.585797258722
Iteration 5200: Loss = -9852.585092976044
Iteration 5300: Loss = -9852.584979541762
Iteration 5400: Loss = -9852.583729911146
Iteration 5500: Loss = -9852.583104971298
Iteration 5600: Loss = -9852.582580347407
Iteration 5700: Loss = -9852.581948920846
Iteration 5800: Loss = -9852.582322122718
1
Iteration 5900: Loss = -9852.580876003009
Iteration 6000: Loss = -9852.580358367826
Iteration 6100: Loss = -9852.580523398094
1
Iteration 6200: Loss = -9852.581148429255
2
Iteration 6300: Loss = -9852.579428540887
Iteration 6400: Loss = -9852.578671546376
Iteration 6500: Loss = -9852.578311556277
Iteration 6600: Loss = -9852.57946988827
1
Iteration 6700: Loss = -9852.577653036416
Iteration 6800: Loss = -9852.57740244817
Iteration 6900: Loss = -9852.577127997247
Iteration 7000: Loss = -9852.576860612884
Iteration 7100: Loss = -9852.576625799997
Iteration 7200: Loss = -9852.57637555987
Iteration 7300: Loss = -9852.576160997696
Iteration 7400: Loss = -9852.575979498597
Iteration 7500: Loss = -9852.575796035791
Iteration 7600: Loss = -9852.575806040377
Iteration 7700: Loss = -9852.575416431446
Iteration 7800: Loss = -9852.57524735909
Iteration 7900: Loss = -9852.575138642354
Iteration 8000: Loss = -9852.574920995281
Iteration 8100: Loss = -9852.578483644305
1
Iteration 8200: Loss = -9852.574653414398
Iteration 8300: Loss = -9852.574524433065
Iteration 8400: Loss = -9852.57506131497
1
Iteration 8500: Loss = -9852.574272371356
Iteration 8600: Loss = -9852.574199154242
Iteration 8700: Loss = -9852.574089880976
Iteration 8800: Loss = -9852.573985926489
Iteration 8900: Loss = -9852.575827648028
1
Iteration 9000: Loss = -9852.573819077354
Iteration 9100: Loss = -9852.573665617047
Iteration 9200: Loss = -9852.573742989285
Iteration 9300: Loss = -9852.573558241445
Iteration 9400: Loss = -9852.584210962714
1
Iteration 9500: Loss = -9852.57342423869
Iteration 9600: Loss = -9852.573352493788
Iteration 9700: Loss = -9852.573487095618
1
Iteration 9800: Loss = -9852.573221460816
Iteration 9900: Loss = -9852.573341675694
1
Iteration 10000: Loss = -9852.57310999884
Iteration 10100: Loss = -9852.579328081149
1
Iteration 10200: Loss = -9852.573022625225
Iteration 10300: Loss = -9852.58781921788
1
Iteration 10400: Loss = -9852.57295695923
Iteration 10500: Loss = -9852.573168432413
1
Iteration 10600: Loss = -9852.572902226897
Iteration 10700: Loss = -9852.572818466446
Iteration 10800: Loss = -9852.572824083736
Iteration 10900: Loss = -9852.57277622536
Iteration 11000: Loss = -9852.605164290573
1
Iteration 11100: Loss = -9852.577076547901
2
Iteration 11200: Loss = -9852.572876533066
3
Iteration 11300: Loss = -9852.719166058956
4
Iteration 11400: Loss = -9852.57261062351
Iteration 11500: Loss = -9852.603758826372
1
Iteration 11600: Loss = -9852.572592797362
Iteration 11700: Loss = -9852.572637678983
Iteration 11800: Loss = -9852.676546398223
1
Iteration 11900: Loss = -9852.572490544337
Iteration 12000: Loss = -9852.578677073456
1
Iteration 12100: Loss = -9852.574233487345
2
Iteration 12200: Loss = -9852.57251731153
Iteration 12300: Loss = -9852.572515706006
Iteration 12400: Loss = -9852.572396793503
Iteration 12500: Loss = -9852.573155540886
1
Iteration 12600: Loss = -9852.57237246358
Iteration 12700: Loss = -9852.572413644015
Iteration 12800: Loss = -9852.572520187994
1
Iteration 12900: Loss = -9852.572691682997
2
Iteration 13000: Loss = -9852.57234455376
Iteration 13100: Loss = -9852.589727738246
1
Iteration 13200: Loss = -9852.572315393423
Iteration 13300: Loss = -9852.572849153079
1
Iteration 13400: Loss = -9852.57266520637
2
Iteration 13500: Loss = -9852.617766252417
3
Iteration 13600: Loss = -9852.572276824254
Iteration 13700: Loss = -9852.574953253563
1
Iteration 13800: Loss = -9852.572422475649
2
Iteration 13900: Loss = -9852.572442605939
3
Iteration 14000: Loss = -9852.572736180764
4
Iteration 14100: Loss = -9852.583608070388
5
Iteration 14200: Loss = -9852.574112999271
6
Iteration 14300: Loss = -9852.572585947762
7
Iteration 14400: Loss = -9852.747949455565
8
Iteration 14500: Loss = -9852.572203641888
Iteration 14600: Loss = -9852.575385603772
1
Iteration 14700: Loss = -9852.572317984319
2
Iteration 14800: Loss = -9852.572311222195
3
Iteration 14900: Loss = -9852.727897405393
4
Iteration 15000: Loss = -9852.572191110345
Iteration 15100: Loss = -9852.578897688782
1
Iteration 15200: Loss = -9852.602656692798
2
Iteration 15300: Loss = -9852.57214275314
Iteration 15400: Loss = -9852.58764824153
1
Iteration 15500: Loss = -9852.572150176073
Iteration 15600: Loss = -9852.574494607721
1
Iteration 15700: Loss = -9852.572158444425
Iteration 15800: Loss = -9852.576157927258
1
Iteration 15900: Loss = -9852.63707734502
2
Iteration 16000: Loss = -9852.572181332682
Iteration 16100: Loss = -9852.572680635318
1
Iteration 16200: Loss = -9852.57211153539
Iteration 16300: Loss = -9852.702952567159
1
Iteration 16400: Loss = -9852.572118340138
Iteration 16500: Loss = -9852.582676486758
1
Iteration 16600: Loss = -9852.572122403539
Iteration 16700: Loss = -9852.585210379464
1
Iteration 16800: Loss = -9852.57213306155
Iteration 16900: Loss = -9852.579224554569
1
Iteration 17000: Loss = -9852.572146493821
Iteration 17100: Loss = -9852.574087925876
1
Iteration 17200: Loss = -9852.572115783641
Iteration 17300: Loss = -9852.572409480284
1
Iteration 17400: Loss = -9852.575541540942
2
Iteration 17500: Loss = -9852.572129113008
Iteration 17600: Loss = -9852.593800293924
1
Iteration 17700: Loss = -9852.593037572147
2
Iteration 17800: Loss = -9852.582142668016
3
Iteration 17900: Loss = -9852.596803975244
4
Iteration 18000: Loss = -9852.578009764447
5
Iteration 18100: Loss = -9852.577721369222
6
Iteration 18200: Loss = -9852.572312175467
7
Iteration 18300: Loss = -9852.572116743966
Iteration 18400: Loss = -9852.576366647043
1
Iteration 18500: Loss = -9852.572103914783
Iteration 18600: Loss = -9852.572215870065
1
Iteration 18700: Loss = -9852.572771310839
2
Iteration 18800: Loss = -9852.595004934501
3
Iteration 18900: Loss = -9852.572124947626
Iteration 19000: Loss = -9852.572456024522
1
Iteration 19100: Loss = -9852.572099210163
Iteration 19200: Loss = -9852.572694224846
1
Iteration 19300: Loss = -9852.57209350146
Iteration 19400: Loss = -9852.574218031117
1
Iteration 19500: Loss = -9852.572099545896
Iteration 19600: Loss = -9852.572667262199
1
Iteration 19700: Loss = -9852.572303805231
2
Iteration 19800: Loss = -9852.572901870972
3
Iteration 19900: Loss = -9852.572128711203
pi: tensor([[1.0000e+00, 3.5277e-08],
        [5.0507e-01, 4.9493e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8653, 0.1347], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1325, 0.1534],
         [0.5741, 0.4597]],

        [[0.5464, 0.1638],
         [0.5804, 0.6290]],

        [[0.7229, 0.1796],
         [0.6443, 0.6693]],

        [[0.5735, 0.1159],
         [0.6358, 0.7022]],

        [[0.7039, 0.0745],
         [0.6828, 0.6885]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.009553639649501185
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.003485103334991517
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.009201769803340854
Average Adjusted Rand Index: 0.0012598235225581172
9967.869077805504
[0.009201769803340854, 0.009201769803340854] [0.0012598235225581172, 0.0012598235225581172] [9852.572073635612, 9852.60568202635]
-------------------------------------
This iteration is 43
True Objective function: Loss = -10189.6455065645
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23989.09162109278
Iteration 100: Loss = -10078.496873410095
Iteration 200: Loss = -10075.337698768402
Iteration 300: Loss = -10074.239881401914
Iteration 400: Loss = -10073.888529533111
Iteration 500: Loss = -10073.731103978027
Iteration 600: Loss = -10073.650927136629
Iteration 700: Loss = -10073.604778646371
Iteration 800: Loss = -10073.574923543269
Iteration 900: Loss = -10073.553575766146
Iteration 1000: Loss = -10073.536881248943
Iteration 1100: Loss = -10073.522670141598
Iteration 1200: Loss = -10073.50971478931
Iteration 1300: Loss = -10073.497173977681
Iteration 1400: Loss = -10073.48449060789
Iteration 1500: Loss = -10073.47147864237
Iteration 1600: Loss = -10073.45804752881
Iteration 1700: Loss = -10073.444553878651
Iteration 1800: Loss = -10073.430702374113
Iteration 1900: Loss = -10073.416387001633
Iteration 2000: Loss = -10073.401307698125
Iteration 2100: Loss = -10073.385379909545
Iteration 2200: Loss = -10073.3683570797
Iteration 2300: Loss = -10073.349868915697
Iteration 2400: Loss = -10073.328578161218
Iteration 2500: Loss = -10073.291857412256
Iteration 2600: Loss = -10071.671302744153
Iteration 2700: Loss = -10070.759312249742
Iteration 2800: Loss = -10070.374518311019
Iteration 2900: Loss = -10070.123747685384
Iteration 3000: Loss = -10069.928845000013
Iteration 3100: Loss = -10069.758566315497
Iteration 3200: Loss = -10069.601178426614
Iteration 3300: Loss = -10069.458153417416
Iteration 3400: Loss = -10069.329921640328
Iteration 3500: Loss = -10069.214901330728
Iteration 3600: Loss = -10069.11283938376
Iteration 3700: Loss = -10069.022860359337
Iteration 3800: Loss = -10068.943502023065
Iteration 3900: Loss = -10068.872639594685
Iteration 4000: Loss = -10068.807746893373
Iteration 4100: Loss = -10068.74955334962
Iteration 4200: Loss = -10068.701901514294
Iteration 4300: Loss = -10068.661914355353
Iteration 4400: Loss = -10068.627922934455
Iteration 4500: Loss = -10068.5988412249
Iteration 4600: Loss = -10068.573914084776
Iteration 4700: Loss = -10068.55227705599
Iteration 4800: Loss = -10068.533384171185
Iteration 4900: Loss = -10068.516781796488
Iteration 5000: Loss = -10068.502392954191
Iteration 5100: Loss = -10068.490184327666
Iteration 5200: Loss = -10068.47981321351
Iteration 5300: Loss = -10068.470759267491
Iteration 5400: Loss = -10068.462791203232
Iteration 5500: Loss = -10068.455733052218
Iteration 5600: Loss = -10068.449455540418
Iteration 5700: Loss = -10068.443871809464
Iteration 5800: Loss = -10068.438833063374
Iteration 5900: Loss = -10068.43437427243
Iteration 6000: Loss = -10068.430346106365
Iteration 6100: Loss = -10068.426733492222
Iteration 6200: Loss = -10068.42343679328
Iteration 6300: Loss = -10068.420452935585
Iteration 6400: Loss = -10068.41777158842
Iteration 6500: Loss = -10068.415295712382
Iteration 6600: Loss = -10068.413064956056
Iteration 6700: Loss = -10068.41101841465
Iteration 6800: Loss = -10068.4091387955
Iteration 6900: Loss = -10068.40742974101
Iteration 7000: Loss = -10068.40588402139
Iteration 7100: Loss = -10068.40438024538
Iteration 7200: Loss = -10068.403064822938
Iteration 7300: Loss = -10068.401867364195
Iteration 7400: Loss = -10068.40076856456
Iteration 7500: Loss = -10068.399731238378
Iteration 7600: Loss = -10068.398826198722
Iteration 7700: Loss = -10068.397958521038
Iteration 7800: Loss = -10068.397165943972
Iteration 7900: Loss = -10068.396447909128
Iteration 8000: Loss = -10068.395767257176
Iteration 8100: Loss = -10068.395157024122
Iteration 8200: Loss = -10068.394570255436
Iteration 8300: Loss = -10068.39400842705
Iteration 8400: Loss = -10068.393595877516
Iteration 8500: Loss = -10068.393079400561
Iteration 8600: Loss = -10068.3925982532
Iteration 8700: Loss = -10068.393310107098
1
Iteration 8800: Loss = -10068.39186433295
Iteration 8900: Loss = -10068.391489968944
Iteration 9000: Loss = -10068.391209004456
Iteration 9100: Loss = -10068.392045682638
1
Iteration 9200: Loss = -10068.390600652225
Iteration 9300: Loss = -10068.390337903673
Iteration 9400: Loss = -10068.390405996468
Iteration 9500: Loss = -10068.389809506638
Iteration 9600: Loss = -10068.389632337554
Iteration 9700: Loss = -10068.415107577754
1
Iteration 9800: Loss = -10068.389243282774
Iteration 9900: Loss = -10068.389044690166
Iteration 10000: Loss = -10068.388892338216
Iteration 10100: Loss = -10068.388987537093
Iteration 10200: Loss = -10068.388570374942
Iteration 10300: Loss = -10068.38843909386
Iteration 10400: Loss = -10068.393548598231
1
Iteration 10500: Loss = -10068.388185371728
Iteration 10600: Loss = -10068.388060522804
Iteration 10700: Loss = -10068.388009901266
Iteration 10800: Loss = -10068.387816996323
Iteration 10900: Loss = -10068.387710698711
Iteration 11000: Loss = -10068.823520664955
1
Iteration 11100: Loss = -10068.387516639223
Iteration 11200: Loss = -10068.387430737303
Iteration 11300: Loss = -10068.387376633304
Iteration 11400: Loss = -10068.389641428967
1
Iteration 11500: Loss = -10068.38720242663
Iteration 11600: Loss = -10068.38715476019
Iteration 11700: Loss = -10068.497258984416
1
Iteration 11800: Loss = -10068.387015699642
Iteration 11900: Loss = -10068.386927537447
Iteration 12000: Loss = -10068.38690295942
Iteration 12100: Loss = -10068.38729305873
1
Iteration 12200: Loss = -10068.386796398763
Iteration 12300: Loss = -10068.386773664612
Iteration 12400: Loss = -10068.38670614479
Iteration 12500: Loss = -10068.387202492591
1
Iteration 12600: Loss = -10068.386644138678
Iteration 12700: Loss = -10068.38660420479
Iteration 12800: Loss = -10068.387027695098
1
Iteration 12900: Loss = -10068.386528759862
Iteration 13000: Loss = -10068.386497751228
Iteration 13100: Loss = -10068.38743230543
1
Iteration 13200: Loss = -10068.386426135761
Iteration 13300: Loss = -10068.386409327242
Iteration 13400: Loss = -10068.386395216046
Iteration 13500: Loss = -10068.399369643363
1
Iteration 13600: Loss = -10068.386297187912
Iteration 13700: Loss = -10068.386654717611
1
Iteration 13800: Loss = -10068.386274809893
Iteration 13900: Loss = -10068.38641744209
1
Iteration 14000: Loss = -10068.386255813228
Iteration 14100: Loss = -10068.387632696527
1
Iteration 14200: Loss = -10068.386186804773
Iteration 14300: Loss = -10068.685603766615
1
Iteration 14400: Loss = -10068.38618312224
Iteration 14500: Loss = -10068.386118779574
Iteration 14600: Loss = -10068.388018028385
1
Iteration 14700: Loss = -10068.38609926062
Iteration 14800: Loss = -10068.386092694327
Iteration 14900: Loss = -10068.403684071076
1
Iteration 15000: Loss = -10068.38607665395
Iteration 15100: Loss = -10068.386039163603
Iteration 15200: Loss = -10068.38605271911
Iteration 15300: Loss = -10068.386218461537
1
Iteration 15400: Loss = -10068.385996207584
Iteration 15500: Loss = -10068.386005490387
Iteration 15600: Loss = -10068.411054748003
1
Iteration 15700: Loss = -10068.383871304677
Iteration 15800: Loss = -10068.383845506349
Iteration 15900: Loss = -10068.384341260726
1
Iteration 16000: Loss = -10068.384062441253
2
Iteration 16100: Loss = -10068.47819633611
3
Iteration 16200: Loss = -10068.383874626683
Iteration 16300: Loss = -10068.486588805676
1
Iteration 16400: Loss = -10068.383850913855
Iteration 16500: Loss = -10068.384618160833
1
Iteration 16600: Loss = -10068.382504541682
Iteration 16700: Loss = -10068.382413091502
Iteration 16800: Loss = -10068.389498757486
1
Iteration 16900: Loss = -10068.382376409489
Iteration 17000: Loss = -10068.382396166733
Iteration 17100: Loss = -10068.38454841121
1
Iteration 17200: Loss = -10068.382382582227
Iteration 17300: Loss = -10068.38238621584
Iteration 17400: Loss = -10068.3840005336
1
Iteration 17500: Loss = -10068.382355521218
Iteration 17600: Loss = -10068.382367989703
Iteration 17700: Loss = -10068.387412332451
1
Iteration 17800: Loss = -10068.382386334402
Iteration 17900: Loss = -10068.382373183504
Iteration 18000: Loss = -10068.413383589943
1
Iteration 18100: Loss = -10068.38239035783
Iteration 18200: Loss = -10068.383379117746
1
Iteration 18300: Loss = -10068.382378386737
Iteration 18400: Loss = -10068.382580668916
1
Iteration 18500: Loss = -10068.382413025509
Iteration 18600: Loss = -10068.383837556621
1
Iteration 18700: Loss = -10068.38238462705
Iteration 18800: Loss = -10068.393197325884
1
Iteration 18900: Loss = -10068.382375918554
Iteration 19000: Loss = -10068.382372558273
Iteration 19100: Loss = -10068.382398669595
Iteration 19200: Loss = -10068.382385881281
Iteration 19300: Loss = -10068.38238635661
Iteration 19400: Loss = -10068.382449611887
Iteration 19500: Loss = -10068.382366618074
Iteration 19600: Loss = -10068.382377140264
Iteration 19700: Loss = -10068.382424411357
Iteration 19800: Loss = -10068.382388299908
Iteration 19900: Loss = -10068.391490262004
1
pi: tensor([[1.0000e+00, 4.3471e-07],
        [5.6518e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0259, 0.9741], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0038, 0.2119],
         [0.6566, 0.1387]],

        [[0.6502, 0.1835],
         [0.5796, 0.6834]],

        [[0.7168, 0.2244],
         [0.5939, 0.5136]],

        [[0.6431, 0.1734],
         [0.6212, 0.5591]],

        [[0.6710, 0.1466],
         [0.6825, 0.5344]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.007262881945936654
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.010547661869155423
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
Global Adjusted Rand Index: 0.004919286947653712
Average Adjusted Rand Index: 0.004396244662230998
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22186.525792866934
Iteration 100: Loss = -10075.274112518928
Iteration 200: Loss = -10074.16350618255
Iteration 300: Loss = -10073.834414318591
Iteration 400: Loss = -10073.687533828659
Iteration 500: Loss = -10073.605402884954
Iteration 600: Loss = -10073.55022694762
Iteration 700: Loss = -10073.50634530211
Iteration 800: Loss = -10073.466927613958
Iteration 900: Loss = -10073.427960625435
Iteration 1000: Loss = -10073.386205884319
Iteration 1100: Loss = -10073.334724061186
Iteration 1200: Loss = -10073.181829463654
Iteration 1300: Loss = -10071.703840841445
Iteration 1400: Loss = -10071.05944219608
Iteration 1500: Loss = -10070.720266454598
Iteration 1600: Loss = -10070.504469754673
Iteration 1700: Loss = -10070.329186352967
Iteration 1800: Loss = -10070.177449801273
Iteration 1900: Loss = -10070.05021507277
Iteration 2000: Loss = -10069.941269491013
Iteration 2100: Loss = -10069.845338040734
Iteration 2200: Loss = -10069.75837901854
Iteration 2300: Loss = -10069.677659120392
Iteration 2400: Loss = -10069.601228377644
Iteration 2500: Loss = -10069.52782258177
Iteration 2600: Loss = -10069.457115883115
Iteration 2700: Loss = -10069.388909837757
Iteration 2800: Loss = -10069.32300972369
Iteration 2900: Loss = -10069.259418995704
Iteration 3000: Loss = -10069.198268859756
Iteration 3100: Loss = -10069.13969050735
Iteration 3200: Loss = -10069.083823393688
Iteration 3300: Loss = -10069.030490884395
Iteration 3400: Loss = -10068.97959234216
Iteration 3500: Loss = -10068.930958952273
Iteration 3600: Loss = -10068.884606039293
Iteration 3700: Loss = -10068.84054428336
Iteration 3800: Loss = -10068.798874160793
Iteration 3900: Loss = -10068.759736809736
Iteration 4000: Loss = -10068.723588888999
Iteration 4100: Loss = -10068.690870057515
Iteration 4200: Loss = -10068.661458856353
Iteration 4300: Loss = -10068.6351835268
Iteration 4400: Loss = -10068.611942523452
Iteration 4500: Loss = -10068.591279622517
Iteration 4600: Loss = -10068.573061200854
Iteration 4700: Loss = -10068.557056554118
Iteration 4800: Loss = -10068.542850184664
Iteration 4900: Loss = -10068.530319139503
Iteration 5000: Loss = -10068.519173956263
Iteration 5100: Loss = -10068.50929490727
Iteration 5200: Loss = -10068.500455941112
Iteration 5300: Loss = -10068.492594715957
Iteration 5400: Loss = -10068.485464477579
Iteration 5500: Loss = -10068.478933590974
Iteration 5600: Loss = -10068.472591396214
Iteration 5700: Loss = -10068.463184428658
Iteration 5800: Loss = -10068.43512715176
Iteration 5900: Loss = -10068.43006068973
Iteration 6000: Loss = -10068.4260232311
Iteration 6100: Loss = -10068.422428971751
Iteration 6200: Loss = -10068.41917727229
Iteration 6300: Loss = -10068.416332942621
Iteration 6400: Loss = -10068.413725018294
Iteration 6500: Loss = -10068.411391728385
Iteration 6600: Loss = -10068.409257630283
Iteration 6700: Loss = -10068.40734782954
Iteration 6800: Loss = -10068.40557860011
Iteration 6900: Loss = -10068.40400470003
Iteration 7000: Loss = -10068.40255179386
Iteration 7100: Loss = -10068.401229804243
Iteration 7200: Loss = -10068.400026982152
Iteration 7300: Loss = -10068.398957225103
Iteration 7400: Loss = -10068.39795720553
Iteration 7500: Loss = -10068.39705799038
Iteration 7600: Loss = -10068.396202855163
Iteration 7700: Loss = -10068.395482455713
Iteration 7800: Loss = -10068.394771819767
Iteration 7900: Loss = -10068.394170190808
Iteration 8000: Loss = -10068.398733505685
1
Iteration 8100: Loss = -10068.393061059383
Iteration 8200: Loss = -10068.392549186534
Iteration 8300: Loss = -10068.4123962143
1
Iteration 8400: Loss = -10068.391726159674
Iteration 8500: Loss = -10068.391353052633
Iteration 8600: Loss = -10068.39103979389
Iteration 8700: Loss = -10068.390889999127
Iteration 8800: Loss = -10068.390465960887
Iteration 8900: Loss = -10068.39023567286
Iteration 9000: Loss = -10068.545405017274
1
Iteration 9100: Loss = -10068.389745468323
Iteration 9200: Loss = -10068.389560110803
Iteration 9300: Loss = -10068.389427905535
Iteration 9400: Loss = -10068.38936129912
Iteration 9500: Loss = -10068.389100762857
Iteration 9600: Loss = -10068.38893430911
Iteration 9700: Loss = -10068.416362702541
1
Iteration 9800: Loss = -10068.388688435374
Iteration 9900: Loss = -10068.38856599008
Iteration 10000: Loss = -10068.388467488216
Iteration 10100: Loss = -10068.388491565393
Iteration 10200: Loss = -10068.388294287051
Iteration 10300: Loss = -10068.388234157655
Iteration 10400: Loss = -10068.389011027566
1
Iteration 10500: Loss = -10068.388063858221
Iteration 10600: Loss = -10068.388027848314
Iteration 10700: Loss = -10068.402211593855
1
Iteration 10800: Loss = -10068.387903481116
Iteration 10900: Loss = -10068.38785613145
Iteration 11000: Loss = -10068.387802101217
Iteration 11100: Loss = -10068.38782997927
Iteration 11200: Loss = -10068.387712932159
Iteration 11300: Loss = -10068.387650494626
Iteration 11400: Loss = -10068.38810356461
1
Iteration 11500: Loss = -10068.387584047621
Iteration 11600: Loss = -10068.387548945477
Iteration 11700: Loss = -10068.387568860791
Iteration 11800: Loss = -10068.387480313337
Iteration 11900: Loss = -10068.387460200393
Iteration 12000: Loss = -10068.391230251673
1
Iteration 12100: Loss = -10068.387416543681
Iteration 12200: Loss = -10068.387393926954
Iteration 12300: Loss = -10068.401370208709
1
Iteration 12400: Loss = -10068.38729298321
Iteration 12500: Loss = -10068.387297012378
Iteration 12600: Loss = -10068.39159359862
1
Iteration 12700: Loss = -10068.387283129594
Iteration 12800: Loss = -10068.387174932084
Iteration 12900: Loss = -10068.386759549152
Iteration 13000: Loss = -10068.40603097197
1
Iteration 13100: Loss = -10068.386626084723
Iteration 13200: Loss = -10068.386610895288
Iteration 13300: Loss = -10068.386576256933
Iteration 13400: Loss = -10068.386568050877
Iteration 13500: Loss = -10068.38655726045
Iteration 13600: Loss = -10068.38651378302
Iteration 13700: Loss = -10068.38872192154
1
Iteration 13800: Loss = -10068.405121924849
2
Iteration 13900: Loss = -10068.386504987275
Iteration 14000: Loss = -10068.387555608648
1
Iteration 14100: Loss = -10068.386529269761
Iteration 14200: Loss = -10068.392293107005
1
Iteration 14300: Loss = -10068.386516145492
Iteration 14400: Loss = -10068.386522361954
Iteration 14500: Loss = -10068.386584616388
Iteration 14600: Loss = -10068.386509536074
Iteration 14700: Loss = -10068.386561168194
Iteration 14800: Loss = -10068.38657935284
Iteration 14900: Loss = -10068.38650937631
Iteration 15000: Loss = -10068.38652850332
Iteration 15100: Loss = -10068.386855404255
1
Iteration 15200: Loss = -10068.3864721989
Iteration 15300: Loss = -10068.386450677674
Iteration 15400: Loss = -10068.387110447691
1
Iteration 15500: Loss = -10068.386490013218
Iteration 15600: Loss = -10068.391162133808
1
Iteration 15700: Loss = -10068.386484275705
Iteration 15800: Loss = -10068.38643582765
Iteration 15900: Loss = -10068.386436622732
Iteration 16000: Loss = -10068.38658092773
1
Iteration 16100: Loss = -10068.38642703836
Iteration 16200: Loss = -10068.386278205504
Iteration 16300: Loss = -10068.385460323372
Iteration 16400: Loss = -10068.383811780404
Iteration 16500: Loss = -10068.383816872185
Iteration 16600: Loss = -10068.3845271485
1
Iteration 16700: Loss = -10068.383808078437
Iteration 16800: Loss = -10068.383791436128
Iteration 16900: Loss = -10068.385293806177
1
Iteration 17000: Loss = -10068.383859119898
Iteration 17100: Loss = -10068.383821488484
Iteration 17200: Loss = -10068.392087991177
1
Iteration 17300: Loss = -10068.383828589735
Iteration 17400: Loss = -10068.385933950618
1
Iteration 17500: Loss = -10068.383726517483
Iteration 17600: Loss = -10068.38845563942
1
Iteration 17700: Loss = -10068.383750234849
Iteration 17800: Loss = -10068.383924559179
1
Iteration 17900: Loss = -10068.383743473767
Iteration 18000: Loss = -10068.384103692815
1
Iteration 18100: Loss = -10068.383746586927
Iteration 18200: Loss = -10068.383964941248
1
Iteration 18300: Loss = -10068.383734867863
Iteration 18400: Loss = -10068.403185436093
1
Iteration 18500: Loss = -10068.383734370245
Iteration 18600: Loss = -10068.383712786786
Iteration 18700: Loss = -10068.397384344395
1
Iteration 18800: Loss = -10068.38372608158
Iteration 18900: Loss = -10068.383729361287
Iteration 19000: Loss = -10068.413457623898
1
Iteration 19100: Loss = -10068.38373645484
Iteration 19200: Loss = -10068.383742069955
Iteration 19300: Loss = -10068.959135460489
1
Iteration 19400: Loss = -10068.383760072558
Iteration 19500: Loss = -10068.383741192647
Iteration 19600: Loss = -10068.385030894313
1
Iteration 19700: Loss = -10068.383763003583
Iteration 19800: Loss = -10068.384357261579
1
Iteration 19900: Loss = -10068.383914152846
2
pi: tensor([[1.0000e+00, 2.1633e-08],
        [3.3654e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9741, 0.0259], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1387, 0.2119],
         [0.5333, 0.0038]],

        [[0.5076, 0.1835],
         [0.7055, 0.6230]],

        [[0.6501, 0.2244],
         [0.5601, 0.5746]],

        [[0.6008, 0.1735],
         [0.6394, 0.6286]],

        [[0.7168, 0.1466],
         [0.6064, 0.6487]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.007262881945936654
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.010547661869155423
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.002056465888134684
Global Adjusted Rand Index: 0.004919286947653712
Average Adjusted Rand Index: 0.004396244662230998
10189.6455065645
[0.004919286947653712, 0.004919286947653712] [0.004396244662230998, 0.004396244662230998] [10068.38237095224, 10068.383752036554]
-------------------------------------
This iteration is 44
True Objective function: Loss = -9948.722408716018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26529.870086356354
Iteration 100: Loss = -9845.915484174948
Iteration 200: Loss = -9843.986833814326
Iteration 300: Loss = -9843.222722965951
Iteration 400: Loss = -9843.01980714921
Iteration 500: Loss = -9842.869736403598
Iteration 600: Loss = -9842.565705926
Iteration 700: Loss = -9841.646666875855
Iteration 800: Loss = -9841.484833779701
Iteration 900: Loss = -9841.444061750397
Iteration 1000: Loss = -9841.42101949679
Iteration 1100: Loss = -9841.405214895196
Iteration 1200: Loss = -9841.384664350639
Iteration 1300: Loss = -9841.322758229537
Iteration 1400: Loss = -9839.884642910316
Iteration 1500: Loss = -9835.56875350982
Iteration 1600: Loss = -9834.591583195792
Iteration 1700: Loss = -9834.343227507086
Iteration 1800: Loss = -9834.2151104209
Iteration 1900: Loss = -9834.153605119858
Iteration 2000: Loss = -9834.125208971609
Iteration 2100: Loss = -9834.11560528801
Iteration 2200: Loss = -9834.104820719478
Iteration 2300: Loss = -9834.097039826067
Iteration 2400: Loss = -9834.091907715998
Iteration 2500: Loss = -9834.087645322159
Iteration 2600: Loss = -9834.082881236862
Iteration 2700: Loss = -9834.07977318963
Iteration 2800: Loss = -9834.079859450925
Iteration 2900: Loss = -9834.07678669602
Iteration 3000: Loss = -9834.076383071488
Iteration 3100: Loss = -9834.076123877087
Iteration 3200: Loss = -9834.075572057029
Iteration 3300: Loss = -9834.075031259346
Iteration 3400: Loss = -9834.07444674565
Iteration 3500: Loss = -9834.07428620299
Iteration 3600: Loss = -9834.074269575236
Iteration 3700: Loss = -9834.074191467542
Iteration 3800: Loss = -9834.074207261112
Iteration 3900: Loss = -9834.074110392048
Iteration 4000: Loss = -9834.074036046066
Iteration 4100: Loss = -9834.074008591122
Iteration 4200: Loss = -9834.073923542352
Iteration 4300: Loss = -9834.084815428045
1
Iteration 4400: Loss = -9834.073768172717
Iteration 4500: Loss = -9834.074031848282
1
Iteration 4600: Loss = -9834.073685159927
Iteration 4700: Loss = -9834.073711194436
Iteration 4800: Loss = -9834.073513605528
Iteration 4900: Loss = -9834.073431480267
Iteration 5000: Loss = -9834.073531439997
Iteration 5100: Loss = -9834.073771618905
1
Iteration 5200: Loss = -9834.073079203596
Iteration 5300: Loss = -9834.074747330991
1
Iteration 5400: Loss = -9834.072947972585
Iteration 5500: Loss = -9834.072888925864
Iteration 5600: Loss = -9834.07280793894
Iteration 5700: Loss = -9834.078207629676
1
Iteration 5800: Loss = -9834.072660990689
Iteration 5900: Loss = -9834.084180278229
1
Iteration 6000: Loss = -9834.072488171849
Iteration 6100: Loss = -9834.079087048376
1
Iteration 6200: Loss = -9834.07230619408
Iteration 6300: Loss = -9834.115307553338
1
Iteration 6400: Loss = -9834.072323763063
Iteration 6500: Loss = -9834.072280266855
Iteration 6600: Loss = -9834.07231865614
Iteration 6700: Loss = -9834.072243344152
Iteration 6800: Loss = -9834.07249922677
1
Iteration 6900: Loss = -9834.072172881502
Iteration 7000: Loss = -9834.072914171855
1
Iteration 7100: Loss = -9834.072138961696
Iteration 7200: Loss = -9834.072209027569
Iteration 7300: Loss = -9834.072235448277
Iteration 7400: Loss = -9834.072094828622
Iteration 7500: Loss = -9834.07216753849
Iteration 7600: Loss = -9834.073776633588
1
Iteration 7700: Loss = -9834.074242441397
2
Iteration 7800: Loss = -9834.072147835317
Iteration 7900: Loss = -9834.071995519736
Iteration 8000: Loss = -9834.07224148409
1
Iteration 8100: Loss = -9834.07197210483
Iteration 8200: Loss = -9834.071976109684
Iteration 8300: Loss = -9834.072015672386
Iteration 8400: Loss = -9834.071925227378
Iteration 8500: Loss = -9834.072607105156
1
Iteration 8600: Loss = -9834.071927784586
Iteration 8700: Loss = -9834.071878849227
Iteration 8800: Loss = -9834.07189852023
Iteration 8900: Loss = -9834.071891280211
Iteration 9000: Loss = -9834.071883125242
Iteration 9100: Loss = -9834.07179957424
Iteration 9200: Loss = -9834.072111906293
1
Iteration 9300: Loss = -9834.0717976873
Iteration 9400: Loss = -9834.075089744301
1
Iteration 9500: Loss = -9834.071798515415
Iteration 9600: Loss = -9834.081408442531
1
Iteration 9700: Loss = -9834.071783302648
Iteration 9800: Loss = -9834.105459483973
1
Iteration 9900: Loss = -9834.071749819677
Iteration 10000: Loss = -9834.075130562542
1
Iteration 10100: Loss = -9834.071781772378
Iteration 10200: Loss = -9834.071766872074
Iteration 10300: Loss = -9834.071749651834
Iteration 10400: Loss = -9834.071707994095
Iteration 10500: Loss = -9834.071769133616
Iteration 10600: Loss = -9834.071701513969
Iteration 10700: Loss = -9834.071834082215
1
Iteration 10800: Loss = -9834.071707236692
Iteration 10900: Loss = -9834.07330452887
1
Iteration 11000: Loss = -9834.071662574255
Iteration 11100: Loss = -9834.342752833309
1
Iteration 11200: Loss = -9834.071681491287
Iteration 11300: Loss = -9834.072765134712
1
Iteration 11400: Loss = -9834.072297561599
2
Iteration 11500: Loss = -9834.071671036334
Iteration 11600: Loss = -9834.071633701755
Iteration 11700: Loss = -9834.071813862904
1
Iteration 11800: Loss = -9834.071613264443
Iteration 11900: Loss = -9834.078449062747
1
Iteration 12000: Loss = -9834.071624569642
Iteration 12100: Loss = -9834.292790015696
1
Iteration 12200: Loss = -9834.071622153468
Iteration 12300: Loss = -9834.071619832841
Iteration 12400: Loss = -9834.071847554755
1
Iteration 12500: Loss = -9834.071885562613
2
Iteration 12600: Loss = -9834.071631516494
Iteration 12700: Loss = -9834.108366566545
1
Iteration 12800: Loss = -9834.071606779396
Iteration 12900: Loss = -9834.098209556876
1
Iteration 13000: Loss = -9834.071568566227
Iteration 13100: Loss = -9834.073138245036
1
Iteration 13200: Loss = -9834.07164838912
Iteration 13300: Loss = -9834.072682947688
1
Iteration 13400: Loss = -9834.071641346107
Iteration 13500: Loss = -9834.10432265568
1
Iteration 13600: Loss = -9834.071584999807
Iteration 13700: Loss = -9834.111068118047
1
Iteration 13800: Loss = -9834.071596185862
Iteration 13900: Loss = -9834.07155591484
Iteration 14000: Loss = -9834.085598984228
1
Iteration 14100: Loss = -9834.071569507696
Iteration 14200: Loss = -9834.071522998433
Iteration 14300: Loss = -9834.075210675113
1
Iteration 14400: Loss = -9834.113591062627
2
Iteration 14500: Loss = -9834.071558682048
Iteration 14600: Loss = -9834.07181574357
1
Iteration 14700: Loss = -9834.18232670181
2
Iteration 14800: Loss = -9834.07156484305
Iteration 14900: Loss = -9834.077829397302
1
Iteration 15000: Loss = -9834.071604282233
Iteration 15100: Loss = -9834.078931719396
1
Iteration 15200: Loss = -9834.071597484744
Iteration 15300: Loss = -9834.071636304614
Iteration 15400: Loss = -9834.075765285166
1
Iteration 15500: Loss = -9834.071556893025
Iteration 15600: Loss = -9834.0718078557
1
Iteration 15700: Loss = -9834.07154392371
Iteration 15800: Loss = -9834.07187501152
1
Iteration 15900: Loss = -9834.071536206124
Iteration 16000: Loss = -9834.082623806184
1
Iteration 16100: Loss = -9834.071560192762
Iteration 16200: Loss = -9834.07356445099
1
Iteration 16300: Loss = -9834.071554921164
Iteration 16400: Loss = -9834.073795647078
1
Iteration 16500: Loss = -9834.071574119278
Iteration 16600: Loss = -9834.089136851713
1
Iteration 16700: Loss = -9834.07155429215
Iteration 16800: Loss = -9834.308869623466
1
Iteration 16900: Loss = -9834.071566574565
Iteration 17000: Loss = -9834.071532713675
Iteration 17100: Loss = -9834.071634796885
1
Iteration 17200: Loss = -9834.071521579033
Iteration 17300: Loss = -9834.072157655199
1
Iteration 17400: Loss = -9834.071911389745
2
Iteration 17500: Loss = -9834.071549086473
Iteration 17600: Loss = -9834.090965217145
1
Iteration 17700: Loss = -9834.071552966992
Iteration 17800: Loss = -9834.072187257376
1
Iteration 17900: Loss = -9834.071516296803
Iteration 18000: Loss = -9834.072014604315
1
Iteration 18100: Loss = -9834.071521123917
Iteration 18200: Loss = -9834.081985432353
1
Iteration 18300: Loss = -9834.071528870805
Iteration 18400: Loss = -9834.071527106176
Iteration 18500: Loss = -9834.071684834556
1
Iteration 18600: Loss = -9834.087379993167
2
Iteration 18700: Loss = -9834.07158146603
Iteration 18800: Loss = -9834.093744189207
1
Iteration 18900: Loss = -9834.0715453801
Iteration 19000: Loss = -9834.082144470054
1
Iteration 19100: Loss = -9834.071590424172
Iteration 19200: Loss = -9834.388847652992
1
Iteration 19300: Loss = -9834.071549044489
Iteration 19400: Loss = -9834.071541150091
Iteration 19500: Loss = -9834.071694849823
1
Iteration 19600: Loss = -9834.071541594956
Iteration 19700: Loss = -9834.087475310496
1
Iteration 19800: Loss = -9834.07153251448
Iteration 19900: Loss = -9834.07159696021
pi: tensor([[6.5125e-01, 3.4875e-01],
        [3.8990e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6011, 0.3989], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1992, 0.1024],
         [0.6259, 0.1239]],

        [[0.6835, 0.1341],
         [0.6571, 0.7074]],

        [[0.5353, 0.1471],
         [0.5401, 0.5290]],

        [[0.5943, 0.1493],
         [0.5375, 0.6168]],

        [[0.6547, 0.1465],
         [0.7231, 0.7105]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 17
Adjusted Rand Index: 0.4295078351854348
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 31
Adjusted Rand Index: 0.13582867745684102
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.06762589928057554
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01802990325417766
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
Global Adjusted Rand Index: 0.09341945346707763
Average Adjusted Rand Index: 0.1306838822555798
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22671.844876356558
Iteration 100: Loss = -9846.803208735755
Iteration 200: Loss = -9845.390616127232
Iteration 300: Loss = -9844.69394975905
Iteration 400: Loss = -9842.927928959172
Iteration 500: Loss = -9841.988430215866
Iteration 600: Loss = -9841.37191640555
Iteration 700: Loss = -9840.810959278195
Iteration 800: Loss = -9839.831557281474
Iteration 900: Loss = -9839.518277630112
Iteration 1000: Loss = -9839.37717433536
Iteration 1100: Loss = -9839.303766409892
Iteration 1200: Loss = -9839.263877745714
Iteration 1300: Loss = -9839.23949703325
Iteration 1400: Loss = -9839.223015642614
Iteration 1500: Loss = -9839.21101232746
Iteration 1600: Loss = -9839.201541494756
Iteration 1700: Loss = -9839.193658969685
Iteration 1800: Loss = -9839.186920884558
Iteration 1900: Loss = -9839.180936094786
Iteration 2000: Loss = -9839.175562782168
Iteration 2100: Loss = -9839.170638801428
Iteration 2200: Loss = -9839.16608904492
Iteration 2300: Loss = -9839.161788101814
Iteration 2400: Loss = -9839.157603767098
Iteration 2500: Loss = -9839.153518017712
Iteration 2600: Loss = -9839.149341496914
Iteration 2700: Loss = -9839.144864010585
Iteration 2800: Loss = -9839.139853421737
Iteration 2900: Loss = -9839.13384085703
Iteration 3000: Loss = -9839.126075677417
Iteration 3100: Loss = -9839.115415575696
Iteration 3200: Loss = -9839.100362694619
Iteration 3300: Loss = -9839.081310199357
Iteration 3400: Loss = -9839.064282003954
Iteration 3500: Loss = -9839.054423072337
Iteration 3600: Loss = -9839.049518771137
Iteration 3700: Loss = -9839.046924327
Iteration 3800: Loss = -9839.044884148878
Iteration 3900: Loss = -9839.043612719142
Iteration 4000: Loss = -9839.04269411064
Iteration 4100: Loss = -9839.041879950802
Iteration 4200: Loss = -9839.041512229018
Iteration 4300: Loss = -9839.040694429974
Iteration 4400: Loss = -9839.041621571017
1
Iteration 4500: Loss = -9839.039806866766
Iteration 4600: Loss = -9839.03945456447
Iteration 4700: Loss = -9839.040270235504
1
Iteration 4800: Loss = -9839.038789818442
Iteration 4900: Loss = -9839.038468651372
Iteration 5000: Loss = -9839.038384766272
Iteration 5100: Loss = -9839.037941175104
Iteration 5200: Loss = -9839.037724296033
Iteration 5300: Loss = -9839.03752830053
Iteration 5400: Loss = -9839.03731051695
Iteration 5500: Loss = -9839.037145696815
Iteration 5600: Loss = -9839.03796176689
1
Iteration 5700: Loss = -9839.036859084648
Iteration 5800: Loss = -9839.037978273882
1
Iteration 5900: Loss = -9839.03708345136
2
Iteration 6000: Loss = -9839.03653947449
Iteration 6100: Loss = -9839.038038660206
1
Iteration 6200: Loss = -9839.036395297522
Iteration 6300: Loss = -9839.043306760877
1
Iteration 6400: Loss = -9839.036440780288
Iteration 6500: Loss = -9839.03571493521
Iteration 6600: Loss = -9839.035877378763
1
Iteration 6700: Loss = -9839.03543973352
Iteration 6800: Loss = -9839.035415451835
Iteration 6900: Loss = -9839.03550982679
Iteration 7000: Loss = -9839.037121695632
1
Iteration 7100: Loss = -9839.035196026965
Iteration 7200: Loss = -9839.035098992472
Iteration 7300: Loss = -9839.034864120315
Iteration 7400: Loss = -9839.034828157002
Iteration 7500: Loss = -9839.034831331039
Iteration 7600: Loss = -9839.036312493015
1
Iteration 7700: Loss = -9839.034532064463
Iteration 7800: Loss = -9839.034504639463
Iteration 7900: Loss = -9839.042084557694
1
Iteration 8000: Loss = -9839.034375559651
Iteration 8100: Loss = -9839.034714884512
1
Iteration 8200: Loss = -9839.034249760902
Iteration 8300: Loss = -9839.034239231394
Iteration 8400: Loss = -9839.03416094754
Iteration 8500: Loss = -9839.034579496369
1
Iteration 8600: Loss = -9839.034092334437
Iteration 8700: Loss = -9839.034115888342
Iteration 8800: Loss = -9839.03543157041
1
Iteration 8900: Loss = -9839.033948899993
Iteration 9000: Loss = -9839.033947472493
Iteration 9100: Loss = -9839.034506741884
1
Iteration 9200: Loss = -9839.034128804402
2
Iteration 9300: Loss = -9839.034480936953
3
Iteration 9400: Loss = -9839.033773734553
Iteration 9500: Loss = -9839.061690808752
1
Iteration 9600: Loss = -9839.033694862512
Iteration 9700: Loss = -9839.068887441057
1
Iteration 9800: Loss = -9839.039616845674
2
Iteration 9900: Loss = -9839.03368035834
Iteration 10000: Loss = -9839.03416660473
1
Iteration 10100: Loss = -9839.03375608113
Iteration 10200: Loss = -9839.033558079278
Iteration 10300: Loss = -9839.03849765853
1
Iteration 10400: Loss = -9839.033543917645
Iteration 10500: Loss = -9839.033621284063
Iteration 10600: Loss = -9839.060859952722
1
Iteration 10700: Loss = -9839.033681542418
Iteration 10800: Loss = -9839.033618806272
Iteration 10900: Loss = -9839.033721154785
1
Iteration 11000: Loss = -9839.033803963956
2
Iteration 11100: Loss = -9839.03425120958
3
Iteration 11200: Loss = -9839.034638867124
4
Iteration 11300: Loss = -9839.07120857518
5
Iteration 11400: Loss = -9839.04445372119
6
Iteration 11500: Loss = -9839.034096069583
7
Iteration 11600: Loss = -9839.033721890632
8
Iteration 11700: Loss = -9839.047010337306
9
Iteration 11800: Loss = -9839.033437960525
Iteration 11900: Loss = -9839.033933736742
1
Iteration 12000: Loss = -9839.061941724103
2
Iteration 12100: Loss = -9839.034598098608
3
Iteration 12200: Loss = -9839.035648957666
4
Iteration 12300: Loss = -9839.066447458112
5
Iteration 12400: Loss = -9839.035424614354
6
Iteration 12500: Loss = -9839.033978685984
7
Iteration 12600: Loss = -9839.035012688468
8
Iteration 12700: Loss = -9839.093205328478
9
Iteration 12800: Loss = -9839.03323962482
Iteration 12900: Loss = -9839.034318107499
1
Iteration 13000: Loss = -9839.033341399967
2
Iteration 13100: Loss = -9839.048112261436
3
Iteration 13200: Loss = -9839.077271558675
4
Iteration 13300: Loss = -9839.033593787404
5
Iteration 13400: Loss = -9839.048966387327
6
Iteration 13500: Loss = -9839.033474690545
7
Iteration 13600: Loss = -9839.03327645559
Iteration 13700: Loss = -9839.033895494014
1
Iteration 13800: Loss = -9839.033223142133
Iteration 13900: Loss = -9839.033557896733
1
Iteration 14000: Loss = -9839.03327988996
Iteration 14100: Loss = -9839.076262440874
1
Iteration 14200: Loss = -9839.034142719747
2
Iteration 14300: Loss = -9839.043567768142
3
Iteration 14400: Loss = -9839.038437243915
4
Iteration 14500: Loss = -9839.045447716404
5
Iteration 14600: Loss = -9839.038932089537
6
Iteration 14700: Loss = -9839.033898932643
7
Iteration 14800: Loss = -9839.03370919622
8
Iteration 14900: Loss = -9839.038177637543
9
Iteration 15000: Loss = -9839.035143800887
10
Iteration 15100: Loss = -9839.03645723708
11
Iteration 15200: Loss = -9839.034786880937
12
Iteration 15300: Loss = -9839.033171880556
Iteration 15400: Loss = -9839.033783742925
1
Iteration 15500: Loss = -9839.115300964633
2
Iteration 15600: Loss = -9839.036851952245
3
Iteration 15700: Loss = -9839.045213616948
4
Iteration 15800: Loss = -9839.034633017951
5
Iteration 15900: Loss = -9839.037260290661
6
Iteration 16000: Loss = -9839.033825730732
7
Iteration 16100: Loss = -9839.033538887703
8
Iteration 16200: Loss = -9839.158542007182
9
Iteration 16300: Loss = -9839.033531870771
10
Iteration 16400: Loss = -9839.033222839746
Iteration 16500: Loss = -9839.039927954913
1
Iteration 16600: Loss = -9839.03316056225
Iteration 16700: Loss = -9839.033295917034
1
Iteration 16800: Loss = -9839.179316223132
2
Iteration 16900: Loss = -9839.081842959724
3
Iteration 17000: Loss = -9839.033733820786
4
Iteration 17100: Loss = -9839.033214295485
Iteration 17200: Loss = -9839.035060644004
1
Iteration 17300: Loss = -9839.033165734167
Iteration 17400: Loss = -9839.033655307161
1
Iteration 17500: Loss = -9839.03324517555
Iteration 17600: Loss = -9839.036448079092
1
Iteration 17700: Loss = -9839.039448391304
2
Iteration 17800: Loss = -9839.033360533824
3
Iteration 17900: Loss = -9839.034590282286
4
Iteration 18000: Loss = -9839.033458729271
5
Iteration 18100: Loss = -9839.033775555814
6
Iteration 18200: Loss = -9839.055519748495
7
Iteration 18300: Loss = -9839.03412566284
8
Iteration 18400: Loss = -9839.162462381555
9
Iteration 18500: Loss = -9839.033170834875
Iteration 18600: Loss = -9839.045003634465
1
Iteration 18700: Loss = -9839.117447811508
2
Iteration 18800: Loss = -9839.07497287236
3
Iteration 18900: Loss = -9839.03514626236
4
Iteration 19000: Loss = -9839.036075848157
5
Iteration 19100: Loss = -9839.033752669366
6
Iteration 19200: Loss = -9839.0469107508
7
Iteration 19300: Loss = -9839.033227822316
Iteration 19400: Loss = -9839.033438184335
1
Iteration 19500: Loss = -9839.034403057625
2
Iteration 19600: Loss = -9839.033670782863
3
Iteration 19700: Loss = -9839.035986220782
4
Iteration 19800: Loss = -9839.033147242004
Iteration 19900: Loss = -9839.033376037292
1
pi: tensor([[5.9195e-07, 1.0000e+00],
        [8.4882e-02, 9.1512e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3365, 0.6635], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2580, 0.1321],
         [0.6573, 0.1270]],

        [[0.5671, 0.2064],
         [0.5231, 0.6472]],

        [[0.5904, 0.1736],
         [0.5680, 0.5008]],

        [[0.5788, 0.1673],
         [0.6249, 0.5258]],

        [[0.6425, 0.1537],
         [0.6056, 0.7122]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 33
Adjusted Rand Index: 0.10297643196990373
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.011942367295838408
Average Adjusted Rand Index: 0.022441828253026798
9948.722408716018
[0.09341945346707763, 0.011942367295838408] [0.1306838822555798, 0.022441828253026798] [9834.071638362566, 9839.033285829195]
-------------------------------------
This iteration is 45
True Objective function: Loss = -10304.374629323027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22836.176596238423
Iteration 100: Loss = -10185.979704500558
Iteration 200: Loss = -10185.028387962111
Iteration 300: Loss = -10184.61818801696
Iteration 400: Loss = -10183.762217108457
Iteration 500: Loss = -10182.421249686013
Iteration 600: Loss = -10181.631415881544
Iteration 700: Loss = -10180.890372580501
Iteration 800: Loss = -10180.035985780205
Iteration 900: Loss = -10179.551597370562
Iteration 1000: Loss = -10179.168440473792
Iteration 1100: Loss = -10178.80046408144
Iteration 1200: Loss = -10178.42304042687
Iteration 1300: Loss = -10178.041332044195
Iteration 1400: Loss = -10177.680151194789
Iteration 1500: Loss = -10177.32640884765
Iteration 1600: Loss = -10176.978257575558
Iteration 1700: Loss = -10176.607965020745
Iteration 1800: Loss = -10175.788228905038
Iteration 1900: Loss = -10165.276210883718
Iteration 2000: Loss = -10164.497074260698
Iteration 2100: Loss = -10164.362291477697
Iteration 2200: Loss = -10164.309155696708
Iteration 2300: Loss = -10164.280468986442
Iteration 2400: Loss = -10164.262903855692
Iteration 2500: Loss = -10164.249366421118
Iteration 2600: Loss = -10164.23667696728
Iteration 2700: Loss = -10164.226977637523
Iteration 2800: Loss = -10164.21993626935
Iteration 2900: Loss = -10164.2120120106
Iteration 3000: Loss = -10164.20388694367
Iteration 3100: Loss = -10164.19969078737
Iteration 3200: Loss = -10164.190261637605
Iteration 3300: Loss = -10164.186248530119
Iteration 3400: Loss = -10164.183485946458
Iteration 3500: Loss = -10164.18177587684
Iteration 3600: Loss = -10164.180193470524
Iteration 3700: Loss = -10164.179530125892
Iteration 3800: Loss = -10164.1795935404
Iteration 3900: Loss = -10164.1788955099
Iteration 4000: Loss = -10164.178717261424
Iteration 4100: Loss = -10164.178706877816
Iteration 4200: Loss = -10164.178420344711
Iteration 4300: Loss = -10164.17828742852
Iteration 4400: Loss = -10164.178445062045
1
Iteration 4500: Loss = -10164.178140394279
Iteration 4600: Loss = -10164.178065874434
Iteration 4700: Loss = -10164.178063434578
Iteration 4800: Loss = -10164.178022160855
Iteration 4900: Loss = -10164.17815860118
1
Iteration 5000: Loss = -10164.17794074708
Iteration 5100: Loss = -10164.177902525285
Iteration 5200: Loss = -10164.177898428346
Iteration 5300: Loss = -10164.17785611194
Iteration 5400: Loss = -10164.178625944169
1
Iteration 5500: Loss = -10164.177853969006
Iteration 5600: Loss = -10164.177824130446
Iteration 5700: Loss = -10164.177834775013
Iteration 5800: Loss = -10164.177847579087
Iteration 5900: Loss = -10164.177846206125
Iteration 6000: Loss = -10164.182884851165
1
Iteration 6100: Loss = -10164.177827925176
Iteration 6200: Loss = -10164.17782593705
Iteration 6300: Loss = -10164.177859197005
Iteration 6400: Loss = -10164.177810288944
Iteration 6500: Loss = -10164.177791160018
Iteration 6600: Loss = -10164.177802248963
Iteration 6700: Loss = -10164.177796131942
Iteration 6800: Loss = -10164.177807967722
Iteration 6900: Loss = -10164.177807671795
Iteration 7000: Loss = -10164.177803669847
Iteration 7100: Loss = -10164.179839865294
1
Iteration 7200: Loss = -10164.177781373164
Iteration 7300: Loss = -10164.177807732147
Iteration 7400: Loss = -10164.178593444054
1
Iteration 7500: Loss = -10164.177755819981
Iteration 7600: Loss = -10164.177816940613
Iteration 7700: Loss = -10164.177861329126
Iteration 7800: Loss = -10164.177769604177
Iteration 7900: Loss = -10164.216328780809
1
Iteration 8000: Loss = -10164.177792762799
Iteration 8100: Loss = -10164.181179557447
1
Iteration 8200: Loss = -10164.17779239753
Iteration 8300: Loss = -10164.17826285161
1
Iteration 8400: Loss = -10164.179049635186
2
Iteration 8500: Loss = -10164.177777448833
Iteration 8600: Loss = -10164.177814045108
Iteration 8700: Loss = -10164.185057323823
1
Iteration 8800: Loss = -10164.177915750848
2
Iteration 8900: Loss = -10164.177826569248
Iteration 9000: Loss = -10164.189675096102
1
Iteration 9100: Loss = -10164.178809639943
2
Iteration 9200: Loss = -10164.21036743642
3
Iteration 9300: Loss = -10164.18874755319
4
Iteration 9400: Loss = -10164.242764609038
5
Iteration 9500: Loss = -10164.178837962894
6
Iteration 9600: Loss = -10164.188241628752
7
Iteration 9700: Loss = -10164.187956386777
8
Iteration 9800: Loss = -10164.267814390389
9
Iteration 9900: Loss = -10164.235445123042
10
Iteration 10000: Loss = -10164.20363286731
11
Iteration 10100: Loss = -10164.17786356621
Iteration 10200: Loss = -10164.2596348049
1
Iteration 10300: Loss = -10164.17872890894
2
Iteration 10400: Loss = -10164.177814102666
Iteration 10500: Loss = -10164.182573630076
1
Iteration 10600: Loss = -10164.17795889629
2
Iteration 10700: Loss = -10164.178735348314
3
Iteration 10800: Loss = -10164.186962060054
4
Iteration 10900: Loss = -10164.187168798291
5
Iteration 11000: Loss = -10164.225352518355
6
Iteration 11100: Loss = -10164.177843796671
Iteration 11200: Loss = -10164.179981781992
1
Iteration 11300: Loss = -10164.227004812657
2
Iteration 11400: Loss = -10164.183794065191
3
Iteration 11500: Loss = -10164.178500823758
4
Iteration 11600: Loss = -10164.188749883533
5
Iteration 11700: Loss = -10164.178148637422
6
Iteration 11800: Loss = -10164.178681904612
7
Iteration 11900: Loss = -10164.178166331154
8
Iteration 12000: Loss = -10164.178057253985
9
Iteration 12100: Loss = -10164.177839282276
Iteration 12200: Loss = -10164.178959517818
1
Iteration 12300: Loss = -10164.185377499763
2
Iteration 12400: Loss = -10164.180250613512
3
Iteration 12500: Loss = -10164.192395995888
4
Iteration 12600: Loss = -10164.407180960652
5
Iteration 12700: Loss = -10164.180149770466
6
Iteration 12800: Loss = -10164.243900874677
7
Iteration 12900: Loss = -10164.180695421557
8
Iteration 13000: Loss = -10164.178333450427
9
Iteration 13100: Loss = -10164.179770118926
10
Iteration 13200: Loss = -10164.178576307366
11
Iteration 13300: Loss = -10164.182414552479
12
Iteration 13400: Loss = -10164.206294377742
13
Iteration 13500: Loss = -10164.179161634585
14
Iteration 13600: Loss = -10164.177982004741
15
Stopping early at iteration 13600 due to no improvement.
pi: tensor([[0.8403, 0.1597],
        [0.2377, 0.7623]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1002, 0.8998], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2193, 0.1772],
         [0.6776, 0.1322]],

        [[0.5640, 0.1511],
         [0.5868, 0.6935]],

        [[0.7124, 0.1423],
         [0.5395, 0.5347]],

        [[0.5987, 0.1100],
         [0.5512, 0.7001]],

        [[0.5319, 0.0928],
         [0.6288, 0.5670]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.00038912871648324923
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.04678160671514576
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 24
Adjusted Rand Index: 0.26195497572220316
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 15
Adjusted Rand Index: 0.48483637448226513
time is 4
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 9
Adjusted Rand Index: 0.669076647074352
Global Adjusted Rand Index: 0.21660491133358198
Average Adjusted Rand Index: 0.2926077465420899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23711.542185591174
Iteration 100: Loss = -10186.182046461408
Iteration 200: Loss = -10184.649470976135
Iteration 300: Loss = -10184.018231989174
Iteration 400: Loss = -10182.467088368781
Iteration 500: Loss = -10181.27275434246
Iteration 600: Loss = -10180.7846471164
Iteration 700: Loss = -10179.5868697064
Iteration 800: Loss = -10178.702500868556
Iteration 900: Loss = -10178.121973653186
Iteration 1000: Loss = -10177.590840800674
Iteration 1100: Loss = -10177.103715368105
Iteration 1200: Loss = -10176.625049472947
Iteration 1300: Loss = -10173.582963723871
Iteration 1400: Loss = -10164.570177986505
Iteration 1500: Loss = -10164.341961218133
Iteration 1600: Loss = -10164.272372863958
Iteration 1700: Loss = -10164.247290918498
Iteration 1800: Loss = -10164.226853317563
Iteration 1900: Loss = -10164.212399151913
Iteration 2000: Loss = -10164.20140494914
Iteration 2100: Loss = -10164.19218399371
Iteration 2200: Loss = -10164.18623630514
Iteration 2300: Loss = -10164.182761607684
Iteration 2400: Loss = -10164.180747517788
Iteration 2500: Loss = -10164.179620883797
Iteration 2600: Loss = -10164.179174128063
Iteration 2700: Loss = -10164.178734302915
Iteration 2800: Loss = -10164.1785581999
Iteration 2900: Loss = -10164.178468502581
Iteration 3000: Loss = -10164.178264909311
Iteration 3100: Loss = -10164.178183656344
Iteration 3200: Loss = -10164.178082432993
Iteration 3300: Loss = -10164.17800608687
Iteration 3400: Loss = -10164.17796100312
Iteration 3500: Loss = -10164.17798101694
Iteration 3600: Loss = -10164.177911964915
Iteration 3700: Loss = -10164.179380621183
1
Iteration 3800: Loss = -10164.177886827907
Iteration 3900: Loss = -10164.17782681543
Iteration 4000: Loss = -10164.178267725873
1
Iteration 4100: Loss = -10164.177817335456
Iteration 4200: Loss = -10164.177831262175
Iteration 4300: Loss = -10164.1778120361
Iteration 4400: Loss = -10164.177808359549
Iteration 4500: Loss = -10164.18439817388
1
Iteration 4600: Loss = -10164.177816234935
Iteration 4700: Loss = -10164.17782346441
Iteration 4800: Loss = -10164.178628985228
1
Iteration 4900: Loss = -10164.177792928023
Iteration 5000: Loss = -10164.177824656832
Iteration 5100: Loss = -10164.177971258732
1
Iteration 5200: Loss = -10164.177787248223
Iteration 5300: Loss = -10164.177810329778
Iteration 5400: Loss = -10164.17790364584
Iteration 5500: Loss = -10164.177788627958
Iteration 5600: Loss = -10164.177749884651
Iteration 5700: Loss = -10164.178235530215
1
Iteration 5800: Loss = -10164.177758029356
Iteration 5900: Loss = -10164.17778219366
Iteration 6000: Loss = -10164.183714165165
1
Iteration 6100: Loss = -10164.177790621869
Iteration 6200: Loss = -10164.177755635399
Iteration 6300: Loss = -10164.177838208996
Iteration 6400: Loss = -10164.177785451746
Iteration 6500: Loss = -10164.177775880415
Iteration 6600: Loss = -10164.17807024655
1
Iteration 6700: Loss = -10164.177776341063
Iteration 6800: Loss = -10164.177770473027
Iteration 6900: Loss = -10164.177820476963
Iteration 7000: Loss = -10164.177785238575
Iteration 7100: Loss = -10164.182976716402
1
Iteration 7200: Loss = -10164.177800072875
Iteration 7300: Loss = -10164.177790016602
Iteration 7400: Loss = -10164.178099103852
1
Iteration 7500: Loss = -10164.177754718645
Iteration 7600: Loss = -10164.178674547453
1
Iteration 7700: Loss = -10164.178460910587
2
Iteration 7800: Loss = -10164.17777249251
Iteration 7900: Loss = -10164.178287936998
1
Iteration 8000: Loss = -10164.178616405383
2
Iteration 8100: Loss = -10164.17812696586
3
Iteration 8200: Loss = -10164.177906438676
4
Iteration 8300: Loss = -10164.180950369639
5
Iteration 8400: Loss = -10164.178432153029
6
Iteration 8500: Loss = -10164.178070048338
7
Iteration 8600: Loss = -10164.178053225362
8
Iteration 8700: Loss = -10164.178140941955
9
Iteration 8800: Loss = -10164.196343325246
10
Iteration 8900: Loss = -10164.181279260401
11
Iteration 9000: Loss = -10164.17894050898
12
Iteration 9100: Loss = -10164.180565902367
13
Iteration 9200: Loss = -10164.180764676854
14
Iteration 9300: Loss = -10164.193722000236
15
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.8399, 0.1601],
        [0.2357, 0.7643]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0996, 0.9004], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2199, 0.1773],
         [0.6243, 0.1321]],

        [[0.6888, 0.1516],
         [0.5252, 0.7254]],

        [[0.5582, 0.1428],
         [0.6999, 0.6778]],

        [[0.6850, 0.1104],
         [0.6038, 0.6755]],

        [[0.6759, 0.0930],
         [0.6998, 0.5240]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.00038912871648324923
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.04678160671514576
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 24
Adjusted Rand Index: 0.26195497572220316
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 15
Adjusted Rand Index: 0.48483637448226513
time is 4
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 9
Adjusted Rand Index: 0.669076647074352
Global Adjusted Rand Index: 0.21660491133358198
Average Adjusted Rand Index: 0.2926077465420899
10304.374629323027
[0.21660491133358198, 0.21660491133358198] [0.2926077465420899, 0.2926077465420899] [10164.177982004741, 10164.193722000236]
-------------------------------------
This iteration is 46
True Objective function: Loss = -10059.786738698027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21023.731211304967
Iteration 100: Loss = -9927.402847970228
Iteration 200: Loss = -9926.733037674283
Iteration 300: Loss = -9926.48196232743
Iteration 400: Loss = -9926.321299131514
Iteration 500: Loss = -9926.206645643488
Iteration 600: Loss = -9926.117709153294
Iteration 700: Loss = -9926.043985848328
Iteration 800: Loss = -9925.97959031776
Iteration 900: Loss = -9925.9205588281
Iteration 1000: Loss = -9925.864044546162
Iteration 1100: Loss = -9925.80768910865
Iteration 1200: Loss = -9925.749206576407
Iteration 1300: Loss = -9925.686404879254
Iteration 1400: Loss = -9925.61715744564
Iteration 1500: Loss = -9925.53938163071
Iteration 1600: Loss = -9925.451741802204
Iteration 1700: Loss = -9925.35331668834
Iteration 1800: Loss = -9925.244296186478
Iteration 1900: Loss = -9925.126111344787
Iteration 2000: Loss = -9925.001142071793
Iteration 2100: Loss = -9924.870488962302
Iteration 2200: Loss = -9924.734259223578
Iteration 2300: Loss = -9924.594233522517
Iteration 2400: Loss = -9924.453707388784
Iteration 2500: Loss = -9924.316527495004
Iteration 2600: Loss = -9924.184751991826
Iteration 2700: Loss = -9924.058884389005
Iteration 2800: Loss = -9923.935687988238
Iteration 2900: Loss = -9923.822331787294
Iteration 3000: Loss = -9923.731234632845
Iteration 3100: Loss = -9923.6684082374
Iteration 3200: Loss = -9923.629174502854
Iteration 3300: Loss = -9923.614634566395
Iteration 3400: Loss = -9923.58954183964
Iteration 3500: Loss = -9923.579465027946
Iteration 3600: Loss = -9923.572379029643
Iteration 3700: Loss = -9923.56688185074
Iteration 3800: Loss = -9923.562743687204
Iteration 3900: Loss = -9923.559747884241
Iteration 4000: Loss = -9923.556692773695
Iteration 4100: Loss = -9923.553857143525
Iteration 4200: Loss = -9923.559039277847
1
Iteration 4300: Loss = -9923.54991998435
Iteration 4400: Loss = -9923.547906063823
Iteration 4500: Loss = -9923.54735214113
Iteration 4600: Loss = -9923.544997863124
Iteration 4700: Loss = -9923.543866909129
Iteration 4800: Loss = -9923.543021263591
Iteration 4900: Loss = -9923.541957256724
Iteration 5000: Loss = -9923.54293293653
1
Iteration 5100: Loss = -9923.540921179974
Iteration 5200: Loss = -9923.540221344834
Iteration 5300: Loss = -9923.540898331405
1
Iteration 5400: Loss = -9923.539602981113
Iteration 5500: Loss = -9923.539767541784
1
Iteration 5600: Loss = -9923.539195215651
Iteration 5700: Loss = -9923.539080577066
Iteration 5800: Loss = -9923.540282530796
1
Iteration 5900: Loss = -9923.538892284805
Iteration 6000: Loss = -9923.538927384176
Iteration 6100: Loss = -9923.538768850602
Iteration 6200: Loss = -9923.53884550368
Iteration 6300: Loss = -9923.538767151435
Iteration 6400: Loss = -9923.538721637315
Iteration 6500: Loss = -9923.538778237898
Iteration 6600: Loss = -9923.538724949876
Iteration 6700: Loss = -9923.539241911938
1
Iteration 6800: Loss = -9923.53872107441
Iteration 6900: Loss = -9923.538990597423
1
Iteration 7000: Loss = -9923.539182662875
2
Iteration 7100: Loss = -9923.54346658893
3
Iteration 7200: Loss = -9923.539009512333
4
Iteration 7300: Loss = -9923.538761325639
Iteration 7400: Loss = -9923.539281298568
1
Iteration 7500: Loss = -9923.538742857814
Iteration 7600: Loss = -9923.538759340894
Iteration 7700: Loss = -9923.538700617668
Iteration 7800: Loss = -9923.542471231669
1
Iteration 7900: Loss = -9923.540257128187
2
Iteration 8000: Loss = -9923.558414474035
3
Iteration 8100: Loss = -9923.538712477142
Iteration 8200: Loss = -9923.540166094275
1
Iteration 8300: Loss = -9923.538762336502
Iteration 8400: Loss = -9923.539085674385
1
Iteration 8500: Loss = -9923.538791248411
Iteration 8600: Loss = -9923.539451373192
1
Iteration 8700: Loss = -9923.538846266494
Iteration 8800: Loss = -9923.539318731157
1
Iteration 8900: Loss = -9923.544327752168
2
Iteration 9000: Loss = -9923.547769386047
3
Iteration 9100: Loss = -9923.541236802312
4
Iteration 9200: Loss = -9923.53985491362
5
Iteration 9300: Loss = -9923.549536110157
6
Iteration 9400: Loss = -9923.538779480974
Iteration 9500: Loss = -9923.539673688425
1
Iteration 9600: Loss = -9923.539267424187
2
Iteration 9700: Loss = -9923.545908902943
3
Iteration 9800: Loss = -9923.62597610909
4
Iteration 9900: Loss = -9923.555617501841
5
Iteration 10000: Loss = -9923.539023484318
6
Iteration 10100: Loss = -9923.538736538803
Iteration 10200: Loss = -9923.544266719655
1
Iteration 10300: Loss = -9923.550611495892
2
Iteration 10400: Loss = -9923.539704725468
3
Iteration 10500: Loss = -9923.539234020309
4
Iteration 10600: Loss = -9923.538712174259
Iteration 10700: Loss = -9923.589482569874
1
Iteration 10800: Loss = -9923.540304743374
2
Iteration 10900: Loss = -9923.538914969542
3
Iteration 11000: Loss = -9923.547427091662
4
Iteration 11100: Loss = -9923.750577904239
5
Iteration 11200: Loss = -9923.538751421716
Iteration 11300: Loss = -9923.542457106627
1
Iteration 11400: Loss = -9923.559971604296
2
Iteration 11500: Loss = -9923.624083365696
3
Iteration 11600: Loss = -9923.544805075842
4
Iteration 11700: Loss = -9923.547916068024
5
Iteration 11800: Loss = -9923.589989797787
6
Iteration 11900: Loss = -9923.539065347111
7
Iteration 12000: Loss = -9923.538798742675
Iteration 12100: Loss = -9923.543633522202
1
Iteration 12200: Loss = -9923.538788048812
Iteration 12300: Loss = -9923.540472915929
1
Iteration 12400: Loss = -9923.53870280265
Iteration 12500: Loss = -9923.538913981169
1
Iteration 12600: Loss = -9923.538753154615
Iteration 12700: Loss = -9923.543352032548
1
Iteration 12800: Loss = -9923.53871601685
Iteration 12900: Loss = -9923.53920353759
1
Iteration 13000: Loss = -9923.54101842619
2
Iteration 13100: Loss = -9923.541823814538
3
Iteration 13200: Loss = -9923.613677979127
4
Iteration 13300: Loss = -9923.538802200586
Iteration 13400: Loss = -9923.53935503155
1
Iteration 13500: Loss = -9923.538981376587
2
Iteration 13600: Loss = -9923.542744779988
3
Iteration 13700: Loss = -9923.540522662568
4
Iteration 13800: Loss = -9923.751030601163
5
Iteration 13900: Loss = -9923.538713372911
Iteration 14000: Loss = -9923.53902074274
1
Iteration 14100: Loss = -9923.538709754508
Iteration 14200: Loss = -9923.539408899305
1
Iteration 14300: Loss = -9923.540443458778
2
Iteration 14400: Loss = -9923.541942601198
3
Iteration 14500: Loss = -9923.555057500107
4
Iteration 14600: Loss = -9923.539158449586
5
Iteration 14700: Loss = -9923.573980014942
6
Iteration 14800: Loss = -9923.540987584925
7
Iteration 14900: Loss = -9923.766030588233
8
Iteration 15000: Loss = -9923.538962405255
9
Iteration 15100: Loss = -9923.53872585211
Iteration 15200: Loss = -9923.53946371202
1
Iteration 15300: Loss = -9923.546246258906
2
Iteration 15400: Loss = -9923.540208425604
3
Iteration 15500: Loss = -9923.610262820126
4
Iteration 15600: Loss = -9923.539404228666
5
Iteration 15700: Loss = -9923.53961520321
6
Iteration 15800: Loss = -9923.540338283909
7
Iteration 15900: Loss = -9923.540449686125
8
Iteration 16000: Loss = -9923.538914075001
9
Iteration 16100: Loss = -9923.538866999983
10
Iteration 16200: Loss = -9923.540205652467
11
Iteration 16300: Loss = -9923.539422117648
12
Iteration 16400: Loss = -9923.538859378968
13
Iteration 16500: Loss = -9923.539121195588
14
Iteration 16600: Loss = -9923.538853902399
15
Stopping early at iteration 16600 due to no improvement.
pi: tensor([[0.7585, 0.2415],
        [0.6018, 0.3982]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0069, 0.9931], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1439, 0.1074],
         [0.5222, 0.1304]],

        [[0.6896, 0.1366],
         [0.5724, 0.5001]],

        [[0.7084, 0.1411],
         [0.6463, 0.7207]],

        [[0.6677, 0.1437],
         [0.7244, 0.6802]],

        [[0.6168, 0.1289],
         [0.5433, 0.6451]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.005005903861901204
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21735.344735924406
Iteration 100: Loss = -9927.72443093221
Iteration 200: Loss = -9926.665912593084
Iteration 300: Loss = -9926.330162661172
Iteration 400: Loss = -9926.195281851631
Iteration 500: Loss = -9926.122370723057
Iteration 600: Loss = -9926.070113020169
Iteration 700: Loss = -9926.024962993131
Iteration 800: Loss = -9925.981432160312
Iteration 900: Loss = -9925.936934789135
Iteration 1000: Loss = -9925.89009129277
Iteration 1100: Loss = -9925.840231202217
Iteration 1200: Loss = -9925.78623959313
Iteration 1300: Loss = -9925.72658974854
Iteration 1400: Loss = -9925.65922711678
Iteration 1500: Loss = -9925.582200662931
Iteration 1600: Loss = -9925.492905687852
Iteration 1700: Loss = -9925.387202914171
Iteration 1800: Loss = -9925.260180077828
Iteration 1900: Loss = -9925.108989330682
Iteration 2000: Loss = -9924.935778667947
Iteration 2100: Loss = -9924.749020875834
Iteration 2200: Loss = -9924.566233987318
Iteration 2300: Loss = -9924.404997485984
Iteration 2400: Loss = -9924.270594623087
Iteration 2500: Loss = -9924.161974866935
Iteration 2600: Loss = -9924.058406115391
Iteration 2700: Loss = -9923.94211541499
Iteration 2800: Loss = -9923.80486717266
Iteration 2900: Loss = -9923.69177834842
Iteration 3000: Loss = -9923.63064857882
Iteration 3100: Loss = -9923.600171993192
Iteration 3200: Loss = -9923.583957124663
Iteration 3300: Loss = -9923.573684168223
Iteration 3400: Loss = -9923.56682463466
Iteration 3500: Loss = -9923.561802619764
Iteration 3600: Loss = -9923.557833486999
Iteration 3700: Loss = -9923.554465921976
Iteration 3800: Loss = -9923.551689950265
Iteration 3900: Loss = -9923.549376451321
Iteration 4000: Loss = -9923.547463532681
Iteration 4100: Loss = -9923.545918945785
Iteration 4200: Loss = -9923.544500453581
Iteration 4300: Loss = -9923.55229408242
1
Iteration 4400: Loss = -9923.542490629185
Iteration 4500: Loss = -9923.54177374627
Iteration 4600: Loss = -9923.541160074616
Iteration 4700: Loss = -9923.540868187261
Iteration 4800: Loss = -9923.540290092156
Iteration 4900: Loss = -9923.540447881269
1
Iteration 5000: Loss = -9923.539834592326
Iteration 5100: Loss = -9923.539624604473
Iteration 5200: Loss = -9923.539510658838
Iteration 5300: Loss = -9923.539373763353
Iteration 5400: Loss = -9923.539340084542
Iteration 5500: Loss = -9923.539202340196
Iteration 5600: Loss = -9923.539070805815
Iteration 5700: Loss = -9923.539698165727
1
Iteration 5800: Loss = -9923.542518369355
2
Iteration 5900: Loss = -9923.538940501629
Iteration 6000: Loss = -9923.547677758339
1
Iteration 6100: Loss = -9923.538905560647
Iteration 6200: Loss = -9923.550023851702
1
Iteration 6300: Loss = -9923.538846267307
Iteration 6400: Loss = -9923.53885694098
Iteration 6500: Loss = -9923.538897649869
Iteration 6600: Loss = -9923.538831028534
Iteration 6700: Loss = -9923.542069798948
1
Iteration 6800: Loss = -9923.538833039689
Iteration 6900: Loss = -9923.544463610566
1
Iteration 7000: Loss = -9923.538787078727
Iteration 7100: Loss = -9923.540235514865
1
Iteration 7200: Loss = -9923.54026000809
2
Iteration 7300: Loss = -9923.54195513957
3
Iteration 7400: Loss = -9923.540320994522
4
Iteration 7500: Loss = -9923.551670015551
5
Iteration 7600: Loss = -9923.53881404067
Iteration 7700: Loss = -9923.538798802707
Iteration 7800: Loss = -9923.590853468282
1
Iteration 7900: Loss = -9923.538752640665
Iteration 8000: Loss = -9923.590385715806
1
Iteration 8100: Loss = -9923.543027634563
2
Iteration 8200: Loss = -9923.54424287894
3
Iteration 8300: Loss = -9923.541631997967
4
Iteration 8400: Loss = -9923.538987496333
5
Iteration 8500: Loss = -9923.542299541079
6
Iteration 8600: Loss = -9923.542264043921
7
Iteration 8700: Loss = -9923.540979497513
8
Iteration 8800: Loss = -9923.538920762232
9
Iteration 8900: Loss = -9923.548417207918
10
Iteration 9000: Loss = -9923.61551031095
11
Iteration 9100: Loss = -9923.5433807529
12
Iteration 9200: Loss = -9923.538868113035
13
Iteration 9300: Loss = -9923.538936539093
14
Iteration 9400: Loss = -9923.541367733276
15
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.7555, 0.2445],
        [0.5926, 0.4074]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0070, 0.9930], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1441, 0.1070],
         [0.6580, 0.1305]],

        [[0.6868, 0.1368],
         [0.7170, 0.5057]],

        [[0.6230, 0.1411],
         [0.5731, 0.7094]],

        [[0.5351, 0.1437],
         [0.5158, 0.6665]],

        [[0.5618, 0.1291],
         [0.6696, 0.7080]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.005737907951168538
Average Adjusted Rand Index: -0.00015692302765368048
10059.786738698027
[0.005005903861901204, 0.005737907951168538] [0.0, -0.00015692302765368048] [9923.538853902399, 9923.541367733276]
-------------------------------------
This iteration is 47
True Objective function: Loss = -10159.731197778518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22792.010955681537
Iteration 100: Loss = -10024.109231221708
Iteration 200: Loss = -10022.136760514706
Iteration 300: Loss = -10021.499985403532
Iteration 400: Loss = -10021.25403583788
Iteration 500: Loss = -10021.132383318676
Iteration 600: Loss = -10021.052142574381
Iteration 700: Loss = -10020.983987788595
Iteration 800: Loss = -10020.909340910266
Iteration 900: Loss = -10020.802714165764
Iteration 1000: Loss = -10020.634354663924
Iteration 1100: Loss = -10020.440471707323
Iteration 1200: Loss = -10020.260899924984
Iteration 1300: Loss = -10020.081281723662
Iteration 1400: Loss = -10019.884125643062
Iteration 1500: Loss = -10019.660750628927
Iteration 1600: Loss = -10019.417406935996
Iteration 1700: Loss = -10019.174060366478
Iteration 1800: Loss = -10018.95139538198
Iteration 1900: Loss = -10018.760391135575
Iteration 2000: Loss = -10018.596072075232
Iteration 2100: Loss = -10018.41141940685
Iteration 2200: Loss = -10018.114518712653
Iteration 2300: Loss = -10017.952185362139
Iteration 2400: Loss = -10017.851692978733
Iteration 2500: Loss = -10017.772789132167
Iteration 2600: Loss = -10017.701894370915
Iteration 2700: Loss = -10017.628599435067
Iteration 2800: Loss = -10017.539692185477
Iteration 2900: Loss = -10017.430005160748
Iteration 3000: Loss = -10017.30890732571
Iteration 3100: Loss = -10017.180139855698
Iteration 3200: Loss = -10017.048757989483
Iteration 3300: Loss = -10016.85002940678
Iteration 3400: Loss = -10016.44927163735
Iteration 3500: Loss = -10015.391167472744
Iteration 3600: Loss = -10013.659833884787
Iteration 3700: Loss = -10012.1940879536
Iteration 3800: Loss = -10011.710744488555
Iteration 3900: Loss = -10011.583494567642
Iteration 4000: Loss = -10011.553027576458
Iteration 4100: Loss = -10011.545547708325
Iteration 4200: Loss = -10011.54355026976
Iteration 4300: Loss = -10011.54294189161
Iteration 4400: Loss = -10011.542731366428
Iteration 4500: Loss = -10011.5426144459
Iteration 4600: Loss = -10011.543210998132
1
Iteration 4700: Loss = -10011.542554786536
Iteration 4800: Loss = -10011.542538379077
Iteration 4900: Loss = -10011.54273564301
1
Iteration 5000: Loss = -10011.542518737711
Iteration 5100: Loss = -10011.542539319566
Iteration 5200: Loss = -10011.54413730941
1
Iteration 5300: Loss = -10011.542497237728
Iteration 5400: Loss = -10011.543985521264
1
Iteration 5500: Loss = -10011.543382875308
2
Iteration 5600: Loss = -10011.542476237128
Iteration 5700: Loss = -10011.542589795221
1
Iteration 5800: Loss = -10011.54256762823
Iteration 5900: Loss = -10011.542569449313
Iteration 6000: Loss = -10011.544019482139
1
Iteration 6100: Loss = -10011.542517508404
Iteration 6200: Loss = -10011.542731127069
1
Iteration 6300: Loss = -10011.542589269024
Iteration 6400: Loss = -10011.543007558195
1
Iteration 6500: Loss = -10011.542595625051
Iteration 6600: Loss = -10011.544107371696
1
Iteration 6700: Loss = -10011.542455451738
Iteration 6800: Loss = -10011.54339060207
1
Iteration 6900: Loss = -10011.54251517577
Iteration 7000: Loss = -10011.54265636176
1
Iteration 7100: Loss = -10011.542481069659
Iteration 7200: Loss = -10011.542479101425
Iteration 7300: Loss = -10011.542556458495
Iteration 7400: Loss = -10011.542515151688
Iteration 7500: Loss = -10011.542532160563
Iteration 7600: Loss = -10011.542641681755
1
Iteration 7700: Loss = -10011.542453664193
Iteration 7800: Loss = -10011.54252813178
Iteration 7900: Loss = -10011.543379888222
1
Iteration 8000: Loss = -10011.542495242618
Iteration 8100: Loss = -10011.542517458502
Iteration 8200: Loss = -10011.542527404732
Iteration 8300: Loss = -10011.542524114124
Iteration 8400: Loss = -10011.542494889136
Iteration 8500: Loss = -10011.54253574677
Iteration 8600: Loss = -10011.542510645168
Iteration 8700: Loss = -10011.641432561428
1
Iteration 8800: Loss = -10011.542503218458
Iteration 8900: Loss = -10011.54249495913
Iteration 9000: Loss = -10011.569501614735
1
Iteration 9100: Loss = -10011.542513296496
Iteration 9200: Loss = -10011.542533761743
Iteration 9300: Loss = -10011.54869786409
1
Iteration 9400: Loss = -10011.542503928713
Iteration 9500: Loss = -10011.542517634496
Iteration 9600: Loss = -10011.54250703272
Iteration 9700: Loss = -10011.54248341695
Iteration 9800: Loss = -10011.969995936972
1
Iteration 9900: Loss = -10011.542509327977
Iteration 10000: Loss = -10011.54248198583
Iteration 10100: Loss = -10011.542478679294
Iteration 10200: Loss = -10011.542734196217
1
Iteration 10300: Loss = -10011.542490199532
Iteration 10400: Loss = -10011.582108291996
1
Iteration 10500: Loss = -10011.542524592378
Iteration 10600: Loss = -10011.542517260556
Iteration 10700: Loss = -10011.663300707738
1
Iteration 10800: Loss = -10011.542513739629
Iteration 10900: Loss = -10011.542484820318
Iteration 11000: Loss = -10011.547820545318
1
Iteration 11100: Loss = -10011.542492180772
Iteration 11200: Loss = -10011.542520658868
Iteration 11300: Loss = -10011.54254196382
Iteration 11400: Loss = -10011.542495633206
Iteration 11500: Loss = -10011.542682351725
1
Iteration 11600: Loss = -10011.549167706376
2
Iteration 11700: Loss = -10011.54249783062
Iteration 11800: Loss = -10011.96188181979
1
Iteration 11900: Loss = -10011.542496814493
Iteration 12000: Loss = -10011.549640298386
1
Iteration 12100: Loss = -10011.542494698384
Iteration 12200: Loss = -10011.57200762564
1
Iteration 12300: Loss = -10011.542514709974
Iteration 12400: Loss = -10011.573665708
1
Iteration 12500: Loss = -10011.542510259373
Iteration 12600: Loss = -10011.552951343974
1
Iteration 12700: Loss = -10011.542529669967
Iteration 12800: Loss = -10011.542839812919
1
Iteration 12900: Loss = -10011.683932436947
2
Iteration 13000: Loss = -10011.542503494542
Iteration 13100: Loss = -10011.55946594526
1
Iteration 13200: Loss = -10011.542519047123
Iteration 13300: Loss = -10011.614941057927
1
Iteration 13400: Loss = -10011.54251300334
Iteration 13500: Loss = -10011.542510070465
Iteration 13600: Loss = -10011.546632995894
1
Iteration 13700: Loss = -10011.542539236369
Iteration 13800: Loss = -10011.542502649836
Iteration 13900: Loss = -10011.54522898304
1
Iteration 14000: Loss = -10011.54250428345
Iteration 14100: Loss = -10011.542474658294
Iteration 14200: Loss = -10011.551245018838
1
Iteration 14300: Loss = -10011.542502841572
Iteration 14400: Loss = -10011.542919415537
1
Iteration 14500: Loss = -10011.542476538114
Iteration 14600: Loss = -10011.543259759446
1
Iteration 14700: Loss = -10011.542520311361
Iteration 14800: Loss = -10011.542694223679
1
Iteration 14900: Loss = -10011.542557457291
Iteration 15000: Loss = -10011.542511413467
Iteration 15100: Loss = -10011.564913018728
1
Iteration 15200: Loss = -10011.54253209342
Iteration 15300: Loss = -10011.813446060523
1
Iteration 15400: Loss = -10011.542510552903
Iteration 15500: Loss = -10011.54952832318
1
Iteration 15600: Loss = -10011.5425076953
Iteration 15700: Loss = -10011.642601322617
1
Iteration 15800: Loss = -10011.542505194113
Iteration 15900: Loss = -10011.542531707088
Iteration 16000: Loss = -10011.542526538493
Iteration 16100: Loss = -10011.570036417175
1
Iteration 16200: Loss = -10011.542545584069
Iteration 16300: Loss = -10011.542472177996
Iteration 16400: Loss = -10011.543128546702
1
Iteration 16500: Loss = -10011.542499750381
Iteration 16600: Loss = -10011.556977800019
1
Iteration 16700: Loss = -10011.542532540374
Iteration 16800: Loss = -10011.542905132635
1
Iteration 16900: Loss = -10011.560634650248
2
Iteration 17000: Loss = -10011.54250390501
Iteration 17100: Loss = -10011.562069408503
1
Iteration 17200: Loss = -10011.542530697734
Iteration 17300: Loss = -10011.545409836404
1
Iteration 17400: Loss = -10011.542511205967
Iteration 17500: Loss = -10011.542584099323
Iteration 17600: Loss = -10011.542797055145
1
Iteration 17700: Loss = -10011.542509063825
Iteration 17800: Loss = -10011.592196062642
1
Iteration 17900: Loss = -10011.542517392994
Iteration 18000: Loss = -10011.717701110145
1
Iteration 18100: Loss = -10011.542515359879
Iteration 18200: Loss = -10011.54249108161
Iteration 18300: Loss = -10011.54263094216
1
Iteration 18400: Loss = -10011.542471054463
Iteration 18500: Loss = -10011.54345248296
1
Iteration 18600: Loss = -10011.54250946199
Iteration 18700: Loss = -10011.554290109218
1
Iteration 18800: Loss = -10011.542483072773
Iteration 18900: Loss = -10011.596363820225
1
Iteration 19000: Loss = -10011.5425191634
Iteration 19100: Loss = -10011.543514753064
1
Iteration 19200: Loss = -10011.542519273153
Iteration 19300: Loss = -10011.544552180361
1
Iteration 19400: Loss = -10011.542493809286
Iteration 19500: Loss = -10011.604686312357
1
Iteration 19600: Loss = -10011.54251552888
Iteration 19700: Loss = -10011.546739578534
1
Iteration 19800: Loss = -10011.54247052812
Iteration 19900: Loss = -10011.883953821705
1
pi: tensor([[0.8957, 0.1043],
        [0.6035, 0.3965]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8313, 0.1687], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1544, 0.1193],
         [0.5254, 0.0756]],

        [[0.6546, 0.0950],
         [0.6984, 0.5917]],

        [[0.6196, 0.1181],
         [0.5481, 0.5305]],

        [[0.5249, 0.1025],
         [0.6912, 0.7045]],

        [[0.6361, 0.0964],
         [0.6680, 0.6691]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.003485103334991517
time is 1
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0026082811587838017
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.030303030303030304
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.010344451541541386
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.024585855411109966
Global Adjusted Rand Index: 0.014680711126021482
Average Adjusted Rand Index: 0.013222031886377874
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22985.457781736714
Iteration 100: Loss = -10022.194553307063
Iteration 200: Loss = -10021.164547924258
Iteration 300: Loss = -10020.864354071728
Iteration 400: Loss = -10020.653947581946
Iteration 500: Loss = -10020.42040981609
Iteration 600: Loss = -10020.176192332387
Iteration 700: Loss = -10019.941938791151
Iteration 800: Loss = -10019.712340927088
Iteration 900: Loss = -10019.462268377158
Iteration 1000: Loss = -10019.179156721522
Iteration 1100: Loss = -10018.854449861416
Iteration 1200: Loss = -10018.445683376614
Iteration 1300: Loss = -10018.004160952596
Iteration 1400: Loss = -10017.78044451472
Iteration 1500: Loss = -10017.70449644335
Iteration 1600: Loss = -10017.670804430449
Iteration 1700: Loss = -10017.650365910284
Iteration 1800: Loss = -10017.63537366894
Iteration 1900: Loss = -10017.623190938117
Iteration 2000: Loss = -10017.612809201066
Iteration 2100: Loss = -10017.603659281172
Iteration 2200: Loss = -10017.59549609604
Iteration 2300: Loss = -10017.58812007962
Iteration 2400: Loss = -10017.58144067993
Iteration 2500: Loss = -10017.57532696259
Iteration 2600: Loss = -10017.569690199125
Iteration 2700: Loss = -10017.564472741173
Iteration 2800: Loss = -10017.55965950121
Iteration 2900: Loss = -10017.555219365078
Iteration 3000: Loss = -10017.551104071465
Iteration 3100: Loss = -10017.547279100158
Iteration 3200: Loss = -10017.54373377418
Iteration 3300: Loss = -10017.540470331654
Iteration 3400: Loss = -10017.53746218551
Iteration 3500: Loss = -10017.534662072318
Iteration 3600: Loss = -10017.532187367526
Iteration 3700: Loss = -10017.529818649367
Iteration 3800: Loss = -10017.527677587863
Iteration 3900: Loss = -10017.525723731698
Iteration 4000: Loss = -10017.523987034518
Iteration 4100: Loss = -10017.522324787169
Iteration 4200: Loss = -10017.520847547714
Iteration 4300: Loss = -10017.520731670844
Iteration 4400: Loss = -10017.518318994766
Iteration 4500: Loss = -10017.517205845663
Iteration 4600: Loss = -10017.516642079268
Iteration 4700: Loss = -10017.515301976542
Iteration 4800: Loss = -10017.514474285754
Iteration 4900: Loss = -10017.513816231354
Iteration 5000: Loss = -10017.513030289789
Iteration 5100: Loss = -10017.51245831482
Iteration 5200: Loss = -10017.511840852556
Iteration 5300: Loss = -10017.511300824252
Iteration 5400: Loss = -10017.51098201743
Iteration 5500: Loss = -10017.51039662726
Iteration 5600: Loss = -10017.5099492235
Iteration 5700: Loss = -10017.509679462311
Iteration 5800: Loss = -10017.509251964131
Iteration 5900: Loss = -10017.508932969393
Iteration 6000: Loss = -10017.508624542139
Iteration 6100: Loss = -10017.508342665646
Iteration 6200: Loss = -10017.510881421405
1
Iteration 6300: Loss = -10017.507834007069
Iteration 6400: Loss = -10017.507578239305
Iteration 6500: Loss = -10017.50739141074
Iteration 6600: Loss = -10017.507188274216
Iteration 6700: Loss = -10017.507273024537
Iteration 6800: Loss = -10017.506828567508
Iteration 6900: Loss = -10017.50662591822
Iteration 7000: Loss = -10017.506472757508
Iteration 7100: Loss = -10017.506311190076
Iteration 7200: Loss = -10017.506186551984
Iteration 7300: Loss = -10017.506061001628
Iteration 7400: Loss = -10017.519358678384
1
Iteration 7500: Loss = -10017.505796607687
Iteration 7600: Loss = -10017.50567659788
Iteration 7700: Loss = -10017.505555884885
Iteration 7800: Loss = -10017.505460320363
Iteration 7900: Loss = -10017.505366579702
Iteration 8000: Loss = -10017.505283474184
Iteration 8100: Loss = -10017.50573871088
1
Iteration 8200: Loss = -10017.511519308391
2
Iteration 8300: Loss = -10017.504978124694
Iteration 8400: Loss = -10017.506222616368
1
Iteration 8500: Loss = -10017.505469679656
2
Iteration 8600: Loss = -10017.504799552007
Iteration 8700: Loss = -10017.50472398158
Iteration 8800: Loss = -10017.504705118947
Iteration 8900: Loss = -10017.504610926972
Iteration 9000: Loss = -10017.506298926673
1
Iteration 9100: Loss = -10017.504499434084
Iteration 9200: Loss = -10017.504443699612
Iteration 9300: Loss = -10017.98060341893
1
Iteration 9400: Loss = -10017.5043450688
Iteration 9500: Loss = -10017.504330135833
Iteration 9600: Loss = -10017.504394492724
Iteration 9700: Loss = -10017.504214908284
Iteration 9800: Loss = -10017.504221799942
Iteration 9900: Loss = -10017.504945455292
1
Iteration 10000: Loss = -10017.504115820431
Iteration 10100: Loss = -10017.50408999767
Iteration 10200: Loss = -10017.505044589312
1
Iteration 10300: Loss = -10017.50402489871
Iteration 10400: Loss = -10017.50400538268
Iteration 10500: Loss = -10017.527553275957
1
Iteration 10600: Loss = -10017.503921779757
Iteration 10700: Loss = -10017.503926996376
Iteration 10800: Loss = -10017.503915923799
Iteration 10900: Loss = -10017.503997880607
Iteration 11000: Loss = -10017.50383070506
Iteration 11100: Loss = -10017.50385088425
Iteration 11200: Loss = -10017.503816396587
Iteration 11300: Loss = -10017.503818451263
Iteration 11400: Loss = -10017.51575937779
1
Iteration 11500: Loss = -10017.60676503336
2
Iteration 11600: Loss = -10017.50377068986
Iteration 11700: Loss = -10017.50381342649
Iteration 11800: Loss = -10017.509916469786
1
Iteration 11900: Loss = -10017.504713725588
2
Iteration 12000: Loss = -10017.50415738993
3
Iteration 12100: Loss = -10017.506750836947
4
Iteration 12200: Loss = -10017.503687576042
Iteration 12300: Loss = -10017.503678808664
Iteration 12400: Loss = -10017.522041622462
1
Iteration 12500: Loss = -10017.50442796383
2
Iteration 12600: Loss = -10017.503953769361
3
Iteration 12700: Loss = -10017.506250330316
4
Iteration 12800: Loss = -10017.505366962348
5
Iteration 12900: Loss = -10017.504158164851
6
Iteration 13000: Loss = -10017.504595576054
7
Iteration 13100: Loss = -10017.64229181493
8
Iteration 13200: Loss = -10017.504499685683
9
Iteration 13300: Loss = -10017.50391726176
10
Iteration 13400: Loss = -10017.59999410941
11
Iteration 13500: Loss = -10017.61144327556
12
Iteration 13600: Loss = -10017.503706814223
Iteration 13700: Loss = -10017.51597714396
1
Iteration 13800: Loss = -10017.509528540157
2
Iteration 13900: Loss = -10017.50397701907
3
Iteration 14000: Loss = -10017.50353287697
Iteration 14100: Loss = -10017.504608678108
1
Iteration 14200: Loss = -10017.50355151221
Iteration 14300: Loss = -10017.503618885381
Iteration 14400: Loss = -10017.50441377411
1
Iteration 14500: Loss = -10017.50404793967
2
Iteration 14600: Loss = -10017.543826903062
3
Iteration 14700: Loss = -10017.507702522766
4
Iteration 14800: Loss = -10017.503577606638
Iteration 14900: Loss = -10017.504142235404
1
Iteration 15000: Loss = -10017.504654995417
2
Iteration 15100: Loss = -10017.50386013854
3
Iteration 15200: Loss = -10017.503706793994
4
Iteration 15300: Loss = -10017.506183462812
5
Iteration 15400: Loss = -10017.50350686293
Iteration 15500: Loss = -10017.503507471763
Iteration 15600: Loss = -10017.50392939981
1
Iteration 15700: Loss = -10017.503491577689
Iteration 15800: Loss = -10017.50589341722
1
Iteration 15900: Loss = -10017.503596593748
2
Iteration 16000: Loss = -10017.505119252966
3
Iteration 16100: Loss = -10017.5038392699
4
Iteration 16200: Loss = -10017.507681843283
5
Iteration 16300: Loss = -10017.551486361333
6
Iteration 16400: Loss = -10017.50345186636
Iteration 16500: Loss = -10017.503705236388
1
Iteration 16600: Loss = -10017.503880455577
2
Iteration 16700: Loss = -10017.50410995794
3
Iteration 16800: Loss = -10017.54855819713
4
Iteration 16900: Loss = -10017.503479533403
Iteration 17000: Loss = -10017.504381184808
1
Iteration 17100: Loss = -10017.514695831147
2
Iteration 17200: Loss = -10017.5445241597
3
Iteration 17300: Loss = -10017.506757734594
4
Iteration 17400: Loss = -10017.503502231675
Iteration 17500: Loss = -10017.503619618612
1
Iteration 17600: Loss = -10017.504670727863
2
Iteration 17700: Loss = -10017.504212838077
3
Iteration 17800: Loss = -10017.504000980825
4
Iteration 17900: Loss = -10017.507529013541
5
Iteration 18000: Loss = -10017.504125289159
6
Iteration 18100: Loss = -10017.50618203222
7
Iteration 18200: Loss = -10017.505078761616
8
Iteration 18300: Loss = -10017.505276044716
9
Iteration 18400: Loss = -10017.503571558966
Iteration 18500: Loss = -10017.505276821083
1
Iteration 18600: Loss = -10017.503429001168
Iteration 18700: Loss = -10017.67554170511
1
Iteration 18800: Loss = -10017.503459446569
Iteration 18900: Loss = -10017.529131880701
1
Iteration 19000: Loss = -10017.508176823456
2
Iteration 19100: Loss = -10017.503556780828
Iteration 19200: Loss = -10017.503808588846
1
Iteration 19300: Loss = -10017.505351724252
2
Iteration 19400: Loss = -10017.50544376368
3
Iteration 19500: Loss = -10017.503494869843
Iteration 19600: Loss = -10017.508311410145
1
Iteration 19700: Loss = -10017.521444358294
2
Iteration 19800: Loss = -10017.538952043875
3
Iteration 19900: Loss = -10017.513301389865
4
pi: tensor([[9.4685e-01, 5.3148e-02],
        [1.0000e+00, 2.4417e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9930, 0.0070], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1380, 0.2197],
         [0.5935, 0.1943]],

        [[0.6271, 0.0733],
         [0.7151, 0.6594]],

        [[0.6788, 0.1882],
         [0.6675, 0.7259]],

        [[0.6081, 0.1929],
         [0.6575, 0.6977]],

        [[0.5110, 0.1622],
         [0.5146, 0.7229]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -6.397451132144595e-06
Average Adjusted Rand Index: 0.0001765225229322539
10159.731197778518
[0.014680711126021482, -6.397451132144595e-06] [0.013222031886377874, 0.0001765225229322539] [10011.542499983147, 10017.51922965355]
-------------------------------------
This iteration is 48
True Objective function: Loss = -9989.461563909359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25143.332122200758
Iteration 100: Loss = -9854.335549086103
Iteration 200: Loss = -9849.70296399017
Iteration 300: Loss = -9849.061032559976
Iteration 400: Loss = -9848.91001805699
Iteration 500: Loss = -9848.817415915013
Iteration 600: Loss = -9848.718136478237
Iteration 700: Loss = -9848.513572289745
Iteration 800: Loss = -9848.0378405801
Iteration 900: Loss = -9847.680810761343
Iteration 1000: Loss = -9847.4643109707
Iteration 1100: Loss = -9847.31001374826
Iteration 1200: Loss = -9847.181385810914
Iteration 1300: Loss = -9847.075361535395
Iteration 1400: Loss = -9847.000203660631
Iteration 1500: Loss = -9846.954603905087
Iteration 1600: Loss = -9846.929496966484
Iteration 1700: Loss = -9846.916125796588
Iteration 1800: Loss = -9846.9086534895
Iteration 1900: Loss = -9846.903485397796
Iteration 2000: Loss = -9846.898263399436
Iteration 2100: Loss = -9846.889025824903
Iteration 2200: Loss = -9846.856390976449
Iteration 2300: Loss = -9846.569012232756
Iteration 2400: Loss = -9845.739406002065
Iteration 2500: Loss = -9845.499095471261
Iteration 2600: Loss = -9845.452755407166
Iteration 2700: Loss = -9845.441188761242
Iteration 2800: Loss = -9845.437092516384
Iteration 2900: Loss = -9845.434911966326
Iteration 3000: Loss = -9845.433348208915
Iteration 3100: Loss = -9845.43204472935
Iteration 3200: Loss = -9845.430840323308
Iteration 3300: Loss = -9845.429750768928
Iteration 3400: Loss = -9845.428647081853
Iteration 3500: Loss = -9845.42753269096
Iteration 3600: Loss = -9845.426556859846
Iteration 3700: Loss = -9845.42551559129
Iteration 3800: Loss = -9845.424500084362
Iteration 3900: Loss = -9845.42349094111
Iteration 4000: Loss = -9845.422532415312
Iteration 4100: Loss = -9845.421578122452
Iteration 4200: Loss = -9845.420603924049
Iteration 4300: Loss = -9845.419656682194
Iteration 4400: Loss = -9845.418723285313
Iteration 4500: Loss = -9845.41783885756
Iteration 4600: Loss = -9845.416936113787
Iteration 4700: Loss = -9845.416109532896
Iteration 4800: Loss = -9845.415253719575
Iteration 4900: Loss = -9845.41441797764
Iteration 5000: Loss = -9845.41366571758
Iteration 5100: Loss = -9845.412987338457
Iteration 5200: Loss = -9845.412188225811
Iteration 5300: Loss = -9845.411502809018
Iteration 5400: Loss = -9845.41085208033
Iteration 5500: Loss = -9845.410245773626
Iteration 5600: Loss = -9845.409686939725
Iteration 5700: Loss = -9845.4091426979
Iteration 5800: Loss = -9845.408666547932
Iteration 5900: Loss = -9845.408240660188
Iteration 6000: Loss = -9845.407817908066
Iteration 6100: Loss = -9845.407459050813
Iteration 6200: Loss = -9845.407109981224
Iteration 6300: Loss = -9845.406822738372
Iteration 6400: Loss = -9845.406573141072
Iteration 6500: Loss = -9845.406294274066
Iteration 6600: Loss = -9845.406122102384
Iteration 6700: Loss = -9845.405954559217
Iteration 6800: Loss = -9845.405801279849
Iteration 6900: Loss = -9845.405674411526
Iteration 7000: Loss = -9845.405547695358
Iteration 7100: Loss = -9845.405481482601
Iteration 7200: Loss = -9845.405402966095
Iteration 7300: Loss = -9845.405396415163
Iteration 7400: Loss = -9845.405301889676
Iteration 7500: Loss = -9845.405369436565
Iteration 7600: Loss = -9845.405172371475
Iteration 7700: Loss = -9845.405186980204
Iteration 7800: Loss = -9845.405143178225
Iteration 7900: Loss = -9845.405125601808
Iteration 8000: Loss = -9845.44101105923
1
Iteration 8100: Loss = -9845.47379230866
2
Iteration 8200: Loss = -9845.405331361299
3
Iteration 8300: Loss = -9845.405084915832
Iteration 8400: Loss = -9845.424001484975
1
Iteration 8500: Loss = -9845.405039325578
Iteration 8600: Loss = -9845.445609920398
1
Iteration 8700: Loss = -9845.404980133791
Iteration 8800: Loss = -9845.404988955679
Iteration 8900: Loss = -9845.405336871543
1
Iteration 9000: Loss = -9845.405003540538
Iteration 9100: Loss = -9845.404987619058
Iteration 9200: Loss = -9845.404999187054
Iteration 9300: Loss = -9845.404952282444
Iteration 9400: Loss = -9845.404956202861
Iteration 9500: Loss = -9845.40584542825
1
Iteration 9600: Loss = -9845.404919235058
Iteration 9700: Loss = -9845.404924770597
Iteration 9800: Loss = -9845.40496437024
Iteration 9900: Loss = -9845.40490419561
Iteration 10000: Loss = -9845.404905198406
Iteration 10100: Loss = -9845.405089652213
1
Iteration 10200: Loss = -9845.40489782498
Iteration 10300: Loss = -9845.404885659478
Iteration 10400: Loss = -9845.907997030256
1
Iteration 10500: Loss = -9845.404895008802
Iteration 10600: Loss = -9845.404889962034
Iteration 10700: Loss = -9845.404895338048
Iteration 10800: Loss = -9845.405109242225
1
Iteration 10900: Loss = -9845.404885856999
Iteration 11000: Loss = -9845.404880320591
Iteration 11100: Loss = -9845.405732360658
1
Iteration 11200: Loss = -9845.404890423346
Iteration 11300: Loss = -9845.404895591442
Iteration 11400: Loss = -9845.40605029711
1
Iteration 11500: Loss = -9845.404889298405
Iteration 11600: Loss = -9845.404931195464
Iteration 11700: Loss = -9845.404878817488
Iteration 11800: Loss = -9845.405237267954
1
Iteration 11900: Loss = -9845.404870771117
Iteration 12000: Loss = -9845.404900784397
Iteration 12100: Loss = -9845.404915255082
Iteration 12200: Loss = -9845.404949770958
Iteration 12300: Loss = -9845.404858760681
Iteration 12400: Loss = -9845.404903681374
Iteration 12500: Loss = -9845.40529441293
1
Iteration 12600: Loss = -9845.40487634189
Iteration 12700: Loss = -9845.404859247668
Iteration 12800: Loss = -9845.405383365114
1
Iteration 12900: Loss = -9845.404864960869
Iteration 13000: Loss = -9845.404838662149
Iteration 13100: Loss = -9845.407835940146
1
Iteration 13200: Loss = -9845.404851467303
Iteration 13300: Loss = -9845.404865921715
Iteration 13400: Loss = -9845.498839317363
1
Iteration 13500: Loss = -9845.404864760045
Iteration 13600: Loss = -9845.404930885867
Iteration 13700: Loss = -9845.404854745382
Iteration 13800: Loss = -9845.417491838
1
Iteration 13900: Loss = -9845.404876764054
Iteration 14000: Loss = -9845.404841477686
Iteration 14100: Loss = -9845.464636087894
1
Iteration 14200: Loss = -9845.404852153953
Iteration 14300: Loss = -9845.404834239327
Iteration 14400: Loss = -9845.49175456427
1
Iteration 14500: Loss = -9845.404842476539
Iteration 14600: Loss = -9845.4048208208
Iteration 14700: Loss = -9845.40482693974
Iteration 14800: Loss = -9845.404851179726
Iteration 14900: Loss = -9845.40483855219
Iteration 15000: Loss = -9845.404829568435
Iteration 15100: Loss = -9845.414178971487
1
Iteration 15200: Loss = -9845.404800464037
Iteration 15300: Loss = -9845.404823350198
Iteration 15400: Loss = -9845.725703229253
1
Iteration 15500: Loss = -9845.404805992117
Iteration 15600: Loss = -9845.404835213458
Iteration 15700: Loss = -9845.404825733425
Iteration 15800: Loss = -9845.405488776141
1
Iteration 15900: Loss = -9845.404819279598
Iteration 16000: Loss = -9845.404829601008
Iteration 16100: Loss = -9845.57847890596
1
Iteration 16200: Loss = -9845.404819329036
Iteration 16300: Loss = -9845.404833312094
Iteration 16400: Loss = -9845.405061081821
1
Iteration 16500: Loss = -9845.40486696493
Iteration 16600: Loss = -9845.404825182366
Iteration 16700: Loss = -9845.428531045725
1
Iteration 16800: Loss = -9845.404823529174
Iteration 16900: Loss = -9845.404850896271
Iteration 17000: Loss = -9845.427479837013
1
Iteration 17100: Loss = -9845.40481770024
Iteration 17200: Loss = -9845.40483563442
Iteration 17300: Loss = -9845.404853878692
Iteration 17400: Loss = -9845.404831960246
Iteration 17500: Loss = -9845.40485104445
Iteration 17600: Loss = -9845.40481842249
Iteration 17700: Loss = -9845.404968374776
1
Iteration 17800: Loss = -9845.404831195028
Iteration 17900: Loss = -9845.404814161073
Iteration 18000: Loss = -9845.405114382558
1
Iteration 18100: Loss = -9845.404808280235
Iteration 18200: Loss = -9845.407534590317
1
Iteration 18300: Loss = -9845.404876874438
Iteration 18400: Loss = -9845.404823153172
Iteration 18500: Loss = -9845.414934739356
1
Iteration 18600: Loss = -9845.404852745572
Iteration 18700: Loss = -9845.404817654546
Iteration 18800: Loss = -9845.404825660113
Iteration 18900: Loss = -9845.404906443851
Iteration 19000: Loss = -9845.404824564977
Iteration 19100: Loss = -9845.404841801486
Iteration 19200: Loss = -9845.404890993144
Iteration 19300: Loss = -9845.404822540244
Iteration 19400: Loss = -9845.404968198945
1
Iteration 19500: Loss = -9845.404838477278
Iteration 19600: Loss = -9845.40484536253
Iteration 19700: Loss = -9845.404817974677
Iteration 19800: Loss = -9845.40566660359
1
Iteration 19900: Loss = -9845.404836127165
pi: tensor([[2.5066e-07, 1.0000e+00],
        [1.4304e-02, 9.8570e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0250, 0.9750], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1850, 0.0680],
         [0.6975, 0.1380]],

        [[0.5145, 0.1180],
         [0.5346, 0.6700]],

        [[0.5521, 0.1503],
         [0.5458, 0.7141]],

        [[0.5176, 0.0434],
         [0.5741, 0.7280]],

        [[0.5182, 0.0388],
         [0.5705, 0.6536]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: -0.0012622497307266862
Average Adjusted Rand Index: -0.0005348555226712332
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23461.623541893652
Iteration 100: Loss = -9851.04212189696
Iteration 200: Loss = -9848.439496787832
Iteration 300: Loss = -9846.629164346316
Iteration 400: Loss = -9842.844143328908
Iteration 500: Loss = -9842.46154380263
Iteration 600: Loss = -9842.319867034865
Iteration 700: Loss = -9842.255304748784
Iteration 800: Loss = -9842.219003660615
Iteration 900: Loss = -9842.192523298621
Iteration 1000: Loss = -9842.168968946095
Iteration 1100: Loss = -9842.145728898426
Iteration 1200: Loss = -9842.121367290316
Iteration 1300: Loss = -9842.09475042859
Iteration 1400: Loss = -9842.064684472323
Iteration 1500: Loss = -9842.029731423334
Iteration 1600: Loss = -9841.990312838017
Iteration 1700: Loss = -9841.951139127088
Iteration 1800: Loss = -9841.91832856086
Iteration 1900: Loss = -9841.893654483567
Iteration 2000: Loss = -9841.875650579279
Iteration 2100: Loss = -9841.862279307976
Iteration 2200: Loss = -9841.85219056729
Iteration 2300: Loss = -9841.844336397218
Iteration 2400: Loss = -9841.838118369837
Iteration 2500: Loss = -9841.83310566569
Iteration 2600: Loss = -9841.829002991068
Iteration 2700: Loss = -9841.825562919494
Iteration 2800: Loss = -9841.822663932096
Iteration 2900: Loss = -9841.82018920645
Iteration 3000: Loss = -9841.818078056309
Iteration 3100: Loss = -9841.816231052486
Iteration 3200: Loss = -9841.814623319102
Iteration 3300: Loss = -9841.813213641628
Iteration 3400: Loss = -9841.811940046748
Iteration 3500: Loss = -9841.810816826506
Iteration 3600: Loss = -9841.809851524164
Iteration 3700: Loss = -9841.808975058831
Iteration 3800: Loss = -9841.80815276574
Iteration 3900: Loss = -9841.807434145221
Iteration 4000: Loss = -9841.80675030054
Iteration 4100: Loss = -9841.806147144522
Iteration 4200: Loss = -9841.805603387544
Iteration 4300: Loss = -9841.805106918064
Iteration 4400: Loss = -9841.804634343463
Iteration 4500: Loss = -9841.80419712618
Iteration 4600: Loss = -9841.803798638657
Iteration 4700: Loss = -9841.803413108366
Iteration 4800: Loss = -9841.803096494783
Iteration 4900: Loss = -9841.802758658136
Iteration 5000: Loss = -9841.802466322059
Iteration 5100: Loss = -9841.802205097587
Iteration 5200: Loss = -9841.801957698137
Iteration 5300: Loss = -9841.801705833386
Iteration 5400: Loss = -9841.801491184646
Iteration 5500: Loss = -9841.801284980354
Iteration 5600: Loss = -9841.80110113302
Iteration 5700: Loss = -9841.80142106825
1
Iteration 5800: Loss = -9841.800765403837
Iteration 5900: Loss = -9841.800615112687
Iteration 6000: Loss = -9841.80046193894
Iteration 6100: Loss = -9841.800310834153
Iteration 6200: Loss = -9841.80024778177
Iteration 6300: Loss = -9841.800080539811
Iteration 6400: Loss = -9841.799971121618
Iteration 6500: Loss = -9841.799872800837
Iteration 6600: Loss = -9841.799793336877
Iteration 6700: Loss = -9841.805316682192
1
Iteration 6800: Loss = -9841.799589356864
Iteration 6900: Loss = -9841.79949477664
Iteration 7000: Loss = -9841.799481527687
Iteration 7100: Loss = -9841.799337031644
Iteration 7200: Loss = -9841.799345771191
Iteration 7300: Loss = -9841.799236998233
Iteration 7400: Loss = -9841.799208258231
Iteration 7500: Loss = -9841.799106858221
Iteration 7600: Loss = -9841.799064984483
Iteration 7700: Loss = -9841.799024956423
Iteration 7800: Loss = -9841.798932544107
Iteration 7900: Loss = -9841.79933719728
1
Iteration 8000: Loss = -9841.798859333257
Iteration 8100: Loss = -9841.79922087877
1
Iteration 8200: Loss = -9841.798755154861
Iteration 8300: Loss = -9841.798734755212
Iteration 8400: Loss = -9841.799387745652
1
Iteration 8500: Loss = -9841.811881705886
2
Iteration 8600: Loss = -9841.798628966648
Iteration 8700: Loss = -9841.79860144231
Iteration 8800: Loss = -9841.79858187121
Iteration 8900: Loss = -9841.799105598582
1
Iteration 9000: Loss = -9841.79850384379
Iteration 9100: Loss = -9841.80021653516
1
Iteration 9200: Loss = -9841.79844793453
Iteration 9300: Loss = -9841.798437593738
Iteration 9400: Loss = -9841.79854654705
1
Iteration 9500: Loss = -9841.798409435
Iteration 9600: Loss = -9841.798376445444
Iteration 9700: Loss = -9841.799548481444
1
Iteration 9800: Loss = -9841.798334805959
Iteration 9900: Loss = -9841.798309448232
Iteration 10000: Loss = -9841.798573434204
1
Iteration 10100: Loss = -9841.798306887787
Iteration 10200: Loss = -9841.798259669398
Iteration 10300: Loss = -9841.81699009758
1
Iteration 10400: Loss = -9841.798262661388
Iteration 10500: Loss = -9841.798225976496
Iteration 10600: Loss = -9841.803580016775
1
Iteration 10700: Loss = -9841.798232588262
Iteration 10800: Loss = -9841.798204961182
Iteration 10900: Loss = -9841.801494731923
1
Iteration 11000: Loss = -9841.79820334815
Iteration 11100: Loss = -9841.798183015844
Iteration 11200: Loss = -9841.806782930764
1
Iteration 11300: Loss = -9841.798174407815
Iteration 11400: Loss = -9841.798165264901
Iteration 11500: Loss = -9841.966216763589
1
Iteration 11600: Loss = -9841.798163468164
Iteration 11700: Loss = -9841.798130897036
Iteration 11800: Loss = -9841.800369969074
1
Iteration 11900: Loss = -9841.79810115955
Iteration 12000: Loss = -9841.798126650134
Iteration 12100: Loss = -9841.798541639386
1
Iteration 12200: Loss = -9841.798110761632
Iteration 12300: Loss = -9841.800759798132
1
Iteration 12400: Loss = -9841.79815255054
Iteration 12500: Loss = -9841.800106677023
1
Iteration 12600: Loss = -9841.798145631186
Iteration 12700: Loss = -9841.798139938854
Iteration 12800: Loss = -9841.798101713317
Iteration 12900: Loss = -9841.798095551878
Iteration 13000: Loss = -9841.798051027527
Iteration 13100: Loss = -9841.798727932692
1
Iteration 13200: Loss = -9841.798055210174
Iteration 13300: Loss = -9841.798720890854
1
Iteration 13400: Loss = -9841.800579234292
2
Iteration 13500: Loss = -9841.800695994278
3
Iteration 13600: Loss = -9841.798102253537
Iteration 13700: Loss = -9841.949530634405
1
Iteration 13800: Loss = -9841.798084452867
Iteration 13900: Loss = -9841.798342625449
1
Iteration 14000: Loss = -9841.798061714777
Iteration 14100: Loss = -9841.798147235713
Iteration 14200: Loss = -9841.803330187075
1
Iteration 14300: Loss = -9841.800457964018
2
Iteration 14400: Loss = -9841.798769427262
3
Iteration 14500: Loss = -9841.79812210071
Iteration 14600: Loss = -9841.798237448997
1
Iteration 14700: Loss = -9841.798125104704
Iteration 14800: Loss = -9841.798163223682
Iteration 14900: Loss = -9841.798610731154
1
Iteration 15000: Loss = -9841.799371131448
2
Iteration 15100: Loss = -9841.798925473759
3
Iteration 15200: Loss = -9841.81073440809
4
Iteration 15300: Loss = -9841.79805087348
Iteration 15400: Loss = -9841.79873219499
1
Iteration 15500: Loss = -9841.798047503851
Iteration 15600: Loss = -9841.808362882599
1
Iteration 15700: Loss = -9841.79803119898
Iteration 15800: Loss = -9841.798052978429
Iteration 15900: Loss = -9841.891740235573
1
Iteration 16000: Loss = -9841.798025846672
Iteration 16100: Loss = -9841.798020824706
Iteration 16200: Loss = -9841.821321107145
1
Iteration 16300: Loss = -9841.798036202166
Iteration 16400: Loss = -9841.798147605354
1
Iteration 16500: Loss = -9841.799157709425
2
Iteration 16600: Loss = -9841.798039508942
Iteration 16700: Loss = -9841.79814950489
1
Iteration 16800: Loss = -9841.809526625088
2
Iteration 16900: Loss = -9841.798011917106
Iteration 17000: Loss = -9841.804655394384
1
Iteration 17100: Loss = -9841.798042074262
Iteration 17200: Loss = -9841.800981733568
1
Iteration 17300: Loss = -9841.798005168777
Iteration 17400: Loss = -9841.829775925158
1
Iteration 17500: Loss = -9841.798030615008
Iteration 17600: Loss = -9841.798054872655
Iteration 17700: Loss = -9841.798612802366
1
Iteration 17800: Loss = -9841.798019710117
Iteration 17900: Loss = -9841.798375418946
1
Iteration 18000: Loss = -9841.798049707255
Iteration 18100: Loss = -9841.811091170153
1
Iteration 18200: Loss = -9841.800730720159
2
Iteration 18300: Loss = -9841.798045773983
Iteration 18400: Loss = -9841.79811153167
Iteration 18500: Loss = -9841.800076090303
1
Iteration 18600: Loss = -9841.79990975555
2
Iteration 18700: Loss = -9841.7980365774
Iteration 18800: Loss = -9841.805636697865
1
Iteration 18900: Loss = -9841.7994965437
2
Iteration 19000: Loss = -9841.798475898659
3
Iteration 19100: Loss = -9841.803533767948
4
Iteration 19200: Loss = -9841.79803382897
Iteration 19300: Loss = -9841.79851175439
1
Iteration 19400: Loss = -9841.798158141175
2
Iteration 19500: Loss = -9841.79805993294
Iteration 19600: Loss = -9841.832993525344
1
Iteration 19700: Loss = -9841.799025345486
2
Iteration 19800: Loss = -9841.798058228556
Iteration 19900: Loss = -9841.799168610532
1
pi: tensor([[1.0000e+00, 4.6441e-07],
        [2.1326e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1049, 0.8951], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1065, 0.0989],
         [0.5719, 0.1429]],

        [[0.6512, 0.1343],
         [0.5500, 0.7143]],

        [[0.5642, 0.1061],
         [0.5842, 0.6566]],

        [[0.6203, 0.0822],
         [0.6487, 0.5951]],

        [[0.5310, 0.1221],
         [0.6363, 0.5619]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.007246281224304942
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.004141498517340605
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.020455891982911846
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.008732110438009916
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.01999651827394162
Global Adjusted Rand Index: 0.002115715801741338
Average Adjusted Rand Index: -0.0006230086025211711
9989.461563909359
[-0.0012622497307266862, 0.002115715801741338] [-0.0005348555226712332, -0.0006230086025211711] [9845.404813904473, 9841.80276825379]
-------------------------------------
This iteration is 49
True Objective function: Loss = -9914.974423412514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22307.153958764062
Iteration 100: Loss = -9789.782447914542
Iteration 200: Loss = -9788.4960113888
Iteration 300: Loss = -9787.98572149456
Iteration 400: Loss = -9787.74503121623
Iteration 500: Loss = -9787.572220332258
Iteration 600: Loss = -9787.455112080652
Iteration 700: Loss = -9787.408771830856
Iteration 800: Loss = -9787.378672781195
Iteration 900: Loss = -9787.354824826309
Iteration 1000: Loss = -9787.334573171287
Iteration 1100: Loss = -9787.31631334124
Iteration 1200: Loss = -9787.299156646624
Iteration 1300: Loss = -9787.282626015813
Iteration 1400: Loss = -9787.266110124512
Iteration 1500: Loss = -9787.248514343066
Iteration 1600: Loss = -9787.227483502986
Iteration 1700: Loss = -9787.190354087807
Iteration 1800: Loss = -9785.685359334477
Iteration 1900: Loss = -9784.820898120513
Iteration 2000: Loss = -9784.448352701223
Iteration 2100: Loss = -9784.216098397897
Iteration 2200: Loss = -9784.058356561336
Iteration 2300: Loss = -9783.954305778896
Iteration 2400: Loss = -9783.879445293895
Iteration 2500: Loss = -9783.823589775398
Iteration 2600: Loss = -9783.780770427053
Iteration 2700: Loss = -9783.747255219234
Iteration 2800: Loss = -9783.720490169604
Iteration 2900: Loss = -9783.698799474894
Iteration 3000: Loss = -9783.680868168642
Iteration 3100: Loss = -9783.66591243928
Iteration 3200: Loss = -9783.653375083806
Iteration 3300: Loss = -9783.64265363098
Iteration 3400: Loss = -9783.633406320214
Iteration 3500: Loss = -9783.625454301282
Iteration 3600: Loss = -9783.618519009724
Iteration 3700: Loss = -9783.612391920136
Iteration 3800: Loss = -9783.606995996604
Iteration 3900: Loss = -9783.602221516516
Iteration 4000: Loss = -9783.59793892043
Iteration 4100: Loss = -9783.594149778091
Iteration 4200: Loss = -9783.590712173254
Iteration 4300: Loss = -9783.587604561433
Iteration 4400: Loss = -9783.584793121432
Iteration 4500: Loss = -9783.582254961755
Iteration 4600: Loss = -9783.579953494876
Iteration 4700: Loss = -9783.577793274211
Iteration 4800: Loss = -9783.575846844777
Iteration 4900: Loss = -9783.574068786847
Iteration 5000: Loss = -9783.572423451242
Iteration 5100: Loss = -9783.570903282382
Iteration 5200: Loss = -9783.569468169952
Iteration 5300: Loss = -9783.568205557569
Iteration 5400: Loss = -9783.566954162157
Iteration 5500: Loss = -9783.565828263
Iteration 5600: Loss = -9783.564768719521
Iteration 5700: Loss = -9783.563785876286
Iteration 5800: Loss = -9783.562884818879
Iteration 5900: Loss = -9783.562005546863
Iteration 6000: Loss = -9783.561245734389
Iteration 6100: Loss = -9783.560456429625
Iteration 6200: Loss = -9783.559735614477
Iteration 6300: Loss = -9783.559232295238
Iteration 6400: Loss = -9783.55834807418
Iteration 6500: Loss = -9783.557674101334
Iteration 6600: Loss = -9783.556962177048
Iteration 6700: Loss = -9783.556240383652
Iteration 6800: Loss = -9783.55559445764
Iteration 6900: Loss = -9783.555046353345
Iteration 7000: Loss = -9783.554582033648
Iteration 7100: Loss = -9783.55413132897
Iteration 7200: Loss = -9783.55372715592
Iteration 7300: Loss = -9783.553309946079
Iteration 7400: Loss = -9783.5529429574
Iteration 7500: Loss = -9783.552595887582
Iteration 7600: Loss = -9783.552268643403
Iteration 7700: Loss = -9783.551949211495
Iteration 7800: Loss = -9783.551612011597
Iteration 7900: Loss = -9783.551259013082
Iteration 8000: Loss = -9783.550820650018
Iteration 8100: Loss = -9783.550069148749
Iteration 8200: Loss = -9783.549536460543
Iteration 8300: Loss = -9783.549429688359
Iteration 8400: Loss = -9783.549142172482
Iteration 8500: Loss = -9783.54886985327
Iteration 8600: Loss = -9783.54915806277
1
Iteration 8700: Loss = -9783.548539796777
Iteration 8800: Loss = -9783.556652737998
1
Iteration 8900: Loss = -9783.54824085078
Iteration 9000: Loss = -9783.548101867753
Iteration 9100: Loss = -9783.565112810466
1
Iteration 9200: Loss = -9783.547902676197
Iteration 9300: Loss = -9783.547720051745
Iteration 9400: Loss = -9783.547593917896
Iteration 9500: Loss = -9783.547506456815
Iteration 9600: Loss = -9783.547390176616
Iteration 9700: Loss = -9783.547304421665
Iteration 9800: Loss = -9783.547206944586
Iteration 9900: Loss = -9783.547165141443
Iteration 10000: Loss = -9783.5470281023
Iteration 10100: Loss = -9783.546957350516
Iteration 10200: Loss = -9783.546870504151
Iteration 10300: Loss = -9783.546833960043
Iteration 10400: Loss = -9783.546756652104
Iteration 10500: Loss = -9783.546693412613
Iteration 10600: Loss = -9783.546682198154
Iteration 10700: Loss = -9783.546680516316
Iteration 10800: Loss = -9783.546542410553
Iteration 10900: Loss = -9783.546496293302
Iteration 11000: Loss = -9783.79882814256
1
Iteration 11100: Loss = -9783.546407634793
Iteration 11200: Loss = -9783.546318396966
Iteration 11300: Loss = -9783.546275871991
Iteration 11400: Loss = -9783.920612710543
1
Iteration 11500: Loss = -9783.546126881985
Iteration 11600: Loss = -9783.546034910694
Iteration 11700: Loss = -9783.54594911205
Iteration 11800: Loss = -9783.553582485261
1
Iteration 11900: Loss = -9783.545845746135
Iteration 12000: Loss = -9783.545794717642
Iteration 12100: Loss = -9783.545829555324
Iteration 12200: Loss = -9783.546106570253
1
Iteration 12300: Loss = -9783.545747104103
Iteration 12400: Loss = -9783.545726820015
Iteration 12500: Loss = -9783.54569331078
Iteration 12600: Loss = -9783.545722562589
Iteration 12700: Loss = -9783.545647789095
Iteration 12800: Loss = -9783.545622004911
Iteration 12900: Loss = -9783.545631313229
Iteration 13000: Loss = -9783.546879103003
1
Iteration 13100: Loss = -9783.545534177138
Iteration 13200: Loss = -9783.545353706011
Iteration 13300: Loss = -9783.544461414484
Iteration 13400: Loss = -9783.54907857298
1
Iteration 13500: Loss = -9783.544379810737
Iteration 13600: Loss = -9783.544355503085
Iteration 13700: Loss = -9783.544334846745
Iteration 13800: Loss = -9783.544451949378
1
Iteration 13900: Loss = -9783.544328048329
Iteration 14000: Loss = -9783.544311841839
Iteration 14100: Loss = -9783.700249844895
1
Iteration 14200: Loss = -9783.544344280133
Iteration 14300: Loss = -9783.544303519293
Iteration 14400: Loss = -9783.54430291379
Iteration 14500: Loss = -9783.546767617227
1
Iteration 14600: Loss = -9783.544346325638
Iteration 14700: Loss = -9783.544295927151
Iteration 14800: Loss = -9783.544326067078
Iteration 14900: Loss = -9783.700358261589
1
Iteration 15000: Loss = -9783.544345858221
Iteration 15100: Loss = -9783.544321258976
Iteration 15200: Loss = -9783.544297861921
Iteration 15300: Loss = -9783.614968089258
1
Iteration 15400: Loss = -9783.544303255168
Iteration 15500: Loss = -9783.544291274407
Iteration 15600: Loss = -9783.860737728035
1
Iteration 15700: Loss = -9783.54428405595
Iteration 15800: Loss = -9783.544274776992
Iteration 15900: Loss = -9783.544565239155
1
Iteration 16000: Loss = -9783.544272192623
Iteration 16100: Loss = -9783.54427940014
Iteration 16200: Loss = -9783.545570228622
1
Iteration 16300: Loss = -9783.544284256566
Iteration 16400: Loss = -9783.544237239264
Iteration 16500: Loss = -9783.544208642057
Iteration 16600: Loss = -9783.544091908827
Iteration 16700: Loss = -9783.544228594692
1
Iteration 16800: Loss = -9783.544104635395
Iteration 16900: Loss = -9783.544764018648
1
Iteration 17000: Loss = -9783.544080452213
Iteration 17100: Loss = -9783.544035769777
Iteration 17200: Loss = -9783.544273201001
1
Iteration 17300: Loss = -9783.544055617931
Iteration 17400: Loss = -9783.622619328225
1
Iteration 17500: Loss = -9783.544091793468
Iteration 17600: Loss = -9783.544045572375
Iteration 17700: Loss = -9783.544033567268
Iteration 17800: Loss = -9783.54469619419
1
Iteration 17900: Loss = -9783.544039134307
Iteration 18000: Loss = -9783.544003203793
Iteration 18100: Loss = -9783.550926452594
1
Iteration 18200: Loss = -9783.544005582153
Iteration 18300: Loss = -9783.544011292312
Iteration 18400: Loss = -9783.544008198882
Iteration 18500: Loss = -9783.54394882773
Iteration 18600: Loss = -9783.543894712082
Iteration 18700: Loss = -9783.543944144156
Iteration 18800: Loss = -9783.543882768947
Iteration 18900: Loss = -9783.544576360433
1
Iteration 19000: Loss = -9783.543884507922
Iteration 19100: Loss = -9783.54493108715
1
Iteration 19200: Loss = -9783.543875030658
Iteration 19300: Loss = -9783.578957190424
1
Iteration 19400: Loss = -9783.543857422008
Iteration 19500: Loss = -9783.54396108227
1
Iteration 19600: Loss = -9783.543876080921
Iteration 19700: Loss = -9783.641513431548
1
Iteration 19800: Loss = -9783.543880375168
Iteration 19900: Loss = -9783.54398403648
1
pi: tensor([[1.0000e+00, 9.0581e-07],
        [2.0818e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0106, 0.9894], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2700, 0.2499],
         [0.6918, 0.1338]],

        [[0.5109, 0.1014],
         [0.6264, 0.5655]],

        [[0.6303, 0.2089],
         [0.7071, 0.6560]],

        [[0.5465, 0.1984],
         [0.7169, 0.6054]],

        [[0.5739, 0.1599],
         [0.5909, 0.6644]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.009987515605493134
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.0155095116927178
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
Global Adjusted Rand Index: 0.0049617437634031656
Average Adjusted Rand Index: 0.005752595214864413
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20285.208499108823
Iteration 100: Loss = -9789.399988206254
Iteration 200: Loss = -9788.440522865967
Iteration 300: Loss = -9788.066345267305
Iteration 400: Loss = -9787.902811007612
Iteration 500: Loss = -9787.809671910054
Iteration 600: Loss = -9787.73888699225
Iteration 700: Loss = -9787.666296482566
Iteration 800: Loss = -9787.556220184544
Iteration 900: Loss = -9787.423362573978
Iteration 1000: Loss = -9787.365341651028
Iteration 1100: Loss = -9787.321066229671
Iteration 1200: Loss = -9787.280843274224
Iteration 1300: Loss = -9787.23987635061
Iteration 1400: Loss = -9787.188690514715
Iteration 1500: Loss = -9787.018170266922
Iteration 1600: Loss = -9785.486176842314
Iteration 1700: Loss = -9784.856668841103
Iteration 1800: Loss = -9784.540353809583
Iteration 1900: Loss = -9784.348782805919
Iteration 2000: Loss = -9784.214223053095
Iteration 2100: Loss = -9784.11126762815
Iteration 2200: Loss = -9784.02627501795
Iteration 2300: Loss = -9783.953369456962
Iteration 2400: Loss = -9783.891189903436
Iteration 2500: Loss = -9783.839341623208
Iteration 2600: Loss = -9783.79676433229
Iteration 2700: Loss = -9783.761914953458
Iteration 2800: Loss = -9783.733322636177
Iteration 2900: Loss = -9783.709723454798
Iteration 3000: Loss = -9783.690088789772
Iteration 3100: Loss = -9783.673607086075
Iteration 3200: Loss = -9783.659633301533
Iteration 3300: Loss = -9783.647723158841
Iteration 3400: Loss = -9783.637489814215
Iteration 3500: Loss = -9783.62861643556
Iteration 3600: Loss = -9783.620898064659
Iteration 3700: Loss = -9783.614110679773
Iteration 3800: Loss = -9783.608120358545
Iteration 3900: Loss = -9783.602767198749
Iteration 4000: Loss = -9783.597980192491
Iteration 4100: Loss = -9783.59375014513
Iteration 4200: Loss = -9783.589908622567
Iteration 4300: Loss = -9783.586412347839
Iteration 4400: Loss = -9783.583255599962
Iteration 4500: Loss = -9783.580370307129
Iteration 4600: Loss = -9783.577737445705
Iteration 4700: Loss = -9783.575357252575
Iteration 4800: Loss = -9783.573165188862
Iteration 4900: Loss = -9783.571283295978
Iteration 5000: Loss = -9783.569430954753
Iteration 5100: Loss = -9783.567735787596
Iteration 5200: Loss = -9783.566236753537
Iteration 5300: Loss = -9783.56485770114
Iteration 5400: Loss = -9783.563577196699
Iteration 5500: Loss = -9783.562399819306
Iteration 5600: Loss = -9783.561909040893
Iteration 5700: Loss = -9783.56023920178
Iteration 5800: Loss = -9783.559293255736
Iteration 5900: Loss = -9783.558417092514
Iteration 6000: Loss = -9783.557560570998
Iteration 6100: Loss = -9783.556813588799
Iteration 6200: Loss = -9783.556083651103
Iteration 6300: Loss = -9783.555419037739
Iteration 6400: Loss = -9783.554784018237
Iteration 6500: Loss = -9783.554185250716
Iteration 6600: Loss = -9783.553670617928
Iteration 6700: Loss = -9783.55313644194
Iteration 6800: Loss = -9783.55264658485
Iteration 6900: Loss = -9783.552352203948
Iteration 7000: Loss = -9783.551748187496
Iteration 7100: Loss = -9783.551327466756
Iteration 7200: Loss = -9783.551065162394
Iteration 7300: Loss = -9783.550616924795
Iteration 7400: Loss = -9783.550250795333
Iteration 7500: Loss = -9783.549929841407
Iteration 7600: Loss = -9783.549641139682
Iteration 7700: Loss = -9783.54937250382
Iteration 7800: Loss = -9783.549094848275
Iteration 7900: Loss = -9783.548847474338
Iteration 8000: Loss = -9783.642349453448
1
Iteration 8100: Loss = -9783.548363083859
Iteration 8200: Loss = -9783.54812291262
Iteration 8300: Loss = -9783.606484990109
1
Iteration 8400: Loss = -9783.547726145314
Iteration 8500: Loss = -9783.547558779701
Iteration 8600: Loss = -9783.57309500912
1
Iteration 8700: Loss = -9783.547245281743
Iteration 8800: Loss = -9783.54704712648
Iteration 8900: Loss = -9783.549428922706
1
Iteration 9000: Loss = -9783.546733502839
Iteration 9100: Loss = -9783.54669297657
Iteration 9200: Loss = -9783.546515110344
Iteration 9300: Loss = -9783.546415090299
Iteration 9400: Loss = -9783.546413624796
Iteration 9500: Loss = -9783.546173309995
Iteration 9600: Loss = -9783.546070348923
Iteration 9700: Loss = -9783.54615700543
Iteration 9800: Loss = -9783.545872363105
Iteration 9900: Loss = -9783.545818336666
Iteration 10000: Loss = -9783.546131215047
1
Iteration 10100: Loss = -9783.545654019637
Iteration 10200: Loss = -9783.545571828206
Iteration 10300: Loss = -9783.578942825892
1
Iteration 10400: Loss = -9783.545429227664
Iteration 10500: Loss = -9783.545353115725
Iteration 10600: Loss = -9783.545270651533
Iteration 10700: Loss = -9783.545250843536
Iteration 10800: Loss = -9783.545175366708
Iteration 10900: Loss = -9783.545081221078
Iteration 11000: Loss = -9784.086255442491
1
Iteration 11100: Loss = -9783.545012719458
Iteration 11200: Loss = -9783.544916399374
Iteration 11300: Loss = -9783.544858454872
Iteration 11400: Loss = -9783.548414125591
1
Iteration 11500: Loss = -9783.544767137953
Iteration 11600: Loss = -9783.544709909953
Iteration 11700: Loss = -9783.544664418569
Iteration 11800: Loss = -9783.54968221308
1
Iteration 11900: Loss = -9783.544437907321
Iteration 12000: Loss = -9783.544418340578
Iteration 12100: Loss = -9783.544401967549
Iteration 12200: Loss = -9783.548295783488
1
Iteration 12300: Loss = -9783.544303404331
Iteration 12400: Loss = -9783.544287296481
Iteration 12500: Loss = -9783.544261925364
Iteration 12600: Loss = -9783.544612138126
1
Iteration 12700: Loss = -9783.544198165671
Iteration 12800: Loss = -9783.544179957715
Iteration 12900: Loss = -9783.566641174835
1
Iteration 13000: Loss = -9783.54414810734
Iteration 13100: Loss = -9783.54411126035
Iteration 13200: Loss = -9783.544080775308
Iteration 13300: Loss = -9783.547453444442
1
Iteration 13400: Loss = -9783.544056365818
Iteration 13500: Loss = -9783.544029621642
Iteration 13600: Loss = -9784.153930738794
1
Iteration 13700: Loss = -9783.544005069216
Iteration 13800: Loss = -9783.54395579859
Iteration 13900: Loss = -9783.54400472867
Iteration 14000: Loss = -9783.851627298089
1
Iteration 14100: Loss = -9783.543958951053
Iteration 14200: Loss = -9783.54392388054
Iteration 14300: Loss = -9783.543919972946
Iteration 14400: Loss = -9783.544918234158
1
Iteration 14500: Loss = -9783.544459967412
2
Iteration 14600: Loss = -9783.543937560635
Iteration 14700: Loss = -9783.54503025891
1
Iteration 14800: Loss = -9783.543904589105
Iteration 14900: Loss = -9783.543855170074
Iteration 15000: Loss = -9783.54421474606
1
Iteration 15100: Loss = -9783.543863086847
Iteration 15200: Loss = -9783.543835582015
Iteration 15300: Loss = -9783.54526231849
1
Iteration 15400: Loss = -9783.543798977802
Iteration 15500: Loss = -9783.543804695384
Iteration 15600: Loss = -9783.565400823369
1
Iteration 15700: Loss = -9783.54380787053
Iteration 15800: Loss = -9783.543831066114
Iteration 15900: Loss = -9783.545133534237
1
Iteration 16000: Loss = -9783.56718696678
2
Iteration 16100: Loss = -9783.543783141347
Iteration 16200: Loss = -9783.54424364021
1
Iteration 16300: Loss = -9783.543819360391
Iteration 16400: Loss = -9783.543761095141
Iteration 16500: Loss = -9783.558723327558
1
Iteration 16600: Loss = -9783.543769871118
Iteration 16700: Loss = -9783.868831725447
1
Iteration 16800: Loss = -9783.54376889763
Iteration 16900: Loss = -9783.543768568765
Iteration 17000: Loss = -9783.545466547395
1
Iteration 17100: Loss = -9783.543765806633
Iteration 17200: Loss = -9783.544055659377
1
Iteration 17300: Loss = -9783.54376219812
Iteration 17400: Loss = -9783.543715775395
Iteration 17500: Loss = -9783.696824545215
1
Iteration 17600: Loss = -9783.543683273805
Iteration 17700: Loss = -9783.543671068022
Iteration 17800: Loss = -9783.562143870682
1
Iteration 17900: Loss = -9783.543698991329
Iteration 18000: Loss = -9783.545532903332
1
Iteration 18100: Loss = -9783.543706853225
Iteration 18200: Loss = -9783.548707935748
1
Iteration 18300: Loss = -9783.543676428091
Iteration 18400: Loss = -9783.544913775808
1
Iteration 18500: Loss = -9783.547658512793
2
Iteration 18600: Loss = -9783.543680876834
Iteration 18700: Loss = -9783.545597404896
1
Iteration 18800: Loss = -9783.543944423984
2
Iteration 18900: Loss = -9783.54379853437
3
Iteration 19000: Loss = -9783.655162296325
4
Iteration 19100: Loss = -9783.54366463911
Iteration 19200: Loss = -9783.6074856772
1
Iteration 19300: Loss = -9783.543657905697
Iteration 19400: Loss = -9783.545439990392
1
Iteration 19500: Loss = -9783.543649315352
Iteration 19600: Loss = -9783.543597145515
Iteration 19700: Loss = -9783.549083397511
1
Iteration 19800: Loss = -9783.543592446315
Iteration 19900: Loss = -9783.543610290857
pi: tensor([[1.0000e+00, 1.0655e-06],
        [2.1987e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0106, 0.9894], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2695, 0.2499],
         [0.5993, 0.1338]],

        [[0.6842, 0.1013],
         [0.5703, 0.5931]],

        [[0.6185, 0.2089],
         [0.6367, 0.6845]],

        [[0.5758, 0.1985],
         [0.5655, 0.7110]],

        [[0.6262, 0.1601],
         [0.5309, 0.6147]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.009987515605493134
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.0155095116927178
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
Global Adjusted Rand Index: 0.0049617437634031656
Average Adjusted Rand Index: 0.005752595214864413
9914.974423412514
[0.0049617437634031656, 0.0049617437634031656] [0.005752595214864413, 0.005752595214864413] [9783.543923410349, 9783.543663968416]
-------------------------------------
This iteration is 50
True Objective function: Loss = -9912.056454662514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24780.986519603837
Iteration 100: Loss = -9794.437902140573
Iteration 200: Loss = -9793.39359015106
Iteration 300: Loss = -9792.995418673176
Iteration 400: Loss = -9792.365396856225
Iteration 500: Loss = -9791.697304216521
Iteration 600: Loss = -9791.328963283107
Iteration 700: Loss = -9791.055681856646
Iteration 800: Loss = -9790.839562995052
Iteration 900: Loss = -9790.664299503325
Iteration 1000: Loss = -9790.535703532369
Iteration 1100: Loss = -9790.456629666884
Iteration 1200: Loss = -9790.401635419852
Iteration 1300: Loss = -9790.365178793067
Iteration 1400: Loss = -9790.342577231668
Iteration 1500: Loss = -9790.325693849194
Iteration 1600: Loss = -9790.312798062896
Iteration 1700: Loss = -9790.302291686063
Iteration 1800: Loss = -9790.293181713587
Iteration 1900: Loss = -9790.285278562957
Iteration 2000: Loss = -9790.278183247647
Iteration 2100: Loss = -9790.271785445117
Iteration 2200: Loss = -9790.26587430965
Iteration 2300: Loss = -9790.260350073677
Iteration 2400: Loss = -9790.255101132176
Iteration 2500: Loss = -9790.24996891727
Iteration 2600: Loss = -9790.244739024847
Iteration 2700: Loss = -9790.239156033816
Iteration 2800: Loss = -9790.232754515488
Iteration 2900: Loss = -9790.225008754256
Iteration 3000: Loss = -9790.21468614276
Iteration 3100: Loss = -9790.19934211943
Iteration 3200: Loss = -9790.173729185164
Iteration 3300: Loss = -9790.124999298134
Iteration 3400: Loss = -9790.024633689982
Iteration 3500: Loss = -9789.908756330617
Iteration 3600: Loss = -9789.846818113303
Iteration 3700: Loss = -9789.817435648803
Iteration 3800: Loss = -9789.803449141493
Iteration 3900: Loss = -9789.795393681547
Iteration 4000: Loss = -9789.790146283189
Iteration 4100: Loss = -9789.786413955926
Iteration 4200: Loss = -9789.783674006154
Iteration 4300: Loss = -9789.781485009236
Iteration 4400: Loss = -9789.779635159493
Iteration 4500: Loss = -9789.778130881772
Iteration 4600: Loss = -9789.776863074907
Iteration 4700: Loss = -9789.775745291725
Iteration 4800: Loss = -9789.774770571388
Iteration 4900: Loss = -9789.773879197488
Iteration 5000: Loss = -9789.7731217868
Iteration 5100: Loss = -9789.772441055273
Iteration 5200: Loss = -9789.771812583254
Iteration 5300: Loss = -9789.771269264244
Iteration 5400: Loss = -9789.770811059194
Iteration 5500: Loss = -9789.770345792287
Iteration 5600: Loss = -9789.769920954002
Iteration 5700: Loss = -9789.769558201564
Iteration 5800: Loss = -9789.769193598211
Iteration 5900: Loss = -9789.768874328189
Iteration 6000: Loss = -9789.768528572795
Iteration 6100: Loss = -9789.768262845851
Iteration 6200: Loss = -9789.768039443805
Iteration 6300: Loss = -9789.767801701899
Iteration 6400: Loss = -9789.767594546187
Iteration 6500: Loss = -9789.767384660328
Iteration 6600: Loss = -9789.767174393286
Iteration 6700: Loss = -9789.767004766261
Iteration 6800: Loss = -9789.766823787282
Iteration 6900: Loss = -9789.7666813467
Iteration 7000: Loss = -9789.766533207316
Iteration 7100: Loss = -9789.766380952567
Iteration 7200: Loss = -9789.766251334295
Iteration 7300: Loss = -9789.76641793163
1
Iteration 7400: Loss = -9789.7660118946
Iteration 7500: Loss = -9789.766663889879
1
Iteration 7600: Loss = -9789.765814571583
Iteration 7700: Loss = -9789.765791567406
Iteration 7800: Loss = -9789.765625791817
Iteration 7900: Loss = -9789.76613966911
1
Iteration 8000: Loss = -9789.765469393888
Iteration 8100: Loss = -9789.765457614403
Iteration 8200: Loss = -9789.76530423455
Iteration 8300: Loss = -9789.765385311717
Iteration 8400: Loss = -9789.765198001665
Iteration 8500: Loss = -9789.765162964894
Iteration 8600: Loss = -9789.765070732865
Iteration 8700: Loss = -9789.765001417627
Iteration 8800: Loss = -9789.764996558737
Iteration 8900: Loss = -9789.764898900008
Iteration 9000: Loss = -9789.767859739208
1
Iteration 9100: Loss = -9789.764837399114
Iteration 9200: Loss = -9789.764879683433
Iteration 9300: Loss = -9789.76475735083
Iteration 9400: Loss = -9789.764762471763
Iteration 9500: Loss = -9789.764661627125
Iteration 9600: Loss = -9789.764674346603
Iteration 9700: Loss = -9789.764654743292
Iteration 9800: Loss = -9789.764576507172
Iteration 9900: Loss = -9789.764548462272
Iteration 10000: Loss = -9789.764665924878
1
Iteration 10100: Loss = -9789.76447089412
Iteration 10200: Loss = -9789.764452477262
Iteration 10300: Loss = -9789.764569590625
1
Iteration 10400: Loss = -9789.764439879948
Iteration 10500: Loss = -9789.84702581971
1
Iteration 10600: Loss = -9789.764384550124
Iteration 10700: Loss = -9789.764357143187
Iteration 10800: Loss = -9789.853408548925
1
Iteration 10900: Loss = -9789.764345819563
Iteration 11000: Loss = -9789.764318839358
Iteration 11100: Loss = -9789.764329755131
Iteration 11200: Loss = -9789.764338955378
Iteration 11300: Loss = -9789.764256999813
Iteration 11400: Loss = -9789.7642490771
Iteration 11500: Loss = -9789.764805882915
1
Iteration 11600: Loss = -9789.764253126245
Iteration 11700: Loss = -9789.770229585372
1
Iteration 11800: Loss = -9789.794108521484
2
Iteration 11900: Loss = -9789.764193368215
Iteration 12000: Loss = -9789.790156148116
1
Iteration 12100: Loss = -9789.764193440466
Iteration 12200: Loss = -9789.892029560933
1
Iteration 12300: Loss = -9789.764211984284
Iteration 12400: Loss = -9789.764289143535
Iteration 12500: Loss = -9789.76663022478
1
Iteration 12600: Loss = -9789.764179551368
Iteration 12700: Loss = -9789.849752156477
1
Iteration 12800: Loss = -9789.764156373782
Iteration 12900: Loss = -9789.764357673812
1
Iteration 13000: Loss = -9789.975597620494
2
Iteration 13100: Loss = -9789.764149042066
Iteration 13200: Loss = -9789.892086377697
1
Iteration 13300: Loss = -9789.765430375981
2
Iteration 13400: Loss = -9789.764469047921
3
Iteration 13500: Loss = -9789.765793202663
4
Iteration 13600: Loss = -9789.768292730812
5
Iteration 13700: Loss = -9789.809807452733
6
Iteration 13800: Loss = -9789.76410524535
Iteration 13900: Loss = -9789.764247462912
1
Iteration 14000: Loss = -9789.772461300223
2
Iteration 14100: Loss = -9789.76534240776
3
Iteration 14200: Loss = -9789.764321400795
4
Iteration 14300: Loss = -9789.765737499305
5
Iteration 14400: Loss = -9789.904638643078
6
Iteration 14500: Loss = -9789.764094793316
Iteration 14600: Loss = -9789.764367341493
1
Iteration 14700: Loss = -9789.779163181593
2
Iteration 14800: Loss = -9789.764073082924
Iteration 14900: Loss = -9789.831585524651
1
Iteration 15000: Loss = -9789.764133927138
Iteration 15100: Loss = -9789.76527621479
1
Iteration 15200: Loss = -9789.764238059492
2
Iteration 15300: Loss = -9789.764144656674
Iteration 15400: Loss = -9789.769744536407
1
Iteration 15500: Loss = -9789.764072632075
Iteration 15600: Loss = -9789.764137826882
Iteration 15700: Loss = -9789.764183474843
Iteration 15800: Loss = -9789.770720912811
1
Iteration 15900: Loss = -9789.764027972007
Iteration 16000: Loss = -9789.799202898843
1
Iteration 16100: Loss = -9789.769074852647
2
Iteration 16200: Loss = -9789.783415923708
3
Iteration 16300: Loss = -9789.765786459282
4
Iteration 16400: Loss = -9789.767931232518
5
Iteration 16500: Loss = -9789.814169964775
6
Iteration 16600: Loss = -9789.764617506436
7
Iteration 16700: Loss = -9789.764372049087
8
Iteration 16800: Loss = -9789.764406822098
9
Iteration 16900: Loss = -9789.77057524757
10
Iteration 17000: Loss = -9789.765356985554
11
Iteration 17100: Loss = -9789.766776700066
12
Iteration 17200: Loss = -9789.76962543634
13
Iteration 17300: Loss = -9789.76596282279
14
Iteration 17400: Loss = -9789.76429098977
15
Stopping early at iteration 17400 due to no improvement.
pi: tensor([[1.0000e+00, 1.4581e-06],
        [6.0518e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0712, 0.9288], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1715, 0.1191],
         [0.6919, 0.1314]],

        [[0.5871, 0.1400],
         [0.6334, 0.5922]],

        [[0.6245, 0.1625],
         [0.6929, 0.6873]],

        [[0.6127, 0.2024],
         [0.7096, 0.5253]],

        [[0.7064, 0.1577],
         [0.6089, 0.6534]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: -0.007817426793427174
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0010162821372197116
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01851330918808678
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.03878787878787879
Global Adjusted Rand Index: 0.010684048734597459
Average Adjusted Rand Index: 0.01010000866395162
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22924.068008124206
Iteration 100: Loss = -9792.663492863501
Iteration 200: Loss = -9791.37855402926
Iteration 300: Loss = -9790.14744019276
Iteration 400: Loss = -9789.70853080469
Iteration 500: Loss = -9789.439996205248
Iteration 600: Loss = -9789.258108608306
Iteration 700: Loss = -9789.134831374267
Iteration 800: Loss = -9789.053514272327
Iteration 900: Loss = -9789.001167717457
Iteration 1000: Loss = -9788.967713591888
Iteration 1100: Loss = -9788.945982602829
Iteration 1200: Loss = -9788.931207142781
Iteration 1300: Loss = -9788.920614651712
Iteration 1400: Loss = -9788.912538118042
Iteration 1500: Loss = -9788.906104662628
Iteration 1600: Loss = -9788.90094441389
Iteration 1700: Loss = -9788.896631313733
Iteration 1800: Loss = -9788.893150700362
Iteration 1900: Loss = -9788.890315807645
Iteration 2000: Loss = -9788.887982167706
Iteration 2100: Loss = -9788.886115505935
Iteration 2200: Loss = -9788.88468686091
Iteration 2300: Loss = -9788.883542786692
Iteration 2400: Loss = -9788.882646997949
Iteration 2500: Loss = -9788.881908690522
Iteration 2600: Loss = -9788.881376149951
Iteration 2700: Loss = -9788.880858281947
Iteration 2800: Loss = -9788.880497754291
Iteration 2900: Loss = -9788.880097895797
Iteration 3000: Loss = -9788.879825006485
Iteration 3100: Loss = -9788.879551709593
Iteration 3200: Loss = -9788.879452324869
Iteration 3300: Loss = -9788.879109173153
Iteration 3400: Loss = -9788.87893125437
Iteration 3500: Loss = -9788.878758743136
Iteration 3600: Loss = -9788.878612213877
Iteration 3700: Loss = -9788.8784866517
Iteration 3800: Loss = -9788.878360265786
Iteration 3900: Loss = -9788.878229724041
Iteration 4000: Loss = -9788.87813306261
Iteration 4100: Loss = -9788.878024231184
Iteration 4200: Loss = -9788.877971390664
Iteration 4300: Loss = -9788.877879500962
Iteration 4400: Loss = -9788.87781035968
Iteration 4500: Loss = -9788.877765725747
Iteration 4600: Loss = -9788.877690314403
Iteration 4700: Loss = -9788.877683213015
Iteration 4800: Loss = -9788.877534968462
Iteration 4900: Loss = -9788.877492311165
Iteration 5000: Loss = -9788.87746564836
Iteration 5100: Loss = -9788.87742422654
Iteration 5200: Loss = -9788.87744439462
Iteration 5300: Loss = -9788.877351447469
Iteration 5400: Loss = -9788.877373283281
Iteration 5500: Loss = -9788.87728997311
Iteration 5600: Loss = -9788.877281029612
Iteration 5700: Loss = -9788.877746955119
1
Iteration 5800: Loss = -9788.877223137471
Iteration 5900: Loss = -9788.877197608872
Iteration 6000: Loss = -9788.877199511531
Iteration 6100: Loss = -9788.87997207547
1
Iteration 6200: Loss = -9788.877114408238
Iteration 6300: Loss = -9788.877118220584
Iteration 6400: Loss = -9788.877089057869
Iteration 6500: Loss = -9788.877092459665
Iteration 6600: Loss = -9788.877034382349
Iteration 6700: Loss = -9788.876999005553
Iteration 6800: Loss = -9788.877248110188
1
Iteration 6900: Loss = -9788.87701745755
Iteration 7000: Loss = -9788.890581140033
1
Iteration 7100: Loss = -9788.87698408208
Iteration 7200: Loss = -9788.877193328322
1
Iteration 7300: Loss = -9788.89906360736
2
Iteration 7400: Loss = -9788.87695467088
Iteration 7500: Loss = -9788.876954165275
Iteration 7600: Loss = -9788.876946992614
Iteration 7700: Loss = -9788.877316726845
1
Iteration 7800: Loss = -9788.876878733035
Iteration 7900: Loss = -9788.876909380153
Iteration 8000: Loss = -9788.877430452589
1
Iteration 8100: Loss = -9788.876885368189
Iteration 8200: Loss = -9788.87685205315
Iteration 8300: Loss = -9788.877950729267
1
Iteration 8400: Loss = -9788.876863373534
Iteration 8500: Loss = -9788.876853877624
Iteration 8600: Loss = -9788.8769740673
1
Iteration 8700: Loss = -9788.876858236046
Iteration 8800: Loss = -9788.951508038796
1
Iteration 8900: Loss = -9788.876839554334
Iteration 9000: Loss = -9788.876841852887
Iteration 9100: Loss = -9788.87696279692
1
Iteration 9200: Loss = -9788.87685216158
Iteration 9300: Loss = -9788.880557857505
1
Iteration 9400: Loss = -9788.881959768061
2
Iteration 9500: Loss = -9788.898309324395
3
Iteration 9600: Loss = -9788.903311162443
4
Iteration 9700: Loss = -9788.8768412025
Iteration 9800: Loss = -9788.878721100993
1
Iteration 9900: Loss = -9788.87677645934
Iteration 10000: Loss = -9788.876983329912
1
Iteration 10100: Loss = -9788.877029586229
2
Iteration 10200: Loss = -9788.890579131303
3
Iteration 10300: Loss = -9788.89299302812
4
Iteration 10400: Loss = -9788.876802936258
Iteration 10500: Loss = -9788.87853372473
1
Iteration 10600: Loss = -9788.879892472252
2
Iteration 10700: Loss = -9788.88611459956
3
Iteration 10800: Loss = -9788.876780518838
Iteration 10900: Loss = -9788.87680832711
Iteration 11000: Loss = -9788.878764127663
1
Iteration 11100: Loss = -9788.948107227174
2
Iteration 11200: Loss = -9788.909677057658
3
Iteration 11300: Loss = -9788.886094557012
4
Iteration 11400: Loss = -9788.877187261985
5
Iteration 11500: Loss = -9788.885087691266
6
Iteration 11600: Loss = -9788.87721995449
7
Iteration 11700: Loss = -9788.877575897648
8
Iteration 11800: Loss = -9788.87870676218
9
Iteration 11900: Loss = -9788.8768789971
Iteration 12000: Loss = -9788.877169908194
1
Iteration 12100: Loss = -9788.890822547846
2
Iteration 12200: Loss = -9788.88719497037
3
Iteration 12300: Loss = -9788.877178297113
4
Iteration 12400: Loss = -9788.877483055323
5
Iteration 12500: Loss = -9788.876944550011
Iteration 12600: Loss = -9788.877357216159
1
Iteration 12700: Loss = -9788.87685823394
Iteration 12800: Loss = -9788.876789855181
Iteration 12900: Loss = -9788.990325856703
1
Iteration 13000: Loss = -9788.876800230837
Iteration 13100: Loss = -9788.87709259355
1
Iteration 13200: Loss = -9788.876801449207
Iteration 13300: Loss = -9788.87691249
1
Iteration 13400: Loss = -9788.876779590857
Iteration 13500: Loss = -9788.877006449338
1
Iteration 13600: Loss = -9788.876877522509
Iteration 13700: Loss = -9788.927645109183
1
Iteration 13800: Loss = -9788.876764716326
Iteration 13900: Loss = -9788.885984540495
1
Iteration 14000: Loss = -9788.905991326388
2
Iteration 14100: Loss = -9788.878700943033
3
Iteration 14200: Loss = -9788.876761360685
Iteration 14300: Loss = -9788.87752278395
1
Iteration 14400: Loss = -9788.876809981783
Iteration 14500: Loss = -9788.876847838836
Iteration 14600: Loss = -9788.876830453793
Iteration 14700: Loss = -9788.877890898986
1
Iteration 14800: Loss = -9788.880911301725
2
Iteration 14900: Loss = -9788.877003086283
3
Iteration 15000: Loss = -9788.87737632554
4
Iteration 15100: Loss = -9788.878004644985
5
Iteration 15200: Loss = -9788.876800554319
Iteration 15300: Loss = -9788.878238248459
1
Iteration 15400: Loss = -9788.876803745596
Iteration 15500: Loss = -9788.876989401582
1
Iteration 15600: Loss = -9788.876975778496
2
Iteration 15700: Loss = -9788.878850861312
3
Iteration 15800: Loss = -9788.882003901554
4
Iteration 15900: Loss = -9788.892254461936
5
Iteration 16000: Loss = -9788.881627100234
6
Iteration 16100: Loss = -9788.876826542024
Iteration 16200: Loss = -9788.964936158283
1
Iteration 16300: Loss = -9788.96911868074
2
Iteration 16400: Loss = -9788.877357058016
3
Iteration 16500: Loss = -9788.880463951838
4
Iteration 16600: Loss = -9788.87974548706
5
Iteration 16700: Loss = -9788.877291646317
6
Iteration 16800: Loss = -9789.053823936929
7
Iteration 16900: Loss = -9788.87676418394
Iteration 17000: Loss = -9788.877644825088
1
Iteration 17100: Loss = -9788.877893709141
2
Iteration 17200: Loss = -9788.886750688542
3
Iteration 17300: Loss = -9788.879027655483
4
Iteration 17400: Loss = -9788.88023301345
5
Iteration 17500: Loss = -9788.87851153712
6
Iteration 17600: Loss = -9789.170649373793
7
Iteration 17700: Loss = -9788.87681212558
Iteration 17800: Loss = -9788.879616044967
1
Iteration 17900: Loss = -9788.879242276764
2
Iteration 18000: Loss = -9788.877444926318
3
Iteration 18100: Loss = -9788.876986615924
4
Iteration 18200: Loss = -9788.877520007529
5
Iteration 18300: Loss = -9788.89566386879
6
Iteration 18400: Loss = -9788.884623297203
7
Iteration 18500: Loss = -9788.87701801965
8
Iteration 18600: Loss = -9788.93107363746
9
Iteration 18700: Loss = -9788.876901828975
Iteration 18800: Loss = -9788.926148336142
1
Iteration 18900: Loss = -9788.877078301028
2
Iteration 19000: Loss = -9788.878250861868
3
Iteration 19100: Loss = -9788.876915143981
Iteration 19200: Loss = -9788.88776825976
1
Iteration 19300: Loss = -9788.879276933949
2
Iteration 19400: Loss = -9788.90990807922
3
Iteration 19500: Loss = -9789.02912507786
4
Iteration 19600: Loss = -9788.877645987155
5
Iteration 19700: Loss = -9788.87815552705
6
Iteration 19800: Loss = -9788.877595170814
7
Iteration 19900: Loss = -9788.87684818176
pi: tensor([[1.0000e+00, 5.0876e-09],
        [6.4229e-01, 3.5771e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7020, 0.2980], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1392, 0.1244],
         [0.7295, 0.1125]],

        [[0.6738, 0.1089],
         [0.6831, 0.6756]],

        [[0.5663, 0.0930],
         [0.6618, 0.6680]],

        [[0.5494, 0.0366],
         [0.6831, 0.5711]],

        [[0.7238, 0.1310],
         [0.7106, 0.5306]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.019445650151919142
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.004184354844884226
Average Adjusted Rand Index: -0.004777639573220132
9912.056454662514
[0.010684048734597459, -0.004184354844884226] [0.01010000866395162, -0.004777639573220132] [9789.76429098977, 9788.878110697175]
-------------------------------------
This iteration is 51
True Objective function: Loss = -10195.828185323497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21109.56650913177
Iteration 100: Loss = -10120.550248728621
Iteration 200: Loss = -10119.42467450951
Iteration 300: Loss = -10119.01398084619
Iteration 400: Loss = -10118.802587141712
Iteration 500: Loss = -10118.6700287682
Iteration 600: Loss = -10118.562982182219
Iteration 700: Loss = -10118.445427244578
Iteration 800: Loss = -10118.302705704415
Iteration 900: Loss = -10118.155278347787
Iteration 1000: Loss = -10118.003076049998
Iteration 1100: Loss = -10117.834810587196
Iteration 1200: Loss = -10117.643605197758
Iteration 1300: Loss = -10117.379196607535
Iteration 1400: Loss = -10116.71130406599
Iteration 1500: Loss = -10116.005723254602
Iteration 1600: Loss = -10115.559200119613
Iteration 1700: Loss = -10115.133142225619
Iteration 1800: Loss = -10114.296566556815
Iteration 1900: Loss = -10113.864962649513
Iteration 2000: Loss = -10113.550089888658
Iteration 2100: Loss = -10113.413905647963
Iteration 2200: Loss = -10113.26231676477
Iteration 2300: Loss = -10113.051908459029
Iteration 2400: Loss = -10112.983283244326
Iteration 2500: Loss = -10112.90143305817
Iteration 2600: Loss = -10112.828884128437
Iteration 2700: Loss = -10111.745403753626
Iteration 2800: Loss = -10110.923849578176
Iteration 2900: Loss = -10110.73001792937
Iteration 3000: Loss = -10110.653774596925
Iteration 3100: Loss = -10110.624596085143
Iteration 3200: Loss = -10110.588757429527
Iteration 3300: Loss = -10110.527266761886
Iteration 3400: Loss = -10110.469849203575
Iteration 3500: Loss = -10110.43557365652
Iteration 3600: Loss = -10110.414499857896
Iteration 3700: Loss = -10110.399036346345
Iteration 3800: Loss = -10110.371822839828
Iteration 3900: Loss = -10110.353640165175
Iteration 4000: Loss = -10110.343194122717
Iteration 4100: Loss = -10110.33819021982
Iteration 4200: Loss = -10110.334193135823
Iteration 4300: Loss = -10110.330745800664
Iteration 4400: Loss = -10110.327487345656
Iteration 4500: Loss = -10110.324438323687
Iteration 4600: Loss = -10110.321055507931
Iteration 4700: Loss = -10110.316187871811
Iteration 4800: Loss = -10110.307065959254
Iteration 4900: Loss = -10110.294633997306
Iteration 5000: Loss = -10110.235274945502
Iteration 5100: Loss = -10110.224875141512
Iteration 5200: Loss = -10110.182671259405
Iteration 5300: Loss = -10110.177664023886
Iteration 5400: Loss = -10110.173692602497
Iteration 5500: Loss = -10110.169460794577
Iteration 5600: Loss = -10110.165998431978
Iteration 5700: Loss = -10110.161064429112
Iteration 5800: Loss = -10110.14614541435
Iteration 5900: Loss = -10110.14370606686
Iteration 6000: Loss = -10110.142553855729
Iteration 6100: Loss = -10110.141563446374
Iteration 6200: Loss = -10110.140512808448
Iteration 6300: Loss = -10110.139208193728
Iteration 6400: Loss = -10110.136944380913
Iteration 6500: Loss = -10110.129127659766
Iteration 6600: Loss = -10110.119465367337
Iteration 6700: Loss = -10110.118363581516
Iteration 6800: Loss = -10110.117574207683
Iteration 6900: Loss = -10110.116963148597
Iteration 7000: Loss = -10110.116429354544
Iteration 7100: Loss = -10110.115943479837
Iteration 7200: Loss = -10110.115368876957
Iteration 7300: Loss = -10110.116740924226
1
Iteration 7400: Loss = -10110.104932605935
Iteration 7500: Loss = -10110.10341963245
Iteration 7600: Loss = -10110.09899879753
Iteration 7700: Loss = -10110.097763580912
Iteration 7800: Loss = -10110.097429518866
Iteration 7900: Loss = -10110.097164780367
Iteration 8000: Loss = -10110.096890774883
Iteration 8100: Loss = -10110.098942472789
1
Iteration 8200: Loss = -10110.098051569796
2
Iteration 8300: Loss = -10110.095904734635
Iteration 8400: Loss = -10110.092256678574
Iteration 8500: Loss = -10110.089573936006
Iteration 8600: Loss = -10110.084038790534
Iteration 8700: Loss = -10110.083593842155
Iteration 8800: Loss = -10110.084149541533
1
Iteration 8900: Loss = -10110.082407467342
Iteration 9000: Loss = -10110.081982887392
Iteration 9100: Loss = -10110.08133120269
Iteration 9200: Loss = -10110.07920234298
Iteration 9300: Loss = -10110.075949009359
Iteration 9400: Loss = -10110.075797902036
Iteration 9500: Loss = -10110.075509335662
Iteration 9600: Loss = -10110.07542387981
Iteration 9700: Loss = -10110.075384216854
Iteration 9800: Loss = -10110.075221537638
Iteration 9900: Loss = -10110.078528101192
1
Iteration 10000: Loss = -10110.07504678551
Iteration 10100: Loss = -10110.07497042213
Iteration 10200: Loss = -10110.395154581614
1
Iteration 10300: Loss = -10110.074794154138
Iteration 10400: Loss = -10110.07472078149
Iteration 10500: Loss = -10110.079110905534
1
Iteration 10600: Loss = -10110.074590214594
Iteration 10700: Loss = -10110.203122378833
1
Iteration 10800: Loss = -10110.074418920165
Iteration 10900: Loss = -10110.086687558542
1
Iteration 11000: Loss = -10110.062610987134
Iteration 11100: Loss = -10110.075005168603
1
Iteration 11200: Loss = -10110.061880006362
Iteration 11300: Loss = -10110.110030512664
1
Iteration 11400: Loss = -10110.060793325705
Iteration 11500: Loss = -10110.069423395284
1
Iteration 11600: Loss = -10110.057706433312
Iteration 11700: Loss = -10110.116693639611
1
Iteration 11800: Loss = -10110.126993481856
2
Iteration 11900: Loss = -10110.14288445315
3
Iteration 12000: Loss = -10110.048315566522
Iteration 12100: Loss = -10110.048054346053
Iteration 12200: Loss = -10110.092049479794
1
Iteration 12300: Loss = -10110.128984957677
2
Iteration 12400: Loss = -10110.04772328315
Iteration 12500: Loss = -10110.04717048668
Iteration 12600: Loss = -10110.046319187324
Iteration 12700: Loss = -10110.044795663885
Iteration 12800: Loss = -10110.072561095245
1
Iteration 12900: Loss = -10110.042666392286
Iteration 13000: Loss = -10110.046183137214
1
Iteration 13100: Loss = -10110.043304859722
2
Iteration 13200: Loss = -10110.040104446307
Iteration 13300: Loss = -10110.040303712663
1
Iteration 13400: Loss = -10110.040494151734
2
Iteration 13500: Loss = -10110.043457853983
3
Iteration 13600: Loss = -10110.04044244973
4
Iteration 13700: Loss = -10110.144716430123
5
Iteration 13800: Loss = -10110.036091950295
Iteration 13900: Loss = -10110.149401970213
1
Iteration 14000: Loss = -10110.035853622512
Iteration 14100: Loss = -10110.035795414837
Iteration 14200: Loss = -10110.035823384656
Iteration 14300: Loss = -10110.163608833805
1
Iteration 14400: Loss = -10110.03635223773
2
Iteration 14500: Loss = -10110.036395211719
3
Iteration 14600: Loss = -10110.035866278242
Iteration 14700: Loss = -10110.035806157804
Iteration 14800: Loss = -10110.042101720437
1
Iteration 14900: Loss = -10110.043307490125
2
Iteration 15000: Loss = -10110.038029350308
3
Iteration 15100: Loss = -10110.033205334976
Iteration 15200: Loss = -10110.034614213339
1
Iteration 15300: Loss = -10110.03900022763
2
Iteration 15400: Loss = -10110.02333679396
Iteration 15500: Loss = -10110.01388489622
Iteration 15600: Loss = -10110.017063938018
1
Iteration 15700: Loss = -10110.013019451248
Iteration 15800: Loss = -10110.012537540328
Iteration 15900: Loss = -10110.013048065357
1
Iteration 16000: Loss = -10110.011814416475
Iteration 16100: Loss = -10110.011841712654
Iteration 16200: Loss = -10110.013729655808
1
Iteration 16300: Loss = -10110.08260249602
2
Iteration 16400: Loss = -10110.011468726672
Iteration 16500: Loss = -10110.12044410652
1
Iteration 16600: Loss = -10110.012666891329
2
Iteration 16700: Loss = -10110.013558760937
3
Iteration 16800: Loss = -10110.011568171063
Iteration 16900: Loss = -10110.01149706429
Iteration 17000: Loss = -10110.012187613027
1
Iteration 17100: Loss = -10110.177575718866
2
Iteration 17200: Loss = -10110.011457959305
Iteration 17300: Loss = -10110.017794698088
1
Iteration 17400: Loss = -10110.10673485798
2
Iteration 17500: Loss = -10110.011572561665
3
Iteration 17600: Loss = -10110.01151957393
Iteration 17700: Loss = -10110.011959134266
1
Iteration 17800: Loss = -10110.012399435782
2
Iteration 17900: Loss = -10110.01228268362
3
Iteration 18000: Loss = -10110.011481243704
Iteration 18100: Loss = -10110.013636530179
1
Iteration 18200: Loss = -10110.01458067052
2
Iteration 18300: Loss = -10110.022480287882
3
Iteration 18400: Loss = -10110.011228339445
Iteration 18500: Loss = -10110.01983759388
1
Iteration 18600: Loss = -10110.010102280088
Iteration 18700: Loss = -10110.010265697869
1
Iteration 18800: Loss = -10110.128999770826
2
Iteration 18900: Loss = -10110.010120522571
Iteration 19000: Loss = -10110.015845847895
1
Iteration 19100: Loss = -10110.00949295396
Iteration 19200: Loss = -10110.012931924444
1
Iteration 19300: Loss = -10110.029293611537
2
Iteration 19400: Loss = -10110.009448471823
Iteration 19500: Loss = -10110.009509971407
Iteration 19600: Loss = -10110.009969577484
1
Iteration 19700: Loss = -10110.059220981917
2
Iteration 19800: Loss = -10110.00945608057
Iteration 19900: Loss = -10110.009661132584
1
pi: tensor([[1.0000e+00, 5.3478e-07],
        [6.0065e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1358, 0.8642], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2542, 0.1507],
         [0.5127, 0.1354]],

        [[0.6927, 0.1642],
         [0.6966, 0.6380]],

        [[0.7259, 0.1343],
         [0.6636, 0.5458]],

        [[0.5855, 0.1864],
         [0.5052, 0.6328]],

        [[0.7084, 0.1223],
         [0.5553, 0.7049]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.007482543419715857
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.034351502981690596
time is 2
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.0026936560438907487
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.009175716042558325
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.010245671163764708
Global Adjusted Rand Index: -0.004847445989897429
Average Adjusted Rand Index: -0.008691549464818163
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21402.89569668737
Iteration 100: Loss = -10119.097296348591
Iteration 200: Loss = -10118.809872158523
Iteration 300: Loss = -10118.737219952647
Iteration 400: Loss = -10118.696294065727
Iteration 500: Loss = -10118.669525928151
Iteration 600: Loss = -10118.650473505602
Iteration 700: Loss = -10118.635852898336
Iteration 800: Loss = -10118.624062111097
Iteration 900: Loss = -10118.61382392871
Iteration 1000: Loss = -10118.604292726322
Iteration 1100: Loss = -10118.594459635036
Iteration 1200: Loss = -10118.583153938811
Iteration 1300: Loss = -10118.568185218212
Iteration 1400: Loss = -10118.545258890079
Iteration 1500: Loss = -10118.508599625457
Iteration 1600: Loss = -10118.467134215653
Iteration 1700: Loss = -10118.442062656119
Iteration 1800: Loss = -10118.427450252062
Iteration 1900: Loss = -10118.41720889586
Iteration 2000: Loss = -10118.40837811794
Iteration 2100: Loss = -10118.399457860314
Iteration 2200: Loss = -10118.38969699132
Iteration 2300: Loss = -10118.378529277137
Iteration 2400: Loss = -10118.365456003949
Iteration 2500: Loss = -10118.349606437101
Iteration 2600: Loss = -10118.330065105816
Iteration 2700: Loss = -10118.305200073391
Iteration 2800: Loss = -10118.272349976804
Iteration 2900: Loss = -10118.226768049426
Iteration 3000: Loss = -10118.159127173689
Iteration 3100: Loss = -10118.054094739118
Iteration 3200: Loss = -10117.964948992507
Iteration 3300: Loss = -10117.951084585255
Iteration 3400: Loss = -10117.944319172599
Iteration 3500: Loss = -10117.935882844664
Iteration 3600: Loss = -10117.906802410931
Iteration 3700: Loss = -10117.536706470704
Iteration 3800: Loss = -10116.405270408493
Iteration 3900: Loss = -10116.325471069063
Iteration 4000: Loss = -10116.314275518676
Iteration 4100: Loss = -10116.309754510292
Iteration 4200: Loss = -10116.30732150974
Iteration 4300: Loss = -10116.305721878176
Iteration 4400: Loss = -10116.304637668292
Iteration 4500: Loss = -10116.303806812246
Iteration 4600: Loss = -10116.303416875311
Iteration 4700: Loss = -10116.302802190445
Iteration 4800: Loss = -10116.302273920313
Iteration 4900: Loss = -10116.301947148855
Iteration 5000: Loss = -10116.301646775679
Iteration 5100: Loss = -10116.301410556001
Iteration 5200: Loss = -10116.302385380257
1
Iteration 5300: Loss = -10116.301016778714
Iteration 5400: Loss = -10116.30086904793
Iteration 5500: Loss = -10116.300935434057
Iteration 5600: Loss = -10116.300559002995
Iteration 5700: Loss = -10116.303149021605
1
Iteration 5800: Loss = -10116.300345066062
Iteration 5900: Loss = -10116.300585985815
1
Iteration 6000: Loss = -10116.30076450747
2
Iteration 6100: Loss = -10116.300143758872
Iteration 6200: Loss = -10116.300074151255
Iteration 6300: Loss = -10116.300049499941
Iteration 6400: Loss = -10116.299916984544
Iteration 6500: Loss = -10116.30085356545
1
Iteration 6600: Loss = -10116.299809306176
Iteration 6700: Loss = -10116.300054079882
1
Iteration 6800: Loss = -10116.2997221012
Iteration 6900: Loss = -10116.299667464631
Iteration 7000: Loss = -10116.299664005754
Iteration 7100: Loss = -10116.299572201058
Iteration 7200: Loss = -10116.29955460465
Iteration 7300: Loss = -10116.299515691302
Iteration 7400: Loss = -10116.299622085891
1
Iteration 7500: Loss = -10116.29947210526
Iteration 7600: Loss = -10116.29967094367
1
Iteration 7700: Loss = -10116.299486138667
Iteration 7800: Loss = -10116.299770413993
1
Iteration 7900: Loss = -10116.308001295118
2
Iteration 8000: Loss = -10116.337796612312
3
Iteration 8100: Loss = -10116.358432836885
4
Iteration 8200: Loss = -10116.301439610384
5
Iteration 8300: Loss = -10116.2993518965
Iteration 8400: Loss = -10116.305590412525
1
Iteration 8500: Loss = -10116.299293254957
Iteration 8600: Loss = -10116.304566555818
1
Iteration 8700: Loss = -10116.29925936599
Iteration 8800: Loss = -10116.29926348622
Iteration 8900: Loss = -10116.300035525308
1
Iteration 9000: Loss = -10116.299207486101
Iteration 9100: Loss = -10116.299505074676
1
Iteration 9200: Loss = -10116.299193760744
Iteration 9300: Loss = -10116.334583912081
1
Iteration 9400: Loss = -10116.300066302907
2
Iteration 9500: Loss = -10116.299192718423
Iteration 9600: Loss = -10116.300012555517
1
Iteration 9700: Loss = -10116.299136514408
Iteration 9800: Loss = -10116.312387569164
1
Iteration 9900: Loss = -10116.301009116114
2
Iteration 10000: Loss = -10116.327063577908
3
Iteration 10100: Loss = -10116.299136653512
Iteration 10200: Loss = -10116.300254604941
1
Iteration 10300: Loss = -10116.301481250723
2
Iteration 10400: Loss = -10116.299129798696
Iteration 10500: Loss = -10116.302497797153
1
Iteration 10600: Loss = -10116.301206646229
2
Iteration 10700: Loss = -10116.300198434561
3
Iteration 10800: Loss = -10116.29914646119
Iteration 10900: Loss = -10116.304998859214
1
Iteration 11000: Loss = -10116.300173863132
2
Iteration 11100: Loss = -10116.299150101162
Iteration 11200: Loss = -10116.30388549561
1
Iteration 11300: Loss = -10116.301183043783
2
Iteration 11400: Loss = -10116.299144658879
Iteration 11500: Loss = -10116.299341596932
1
Iteration 11600: Loss = -10116.322532323737
2
Iteration 11700: Loss = -10116.302536220377
3
Iteration 11800: Loss = -10116.300325406806
4
Iteration 11900: Loss = -10116.465041885389
5
Iteration 12000: Loss = -10116.29908955157
Iteration 12100: Loss = -10116.299474971807
1
Iteration 12200: Loss = -10116.299076212345
Iteration 12300: Loss = -10116.299104193451
Iteration 12400: Loss = -10116.299124930494
Iteration 12500: Loss = -10116.299295787905
1
Iteration 12600: Loss = -10116.302297163571
2
Iteration 12700: Loss = -10116.299091494027
Iteration 12800: Loss = -10116.312248120204
1
Iteration 12900: Loss = -10116.29947044902
2
Iteration 13000: Loss = -10116.2991691363
Iteration 13100: Loss = -10116.320916184597
1
Iteration 13200: Loss = -10116.299044788657
Iteration 13300: Loss = -10116.691574182141
1
Iteration 13400: Loss = -10116.299051980784
Iteration 13500: Loss = -10116.299120010863
Iteration 13600: Loss = -10116.299186939017
Iteration 13700: Loss = -10116.29938550401
1
Iteration 13800: Loss = -10116.356347781053
2
Iteration 13900: Loss = -10116.302362240956
3
Iteration 14000: Loss = -10116.299066940384
Iteration 14100: Loss = -10116.299283643646
1
Iteration 14200: Loss = -10116.302667649443
2
Iteration 14300: Loss = -10116.358800535227
3
Iteration 14400: Loss = -10116.299124309942
Iteration 14500: Loss = -10116.299199825913
Iteration 14600: Loss = -10116.300997933091
1
Iteration 14700: Loss = -10116.299071576763
Iteration 14800: Loss = -10116.3003293063
1
Iteration 14900: Loss = -10116.307349518127
2
Iteration 15000: Loss = -10116.299100207947
Iteration 15100: Loss = -10116.39309697649
1
Iteration 15200: Loss = -10116.299059171239
Iteration 15300: Loss = -10116.452434398727
1
Iteration 15400: Loss = -10116.299076736443
Iteration 15500: Loss = -10116.362742886406
1
Iteration 15600: Loss = -10116.299647954165
2
Iteration 15700: Loss = -10116.302594328645
3
Iteration 15800: Loss = -10116.302884011819
4
Iteration 15900: Loss = -10116.30406291061
5
Iteration 16000: Loss = -10116.308776103853
6
Iteration 16100: Loss = -10116.30445505309
7
Iteration 16200: Loss = -10116.29905534156
Iteration 16300: Loss = -10116.3005046938
1
Iteration 16400: Loss = -10116.299142671913
Iteration 16500: Loss = -10116.29988943108
1
Iteration 16600: Loss = -10116.30086905621
2
Iteration 16700: Loss = -10116.300744435812
3
Iteration 16800: Loss = -10116.29915391448
Iteration 16900: Loss = -10116.299864140918
1
Iteration 17000: Loss = -10116.299133326045
Iteration 17100: Loss = -10116.341492812186
1
Iteration 17200: Loss = -10116.299008318118
Iteration 17300: Loss = -10116.299884133974
1
Iteration 17400: Loss = -10116.299409394309
2
Iteration 17500: Loss = -10116.318647525362
3
Iteration 17600: Loss = -10116.362629454119
4
Iteration 17700: Loss = -10116.299784189197
5
Iteration 17800: Loss = -10116.299161406512
6
Iteration 17900: Loss = -10116.35403584562
7
Iteration 18000: Loss = -10116.308761068465
8
Iteration 18100: Loss = -10116.299131812615
9
Iteration 18200: Loss = -10116.29934076242
10
Iteration 18300: Loss = -10116.300490015788
11
Iteration 18400: Loss = -10116.348392377044
12
Iteration 18500: Loss = -10116.29901710165
Iteration 18600: Loss = -10116.299193877705
1
Iteration 18700: Loss = -10116.302942294584
2
Iteration 18800: Loss = -10116.301335637283
3
Iteration 18900: Loss = -10116.299196490801
4
Iteration 19000: Loss = -10116.299072822978
Iteration 19100: Loss = -10116.302477061743
1
Iteration 19200: Loss = -10116.300146189882
2
Iteration 19300: Loss = -10116.305709280612
3
Iteration 19400: Loss = -10116.302055011893
4
Iteration 19500: Loss = -10116.299639818324
5
Iteration 19600: Loss = -10116.299316279827
6
Iteration 19700: Loss = -10116.317346484113
7
Iteration 19800: Loss = -10116.2997195259
8
Iteration 19900: Loss = -10116.299450850805
9
pi: tensor([[1.5033e-07, 1.0000e+00],
        [3.0036e-02, 9.6996e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1526, 0.8474], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1120, 0.1233],
         [0.5660, 0.1438]],

        [[0.5182, 0.1648],
         [0.6445, 0.6341]],

        [[0.5488, 0.1133],
         [0.5436, 0.6623]],

        [[0.6834, 0.2088],
         [0.6187, 0.5436]],

        [[0.5570, 0.0733],
         [0.5926, 0.7078]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 0.002049732657167134
Average Adjusted Rand Index: 0.0012540423696852823
10195.828185323497
[-0.004847445989897429, 0.002049732657167134] [-0.008691549464818163, 0.0012540423696852823] [10110.09092839651, 10116.31339170811]
-------------------------------------
This iteration is 52
True Objective function: Loss = -9949.631588403518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23151.568137903858
Iteration 100: Loss = -9811.250347268986
Iteration 200: Loss = -9808.971653690802
Iteration 300: Loss = -9807.739173853766
Iteration 400: Loss = -9807.177130403028
Iteration 500: Loss = -9806.718243531577
Iteration 600: Loss = -9806.394869320035
Iteration 700: Loss = -9806.16053332352
Iteration 800: Loss = -9805.932352206193
Iteration 900: Loss = -9805.684458460737
Iteration 1000: Loss = -9805.420372472
Iteration 1100: Loss = -9805.163771985533
Iteration 1200: Loss = -9804.948336366622
Iteration 1300: Loss = -9804.787373625577
Iteration 1400: Loss = -9804.688002001141
Iteration 1500: Loss = -9804.632404100095
Iteration 1600: Loss = -9804.601759707204
Iteration 1700: Loss = -9804.582285394039
Iteration 1800: Loss = -9804.568870998366
Iteration 1900: Loss = -9804.559268052843
Iteration 2000: Loss = -9804.553489885175
Iteration 2100: Loss = -9804.55024996766
Iteration 2200: Loss = -9804.548416551756
Iteration 2300: Loss = -9804.547234702492
Iteration 2400: Loss = -9804.546405529496
Iteration 2500: Loss = -9804.54573205526
Iteration 2600: Loss = -9804.54516361254
Iteration 2700: Loss = -9804.544719273837
Iteration 2800: Loss = -9804.544371823155
Iteration 2900: Loss = -9804.544075620448
Iteration 3000: Loss = -9804.5438361892
Iteration 3100: Loss = -9804.543643430135
Iteration 3200: Loss = -9804.543532430966
Iteration 3300: Loss = -9804.560018401346
1
Iteration 3400: Loss = -9804.54328925149
Iteration 3500: Loss = -9804.543247389262
Iteration 3600: Loss = -9804.547670584063
1
Iteration 3700: Loss = -9804.543186789275
Iteration 3800: Loss = -9804.54317291297
Iteration 3900: Loss = -9804.543143867833
Iteration 4000: Loss = -9804.543128911648
Iteration 4100: Loss = -9804.545739223438
1
Iteration 4200: Loss = -9804.543131908633
Iteration 4300: Loss = -9804.54324659063
1
Iteration 4400: Loss = -9804.543094163872
Iteration 4500: Loss = -9804.546000689154
1
Iteration 4600: Loss = -9804.543173260707
Iteration 4700: Loss = -9804.543120697836
Iteration 4800: Loss = -9804.543140648877
Iteration 4900: Loss = -9804.543124868664
Iteration 5000: Loss = -9804.543119474334
Iteration 5100: Loss = -9804.543129127163
Iteration 5200: Loss = -9804.54312608026
Iteration 5300: Loss = -9804.543805216974
1
Iteration 5400: Loss = -9804.543667007927
2
Iteration 5500: Loss = -9804.549553521432
3
Iteration 5600: Loss = -9804.54403841835
4
Iteration 5700: Loss = -9804.543246102765
5
Iteration 5800: Loss = -9804.543150992271
Iteration 5900: Loss = -9804.543135440921
Iteration 6000: Loss = -9804.54493286689
1
Iteration 6100: Loss = -9804.543128181353
Iteration 6200: Loss = -9804.544398192178
1
Iteration 6300: Loss = -9804.543116488607
Iteration 6400: Loss = -9804.578694085336
1
Iteration 6500: Loss = -9804.543104257846
Iteration 6600: Loss = -9804.543133369396
Iteration 6700: Loss = -9804.543168745866
Iteration 6800: Loss = -9804.545873964797
1
Iteration 6900: Loss = -9804.54379990885
2
Iteration 7000: Loss = -9804.559763067982
3
Iteration 7100: Loss = -9804.54331427594
4
Iteration 7200: Loss = -9804.54311357447
Iteration 7300: Loss = -9804.543142563438
Iteration 7400: Loss = -9804.54339765622
1
Iteration 7500: Loss = -9804.545073181298
2
Iteration 7600: Loss = -9804.543132105495
Iteration 7700: Loss = -9804.543327887806
1
Iteration 7800: Loss = -9804.552780711749
2
Iteration 7900: Loss = -9804.543090783274
Iteration 8000: Loss = -9804.543254111995
1
Iteration 8100: Loss = -9804.543126137653
Iteration 8200: Loss = -9804.543121324215
Iteration 8300: Loss = -9804.543100958512
Iteration 8400: Loss = -9804.54312160622
Iteration 8500: Loss = -9804.543113508047
Iteration 8600: Loss = -9804.544668336674
1
Iteration 8700: Loss = -9804.543127996012
Iteration 8800: Loss = -9804.559772979037
1
Iteration 8900: Loss = -9804.543117252892
Iteration 9000: Loss = -9804.559314982613
1
Iteration 9100: Loss = -9804.543108996893
Iteration 9200: Loss = -9804.695716481461
1
Iteration 9300: Loss = -9804.5431105558
Iteration 9400: Loss = -9804.543171487358
Iteration 9500: Loss = -9804.54310951299
Iteration 9600: Loss = -9804.543098578673
Iteration 9700: Loss = -9804.549766855675
1
Iteration 9800: Loss = -9804.543108961037
Iteration 9900: Loss = -9804.543192885658
Iteration 10000: Loss = -9804.543288387516
Iteration 10100: Loss = -9804.668642418934
1
Iteration 10200: Loss = -9804.543651333162
2
Iteration 10300: Loss = -9804.544057362915
3
Iteration 10400: Loss = -9804.544749219447
4
Iteration 10500: Loss = -9804.702224851691
5
Iteration 10600: Loss = -9804.549633501862
6
Iteration 10700: Loss = -9804.543257515168
Iteration 10800: Loss = -9804.543250712351
Iteration 10900: Loss = -9804.550028365526
1
Iteration 11000: Loss = -9804.543421045884
2
Iteration 11100: Loss = -9804.543901383133
3
Iteration 11200: Loss = -9804.54328059333
Iteration 11300: Loss = -9804.559424342975
1
Iteration 11400: Loss = -9804.627941377268
2
Iteration 11500: Loss = -9804.587041464012
3
Iteration 11600: Loss = -9804.544661242579
4
Iteration 11700: Loss = -9804.543154801551
Iteration 11800: Loss = -9804.543545012757
1
Iteration 11900: Loss = -9804.545866881277
2
Iteration 12000: Loss = -9804.54955805977
3
Iteration 12100: Loss = -9804.601114314684
4
Iteration 12200: Loss = -9804.559017932945
5
Iteration 12300: Loss = -9804.550445621728
6
Iteration 12400: Loss = -9804.557629633104
7
Iteration 12500: Loss = -9804.554313514589
8
Iteration 12600: Loss = -9804.54357165437
9
Iteration 12700: Loss = -9804.566173924402
10
Iteration 12800: Loss = -9804.544193005997
11
Iteration 12900: Loss = -9804.54632966806
12
Iteration 13000: Loss = -9804.546836588195
13
Iteration 13100: Loss = -9804.544093087046
14
Iteration 13200: Loss = -9804.543128386726
Iteration 13300: Loss = -9804.543109472053
Iteration 13400: Loss = -9804.557709703418
1
Iteration 13500: Loss = -9804.543126878352
Iteration 13600: Loss = -9804.543502252081
1
Iteration 13700: Loss = -9804.557389424634
2
Iteration 13800: Loss = -9804.544014630517
3
Iteration 13900: Loss = -9804.543208498546
Iteration 14000: Loss = -9804.543157599743
Iteration 14100: Loss = -9804.543166763526
Iteration 14200: Loss = -9804.544751406202
1
Iteration 14300: Loss = -9804.606665013278
2
Iteration 14400: Loss = -9804.54338085652
3
Iteration 14500: Loss = -9804.66667021739
4
Iteration 14600: Loss = -9804.543597408505
5
Iteration 14700: Loss = -9804.629497647713
6
Iteration 14800: Loss = -9804.547757907829
7
Iteration 14900: Loss = -9804.549874989245
8
Iteration 15000: Loss = -9804.543243984323
Iteration 15100: Loss = -9804.543596431768
1
Iteration 15200: Loss = -9804.545115422705
2
Iteration 15300: Loss = -9804.54657091625
3
Iteration 15400: Loss = -9804.546288164416
4
Iteration 15500: Loss = -9804.548823194933
5
Iteration 15600: Loss = -9804.544125664746
6
Iteration 15700: Loss = -9804.543274556063
Iteration 15800: Loss = -9804.547581338282
1
Iteration 15900: Loss = -9804.561618760305
2
Iteration 16000: Loss = -9804.543610859284
3
Iteration 16100: Loss = -9804.545269642425
4
Iteration 16200: Loss = -9804.594004774612
5
Iteration 16300: Loss = -9804.543377429009
6
Iteration 16400: Loss = -9804.544722518409
7
Iteration 16500: Loss = -9804.543694839156
8
Iteration 16600: Loss = -9804.543170575429
Iteration 16700: Loss = -9804.546097285953
1
Iteration 16800: Loss = -9804.544897920838
2
Iteration 16900: Loss = -9804.54336499103
3
Iteration 17000: Loss = -9804.544274136695
4
Iteration 17100: Loss = -9804.545316010528
5
Iteration 17200: Loss = -9804.559242708645
6
Iteration 17300: Loss = -9804.543155968398
Iteration 17400: Loss = -9804.543810335275
1
Iteration 17500: Loss = -9804.560537990468
2
Iteration 17600: Loss = -9804.56843249384
3
Iteration 17700: Loss = -9804.544208558067
4
Iteration 17800: Loss = -9804.545834236544
5
Iteration 17900: Loss = -9804.556060830233
6
Iteration 18000: Loss = -9804.54359038571
7
Iteration 18100: Loss = -9804.54323146504
Iteration 18200: Loss = -9804.709422825379
1
Iteration 18300: Loss = -9804.543143103763
Iteration 18400: Loss = -9804.54331363653
1
Iteration 18500: Loss = -9804.547504452477
2
Iteration 18600: Loss = -9804.559008158929
3
Iteration 18700: Loss = -9804.544477218677
4
Iteration 18800: Loss = -9804.57055119988
5
Iteration 18900: Loss = -9804.547905988395
6
Iteration 19000: Loss = -9804.543707843048
7
Iteration 19100: Loss = -9804.54352293076
8
Iteration 19200: Loss = -9804.543347741888
9
Iteration 19300: Loss = -9804.553605724688
10
Iteration 19400: Loss = -9804.55143234494
11
Iteration 19500: Loss = -9804.543186983197
Iteration 19600: Loss = -9804.544583589084
1
Iteration 19700: Loss = -9804.54320597226
Iteration 19800: Loss = -9804.544956268115
1
Iteration 19900: Loss = -9804.543499541456
2
pi: tensor([[0.8888, 0.1112],
        [0.9436, 0.0564]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8740, 0.1260], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1261, 0.1568],
         [0.6599, 0.2131]],

        [[0.5564, 0.1668],
         [0.5770, 0.5474]],

        [[0.5535, 0.1689],
         [0.5844, 0.6631]],

        [[0.5597, 0.1954],
         [0.6236, 0.5203]],

        [[0.6194, 0.1527],
         [0.6506, 0.6783]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.010101010101010102
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0032546711049559027
Average Adjusted Rand Index: -0.0012309823814612615
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23284.03412660616
Iteration 100: Loss = -9808.36521674406
Iteration 200: Loss = -9806.476443847183
Iteration 300: Loss = -9805.984574136946
Iteration 400: Loss = -9805.627582419906
Iteration 500: Loss = -9805.26573729207
Iteration 600: Loss = -9805.049765848005
Iteration 700: Loss = -9804.955030801722
Iteration 800: Loss = -9804.875470527768
Iteration 900: Loss = -9804.805458672008
Iteration 1000: Loss = -9804.750812167042
Iteration 1100: Loss = -9804.713304194122
Iteration 1200: Loss = -9804.689808214056
Iteration 1300: Loss = -9804.67587615385
Iteration 1400: Loss = -9804.66813072029
Iteration 1500: Loss = -9804.66407458225
Iteration 1600: Loss = -9804.662128497965
Iteration 1700: Loss = -9804.661180734212
Iteration 1800: Loss = -9804.66073503043
Iteration 1900: Loss = -9804.660497713452
Iteration 2000: Loss = -9804.66036988689
Iteration 2100: Loss = -9804.660291816881
Iteration 2200: Loss = -9804.66023675529
Iteration 2300: Loss = -9804.66018383271
Iteration 2400: Loss = -9804.66014875251
Iteration 2500: Loss = -9804.660127629706
Iteration 2600: Loss = -9804.660102692444
Iteration 2700: Loss = -9804.660046049617
Iteration 2800: Loss = -9804.660004281208
Iteration 2900: Loss = -9804.659994405596
Iteration 3000: Loss = -9804.659999917061
Iteration 3100: Loss = -9804.659994395966
Iteration 3200: Loss = -9804.659977200294
Iteration 3300: Loss = -9804.659986409972
Iteration 3400: Loss = -9804.659940577692
Iteration 3500: Loss = -9804.659966883382
Iteration 3600: Loss = -9804.65993558387
Iteration 3700: Loss = -9804.65991635688
Iteration 3800: Loss = -9804.659941658601
Iteration 3900: Loss = -9804.659943528124
Iteration 4000: Loss = -9804.659960013267
Iteration 4100: Loss = -9804.65993614876
Iteration 4200: Loss = -9804.659910923843
Iteration 4300: Loss = -9804.65993867534
Iteration 4400: Loss = -9804.659921162565
Iteration 4500: Loss = -9804.659901453158
Iteration 4600: Loss = -9804.65991485602
Iteration 4700: Loss = -9804.65990254605
Iteration 4800: Loss = -9804.66097871208
1
Iteration 4900: Loss = -9804.659872320053
Iteration 5000: Loss = -9804.659900890289
Iteration 5100: Loss = -9804.659918394316
Iteration 5200: Loss = -9804.660533291148
1
Iteration 5300: Loss = -9804.659903585447
Iteration 5400: Loss = -9804.659898068678
Iteration 5500: Loss = -9804.659917225556
Iteration 5600: Loss = -9804.661101771497
1
Iteration 5700: Loss = -9804.661149398555
2
Iteration 5800: Loss = -9804.660072791672
3
Iteration 5900: Loss = -9804.66025316854
4
Iteration 6000: Loss = -9804.659923561998
Iteration 6100: Loss = -9804.659947241065
Iteration 6200: Loss = -9804.661210798324
1
Iteration 6300: Loss = -9804.65988845291
Iteration 6400: Loss = -9804.660001072956
1
Iteration 6500: Loss = -9804.659872422773
Iteration 6600: Loss = -9804.659931536848
Iteration 6700: Loss = -9804.659956894777
Iteration 6800: Loss = -9804.662399181092
1
Iteration 6900: Loss = -9804.659912808906
Iteration 7000: Loss = -9804.659895759767
Iteration 7100: Loss = -9804.660093457902
1
Iteration 7200: Loss = -9804.659882728865
Iteration 7300: Loss = -9804.663679588724
1
Iteration 7400: Loss = -9804.660038452694
2
Iteration 7500: Loss = -9804.659960236619
Iteration 7600: Loss = -9804.659993047158
Iteration 7700: Loss = -9804.661820247733
1
Iteration 7800: Loss = -9804.65992803471
Iteration 7900: Loss = -9804.659887745578
Iteration 8000: Loss = -9804.660127048866
1
Iteration 8100: Loss = -9804.660384001732
2
Iteration 8200: Loss = -9804.661115342904
3
Iteration 8300: Loss = -9804.660002549776
4
Iteration 8400: Loss = -9804.659966961162
Iteration 8500: Loss = -9804.730141244881
1
Iteration 8600: Loss = -9804.659881771095
Iteration 8700: Loss = -9804.660810919295
1
Iteration 8800: Loss = -9804.659942298087
Iteration 8900: Loss = -9804.659972277163
Iteration 9000: Loss = -9804.666061693852
1
Iteration 9100: Loss = -9804.659882268299
Iteration 9200: Loss = -9804.687481650606
1
Iteration 9300: Loss = -9804.659925559836
Iteration 9400: Loss = -9804.659964048633
Iteration 9500: Loss = -9804.660012714685
Iteration 9600: Loss = -9804.660133501766
1
Iteration 9700: Loss = -9804.663046570877
2
Iteration 9800: Loss = -9804.659907450397
Iteration 9900: Loss = -9804.687029420016
1
Iteration 10000: Loss = -9804.685224973406
2
Iteration 10100: Loss = -9804.659877798518
Iteration 10200: Loss = -9804.661404683924
1
Iteration 10300: Loss = -9804.659878076567
Iteration 10400: Loss = -9804.660324960925
1
Iteration 10500: Loss = -9804.660697416542
2
Iteration 10600: Loss = -9804.661213961132
3
Iteration 10700: Loss = -9804.671899695211
4
Iteration 10800: Loss = -9804.682860244606
5
Iteration 10900: Loss = -9804.659910075063
Iteration 11000: Loss = -9804.659922402767
Iteration 11100: Loss = -9804.663755396616
1
Iteration 11200: Loss = -9804.65996859832
Iteration 11300: Loss = -9804.66028441704
1
Iteration 11400: Loss = -9804.660187148083
2
Iteration 11500: Loss = -9804.660142484123
3
Iteration 11600: Loss = -9804.661576849607
4
Iteration 11700: Loss = -9804.664733915679
5
Iteration 11800: Loss = -9804.664325575945
6
Iteration 11900: Loss = -9804.66002482861
Iteration 12000: Loss = -9804.66605265234
1
Iteration 12100: Loss = -9804.66309309106
2
Iteration 12200: Loss = -9804.660160349727
3
Iteration 12300: Loss = -9804.679920701125
4
Iteration 12400: Loss = -9804.660712482775
5
Iteration 12500: Loss = -9804.66036733531
6
Iteration 12600: Loss = -9804.659988935036
Iteration 12700: Loss = -9804.660392158272
1
Iteration 12800: Loss = -9804.66559673482
2
Iteration 12900: Loss = -9804.68921349976
3
Iteration 13000: Loss = -9804.66142923739
4
Iteration 13100: Loss = -9804.664473923427
5
Iteration 13200: Loss = -9804.669025826763
6
Iteration 13300: Loss = -9804.659917556568
Iteration 13400: Loss = -9804.660000165863
Iteration 13500: Loss = -9804.661206081848
1
Iteration 13600: Loss = -9804.65991864524
Iteration 13700: Loss = -9804.6604915798
1
Iteration 13800: Loss = -9804.65990756064
Iteration 13900: Loss = -9804.706394901526
1
Iteration 14000: Loss = -9804.65991347194
Iteration 14100: Loss = -9804.660937066668
1
Iteration 14200: Loss = -9804.659848319128
Iteration 14300: Loss = -9804.659933741996
Iteration 14400: Loss = -9804.66162106799
1
Iteration 14500: Loss = -9804.660308426395
2
Iteration 14600: Loss = -9804.65989153731
Iteration 14700: Loss = -9804.663088566776
1
Iteration 14800: Loss = -9804.682704729279
2
Iteration 14900: Loss = -9804.699171172191
3
Iteration 15000: Loss = -9804.660163539971
4
Iteration 15100: Loss = -9804.725124849208
5
Iteration 15200: Loss = -9804.659912637535
Iteration 15300: Loss = -9804.659883853346
Iteration 15400: Loss = -9804.66270003375
1
Iteration 15500: Loss = -9804.679528146089
2
Iteration 15600: Loss = -9804.664761732665
3
Iteration 15700: Loss = -9804.659932041723
Iteration 15800: Loss = -9804.660235480642
1
Iteration 15900: Loss = -9804.676444303652
2
Iteration 16000: Loss = -9804.660056116281
3
Iteration 16100: Loss = -9804.660490413506
4
Iteration 16200: Loss = -9804.662767977228
5
Iteration 16300: Loss = -9804.668537843289
6
Iteration 16400: Loss = -9804.678650521151
7
Iteration 16500: Loss = -9804.68127850959
8
Iteration 16600: Loss = -9804.666543554928
9
Iteration 16700: Loss = -9804.661657171619
10
Iteration 16800: Loss = -9804.662981025129
11
Iteration 16900: Loss = -9804.667428996063
12
Iteration 17000: Loss = -9804.68916308934
13
Iteration 17100: Loss = -9804.65995882018
Iteration 17200: Loss = -9804.6621118606
1
Iteration 17300: Loss = -9804.660506095499
2
Iteration 17400: Loss = -9804.665051023327
3
Iteration 17500: Loss = -9804.666223919232
4
Iteration 17600: Loss = -9804.689067297862
5
Iteration 17700: Loss = -9804.660122147678
6
Iteration 17800: Loss = -9804.661687115058
7
Iteration 17900: Loss = -9804.672895100624
8
Iteration 18000: Loss = -9804.659897539905
Iteration 18100: Loss = -9804.659949186012
Iteration 18200: Loss = -9804.66119514022
1
Iteration 18300: Loss = -9804.660030374083
Iteration 18400: Loss = -9804.67598214572
1
Iteration 18500: Loss = -9804.659924602382
Iteration 18600: Loss = -9804.660758606753
1
Iteration 18700: Loss = -9804.685836611534
2
Iteration 18800: Loss = -9804.65998954849
Iteration 18900: Loss = -9804.659992587105
Iteration 19000: Loss = -9804.662109281304
1
Iteration 19100: Loss = -9804.725809752606
2
Iteration 19200: Loss = -9804.659923855155
Iteration 19300: Loss = -9804.66011334809
1
Iteration 19400: Loss = -9804.660011241574
Iteration 19500: Loss = -9804.663831619217
1
Iteration 19600: Loss = -9804.660075502892
Iteration 19700: Loss = -9804.660111839134
Iteration 19800: Loss = -9804.659950729132
Iteration 19900: Loss = -9804.661234872035
1
pi: tensor([[0.3389, 0.6611],
        [0.0242, 0.9758]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2447, 0.7553], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1609, 0.1430],
         [0.6160, 0.1300]],

        [[0.5789, 0.1539],
         [0.6569, 0.7141]],

        [[0.5553, 0.1958],
         [0.5730, 0.5730]],

        [[0.5337, 0.2326],
         [0.6245, 0.6331]],

        [[0.5893, 0.1291],
         [0.6403, 0.5694]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00030059320709693686
Average Adjusted Rand Index: -0.003428561361329936
9949.631588403518
[0.0032546711049559027, 0.00030059320709693686] [-0.0012309823814612615, -0.003428561361329936] [9804.546118432389, 9804.66029893141]
-------------------------------------
This iteration is 53
True Objective function: Loss = -9937.889318808191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25083.40660272062
Iteration 100: Loss = -9797.099481602663
Iteration 200: Loss = -9794.871310964263
Iteration 300: Loss = -9793.96858807893
Iteration 400: Loss = -9793.548277190688
Iteration 500: Loss = -9793.322843614442
Iteration 600: Loss = -9793.171048235981
Iteration 700: Loss = -9793.043201277016
Iteration 800: Loss = -9792.878546612226
Iteration 900: Loss = -9792.629199089315
Iteration 1000: Loss = -9792.350739151128
Iteration 1100: Loss = -9792.060419253634
Iteration 1200: Loss = -9791.77394444161
Iteration 1300: Loss = -9791.485265385883
Iteration 1400: Loss = -9791.166848251652
Iteration 1500: Loss = -9790.745831953333
Iteration 1600: Loss = -9790.073989876997
Iteration 1700: Loss = -9789.476168098161
Iteration 1800: Loss = -9789.097052374607
Iteration 1900: Loss = -9788.698108163626
Iteration 2000: Loss = -9788.397325965718
Iteration 2100: Loss = -9788.164731018738
Iteration 2200: Loss = -9787.903035811074
Iteration 2300: Loss = -9787.718958443582
Iteration 2400: Loss = -9787.582805062671
Iteration 2500: Loss = -9787.467237496154
Iteration 2600: Loss = -9787.362948085974
Iteration 2700: Loss = -9787.253032567234
Iteration 2800: Loss = -9787.140677902653
Iteration 2900: Loss = -9787.028427771194
Iteration 3000: Loss = -9786.963441221924
Iteration 3100: Loss = -9786.917633491279
Iteration 3200: Loss = -9786.880273773657
Iteration 3300: Loss = -9786.853899592697
Iteration 3400: Loss = -9786.838518726592
Iteration 3500: Loss = -9786.83111034174
Iteration 3600: Loss = -9786.827380766863
Iteration 3700: Loss = -9786.825662195482
Iteration 3800: Loss = -9786.824723396998
Iteration 3900: Loss = -9786.824065560715
Iteration 4000: Loss = -9786.8235501405
Iteration 4100: Loss = -9786.82322572839
Iteration 4200: Loss = -9786.822996938798
Iteration 4300: Loss = -9786.822838053246
Iteration 4400: Loss = -9786.82274972562
Iteration 4500: Loss = -9786.823024963367
1
Iteration 4600: Loss = -9786.825354039904
2
Iteration 4700: Loss = -9786.822758062286
Iteration 4800: Loss = -9786.82274778598
Iteration 4900: Loss = -9786.823787274483
1
Iteration 5000: Loss = -9786.82294435629
2
Iteration 5100: Loss = -9786.823357278443
3
Iteration 5200: Loss = -9786.822840153116
Iteration 5300: Loss = -9786.8229461929
1
Iteration 5400: Loss = -9786.823779475364
2
Iteration 5500: Loss = -9786.823359101907
3
Iteration 5600: Loss = -9786.822821651955
Iteration 5700: Loss = -9786.822726604212
Iteration 5800: Loss = -9786.822701109433
Iteration 5900: Loss = -9786.822714264334
Iteration 6000: Loss = -9786.83158094608
1
Iteration 6100: Loss = -9786.822675852241
Iteration 6200: Loss = -9786.822877499864
1
Iteration 6300: Loss = -9786.82272599112
Iteration 6400: Loss = -9786.822922743479
1
Iteration 6500: Loss = -9786.822756800902
Iteration 6600: Loss = -9786.82680212555
1
Iteration 6700: Loss = -9786.82266546986
Iteration 6800: Loss = -9786.822829606555
1
Iteration 6900: Loss = -9786.822917517884
2
Iteration 7000: Loss = -9786.822944926087
3
Iteration 7100: Loss = -9786.82280970737
4
Iteration 7200: Loss = -9786.82268474739
Iteration 7300: Loss = -9786.822729515292
Iteration 7400: Loss = -9786.823768289478
1
Iteration 7500: Loss = -9786.822700704557
Iteration 7600: Loss = -9786.823680730036
1
Iteration 7700: Loss = -9786.822695751249
Iteration 7800: Loss = -9786.824499814436
1
Iteration 7900: Loss = -9786.82269448749
Iteration 8000: Loss = -9786.822944560168
1
Iteration 8100: Loss = -9786.822707145066
Iteration 8200: Loss = -9786.822705315766
Iteration 8300: Loss = -9786.8227035269
Iteration 8400: Loss = -9786.82269003027
Iteration 8500: Loss = -9786.824484378296
1
Iteration 8600: Loss = -9786.822669038882
Iteration 8700: Loss = -9786.866325329154
1
Iteration 8800: Loss = -9786.822682950784
Iteration 8900: Loss = -9786.851164032649
1
Iteration 9000: Loss = -9786.82267971378
Iteration 9100: Loss = -9786.998307809205
1
Iteration 9200: Loss = -9786.822693719185
Iteration 9300: Loss = -9786.82286106057
1
Iteration 9400: Loss = -9786.822691407617
Iteration 9500: Loss = -9786.822683326263
Iteration 9600: Loss = -9786.823224299873
1
Iteration 9700: Loss = -9786.822693658994
Iteration 9800: Loss = -9786.827092614349
1
Iteration 9900: Loss = -9786.822686217098
Iteration 10000: Loss = -9786.832673850884
1
Iteration 10100: Loss = -9786.822680921276
Iteration 10200: Loss = -9786.825760949661
1
Iteration 10300: Loss = -9786.822735494807
Iteration 10400: Loss = -9786.82270054385
Iteration 10500: Loss = -9786.823789717128
1
Iteration 10600: Loss = -9786.822685614074
Iteration 10700: Loss = -9786.822685425992
Iteration 10800: Loss = -9786.82298337918
1
Iteration 10900: Loss = -9786.822712003543
Iteration 11000: Loss = -9786.829890225366
1
Iteration 11100: Loss = -9786.822684189403
Iteration 11200: Loss = -9786.896431288249
1
Iteration 11300: Loss = -9786.82271076064
Iteration 11400: Loss = -9786.822945992313
1
Iteration 11500: Loss = -9786.822757576309
Iteration 11600: Loss = -9786.822746184986
Iteration 11700: Loss = -9786.823021981334
1
Iteration 11800: Loss = -9786.82269724935
Iteration 11900: Loss = -9786.82969251383
1
Iteration 12000: Loss = -9786.822685194413
Iteration 12100: Loss = -9786.837371952595
1
Iteration 12200: Loss = -9786.822712446628
Iteration 12300: Loss = -9786.822689906696
Iteration 12400: Loss = -9786.822842359126
1
Iteration 12500: Loss = -9786.822678951106
Iteration 12600: Loss = -9786.822931424096
1
Iteration 12700: Loss = -9786.822658082423
Iteration 12800: Loss = -9786.832524736912
1
Iteration 12900: Loss = -9786.82270259837
Iteration 13000: Loss = -9786.889979047171
1
Iteration 13100: Loss = -9786.82267918393
Iteration 13200: Loss = -9786.909619546392
1
Iteration 13300: Loss = -9786.822675722397
Iteration 13400: Loss = -9786.82290719509
1
Iteration 13500: Loss = -9786.822725271326
Iteration 13600: Loss = -9786.86238620225
1
Iteration 13700: Loss = -9786.822732043705
Iteration 13800: Loss = -9787.126785335236
1
Iteration 13900: Loss = -9786.822705959845
Iteration 14000: Loss = -9787.029728986709
1
Iteration 14100: Loss = -9786.822700121738
Iteration 14200: Loss = -9787.142112690199
1
Iteration 14300: Loss = -9786.82270990961
Iteration 14400: Loss = -9787.027523941708
1
Iteration 14500: Loss = -9786.822676952692
Iteration 14600: Loss = -9786.888034168787
1
Iteration 14700: Loss = -9786.822686091191
Iteration 14800: Loss = -9786.992955510861
1
Iteration 14900: Loss = -9786.822705605462
Iteration 15000: Loss = -9786.822719182268
Iteration 15100: Loss = -9786.822770589688
Iteration 15200: Loss = -9786.822711685043
Iteration 15300: Loss = -9786.823523388373
1
Iteration 15400: Loss = -9786.822728940146
Iteration 15500: Loss = -9786.823706150855
1
Iteration 15600: Loss = -9786.822708096453
Iteration 15700: Loss = -9786.825769290102
1
Iteration 15800: Loss = -9786.822696658819
Iteration 15900: Loss = -9786.835780102076
1
Iteration 16000: Loss = -9786.822691565603
Iteration 16100: Loss = -9786.884705490185
1
Iteration 16200: Loss = -9786.822672736065
Iteration 16300: Loss = -9786.8494817887
1
Iteration 16400: Loss = -9786.822686903532
Iteration 16500: Loss = -9786.833230443826
1
Iteration 16600: Loss = -9786.822696049343
Iteration 16700: Loss = -9786.826589965103
1
Iteration 16800: Loss = -9786.822707455169
Iteration 16900: Loss = -9786.82332812337
1
Iteration 17000: Loss = -9786.822674540657
Iteration 17100: Loss = -9786.822763092488
Iteration 17200: Loss = -9786.82304440815
1
Iteration 17300: Loss = -9786.82271464181
Iteration 17400: Loss = -9787.087292586015
1
Iteration 17500: Loss = -9786.822663797464
Iteration 17600: Loss = -9786.835981726166
1
Iteration 17700: Loss = -9786.822693983266
Iteration 17800: Loss = -9786.850991783444
1
Iteration 17900: Loss = -9786.822701701913
Iteration 18000: Loss = -9786.836914936255
1
Iteration 18100: Loss = -9786.82268873721
Iteration 18200: Loss = -9786.847279975014
1
Iteration 18300: Loss = -9786.82271264338
Iteration 18400: Loss = -9786.855375301235
1
Iteration 18500: Loss = -9786.82269684793
Iteration 18600: Loss = -9787.10425064682
1
Iteration 18700: Loss = -9786.822702268924
Iteration 18800: Loss = -9786.83077784446
1
Iteration 18900: Loss = -9786.82269809219
Iteration 19000: Loss = -9786.822743072593
Iteration 19100: Loss = -9786.822714597834
Iteration 19200: Loss = -9786.822701165387
Iteration 19300: Loss = -9786.822900973624
1
Iteration 19400: Loss = -9786.822825904133
2
Iteration 19500: Loss = -9786.822779128484
Iteration 19600: Loss = -9786.825855371138
1
Iteration 19700: Loss = -9786.822679415927
Iteration 19800: Loss = -9786.89241294935
1
Iteration 19900: Loss = -9786.822693249082
pi: tensor([[0.6945, 0.3055],
        [0.0602, 0.9398]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1926, 0.8074], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.1502],
         [0.6035, 0.1225]],

        [[0.5720, 0.1733],
         [0.5985, 0.6924]],

        [[0.5499, 0.1479],
         [0.7006, 0.7097]],

        [[0.6469, 0.1622],
         [0.5488, 0.5265]],

        [[0.6537, 0.1488],
         [0.5574, 0.6928]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.006132953674296353
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.011439885601143989
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0002920454130617311
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.005102450998783344
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.04122907359258673
Global Adjusted Rand Index: 0.015391805389384096
Average Adjusted Rand Index: 0.012722463690749735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21298.432986010313
Iteration 100: Loss = -9795.127753093502
Iteration 200: Loss = -9793.998236103818
Iteration 300: Loss = -9793.473231891681
Iteration 400: Loss = -9793.231387720956
Iteration 500: Loss = -9792.997072392582
Iteration 600: Loss = -9792.716982508096
Iteration 700: Loss = -9792.173460647218
Iteration 800: Loss = -9791.015084507924
Iteration 900: Loss = -9790.063541931582
Iteration 1000: Loss = -9789.29440852528
Iteration 1100: Loss = -9788.694478140716
Iteration 1200: Loss = -9788.166462808986
Iteration 1300: Loss = -9787.773525521903
Iteration 1400: Loss = -9787.433443180897
Iteration 1500: Loss = -9787.201577775137
Iteration 1600: Loss = -9787.053000576769
Iteration 1700: Loss = -9786.955967993723
Iteration 1800: Loss = -9786.896625207031
Iteration 1900: Loss = -9786.85920232986
Iteration 2000: Loss = -9786.839262451616
Iteration 2100: Loss = -9786.831566793451
Iteration 2200: Loss = -9786.827524200415
Iteration 2300: Loss = -9786.825243641028
Iteration 2400: Loss = -9786.82396574836
Iteration 2500: Loss = -9786.823237004233
Iteration 2600: Loss = -9786.822870823524
Iteration 2700: Loss = -9786.822774570031
Iteration 2800: Loss = -9786.822699492617
Iteration 2900: Loss = -9786.822672125993
Iteration 3000: Loss = -9786.822685482573
Iteration 3100: Loss = -9786.82270095185
Iteration 3200: Loss = -9786.822701605128
Iteration 3300: Loss = -9786.822697916528
Iteration 3400: Loss = -9786.822710422923
Iteration 3500: Loss = -9786.822684831006
Iteration 3600: Loss = -9786.82270138611
Iteration 3700: Loss = -9786.822705450933
Iteration 3800: Loss = -9786.822680634263
Iteration 3900: Loss = -9786.822684780062
Iteration 4000: Loss = -9786.822699131071
Iteration 4100: Loss = -9786.822673696637
Iteration 4200: Loss = -9786.822724360922
Iteration 4300: Loss = -9786.826851531165
1
Iteration 4400: Loss = -9786.822710867971
Iteration 4500: Loss = -9786.822701525334
Iteration 4600: Loss = -9786.82269680505
Iteration 4700: Loss = -9786.826336156848
1
Iteration 4800: Loss = -9786.823883969655
2
Iteration 4900: Loss = -9786.823276288658
3
Iteration 5000: Loss = -9786.822766424824
Iteration 5100: Loss = -9786.822744449839
Iteration 5200: Loss = -9786.822695313927
Iteration 5300: Loss = -9786.822710886625
Iteration 5400: Loss = -9786.822662317196
Iteration 5500: Loss = -9786.82268078501
Iteration 5600: Loss = -9786.822671116535
Iteration 5700: Loss = -9786.82327064646
1
Iteration 5800: Loss = -9786.82267709739
Iteration 5900: Loss = -9786.847568264737
1
Iteration 6000: Loss = -9786.822672781658
Iteration 6100: Loss = -9786.822685535848
Iteration 6200: Loss = -9786.822711424757
Iteration 6300: Loss = -9786.822700798173
Iteration 6400: Loss = -9786.822733512503
Iteration 6500: Loss = -9786.822681433596
Iteration 6600: Loss = -9786.822718918615
Iteration 6700: Loss = -9786.822711700737
Iteration 6800: Loss = -9786.82268714547
Iteration 6900: Loss = -9786.82272505106
Iteration 7000: Loss = -9786.822684495713
Iteration 7100: Loss = -9786.822758951404
Iteration 7200: Loss = -9786.822704107852
Iteration 7300: Loss = -9786.82275246777
Iteration 7400: Loss = -9786.822716697858
Iteration 7500: Loss = -9786.8227001528
Iteration 7600: Loss = -9786.822744694286
Iteration 7700: Loss = -9786.822704228198
Iteration 7800: Loss = -9786.827275756432
1
Iteration 7900: Loss = -9786.824162129007
2
Iteration 8000: Loss = -9786.82557560425
3
Iteration 8100: Loss = -9786.826401087366
4
Iteration 8200: Loss = -9786.83388749033
5
Iteration 8300: Loss = -9786.82270319115
Iteration 8400: Loss = -9786.822719946242
Iteration 8500: Loss = -9786.83403786787
1
Iteration 8600: Loss = -9786.822695444893
Iteration 8700: Loss = -9786.823567065443
1
Iteration 8800: Loss = -9786.822695154797
Iteration 8900: Loss = -9786.823600765912
1
Iteration 9000: Loss = -9786.82266684265
Iteration 9100: Loss = -9786.822895583571
1
Iteration 9200: Loss = -9786.822700904608
Iteration 9300: Loss = -9786.822723537724
Iteration 9400: Loss = -9786.822656915734
Iteration 9500: Loss = -9786.822875545693
1
Iteration 9600: Loss = -9786.82267727615
Iteration 9700: Loss = -9786.842095640495
1
Iteration 9800: Loss = -9786.822687357435
Iteration 9900: Loss = -9786.822714963118
Iteration 10000: Loss = -9786.822811831626
Iteration 10100: Loss = -9786.822658162575
Iteration 10200: Loss = -9786.846798776429
1
Iteration 10300: Loss = -9786.822718865124
Iteration 10400: Loss = -9787.165080831119
1
Iteration 10500: Loss = -9786.82266899438
Iteration 10600: Loss = -9786.822701596959
Iteration 10700: Loss = -9786.823014459995
1
Iteration 10800: Loss = -9786.82268010953
Iteration 10900: Loss = -9786.92335659518
1
Iteration 11000: Loss = -9786.822702818148
Iteration 11100: Loss = -9786.822689649378
Iteration 11200: Loss = -9786.881674422508
1
Iteration 11300: Loss = -9786.822698623728
Iteration 11400: Loss = -9786.822710600554
Iteration 11500: Loss = -9786.822694875656
Iteration 11600: Loss = -9786.8226991634
Iteration 11700: Loss = -9786.823248755527
1
Iteration 11800: Loss = -9786.82268749923
Iteration 11900: Loss = -9786.82326027052
1
Iteration 12000: Loss = -9786.82266286552
Iteration 12100: Loss = -9786.82328722133
1
Iteration 12200: Loss = -9786.822664464538
Iteration 12300: Loss = -9786.82283644825
1
Iteration 12400: Loss = -9786.822709505986
Iteration 12500: Loss = -9786.82282908034
1
Iteration 12600: Loss = -9786.822712157466
Iteration 12700: Loss = -9786.82314159743
1
Iteration 12800: Loss = -9786.822724494492
Iteration 12900: Loss = -9786.822901309444
1
Iteration 13000: Loss = -9786.822798908623
Iteration 13100: Loss = -9786.82281419923
Iteration 13200: Loss = -9787.020125673329
1
Iteration 13300: Loss = -9786.822665672307
Iteration 13400: Loss = -9786.952953357188
1
Iteration 13500: Loss = -9786.822667481609
Iteration 13600: Loss = -9786.82268495256
Iteration 13700: Loss = -9786.822821040383
1
Iteration 13800: Loss = -9786.822666874337
Iteration 13900: Loss = -9786.822879362217
1
Iteration 14000: Loss = -9786.822698730575
Iteration 14100: Loss = -9786.822855240862
1
Iteration 14200: Loss = -9786.822678681092
Iteration 14300: Loss = -9786.82291924939
1
Iteration 14400: Loss = -9786.822706856465
Iteration 14500: Loss = -9786.822838952454
1
Iteration 14600: Loss = -9786.822815255957
2
Iteration 14700: Loss = -9786.822789924989
Iteration 14800: Loss = -9786.8226846737
Iteration 14900: Loss = -9786.822842711375
1
Iteration 15000: Loss = -9786.822715439623
Iteration 15100: Loss = -9786.823124263505
1
Iteration 15200: Loss = -9786.822701498588
Iteration 15300: Loss = -9786.822887623672
1
Iteration 15400: Loss = -9786.828814239148
2
Iteration 15500: Loss = -9786.822715026763
Iteration 15600: Loss = -9786.835863896877
1
Iteration 15700: Loss = -9786.822712589537
Iteration 15800: Loss = -9786.822668674191
Iteration 15900: Loss = -9786.823274646178
1
Iteration 16000: Loss = -9786.828439575125
2
Iteration 16100: Loss = -9786.822710190703
Iteration 16200: Loss = -9786.856001126378
1
Iteration 16300: Loss = -9786.822710796869
Iteration 16400: Loss = -9786.847585474847
1
Iteration 16500: Loss = -9786.82269309717
Iteration 16600: Loss = -9786.831551697927
1
Iteration 16700: Loss = -9786.822676742893
Iteration 16800: Loss = -9786.828746831527
1
Iteration 16900: Loss = -9786.822651885257
Iteration 17000: Loss = -9786.823173598932
1
Iteration 17100: Loss = -9786.822709889575
Iteration 17200: Loss = -9786.822842745665
1
Iteration 17300: Loss = -9786.822740472167
Iteration 17400: Loss = -9786.822693370395
Iteration 17500: Loss = -9786.88988603477
1
Iteration 17600: Loss = -9786.822701628125
Iteration 17700: Loss = -9786.849018779416
1
Iteration 17800: Loss = -9786.82268047662
Iteration 17900: Loss = -9786.840271607965
1
Iteration 18000: Loss = -9786.822702854071
Iteration 18100: Loss = -9786.822711644927
Iteration 18200: Loss = -9786.822931272372
1
Iteration 18300: Loss = -9786.822763605944
Iteration 18400: Loss = -9786.822799735266
Iteration 18500: Loss = -9786.825633882587
1
Iteration 18600: Loss = -9786.822731812414
Iteration 18700: Loss = -9786.822740987842
Iteration 18800: Loss = -9786.82280434211
Iteration 18900: Loss = -9786.822840566698
Iteration 19000: Loss = -9786.822754125915
Iteration 19100: Loss = -9786.853633231218
1
Iteration 19200: Loss = -9786.822736640805
Iteration 19300: Loss = -9786.838849527925
1
Iteration 19400: Loss = -9786.822678456301
Iteration 19500: Loss = -9786.830158621513
1
Iteration 19600: Loss = -9786.822671811038
Iteration 19700: Loss = -9786.835222584756
1
Iteration 19800: Loss = -9786.822672556716
Iteration 19900: Loss = -9786.823596265178
1
pi: tensor([[0.9379, 0.0621],
        [0.3063, 0.6937]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8035, 0.1965], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1223, 0.1491],
         [0.6391, 0.2008]],

        [[0.6502, 0.1722],
         [0.7151, 0.7273]],

        [[0.6193, 0.1471],
         [0.5854, 0.5771]],

        [[0.6198, 0.1613],
         [0.7139, 0.7301]],

        [[0.5578, 0.1478],
         [0.7151, 0.6720]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.006132953674296353
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.011439885601143989
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0002920454130617311
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.005102450998783344
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.04122907359258673
Global Adjusted Rand Index: 0.015391805389384096
Average Adjusted Rand Index: 0.012722463690749735
9937.889318808191
[0.015391805389384096, 0.015391805389384096] [0.012722463690749735, 0.012722463690749735] [9786.851202444423, 9786.822693239224]
-------------------------------------
This iteration is 54
True Objective function: Loss = -10090.724341317185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24286.219249377948
Iteration 100: Loss = -10018.023634803269
Iteration 200: Loss = -10015.731808772496
Iteration 300: Loss = -10014.645326531505
Iteration 400: Loss = -10013.200508970816
Iteration 500: Loss = -10011.882475209675
Iteration 600: Loss = -10011.029088111265
Iteration 700: Loss = -10010.34768862728
Iteration 800: Loss = -10008.789644159686
Iteration 900: Loss = -10007.50882195333
Iteration 1000: Loss = -10006.614928730565
Iteration 1100: Loss = -10005.939778514505
Iteration 1200: Loss = -10005.070330703038
Iteration 1300: Loss = -10003.7660835107
Iteration 1400: Loss = -10002.574375857721
Iteration 1500: Loss = -10001.954907425776
Iteration 1600: Loss = -10001.3097205492
Iteration 1700: Loss = -10001.198488921113
Iteration 1800: Loss = -10001.053764287562
Iteration 1900: Loss = -10000.800601120744
Iteration 2000: Loss = -10000.724883120281
Iteration 2100: Loss = -10000.669601352165
Iteration 2200: Loss = -10000.587602315913
Iteration 2300: Loss = -10000.563142363932
Iteration 2400: Loss = -10000.547962118713
Iteration 2500: Loss = -10000.536303791776
Iteration 2600: Loss = -10000.526193373973
Iteration 2700: Loss = -10000.515937143098
Iteration 2800: Loss = -10000.500586806995
Iteration 2900: Loss = -10000.49727797754
Iteration 3000: Loss = -10000.48367645099
Iteration 3100: Loss = -10000.47733562917
Iteration 3200: Loss = -10000.476333142165
Iteration 3300: Loss = -10000.347685187735
Iteration 3400: Loss = -10000.342330868745
Iteration 3500: Loss = -10000.340381642702
Iteration 3600: Loss = -10000.334698947687
Iteration 3700: Loss = -10000.328663923186
Iteration 3800: Loss = -10000.279686580687
Iteration 3900: Loss = -10000.271636762
Iteration 4000: Loss = -10000.266589782594
Iteration 4100: Loss = -10000.24782110548
Iteration 4200: Loss = -10000.246901109476
Iteration 4300: Loss = -10000.244076473904
Iteration 4400: Loss = -10000.24279127227
Iteration 4500: Loss = -10000.242221227116
Iteration 4600: Loss = -10000.240542958209
Iteration 4700: Loss = -10000.239269822352
Iteration 4800: Loss = -10000.237801819176
Iteration 4900: Loss = -10000.236791580428
Iteration 5000: Loss = -10000.236046581353
Iteration 5100: Loss = -10000.235463751975
Iteration 5200: Loss = -10000.234809399582
Iteration 5300: Loss = -10000.23426795281
Iteration 5400: Loss = -10000.235582013685
1
Iteration 5500: Loss = -10000.233222724693
Iteration 5600: Loss = -10000.232715436443
Iteration 5700: Loss = -10000.232416139248
Iteration 5800: Loss = -10000.231661732763
Iteration 5900: Loss = -10000.231179520355
Iteration 6000: Loss = -10000.230820007351
Iteration 6100: Loss = -10000.230506306652
Iteration 6200: Loss = -10000.23025213777
Iteration 6300: Loss = -10000.229984823403
Iteration 6400: Loss = -10000.22969884481
Iteration 6500: Loss = -10000.230121293544
1
Iteration 6600: Loss = -10000.229301428652
Iteration 6700: Loss = -10000.229120610424
Iteration 6800: Loss = -10000.228897730873
Iteration 6900: Loss = -10000.228738517864
Iteration 7000: Loss = -10000.239868013106
1
Iteration 7100: Loss = -10000.228370675613
Iteration 7200: Loss = -10000.228209371546
Iteration 7300: Loss = -10000.228051731012
Iteration 7400: Loss = -10000.227632847165
Iteration 7500: Loss = -10000.227322736017
Iteration 7600: Loss = -10000.227177715107
Iteration 7700: Loss = -10000.228197090917
1
Iteration 7800: Loss = -10000.226917264334
Iteration 7900: Loss = -10000.227162571982
1
Iteration 8000: Loss = -10000.226666364293
Iteration 8100: Loss = -10000.226633669512
Iteration 8200: Loss = -10000.226482483784
Iteration 8300: Loss = -10000.22637210409
Iteration 8400: Loss = -10000.228691530481
1
Iteration 8500: Loss = -10000.226202188644
Iteration 8600: Loss = -10000.226178676843
Iteration 8700: Loss = -10000.226083208243
Iteration 8800: Loss = -10000.226924119554
1
Iteration 8900: Loss = -10000.23048411728
2
Iteration 9000: Loss = -10000.225877362704
Iteration 9100: Loss = -10000.225891615357
Iteration 9200: Loss = -10000.225858951488
Iteration 9300: Loss = -10000.22583010011
Iteration 9400: Loss = -10000.22602530354
1
Iteration 9500: Loss = -10000.226176364298
2
Iteration 9600: Loss = -10000.226108674844
3
Iteration 9700: Loss = -10000.248463050517
4
Iteration 9800: Loss = -10000.226150401304
5
Iteration 9900: Loss = -10000.225538830011
Iteration 10000: Loss = -10000.22589993392
1
Iteration 10100: Loss = -10000.282102081857
2
Iteration 10200: Loss = -10000.225462060715
Iteration 10300: Loss = -10000.22786202168
1
Iteration 10400: Loss = -10000.225397869654
Iteration 10500: Loss = -10000.22644332503
1
Iteration 10600: Loss = -10000.28043610618
2
Iteration 10700: Loss = -10000.225282222646
Iteration 10800: Loss = -10000.225269966426
Iteration 10900: Loss = -10000.330946687605
1
Iteration 11000: Loss = -10000.225078199448
Iteration 11100: Loss = -10000.302584981058
1
Iteration 11200: Loss = -10000.227944645752
2
Iteration 11300: Loss = -10000.226402970706
3
Iteration 11400: Loss = -10000.225103449404
Iteration 11500: Loss = -10000.241026453512
1
Iteration 11600: Loss = -10000.224968691478
Iteration 11700: Loss = -10000.225299345084
1
Iteration 11800: Loss = -10000.225169816416
2
Iteration 11900: Loss = -10000.37656538722
3
Iteration 12000: Loss = -10000.224946656823
Iteration 12100: Loss = -10000.225758948825
1
Iteration 12200: Loss = -10000.224906065883
Iteration 12300: Loss = -10000.225794311182
1
Iteration 12400: Loss = -10000.22492811789
Iteration 12500: Loss = -10000.22689745183
1
Iteration 12600: Loss = -10000.298767315446
2
Iteration 12700: Loss = -10000.224896495776
Iteration 12800: Loss = -10000.225306173621
1
Iteration 12900: Loss = -10000.225208173773
2
Iteration 13000: Loss = -10000.22727672793
3
Iteration 13100: Loss = -10000.224835886254
Iteration 13200: Loss = -10000.225181531125
1
Iteration 13300: Loss = -10000.225464479134
2
Iteration 13400: Loss = -10000.224845137196
Iteration 13500: Loss = -10000.224872179519
Iteration 13600: Loss = -10000.22482353865
Iteration 13700: Loss = -10000.225030758178
1
Iteration 13800: Loss = -10000.225136602861
2
Iteration 13900: Loss = -10000.226169828962
3
Iteration 14000: Loss = -10000.248282465198
4
Iteration 14100: Loss = -10000.224787187444
Iteration 14200: Loss = -10000.225911673633
1
Iteration 14300: Loss = -10000.224768843855
Iteration 14400: Loss = -10000.226458320441
1
Iteration 14500: Loss = -10000.224747293976
Iteration 14600: Loss = -10000.22558092529
1
Iteration 14700: Loss = -10000.22480761103
Iteration 14800: Loss = -10000.246742559926
1
Iteration 14900: Loss = -10000.224759517587
Iteration 15000: Loss = -10000.228357386912
1
Iteration 15100: Loss = -10000.2247577324
Iteration 15200: Loss = -10000.224969695983
1
Iteration 15300: Loss = -10000.224868943906
2
Iteration 15400: Loss = -10000.232467466622
3
Iteration 15500: Loss = -10000.224700742303
Iteration 15600: Loss = -10000.224789005662
Iteration 15700: Loss = -10000.224728305755
Iteration 15800: Loss = -10000.226448464462
1
Iteration 15900: Loss = -10000.224705704177
Iteration 16000: Loss = -10000.324612569932
1
Iteration 16100: Loss = -10000.22472081094
Iteration 16200: Loss = -10000.226769063991
1
Iteration 16300: Loss = -10000.282359466397
2
Iteration 16400: Loss = -10000.224718079691
Iteration 16500: Loss = -10000.225124978144
1
Iteration 16600: Loss = -10000.22470172585
Iteration 16700: Loss = -10000.224785174714
Iteration 16800: Loss = -10000.224733277832
Iteration 16900: Loss = -10000.22503553681
1
Iteration 17000: Loss = -10000.224740862217
Iteration 17100: Loss = -10000.783014886609
1
Iteration 17200: Loss = -10000.224666869764
Iteration 17300: Loss = -10000.232765595849
1
Iteration 17400: Loss = -10000.229750763398
2
Iteration 17500: Loss = -10000.224732148296
Iteration 17600: Loss = -10000.22473213878
Iteration 17700: Loss = -10000.226529536374
1
Iteration 17800: Loss = -10000.224700469687
Iteration 17900: Loss = -10000.225574729884
1
Iteration 18000: Loss = -10000.224708227068
Iteration 18100: Loss = -10000.23401468421
1
Iteration 18200: Loss = -10000.242638385931
2
Iteration 18300: Loss = -10000.372748641492
3
Iteration 18400: Loss = -10000.224764576507
Iteration 18500: Loss = -10000.224732584424
Iteration 18600: Loss = -10000.257939573312
1
Iteration 18700: Loss = -10000.224684620827
Iteration 18800: Loss = -10000.225106508879
1
Iteration 18900: Loss = -10000.225152792435
2
Iteration 19000: Loss = -10000.239699336193
3
Iteration 19100: Loss = -10000.22470305449
Iteration 19200: Loss = -10000.224794375681
Iteration 19300: Loss = -10000.224779192056
Iteration 19400: Loss = -10000.224686875932
Iteration 19500: Loss = -10000.224824299685
1
Iteration 19600: Loss = -10000.22467979644
Iteration 19700: Loss = -10000.225515986132
1
Iteration 19800: Loss = -10000.224660683323
Iteration 19900: Loss = -10000.3409137372
1
pi: tensor([[1.0000e+00, 1.2654e-07],
        [1.4649e-01, 8.5351e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0420, 0.9580], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2282, 0.1763],
         [0.6566, 0.1346]],

        [[0.6909, 0.1303],
         [0.6139, 0.6696]],

        [[0.5764, 0.1317],
         [0.6122, 0.6825]],

        [[0.6577, 0.1169],
         [0.5875, 0.5886]],

        [[0.5141, 0.1036],
         [0.5461, 0.6952]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0020727689972979977
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 32
Adjusted Rand Index: 0.1180991578416813
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 31
Adjusted Rand Index: 0.1351646062804004
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 23
Adjusted Rand Index: 0.28455376515470915
Global Adjusted Rand Index: 0.08049847895406777
Average Adjusted Rand Index: 0.10814138770400832
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24718.6422540415
Iteration 100: Loss = -10018.904921783256
Iteration 200: Loss = -10017.571146339007
Iteration 300: Loss = -10017.254854318224
Iteration 400: Loss = -10017.046601429822
Iteration 500: Loss = -10016.809674683553
Iteration 600: Loss = -10016.508982961052
Iteration 700: Loss = -10016.19036660164
Iteration 800: Loss = -10016.018720748212
Iteration 900: Loss = -10015.914540520647
Iteration 1000: Loss = -10015.835250851414
Iteration 1100: Loss = -10015.761885326707
Iteration 1200: Loss = -10015.672484751234
Iteration 1300: Loss = -10015.510138768419
Iteration 1400: Loss = -10015.216271306543
Iteration 1500: Loss = -10014.998204902548
Iteration 1600: Loss = -10014.910069912594
Iteration 1700: Loss = -10014.868074064363
Iteration 1800: Loss = -10014.842749759815
Iteration 1900: Loss = -10014.825593860312
Iteration 2000: Loss = -10014.813354905773
Iteration 2100: Loss = -10014.804428554646
Iteration 2200: Loss = -10014.797785429331
Iteration 2300: Loss = -10014.792810562298
Iteration 2400: Loss = -10014.78890503049
Iteration 2500: Loss = -10014.785832886282
Iteration 2600: Loss = -10014.783329097121
Iteration 2700: Loss = -10014.781233173799
Iteration 2800: Loss = -10014.779422501237
Iteration 2900: Loss = -10014.777850690367
Iteration 3000: Loss = -10014.776438543055
Iteration 3100: Loss = -10014.775185000035
Iteration 3200: Loss = -10014.774039772054
Iteration 3300: Loss = -10014.77301894955
Iteration 3400: Loss = -10014.77203765848
Iteration 3500: Loss = -10014.771170287038
Iteration 3600: Loss = -10014.770356393246
Iteration 3700: Loss = -10014.769584452015
Iteration 3800: Loss = -10014.768928341053
Iteration 3900: Loss = -10014.768229553756
Iteration 4000: Loss = -10014.767638943791
Iteration 4100: Loss = -10014.767037710735
Iteration 4200: Loss = -10014.76653642755
Iteration 4300: Loss = -10014.766015452162
Iteration 4400: Loss = -10014.765553545116
Iteration 4500: Loss = -10014.765105121463
Iteration 4600: Loss = -10014.76470681506
Iteration 4700: Loss = -10014.764314515398
Iteration 4800: Loss = -10014.76392341414
Iteration 4900: Loss = -10014.763589820795
Iteration 5000: Loss = -10014.763248872152
Iteration 5100: Loss = -10014.76294698342
Iteration 5200: Loss = -10014.762640459037
Iteration 5300: Loss = -10014.762367347106
Iteration 5400: Loss = -10014.762105022824
Iteration 5500: Loss = -10014.761843272801
Iteration 5600: Loss = -10014.761631642436
Iteration 5700: Loss = -10014.761397352479
Iteration 5800: Loss = -10014.761142654837
Iteration 5900: Loss = -10014.76122814393
Iteration 6000: Loss = -10014.76076744717
Iteration 6100: Loss = -10014.760730849088
Iteration 6200: Loss = -10014.760413778306
Iteration 6300: Loss = -10014.760267609676
Iteration 6400: Loss = -10014.76038099751
1
Iteration 6500: Loss = -10014.759955109846
Iteration 6600: Loss = -10014.76208141187
1
Iteration 6700: Loss = -10014.759671554944
Iteration 6800: Loss = -10014.759538859249
Iteration 6900: Loss = -10014.75943281592
Iteration 7000: Loss = -10014.759304215753
Iteration 7100: Loss = -10014.759402238706
Iteration 7200: Loss = -10014.75912230873
Iteration 7300: Loss = -10014.759627292899
1
Iteration 7400: Loss = -10014.7588731766
Iteration 7500: Loss = -10014.758806550411
Iteration 7600: Loss = -10014.763706511902
1
Iteration 7700: Loss = -10014.758648500345
Iteration 7800: Loss = -10014.758554009193
Iteration 7900: Loss = -10014.758498106024
Iteration 8000: Loss = -10014.75840660202
Iteration 8100: Loss = -10014.758329093149
Iteration 8200: Loss = -10014.758225572286
Iteration 8300: Loss = -10014.758884913052
1
Iteration 8400: Loss = -10014.758166141366
Iteration 8500: Loss = -10014.764180397891
1
Iteration 8600: Loss = -10014.758028022325
Iteration 8700: Loss = -10014.75808937834
Iteration 8800: Loss = -10014.976125418229
1
Iteration 8900: Loss = -10014.757860776364
Iteration 9000: Loss = -10014.925725355844
1
Iteration 9100: Loss = -10014.757776612869
Iteration 9200: Loss = -10014.75776747823
Iteration 9300: Loss = -10014.757806046195
Iteration 9400: Loss = -10014.75770992597
Iteration 9500: Loss = -10014.757629039213
Iteration 9600: Loss = -10014.760449384417
1
Iteration 9700: Loss = -10014.757601524716
Iteration 9800: Loss = -10014.757669281533
Iteration 9900: Loss = -10014.757532326574
Iteration 10000: Loss = -10014.759886044214
1
Iteration 10100: Loss = -10014.757480213948
Iteration 10200: Loss = -10014.759827831997
1
Iteration 10300: Loss = -10014.757447076148
Iteration 10400: Loss = -10014.757415962602
Iteration 10500: Loss = -10014.763987595263
1
Iteration 10600: Loss = -10014.757405189377
Iteration 10700: Loss = -10014.775192892694
1
Iteration 10800: Loss = -10014.757326863413
Iteration 10900: Loss = -10014.759328099046
1
Iteration 11000: Loss = -10014.757366955499
Iteration 11100: Loss = -10014.758635175005
1
Iteration 11200: Loss = -10014.773922372626
2
Iteration 11300: Loss = -10014.757268836735
Iteration 11400: Loss = -10014.757580491014
1
Iteration 11500: Loss = -10014.757235638639
Iteration 11600: Loss = -10014.757238921919
Iteration 11700: Loss = -10014.757434525265
1
Iteration 11800: Loss = -10014.757209929732
Iteration 11900: Loss = -10014.76098752947
1
Iteration 12000: Loss = -10014.757198192558
Iteration 12100: Loss = -10014.758296875507
1
Iteration 12200: Loss = -10014.757204682961
Iteration 12300: Loss = -10014.757136730648
Iteration 12400: Loss = -10014.761453955824
1
Iteration 12500: Loss = -10014.757126227947
Iteration 12600: Loss = -10014.76159822106
1
Iteration 12700: Loss = -10014.757142802675
Iteration 12800: Loss = -10014.757116939361
Iteration 12900: Loss = -10014.757473993935
1
Iteration 13000: Loss = -10014.75711536456
Iteration 13100: Loss = -10014.791959507833
1
Iteration 13200: Loss = -10014.757075762545
Iteration 13300: Loss = -10014.75707070159
Iteration 13400: Loss = -10014.797765413463
1
Iteration 13500: Loss = -10014.757066772829
Iteration 13600: Loss = -10014.757112726153
Iteration 13700: Loss = -10014.757676001287
1
Iteration 13800: Loss = -10014.757562322095
2
Iteration 13900: Loss = -10014.757123456478
Iteration 14000: Loss = -10014.816007090518
1
Iteration 14100: Loss = -10014.757077021566
Iteration 14200: Loss = -10014.760423472882
1
Iteration 14300: Loss = -10014.757040357865
Iteration 14400: Loss = -10014.758876778991
1
Iteration 14500: Loss = -10014.757032271164
Iteration 14600: Loss = -10014.757079442048
Iteration 14700: Loss = -10014.757039854469
Iteration 14800: Loss = -10014.757972312113
1
Iteration 14900: Loss = -10014.75705605631
Iteration 15000: Loss = -10014.757137140346
Iteration 15100: Loss = -10014.757046816803
Iteration 15200: Loss = -10014.7571124771
Iteration 15300: Loss = -10014.803518436354
1
Iteration 15400: Loss = -10014.75701986685
Iteration 15500: Loss = -10014.76637009584
1
Iteration 15600: Loss = -10014.756999033532
Iteration 15700: Loss = -10014.75937079569
1
Iteration 15800: Loss = -10014.757013054674
Iteration 15900: Loss = -10014.761879553686
1
Iteration 16000: Loss = -10014.75701051914
Iteration 16100: Loss = -10014.757559516127
1
Iteration 16200: Loss = -10014.757010690702
Iteration 16300: Loss = -10014.757400628177
1
Iteration 16400: Loss = -10014.75700424969
Iteration 16500: Loss = -10014.75901504463
1
Iteration 16600: Loss = -10014.757903904812
2
Iteration 16700: Loss = -10014.86707235396
3
Iteration 16800: Loss = -10014.756995357213
Iteration 16900: Loss = -10014.77557172148
1
Iteration 17000: Loss = -10014.75701700892
Iteration 17100: Loss = -10014.757009380965
Iteration 17200: Loss = -10014.759069172986
1
Iteration 17300: Loss = -10014.757016417758
Iteration 17400: Loss = -10014.789731432815
1
Iteration 17500: Loss = -10014.756985525135
Iteration 17600: Loss = -10014.761260021798
1
Iteration 17700: Loss = -10014.757021036108
Iteration 17800: Loss = -10014.761803318543
1
Iteration 17900: Loss = -10014.756994275149
Iteration 18000: Loss = -10014.78711545699
1
Iteration 18100: Loss = -10014.756991388513
Iteration 18200: Loss = -10014.758575590233
1
Iteration 18300: Loss = -10014.756985990627
Iteration 18400: Loss = -10014.76532080581
1
Iteration 18500: Loss = -10014.756991157468
Iteration 18600: Loss = -10014.757029595461
Iteration 18700: Loss = -10014.75716373353
1
Iteration 18800: Loss = -10014.757018788656
Iteration 18900: Loss = -10014.757871706548
1
Iteration 19000: Loss = -10014.757280446169
2
Iteration 19100: Loss = -10014.757045251818
Iteration 19200: Loss = -10014.758725655178
1
Iteration 19300: Loss = -10014.756982600506
Iteration 19400: Loss = -10014.75788076691
1
Iteration 19500: Loss = -10014.757005694892
Iteration 19600: Loss = -10014.764494908826
1
Iteration 19700: Loss = -10014.756982709638
Iteration 19800: Loss = -10014.757010775447
Iteration 19900: Loss = -10014.757187631109
1
pi: tensor([[9.6314e-01, 3.6859e-02],
        [1.0000e+00, 5.0436e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 8.3671e-07], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1407, 0.1730],
         [0.6412, 0.1452]],

        [[0.5071, 0.0778],
         [0.6472, 0.6361]],

        [[0.5225, 0.1692],
         [0.5025, 0.6022]],

        [[0.6602, 0.0788],
         [0.5291, 0.5036]],

        [[0.5308, 0.2032],
         [0.6116, 0.5349]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.016025857647765086
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0024646260762551993
Average Adjusted Rand Index: 0.003205171529553017
10090.724341317185
[0.08049847895406777, 0.0024646260762551993] [0.10814138770400832, 0.003205171529553017] [10000.224693626638, 10014.758721494225]
-------------------------------------
This iteration is 55
True Objective function: Loss = -10104.008521004685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25254.223409210037
Iteration 100: Loss = -10002.138463959993
Iteration 200: Loss = -10001.03653428568
Iteration 300: Loss = -10000.805507027611
Iteration 400: Loss = -10000.652676118956
Iteration 500: Loss = -10000.482742644635
Iteration 600: Loss = -10000.31055915199
Iteration 700: Loss = -10000.114218396644
Iteration 800: Loss = -9999.900699403572
Iteration 900: Loss = -9999.688395222167
Iteration 1000: Loss = -9999.546130584371
Iteration 1100: Loss = -9999.435421512457
Iteration 1200: Loss = -9999.335884217831
Iteration 1300: Loss = -9999.24910008139
Iteration 1400: Loss = -9999.175325384318
Iteration 1500: Loss = -9999.115819326826
Iteration 1600: Loss = -9999.071403152935
Iteration 1700: Loss = -9999.037239983561
Iteration 1800: Loss = -9999.010176916798
Iteration 1900: Loss = -9998.984064830604
Iteration 2000: Loss = -9998.95154410805
Iteration 2100: Loss = -9998.891789336443
Iteration 2200: Loss = -9998.717913846527
Iteration 2300: Loss = -9998.290556752436
Iteration 2400: Loss = -9998.043199227748
Iteration 2500: Loss = -9997.95948074784
Iteration 2600: Loss = -9997.918641729977
Iteration 2700: Loss = -9997.894117563323
Iteration 2800: Loss = -9997.878043185996
Iteration 2900: Loss = -9997.867190139776
Iteration 3000: Loss = -9997.859266447986
Iteration 3100: Loss = -9997.853230198058
Iteration 3200: Loss = -9997.848476245597
Iteration 3300: Loss = -9997.844608007608
Iteration 3400: Loss = -9997.841474894112
Iteration 3500: Loss = -9997.838787200402
Iteration 3600: Loss = -9997.836551651066
Iteration 3700: Loss = -9997.834514560112
Iteration 3800: Loss = -9997.834196127895
Iteration 3900: Loss = -9997.831120183635
Iteration 4000: Loss = -9997.82972844257
Iteration 4100: Loss = -9997.82814714111
Iteration 4200: Loss = -9997.826438789603
Iteration 4300: Loss = -9997.824372852987
Iteration 4400: Loss = -9997.82211883824
Iteration 4500: Loss = -9997.837695335893
1
Iteration 4600: Loss = -9997.814491553065
Iteration 4700: Loss = -9997.79910081631
Iteration 4800: Loss = -9997.214510285416
Iteration 4900: Loss = -9996.56245941667
Iteration 5000: Loss = -9991.849882069075
Iteration 5100: Loss = -9991.719127561144
Iteration 5200: Loss = -9991.702218130047
Iteration 5300: Loss = -9991.694326996352
Iteration 5400: Loss = -9991.688545640556
Iteration 5500: Loss = -9991.68492208255
Iteration 5600: Loss = -9991.682506663761
Iteration 5700: Loss = -9991.680347996898
Iteration 5800: Loss = -9991.678782476849
Iteration 5900: Loss = -9991.677544330852
Iteration 6000: Loss = -9991.676430090081
Iteration 6100: Loss = -9991.675524489201
Iteration 6200: Loss = -9991.674742127103
Iteration 6300: Loss = -9991.674112011002
Iteration 6400: Loss = -9991.674013132424
Iteration 6500: Loss = -9991.673021828274
Iteration 6600: Loss = -9991.672574083233
Iteration 6700: Loss = -9991.673423127786
1
Iteration 6800: Loss = -9991.67185216141
Iteration 6900: Loss = -9991.671539267907
Iteration 7000: Loss = -9991.671337996186
Iteration 7100: Loss = -9991.67098735254
Iteration 7200: Loss = -9991.670800023157
Iteration 7300: Loss = -9991.6705460865
Iteration 7400: Loss = -9991.670344436434
Iteration 7500: Loss = -9991.670169770692
Iteration 7600: Loss = -9991.670019738849
Iteration 7700: Loss = -9991.669980947392
Iteration 7800: Loss = -9991.66976590873
Iteration 7900: Loss = -9991.669660828067
Iteration 8000: Loss = -9991.669509580202
Iteration 8100: Loss = -9991.669350867478
Iteration 8200: Loss = -9991.66927652295
Iteration 8300: Loss = -9991.669309790232
Iteration 8400: Loss = -9991.669091461565
Iteration 8500: Loss = -9991.669279508955
1
Iteration 8600: Loss = -9991.668922213423
Iteration 8700: Loss = -9991.669035164987
1
Iteration 8800: Loss = -9991.668754981134
Iteration 8900: Loss = -9991.669511793758
1
Iteration 9000: Loss = -9991.668665075764
Iteration 9100: Loss = -9991.669944734229
1
Iteration 9200: Loss = -9991.668552921137
Iteration 9300: Loss = -9991.685995759211
1
Iteration 9400: Loss = -9991.668444228591
Iteration 9500: Loss = -9991.711541651857
1
Iteration 9600: Loss = -9991.66849302122
Iteration 9700: Loss = -9991.668336489533
Iteration 9800: Loss = -9991.740166608584
1
Iteration 9900: Loss = -9991.66823597754
Iteration 10000: Loss = -9991.740905702307
1
Iteration 10100: Loss = -9991.668157755512
Iteration 10200: Loss = -9991.706902759712
1
Iteration 10300: Loss = -9991.668130783668
Iteration 10400: Loss = -9991.674664740392
1
Iteration 10500: Loss = -9991.668367763461
2
Iteration 10600: Loss = -9991.66804851979
Iteration 10700: Loss = -9991.668547577414
1
Iteration 10800: Loss = -9991.668117767396
Iteration 10900: Loss = -9991.668938651661
1
Iteration 11000: Loss = -9991.672163191568
2
Iteration 11100: Loss = -9991.691482298325
3
Iteration 11200: Loss = -9991.669174256538
4
Iteration 11300: Loss = -9991.720668914872
5
Iteration 11400: Loss = -9991.68941245591
6
Iteration 11500: Loss = -9991.670717445444
7
Iteration 11600: Loss = -9991.668419501773
8
Iteration 11700: Loss = -9991.668837525429
9
Iteration 11800: Loss = -9991.669409683682
10
Iteration 11900: Loss = -9991.681521513972
11
Iteration 12000: Loss = -9991.668381293777
12
Iteration 12100: Loss = -9991.672120898169
13
Iteration 12200: Loss = -9991.673172715651
14
Iteration 12300: Loss = -9991.676247732308
15
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[9.9999e-01, 1.1586e-05],
        [1.8763e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3997, 0.6003], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1845, 0.1215],
         [0.6845, 0.1271]],

        [[0.5828, 0.1135],
         [0.5951, 0.5928]],

        [[0.5816, 0.1382],
         [0.6023, 0.5911]],

        [[0.6999, 0.1553],
         [0.7105, 0.6838]],

        [[0.6046, 0.1395],
         [0.7111, 0.5290]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 33
Adjusted Rand Index: 0.10666666666666667
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 28
Adjusted Rand Index: 0.18483694778486595
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 34
Adjusted Rand Index: 0.0937030647552171
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.05818181818181818
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0288881259502673
Global Adjusted Rand Index: 0.0956231161427774
Average Adjusted Rand Index: 0.09445532466776704
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23229.36672148234
Iteration 100: Loss = -10004.239002599197
Iteration 200: Loss = -10002.244385031032
Iteration 300: Loss = -10001.644186060385
Iteration 400: Loss = -10001.36090560899
Iteration 500: Loss = -10001.205673617746
Iteration 600: Loss = -10001.114050477529
Iteration 700: Loss = -10001.056375088505
Iteration 800: Loss = -10001.017198349862
Iteration 900: Loss = -10000.988029233145
Iteration 1000: Loss = -10000.96377876272
Iteration 1100: Loss = -10000.941437684865
Iteration 1200: Loss = -10000.918861586102
Iteration 1300: Loss = -10000.894157818135
Iteration 1400: Loss = -10000.865287756951
Iteration 1500: Loss = -10000.829321508907
Iteration 1600: Loss = -10000.781460194055
Iteration 1700: Loss = -10000.713791449896
Iteration 1800: Loss = -10000.613717136528
Iteration 1900: Loss = -10000.46831538298
Iteration 2000: Loss = -10000.287799071622
Iteration 2100: Loss = -10000.104158613252
Iteration 2200: Loss = -9999.923158691043
Iteration 2300: Loss = -9999.749610087789
Iteration 2400: Loss = -9999.605580337902
Iteration 2500: Loss = -9999.483730467919
Iteration 2600: Loss = -9999.390105205868
Iteration 2700: Loss = -9999.326619401741
Iteration 2800: Loss = -9999.26374060225
Iteration 2900: Loss = -9999.186448366681
Iteration 3000: Loss = -9999.115891232203
Iteration 3100: Loss = -9999.066306256374
Iteration 3200: Loss = -9999.047703709153
Iteration 3300: Loss = -9999.020848619937
Iteration 3400: Loss = -9998.998230085586
Iteration 3500: Loss = -9998.968270466106
Iteration 3600: Loss = -9998.930531713766
Iteration 3700: Loss = -9998.860711630337
Iteration 3800: Loss = -9998.554784514676
Iteration 3900: Loss = -9998.06146071587
Iteration 4000: Loss = -9997.947314370205
Iteration 4100: Loss = -9997.907051562468
Iteration 4200: Loss = -9997.883570425593
Iteration 4300: Loss = -9997.87286026363
Iteration 4400: Loss = -9997.860174457524
Iteration 4500: Loss = -9997.853230064335
Iteration 4600: Loss = -9997.847991117575
Iteration 4700: Loss = -9997.84393209282
Iteration 4800: Loss = -9997.840639627522
Iteration 4900: Loss = -9997.838019206769
Iteration 5000: Loss = -9997.835795121448
Iteration 5100: Loss = -9997.833961755412
Iteration 5200: Loss = -9997.832354164486
Iteration 5300: Loss = -9997.831037570844
Iteration 5400: Loss = -9997.837140752456
1
Iteration 5500: Loss = -9997.828829691909
Iteration 5600: Loss = -9997.827961523484
Iteration 5700: Loss = -9997.827166690608
Iteration 5800: Loss = -9997.826465860124
Iteration 5900: Loss = -9997.825830190728
Iteration 6000: Loss = -9997.82520957387
Iteration 6100: Loss = -9997.827111952236
1
Iteration 6200: Loss = -9997.82425244429
Iteration 6300: Loss = -9997.823817394854
Iteration 6400: Loss = -9997.82353561608
Iteration 6500: Loss = -9997.823067490162
Iteration 6600: Loss = -9997.845011967145
1
Iteration 6700: Loss = -9997.8224247151
Iteration 6800: Loss = -9997.82213148238
Iteration 6900: Loss = -9997.821931926363
Iteration 7000: Loss = -9997.82156813774
Iteration 7100: Loss = -9997.821316851972
Iteration 7200: Loss = -9997.8210360104
Iteration 7300: Loss = -9997.820775020291
Iteration 7400: Loss = -9997.821896968124
1
Iteration 7500: Loss = -9997.820283912508
Iteration 7600: Loss = -9997.820091517939
Iteration 7700: Loss = -9997.819833977497
Iteration 7800: Loss = -9997.819609451317
Iteration 7900: Loss = -9997.81930802646
Iteration 8000: Loss = -9997.818910816797
Iteration 8100: Loss = -9997.818136219148
Iteration 8200: Loss = -9997.81544417372
Iteration 8300: Loss = -9997.563249554325
Iteration 8400: Loss = -9997.20179824776
Iteration 8500: Loss = -9996.96700256713
Iteration 8600: Loss = -9991.725020497419
Iteration 8700: Loss = -9991.696287047278
Iteration 8800: Loss = -9991.687949329906
Iteration 8900: Loss = -9991.683490964662
Iteration 9000: Loss = -9991.680675208778
Iteration 9100: Loss = -9991.678750268362
Iteration 9200: Loss = -9991.67731078247
Iteration 9300: Loss = -9991.676135732394
Iteration 9400: Loss = -9991.675098449823
Iteration 9500: Loss = -9991.674024253394
Iteration 9600: Loss = -9991.67634450853
1
Iteration 9700: Loss = -9991.67179174642
Iteration 9800: Loss = -9991.67305758731
1
Iteration 9900: Loss = -9991.671033422877
Iteration 10000: Loss = -9991.675557363982
1
Iteration 10100: Loss = -9991.67056529727
Iteration 10200: Loss = -9991.670563334548
Iteration 10300: Loss = -9991.67016753682
Iteration 10400: Loss = -9991.672865123119
1
Iteration 10500: Loss = -9991.681573364258
2
Iteration 10600: Loss = -9991.697353825748
3
Iteration 10700: Loss = -9991.724604160569
4
Iteration 10800: Loss = -9991.672243420719
5
Iteration 10900: Loss = -9991.66933148236
Iteration 11000: Loss = -9991.714785667442
1
Iteration 11100: Loss = -9991.674336808452
2
Iteration 11200: Loss = -9991.682525196424
3
Iteration 11300: Loss = -9991.684602350297
4
Iteration 11400: Loss = -9991.67347734888
5
Iteration 11500: Loss = -9991.722641834303
6
Iteration 11600: Loss = -9991.698457212282
7
Iteration 11700: Loss = -9991.669462923774
8
Iteration 11800: Loss = -9991.69463460174
9
Iteration 11900: Loss = -9991.70939106014
10
Iteration 12000: Loss = -9991.679261974177
11
Iteration 12100: Loss = -9991.67120978101
12
Iteration 12200: Loss = -9991.669073836643
Iteration 12300: Loss = -9991.671965838232
1
Iteration 12400: Loss = -9991.705939993746
2
Iteration 12500: Loss = -9991.708856978385
3
Iteration 12600: Loss = -9991.668706555703
Iteration 12700: Loss = -9991.686601660145
1
Iteration 12800: Loss = -9991.689044543205
2
Iteration 12900: Loss = -9991.673867483683
3
Iteration 13000: Loss = -9991.67104982181
4
Iteration 13100: Loss = -9991.687214986587
5
Iteration 13200: Loss = -9991.75030280744
6
Iteration 13300: Loss = -9991.705179484472
7
Iteration 13400: Loss = -9991.68581704251
8
Iteration 13500: Loss = -9991.668656664868
Iteration 13600: Loss = -9991.684306033118
1
Iteration 13700: Loss = -9991.672597821234
2
Iteration 13800: Loss = -9991.668875091149
3
Iteration 13900: Loss = -9991.673101325381
4
Iteration 14000: Loss = -9991.681053591532
5
Iteration 14100: Loss = -9991.67307244634
6
Iteration 14200: Loss = -9991.694817141435
7
Iteration 14300: Loss = -9991.686425741642
8
Iteration 14400: Loss = -9991.669049629687
9
Iteration 14500: Loss = -9991.792512917593
10
Iteration 14600: Loss = -9991.670956661113
11
Iteration 14700: Loss = -9991.681004757662
12
Iteration 14800: Loss = -9991.670199920485
13
Iteration 14900: Loss = -9991.68330146872
14
Iteration 15000: Loss = -9991.669796366328
15
Stopping early at iteration 15000 due to no improvement.
pi: tensor([[1.0000e+00, 2.2567e-06],
        [5.1058e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6012, 0.3988], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1275, 0.1215],
         [0.5970, 0.1848]],

        [[0.5398, 0.1136],
         [0.7266, 0.6432]],

        [[0.6870, 0.1385],
         [0.6732, 0.6811]],

        [[0.7086, 0.1554],
         [0.6854, 0.6897]],

        [[0.7205, 0.1398],
         [0.5427, 0.6315]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 67
Adjusted Rand Index: 0.10666666666666667
time is 1
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 72
Adjusted Rand Index: 0.18483694778486595
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 66
Adjusted Rand Index: 0.0937030647552171
time is 3
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.05818181818181818
time is 4
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0288881259502673
Global Adjusted Rand Index: 0.0956231161427774
Average Adjusted Rand Index: 0.09445532466776704
10104.008521004685
[0.0956231161427774, 0.0956231161427774] [0.09445532466776704, 0.09445532466776704] [9991.676247732308, 9991.669796366328]
-------------------------------------
This iteration is 56
True Objective function: Loss = -9960.526099129685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26469.454531976397
Iteration 100: Loss = -9835.272436615227
Iteration 200: Loss = -9832.98653762635
Iteration 300: Loss = -9832.264130682159
Iteration 400: Loss = -9831.844837859535
Iteration 500: Loss = -9831.577971432174
Iteration 600: Loss = -9831.401382073143
Iteration 700: Loss = -9831.267523028027
Iteration 800: Loss = -9831.162045002355
Iteration 900: Loss = -9831.0741322678
Iteration 1000: Loss = -9830.995819751786
Iteration 1100: Loss = -9830.922395584905
Iteration 1200: Loss = -9830.849638421938
Iteration 1300: Loss = -9830.77509285966
Iteration 1400: Loss = -9830.697163608054
Iteration 1500: Loss = -9830.616236970069
Iteration 1600: Loss = -9830.533751565155
Iteration 1700: Loss = -9830.449968865902
Iteration 1800: Loss = -9830.365399228333
Iteration 1900: Loss = -9830.28326622296
Iteration 2000: Loss = -9830.205820339586
Iteration 2100: Loss = -9830.136249371624
Iteration 2200: Loss = -9830.076339277
Iteration 2300: Loss = -9830.024777589751
Iteration 2400: Loss = -9829.978690542646
Iteration 2500: Loss = -9829.936957249016
Iteration 2600: Loss = -9829.899440470677
Iteration 2700: Loss = -9829.867242416436
Iteration 2800: Loss = -9829.83770700814
Iteration 2900: Loss = -9829.808174966545
Iteration 3000: Loss = -9829.775530727095
Iteration 3100: Loss = -9829.734938075906
Iteration 3200: Loss = -9829.683305743536
Iteration 3300: Loss = -9829.619785055645
Iteration 3400: Loss = -9829.549375972141
Iteration 3500: Loss = -9829.469329494232
Iteration 3600: Loss = -9829.40204391445
Iteration 3700: Loss = -9829.342908383951
Iteration 3800: Loss = -9829.272617443332
Iteration 3900: Loss = -9829.24082849337
Iteration 4000: Loss = -9829.233448559271
Iteration 4100: Loss = -9829.222895244196
Iteration 4200: Loss = -9829.221046816527
Iteration 4300: Loss = -9829.219978783996
Iteration 4400: Loss = -9829.219054366376
Iteration 4500: Loss = -9829.219026713075
Iteration 4600: Loss = -9829.216595570639
Iteration 4700: Loss = -9829.215068460111
Iteration 4800: Loss = -9829.213142412926
Iteration 4900: Loss = -9829.21050337433
Iteration 5000: Loss = -9829.211623511303
1
Iteration 5100: Loss = -9829.203007984537
Iteration 5200: Loss = -9829.196818709202
Iteration 5300: Loss = -9829.189106555428
Iteration 5400: Loss = -9829.179892259906
Iteration 5500: Loss = -9829.170617867272
Iteration 5600: Loss = -9829.156006935851
Iteration 5700: Loss = -9829.144797229277
Iteration 5800: Loss = -9829.13843434416
Iteration 5900: Loss = -9829.135558253185
Iteration 6000: Loss = -9829.13341578306
Iteration 6100: Loss = -9829.132403137835
Iteration 6200: Loss = -9829.134587126215
1
Iteration 6300: Loss = -9829.15092766739
2
Iteration 6400: Loss = -9829.131742058855
Iteration 6500: Loss = -9829.132935916341
1
Iteration 6600: Loss = -9829.13175592976
Iteration 6700: Loss = -9829.13231294189
1
Iteration 6800: Loss = -9829.13528297472
2
Iteration 6900: Loss = -9829.13308211251
3
Iteration 7000: Loss = -9829.131745268507
Iteration 7100: Loss = -9829.133888134738
1
Iteration 7200: Loss = -9829.141463246922
2
Iteration 7300: Loss = -9829.132509302312
3
Iteration 7400: Loss = -9829.133026361806
4
Iteration 7500: Loss = -9829.132010696025
5
Iteration 7600: Loss = -9829.131702950284
Iteration 7700: Loss = -9829.137014497423
1
Iteration 7800: Loss = -9829.13172696327
Iteration 7900: Loss = -9829.135168241535
1
Iteration 8000: Loss = -9829.131734294666
Iteration 8100: Loss = -9829.131801140586
Iteration 8200: Loss = -9829.138857118258
1
Iteration 8300: Loss = -9829.13168891268
Iteration 8400: Loss = -9829.13181958626
1
Iteration 8500: Loss = -9829.132539339085
2
Iteration 8600: Loss = -9829.131741534822
Iteration 8700: Loss = -9829.13290430078
1
Iteration 8800: Loss = -9829.13172229715
Iteration 8900: Loss = -9829.13178192263
Iteration 9000: Loss = -9829.2328052795
1
Iteration 9100: Loss = -9829.131744769635
Iteration 9200: Loss = -9829.131845797217
1
Iteration 9300: Loss = -9829.131852733492
2
Iteration 9400: Loss = -9829.13284307795
3
Iteration 9500: Loss = -9829.142934056827
4
Iteration 9600: Loss = -9829.158653115494
5
Iteration 9700: Loss = -9829.134569522483
6
Iteration 9800: Loss = -9829.148975189672
7
Iteration 9900: Loss = -9829.134910111028
8
Iteration 10000: Loss = -9829.229358839939
9
Iteration 10100: Loss = -9829.254286602078
10
Iteration 10200: Loss = -9829.169820581734
11
Iteration 10300: Loss = -9829.134049679102
12
Iteration 10400: Loss = -9829.14806895329
13
Iteration 10500: Loss = -9829.136918742726
14
Iteration 10600: Loss = -9829.131989356021
15
Stopping early at iteration 10600 due to no improvement.
pi: tensor([[0.3252, 0.6748],
        [0.5568, 0.4432]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4282, 0.5718], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1695, 0.1372],
         [0.6815, 0.1127]],

        [[0.7128, 0.1334],
         [0.6624, 0.6145]],

        [[0.6272, 0.1378],
         [0.6857, 0.5907]],

        [[0.6114, 0.1378],
         [0.7077, 0.5048]],

        [[0.6138, 0.1349],
         [0.7003, 0.6741]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 36
Adjusted Rand Index: 0.06944091551974574
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009906444036082724
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.06909090909090909
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.023295934223244545
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 32
Adjusted Rand Index: 0.12085845586074774
Global Adjusted Rand Index: 0.05970115881338373
Average Adjusted Rand Index: 0.05851853174614596
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22121.574240481463
Iteration 100: Loss = -9832.583787105188
Iteration 200: Loss = -9831.996935691886
Iteration 300: Loss = -9831.760419967824
Iteration 400: Loss = -9831.44721189774
Iteration 500: Loss = -9831.195491758781
Iteration 600: Loss = -9831.073585500539
Iteration 700: Loss = -9831.002085253029
Iteration 800: Loss = -9830.947944908648
Iteration 900: Loss = -9830.903113062881
Iteration 1000: Loss = -9830.863804456252
Iteration 1100: Loss = -9830.831417509044
Iteration 1200: Loss = -9830.806235735901
Iteration 1300: Loss = -9830.787750105435
Iteration 1400: Loss = -9830.774832029292
Iteration 1500: Loss = -9830.765997917175
Iteration 1600: Loss = -9830.760038648321
Iteration 1700: Loss = -9830.75599788581
Iteration 1800: Loss = -9830.753255689502
Iteration 1900: Loss = -9830.751270186794
Iteration 2000: Loss = -9830.749778087276
Iteration 2100: Loss = -9830.74857481804
Iteration 2200: Loss = -9830.747518379736
Iteration 2300: Loss = -9830.746588585034
Iteration 2400: Loss = -9830.745683269522
Iteration 2500: Loss = -9830.744769465962
Iteration 2600: Loss = -9830.743877993371
Iteration 2700: Loss = -9830.74289229599
Iteration 2800: Loss = -9830.741883850837
Iteration 2900: Loss = -9830.740733524912
Iteration 3000: Loss = -9830.7393473145
Iteration 3100: Loss = -9830.737414289291
Iteration 3200: Loss = -9830.734058703329
Iteration 3300: Loss = -9830.725122418708
Iteration 3400: Loss = -9830.671866966759
Iteration 3500: Loss = -9830.530311404225
Iteration 3600: Loss = -9830.323252997521
Iteration 3700: Loss = -9829.05278323458
Iteration 3800: Loss = -9828.95575512997
Iteration 3900: Loss = -9828.90632003143
Iteration 4000: Loss = -9828.871250688077
Iteration 4100: Loss = -9828.843607499412
Iteration 4200: Loss = -9828.820653949744
Iteration 4300: Loss = -9828.800758665251
Iteration 4400: Loss = -9828.783344734817
Iteration 4500: Loss = -9828.767980405906
Iteration 4600: Loss = -9828.754160518627
Iteration 4700: Loss = -9828.741545968845
Iteration 4800: Loss = -9828.729977795192
Iteration 4900: Loss = -9828.71929457253
Iteration 5000: Loss = -9828.709330635107
Iteration 5100: Loss = -9828.700015649492
Iteration 5200: Loss = -9828.690910483647
Iteration 5300: Loss = -9828.667384631617
Iteration 5400: Loss = -9828.65921969814
Iteration 5500: Loss = -9828.651636431996
Iteration 5600: Loss = -9828.644581422463
Iteration 5700: Loss = -9828.637951840949
Iteration 5800: Loss = -9828.631770582022
Iteration 5900: Loss = -9828.626000020124
Iteration 6000: Loss = -9828.620601473573
Iteration 6100: Loss = -9828.615544806866
Iteration 6200: Loss = -9828.610845183803
Iteration 6300: Loss = -9828.606417698154
Iteration 6400: Loss = -9828.602282103453
Iteration 6500: Loss = -9828.598410494194
Iteration 6600: Loss = -9828.594821564393
Iteration 6700: Loss = -9828.591405458556
Iteration 6800: Loss = -9828.588219406516
Iteration 6900: Loss = -9828.585271701204
Iteration 7000: Loss = -9828.582451881526
Iteration 7100: Loss = -9828.57988160797
Iteration 7200: Loss = -9828.577421474576
Iteration 7300: Loss = -9828.575131889693
Iteration 7400: Loss = -9828.57298339512
Iteration 7500: Loss = -9828.570988143863
Iteration 7600: Loss = -9828.569125159584
Iteration 7700: Loss = -9828.567324189644
Iteration 7800: Loss = -9828.565673771704
Iteration 7900: Loss = -9828.564127781505
Iteration 8000: Loss = -9828.562668240802
Iteration 8100: Loss = -9828.57675802117
1
Iteration 8200: Loss = -9828.560075823714
Iteration 8300: Loss = -9828.558825600254
Iteration 8400: Loss = -9828.557708287743
Iteration 8500: Loss = -9829.006003074755
1
Iteration 8600: Loss = -9828.55562346888
Iteration 8700: Loss = -9828.55468577625
Iteration 8800: Loss = -9828.553823994518
Iteration 8900: Loss = -9828.55382753449
Iteration 9000: Loss = -9828.552223119286
Iteration 9100: Loss = -9828.551691499546
Iteration 9200: Loss = -9828.550814709599
Iteration 9300: Loss = -9828.562584878442
1
Iteration 9400: Loss = -9828.549559018487
Iteration 9500: Loss = -9828.548984026736
Iteration 9600: Loss = -9828.548499277196
Iteration 9700: Loss = -9828.547967639419
Iteration 9800: Loss = -9828.547489677194
Iteration 9900: Loss = -9828.547035071266
Iteration 10000: Loss = -9828.546677158016
Iteration 10100: Loss = -9828.546288990252
Iteration 10200: Loss = -9828.545834515247
Iteration 10300: Loss = -9828.545571825587
Iteration 10400: Loss = -9828.545122777114
Iteration 10500: Loss = -9828.548205780004
1
Iteration 10600: Loss = -9828.544527956414
Iteration 10700: Loss = -9828.544283768411
Iteration 10800: Loss = -9828.544313637765
Iteration 10900: Loss = -9828.54378884787
Iteration 11000: Loss = -9828.543564622121
Iteration 11100: Loss = -9828.543391715642
Iteration 11200: Loss = -9828.543120165923
Iteration 11300: Loss = -9828.623592403126
1
Iteration 11400: Loss = -9828.542724371806
Iteration 11500: Loss = -9828.542870201725
1
Iteration 11600: Loss = -9828.542433851171
Iteration 11700: Loss = -9828.542321426436
Iteration 11800: Loss = -9828.542165362374
Iteration 11900: Loss = -9828.541996072929
Iteration 12000: Loss = -9828.543487450652
1
Iteration 12100: Loss = -9828.541780976739
Iteration 12200: Loss = -9828.54180246457
Iteration 12300: Loss = -9828.543556198905
1
Iteration 12400: Loss = -9828.542010958696
2
Iteration 12500: Loss = -9828.54137271024
Iteration 12600: Loss = -9828.542137938313
1
Iteration 12700: Loss = -9828.541200556734
Iteration 12800: Loss = -9828.541117511917
Iteration 12900: Loss = -9828.548643950346
1
Iteration 13000: Loss = -9828.54099425297
Iteration 13100: Loss = -9828.540943559277
Iteration 13200: Loss = -9828.563366301998
1
Iteration 13300: Loss = -9828.540839807567
Iteration 13400: Loss = -9828.629496731137
1
Iteration 13500: Loss = -9828.540750546848
Iteration 13600: Loss = -9828.540684098867
Iteration 13700: Loss = -9828.54219310369
1
Iteration 13800: Loss = -9828.540642687918
Iteration 13900: Loss = -9828.54290604829
1
Iteration 14000: Loss = -9828.540550948199
Iteration 14100: Loss = -9828.546937104393
1
Iteration 14200: Loss = -9828.540508490114
Iteration 14300: Loss = -9828.540630456671
1
Iteration 14400: Loss = -9828.5409894703
2
Iteration 14500: Loss = -9828.540491314645
Iteration 14600: Loss = -9828.570125405486
1
Iteration 14700: Loss = -9828.541222396701
2
Iteration 14800: Loss = -9828.557614148653
3
Iteration 14900: Loss = -9828.540404054904
Iteration 15000: Loss = -9828.540547961687
1
Iteration 15100: Loss = -9828.540332944774
Iteration 15200: Loss = -9828.540523235075
1
Iteration 15300: Loss = -9828.540333577408
Iteration 15400: Loss = -9828.74290062153
1
Iteration 15500: Loss = -9828.540319887827
Iteration 15600: Loss = -9828.555575347027
1
Iteration 15700: Loss = -9828.540292031916
Iteration 15800: Loss = -9828.555602023462
1
Iteration 15900: Loss = -9828.540264732268
Iteration 16000: Loss = -9828.54026654216
Iteration 16100: Loss = -9828.541234963117
1
Iteration 16200: Loss = -9828.540380789433
2
Iteration 16300: Loss = -9828.543811191155
3
Iteration 16400: Loss = -9828.540261628192
Iteration 16500: Loss = -9828.542362311327
1
Iteration 16600: Loss = -9828.58441730134
2
Iteration 16700: Loss = -9828.54024873752
Iteration 16800: Loss = -9828.56402002189
1
Iteration 16900: Loss = -9828.541100932918
2
Iteration 17000: Loss = -9828.542693083371
3
Iteration 17100: Loss = -9828.540236816832
Iteration 17200: Loss = -9828.54058079404
1
Iteration 17300: Loss = -9828.55410855137
2
Iteration 17400: Loss = -9828.54224718258
3
Iteration 17500: Loss = -9828.54204848542
4
Iteration 17600: Loss = -9828.540813857502
5
Iteration 17700: Loss = -9828.541171103827
6
Iteration 17800: Loss = -9828.540401030921
7
Iteration 17900: Loss = -9828.54037026568
8
Iteration 18000: Loss = -9828.54092095112
9
Iteration 18100: Loss = -9828.540247753657
Iteration 18200: Loss = -9828.54387611863
1
Iteration 18300: Loss = -9828.541298026728
2
Iteration 18400: Loss = -9828.540772745515
3
Iteration 18500: Loss = -9828.580081191309
4
Iteration 18600: Loss = -9828.540464336193
5
Iteration 18700: Loss = -9828.541158743818
6
Iteration 18800: Loss = -9828.565900977559
7
Iteration 18900: Loss = -9828.540208786144
Iteration 19000: Loss = -9828.540420104251
1
Iteration 19100: Loss = -9828.595379102164
2
Iteration 19200: Loss = -9828.540194527935
Iteration 19300: Loss = -9828.567975441485
1
Iteration 19400: Loss = -9828.540202212433
Iteration 19500: Loss = -9828.605033906972
1
Iteration 19600: Loss = -9828.5402229496
Iteration 19700: Loss = -9828.540213436072
Iteration 19800: Loss = -9828.543323766798
1
Iteration 19900: Loss = -9828.558339920555
2
pi: tensor([[1.0000e+00, 2.9926e-09],
        [7.8839e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9805, 0.0195], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.3600e-01, 5.7051e-02],
         [5.7086e-01, 1.4676e-04]],

        [[5.6357e-01, 9.6787e-02],
         [5.8558e-01, 6.5691e-01]],

        [[6.8970e-01, 1.4313e-01],
         [7.1783e-01, 7.1777e-01]],

        [[5.6056e-01, 2.1294e-01],
         [5.4002e-01, 6.5635e-01]],

        [[5.9487e-01, 1.4832e-01],
         [6.4331e-01, 6.6290e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: 9.890131034615458e-05
Average Adjusted Rand Index: -0.0006806817322669973
9960.526099129685
[0.05970115881338373, 9.890131034615458e-05] [0.05851853174614596, -0.0006806817322669973] [9829.131989356021, 9828.540231750621]
-------------------------------------
This iteration is 57
True Objective function: Loss = -9966.468399846859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22661.304825109135
Iteration 100: Loss = -9840.917017688851
Iteration 200: Loss = -9839.667069818139
Iteration 300: Loss = -9839.191403761915
Iteration 400: Loss = -9838.907706602637
Iteration 500: Loss = -9838.806097065575
Iteration 600: Loss = -9838.740535747134
Iteration 700: Loss = -9838.686464023553
Iteration 800: Loss = -9838.635985739975
Iteration 900: Loss = -9838.584913504059
Iteration 1000: Loss = -9838.526998737567
Iteration 1100: Loss = -9838.442401957433
Iteration 1200: Loss = -9838.289569217597
Iteration 1300: Loss = -9838.127042206885
Iteration 1400: Loss = -9838.010810067613
Iteration 1500: Loss = -9837.91844984416
Iteration 1600: Loss = -9837.836558573514
Iteration 1700: Loss = -9837.744483568611
Iteration 1800: Loss = -9837.618589588248
Iteration 1900: Loss = -9837.446917307792
Iteration 2000: Loss = -9837.289873504831
Iteration 2100: Loss = -9837.177688282827
Iteration 2200: Loss = -9837.097887253383
Iteration 2300: Loss = -9837.036980933126
Iteration 2400: Loss = -9836.988013720027
Iteration 2500: Loss = -9836.948179041967
Iteration 2600: Loss = -9836.91539535047
Iteration 2700: Loss = -9836.887599659087
Iteration 2800: Loss = -9836.8637064623
Iteration 2900: Loss = -9836.842948015003
Iteration 3000: Loss = -9836.824861916437
Iteration 3100: Loss = -9836.809053514038
Iteration 3200: Loss = -9836.79525874864
Iteration 3300: Loss = -9836.783191298546
Iteration 3400: Loss = -9836.772621659702
Iteration 3500: Loss = -9836.763393162135
Iteration 3600: Loss = -9836.7553208927
Iteration 3700: Loss = -9836.748227817381
Iteration 3800: Loss = -9836.741985416502
Iteration 3900: Loss = -9836.736501619618
Iteration 4000: Loss = -9836.731679280902
Iteration 4100: Loss = -9836.727355260588
Iteration 4200: Loss = -9836.72350892132
Iteration 4300: Loss = -9836.72006304665
Iteration 4400: Loss = -9836.716941432773
Iteration 4500: Loss = -9836.714204139314
Iteration 4600: Loss = -9836.711659773548
Iteration 4700: Loss = -9836.70934757752
Iteration 4800: Loss = -9836.70727924227
Iteration 4900: Loss = -9836.70537205858
Iteration 5000: Loss = -9836.703601136538
Iteration 5100: Loss = -9836.702014202421
Iteration 5200: Loss = -9836.700559463587
Iteration 5300: Loss = -9836.699246619493
Iteration 5400: Loss = -9836.698037356564
Iteration 5500: Loss = -9836.696933303067
Iteration 5600: Loss = -9836.695910222688
Iteration 5700: Loss = -9836.694949786306
Iteration 5800: Loss = -9836.69408817281
Iteration 5900: Loss = -9836.693288450851
Iteration 6000: Loss = -9836.692500698644
Iteration 6100: Loss = -9836.691813050054
Iteration 6200: Loss = -9836.691147659109
Iteration 6300: Loss = -9836.690508396741
Iteration 6400: Loss = -9836.689946236836
Iteration 6500: Loss = -9836.68941692581
Iteration 6600: Loss = -9836.688914605525
Iteration 6700: Loss = -9836.688435631602
Iteration 6800: Loss = -9836.687979901364
Iteration 6900: Loss = -9836.687512843182
Iteration 7000: Loss = -9836.687154036574
Iteration 7100: Loss = -9836.686751277022
Iteration 7200: Loss = -9836.686376624186
Iteration 7300: Loss = -9836.68603096821
Iteration 7400: Loss = -9836.685659960567
Iteration 7500: Loss = -9836.685297729837
Iteration 7600: Loss = -9836.68489783797
Iteration 7700: Loss = -9836.684487006787
Iteration 7800: Loss = -9836.684175500197
Iteration 7900: Loss = -9836.683918020686
Iteration 8000: Loss = -9836.683665731312
Iteration 8100: Loss = -9836.68343770809
Iteration 8200: Loss = -9836.683175687256
Iteration 8300: Loss = -9836.683015408853
Iteration 8400: Loss = -9836.68260970947
Iteration 8500: Loss = -9836.685292908707
1
Iteration 8600: Loss = -9836.68222844536
Iteration 8700: Loss = -9836.68205566661
Iteration 8800: Loss = -9836.687558901045
1
Iteration 8900: Loss = -9836.68180687527
Iteration 9000: Loss = -9836.681700327872
Iteration 9100: Loss = -9836.681579032998
Iteration 9200: Loss = -9836.682937447758
1
Iteration 9300: Loss = -9836.681395627264
Iteration 9400: Loss = -9836.68130490962
Iteration 9500: Loss = -9836.681204291926
Iteration 9600: Loss = -9837.141898829512
1
Iteration 9700: Loss = -9836.681065029063
Iteration 9800: Loss = -9836.68096413773
Iteration 9900: Loss = -9836.680889223953
Iteration 10000: Loss = -9836.680854042763
Iteration 10100: Loss = -9836.680889516701
Iteration 10200: Loss = -9836.680713230806
Iteration 10300: Loss = -9836.680639855933
Iteration 10400: Loss = -9836.681509550228
1
Iteration 10500: Loss = -9836.680585021673
Iteration 10600: Loss = -9836.680487110923
Iteration 10700: Loss = -9836.680454704825
Iteration 10800: Loss = -9836.680594055908
1
Iteration 10900: Loss = -9836.680377234223
Iteration 11000: Loss = -9836.68032430116
Iteration 11100: Loss = -9836.680261967376
Iteration 11200: Loss = -9836.73577875553
1
Iteration 11300: Loss = -9836.68023316952
Iteration 11400: Loss = -9836.680194495162
Iteration 11500: Loss = -9836.680169828614
Iteration 11600: Loss = -9836.681727639421
1
Iteration 11700: Loss = -9836.680120507472
Iteration 11800: Loss = -9836.680079681379
Iteration 11900: Loss = -9836.91758629443
1
Iteration 12000: Loss = -9836.680026999882
Iteration 12100: Loss = -9836.680015928729
Iteration 12200: Loss = -9836.679974774444
Iteration 12300: Loss = -9836.71586998822
1
Iteration 12400: Loss = -9836.67995086074
Iteration 12500: Loss = -9836.679933420444
Iteration 12600: Loss = -9836.679933116047
Iteration 12700: Loss = -9836.679924115997
Iteration 12800: Loss = -9836.685811244344
1
Iteration 12900: Loss = -9836.679869322816
Iteration 13000: Loss = -9836.679838880878
Iteration 13100: Loss = -9836.67982712729
Iteration 13200: Loss = -9836.681606870281
1
Iteration 13300: Loss = -9836.679828523032
Iteration 13400: Loss = -9836.679803409883
Iteration 13500: Loss = -9836.67978230841
Iteration 13600: Loss = -9836.67980859956
Iteration 13700: Loss = -9836.679736676946
Iteration 13800: Loss = -9836.679763298016
Iteration 13900: Loss = -9836.67970431098
Iteration 14000: Loss = -9836.679755106765
Iteration 14100: Loss = -9836.679735203532
Iteration 14200: Loss = -9836.679740822668
Iteration 14300: Loss = -9836.679758444989
Iteration 14400: Loss = -9836.858686082933
1
Iteration 14500: Loss = -9836.67973797525
Iteration 14600: Loss = -9836.679725956432
Iteration 14700: Loss = -9837.120039042455
1
Iteration 14800: Loss = -9836.679715304592
Iteration 14900: Loss = -9836.679709367736
Iteration 15000: Loss = -9836.679737827906
Iteration 15100: Loss = -9836.680289851278
1
Iteration 15200: Loss = -9836.67969538711
Iteration 15300: Loss = -9836.67969772431
Iteration 15400: Loss = -9836.683548953739
1
Iteration 15500: Loss = -9836.679674027204
Iteration 15600: Loss = -9836.679690013274
Iteration 15700: Loss = -9836.679664997917
Iteration 15800: Loss = -9836.679739407531
Iteration 15900: Loss = -9836.679257733653
Iteration 16000: Loss = -9836.728158085652
1
Iteration 16100: Loss = -9836.679125383644
Iteration 16200: Loss = -9836.679258443392
1
Iteration 16300: Loss = -9836.679517073047
2
Iteration 16400: Loss = -9836.679125808963
Iteration 16500: Loss = -9836.679193583974
Iteration 16600: Loss = -9836.679005543041
Iteration 16700: Loss = -9836.685207609971
1
Iteration 16800: Loss = -9836.678961429108
Iteration 16900: Loss = -9836.679070616212
1
Iteration 17000: Loss = -9836.679153522267
2
Iteration 17100: Loss = -9836.678958897008
Iteration 17200: Loss = -9836.678977339301
Iteration 17300: Loss = -9836.678945510186
Iteration 17400: Loss = -9836.707687654063
1
Iteration 17500: Loss = -9836.678929423826
Iteration 17600: Loss = -9836.67896996294
Iteration 17700: Loss = -9836.682279288629
1
Iteration 17800: Loss = -9836.67895533239
Iteration 17900: Loss = -9836.678961426309
Iteration 18000: Loss = -9836.67897156467
Iteration 18100: Loss = -9836.678907641837
Iteration 18200: Loss = -9836.678917739495
Iteration 18300: Loss = -9836.681369549966
1
Iteration 18400: Loss = -9836.678887374259
Iteration 18500: Loss = -9836.678851577954
Iteration 18600: Loss = -9836.689741706128
1
Iteration 18700: Loss = -9836.678845445209
Iteration 18800: Loss = -9836.67884816262
Iteration 18900: Loss = -9836.678862168292
Iteration 19000: Loss = -9836.678549294944
Iteration 19100: Loss = -9836.678481268273
Iteration 19200: Loss = -9836.678450112895
Iteration 19300: Loss = -9836.678470957511
Iteration 19400: Loss = -9836.67843977903
Iteration 19500: Loss = -9836.678410656119
Iteration 19600: Loss = -9836.679068668222
1
Iteration 19700: Loss = -9836.67839853259
Iteration 19800: Loss = -9836.678399789967
Iteration 19900: Loss = -9836.685737117932
1
pi: tensor([[1.0000e+00, 2.6794e-06],
        [1.6018e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0137, 0.9863], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3495, 0.2373],
         [0.6478, 0.1349]],

        [[0.5080, 0.1700],
         [0.7265, 0.7137]],

        [[0.5747, 0.1334],
         [0.5294, 0.5183]],

        [[0.6377, 0.2055],
         [0.6741, 0.5801]],

        [[0.5635, 0.1181],
         [0.6393, 0.5069]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.464641282153854e-05
Average Adjusted Rand Index: 0.00042281872429292397
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22129.937785837912
Iteration 100: Loss = -9842.271616658603
Iteration 200: Loss = -9840.250800931251
Iteration 300: Loss = -9839.609654929796
Iteration 400: Loss = -9839.115026371168
Iteration 500: Loss = -9838.871152324356
Iteration 600: Loss = -9838.745151266216
Iteration 700: Loss = -9838.64478155089
Iteration 800: Loss = -9838.53711896128
Iteration 900: Loss = -9838.435996372336
Iteration 1000: Loss = -9838.372937994374
Iteration 1100: Loss = -9838.332012552974
Iteration 1200: Loss = -9838.299706709347
Iteration 1300: Loss = -9838.2781294311
Iteration 1400: Loss = -9838.262164966613
Iteration 1500: Loss = -9838.249390314979
Iteration 1600: Loss = -9838.238899117963
Iteration 1700: Loss = -9838.23031621117
Iteration 1800: Loss = -9838.223294292497
Iteration 1900: Loss = -9838.217585045799
Iteration 2000: Loss = -9838.21302500372
Iteration 2100: Loss = -9838.209394494814
Iteration 2200: Loss = -9838.206523198676
Iteration 2300: Loss = -9838.204305342057
Iteration 2400: Loss = -9838.202599498885
Iteration 2500: Loss = -9838.20126235754
Iteration 2600: Loss = -9838.200284030832
Iteration 2700: Loss = -9838.199456762219
Iteration 2800: Loss = -9838.19881530844
Iteration 2900: Loss = -9838.198325357958
Iteration 3000: Loss = -9838.197945297454
Iteration 3100: Loss = -9838.197592072725
Iteration 3200: Loss = -9838.197372229886
Iteration 3300: Loss = -9838.197146939607
Iteration 3400: Loss = -9838.196960261435
Iteration 3500: Loss = -9838.19684765719
Iteration 3600: Loss = -9838.196724857027
Iteration 3700: Loss = -9838.196626457695
Iteration 3800: Loss = -9838.196555059067
Iteration 3900: Loss = -9838.19648168934
Iteration 4000: Loss = -9838.19639811346
Iteration 4100: Loss = -9838.196337653864
Iteration 4200: Loss = -9838.196290304682
Iteration 4300: Loss = -9838.196265431343
Iteration 4400: Loss = -9838.196213925783
Iteration 4500: Loss = -9838.19618469909
Iteration 4600: Loss = -9838.196179019682
Iteration 4700: Loss = -9838.19613350996
Iteration 4800: Loss = -9838.196089568171
Iteration 4900: Loss = -9838.196083461951
Iteration 5000: Loss = -9838.196082825676
Iteration 5100: Loss = -9838.196049984443
Iteration 5200: Loss = -9838.19601430426
Iteration 5300: Loss = -9838.196013764695
Iteration 5400: Loss = -9838.196009091724
Iteration 5500: Loss = -9838.195993233328
Iteration 5600: Loss = -9838.195963636268
Iteration 5700: Loss = -9838.195945616775
Iteration 5800: Loss = -9838.195960503692
Iteration 5900: Loss = -9838.19592929609
Iteration 6000: Loss = -9838.195932687664
Iteration 6100: Loss = -9838.195979174168
Iteration 6200: Loss = -9838.19592209864
Iteration 6300: Loss = -9838.195901103601
Iteration 6400: Loss = -9838.197484508162
1
Iteration 6500: Loss = -9838.195911541996
Iteration 6600: Loss = -9838.195935276
Iteration 6700: Loss = -9838.195881025933
Iteration 6800: Loss = -9838.195921614813
Iteration 6900: Loss = -9838.195903402266
Iteration 7000: Loss = -9838.195879882329
Iteration 7100: Loss = -9838.195897218795
Iteration 7200: Loss = -9838.201567401504
1
Iteration 7300: Loss = -9838.195866785469
Iteration 7400: Loss = -9838.195898243699
Iteration 7500: Loss = -9838.19620063721
1
Iteration 7600: Loss = -9838.195891776482
Iteration 7700: Loss = -9838.196026528582
1
Iteration 7800: Loss = -9838.195873037015
Iteration 7900: Loss = -9838.19588982998
Iteration 8000: Loss = -9838.199550342053
1
Iteration 8100: Loss = -9838.195872323497
Iteration 8200: Loss = -9838.195894244285
Iteration 8300: Loss = -9838.19590644272
Iteration 8400: Loss = -9838.195847325922
Iteration 8500: Loss = -9838.195900560957
Iteration 8600: Loss = -9838.197233020679
1
Iteration 8700: Loss = -9838.195873379018
Iteration 8800: Loss = -9838.196184609138
1
Iteration 8900: Loss = -9838.195873364004
Iteration 9000: Loss = -9838.1962957882
1
Iteration 9100: Loss = -9838.19587001726
Iteration 9200: Loss = -9838.451073918774
1
Iteration 9300: Loss = -9838.19590281115
Iteration 9400: Loss = -9838.195870084062
Iteration 9500: Loss = -9838.19594779271
Iteration 9600: Loss = -9838.195873901068
Iteration 9700: Loss = -9838.195877998016
Iteration 9800: Loss = -9838.19592636733
Iteration 9900: Loss = -9838.195874602343
Iteration 10000: Loss = -9838.195855031314
Iteration 10100: Loss = -9838.19863412591
1
Iteration 10200: Loss = -9838.195847236353
Iteration 10300: Loss = -9838.195849390928
Iteration 10400: Loss = -9838.197627119154
1
Iteration 10500: Loss = -9838.195850533435
Iteration 10600: Loss = -9838.195866113481
Iteration 10700: Loss = -9838.19723988367
1
Iteration 10800: Loss = -9838.19586100251
Iteration 10900: Loss = -9838.195882934566
Iteration 11000: Loss = -9838.217436725607
1
Iteration 11100: Loss = -9838.195832129952
Iteration 11200: Loss = -9838.195873765373
Iteration 11300: Loss = -9838.229980375952
1
Iteration 11400: Loss = -9838.19586273313
Iteration 11500: Loss = -9838.195894385715
Iteration 11600: Loss = -9838.195985292954
Iteration 11700: Loss = -9838.195894847973
Iteration 11800: Loss = -9838.195857360864
Iteration 11900: Loss = -9838.209787733733
1
Iteration 12000: Loss = -9838.195840472355
Iteration 12100: Loss = -9838.195867738046
Iteration 12200: Loss = -9838.298380919721
1
Iteration 12300: Loss = -9838.195855188593
Iteration 12400: Loss = -9838.19584617742
Iteration 12500: Loss = -9838.195967058224
1
Iteration 12600: Loss = -9838.195897509362
Iteration 12700: Loss = -9838.19589252128
Iteration 12800: Loss = -9838.195976900513
Iteration 12900: Loss = -9838.195873124627
Iteration 13000: Loss = -9838.195932051134
Iteration 13100: Loss = -9838.195846917853
Iteration 13200: Loss = -9838.219147364709
1
Iteration 13300: Loss = -9838.195874336292
Iteration 13400: Loss = -9838.195859596719
Iteration 13500: Loss = -9838.195887281283
Iteration 13600: Loss = -9838.195845567434
Iteration 13700: Loss = -9838.228413128987
1
Iteration 13800: Loss = -9838.197995051572
2
Iteration 13900: Loss = -9838.445466939625
3
Iteration 14000: Loss = -9838.195976685567
4
Iteration 14100: Loss = -9838.396964048072
5
Iteration 14200: Loss = -9838.196167171378
6
Iteration 14300: Loss = -9838.19590845174
Iteration 14400: Loss = -9838.196263066604
1
Iteration 14500: Loss = -9838.19590168618
Iteration 14600: Loss = -9838.361021874769
1
Iteration 14700: Loss = -9838.196082227192
2
Iteration 14800: Loss = -9838.195821303963
Iteration 14900: Loss = -9838.198040576162
1
Iteration 15000: Loss = -9838.195840656563
Iteration 15100: Loss = -9838.318853821287
1
Iteration 15200: Loss = -9838.19586425646
Iteration 15300: Loss = -9838.1969213501
1
Iteration 15400: Loss = -9838.196137137724
2
Iteration 15500: Loss = -9838.196459029594
3
Iteration 15600: Loss = -9838.196443978133
4
Iteration 15700: Loss = -9838.195870611446
Iteration 15800: Loss = -9838.19830242984
1
Iteration 15900: Loss = -9838.195877718048
Iteration 16000: Loss = -9838.19597861735
1
Iteration 16100: Loss = -9838.195915253953
Iteration 16200: Loss = -9838.349709741355
1
Iteration 16300: Loss = -9838.195877610437
Iteration 16400: Loss = -9838.422682591488
1
Iteration 16500: Loss = -9838.195861897808
Iteration 16600: Loss = -9838.196158106282
1
Iteration 16700: Loss = -9838.196610975314
2
Iteration 16800: Loss = -9838.219713223358
3
Iteration 16900: Loss = -9838.196282735653
4
Iteration 17000: Loss = -9838.197564417473
5
Iteration 17100: Loss = -9838.204331327719
6
Iteration 17200: Loss = -9838.215199309601
7
Iteration 17300: Loss = -9838.19588623262
Iteration 17400: Loss = -9838.197013547984
1
Iteration 17500: Loss = -9838.19941561567
2
Iteration 17600: Loss = -9838.195919393876
Iteration 17700: Loss = -9838.229435723964
1
Iteration 17800: Loss = -9838.195850504291
Iteration 17900: Loss = -9838.198491016934
1
Iteration 18000: Loss = -9838.198445458609
2
Iteration 18100: Loss = -9838.195994663094
3
Iteration 18200: Loss = -9838.226043778724
4
Iteration 18300: Loss = -9838.195855515682
Iteration 18400: Loss = -9838.196511216718
1
Iteration 18500: Loss = -9838.195883018554
Iteration 18600: Loss = -9838.196011806127
1
Iteration 18700: Loss = -9838.195890460203
Iteration 18800: Loss = -9838.196164104378
1
Iteration 18900: Loss = -9838.195934565401
Iteration 19000: Loss = -9838.195967448044
Iteration 19100: Loss = -9838.206662628583
1
Iteration 19200: Loss = -9838.195860457146
Iteration 19300: Loss = -9838.213476452027
1
Iteration 19400: Loss = -9838.195865422047
Iteration 19500: Loss = -9838.19667458851
1
Iteration 19600: Loss = -9838.203169978116
2
Iteration 19700: Loss = -9838.195955691492
Iteration 19800: Loss = -9838.203908466616
1
Iteration 19900: Loss = -9838.195868818606
pi: tensor([[0.9815, 0.0185],
        [0.9210, 0.0790]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9486, 0.0514], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1330, 0.1908],
         [0.5918, 0.2707]],

        [[0.5102, 0.1586],
         [0.5855, 0.5608]],

        [[0.6501, 0.2223],
         [0.6210, 0.5408]],

        [[0.6651, 0.1900],
         [0.5559, 0.7103]],

        [[0.6820, 0.1867],
         [0.5501, 0.5761]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.206515228510384e-05
Average Adjusted Rand Index: 0.000319744204636291
9966.468399846859
[6.464641282153854e-05, 3.206515228510384e-05] [0.00042281872429292397, 0.000319744204636291] [9836.678419195143, 9838.196239580366]
-------------------------------------
This iteration is 58
True Objective function: Loss = -9990.587602043353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20861.142562921323
Iteration 100: Loss = -9853.635720041302
Iteration 200: Loss = -9853.030209923207
Iteration 300: Loss = -9852.845232871783
Iteration 400: Loss = -9852.74860936803
Iteration 500: Loss = -9852.688460234715
Iteration 600: Loss = -9852.645232460152
Iteration 700: Loss = -9852.607914137223
Iteration 800: Loss = -9852.56457010375
Iteration 900: Loss = -9852.49269020229
Iteration 1000: Loss = -9852.412140015203
Iteration 1100: Loss = -9852.37918522007
Iteration 1200: Loss = -9852.355280835214
Iteration 1300: Loss = -9852.33508364106
Iteration 1400: Loss = -9852.31786345429
Iteration 1500: Loss = -9852.303235724892
Iteration 1600: Loss = -9852.290914377018
Iteration 1700: Loss = -9852.280691659911
Iteration 1800: Loss = -9852.272309322178
Iteration 1900: Loss = -9852.265473600659
Iteration 2000: Loss = -9852.259969788169
Iteration 2100: Loss = -9852.25549100783
Iteration 2200: Loss = -9852.25179791653
Iteration 2300: Loss = -9852.248789039948
Iteration 2400: Loss = -9852.246334675297
Iteration 2500: Loss = -9852.244201199297
Iteration 2600: Loss = -9852.242423337606
Iteration 2700: Loss = -9852.240825388111
Iteration 2800: Loss = -9852.239395513274
Iteration 2900: Loss = -9852.238122065235
Iteration 3000: Loss = -9852.236961254423
Iteration 3100: Loss = -9852.235843013124
Iteration 3200: Loss = -9852.234787354666
Iteration 3300: Loss = -9852.233759016528
Iteration 3400: Loss = -9852.232772758389
Iteration 3500: Loss = -9852.2318139529
Iteration 3600: Loss = -9852.230816974497
Iteration 3700: Loss = -9852.229878592647
Iteration 3800: Loss = -9852.228905972295
Iteration 3900: Loss = -9852.227978013663
Iteration 4000: Loss = -9852.227068132563
Iteration 4100: Loss = -9852.226167116345
Iteration 4200: Loss = -9852.225260862
Iteration 4300: Loss = -9852.224410079383
Iteration 4400: Loss = -9852.22361021467
Iteration 4500: Loss = -9852.222825312354
Iteration 4600: Loss = -9852.222102262183
Iteration 4700: Loss = -9852.221422701385
Iteration 4800: Loss = -9852.220806498062
Iteration 4900: Loss = -9852.22025975335
Iteration 5000: Loss = -9852.219739081573
Iteration 5100: Loss = -9852.21928495955
Iteration 5200: Loss = -9852.21886770928
Iteration 5300: Loss = -9852.218476494048
Iteration 5400: Loss = -9852.218169922959
Iteration 5500: Loss = -9852.217845005962
Iteration 5600: Loss = -9852.217547550868
Iteration 5700: Loss = -9852.217310230852
Iteration 5800: Loss = -9852.217063034692
Iteration 5900: Loss = -9852.2168230929
Iteration 6000: Loss = -9852.216631354459
Iteration 6100: Loss = -9852.216453411005
Iteration 6200: Loss = -9852.216282629197
Iteration 6300: Loss = -9852.216122980228
Iteration 6400: Loss = -9852.216001417764
Iteration 6500: Loss = -9852.215850298493
Iteration 6600: Loss = -9852.215737122933
Iteration 6700: Loss = -9852.215633739777
Iteration 6800: Loss = -9852.215535005456
Iteration 6900: Loss = -9852.215416878942
Iteration 7000: Loss = -9852.215306677272
Iteration 7100: Loss = -9852.215235513631
Iteration 7200: Loss = -9852.215154284317
Iteration 7300: Loss = -9852.215084527306
Iteration 7400: Loss = -9852.21499056685
Iteration 7500: Loss = -9852.2149514036
Iteration 7600: Loss = -9852.21488985495
Iteration 7700: Loss = -9852.214841993291
Iteration 7800: Loss = -9852.214735857093
Iteration 7900: Loss = -9852.214718380012
Iteration 8000: Loss = -9852.214647607887
Iteration 8100: Loss = -9852.214566137935
Iteration 8200: Loss = -9852.215079974454
1
Iteration 8300: Loss = -9852.214483072024
Iteration 8400: Loss = -9852.214452485503
Iteration 8500: Loss = -9852.21758227634
1
Iteration 8600: Loss = -9852.214372558446
Iteration 8700: Loss = -9852.214316605445
Iteration 8800: Loss = -9852.214621010513
1
Iteration 8900: Loss = -9852.21415317728
Iteration 9000: Loss = -9852.214093898538
Iteration 9100: Loss = -9852.298188734032
1
Iteration 9200: Loss = -9852.214041390114
Iteration 9300: Loss = -9852.21399685484
Iteration 9400: Loss = -9852.213931959774
Iteration 9500: Loss = -9852.21879108411
1
Iteration 9600: Loss = -9852.213863315214
Iteration 9700: Loss = -9852.213790919131
Iteration 9800: Loss = -9852.213747072761
Iteration 9900: Loss = -9852.21426996597
1
Iteration 10000: Loss = -9852.213625470962
Iteration 10100: Loss = -9852.21364816892
Iteration 10200: Loss = -9852.251637832154
1
Iteration 10300: Loss = -9852.213515579839
Iteration 10400: Loss = -9852.213502245699
Iteration 10500: Loss = -9852.213450721063
Iteration 10600: Loss = -9852.214654621032
1
Iteration 10700: Loss = -9852.213373735052
Iteration 10800: Loss = -9852.21338072615
Iteration 10900: Loss = -9852.563346647275
1
Iteration 11000: Loss = -9852.213320661802
Iteration 11100: Loss = -9852.213273369263
Iteration 11200: Loss = -9852.216869622138
1
Iteration 11300: Loss = -9852.213185470202
Iteration 11400: Loss = -9852.213180925659
Iteration 11500: Loss = -9852.213151456264
Iteration 11600: Loss = -9852.217321256385
1
Iteration 11700: Loss = -9852.213123319047
Iteration 11800: Loss = -9852.21312158047
Iteration 11900: Loss = -9852.213109931356
Iteration 12000: Loss = -9852.215243196733
1
Iteration 12100: Loss = -9852.213070544265
Iteration 12200: Loss = -9852.213050337243
Iteration 12300: Loss = -9852.704870336587
1
Iteration 12400: Loss = -9852.213020051971
Iteration 12500: Loss = -9852.213019745903
Iteration 12600: Loss = -9852.212974721753
Iteration 12700: Loss = -9852.21307690412
1
Iteration 12800: Loss = -9852.212949693501
Iteration 12900: Loss = -9852.212959741144
Iteration 13000: Loss = -9852.26567942213
1
Iteration 13100: Loss = -9852.21294221247
Iteration 13200: Loss = -9852.212937828383
Iteration 13300: Loss = -9852.212931965025
Iteration 13400: Loss = -9852.213362779565
1
Iteration 13500: Loss = -9852.212889827637
Iteration 13600: Loss = -9852.212877467264
Iteration 13700: Loss = -9852.21389650514
1
Iteration 13800: Loss = -9852.212904755395
Iteration 13900: Loss = -9852.219552896264
1
Iteration 14000: Loss = -9852.214343085352
2
Iteration 14100: Loss = -9852.212872203785
Iteration 14200: Loss = -9852.213403307029
1
Iteration 14300: Loss = -9852.21286469979
Iteration 14400: Loss = -9852.21287810025
Iteration 14500: Loss = -9852.21310503638
1
Iteration 14600: Loss = -9852.212857159422
Iteration 14700: Loss = -9852.213055521885
1
Iteration 14800: Loss = -9852.212881902182
Iteration 14900: Loss = -9852.212991809705
1
Iteration 15000: Loss = -9852.234642555537
2
Iteration 15100: Loss = -9852.215621848316
3
Iteration 15200: Loss = -9852.212832116724
Iteration 15300: Loss = -9852.246299472088
1
Iteration 15400: Loss = -9852.212843340882
Iteration 15500: Loss = -9852.21292078016
Iteration 15600: Loss = -9852.399827231293
1
Iteration 15700: Loss = -9852.213024796816
2
Iteration 15800: Loss = -9852.212825619503
Iteration 15900: Loss = -9852.281942463002
1
Iteration 16000: Loss = -9852.212787580915
Iteration 16100: Loss = -9852.212845068143
Iteration 16200: Loss = -9852.490853157893
1
Iteration 16300: Loss = -9852.212884175693
Iteration 16400: Loss = -9852.213791506798
1
Iteration 16500: Loss = -9852.225260282492
2
Iteration 16600: Loss = -9852.212891912532
Iteration 16700: Loss = -9852.213607857397
1
Iteration 16800: Loss = -9852.212848049368
Iteration 16900: Loss = -9852.212798253093
Iteration 17000: Loss = -9852.231628064757
1
Iteration 17100: Loss = -9852.213421064884
2
Iteration 17200: Loss = -9852.212800359928
Iteration 17300: Loss = -9852.213334940941
1
Iteration 17400: Loss = -9852.212805973537
Iteration 17500: Loss = -9852.215029234205
1
Iteration 17600: Loss = -9852.212828894375
Iteration 17700: Loss = -9852.212829504773
Iteration 17800: Loss = -9852.212866064521
Iteration 17900: Loss = -9852.212906368202
Iteration 18000: Loss = -9852.212803888096
Iteration 18100: Loss = -9852.212803655162
Iteration 18200: Loss = -9852.212812758982
Iteration 18300: Loss = -9852.213049062655
1
Iteration 18400: Loss = -9852.21284140699
Iteration 18500: Loss = -9852.212776886327
Iteration 18600: Loss = -9852.213451341451
1
Iteration 18700: Loss = -9852.215026306323
2
Iteration 18800: Loss = -9852.22155414712
3
Iteration 18900: Loss = -9852.212764128433
Iteration 19000: Loss = -9852.238633492207
1
Iteration 19100: Loss = -9852.214636802843
2
Iteration 19200: Loss = -9852.212792180446
Iteration 19300: Loss = -9852.216048518603
1
Iteration 19400: Loss = -9852.212817758154
Iteration 19500: Loss = -9852.242874240574
1
Iteration 19600: Loss = -9852.21279634861
Iteration 19700: Loss = -9852.214225160034
1
Iteration 19800: Loss = -9852.212766529397
Iteration 19900: Loss = -9852.214789274523
1
pi: tensor([[2.3407e-05, 9.9998e-01],
        [1.7239e-03, 9.9828e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0167, 0.9833], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3482, 0.2174],
         [0.5280, 0.1356]],

        [[0.5383, 0.1257],
         [0.5303, 0.5692]],

        [[0.5488, 0.1894],
         [0.6067, 0.7060]],

        [[0.6905, 0.1587],
         [0.5175, 0.5220]],

        [[0.6064, 0.1434],
         [0.6003, 0.6068]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22782.04555665823
Iteration 100: Loss = -9854.184087423719
Iteration 200: Loss = -9853.16311667895
Iteration 300: Loss = -9852.88935422218
Iteration 400: Loss = -9852.760095521226
Iteration 500: Loss = -9852.685962446603
Iteration 600: Loss = -9852.633344385127
Iteration 700: Loss = -9852.583233718646
Iteration 800: Loss = -9852.51306253249
Iteration 900: Loss = -9852.416771054419
Iteration 1000: Loss = -9852.367199466
Iteration 1100: Loss = -9852.338745666153
Iteration 1200: Loss = -9852.316474541794
Iteration 1300: Loss = -9852.298629835648
Iteration 1400: Loss = -9852.284203270387
Iteration 1500: Loss = -9852.272562898896
Iteration 1600: Loss = -9852.263158413527
Iteration 1700: Loss = -9852.255557183462
Iteration 1800: Loss = -9852.24950177659
Iteration 1900: Loss = -9852.244583847243
Iteration 2000: Loss = -9852.240642344312
Iteration 2100: Loss = -9852.237420960166
Iteration 2200: Loss = -9852.234796237019
Iteration 2300: Loss = -9852.23265462976
Iteration 2400: Loss = -9852.230938500334
Iteration 2500: Loss = -9852.229459740827
Iteration 2600: Loss = -9852.228248672414
Iteration 2700: Loss = -9852.227168522893
Iteration 2800: Loss = -9852.226257290387
Iteration 2900: Loss = -9852.225461290462
Iteration 3000: Loss = -9852.224723911251
Iteration 3100: Loss = -9852.224089999156
Iteration 3200: Loss = -9852.223508280147
Iteration 3300: Loss = -9852.222972500462
Iteration 3400: Loss = -9852.2225011928
Iteration 3500: Loss = -9852.22203292244
Iteration 3600: Loss = -9852.221600946403
Iteration 3700: Loss = -9852.221205506274
Iteration 3800: Loss = -9852.220817069243
Iteration 3900: Loss = -9852.220425717875
Iteration 4000: Loss = -9852.22009984419
Iteration 4100: Loss = -9852.219781705739
Iteration 4200: Loss = -9852.219453875785
Iteration 4300: Loss = -9852.219183213816
Iteration 4400: Loss = -9852.218886686715
Iteration 4500: Loss = -9852.21862150582
Iteration 4600: Loss = -9852.218387982086
Iteration 4700: Loss = -9852.218109138603
Iteration 4800: Loss = -9852.217902529022
Iteration 4900: Loss = -9852.217670569025
Iteration 5000: Loss = -9852.21747418478
Iteration 5100: Loss = -9852.217268311786
Iteration 5200: Loss = -9852.217065515497
Iteration 5300: Loss = -9852.21692505169
Iteration 5400: Loss = -9852.216776856028
Iteration 5500: Loss = -9852.216587053443
Iteration 5600: Loss = -9852.216462606331
Iteration 5700: Loss = -9852.216307865781
Iteration 5800: Loss = -9852.21614519872
Iteration 5900: Loss = -9852.216055196059
Iteration 6000: Loss = -9852.215926661775
Iteration 6100: Loss = -9852.21583875032
Iteration 6200: Loss = -9852.215722073382
Iteration 6300: Loss = -9852.215635809704
Iteration 6400: Loss = -9852.215548091559
Iteration 6500: Loss = -9852.215451113607
Iteration 6600: Loss = -9852.215364268952
Iteration 6700: Loss = -9852.215286889768
Iteration 6800: Loss = -9852.215225421449
Iteration 6900: Loss = -9852.215171419044
Iteration 7000: Loss = -9852.215064927483
Iteration 7100: Loss = -9852.215007324616
Iteration 7200: Loss = -9852.21493918034
Iteration 7300: Loss = -9852.214909865825
Iteration 7400: Loss = -9852.21488764051
Iteration 7500: Loss = -9852.214811074251
Iteration 7600: Loss = -9852.214729054633
Iteration 7700: Loss = -9852.21471363709
Iteration 7800: Loss = -9852.214656218728
Iteration 7900: Loss = -9852.214577092662
Iteration 8000: Loss = -9852.214558708041
Iteration 8100: Loss = -9852.214498807289
Iteration 8200: Loss = -9852.214459103558
Iteration 8300: Loss = -9852.214410641263
Iteration 8400: Loss = -9852.281023903832
1
Iteration 8500: Loss = -9852.214355384667
Iteration 8600: Loss = -9852.214283159217
Iteration 8700: Loss = -9852.214237797672
Iteration 8800: Loss = -9852.214173697119
Iteration 8900: Loss = -9852.214164360013
Iteration 9000: Loss = -9852.214124747197
Iteration 9100: Loss = -9852.214032310143
Iteration 9200: Loss = -9852.213975890041
Iteration 9300: Loss = -9852.214138026184
1
Iteration 9400: Loss = -9852.213888116808
Iteration 9500: Loss = -9852.213847082838
Iteration 9600: Loss = -9852.285470578368
1
Iteration 9700: Loss = -9852.2137483998
Iteration 9800: Loss = -9852.213737223314
Iteration 9900: Loss = -9852.213695805092
Iteration 10000: Loss = -9852.213771063085
Iteration 10100: Loss = -9852.213588355724
Iteration 10200: Loss = -9852.213571682893
Iteration 10300: Loss = -9852.22773697917
1
Iteration 10400: Loss = -9852.213490623268
Iteration 10500: Loss = -9852.21344439087
Iteration 10600: Loss = -9852.21669802014
1
Iteration 10700: Loss = -9852.213378973176
Iteration 10800: Loss = -9852.21335902644
Iteration 10900: Loss = -9852.213350923328
Iteration 11000: Loss = -9852.214021911164
1
Iteration 11100: Loss = -9852.213272104063
Iteration 11200: Loss = -9852.213215586069
Iteration 11300: Loss = -9852.21321510342
Iteration 11400: Loss = -9852.213229092536
Iteration 11500: Loss = -9852.213169646815
Iteration 11600: Loss = -9852.213130699809
Iteration 11700: Loss = -9852.451376822715
1
Iteration 11800: Loss = -9852.21311322205
Iteration 11900: Loss = -9852.213125703202
Iteration 12000: Loss = -9852.21309187746
Iteration 12100: Loss = -9852.475677958915
1
Iteration 12200: Loss = -9852.213063944937
Iteration 12300: Loss = -9852.21305315024
Iteration 12400: Loss = -9852.213046152772
Iteration 12500: Loss = -9852.213636324075
1
Iteration 12600: Loss = -9852.212987146671
Iteration 12700: Loss = -9852.212966291961
Iteration 12800: Loss = -9852.213001723389
Iteration 12900: Loss = -9852.22120423596
1
Iteration 13000: Loss = -9852.212945171837
Iteration 13100: Loss = -9852.212941892189
Iteration 13200: Loss = -9852.212916274892
Iteration 13300: Loss = -9852.212949863368
Iteration 13400: Loss = -9852.212939090254
Iteration 13500: Loss = -9852.212899800055
Iteration 13600: Loss = -9852.233463129247
1
Iteration 13700: Loss = -9852.212863780229
Iteration 13800: Loss = -9852.212889881997
Iteration 13900: Loss = -9852.212887995469
Iteration 14000: Loss = -9852.213564271873
1
Iteration 14100: Loss = -9852.212864591691
Iteration 14200: Loss = -9852.21296162186
Iteration 14300: Loss = -9852.214329623881
1
Iteration 14400: Loss = -9852.212850690301
Iteration 14500: Loss = -9852.21288780902
Iteration 14600: Loss = -9852.213479416387
1
Iteration 14700: Loss = -9852.223691789612
2
Iteration 14800: Loss = -9852.212844911597
Iteration 14900: Loss = -9852.212866674015
Iteration 15000: Loss = -9852.215420808028
1
Iteration 15100: Loss = -9852.212833217342
Iteration 15200: Loss = -9852.21285904232
Iteration 15300: Loss = -9852.266601712114
1
Iteration 15400: Loss = -9852.213017478727
2
Iteration 15500: Loss = -9852.21281355611
Iteration 15600: Loss = -9852.215355764241
1
Iteration 15700: Loss = -9852.21288935977
Iteration 15800: Loss = -9852.228635022911
1
Iteration 15900: Loss = -9852.212828539608
Iteration 16000: Loss = -9852.213529824474
1
Iteration 16100: Loss = -9852.212819388082
Iteration 16200: Loss = -9852.212825111961
Iteration 16300: Loss = -9852.54762588092
1
Iteration 16400: Loss = -9852.21277079374
Iteration 16500: Loss = -9852.212816750105
Iteration 16600: Loss = -9852.602419682567
1
Iteration 16700: Loss = -9852.212951093954
2
Iteration 16800: Loss = -9852.212851363889
Iteration 16900: Loss = -9852.230734743604
1
Iteration 17000: Loss = -9852.212819621798
Iteration 17100: Loss = -9852.308913070794
1
Iteration 17200: Loss = -9852.212938005852
2
Iteration 17300: Loss = -9852.212950125848
3
Iteration 17400: Loss = -9852.21305256352
4
Iteration 17500: Loss = -9852.212794481531
Iteration 17600: Loss = -9852.216915722393
1
Iteration 17700: Loss = -9852.212793352659
Iteration 17800: Loss = -9852.214954990739
1
Iteration 17900: Loss = -9852.213525491048
2
Iteration 18000: Loss = -9852.212801926216
Iteration 18100: Loss = -9852.21383815816
1
Iteration 18200: Loss = -9852.213113566746
2
Iteration 18300: Loss = -9852.212900235709
Iteration 18400: Loss = -9852.212833391639
Iteration 18500: Loss = -9852.21280274705
Iteration 18600: Loss = -9852.212798246534
Iteration 18700: Loss = -9852.21278618664
Iteration 18800: Loss = -9852.218907084418
1
Iteration 18900: Loss = -9852.212802375758
Iteration 19000: Loss = -9852.21338559295
1
Iteration 19100: Loss = -9852.214219507965
2
Iteration 19200: Loss = -9852.458298055579
3
Iteration 19300: Loss = -9852.212805715595
Iteration 19400: Loss = -9852.222912058225
1
Iteration 19500: Loss = -9852.212795026202
Iteration 19600: Loss = -9852.212823632883
Iteration 19700: Loss = -9852.212842313495
Iteration 19800: Loss = -9852.212795143989
Iteration 19900: Loss = -9852.412380811189
1
pi: tensor([[2.4161e-05, 9.9998e-01],
        [1.7217e-03, 9.9828e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0167, 0.9833], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3482, 0.2174],
         [0.5055, 0.1356]],

        [[0.6338, 0.1257],
         [0.6968, 0.5800]],

        [[0.7113, 0.1894],
         [0.6162, 0.6705]],

        [[0.6622, 0.1587],
         [0.6448, 0.6891]],

        [[0.7131, 0.1434],
         [0.6984, 0.6304]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
9990.587602043353
[0.0, 0.0] [0.0, 0.0] [9852.212793160157, 9852.212812096614]
-------------------------------------
This iteration is 59
True Objective function: Loss = -10081.171668513682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18717.638766835324
Iteration 100: Loss = -9957.434877034924
Iteration 200: Loss = -9956.756159856008
Iteration 300: Loss = -9954.882778610428
Iteration 400: Loss = -9954.63257674425
Iteration 500: Loss = -9954.540033231187
Iteration 600: Loss = -9954.478592976522
Iteration 700: Loss = -9954.392820562325
Iteration 800: Loss = -9954.255588338312
Iteration 900: Loss = -9954.100717979112
Iteration 1000: Loss = -9954.007353842302
Iteration 1100: Loss = -9953.956828733266
Iteration 1200: Loss = -9953.926233616377
Iteration 1300: Loss = -9953.90721006775
Iteration 1400: Loss = -9953.895754779802
Iteration 1500: Loss = -9953.889297482818
Iteration 1600: Loss = -9953.885849783695
Iteration 1700: Loss = -9953.884010080124
Iteration 1800: Loss = -9953.88302747894
Iteration 1900: Loss = -9953.88236016952
Iteration 2000: Loss = -9953.881904919523
Iteration 2100: Loss = -9953.88147826821
Iteration 2200: Loss = -9953.881072748072
Iteration 2300: Loss = -9953.880687613186
Iteration 2400: Loss = -9953.88032824458
Iteration 2500: Loss = -9953.879957069481
Iteration 2600: Loss = -9953.879609071219
Iteration 2700: Loss = -9953.879283215965
Iteration 2800: Loss = -9953.87896626431
Iteration 2900: Loss = -9953.878641542644
Iteration 3000: Loss = -9953.878338177708
Iteration 3100: Loss = -9953.878055936217
Iteration 3200: Loss = -9953.877808325435
Iteration 3300: Loss = -9953.877514036101
Iteration 3400: Loss = -9953.877252439386
Iteration 3500: Loss = -9953.876999032294
Iteration 3600: Loss = -9953.876771747306
Iteration 3700: Loss = -9953.876515006032
Iteration 3800: Loss = -9953.87630666863
Iteration 3900: Loss = -9953.87608900317
Iteration 4000: Loss = -9953.875899014864
Iteration 4100: Loss = -9953.875737875127
Iteration 4200: Loss = -9953.875642676841
Iteration 4300: Loss = -9953.87537360271
Iteration 4400: Loss = -9953.88329986733
1
Iteration 4500: Loss = -9953.87502302593
Iteration 4600: Loss = -9953.874900300587
Iteration 4700: Loss = -9953.874771635974
Iteration 4800: Loss = -9953.874582866732
Iteration 4900: Loss = -9953.887974656003
1
Iteration 5000: Loss = -9953.87433808597
Iteration 5100: Loss = -9953.874206207936
Iteration 5200: Loss = -9953.874116041736
Iteration 5300: Loss = -9953.873960244437
Iteration 5400: Loss = -9953.882607915113
1
Iteration 5500: Loss = -9953.8737368628
Iteration 5600: Loss = -9953.873681836056
Iteration 5700: Loss = -9953.873585701307
Iteration 5800: Loss = -9953.873670847735
Iteration 5900: Loss = -9953.873433451376
Iteration 6000: Loss = -9953.873378133423
Iteration 6100: Loss = -9953.873253577758
Iteration 6200: Loss = -9953.873267247556
Iteration 6300: Loss = -9953.873131431475
Iteration 6400: Loss = -9953.873029441134
Iteration 6500: Loss = -9953.872963220625
Iteration 6600: Loss = -9953.87427401315
1
Iteration 6700: Loss = -9953.875314975072
2
Iteration 6800: Loss = -9953.87292586129
Iteration 6900: Loss = -9953.87278910562
Iteration 7000: Loss = -9953.872721231288
Iteration 7100: Loss = -9953.873129457928
1
Iteration 7200: Loss = -9953.872585640625
Iteration 7300: Loss = -9953.872539544514
Iteration 7400: Loss = -9953.883113273116
1
Iteration 7500: Loss = -9953.872430795438
Iteration 7600: Loss = -9953.873946387148
1
Iteration 7700: Loss = -9953.87253058135
Iteration 7800: Loss = -9953.872414959658
Iteration 7900: Loss = -9953.872573998098
1
Iteration 8000: Loss = -9953.924033297058
2
Iteration 8100: Loss = -9953.872289708886
Iteration 8200: Loss = -9953.8722814188
Iteration 8300: Loss = -9953.872227480086
Iteration 8400: Loss = -9953.891428240222
1
Iteration 8500: Loss = -9953.872155904748
Iteration 8600: Loss = -9953.872177969557
Iteration 8700: Loss = -9953.8749256
1
Iteration 8800: Loss = -9953.872096144534
Iteration 8900: Loss = -9953.872258678708
1
Iteration 9000: Loss = -9953.872140129464
Iteration 9100: Loss = -9953.872062330745
Iteration 9200: Loss = -9953.883147341443
1
Iteration 9300: Loss = -9953.872010976696
Iteration 9400: Loss = -9953.8720054002
Iteration 9500: Loss = -9953.939875088756
1
Iteration 9600: Loss = -9953.871982073373
Iteration 9700: Loss = -9953.87195490874
Iteration 9800: Loss = -9953.872348810732
1
Iteration 9900: Loss = -9953.871937352947
Iteration 10000: Loss = -9953.925531150318
1
Iteration 10100: Loss = -9953.871898927813
Iteration 10200: Loss = -9953.871908757119
Iteration 10300: Loss = -9953.872027960952
1
Iteration 10400: Loss = -9953.871938639682
Iteration 10500: Loss = -9953.930506134553
1
Iteration 10600: Loss = -9953.871891831908
Iteration 10700: Loss = -9953.872272690154
1
Iteration 10800: Loss = -9953.873873036038
2
Iteration 10900: Loss = -9953.871968394584
Iteration 11000: Loss = -9953.899432168027
1
Iteration 11100: Loss = -9953.87185021169
Iteration 11200: Loss = -9953.871921650485
Iteration 11300: Loss = -9953.871851249474
Iteration 11400: Loss = -9953.893797281304
1
Iteration 11500: Loss = -9953.878182921588
2
Iteration 11600: Loss = -9953.876891341133
3
Iteration 11700: Loss = -9953.873407419593
4
Iteration 11800: Loss = -9953.87928545084
5
Iteration 11900: Loss = -9953.872339651385
6
Iteration 12000: Loss = -9953.872043084031
7
Iteration 12100: Loss = -9953.958403651637
8
Iteration 12200: Loss = -9953.876970670315
9
Iteration 12300: Loss = -9953.885513088751
10
Iteration 12400: Loss = -9953.871782406615
Iteration 12500: Loss = -9953.890728157221
1
Iteration 12600: Loss = -9953.875305632713
2
Iteration 12700: Loss = -9953.871743734071
Iteration 12800: Loss = -9953.87339305865
1
Iteration 12900: Loss = -9953.871912977187
2
Iteration 13000: Loss = -9953.87861207577
3
Iteration 13100: Loss = -9953.872821014173
4
Iteration 13200: Loss = -9953.872137036838
5
Iteration 13300: Loss = -9953.871841016657
Iteration 13400: Loss = -9953.872212671633
1
Iteration 13500: Loss = -9953.872207475419
2
Iteration 13600: Loss = -9953.919426031967
3
Iteration 13700: Loss = -9953.871907832481
Iteration 13800: Loss = -9953.874147918492
1
Iteration 13900: Loss = -9953.882083471179
2
Iteration 14000: Loss = -9953.881493315479
3
Iteration 14100: Loss = -9953.873089817538
4
Iteration 14200: Loss = -9953.872602066316
5
Iteration 14300: Loss = -9953.872519149001
6
Iteration 14400: Loss = -9953.878013021338
7
Iteration 14500: Loss = -9953.894129609229
8
Iteration 14600: Loss = -9953.899762244446
9
Iteration 14700: Loss = -9953.90008794328
10
Iteration 14800: Loss = -9953.871716281401
Iteration 14900: Loss = -9953.873008174163
1
Iteration 15000: Loss = -9953.871841422857
2
Iteration 15100: Loss = -9953.871747902958
Iteration 15200: Loss = -9953.880381075964
1
Iteration 15300: Loss = -9953.876199043714
2
Iteration 15400: Loss = -9953.871936117284
3
Iteration 15500: Loss = -9953.873888988608
4
Iteration 15600: Loss = -9953.872702843672
5
Iteration 15700: Loss = -9953.873184659598
6
Iteration 15800: Loss = -9953.8782264188
7
Iteration 15900: Loss = -9953.871725331352
Iteration 16000: Loss = -9953.872011107751
1
Iteration 16100: Loss = -9953.872099775934
2
Iteration 16200: Loss = -9953.871698383284
Iteration 16300: Loss = -9953.936796936097
1
Iteration 16400: Loss = -9953.871714133335
Iteration 16500: Loss = -9953.893517892684
1
Iteration 16600: Loss = -9953.903747762244
2
Iteration 16700: Loss = -9953.896912831235
3
Iteration 16800: Loss = -9953.877685799276
4
Iteration 16900: Loss = -9953.974754293627
5
Iteration 17000: Loss = -9953.871887674339
6
Iteration 17100: Loss = -9954.04809282696
7
Iteration 17200: Loss = -9953.881367756116
8
Iteration 17300: Loss = -9953.87231944675
9
Iteration 17400: Loss = -9954.051051205339
10
Iteration 17500: Loss = -9953.871735576859
Iteration 17600: Loss = -9954.01638307318
1
Iteration 17700: Loss = -9953.873230465513
2
Iteration 17800: Loss = -9953.872294952542
3
Iteration 17900: Loss = -9953.878518288804
4
Iteration 18000: Loss = -9953.872027522193
5
Iteration 18100: Loss = -9953.874941177715
6
Iteration 18200: Loss = -9953.875773763215
7
Iteration 18300: Loss = -9953.871729182772
Iteration 18400: Loss = -9953.875186019912
1
Iteration 18500: Loss = -9953.90966485959
2
Iteration 18600: Loss = -9953.873272066794
3
Iteration 18700: Loss = -9953.87207657522
4
Iteration 18800: Loss = -9953.878172404953
5
Iteration 18900: Loss = -9953.871806232291
Iteration 19000: Loss = -9953.871905901762
Iteration 19100: Loss = -9953.871787296452
Iteration 19200: Loss = -9953.871707732234
Iteration 19300: Loss = -9953.873496758622
1
Iteration 19400: Loss = -9953.873237403051
2
Iteration 19500: Loss = -9953.8744961075
3
Iteration 19600: Loss = -9953.880638784736
4
Iteration 19700: Loss = -9953.87208724328
5
Iteration 19800: Loss = -9953.89723050918
6
Iteration 19900: Loss = -9953.89005354355
7
pi: tensor([[9.4710e-01, 5.2900e-02],
        [9.9999e-01, 5.3517e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7943, 0.2057], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1332, 0.1661],
         [0.6313, 0.2172]],

        [[0.7107, 0.1529],
         [0.6350, 0.6226]],

        [[0.6809, 0.1268],
         [0.6309, 0.7305]],

        [[0.5505, 0.1703],
         [0.6324, 0.6158]],

        [[0.6756, 0.1766],
         [0.6526, 0.5895]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.006860085711622944
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008310317715881763
Average Adjusted Rand Index: -0.0013720171423245889
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20095.7322564762
Iteration 100: Loss = -9958.876834773804
Iteration 200: Loss = -9957.919028249644
Iteration 300: Loss = -9957.379657137097
Iteration 400: Loss = -9955.550611105078
Iteration 500: Loss = -9954.874206407856
Iteration 600: Loss = -9954.66628033872
Iteration 700: Loss = -9954.605197776515
Iteration 800: Loss = -9954.580012402339
Iteration 900: Loss = -9954.56306368424
Iteration 1000: Loss = -9954.549125159576
Iteration 1100: Loss = -9954.535934632684
Iteration 1200: Loss = -9954.522116158698
Iteration 1300: Loss = -9954.506200324655
Iteration 1400: Loss = -9954.486564233157
Iteration 1500: Loss = -9954.461123027171
Iteration 1600: Loss = -9954.426581909207
Iteration 1700: Loss = -9954.378344692655
Iteration 1800: Loss = -9954.311099701328
Iteration 1900: Loss = -9954.222931916665
Iteration 2000: Loss = -9954.105114800863
Iteration 2100: Loss = -9953.722351048154
Iteration 2200: Loss = -9953.172191952359
Iteration 2300: Loss = -9952.982205548882
Iteration 2400: Loss = -9952.911983704611
Iteration 2500: Loss = -9952.880314854003
Iteration 2600: Loss = -9952.863569313384
Iteration 2700: Loss = -9952.8534273422
Iteration 2800: Loss = -9952.846661893333
Iteration 2900: Loss = -9952.841848280375
Iteration 3000: Loss = -9952.838218743718
Iteration 3100: Loss = -9952.835335800537
Iteration 3200: Loss = -9952.833033592311
Iteration 3300: Loss = -9952.831077798584
Iteration 3400: Loss = -9952.829403516213
Iteration 3500: Loss = -9952.827990262851
Iteration 3600: Loss = -9952.826740712238
Iteration 3700: Loss = -9952.825674906708
Iteration 3800: Loss = -9952.82470631778
Iteration 3900: Loss = -9952.823842805117
Iteration 4000: Loss = -9952.823074500711
Iteration 4100: Loss = -9952.822362757757
Iteration 4200: Loss = -9952.821764979655
Iteration 4300: Loss = -9952.821154507348
Iteration 4400: Loss = -9952.820651285916
Iteration 4500: Loss = -9952.820160544456
Iteration 4600: Loss = -9952.819732462538
Iteration 4700: Loss = -9952.819307059759
Iteration 4800: Loss = -9952.818923022374
Iteration 4900: Loss = -9952.818594761236
Iteration 5000: Loss = -9952.818252791112
Iteration 5100: Loss = -9952.81796274372
Iteration 5200: Loss = -9952.81768031843
Iteration 5300: Loss = -9952.81740011913
Iteration 5400: Loss = -9952.817137525104
Iteration 5500: Loss = -9952.816931171232
Iteration 5600: Loss = -9952.816742208906
Iteration 5700: Loss = -9952.81649382772
Iteration 5800: Loss = -9952.81632170188
Iteration 5900: Loss = -9952.816141286854
Iteration 6000: Loss = -9952.815972649118
Iteration 6100: Loss = -9952.815792220043
Iteration 6200: Loss = -9952.815835297191
Iteration 6300: Loss = -9952.81570219332
Iteration 6400: Loss = -9952.815512189147
Iteration 6500: Loss = -9952.816690543035
1
Iteration 6600: Loss = -9952.815137180885
Iteration 6700: Loss = -9952.815136356974
Iteration 6800: Loss = -9952.814893856586
Iteration 6900: Loss = -9952.814820974576
Iteration 7000: Loss = -9952.814698271965
Iteration 7100: Loss = -9952.814621627564
Iteration 7200: Loss = -9952.815463680023
1
Iteration 7300: Loss = -9952.81443793221
Iteration 7400: Loss = -9952.817124591025
1
Iteration 7500: Loss = -9952.814292463952
Iteration 7600: Loss = -9952.814204406191
Iteration 7700: Loss = -9952.820720562375
1
Iteration 7800: Loss = -9952.86465762061
2
Iteration 7900: Loss = -9952.814058212267
Iteration 8000: Loss = -9952.814151280372
Iteration 8100: Loss = -9952.890043671061
1
Iteration 8200: Loss = -9952.813848241847
Iteration 8300: Loss = -9952.842127907534
1
Iteration 8400: Loss = -9952.813792698245
Iteration 8500: Loss = -9952.815033020815
1
Iteration 8600: Loss = -9952.813684850868
Iteration 8700: Loss = -9952.813659599717
Iteration 8800: Loss = -9952.813937492814
1
Iteration 8900: Loss = -9952.813559870398
Iteration 9000: Loss = -9952.813537443748
Iteration 9100: Loss = -9952.8154225329
1
Iteration 9200: Loss = -9952.813445317428
Iteration 9300: Loss = -9952.813437114679
Iteration 9400: Loss = -9952.81401472524
1
Iteration 9500: Loss = -9952.813364740327
Iteration 9600: Loss = -9952.813344408907
Iteration 9700: Loss = -9952.820307552325
1
Iteration 9800: Loss = -9952.813314642706
Iteration 9900: Loss = -9952.813276524008
Iteration 10000: Loss = -9952.81377547008
1
Iteration 10100: Loss = -9952.813251594725
Iteration 10200: Loss = -9952.813216067612
Iteration 10300: Loss = -9952.816508356213
1
Iteration 10400: Loss = -9952.81320294151
Iteration 10500: Loss = -9952.8131751061
Iteration 10600: Loss = -9952.827423853872
1
Iteration 10700: Loss = -9952.813128288504
Iteration 10800: Loss = -9952.813121627809
Iteration 10900: Loss = -9952.813079277106
Iteration 11000: Loss = -9952.8131533546
Iteration 11100: Loss = -9952.813074877193
Iteration 11200: Loss = -9952.813078964777
Iteration 11300: Loss = -9952.813149258212
Iteration 11400: Loss = -9952.813089326579
Iteration 11500: Loss = -9952.857715666245
1
Iteration 11600: Loss = -9952.81303396501
Iteration 11700: Loss = -9952.813034980802
Iteration 11800: Loss = -9952.813491726398
1
Iteration 11900: Loss = -9952.812998969246
Iteration 12000: Loss = -9952.907219707426
1
Iteration 12100: Loss = -9952.81299107839
Iteration 12200: Loss = -9952.812975823028
Iteration 12300: Loss = -9952.897975341713
1
Iteration 12400: Loss = -9952.812960925148
Iteration 12500: Loss = -9952.812944942658
Iteration 12600: Loss = -9952.81489111953
1
Iteration 12700: Loss = -9952.812936462582
Iteration 12800: Loss = -9952.812959593497
Iteration 12900: Loss = -9952.816323486382
1
Iteration 13000: Loss = -9952.812949224552
Iteration 13100: Loss = -9952.813104746272
1
Iteration 13200: Loss = -9952.813083243336
2
Iteration 13300: Loss = -9952.818130257501
3
Iteration 13400: Loss = -9952.814796232195
4
Iteration 13500: Loss = -9952.828260072967
5
Iteration 13600: Loss = -9952.919856124898
6
Iteration 13700: Loss = -9952.816369967544
7
Iteration 13800: Loss = -9952.812917957699
Iteration 13900: Loss = -9952.813364488575
1
Iteration 14000: Loss = -9952.817225980543
2
Iteration 14100: Loss = -9952.814318714287
3
Iteration 14200: Loss = -9952.813129348513
4
Iteration 14300: Loss = -9952.814158777008
5
Iteration 14400: Loss = -9952.814364944434
6
Iteration 14500: Loss = -9952.990364608831
7
Iteration 14600: Loss = -9952.817575244135
8
Iteration 14700: Loss = -9952.816430472025
9
Iteration 14800: Loss = -9952.82843976268
10
Iteration 14900: Loss = -9952.817782762328
11
Iteration 15000: Loss = -9952.812863561181
Iteration 15100: Loss = -9952.81304713981
1
Iteration 15200: Loss = -9952.818337263514
2
Iteration 15300: Loss = -9952.813120256726
3
Iteration 15400: Loss = -9952.813134781863
4
Iteration 15500: Loss = -9952.812899451379
Iteration 15600: Loss = -9952.814566900006
1
Iteration 15700: Loss = -9952.817884304108
2
Iteration 15800: Loss = -9952.876739438634
3
Iteration 15900: Loss = -9952.81288781396
Iteration 16000: Loss = -9952.81323100602
1
Iteration 16100: Loss = -9952.866408070946
2
Iteration 16200: Loss = -9952.825499740811
3
Iteration 16300: Loss = -9952.862441030857
4
Iteration 16400: Loss = -9952.84923502463
5
Iteration 16500: Loss = -9952.812969361294
Iteration 16600: Loss = -9952.855997179971
1
Iteration 16700: Loss = -9952.826158213422
2
Iteration 16800: Loss = -9952.817562214826
3
Iteration 16900: Loss = -9952.813004771984
Iteration 17000: Loss = -9952.84872941442
1
Iteration 17100: Loss = -9952.81961362457
2
Iteration 17200: Loss = -9952.81397023866
3
Iteration 17300: Loss = -9952.827715240037
4
Iteration 17400: Loss = -9952.812876370452
Iteration 17500: Loss = -9952.813391621148
1
Iteration 17600: Loss = -9952.837245830362
2
Iteration 17700: Loss = -9952.820071585475
3
Iteration 17800: Loss = -9952.81295509514
Iteration 17900: Loss = -9952.814246397938
1
Iteration 18000: Loss = -9952.8131409952
2
Iteration 18100: Loss = -9952.813245503163
3
Iteration 18200: Loss = -9952.812859637947
Iteration 18300: Loss = -9952.815926531988
1
Iteration 18400: Loss = -9952.836146217225
2
Iteration 18500: Loss = -9952.812856564777
Iteration 18600: Loss = -9952.813033848015
1
Iteration 18700: Loss = -9952.812823471206
Iteration 18800: Loss = -9952.81443861212
1
Iteration 18900: Loss = -9952.812835340355
Iteration 19000: Loss = -9952.814920677145
1
Iteration 19100: Loss = -9952.818125901951
2
Iteration 19200: Loss = -9952.844035708964
3
Iteration 19300: Loss = -9952.833652879955
4
Iteration 19400: Loss = -9952.976799807979
5
Iteration 19500: Loss = -9952.81283402183
Iteration 19600: Loss = -9952.813568346084
1
Iteration 19700: Loss = -9952.96361466742
2
Iteration 19800: Loss = -9952.812860224105
Iteration 19900: Loss = -9952.83117931612
1
pi: tensor([[9.8265e-01, 1.7348e-02],
        [1.0000e+00, 1.7884e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8450, 0.1550], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1359, 0.1728],
         [0.6014, 0.2386]],

        [[0.5383, 0.1488],
         [0.5429, 0.7128]],

        [[0.6000, 0.0404],
         [0.6594, 0.5224]],

        [[0.7113, 0.1772],
         [0.6439, 0.6549]],

        [[0.5311, 0.1860],
         [0.5860, 0.5581]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.012298440577296121
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008969453283075093
Average Adjusted Rand Index: -0.002616611143112905
10081.171668513682
[-0.0008310317715881763, -0.0008969453283075093] [-0.0013720171423245889, -0.002616611143112905] [9953.915687371795, 9952.812842871197]
-------------------------------------
This iteration is 60
True Objective function: Loss = -10034.783911629685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21227.10115152982
Iteration 100: Loss = -9932.183831430186
Iteration 200: Loss = -9931.017697131949
Iteration 300: Loss = -9930.096197927433
Iteration 400: Loss = -9929.044151661383
Iteration 500: Loss = -9928.169502338762
Iteration 600: Loss = -9927.808119229918
Iteration 700: Loss = -9927.625722734603
Iteration 800: Loss = -9927.505015926234
Iteration 900: Loss = -9927.420090758225
Iteration 1000: Loss = -9927.35645580014
Iteration 1100: Loss = -9927.306172471586
Iteration 1200: Loss = -9927.265104408069
Iteration 1300: Loss = -9927.230710624197
Iteration 1400: Loss = -9927.201138447052
Iteration 1500: Loss = -9927.174755278425
Iteration 1600: Loss = -9927.150008435625
Iteration 1700: Loss = -9927.124534917339
Iteration 1800: Loss = -9927.092745740234
Iteration 1900: Loss = -9927.037929058706
Iteration 2000: Loss = -9926.923788416983
Iteration 2100: Loss = -9926.785149584117
Iteration 2200: Loss = -9926.691985680127
Iteration 2300: Loss = -9926.61975109398
Iteration 2400: Loss = -9926.54328244166
Iteration 2500: Loss = -9926.44805474538
Iteration 2600: Loss = -9926.36329912646
Iteration 2700: Loss = -9926.301294166555
Iteration 2800: Loss = -9926.255804707356
Iteration 2900: Loss = -9926.221543377662
Iteration 3000: Loss = -9926.194983566576
Iteration 3100: Loss = -9926.173814240392
Iteration 3200: Loss = -9926.156570122257
Iteration 3300: Loss = -9926.14224188564
Iteration 3400: Loss = -9926.13021776446
Iteration 3500: Loss = -9926.120005849823
Iteration 3600: Loss = -9926.111272504197
Iteration 3700: Loss = -9926.10375040486
Iteration 3800: Loss = -9926.097192705238
Iteration 3900: Loss = -9926.091548268767
Iteration 4000: Loss = -9926.086626971331
Iteration 4100: Loss = -9926.082355252836
Iteration 4200: Loss = -9926.078586071844
Iteration 4300: Loss = -9926.075271285277
Iteration 4400: Loss = -9926.072340690083
Iteration 4500: Loss = -9926.069692618184
Iteration 4600: Loss = -9926.06733195908
Iteration 4700: Loss = -9926.065204302884
Iteration 4800: Loss = -9926.063263706345
Iteration 4900: Loss = -9926.061490723661
Iteration 5000: Loss = -9926.059871434187
Iteration 5100: Loss = -9926.05840474694
Iteration 5200: Loss = -9926.057000379897
Iteration 5300: Loss = -9926.055685189458
Iteration 5400: Loss = -9926.054421795661
Iteration 5500: Loss = -9926.05313469693
Iteration 5600: Loss = -9926.05173615359
Iteration 5700: Loss = -9926.05014428374
Iteration 5800: Loss = -9926.048587698891
Iteration 5900: Loss = -9926.047569756067
Iteration 6000: Loss = -9926.04667169844
Iteration 6100: Loss = -9926.045964079209
Iteration 6200: Loss = -9926.04528798136
Iteration 6300: Loss = -9926.044666007381
Iteration 6400: Loss = -9926.044077645245
Iteration 6500: Loss = -9926.043543799433
Iteration 6600: Loss = -9926.04300881825
Iteration 6700: Loss = -9926.042500698453
Iteration 6800: Loss = -9926.04202938539
Iteration 6900: Loss = -9926.041506154135
Iteration 7000: Loss = -9926.041042738549
Iteration 7100: Loss = -9926.040572736205
Iteration 7200: Loss = -9926.04004436721
Iteration 7300: Loss = -9926.039619763846
Iteration 7400: Loss = -9926.039254565714
Iteration 7500: Loss = -9926.038942723944
Iteration 7600: Loss = -9926.03861967655
Iteration 7700: Loss = -9926.038357270463
Iteration 7800: Loss = -9926.03810267674
Iteration 7900: Loss = -9926.038361529252
1
Iteration 8000: Loss = -9926.037664041402
Iteration 8100: Loss = -9926.037432200746
Iteration 8200: Loss = -9926.048289684084
1
Iteration 8300: Loss = -9926.03703677576
Iteration 8400: Loss = -9926.036888784976
Iteration 8500: Loss = -9926.036712096538
Iteration 8600: Loss = -9926.036620321793
Iteration 8700: Loss = -9926.03639484115
Iteration 8800: Loss = -9926.036232803908
Iteration 8900: Loss = -9926.130271466654
1
Iteration 9000: Loss = -9926.035890584428
Iteration 9100: Loss = -9926.0357028301
Iteration 9200: Loss = -9926.03546151
Iteration 9300: Loss = -9926.035800878428
1
Iteration 9400: Loss = -9926.035173352391
Iteration 9500: Loss = -9926.03501745843
Iteration 9600: Loss = -9926.0733285793
1
Iteration 9700: Loss = -9926.034694630409
Iteration 9800: Loss = -9926.03428048859
Iteration 9900: Loss = -9926.03400792958
Iteration 10000: Loss = -9926.03400109406
Iteration 10100: Loss = -9926.033814561595
Iteration 10200: Loss = -9926.033735938905
Iteration 10300: Loss = -9926.03371900539
Iteration 10400: Loss = -9926.035059087262
1
Iteration 10500: Loss = -9926.033492865
Iteration 10600: Loss = -9926.033390045517
Iteration 10700: Loss = -9926.033086618047
Iteration 10800: Loss = -9926.221637897792
1
Iteration 10900: Loss = -9926.032636246398
Iteration 11000: Loss = -9926.032583662945
Iteration 11100: Loss = -9926.0324889505
Iteration 11200: Loss = -9926.032652989767
1
Iteration 11300: Loss = -9926.032203702993
Iteration 11400: Loss = -9926.032068931207
Iteration 11500: Loss = -9926.10816046124
1
Iteration 11600: Loss = -9926.03117292599
Iteration 11700: Loss = -9926.031022868998
Iteration 11800: Loss = -9926.030891639684
Iteration 11900: Loss = -9926.12839291317
1
Iteration 12000: Loss = -9926.030757727121
Iteration 12100: Loss = -9926.030734046833
Iteration 12200: Loss = -9926.030668908876
Iteration 12300: Loss = -9926.030703765822
Iteration 12400: Loss = -9926.030637712096
Iteration 12500: Loss = -9926.030583336904
Iteration 12600: Loss = -9926.272173682937
1
Iteration 12700: Loss = -9926.03048234852
Iteration 12800: Loss = -9926.030119541514
Iteration 12900: Loss = -9926.02997121853
Iteration 13000: Loss = -9926.051750547194
1
Iteration 13100: Loss = -9926.029831735144
Iteration 13200: Loss = -9926.02982287859
Iteration 13300: Loss = -9926.02977676166
Iteration 13400: Loss = -9926.038199233624
1
Iteration 13500: Loss = -9926.02974848538
Iteration 13600: Loss = -9926.02966419013
Iteration 13700: Loss = -9926.035812016518
1
Iteration 13800: Loss = -9926.029620983389
Iteration 13900: Loss = -9926.029229530806
Iteration 14000: Loss = -9926.029042136299
Iteration 14100: Loss = -9926.029035350834
Iteration 14200: Loss = -9926.02900676997
Iteration 14300: Loss = -9926.02898539031
Iteration 14400: Loss = -9926.029043715822
Iteration 14500: Loss = -9926.028935669925
Iteration 14600: Loss = -9926.028847111596
Iteration 14700: Loss = -9926.029018874255
1
Iteration 14800: Loss = -9926.028799119244
Iteration 14900: Loss = -9926.02988519371
1
Iteration 15000: Loss = -9926.028703732427
Iteration 15100: Loss = -9926.042598551816
1
Iteration 15200: Loss = -9926.028214717377
Iteration 15300: Loss = -9926.029223236239
1
Iteration 15400: Loss = -9926.028163905245
Iteration 15500: Loss = -9926.378551866259
1
Iteration 15600: Loss = -9926.028025434383
Iteration 15700: Loss = -9926.027919426402
Iteration 15800: Loss = -9926.05052105863
1
Iteration 15900: Loss = -9926.027866155278
Iteration 16000: Loss = -9926.027836212763
Iteration 16100: Loss = -9926.188275227601
1
Iteration 16200: Loss = -9926.027856224955
Iteration 16300: Loss = -9926.02782100787
Iteration 16400: Loss = -9926.027791358189
Iteration 16500: Loss = -9926.027996797206
1
Iteration 16600: Loss = -9926.027622827633
Iteration 16700: Loss = -9926.02759465348
Iteration 16800: Loss = -9926.078024148575
1
Iteration 16900: Loss = -9926.02758745787
Iteration 17000: Loss = -9926.027581147042
Iteration 17100: Loss = -9926.027588467357
Iteration 17200: Loss = -9926.166804894232
1
Iteration 17300: Loss = -9926.02753273038
Iteration 17400: Loss = -9926.0274773224
Iteration 17500: Loss = -9926.02742091273
Iteration 17600: Loss = -9926.027897185346
1
Iteration 17700: Loss = -9926.027432406843
Iteration 17800: Loss = -9926.027430141035
Iteration 17900: Loss = -9926.027436318052
Iteration 18000: Loss = -9926.027414178845
Iteration 18100: Loss = -9926.027392942335
Iteration 18200: Loss = -9926.027397608561
Iteration 18300: Loss = -9926.027400611918
Iteration 18400: Loss = -9926.027414889964
Iteration 18500: Loss = -9926.02740632652
Iteration 18600: Loss = -9926.027382866883
Iteration 18700: Loss = -9926.06397556562
1
Iteration 18800: Loss = -9926.027404183309
Iteration 18900: Loss = -9926.034614072825
1
Iteration 19000: Loss = -9926.027419887396
Iteration 19100: Loss = -9926.347823814573
1
Iteration 19200: Loss = -9926.0274248412
Iteration 19300: Loss = -9926.027381408863
Iteration 19400: Loss = -9926.163302845862
1
Iteration 19500: Loss = -9926.027396303394
Iteration 19600: Loss = -9926.041822032059
1
Iteration 19700: Loss = -9926.027393432618
Iteration 19800: Loss = -9926.161058453847
1
Iteration 19900: Loss = -9926.02713107595
pi: tensor([[1.0000e+00, 1.3772e-08],
        [1.0325e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9814, 0.0186], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1362, 0.1969],
         [0.6179, 0.2006]],

        [[0.6609, 0.2554],
         [0.6542, 0.6758]],

        [[0.6982, 0.1919],
         [0.5599, 0.7187]],

        [[0.6469, 0.1561],
         [0.6164, 0.5996]],

        [[0.5828, 0.1170],
         [0.5710, 0.6452]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
Global Adjusted Rand Index: 0.002414897838224976
Average Adjusted Rand Index: 0.002713871438425107
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23156.57719668211
Iteration 100: Loss = -9933.26696256939
Iteration 200: Loss = -9931.5694581503
Iteration 300: Loss = -9929.374913841719
Iteration 400: Loss = -9927.776238899232
Iteration 500: Loss = -9927.551339718064
Iteration 600: Loss = -9927.42822623882
Iteration 700: Loss = -9927.34921175425
Iteration 800: Loss = -9927.288660138605
Iteration 900: Loss = -9927.234619027822
Iteration 1000: Loss = -9927.19198469111
Iteration 1100: Loss = -9927.154457706989
Iteration 1200: Loss = -9927.114055171369
Iteration 1300: Loss = -9927.06070988658
Iteration 1400: Loss = -9926.967460669552
Iteration 1500: Loss = -9926.792642822196
Iteration 1600: Loss = -9926.599988847818
Iteration 1700: Loss = -9926.373257199502
Iteration 1800: Loss = -9926.26272792697
Iteration 1900: Loss = -9926.214902571417
Iteration 2000: Loss = -9926.185865134772
Iteration 2100: Loss = -9926.165596897103
Iteration 2200: Loss = -9926.150436279093
Iteration 2300: Loss = -9926.138580044595
Iteration 2400: Loss = -9926.129048789713
Iteration 2500: Loss = -9926.121188820167
Iteration 2600: Loss = -9926.11455288188
Iteration 2700: Loss = -9926.108815451289
Iteration 2800: Loss = -9926.10314683566
Iteration 2900: Loss = -9926.093683880319
Iteration 3000: Loss = -9926.087204881123
Iteration 3100: Loss = -9926.083809281243
Iteration 3200: Loss = -9926.08099991342
Iteration 3300: Loss = -9926.078544809325
Iteration 3400: Loss = -9926.07636436396
Iteration 3500: Loss = -9926.074364295691
Iteration 3600: Loss = -9926.072604242749
Iteration 3700: Loss = -9926.071017755592
Iteration 3800: Loss = -9926.069560837175
Iteration 3900: Loss = -9926.06826894167
Iteration 4000: Loss = -9926.067036232991
Iteration 4100: Loss = -9926.06597228577
Iteration 4200: Loss = -9926.064933879947
Iteration 4300: Loss = -9926.064036561049
Iteration 4400: Loss = -9926.063194297787
Iteration 4500: Loss = -9926.06237857772
Iteration 4600: Loss = -9926.06166724241
Iteration 4700: Loss = -9926.060992975586
Iteration 4800: Loss = -9926.060353518404
Iteration 4900: Loss = -9926.05976721953
Iteration 5000: Loss = -9926.059268571888
Iteration 5100: Loss = -9926.058717044463
Iteration 5200: Loss = -9926.058257283745
Iteration 5300: Loss = -9926.057828273224
Iteration 5400: Loss = -9926.057413267314
Iteration 5500: Loss = -9926.057027565934
Iteration 5600: Loss = -9926.056670056947
Iteration 5700: Loss = -9926.056313984875
Iteration 5800: Loss = -9926.056034440337
Iteration 5900: Loss = -9926.05571048538
Iteration 6000: Loss = -9926.055444424957
Iteration 6100: Loss = -9926.055194921231
Iteration 6200: Loss = -9926.054957581977
Iteration 6300: Loss = -9926.054701448671
Iteration 6400: Loss = -9926.054472173495
Iteration 6500: Loss = -9926.054201694535
Iteration 6600: Loss = -9926.053841739857
Iteration 6700: Loss = -9926.052479798185
Iteration 6800: Loss = -9926.051885892688
Iteration 6900: Loss = -9926.051674211752
Iteration 7000: Loss = -9926.05147746432
Iteration 7100: Loss = -9926.051361100448
Iteration 7200: Loss = -9926.051245080882
Iteration 7300: Loss = -9926.05103181917
Iteration 7400: Loss = -9926.050948705324
Iteration 7500: Loss = -9926.050831185179
Iteration 7600: Loss = -9926.050711923277
Iteration 7700: Loss = -9926.050655487017
Iteration 7800: Loss = -9926.050510543999
Iteration 7900: Loss = -9926.050443253009
Iteration 8000: Loss = -9926.050338945442
Iteration 8100: Loss = -9926.050264766554
Iteration 8200: Loss = -9926.05020731487
Iteration 8300: Loss = -9926.050116039007
Iteration 8400: Loss = -9926.050042447501
Iteration 8500: Loss = -9926.050170028919
1
Iteration 8600: Loss = -9926.05014084419
Iteration 8700: Loss = -9926.050016328269
Iteration 8800: Loss = -9926.12179382887
1
Iteration 8900: Loss = -9926.166095627035
2
Iteration 9000: Loss = -9926.049731890973
Iteration 9100: Loss = -9926.04967883981
Iteration 9200: Loss = -9926.269809095618
1
Iteration 9300: Loss = -9926.049549181904
Iteration 9400: Loss = -9926.049509575401
Iteration 9500: Loss = -9926.049514932087
Iteration 9600: Loss = -9926.049464469503
Iteration 9700: Loss = -9926.049414404411
Iteration 9800: Loss = -9926.050903770883
1
Iteration 9900: Loss = -9926.049342822487
Iteration 10000: Loss = -9926.049320931645
Iteration 10100: Loss = -9926.050467726345
1
Iteration 10200: Loss = -9926.049284050048
Iteration 10300: Loss = -9926.049295905012
Iteration 10400: Loss = -9926.16237867579
1
Iteration 10500: Loss = -9926.049208281247
Iteration 10600: Loss = -9926.04919486805
Iteration 10700: Loss = -9926.049134702547
Iteration 10800: Loss = -9926.059154265755
1
Iteration 10900: Loss = -9926.049122451066
Iteration 11000: Loss = -9926.049083770227
Iteration 11100: Loss = -9926.135275563496
1
Iteration 11200: Loss = -9926.046400254241
Iteration 11300: Loss = -9926.046406128662
Iteration 11400: Loss = -9926.046362558449
Iteration 11500: Loss = -9926.047350820836
1
Iteration 11600: Loss = -9926.046321879827
Iteration 11700: Loss = -9926.04628040625
Iteration 11800: Loss = -9926.046270122579
Iteration 11900: Loss = -9926.046441571898
1
Iteration 12000: Loss = -9926.046271180181
Iteration 12100: Loss = -9926.046281652818
Iteration 12200: Loss = -9926.106978222051
1
Iteration 12300: Loss = -9926.046296736416
Iteration 12400: Loss = -9926.046252829
Iteration 12500: Loss = -9926.046251257161
Iteration 12600: Loss = -9926.061774424395
1
Iteration 12700: Loss = -9926.046058662687
Iteration 12800: Loss = -9926.046090491292
Iteration 12900: Loss = -9926.046083137013
Iteration 13000: Loss = -9926.046690863303
1
Iteration 13100: Loss = -9926.046160198463
Iteration 13200: Loss = -9926.04618581349
Iteration 13300: Loss = -9926.046162467092
Iteration 13400: Loss = -9926.046304529535
1
Iteration 13500: Loss = -9926.046144422084
Iteration 13600: Loss = -9926.046159933661
Iteration 13700: Loss = -9926.048961539778
1
Iteration 13800: Loss = -9926.046147807141
Iteration 13900: Loss = -9926.046103021385
Iteration 14000: Loss = -9926.046900384075
1
Iteration 14100: Loss = -9926.046096647851
Iteration 14200: Loss = -9926.050352669236
1
Iteration 14300: Loss = -9926.046098167802
Iteration 14400: Loss = -9926.046075542552
Iteration 14500: Loss = -9926.046220384549
1
Iteration 14600: Loss = -9926.046064916716
Iteration 14700: Loss = -9926.0460733079
Iteration 14800: Loss = -9926.046188841763
1
Iteration 14900: Loss = -9926.046062919426
Iteration 15000: Loss = -9926.046066990315
Iteration 15100: Loss = -9926.049505886409
1
Iteration 15200: Loss = -9926.04604152196
Iteration 15300: Loss = -9926.04603965991
Iteration 15400: Loss = -9926.055532657883
1
Iteration 15500: Loss = -9926.046021260798
Iteration 15600: Loss = -9926.046036901276
Iteration 15700: Loss = -9926.052504570418
1
Iteration 15800: Loss = -9926.04603259177
Iteration 15900: Loss = -9926.046014259542
Iteration 16000: Loss = -9926.04911162897
1
Iteration 16100: Loss = -9926.046022865386
Iteration 16200: Loss = -9926.046025283054
Iteration 16300: Loss = -9926.057925637779
1
Iteration 16400: Loss = -9926.046022349783
Iteration 16500: Loss = -9926.046015528544
Iteration 16600: Loss = -9926.051839887627
1
Iteration 16700: Loss = -9926.046004246578
Iteration 16800: Loss = -9926.046014195967
Iteration 16900: Loss = -9926.050568488838
1
Iteration 17000: Loss = -9926.046025477166
Iteration 17100: Loss = -9926.046020347594
Iteration 17200: Loss = -9926.178723904748
1
Iteration 17300: Loss = -9926.0460244621
Iteration 17400: Loss = -9926.046034010693
Iteration 17500: Loss = -9926.460612339042
1
Iteration 17600: Loss = -9926.046043537839
Iteration 17700: Loss = -9926.046019319872
Iteration 17800: Loss = -9926.047897796392
1
Iteration 17900: Loss = -9926.046064988088
Iteration 18000: Loss = -9926.046052710824
Iteration 18100: Loss = -9926.046052926366
Iteration 18200: Loss = -9926.046468341765
1
Iteration 18300: Loss = -9926.046050402823
Iteration 18400: Loss = -9926.046057434625
Iteration 18500: Loss = -9926.054996388992
1
Iteration 18600: Loss = -9926.046058224141
Iteration 18700: Loss = -9926.046055721526
Iteration 18800: Loss = -9926.071016052194
1
Iteration 18900: Loss = -9926.046041649239
Iteration 19000: Loss = -9926.046053086895
Iteration 19100: Loss = -9926.079426513563
1
Iteration 19200: Loss = -9926.046063216012
Iteration 19300: Loss = -9926.046054473682
Iteration 19400: Loss = -9926.056196805377
1
Iteration 19500: Loss = -9926.046051450407
Iteration 19600: Loss = -9926.046045224908
Iteration 19700: Loss = -9926.064737145729
1
Iteration 19800: Loss = -9926.046048677816
Iteration 19900: Loss = -9926.046034490952
pi: tensor([[1.0000e+00, 3.5784e-07],
        [3.8612e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0180, 0.9820], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1989, 0.1960],
         [0.6465, 0.1373]],

        [[0.6422, 0.2584],
         [0.5258, 0.5588]],

        [[0.7310, 0.1940],
         [0.6617, 0.6372]],

        [[0.6625, 0.1550],
         [0.5888, 0.7306]],

        [[0.7075, 0.1160],
         [0.6010, 0.5424]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
Global Adjusted Rand Index: 0.002414897838224976
Average Adjusted Rand Index: 0.002713871438425107
10034.783911629685
[0.002414897838224976, 0.002414897838224976] [0.002713871438425107, 0.002713871438425107] [9926.027129595479, 9926.228370458572]
-------------------------------------
This iteration is 61
True Objective function: Loss = -9895.688085361695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23680.2040784721
Iteration 100: Loss = -9763.06420776681
Iteration 200: Loss = -9758.120085545732
Iteration 300: Loss = -9756.01520719368
Iteration 400: Loss = -9752.613352829212
Iteration 500: Loss = -9750.99008654047
Iteration 600: Loss = -9749.95351553906
Iteration 700: Loss = -9748.802963646673
Iteration 800: Loss = -9748.194341840435
Iteration 900: Loss = -9747.79490784705
Iteration 1000: Loss = -9747.531236426703
Iteration 1100: Loss = -9747.337213364119
Iteration 1200: Loss = -9747.193721370491
Iteration 1300: Loss = -9747.0913241472
Iteration 1400: Loss = -9747.014165842274
Iteration 1500: Loss = -9746.951659533994
Iteration 1600: Loss = -9746.899347749208
Iteration 1700: Loss = -9746.85644239806
Iteration 1800: Loss = -9746.823068735775
Iteration 1900: Loss = -9746.796789393784
Iteration 2000: Loss = -9746.775567007495
Iteration 2100: Loss = -9746.75942759263
Iteration 2200: Loss = -9746.747841642531
Iteration 2300: Loss = -9746.739826306477
Iteration 2400: Loss = -9746.734227492552
Iteration 2500: Loss = -9746.730299317305
Iteration 2600: Loss = -9746.72752701754
Iteration 2700: Loss = -9746.72562969448
Iteration 2800: Loss = -9746.72444753835
Iteration 2900: Loss = -9746.723151088656
Iteration 3000: Loss = -9746.72235576928
Iteration 3100: Loss = -9746.721979669395
Iteration 3200: Loss = -9746.721222199465
Iteration 3300: Loss = -9746.720799204168
Iteration 3400: Loss = -9746.720403577723
Iteration 3500: Loss = -9746.72007138963
Iteration 3600: Loss = -9746.720812438789
1
Iteration 3700: Loss = -9746.724453461318
2
Iteration 3800: Loss = -9746.719210463256
Iteration 3900: Loss = -9746.719395873864
1
Iteration 4000: Loss = -9746.719141573973
Iteration 4100: Loss = -9746.719971290344
1
Iteration 4200: Loss = -9746.718609585558
Iteration 4300: Loss = -9746.719319470125
1
Iteration 4400: Loss = -9746.718481443948
Iteration 4500: Loss = -9746.7195935629
1
Iteration 4600: Loss = -9746.718516919109
Iteration 4700: Loss = -9746.71837010997
Iteration 4800: Loss = -9746.718241220135
Iteration 4900: Loss = -9746.718318703926
Iteration 5000: Loss = -9746.719972566198
1
Iteration 5100: Loss = -9746.724137081088
2
Iteration 5200: Loss = -9746.729780259262
3
Iteration 5300: Loss = -9746.719422717504
4
Iteration 5400: Loss = -9746.719390566723
5
Iteration 5500: Loss = -9746.718199210361
Iteration 5600: Loss = -9746.718188137293
Iteration 5700: Loss = -9746.718165547669
Iteration 5800: Loss = -9746.718162961864
Iteration 5900: Loss = -9746.718125727963
Iteration 6000: Loss = -9746.723052009189
1
Iteration 6100: Loss = -9746.718143064414
Iteration 6200: Loss = -9746.718132296264
Iteration 6300: Loss = -9746.718109111738
Iteration 6400: Loss = -9746.718144386696
Iteration 6500: Loss = -9746.718135471121
Iteration 6600: Loss = -9746.718159532586
Iteration 6700: Loss = -9746.718096826355
Iteration 6800: Loss = -9746.71839707762
1
Iteration 6900: Loss = -9746.728136649728
2
Iteration 7000: Loss = -9746.718296160798
3
Iteration 7100: Loss = -9746.718092051526
Iteration 7200: Loss = -9746.718107695657
Iteration 7300: Loss = -9746.718127425336
Iteration 7400: Loss = -9746.752646259469
1
Iteration 7500: Loss = -9746.718097770623
Iteration 7600: Loss = -9746.731309483725
1
Iteration 7700: Loss = -9746.718089368307
Iteration 7800: Loss = -9746.718262265993
1
Iteration 7900: Loss = -9746.718098063231
Iteration 8000: Loss = -9746.723554186392
1
Iteration 8100: Loss = -9746.71807360803
Iteration 8200: Loss = -9746.718520293014
1
Iteration 8300: Loss = -9746.718091643752
Iteration 8400: Loss = -9746.766238383589
1
Iteration 8500: Loss = -9746.718097545689
Iteration 8600: Loss = -9746.771859020564
1
Iteration 8700: Loss = -9746.718088802416
Iteration 8800: Loss = -9746.718098883928
Iteration 8900: Loss = -9746.718088306787
Iteration 9000: Loss = -9746.718119947009
Iteration 9100: Loss = -9746.718156243709
Iteration 9200: Loss = -9746.719514495735
1
Iteration 9300: Loss = -9746.718119341247
Iteration 9400: Loss = -9746.718480000965
1
Iteration 9500: Loss = -9746.718097829536
Iteration 9600: Loss = -9746.718067390577
Iteration 9700: Loss = -9746.718861511217
1
Iteration 9800: Loss = -9746.718113208393
Iteration 9900: Loss = -9746.808991472722
1
Iteration 10000: Loss = -9746.718090287375
Iteration 10100: Loss = -9746.72038185703
1
Iteration 10200: Loss = -9746.718123741748
Iteration 10300: Loss = -9746.780760189795
1
Iteration 10400: Loss = -9746.718118996887
Iteration 10500: Loss = -9746.817137353759
1
Iteration 10600: Loss = -9746.718093815385
Iteration 10700: Loss = -9746.718098902671
Iteration 10800: Loss = -9746.719097382955
1
Iteration 10900: Loss = -9746.718091322602
Iteration 11000: Loss = -9746.718103563804
Iteration 11100: Loss = -9746.719271880696
1
Iteration 11200: Loss = -9746.718086868383
Iteration 11300: Loss = -9746.757576093463
1
Iteration 11400: Loss = -9746.718126324147
Iteration 11500: Loss = -9746.718077993222
Iteration 11600: Loss = -9746.718653313943
1
Iteration 11700: Loss = -9746.718103235424
Iteration 11800: Loss = -9746.71913355778
1
Iteration 11900: Loss = -9746.718147417574
Iteration 12000: Loss = -9746.718080369597
Iteration 12100: Loss = -9746.761665392807
1
Iteration 12200: Loss = -9746.718110696667
Iteration 12300: Loss = -9746.718083901389
Iteration 12400: Loss = -9746.718139595992
Iteration 12500: Loss = -9746.718082211622
Iteration 12600: Loss = -9746.728626212169
1
Iteration 12700: Loss = -9746.71808823604
Iteration 12800: Loss = -9746.74352340294
1
Iteration 12900: Loss = -9746.718077590547
Iteration 13000: Loss = -9746.720604083508
1
Iteration 13100: Loss = -9746.718120795815
Iteration 13200: Loss = -9746.718073553398
Iteration 13300: Loss = -9746.728273820854
1
Iteration 13400: Loss = -9746.718082587666
Iteration 13500: Loss = -9746.803327458005
1
Iteration 13600: Loss = -9746.718075232126
Iteration 13700: Loss = -9746.718096236618
Iteration 13800: Loss = -9746.71820667556
1
Iteration 13900: Loss = -9746.718085196262
Iteration 14000: Loss = -9746.722193088659
1
Iteration 14100: Loss = -9746.718057811388
Iteration 14200: Loss = -9746.876977766016
1
Iteration 14300: Loss = -9746.71812530471
Iteration 14400: Loss = -9746.730803681725
1
Iteration 14500: Loss = -9746.718110098242
Iteration 14600: Loss = -9746.718250468854
1
Iteration 14700: Loss = -9746.718110381105
Iteration 14800: Loss = -9746.718145944782
Iteration 14900: Loss = -9746.718109391642
Iteration 15000: Loss = -9746.71806341829
Iteration 15100: Loss = -9746.718281037973
1
Iteration 15200: Loss = -9746.718059738127
Iteration 15300: Loss = -9746.721489871234
1
Iteration 15400: Loss = -9746.718122135768
Iteration 15500: Loss = -9746.868325313862
1
Iteration 15600: Loss = -9746.718052786968
Iteration 15700: Loss = -9746.718092924597
Iteration 15800: Loss = -9746.718184694764
Iteration 15900: Loss = -9746.718067218711
Iteration 16000: Loss = -9746.719796061365
1
Iteration 16100: Loss = -9746.718092994503
Iteration 16200: Loss = -9746.753889896107
1
Iteration 16300: Loss = -9746.718055997886
Iteration 16400: Loss = -9746.785439284222
1
Iteration 16500: Loss = -9746.718088508962
Iteration 16600: Loss = -9746.965542690652
1
Iteration 16700: Loss = -9746.718091613035
Iteration 16800: Loss = -9746.718119236803
Iteration 16900: Loss = -9746.71946903568
1
Iteration 17000: Loss = -9746.718066529087
Iteration 17100: Loss = -9747.01158731213
1
Iteration 17200: Loss = -9746.718076112902
Iteration 17300: Loss = -9746.718084064347
Iteration 17400: Loss = -9746.718299858401
1
Iteration 17500: Loss = -9746.718099961625
Iteration 17600: Loss = -9746.71823666578
1
Iteration 17700: Loss = -9746.718065874184
Iteration 17800: Loss = -9746.718717052412
1
Iteration 17900: Loss = -9746.71809632324
Iteration 18000: Loss = -9746.719687896573
1
Iteration 18100: Loss = -9746.718082905354
Iteration 18200: Loss = -9746.724553556764
1
Iteration 18300: Loss = -9746.71807667765
Iteration 18400: Loss = -9746.71805550109
Iteration 18500: Loss = -9746.718194324309
1
Iteration 18600: Loss = -9746.71809042004
Iteration 18700: Loss = -9746.752177482074
1
Iteration 18800: Loss = -9746.718088471001
Iteration 18900: Loss = -9746.76792339159
1
Iteration 19000: Loss = -9746.718089670201
Iteration 19100: Loss = -9746.754690948997
1
Iteration 19200: Loss = -9746.71811265632
Iteration 19300: Loss = -9746.945039005523
1
Iteration 19400: Loss = -9746.718104327536
Iteration 19500: Loss = -9747.040121548202
1
Iteration 19600: Loss = -9746.718107862278
Iteration 19700: Loss = -9746.768860961904
1
Iteration 19800: Loss = -9746.718107701357
Iteration 19900: Loss = -9746.718074720997
pi: tensor([[0.0338, 0.9662],
        [0.1619, 0.8381]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1827, 0.8173], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2611, 0.1557],
         [0.5296, 0.1189]],

        [[0.5564, 0.1771],
         [0.7258, 0.6306]],

        [[0.7021, 0.1811],
         [0.5224, 0.6729]],

        [[0.6331, 0.1564],
         [0.6750, 0.5998]],

        [[0.5156, 0.1631],
         [0.6032, 0.5828]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.004021803333230736
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.03407739186585839
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.012184899471542247
Global Adjusted Rand Index: 0.01442285337406433
Average Adjusted Rand Index: 0.012432996704241254
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19788.899866850337
Iteration 100: Loss = -9760.604258312364
Iteration 200: Loss = -9759.01651746289
Iteration 300: Loss = -9752.621531227902
Iteration 400: Loss = -9751.383140505975
Iteration 500: Loss = -9750.596703543451
Iteration 600: Loss = -9749.877539635927
Iteration 700: Loss = -9748.208231534712
Iteration 800: Loss = -9747.597798607369
Iteration 900: Loss = -9747.281855439698
Iteration 1000: Loss = -9747.085391531022
Iteration 1100: Loss = -9746.9586717216
Iteration 1200: Loss = -9746.872738493079
Iteration 1300: Loss = -9746.816472969154
Iteration 1400: Loss = -9746.77997597437
Iteration 1500: Loss = -9746.756901680821
Iteration 1600: Loss = -9746.743796452267
Iteration 1700: Loss = -9746.736230086415
Iteration 1800: Loss = -9746.731502727256
Iteration 1900: Loss = -9746.728371276517
Iteration 2000: Loss = -9746.726217979085
Iteration 2100: Loss = -9746.724623098347
Iteration 2200: Loss = -9746.72339267423
Iteration 2300: Loss = -9746.722539358145
Iteration 2400: Loss = -9746.721841881415
Iteration 2500: Loss = -9746.72130490497
Iteration 2600: Loss = -9746.720803054595
Iteration 2700: Loss = -9746.720393068563
Iteration 2800: Loss = -9746.720049725704
Iteration 2900: Loss = -9746.719750154378
Iteration 3000: Loss = -9746.71943604532
Iteration 3100: Loss = -9746.719198810537
Iteration 3200: Loss = -9746.718963802045
Iteration 3300: Loss = -9746.718785780684
Iteration 3400: Loss = -9746.718680448468
Iteration 3500: Loss = -9746.718507457188
Iteration 3600: Loss = -9746.71840428931
Iteration 3700: Loss = -9746.718590789595
1
Iteration 3800: Loss = -9746.718332708317
Iteration 3900: Loss = -9746.71830033401
Iteration 4000: Loss = -9746.718388611453
Iteration 4100: Loss = -9746.718266548327
Iteration 4200: Loss = -9746.718217025838
Iteration 4300: Loss = -9746.718305042525
Iteration 4400: Loss = -9746.718179758613
Iteration 4500: Loss = -9746.718189200552
Iteration 4600: Loss = -9746.718208169666
Iteration 4700: Loss = -9746.718199355608
Iteration 4800: Loss = -9746.718462171097
1
Iteration 4900: Loss = -9746.718160574634
Iteration 5000: Loss = -9746.718157825699
Iteration 5100: Loss = -9746.718232606818
Iteration 5200: Loss = -9746.718146634275
Iteration 5300: Loss = -9746.730891425885
1
Iteration 5400: Loss = -9746.718117033497
Iteration 5500: Loss = -9746.718131641599
Iteration 5600: Loss = -9746.718132842512
Iteration 5700: Loss = -9746.718165675396
Iteration 5800: Loss = -9746.719110758902
1
Iteration 5900: Loss = -9746.719166220933
2
Iteration 6000: Loss = -9746.71854184248
3
Iteration 6100: Loss = -9746.731750139727
4
Iteration 6200: Loss = -9746.718615488253
5
Iteration 6300: Loss = -9746.718135492645
Iteration 6400: Loss = -9746.718185566071
Iteration 6500: Loss = -9746.718185206872
Iteration 6600: Loss = -9746.718109048941
Iteration 6700: Loss = -9746.725110903853
1
Iteration 6800: Loss = -9746.718113758734
Iteration 6900: Loss = -9746.718118896046
Iteration 7000: Loss = -9746.719400828611
1
Iteration 7100: Loss = -9746.718096169707
Iteration 7200: Loss = -9746.718145706494
Iteration 7300: Loss = -9746.718116836737
Iteration 7400: Loss = -9746.718118921206
Iteration 7500: Loss = -9746.71810275904
Iteration 7600: Loss = -9746.718356049425
1
Iteration 7700: Loss = -9746.718109721109
Iteration 7800: Loss = -9746.741371655124
1
Iteration 7900: Loss = -9746.718101526914
Iteration 8000: Loss = -9746.71816186582
Iteration 8100: Loss = -9746.718125845515
Iteration 8200: Loss = -9746.718088003825
Iteration 8300: Loss = -9746.718397178625
1
Iteration 8400: Loss = -9746.718087169324
Iteration 8500: Loss = -9746.718477905479
1
Iteration 8600: Loss = -9746.718096917746
Iteration 8700: Loss = -9746.741524882033
1
Iteration 8800: Loss = -9746.718107741039
Iteration 8900: Loss = -9746.71809185504
Iteration 9000: Loss = -9746.718205146673
1
Iteration 9100: Loss = -9746.71809264077
Iteration 9200: Loss = -9747.199457075174
1
Iteration 9300: Loss = -9746.718103001836
Iteration 9400: Loss = -9746.7180981221
Iteration 9500: Loss = -9746.915088123265
1
Iteration 9600: Loss = -9746.71810602594
Iteration 9700: Loss = -9746.718077146597
Iteration 9800: Loss = -9746.720980899056
1
Iteration 9900: Loss = -9746.718059352515
Iteration 10000: Loss = -9746.718261171864
1
Iteration 10100: Loss = -9746.718109644557
Iteration 10200: Loss = -9746.718077627527
Iteration 10300: Loss = -9746.752073284111
1
Iteration 10400: Loss = -9746.718066653128
Iteration 10500: Loss = -9746.718065860981
Iteration 10600: Loss = -9746.721426424221
1
Iteration 10700: Loss = -9746.718101160115
Iteration 10800: Loss = -9746.993566516681
1
Iteration 10900: Loss = -9746.718099221858
Iteration 11000: Loss = -9746.718085148086
Iteration 11100: Loss = -9746.718555238416
1
Iteration 11200: Loss = -9746.71809486504
Iteration 11300: Loss = -9746.805923924141
1
Iteration 11400: Loss = -9746.718108313
Iteration 11500: Loss = -9746.71807480334
Iteration 11600: Loss = -9746.72145954812
1
Iteration 11700: Loss = -9746.718114030615
Iteration 11800: Loss = -9746.778409990513
1
Iteration 11900: Loss = -9746.71809560901
Iteration 12000: Loss = -9746.718066029744
Iteration 12100: Loss = -9746.718180176467
1
Iteration 12200: Loss = -9746.718087913643
Iteration 12300: Loss = -9746.718623299035
1
Iteration 12400: Loss = -9746.718074776561
Iteration 12500: Loss = -9746.718918111015
1
Iteration 12600: Loss = -9746.718117330178
Iteration 12700: Loss = -9746.7244723074
1
Iteration 12800: Loss = -9746.718092090006
Iteration 12900: Loss = -9746.753823395038
1
Iteration 13000: Loss = -9746.718051515974
Iteration 13100: Loss = -9747.112798470776
1
Iteration 13200: Loss = -9746.71810380531
Iteration 13300: Loss = -9746.718090542316
Iteration 13400: Loss = -9746.718463589657
1
Iteration 13500: Loss = -9746.71810534814
Iteration 13600: Loss = -9746.720492001148
1
Iteration 13700: Loss = -9746.718068326687
Iteration 13800: Loss = -9746.728574079254
1
Iteration 13900: Loss = -9746.718133088087
Iteration 14000: Loss = -9746.729705169488
1
Iteration 14100: Loss = -9746.718090741735
Iteration 14200: Loss = -9746.718098391962
Iteration 14300: Loss = -9746.71818274281
Iteration 14400: Loss = -9746.718086744984
Iteration 14500: Loss = -9746.719227904614
1
Iteration 14600: Loss = -9746.718092176925
Iteration 14700: Loss = -9746.722487473275
1
Iteration 14800: Loss = -9746.718130543797
Iteration 14900: Loss = -9746.75558824801
1
Iteration 15000: Loss = -9746.718100851984
Iteration 15100: Loss = -9746.80068092705
1
Iteration 15200: Loss = -9746.718132159864
Iteration 15300: Loss = -9746.71843020642
1
Iteration 15400: Loss = -9746.71813034969
Iteration 15500: Loss = -9746.718088743171
Iteration 15600: Loss = -9746.718138097423
Iteration 15700: Loss = -9746.718094071784
Iteration 15800: Loss = -9746.719181248727
1
Iteration 15900: Loss = -9746.718075355737
Iteration 16000: Loss = -9746.720793476796
1
Iteration 16100: Loss = -9746.71809181588
Iteration 16200: Loss = -9746.727472629493
1
Iteration 16300: Loss = -9746.718103914194
Iteration 16400: Loss = -9746.757666754467
1
Iteration 16500: Loss = -9746.718090180026
Iteration 16600: Loss = -9746.719718262984
1
Iteration 16700: Loss = -9746.718136045847
Iteration 16800: Loss = -9746.71805719391
Iteration 16900: Loss = -9746.71975228173
1
Iteration 17000: Loss = -9746.718077993557
Iteration 17100: Loss = -9746.725834603554
1
Iteration 17200: Loss = -9746.718105393114
Iteration 17300: Loss = -9746.8048552773
1
Iteration 17400: Loss = -9746.718102784866
Iteration 17500: Loss = -9746.718090898776
Iteration 17600: Loss = -9746.718164407986
Iteration 17700: Loss = -9746.718105051496
Iteration 17800: Loss = -9746.718580692095
1
Iteration 17900: Loss = -9746.718083569467
Iteration 18000: Loss = -9746.71818385396
1
Iteration 18100: Loss = -9746.71815351976
Iteration 18200: Loss = -9746.718139153714
Iteration 18300: Loss = -9746.71806116923
Iteration 18400: Loss = -9746.718125550635
Iteration 18500: Loss = -9746.718065686362
Iteration 18600: Loss = -9746.719979119818
1
Iteration 18700: Loss = -9746.718101524255
Iteration 18800: Loss = -9746.731505545622
1
Iteration 18900: Loss = -9746.718081234034
Iteration 19000: Loss = -9746.827569455314
1
Iteration 19100: Loss = -9746.718098696783
Iteration 19200: Loss = -9746.718100808268
Iteration 19300: Loss = -9746.719049392234
1
Iteration 19400: Loss = -9746.71810139265
Iteration 19500: Loss = -9746.7181302117
Iteration 19600: Loss = -9746.71807571343
Iteration 19700: Loss = -9746.718561513328
1
Iteration 19800: Loss = -9746.718095821394
Iteration 19900: Loss = -9746.734149966203
1
pi: tensor([[0.8381, 0.1619],
        [0.9663, 0.0337]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8174, 0.1826], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1189, 0.1557],
         [0.6382, 0.2610]],

        [[0.7236, 0.1771],
         [0.5742, 0.5159]],

        [[0.6771, 0.1811],
         [0.6252, 0.5590]],

        [[0.5890, 0.1563],
         [0.6589, 0.5705]],

        [[0.5359, 0.1630],
         [0.6135, 0.5101]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.004021803333230736
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.01575757575757576
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.03407739186585839
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.012184899471542247
Global Adjusted Rand Index: 0.01442285337406433
Average Adjusted Rand Index: 0.012432996704241254
9895.688085361695
[0.01442285337406433, 0.01442285337406433] [0.012432996704241254, 0.012432996704241254] [9746.7182782251, 9746.71809948372]
-------------------------------------
This iteration is 62
True Objective function: Loss = -9955.321915471859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25306.55859015939
Iteration 100: Loss = -9822.54181386299
Iteration 200: Loss = -9821.677091729636
Iteration 300: Loss = -9821.426516733669
Iteration 400: Loss = -9821.306974431041
Iteration 500: Loss = -9821.23535311787
Iteration 600: Loss = -9821.185890883004
Iteration 700: Loss = -9821.14400411085
Iteration 800: Loss = -9821.093049496632
Iteration 900: Loss = -9820.95709157278
Iteration 1000: Loss = -9818.598366205726
Iteration 1100: Loss = -9816.80051751269
Iteration 1200: Loss = -9815.795821344096
Iteration 1300: Loss = -9815.346413198858
Iteration 1400: Loss = -9815.145445594177
Iteration 1500: Loss = -9815.028888958255
Iteration 1600: Loss = -9814.976343415254
Iteration 1700: Loss = -9814.943049234107
Iteration 1800: Loss = -9814.912725011416
Iteration 1900: Loss = -9814.902132834974
Iteration 2000: Loss = -9814.897680324244
Iteration 2100: Loss = -9814.894161042619
Iteration 2200: Loss = -9814.89100247055
Iteration 2300: Loss = -9814.886823292809
Iteration 2400: Loss = -9814.884499224827
Iteration 2500: Loss = -9814.882992414932
Iteration 2600: Loss = -9814.881794299767
Iteration 2700: Loss = -9814.880769837931
Iteration 2800: Loss = -9814.879903979307
Iteration 2900: Loss = -9814.879208084036
Iteration 3000: Loss = -9814.878657555388
Iteration 3100: Loss = -9814.878199750428
Iteration 3200: Loss = -9814.877773937214
Iteration 3300: Loss = -9814.87746140802
Iteration 3400: Loss = -9814.877131905556
Iteration 3500: Loss = -9814.876889572277
Iteration 3600: Loss = -9814.876627972631
Iteration 3700: Loss = -9814.876445245824
Iteration 3800: Loss = -9814.876246711565
Iteration 3900: Loss = -9814.876098406656
Iteration 4000: Loss = -9814.875960842703
Iteration 4100: Loss = -9814.87579943634
Iteration 4200: Loss = -9814.87567903249
Iteration 4300: Loss = -9814.87556186039
Iteration 4400: Loss = -9814.875439504567
Iteration 4500: Loss = -9814.87579439646
1
Iteration 4600: Loss = -9814.875289231262
Iteration 4700: Loss = -9814.875181769208
Iteration 4800: Loss = -9814.875134121785
Iteration 4900: Loss = -9814.875031769636
Iteration 5000: Loss = -9814.876040073674
1
Iteration 5100: Loss = -9814.874928278028
Iteration 5200: Loss = -9814.879534899383
1
Iteration 5300: Loss = -9814.874800710904
Iteration 5400: Loss = -9814.874764644954
Iteration 5500: Loss = -9814.874673417782
Iteration 5600: Loss = -9814.87464389005
Iteration 5700: Loss = -9814.874564564294
Iteration 5800: Loss = -9814.874528642622
Iteration 5900: Loss = -9814.874478161351
Iteration 6000: Loss = -9814.874458119115
Iteration 6100: Loss = -9814.874412228766
Iteration 6200: Loss = -9814.874361895545
Iteration 6300: Loss = -9814.875255177136
1
Iteration 6400: Loss = -9814.874314052762
Iteration 6500: Loss = -9814.874935290474
1
Iteration 6600: Loss = -9814.874226997115
Iteration 6700: Loss = -9814.87478710954
1
Iteration 6800: Loss = -9814.874158465349
Iteration 6900: Loss = -9814.874596668815
1
Iteration 7000: Loss = -9814.87410919536
Iteration 7100: Loss = -9814.874390806937
1
Iteration 7200: Loss = -9814.874065134467
Iteration 7300: Loss = -9814.874181210682
1
Iteration 7400: Loss = -9814.874002374072
Iteration 7500: Loss = -9814.873976693676
Iteration 7600: Loss = -9814.89832924936
1
Iteration 7700: Loss = -9814.873931425362
Iteration 7800: Loss = -9814.875694931214
1
Iteration 7900: Loss = -9814.873885782088
Iteration 8000: Loss = -9814.874210626209
1
Iteration 8100: Loss = -9814.873823417232
Iteration 8200: Loss = -9814.873830191367
Iteration 8300: Loss = -9814.873823056867
Iteration 8400: Loss = -9814.873779601234
Iteration 8500: Loss = -9814.874033034566
1
Iteration 8600: Loss = -9814.87374587697
Iteration 8700: Loss = -9814.87571529813
1
Iteration 8800: Loss = -9814.873713037188
Iteration 8900: Loss = -9814.88562202656
1
Iteration 9000: Loss = -9814.873673236967
Iteration 9100: Loss = -9814.88561710304
1
Iteration 9200: Loss = -9814.873646856238
Iteration 9300: Loss = -9814.873679912125
Iteration 9400: Loss = -9814.873623620617
Iteration 9500: Loss = -9814.873832644947
1
Iteration 9600: Loss = -9814.873597944452
Iteration 9700: Loss = -9814.874682790483
1
Iteration 9800: Loss = -9814.873602690252
Iteration 9900: Loss = -9814.874517790277
1
Iteration 10000: Loss = -9814.87357198693
Iteration 10100: Loss = -9814.873561055945
Iteration 10200: Loss = -9814.873736032321
1
Iteration 10300: Loss = -9814.873547095203
Iteration 10400: Loss = -9814.927993087027
1
Iteration 10500: Loss = -9814.873495240694
Iteration 10600: Loss = -9815.1799551827
1
Iteration 10700: Loss = -9814.873518456985
Iteration 10800: Loss = -9814.885798330291
1
Iteration 10900: Loss = -9814.873547199912
Iteration 11000: Loss = -9814.873665306548
1
Iteration 11100: Loss = -9814.873530607607
Iteration 11200: Loss = -9814.87470327955
1
Iteration 11300: Loss = -9814.873512884507
Iteration 11400: Loss = -9814.876378133073
1
Iteration 11500: Loss = -9814.87348995759
Iteration 11600: Loss = -9814.887930744799
1
Iteration 11700: Loss = -9814.873419712965
Iteration 11800: Loss = -9815.01829211546
1
Iteration 11900: Loss = -9814.873435211688
Iteration 12000: Loss = -9814.873431226019
Iteration 12100: Loss = -9814.87412183428
1
Iteration 12200: Loss = -9814.873392424066
Iteration 12300: Loss = -9814.90347817669
1
Iteration 12400: Loss = -9814.87343054451
Iteration 12500: Loss = -9814.911276559174
1
Iteration 12600: Loss = -9814.873393442844
Iteration 12700: Loss = -9814.882975819108
1
Iteration 12800: Loss = -9814.873382287797
Iteration 12900: Loss = -9814.873992345993
1
Iteration 13000: Loss = -9815.057302148027
2
Iteration 13100: Loss = -9814.873402169518
Iteration 13200: Loss = -9814.890754085318
1
Iteration 13300: Loss = -9814.873390522538
Iteration 13400: Loss = -9814.873424437605
Iteration 13500: Loss = -9814.877106105758
1
Iteration 13600: Loss = -9814.873403721851
Iteration 13700: Loss = -9814.882224944542
1
Iteration 13800: Loss = -9814.873407183597
Iteration 13900: Loss = -9814.87340930628
Iteration 14000: Loss = -9814.875480435752
1
Iteration 14100: Loss = -9814.880419083112
2
Iteration 14200: Loss = -9814.873385573123
Iteration 14300: Loss = -9814.87871676137
1
Iteration 14400: Loss = -9814.873373691322
Iteration 14500: Loss = -9814.87361780167
1
Iteration 14600: Loss = -9814.918284605055
2
Iteration 14700: Loss = -9814.873382842778
Iteration 14800: Loss = -9814.889419408839
1
Iteration 14900: Loss = -9814.873378717357
Iteration 15000: Loss = -9814.874334973996
1
Iteration 15100: Loss = -9814.873332172465
Iteration 15200: Loss = -9814.873542635487
1
Iteration 15300: Loss = -9814.953164959723
2
Iteration 15400: Loss = -9814.873364293466
Iteration 15500: Loss = -9814.873420622996
Iteration 15600: Loss = -9814.907827158775
1
Iteration 15700: Loss = -9814.873400093189
Iteration 15800: Loss = -9814.873513799559
1
Iteration 15900: Loss = -9814.874211996774
2
Iteration 16000: Loss = -9814.897981442113
3
Iteration 16100: Loss = -9814.873930667383
4
Iteration 16200: Loss = -9814.873395930888
Iteration 16300: Loss = -9814.874240834168
1
Iteration 16400: Loss = -9814.88599267013
2
Iteration 16500: Loss = -9814.920776899175
3
Iteration 16600: Loss = -9814.873379487319
Iteration 16700: Loss = -9814.873491585133
1
Iteration 16800: Loss = -9814.910716914259
2
Iteration 16900: Loss = -9814.873364375107
Iteration 17000: Loss = -9814.878253825947
1
Iteration 17100: Loss = -9814.873364576213
Iteration 17200: Loss = -9814.873370633379
Iteration 17300: Loss = -9814.887226112218
1
Iteration 17400: Loss = -9814.873342748235
Iteration 17500: Loss = -9814.873476262728
1
Iteration 17600: Loss = -9814.88538284861
2
Iteration 17700: Loss = -9814.873356748747
Iteration 17800: Loss = -9814.873463323209
1
Iteration 17900: Loss = -9814.87387959257
2
Iteration 18000: Loss = -9814.873384577051
Iteration 18100: Loss = -9814.920845343617
1
Iteration 18200: Loss = -9814.873356874776
Iteration 18300: Loss = -9814.87364855128
1
Iteration 18400: Loss = -9814.906341145468
2
Iteration 18500: Loss = -9814.873330874892
Iteration 18600: Loss = -9814.873587899778
1
Iteration 18700: Loss = -9814.93422325378
2
Iteration 18800: Loss = -9814.873390638948
Iteration 18900: Loss = -9814.873355591708
Iteration 19000: Loss = -9814.877228752288
1
Iteration 19100: Loss = -9814.894404657647
2
Iteration 19200: Loss = -9814.873373962071
Iteration 19300: Loss = -9814.878346272177
1
Iteration 19400: Loss = -9814.873324392775
Iteration 19500: Loss = -9814.873340269982
Iteration 19600: Loss = -9814.95928629167
1
Iteration 19700: Loss = -9814.873352850123
Iteration 19800: Loss = -9814.873750091823
1
Iteration 19900: Loss = -9814.974978026448
2
pi: tensor([[0.9256, 0.0744],
        [0.1412, 0.8588]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 2.6503e-07], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1283, 0.1254],
         [0.6748, 0.2112]],

        [[0.5273, 0.1408],
         [0.7163, 0.7014]],

        [[0.5816, 0.1471],
         [0.5413, 0.6697]],

        [[0.7241, 0.1747],
         [0.5379, 0.5430]],

        [[0.6389, 0.1530],
         [0.5718, 0.5864]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.010245671163764708
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.027469209953202733
Global Adjusted Rand Index: 0.009441768228618279
Average Adjusted Rand Index: 0.00837636017528561
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23791.262114030764
Iteration 100: Loss = -9824.004954754737
Iteration 200: Loss = -9822.163349234752
Iteration 300: Loss = -9821.659665773554
Iteration 400: Loss = -9821.428675013358
Iteration 500: Loss = -9821.300449717559
Iteration 600: Loss = -9821.221057798697
Iteration 700: Loss = -9821.163833624909
Iteration 800: Loss = -9821.113996354776
Iteration 900: Loss = -9821.057338415369
Iteration 1000: Loss = -9820.932591414206
Iteration 1100: Loss = -9820.099901207612
Iteration 1200: Loss = -9819.691814435822
Iteration 1300: Loss = -9819.44002217992
Iteration 1400: Loss = -9819.017389900817
Iteration 1500: Loss = -9818.434514942928
Iteration 1600: Loss = -9818.13337163107
Iteration 1700: Loss = -9817.95224849474
Iteration 1800: Loss = -9817.818960000177
Iteration 1900: Loss = -9817.722725200247
Iteration 2000: Loss = -9817.616338031034
Iteration 2100: Loss = -9817.509887417189
Iteration 2200: Loss = -9817.335168458623
Iteration 2300: Loss = -9816.91338963205
Iteration 2400: Loss = -9815.890996455024
Iteration 2500: Loss = -9814.841622399332
Iteration 2600: Loss = -9814.566093643336
Iteration 2700: Loss = -9814.348271685667
Iteration 2800: Loss = -9814.124525785754
Iteration 2900: Loss = -9813.952064211879
Iteration 3000: Loss = -9813.89297237171
Iteration 3100: Loss = -9813.847702677469
Iteration 3200: Loss = -9813.823594524101
Iteration 3300: Loss = -9813.80722878639
Iteration 3400: Loss = -9813.797034529573
Iteration 3500: Loss = -9813.789797643745
Iteration 3600: Loss = -9813.785281950917
Iteration 3700: Loss = -9813.78084362575
Iteration 3800: Loss = -9813.778029802197
Iteration 3900: Loss = -9813.775915264916
Iteration 4000: Loss = -9813.774315451432
Iteration 4100: Loss = -9813.774349236117
Iteration 4200: Loss = -9813.772045320191
Iteration 4300: Loss = -9813.77127438185
Iteration 4400: Loss = -9813.770669949878
Iteration 4500: Loss = -9813.770154285117
Iteration 4600: Loss = -9813.772480253903
1
Iteration 4700: Loss = -9813.769419975546
Iteration 4800: Loss = -9813.76913713805
Iteration 4900: Loss = -9813.768946920629
Iteration 5000: Loss = -9813.76875228539
Iteration 5100: Loss = -9813.782216337237
1
Iteration 5200: Loss = -9813.768421555946
Iteration 5300: Loss = -9813.768299659874
Iteration 5400: Loss = -9813.768414841925
1
Iteration 5500: Loss = -9813.768097412598
Iteration 5600: Loss = -9813.774911383882
1
Iteration 5700: Loss = -9813.76793496269
Iteration 5800: Loss = -9813.767840906206
Iteration 5900: Loss = -9813.76772973788
Iteration 6000: Loss = -9813.767443031227
Iteration 6100: Loss = -9813.774385625768
1
Iteration 6200: Loss = -9813.766984446338
Iteration 6300: Loss = -9813.776237020995
1
Iteration 6400: Loss = -9813.766886861096
Iteration 6500: Loss = -9813.766883320617
Iteration 6600: Loss = -9813.766817245742
Iteration 6700: Loss = -9813.766824165476
Iteration 6800: Loss = -9813.766832363897
Iteration 6900: Loss = -9813.766736053143
Iteration 7000: Loss = -9813.769335398632
1
Iteration 7100: Loss = -9813.766729229206
Iteration 7200: Loss = -9813.767052665964
1
Iteration 7300: Loss = -9813.766711658713
Iteration 7400: Loss = -9813.769841461602
1
Iteration 7500: Loss = -9813.766686545423
Iteration 7600: Loss = -9813.790485040683
1
Iteration 7700: Loss = -9813.766686841553
Iteration 7800: Loss = -9813.76665807659
Iteration 7900: Loss = -9813.766649853207
Iteration 8000: Loss = -9813.76665147549
Iteration 8100: Loss = -9813.770491714014
1
Iteration 8200: Loss = -9813.766637673938
Iteration 8300: Loss = -9813.76662390224
Iteration 8400: Loss = -9813.766760353621
1
Iteration 8500: Loss = -9813.76660790026
Iteration 8600: Loss = -9813.76990917102
1
Iteration 8700: Loss = -9813.766622278461
Iteration 8800: Loss = -9813.772684192829
1
Iteration 8900: Loss = -9813.767125551203
2
Iteration 9000: Loss = -9813.769899818497
3
Iteration 9100: Loss = -9813.768168283961
4
Iteration 9200: Loss = -9813.766814801393
5
Iteration 9300: Loss = -9813.766636088287
Iteration 9400: Loss = -9813.768535066616
1
Iteration 9500: Loss = -9813.76665234722
Iteration 9600: Loss = -9813.76666981703
Iteration 9700: Loss = -9813.766841104567
1
Iteration 9800: Loss = -9813.766826710586
2
Iteration 9900: Loss = -9813.79630413543
3
Iteration 10000: Loss = -9813.766652578099
Iteration 10100: Loss = -9813.76661812153
Iteration 10200: Loss = -9813.76667096224
Iteration 10300: Loss = -9813.808526099088
1
Iteration 10400: Loss = -9813.831456199074
2
Iteration 10500: Loss = -9813.766663654334
Iteration 10600: Loss = -9813.77044814791
1
Iteration 10700: Loss = -9813.76664183759
Iteration 10800: Loss = -9813.766623717362
Iteration 10900: Loss = -9813.773268791476
1
Iteration 11000: Loss = -9813.782739033571
2
Iteration 11100: Loss = -9813.767767221787
3
Iteration 11200: Loss = -9813.766672813315
Iteration 11300: Loss = -9813.769519200781
1
Iteration 11400: Loss = -9813.766588291694
Iteration 11500: Loss = -9813.773269850928
1
Iteration 11600: Loss = -9813.766597814922
Iteration 11700: Loss = -9813.76781162491
1
Iteration 11800: Loss = -9813.766614511653
Iteration 11900: Loss = -9813.766802182721
1
Iteration 12000: Loss = -9813.766725832817
2
Iteration 12100: Loss = -9813.82354445303
3
Iteration 12200: Loss = -9813.766594036442
Iteration 12300: Loss = -9813.767380990881
1
Iteration 12400: Loss = -9813.766621991499
Iteration 12500: Loss = -9813.76661169685
Iteration 12600: Loss = -9813.766738707787
1
Iteration 12700: Loss = -9813.76756437897
2
Iteration 12800: Loss = -9813.76679702601
3
Iteration 12900: Loss = -9813.770756707261
4
Iteration 13000: Loss = -9813.771855302035
5
Iteration 13100: Loss = -9813.77430510631
6
Iteration 13200: Loss = -9813.766616550982
Iteration 13300: Loss = -9813.766967590613
1
Iteration 13400: Loss = -9813.767496493147
2
Iteration 13500: Loss = -9813.893395225783
3
Iteration 13600: Loss = -9813.766587834083
Iteration 13700: Loss = -9813.766662066575
Iteration 13800: Loss = -9813.766626289686
Iteration 13900: Loss = -9813.768600336316
1
Iteration 14000: Loss = -9813.767139020947
2
Iteration 14100: Loss = -9813.805042581671
3
Iteration 14200: Loss = -9813.766596917545
Iteration 14300: Loss = -9813.788990318768
1
Iteration 14400: Loss = -9813.766583344543
Iteration 14500: Loss = -9813.770496923562
1
Iteration 14600: Loss = -9813.766570356174
Iteration 14700: Loss = -9814.040770461723
1
Iteration 14800: Loss = -9813.7665568758
Iteration 14900: Loss = -9813.766558946802
Iteration 15000: Loss = -9813.766901419076
1
Iteration 15100: Loss = -9813.76658993194
Iteration 15200: Loss = -9813.76832215361
1
Iteration 15300: Loss = -9813.766619698326
Iteration 15400: Loss = -9813.7724848117
1
Iteration 15500: Loss = -9813.76655327109
Iteration 15600: Loss = -9813.76744914772
1
Iteration 15700: Loss = -9813.766591443604
Iteration 15800: Loss = -9813.76666649255
Iteration 15900: Loss = -9813.830946450513
1
Iteration 16000: Loss = -9813.813381350983
2
Iteration 16100: Loss = -9813.76664045403
Iteration 16200: Loss = -9813.766563791532
Iteration 16300: Loss = -9813.785548608499
1
Iteration 16400: Loss = -9813.766559839094
Iteration 16500: Loss = -9813.766715771146
1
Iteration 16600: Loss = -9813.875163585686
2
Iteration 16700: Loss = -9813.785494746618
3
Iteration 16800: Loss = -9813.766587435655
Iteration 16900: Loss = -9813.766850586073
1
Iteration 17000: Loss = -9813.767389540386
2
Iteration 17100: Loss = -9813.942323546657
3
Iteration 17200: Loss = -9813.766606639134
Iteration 17300: Loss = -9813.766644885753
Iteration 17400: Loss = -9813.767186477355
1
Iteration 17500: Loss = -9813.77412773679
2
Iteration 17600: Loss = -9813.806709732975
3
Iteration 17700: Loss = -9813.766588747943
Iteration 17800: Loss = -9813.770431784369
1
Iteration 17900: Loss = -9813.834002249136
2
Iteration 18000: Loss = -9813.797001585168
3
Iteration 18100: Loss = -9813.80804379353
4
Iteration 18200: Loss = -9813.778747819497
5
Iteration 18300: Loss = -9813.770415062934
6
Iteration 18400: Loss = -9813.77303948059
7
Iteration 18500: Loss = -9813.76655941796
Iteration 18600: Loss = -9813.768972110507
1
Iteration 18700: Loss = -9813.767281261878
2
Iteration 18800: Loss = -9813.766672782367
3
Iteration 18900: Loss = -9813.768964196955
4
Iteration 19000: Loss = -9813.766959637003
5
Iteration 19100: Loss = -9813.766650857378
Iteration 19200: Loss = -9813.77020773485
1
Iteration 19300: Loss = -9813.804306021328
2
Iteration 19400: Loss = -9813.766602527296
Iteration 19500: Loss = -9813.76662336295
Iteration 19600: Loss = -9813.787931855742
1
Iteration 19700: Loss = -9813.76689687927
2
Iteration 19800: Loss = -9813.769758682021
3
Iteration 19900: Loss = -9813.768352378584
4
pi: tensor([[0.9707, 0.0293],
        [0.9710, 0.0290]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4330, 0.5670], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1385, 0.1303],
         [0.7274, 0.1238]],

        [[0.6037, 0.0605],
         [0.5435, 0.5297]],

        [[0.6641, 0.0592],
         [0.7030, 0.7193]],

        [[0.5540, 0.2233],
         [0.6138, 0.5028]],

        [[0.6065, 0.0632],
         [0.5911, 0.5768]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.004257309187079712
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: -0.0005147058470932154
Average Adjusted Rand Index: -0.0007952008724846089
9955.321915471859
[0.009441768228618279, -0.0005147058470932154] [0.00837636017528561, -0.0007952008724846089] [9814.873342071542, 9813.80106593601]
-------------------------------------
This iteration is 63
True Objective function: Loss = -10071.047747695342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21589.581627767868
Iteration 100: Loss = -9981.261310643367
Iteration 200: Loss = -9979.825223440686
Iteration 300: Loss = -9978.976445546123
Iteration 400: Loss = -9977.437994073389
Iteration 500: Loss = -9977.046421367078
Iteration 600: Loss = -9976.888290253246
Iteration 700: Loss = -9976.772098441686
Iteration 800: Loss = -9976.660877471782
Iteration 900: Loss = -9976.54272448057
Iteration 1000: Loss = -9976.441017386976
Iteration 1100: Loss = -9976.3446914441
Iteration 1200: Loss = -9976.265117812014
Iteration 1300: Loss = -9976.199101315204
Iteration 1400: Loss = -9976.142135366365
Iteration 1500: Loss = -9976.094209216357
Iteration 1600: Loss = -9976.054773992446
Iteration 1700: Loss = -9976.02081993567
Iteration 1800: Loss = -9975.99069508795
Iteration 1900: Loss = -9975.963581042708
Iteration 2000: Loss = -9975.939796404242
Iteration 2100: Loss = -9975.919245553849
Iteration 2200: Loss = -9975.901557480061
Iteration 2300: Loss = -9975.886124532277
Iteration 2400: Loss = -9975.872405782318
Iteration 2500: Loss = -9975.860067378237
Iteration 2600: Loss = -9975.848524659634
Iteration 2700: Loss = -9975.837594841494
Iteration 2800: Loss = -9975.827409732323
Iteration 2900: Loss = -9975.81804012188
Iteration 3000: Loss = -9975.809610030099
Iteration 3100: Loss = -9975.80223786218
Iteration 3200: Loss = -9975.795803564764
Iteration 3300: Loss = -9975.790139021183
Iteration 3400: Loss = -9975.78502482699
Iteration 3500: Loss = -9975.780361764366
Iteration 3600: Loss = -9975.776160319801
Iteration 3700: Loss = -9975.775368397255
Iteration 3800: Loss = -9975.76939552467
Iteration 3900: Loss = -9975.766863656818
Iteration 4000: Loss = -9975.764794219047
Iteration 4100: Loss = -9975.762938728873
Iteration 4200: Loss = -9975.761457018056
Iteration 4300: Loss = -9975.760252051381
Iteration 4400: Loss = -9975.759289810112
Iteration 4500: Loss = -9975.758734092222
Iteration 4600: Loss = -9975.758089366544
Iteration 4700: Loss = -9975.758781112549
1
Iteration 4800: Loss = -9975.758514219318
2
Iteration 4900: Loss = -9975.75720240146
Iteration 5000: Loss = -9975.75798201254
1
Iteration 5100: Loss = -9975.756961744013
Iteration 5200: Loss = -9975.757055541426
Iteration 5300: Loss = -9975.756967365494
Iteration 5400: Loss = -9975.757015393827
Iteration 5500: Loss = -9975.7568247733
Iteration 5600: Loss = -9975.757030642642
1
Iteration 5700: Loss = -9975.757033355614
2
Iteration 5800: Loss = -9975.75788406645
3
Iteration 5900: Loss = -9975.756852186118
Iteration 6000: Loss = -9975.757077084038
1
Iteration 6100: Loss = -9975.756789115932
Iteration 6200: Loss = -9975.756725315994
Iteration 6300: Loss = -9975.756773069155
Iteration 6400: Loss = -9975.756758850117
Iteration 6500: Loss = -9975.756980896955
1
Iteration 6600: Loss = -9975.756809110837
Iteration 6700: Loss = -9975.759608380567
1
Iteration 6800: Loss = -9975.75675035009
Iteration 6900: Loss = -9975.756974256927
1
Iteration 7000: Loss = -9975.758171210366
2
Iteration 7100: Loss = -9975.757208481902
3
Iteration 7200: Loss = -9975.75674768361
Iteration 7300: Loss = -9975.75686784287
1
Iteration 7400: Loss = -9975.756784643985
Iteration 7500: Loss = -9975.759041920608
1
Iteration 7600: Loss = -9975.762027132529
2
Iteration 7700: Loss = -9975.75677141264
Iteration 7800: Loss = -9975.757549631231
1
Iteration 7900: Loss = -9975.756754792226
Iteration 8000: Loss = -9975.762832048622
1
Iteration 8100: Loss = -9975.756765930428
Iteration 8200: Loss = -9975.75674583332
Iteration 8300: Loss = -9975.756824834643
Iteration 8400: Loss = -9975.756820078317
Iteration 8500: Loss = -9975.756890276827
Iteration 8600: Loss = -9975.815200104304
1
Iteration 8700: Loss = -9975.756742557303
Iteration 8800: Loss = -9975.756766172555
Iteration 8900: Loss = -9975.766706747692
1
Iteration 9000: Loss = -9975.756729341629
Iteration 9100: Loss = -9975.756743377344
Iteration 9200: Loss = -9975.759079503257
1
Iteration 9300: Loss = -9975.756758444191
Iteration 9400: Loss = -9975.75676828012
Iteration 9500: Loss = -9975.756903859468
1
Iteration 9600: Loss = -9975.756753549635
Iteration 9700: Loss = -9975.7567165695
Iteration 9800: Loss = -9975.757239864333
1
Iteration 9900: Loss = -9975.756773425885
Iteration 10000: Loss = -9975.756756561983
Iteration 10100: Loss = -9975.756810604065
Iteration 10200: Loss = -9975.756736381365
Iteration 10300: Loss = -9976.076162318648
1
Iteration 10400: Loss = -9975.756750414732
Iteration 10500: Loss = -9975.756770922035
Iteration 10600: Loss = -9975.75737894327
1
Iteration 10700: Loss = -9975.756766274479
Iteration 10800: Loss = -9975.75680858898
Iteration 10900: Loss = -9975.756840619966
Iteration 11000: Loss = -9975.756749053926
Iteration 11100: Loss = -9975.782892581296
1
Iteration 11200: Loss = -9975.756748872402
Iteration 11300: Loss = -9975.760900321702
1
Iteration 11400: Loss = -9975.758740498137
2
Iteration 11500: Loss = -9975.757322680462
3
Iteration 11600: Loss = -9975.808876963676
4
Iteration 11700: Loss = -9975.7655313494
5
Iteration 11800: Loss = -9975.76334569365
6
Iteration 11900: Loss = -9975.757514931689
7
Iteration 12000: Loss = -9975.762159686376
8
Iteration 12100: Loss = -9975.756954830695
9
Iteration 12200: Loss = -9975.756951479716
10
Iteration 12300: Loss = -9975.759367486777
11
Iteration 12400: Loss = -9975.7570926749
12
Iteration 12500: Loss = -9975.76952104843
13
Iteration 12600: Loss = -9975.75846038884
14
Iteration 12700: Loss = -9975.881059461482
15
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[0.4006, 0.5994],
        [0.0577, 0.9423]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1183, 0.8817], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2074, 0.1825],
         [0.5632, 0.1332]],

        [[0.6029, 0.1622],
         [0.5008, 0.5446]],

        [[0.5992, 0.1793],
         [0.5086, 0.5308]],

        [[0.6568, 0.1609],
         [0.6894, 0.5036]],

        [[0.5148, 0.1438],
         [0.6956, 0.6921]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.025916162480371957
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.006782224462687857
Average Adjusted Rand Index: -0.008432323016129723
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21665.69266336496
Iteration 100: Loss = -9981.775957420194
Iteration 200: Loss = -9980.276476932773
Iteration 300: Loss = -9978.71967982213
Iteration 400: Loss = -9977.72310724368
Iteration 500: Loss = -9977.512810241604
Iteration 600: Loss = -9977.404037035134
Iteration 700: Loss = -9977.333981967546
Iteration 800: Loss = -9977.279314300109
Iteration 900: Loss = -9977.22011267113
Iteration 1000: Loss = -9977.107348757703
Iteration 1100: Loss = -9976.87730709973
Iteration 1200: Loss = -9976.700032756411
Iteration 1300: Loss = -9976.579444117906
Iteration 1400: Loss = -9976.4920461963
Iteration 1500: Loss = -9976.419673636745
Iteration 1600: Loss = -9976.35569290207
Iteration 1700: Loss = -9976.296104581656
Iteration 1800: Loss = -9976.23758103518
Iteration 1900: Loss = -9976.178274605625
Iteration 2000: Loss = -9976.118127340334
Iteration 2100: Loss = -9976.060940597985
Iteration 2200: Loss = -9976.013095060067
Iteration 2300: Loss = -9975.976280363342
Iteration 2400: Loss = -9975.948786970823
Iteration 2500: Loss = -9975.926784924159
Iteration 2600: Loss = -9975.907642206437
Iteration 2700: Loss = -9975.89068622587
Iteration 2800: Loss = -9975.875468001925
Iteration 2900: Loss = -9975.861633508255
Iteration 3000: Loss = -9975.848903729393
Iteration 3100: Loss = -9975.83709817953
Iteration 3200: Loss = -9975.826163182084
Iteration 3300: Loss = -9975.816022537085
Iteration 3400: Loss = -9975.806765990092
Iteration 3500: Loss = -9975.79854107296
Iteration 3600: Loss = -9975.79135345727
Iteration 3700: Loss = -9975.786077915245
Iteration 3800: Loss = -9975.779965269183
Iteration 3900: Loss = -9975.775498081512
Iteration 4000: Loss = -9975.771769886172
Iteration 4100: Loss = -9975.768705315606
Iteration 4200: Loss = -9975.766125179036
Iteration 4300: Loss = -9975.76407760207
Iteration 4400: Loss = -9975.762359726265
Iteration 4500: Loss = -9975.761033271327
Iteration 4600: Loss = -9975.75980726271
Iteration 4700: Loss = -9975.759756824847
Iteration 4800: Loss = -9975.758219595416
Iteration 4900: Loss = -9975.757961354106
Iteration 5000: Loss = -9975.757509146353
Iteration 5100: Loss = -9975.757298219612
Iteration 5200: Loss = -9975.757066405498
Iteration 5300: Loss = -9975.757349480087
1
Iteration 5400: Loss = -9975.75693732233
Iteration 5500: Loss = -9975.758207285531
1
Iteration 5600: Loss = -9975.756822658019
Iteration 5700: Loss = -9975.757089112962
1
Iteration 5800: Loss = -9975.757304352734
2
Iteration 5900: Loss = -9975.757593404756
3
Iteration 6000: Loss = -9975.756786313075
Iteration 6100: Loss = -9975.756798298062
Iteration 6200: Loss = -9975.757204631229
1
Iteration 6300: Loss = -9975.758085818778
2
Iteration 6400: Loss = -9975.757088795954
3
Iteration 6500: Loss = -9975.757972895939
4
Iteration 6600: Loss = -9975.757658675271
5
Iteration 6700: Loss = -9975.756933312678
6
Iteration 6800: Loss = -9975.756727935006
Iteration 6900: Loss = -9975.756772923341
Iteration 7000: Loss = -9975.756778105755
Iteration 7100: Loss = -9975.757826102918
1
Iteration 7200: Loss = -9975.757951083544
2
Iteration 7300: Loss = -9975.756741553361
Iteration 7400: Loss = -9975.757304681742
1
Iteration 7500: Loss = -9975.761262321259
2
Iteration 7600: Loss = -9975.758506885257
3
Iteration 7700: Loss = -9975.764101308594
4
Iteration 7800: Loss = -9975.75678142974
Iteration 7900: Loss = -9975.756761488272
Iteration 8000: Loss = -9975.756895170682
1
Iteration 8100: Loss = -9975.757934052717
2
Iteration 8200: Loss = -9975.756765870996
Iteration 8300: Loss = -9975.756820312617
Iteration 8400: Loss = -9975.7787064829
1
Iteration 8500: Loss = -9975.75674567227
Iteration 8600: Loss = -9975.756828921292
Iteration 8700: Loss = -9975.756735903697
Iteration 8800: Loss = -9975.75912373808
1
Iteration 8900: Loss = -9975.756733386328
Iteration 9000: Loss = -9975.773659413184
1
Iteration 9100: Loss = -9975.75671682291
Iteration 9200: Loss = -9975.756744810262
Iteration 9300: Loss = -9975.756779822508
Iteration 9400: Loss = -9975.756782959597
Iteration 9500: Loss = -9975.764060554777
1
Iteration 9600: Loss = -9975.756738117598
Iteration 9700: Loss = -9975.757032627262
1
Iteration 9800: Loss = -9975.756834395119
Iteration 9900: Loss = -9975.756766463002
Iteration 10000: Loss = -9975.804847444135
1
Iteration 10100: Loss = -9975.756763057696
Iteration 10200: Loss = -9975.756774355616
Iteration 10300: Loss = -9975.756817742096
Iteration 10400: Loss = -9975.756762612104
Iteration 10500: Loss = -9975.80822641775
1
Iteration 10600: Loss = -9975.756743055965
Iteration 10700: Loss = -9975.758331572031
1
Iteration 10800: Loss = -9975.756774109032
Iteration 10900: Loss = -9975.756736457835
Iteration 11000: Loss = -9975.757067664672
1
Iteration 11100: Loss = -9975.756889600292
2
Iteration 11200: Loss = -9975.757335743454
3
Iteration 11300: Loss = -9975.760461327827
4
Iteration 11400: Loss = -9975.758388993345
5
Iteration 11500: Loss = -9975.756786424605
Iteration 11600: Loss = -9975.757095087134
1
Iteration 11700: Loss = -9975.910552364456
2
Iteration 11800: Loss = -9975.756737684918
Iteration 11900: Loss = -9975.758138642384
1
Iteration 12000: Loss = -9975.756741429217
Iteration 12100: Loss = -9975.757275345297
1
Iteration 12200: Loss = -9975.756764887563
Iteration 12300: Loss = -9975.757210949601
1
Iteration 12400: Loss = -9975.800481333337
2
Iteration 12500: Loss = -9975.756805334655
Iteration 12600: Loss = -9975.757022585603
1
Iteration 12700: Loss = -9975.758440275837
2
Iteration 12800: Loss = -9975.756806216135
Iteration 12900: Loss = -9975.756866271431
Iteration 13000: Loss = -9975.757284636666
1
Iteration 13100: Loss = -9975.77102081935
2
Iteration 13200: Loss = -9975.757760798535
3
Iteration 13300: Loss = -9975.794336347608
4
Iteration 13400: Loss = -9975.76371583913
5
Iteration 13500: Loss = -9975.763429886854
6
Iteration 13600: Loss = -9975.761131515834
7
Iteration 13700: Loss = -9975.81885805559
8
Iteration 13800: Loss = -9975.819239664885
9
Iteration 13900: Loss = -9975.76045141793
10
Iteration 14000: Loss = -9975.757168375703
11
Iteration 14100: Loss = -9975.757141679922
12
Iteration 14200: Loss = -9975.757239810118
13
Iteration 14300: Loss = -9975.757395734012
14
Iteration 14400: Loss = -9975.756771726261
Iteration 14500: Loss = -9975.758211230797
1
Iteration 14600: Loss = -9975.75696136931
2
Iteration 14700: Loss = -9975.763218805723
3
Iteration 14800: Loss = -9975.756792167294
Iteration 14900: Loss = -9975.830294605827
1
Iteration 15000: Loss = -9975.803913908705
2
Iteration 15100: Loss = -9975.761312800742
3
Iteration 15200: Loss = -9975.75854286249
4
Iteration 15300: Loss = -9975.756998856614
5
Iteration 15400: Loss = -9975.756847524779
Iteration 15500: Loss = -9975.759751124146
1
Iteration 15600: Loss = -9975.756938423614
Iteration 15700: Loss = -9975.757261838347
1
Iteration 15800: Loss = -9975.757125774137
2
Iteration 15900: Loss = -9975.772239929851
3
Iteration 16000: Loss = -9975.777097601209
4
Iteration 16100: Loss = -9975.757636864602
5
Iteration 16200: Loss = -9975.75686899224
Iteration 16300: Loss = -9975.7573462445
1
Iteration 16400: Loss = -9975.758244695591
2
Iteration 16500: Loss = -9975.757668995113
3
Iteration 16600: Loss = -9975.75801156967
4
Iteration 16700: Loss = -9975.757015856783
5
Iteration 16800: Loss = -9975.756764795507
Iteration 16900: Loss = -9975.757284757181
1
Iteration 17000: Loss = -9975.756935802225
2
Iteration 17100: Loss = -9975.757752189702
3
Iteration 17200: Loss = -9975.756860131145
Iteration 17300: Loss = -9975.757408115209
1
Iteration 17400: Loss = -9975.76537246601
2
Iteration 17500: Loss = -9975.757059984242
3
Iteration 17600: Loss = -9975.756873671606
Iteration 17700: Loss = -9975.756965029423
Iteration 17800: Loss = -9975.756985251459
Iteration 17900: Loss = -9975.757333785987
1
Iteration 18000: Loss = -9975.757592611033
2
Iteration 18100: Loss = -9975.757616396624
3
Iteration 18200: Loss = -9975.75832884754
4
Iteration 18300: Loss = -9975.758572080164
5
Iteration 18400: Loss = -9975.756881829355
Iteration 18500: Loss = -9975.757135030068
1
Iteration 18600: Loss = -9975.757188512025
2
Iteration 18700: Loss = -9975.759749967741
3
Iteration 18800: Loss = -9975.767902934216
4
Iteration 18900: Loss = -9975.770621827607
5
Iteration 19000: Loss = -9975.803985817076
6
Iteration 19100: Loss = -9975.842337933356
7
Iteration 19200: Loss = -9975.788112859116
8
Iteration 19300: Loss = -9975.863847231009
9
Iteration 19400: Loss = -9975.875588398136
10
Iteration 19500: Loss = -9975.757308488786
11
Iteration 19600: Loss = -9975.756839152711
Iteration 19700: Loss = -9975.758788387066
1
Iteration 19800: Loss = -9975.948358276966
2
Iteration 19900: Loss = -9975.756791915348
pi: tensor([[0.4036, 0.5964],
        [0.0584, 0.9416]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1193, 0.8807], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2074, 0.1821],
         [0.5052, 0.1324]],

        [[0.7083, 0.1622],
         [0.6278, 0.5800]],

        [[0.6369, 0.1787],
         [0.6522, 0.5593]],

        [[0.5680, 0.1609],
         [0.7255, 0.7089]],

        [[0.6680, 0.1442],
         [0.5255, 0.6921]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.025916162480371957
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.006782224462687857
Average Adjusted Rand Index: -0.008432323016129723
10071.047747695342
[-0.006782224462687857, -0.006782224462687857] [-0.008432323016129723, -0.008432323016129723] [9975.881059461482, 9975.779291292449]
-------------------------------------
This iteration is 64
True Objective function: Loss = -9899.085546299195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23307.270735115944
Iteration 100: Loss = -9805.111007013262
Iteration 200: Loss = -9802.987357500759
Iteration 300: Loss = -9802.626094167306
Iteration 400: Loss = -9802.499576528382
Iteration 500: Loss = -9802.442947064515
Iteration 600: Loss = -9802.409478804211
Iteration 700: Loss = -9802.38413165073
Iteration 800: Loss = -9802.361401053544
Iteration 900: Loss = -9802.33936169645
Iteration 1000: Loss = -9802.3172951293
Iteration 1100: Loss = -9802.294703391215
Iteration 1200: Loss = -9802.271149889277
Iteration 1300: Loss = -9802.246041162378
Iteration 1400: Loss = -9802.218617910801
Iteration 1500: Loss = -9802.188237224771
Iteration 1600: Loss = -9802.15408438505
Iteration 1700: Loss = -9802.115527795819
Iteration 1800: Loss = -9802.072027152512
Iteration 1900: Loss = -9802.023735031868
Iteration 2000: Loss = -9801.97204463633
Iteration 2100: Loss = -9801.920053131531
Iteration 2200: Loss = -9801.872079081266
Iteration 2300: Loss = -9801.831618092494
Iteration 2400: Loss = -9801.799697882376
Iteration 2500: Loss = -9801.774329280412
Iteration 2600: Loss = -9801.74866498917
Iteration 2700: Loss = -9801.659616659814
Iteration 2800: Loss = -9800.431191217735
Iteration 2900: Loss = -9799.933032927254
Iteration 3000: Loss = -9799.860500822395
Iteration 3100: Loss = -9799.83235747402
Iteration 3200: Loss = -9799.816572954467
Iteration 3300: Loss = -9799.806204400853
Iteration 3400: Loss = -9799.798743773788
Iteration 3500: Loss = -9799.793091385945
Iteration 3600: Loss = -9799.788614628198
Iteration 3700: Loss = -9799.784998274978
Iteration 3800: Loss = -9799.781943328304
Iteration 3900: Loss = -9799.779343199312
Iteration 4000: Loss = -9799.777151584725
Iteration 4100: Loss = -9799.775161313804
Iteration 4200: Loss = -9799.773403912668
Iteration 4300: Loss = -9799.771809908223
Iteration 4400: Loss = -9799.770342536936
Iteration 4500: Loss = -9799.768995037613
Iteration 4600: Loss = -9799.767716304643
Iteration 4700: Loss = -9799.766469303016
Iteration 4800: Loss = -9799.765187283261
Iteration 4900: Loss = -9799.763931203963
Iteration 5000: Loss = -9799.762619326224
Iteration 5100: Loss = -9799.76116108088
Iteration 5200: Loss = -9799.759631336563
Iteration 5300: Loss = -9799.757821676289
Iteration 5400: Loss = -9799.755824241776
Iteration 5500: Loss = -9799.753502071411
Iteration 5600: Loss = -9799.75094887499
Iteration 5700: Loss = -9799.748134247628
Iteration 5800: Loss = -9799.744876784052
Iteration 5900: Loss = -9799.740839702867
Iteration 6000: Loss = -9799.735665559101
Iteration 6100: Loss = -9799.728810344037
Iteration 6200: Loss = -9799.719364879888
Iteration 6300: Loss = -9799.7048722635
Iteration 6400: Loss = -9799.678675967045
Iteration 6500: Loss = -9799.656504140057
Iteration 6600: Loss = -9799.645642880796
Iteration 6700: Loss = -9799.641545368755
Iteration 6800: Loss = -9799.640127778985
Iteration 6900: Loss = -9799.63713608889
Iteration 7000: Loss = -9799.636122400138
Iteration 7100: Loss = -9799.63568310258
Iteration 7200: Loss = -9799.634910678444
Iteration 7300: Loss = -9799.634520307083
Iteration 7400: Loss = -9799.6342907449
Iteration 7500: Loss = -9799.633983791246
Iteration 7600: Loss = -9799.63379452979
Iteration 7700: Loss = -9799.635267493311
1
Iteration 7800: Loss = -9799.63351533661
Iteration 7900: Loss = -9799.633357862343
Iteration 8000: Loss = -9799.633495763721
1
Iteration 8100: Loss = -9799.633086194337
Iteration 8200: Loss = -9799.632988723424
Iteration 8300: Loss = -9799.63287806671
Iteration 8400: Loss = -9799.632772008417
Iteration 8500: Loss = -9799.63270687339
Iteration 8600: Loss = -9799.632661908412
Iteration 8700: Loss = -9799.632545163258
Iteration 8800: Loss = -9799.632519777153
Iteration 8900: Loss = -9799.635469568262
1
Iteration 9000: Loss = -9799.632379929451
Iteration 9100: Loss = -9799.650194204103
1
Iteration 9200: Loss = -9799.632262863062
Iteration 9300: Loss = -9799.632199443933
Iteration 9400: Loss = -9799.63224518326
Iteration 9500: Loss = -9799.633209013075
1
Iteration 9600: Loss = -9799.632113598667
Iteration 9700: Loss = -9799.632185434171
Iteration 9800: Loss = -9799.632064235722
Iteration 9900: Loss = -9799.637387139395
1
Iteration 10000: Loss = -9799.642025639674
2
Iteration 10100: Loss = -9799.631910715925
Iteration 10200: Loss = -9799.649807905049
1
Iteration 10300: Loss = -9799.631932387412
Iteration 10400: Loss = -9799.631850622689
Iteration 10500: Loss = -9799.633421974248
1
Iteration 10600: Loss = -9799.657670471883
2
Iteration 10700: Loss = -9799.642483309044
3
Iteration 10800: Loss = -9799.631814672253
Iteration 10900: Loss = -9799.63844237566
1
Iteration 11000: Loss = -9799.631772373243
Iteration 11100: Loss = -9799.63203132654
1
Iteration 11200: Loss = -9799.631737024203
Iteration 11300: Loss = -9799.636699451397
1
Iteration 11400: Loss = -9799.633424342112
2
Iteration 11500: Loss = -9799.631707783836
Iteration 11600: Loss = -9799.631844374444
1
Iteration 11700: Loss = -9799.631788204148
Iteration 11800: Loss = -9799.63499074379
1
Iteration 11900: Loss = -9799.647805858809
2
Iteration 12000: Loss = -9799.63163012727
Iteration 12100: Loss = -9799.631923790099
1
Iteration 12200: Loss = -9799.631620576472
Iteration 12300: Loss = -9799.64850689571
1
Iteration 12400: Loss = -9799.633097891072
2
Iteration 12500: Loss = -9799.631586532838
Iteration 12600: Loss = -9799.640756697052
1
Iteration 12700: Loss = -9799.631589407676
Iteration 12800: Loss = -9799.6388417749
1
Iteration 12900: Loss = -9799.631593248725
Iteration 13000: Loss = -9799.760615570694
1
Iteration 13100: Loss = -9799.631653163995
Iteration 13200: Loss = -9799.644051396652
1
Iteration 13300: Loss = -9799.631843698162
2
Iteration 13400: Loss = -9799.63263235653
3
Iteration 13500: Loss = -9799.632197251452
4
Iteration 13600: Loss = -9799.750052955429
5
Iteration 13700: Loss = -9799.63229765223
6
Iteration 13800: Loss = -9799.873810029861
7
Iteration 13900: Loss = -9799.631591445634
Iteration 14000: Loss = -9799.641521336884
1
Iteration 14100: Loss = -9799.636125229243
2
Iteration 14200: Loss = -9799.631638055625
Iteration 14300: Loss = -9799.63161537448
Iteration 14400: Loss = -9799.63151526507
Iteration 14500: Loss = -9799.63176904754
1
Iteration 14600: Loss = -9799.631527881544
Iteration 14700: Loss = -9799.635033947074
1
Iteration 14800: Loss = -9799.672972270015
2
Iteration 14900: Loss = -9799.632081279173
3
Iteration 15000: Loss = -9799.631509794992
Iteration 15100: Loss = -9799.735622947333
1
Iteration 15200: Loss = -9799.631519551946
Iteration 15300: Loss = -9799.69625278961
1
Iteration 15400: Loss = -9799.631700027165
2
Iteration 15500: Loss = -9799.633116546309
3
Iteration 15600: Loss = -9799.631558980333
Iteration 15700: Loss = -9799.631807416334
1
Iteration 15800: Loss = -9799.63155845924
Iteration 15900: Loss = -9799.63285120244
1
Iteration 16000: Loss = -9799.631535605822
Iteration 16100: Loss = -9799.631524936793
Iteration 16200: Loss = -9799.631572450222
Iteration 16300: Loss = -9799.63156793389
Iteration 16400: Loss = -9799.631532222409
Iteration 16500: Loss = -9799.651426299162
1
Iteration 16600: Loss = -9799.631494326719
Iteration 16700: Loss = -9799.63214254128
1
Iteration 16800: Loss = -9799.631568258788
Iteration 16900: Loss = -9799.631616925144
Iteration 17000: Loss = -9799.658631232465
1
Iteration 17100: Loss = -9799.63149183465
Iteration 17200: Loss = -9799.634861520466
1
Iteration 17300: Loss = -9799.678020759644
2
Iteration 17400: Loss = -9799.63195805725
3
Iteration 17500: Loss = -9799.631724270299
4
Iteration 17600: Loss = -9799.631605924755
5
Iteration 17700: Loss = -9799.63154918506
Iteration 17800: Loss = -9799.6414792957
1
Iteration 17900: Loss = -9799.631505770085
Iteration 18000: Loss = -9799.802053630745
1
Iteration 18100: Loss = -9799.63248197482
2
Iteration 18200: Loss = -9799.631703889476
3
Iteration 18300: Loss = -9799.631468809586
Iteration 18400: Loss = -9799.631663498521
1
Iteration 18500: Loss = -9799.631614706535
2
Iteration 18600: Loss = -9799.640604431877
3
Iteration 18700: Loss = -9799.631498421664
Iteration 18800: Loss = -9799.670678351213
1
Iteration 18900: Loss = -9799.632217284308
2
Iteration 19000: Loss = -9799.631577467888
Iteration 19100: Loss = -9799.640419009404
1
Iteration 19200: Loss = -9799.632830578797
2
Iteration 19300: Loss = -9799.632791404132
3
Iteration 19400: Loss = -9799.661254558641
4
Iteration 19500: Loss = -9799.63154718148
Iteration 19600: Loss = -9799.647257830204
1
Iteration 19700: Loss = -9799.633134614529
2
Iteration 19800: Loss = -9799.632600881394
3
Iteration 19900: Loss = -9799.802118444693
4
pi: tensor([[1.8517e-07, 1.0000e+00],
        [2.2249e-02, 9.7775e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2345, 0.7655], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1245, 0.1307],
         [0.5074, 0.1362]],

        [[0.5962, 0.0371],
         [0.6737, 0.7146]],

        [[0.6258, 0.0765],
         [0.5044, 0.5126]],

        [[0.6726, 0.1652],
         [0.6842, 0.5482]],

        [[0.5965, 0.1947],
         [0.5036, 0.6520]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005239263250504326
Average Adjusted Rand Index: 0.000488518314825395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21632.52167186386
Iteration 100: Loss = -9804.769943225072
Iteration 200: Loss = -9803.354868923885
Iteration 300: Loss = -9802.865427298213
Iteration 400: Loss = -9802.664615537566
Iteration 500: Loss = -9802.573426087349
Iteration 600: Loss = -9802.52573111204
Iteration 700: Loss = -9802.496275651829
Iteration 800: Loss = -9802.47474863023
Iteration 900: Loss = -9802.456380480324
Iteration 1000: Loss = -9802.43920781362
Iteration 1100: Loss = -9802.422248522238
Iteration 1200: Loss = -9802.404957733383
Iteration 1300: Loss = -9802.386829872961
Iteration 1400: Loss = -9802.367417560023
Iteration 1500: Loss = -9802.34652791844
Iteration 1600: Loss = -9802.32381560203
Iteration 1700: Loss = -9802.299161322428
Iteration 1800: Loss = -9802.272429674815
Iteration 1900: Loss = -9802.243378360028
Iteration 2000: Loss = -9802.211635543257
Iteration 2100: Loss = -9802.176956669051
Iteration 2200: Loss = -9802.13914349601
Iteration 2300: Loss = -9802.098083337756
Iteration 2400: Loss = -9802.054509574753
Iteration 2500: Loss = -9802.010102887394
Iteration 2600: Loss = -9801.968283149707
Iteration 2700: Loss = -9801.93205141707
Iteration 2800: Loss = -9801.902222246705
Iteration 2900: Loss = -9801.879298095191
Iteration 3000: Loss = -9801.863321655688
Iteration 3100: Loss = -9801.851763015975
Iteration 3200: Loss = -9801.842265614576
Iteration 3300: Loss = -9801.8330488814
Iteration 3400: Loss = -9801.82332981955
Iteration 3500: Loss = -9801.813047290036
Iteration 3600: Loss = -9801.802664760613
Iteration 3700: Loss = -9801.793351923687
Iteration 3800: Loss = -9801.78606739962
Iteration 3900: Loss = -9801.780995252868
Iteration 4000: Loss = -9801.7780575833
Iteration 4100: Loss = -9801.77527167009
Iteration 4200: Loss = -9801.773535909702
Iteration 4300: Loss = -9801.772199690311
Iteration 4400: Loss = -9801.770861947025
Iteration 4500: Loss = -9801.790337099072
1
Iteration 4600: Loss = -9801.767601636464
Iteration 4700: Loss = -9801.765282797734
Iteration 4800: Loss = -9801.762121234633
Iteration 4900: Loss = -9801.757057446057
Iteration 5000: Loss = -9801.75836059532
1
Iteration 5100: Loss = -9801.73272280435
Iteration 5200: Loss = -9801.691131089823
Iteration 5300: Loss = -9801.502800136608
Iteration 5400: Loss = -9801.086919800071
Iteration 5500: Loss = -9801.03964540776
Iteration 5600: Loss = -9801.03717597905
Iteration 5700: Loss = -9801.036729529693
Iteration 5800: Loss = -9801.036622203743
Iteration 5900: Loss = -9801.036584860878
Iteration 6000: Loss = -9801.036571572888
Iteration 6100: Loss = -9801.044067057237
1
Iteration 6200: Loss = -9801.036566425966
Iteration 6300: Loss = -9801.036781884477
1
Iteration 6400: Loss = -9801.036577449702
Iteration 6500: Loss = -9801.038341291345
1
Iteration 6600: Loss = -9801.03655964629
Iteration 6700: Loss = -9801.036556383782
Iteration 6800: Loss = -9801.037407326134
1
Iteration 6900: Loss = -9801.036530781692
Iteration 7000: Loss = -9801.03929198802
1
Iteration 7100: Loss = -9801.036561702684
Iteration 7200: Loss = -9801.036765472847
1
Iteration 7300: Loss = -9801.03655078934
Iteration 7400: Loss = -9801.036719425365
1
Iteration 7500: Loss = -9801.0367333275
2
Iteration 7600: Loss = -9801.036544440023
Iteration 7700: Loss = -9801.0375152306
1
Iteration 7800: Loss = -9801.036591056041
Iteration 7900: Loss = -9801.036605604655
Iteration 8000: Loss = -9801.037091235428
1
Iteration 8100: Loss = -9801.03744952274
2
Iteration 8200: Loss = -9801.037000062439
3
Iteration 8300: Loss = -9801.037624755949
4
Iteration 8400: Loss = -9801.037411395146
5
Iteration 8500: Loss = -9801.044079112056
6
Iteration 8600: Loss = -9801.036917829653
7
Iteration 8700: Loss = -9801.03667971154
Iteration 8800: Loss = -9801.03663290125
Iteration 8900: Loss = -9801.03670765011
Iteration 9000: Loss = -9801.036722986768
Iteration 9100: Loss = -9801.036628370044
Iteration 9200: Loss = -9801.037064121092
1
Iteration 9300: Loss = -9801.036954511941
2
Iteration 9400: Loss = -9801.036947940538
3
Iteration 9500: Loss = -9801.036998451289
4
Iteration 9600: Loss = -9801.038375444543
5
Iteration 9700: Loss = -9801.037001489913
6
Iteration 9800: Loss = -9801.043406026729
7
Iteration 9900: Loss = -9801.047221798834
8
Iteration 10000: Loss = -9801.039537680235
9
Iteration 10100: Loss = -9801.036750214857
10
Iteration 10200: Loss = -9801.144277791467
11
Iteration 10300: Loss = -9801.098679151832
12
Iteration 10400: Loss = -9801.089694585336
13
Iteration 10500: Loss = -9801.147686823098
14
Iteration 10600: Loss = -9801.078719094065
15
Stopping early at iteration 10600 due to no improvement.
pi: tensor([[0.9068, 0.0932],
        [0.1156, 0.8844]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2735, 0.7265], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1620, 0.1403],
         [0.6922, 0.1235]],

        [[0.7066, 0.1366],
         [0.5174, 0.6533]],

        [[0.7125, 0.1327],
         [0.5062, 0.5910]],

        [[0.6318, 0.1415],
         [0.6236, 0.5458]],

        [[0.6346, 0.1395],
         [0.6056, 0.6568]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.003068829460762823
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 32
Adjusted Rand Index: 0.12360995370539879
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 37
Adjusted Rand Index: 0.0598724937632819
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 28
Adjusted Rand Index: 0.1853448275862069
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 34
Adjusted Rand Index: 0.09333333333333334
Global Adjusted Rand Index: 0.07893382340727369
Average Adjusted Rand Index: 0.0918183557854916
9899.085546299195
[-0.0005239263250504326, 0.07893382340727369] [0.000488518314825395, 0.0918183557854916] [9799.631477605526, 9801.078719094065]
-------------------------------------
This iteration is 65
True Objective function: Loss = -10036.306352043353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22679.578083986264
Iteration 100: Loss = -9911.325419609724
Iteration 200: Loss = -9909.949432781517
Iteration 300: Loss = -9909.495413078483
Iteration 400: Loss = -9909.225580398594
Iteration 500: Loss = -9909.015247873878
Iteration 600: Loss = -9908.821228815132
Iteration 700: Loss = -9908.611881807537
Iteration 800: Loss = -9908.377239955038
Iteration 900: Loss = -9908.150109767897
Iteration 1000: Loss = -9907.963316811887
Iteration 1100: Loss = -9907.807570061925
Iteration 1200: Loss = -9907.673543747032
Iteration 1300: Loss = -9907.558461947132
Iteration 1400: Loss = -9907.460416743425
Iteration 1500: Loss = -9907.375826709174
Iteration 1600: Loss = -9907.299462073044
Iteration 1700: Loss = -9907.225461236514
Iteration 1800: Loss = -9907.146560012885
Iteration 1900: Loss = -9907.052586624402
Iteration 2000: Loss = -9906.928061619037
Iteration 2100: Loss = -9906.754630697635
Iteration 2200: Loss = -9906.533210456877
Iteration 2300: Loss = -9906.319827113719
Iteration 2400: Loss = -9906.163875601855
Iteration 2500: Loss = -9906.059627219269
Iteration 2600: Loss = -9905.987288088507
Iteration 2700: Loss = -9905.944591831003
Iteration 2800: Loss = -9905.918581804997
Iteration 2900: Loss = -9905.90005131017
Iteration 3000: Loss = -9905.886142400108
Iteration 3100: Loss = -9905.87613305619
Iteration 3200: Loss = -9905.868923996748
Iteration 3300: Loss = -9905.86363173093
Iteration 3400: Loss = -9905.859901501235
Iteration 3500: Loss = -9905.85736127703
Iteration 3600: Loss = -9905.855493776347
Iteration 3700: Loss = -9905.85411633867
Iteration 3800: Loss = -9905.853093668931
Iteration 3900: Loss = -9905.851832650209
Iteration 4000: Loss = -9905.850846980224
Iteration 4100: Loss = -9905.850042521413
Iteration 4200: Loss = -9905.84872182412
Iteration 4300: Loss = -9905.84768855046
Iteration 4400: Loss = -9905.846595369409
Iteration 4500: Loss = -9905.845490710624
Iteration 4600: Loss = -9905.844877000844
Iteration 4700: Loss = -9905.843085073247
Iteration 4800: Loss = -9905.841598841349
Iteration 4900: Loss = -9905.83975125087
Iteration 5000: Loss = -9905.837430405838
Iteration 5100: Loss = -9905.836359675393
Iteration 5200: Loss = -9905.83031639722
Iteration 5300: Loss = -9905.824317266057
Iteration 5400: Loss = -9905.814872607776
Iteration 5500: Loss = -9905.808552131633
Iteration 5600: Loss = -9905.771839177005
Iteration 5700: Loss = -9905.661573382791
Iteration 5800: Loss = -9905.29998206641
Iteration 5900: Loss = -9903.378930553055
Iteration 6000: Loss = -9903.025051358438
Iteration 6100: Loss = -9903.021795927049
Iteration 6200: Loss = -9903.021693114004
Iteration 6300: Loss = -9903.021656381881
Iteration 6400: Loss = -9903.02162728085
Iteration 6500: Loss = -9903.021631601961
Iteration 6600: Loss = -9903.021629024943
Iteration 6700: Loss = -9903.021710424327
Iteration 6800: Loss = -9903.021647422225
Iteration 6900: Loss = -9903.024071858325
1
Iteration 7000: Loss = -9903.028436394216
2
Iteration 7100: Loss = -9903.021626982925
Iteration 7200: Loss = -9903.022066929287
1
Iteration 7300: Loss = -9903.021643825383
Iteration 7400: Loss = -9903.021601843324
Iteration 7500: Loss = -9903.021896453756
1
Iteration 7600: Loss = -9903.021610302783
Iteration 7700: Loss = -9903.092942723515
1
Iteration 7800: Loss = -9903.021594034703
Iteration 7900: Loss = -9903.021597118177
Iteration 8000: Loss = -9903.13732176632
1
Iteration 8100: Loss = -9903.02159875106
Iteration 8200: Loss = -9903.021610013198
Iteration 8300: Loss = -9903.022648747768
1
Iteration 8400: Loss = -9903.02160596464
Iteration 8500: Loss = -9903.021592819927
Iteration 8600: Loss = -9903.024621933355
1
Iteration 8700: Loss = -9903.021614678471
Iteration 8800: Loss = -9903.021620551552
Iteration 8900: Loss = -9903.021708584707
Iteration 9000: Loss = -9903.021680432134
Iteration 9100: Loss = -9903.021645926952
Iteration 9200: Loss = -9903.078529833123
1
Iteration 9300: Loss = -9903.021636535916
Iteration 9400: Loss = -9903.021610610138
Iteration 9500: Loss = -9903.042808799642
1
Iteration 9600: Loss = -9903.02162641021
Iteration 9700: Loss = -9903.021630621539
Iteration 9800: Loss = -9903.022543938778
1
Iteration 9900: Loss = -9903.021640629784
Iteration 10000: Loss = -9903.021626348494
Iteration 10100: Loss = -9903.021687391209
Iteration 10200: Loss = -9903.021604899313
Iteration 10300: Loss = -9903.032925064468
1
Iteration 10400: Loss = -9903.021628191234
Iteration 10500: Loss = -9903.021618754636
Iteration 10600: Loss = -9903.16185632939
1
Iteration 10700: Loss = -9903.021642956232
Iteration 10800: Loss = -9903.021594623508
Iteration 10900: Loss = -9903.021820516513
1
Iteration 11000: Loss = -9903.02720754885
2
Iteration 11100: Loss = -9903.021764382798
3
Iteration 11200: Loss = -9903.021723496464
4
Iteration 11300: Loss = -9903.034866151933
5
Iteration 11400: Loss = -9903.021626241323
Iteration 11500: Loss = -9903.022448087724
1
Iteration 11600: Loss = -9903.021606539727
Iteration 11700: Loss = -9903.021718316702
1
Iteration 11800: Loss = -9903.021625819725
Iteration 11900: Loss = -9903.021747249451
1
Iteration 12000: Loss = -9903.021612599094
Iteration 12100: Loss = -9903.02182777341
1
Iteration 12200: Loss = -9903.061606787378
2
Iteration 12300: Loss = -9903.021570598816
Iteration 12400: Loss = -9903.03851032203
1
Iteration 12500: Loss = -9903.021610474085
Iteration 12600: Loss = -9903.030478955929
1
Iteration 12700: Loss = -9903.021627289536
Iteration 12800: Loss = -9903.026863450767
1
Iteration 12900: Loss = -9903.02160671026
Iteration 13000: Loss = -9903.036709578262
1
Iteration 13100: Loss = -9903.021613583584
Iteration 13200: Loss = -9903.068405648524
1
Iteration 13300: Loss = -9903.0216441274
Iteration 13400: Loss = -9903.026193914373
1
Iteration 13500: Loss = -9903.021615543441
Iteration 13600: Loss = -9903.021631397914
Iteration 13700: Loss = -9903.023542454512
1
Iteration 13800: Loss = -9903.021607469434
Iteration 13900: Loss = -9903.021593273967
Iteration 14000: Loss = -9903.022320970025
1
Iteration 14100: Loss = -9903.092942925834
2
Iteration 14200: Loss = -9903.021617782326
Iteration 14300: Loss = -9903.024444623854
1
Iteration 14400: Loss = -9903.021579976055
Iteration 14500: Loss = -9903.056498846969
1
Iteration 14600: Loss = -9903.021613992985
Iteration 14700: Loss = -9903.05457811857
1
Iteration 14800: Loss = -9903.021625120333
Iteration 14900: Loss = -9903.086248856369
1
Iteration 15000: Loss = -9903.02161986811
Iteration 15100: Loss = -9903.021926185387
1
Iteration 15200: Loss = -9903.021628943738
Iteration 15300: Loss = -9903.02168399364
Iteration 15400: Loss = -9903.037972752241
1
Iteration 15500: Loss = -9903.021611567205
Iteration 15600: Loss = -9903.114111892624
1
Iteration 15700: Loss = -9903.021594844433
Iteration 15800: Loss = -9903.022753165858
1
Iteration 15900: Loss = -9903.021619306495
Iteration 16000: Loss = -9903.025782602486
1
Iteration 16100: Loss = -9903.027224676593
2
Iteration 16200: Loss = -9903.021641226609
Iteration 16300: Loss = -9903.107663943487
1
Iteration 16400: Loss = -9903.022412488273
2
Iteration 16500: Loss = -9903.021678078847
Iteration 16600: Loss = -9903.02227161183
1
Iteration 16700: Loss = -9903.042672822277
2
Iteration 16800: Loss = -9903.056436323921
3
Iteration 16900: Loss = -9903.021648943552
Iteration 17000: Loss = -9903.021686291671
Iteration 17100: Loss = -9903.047498031434
1
Iteration 17200: Loss = -9903.021637185106
Iteration 17300: Loss = -9903.036639394573
1
Iteration 17400: Loss = -9903.021633488494
Iteration 17500: Loss = -9903.021825067657
1
Iteration 17600: Loss = -9903.022016735844
2
Iteration 17700: Loss = -9903.021647527157
Iteration 17800: Loss = -9903.027183371487
1
Iteration 17900: Loss = -9903.02162165642
Iteration 18000: Loss = -9903.022988450226
1
Iteration 18100: Loss = -9903.024185880655
2
Iteration 18200: Loss = -9903.021618946606
Iteration 18300: Loss = -9903.243486596302
1
Iteration 18400: Loss = -9903.02159954691
Iteration 18500: Loss = -9903.022833303812
1
Iteration 18600: Loss = -9903.021825926608
2
Iteration 18700: Loss = -9903.021636938372
Iteration 18800: Loss = -9903.080865003878
1
Iteration 18900: Loss = -9903.021613171928
Iteration 19000: Loss = -9903.023479897207
1
Iteration 19100: Loss = -9903.06811833483
2
Iteration 19200: Loss = -9903.022184871561
3
Iteration 19300: Loss = -9903.021664447882
Iteration 19400: Loss = -9903.024981881414
1
Iteration 19500: Loss = -9903.02163393603
Iteration 19600: Loss = -9903.021672859919
Iteration 19700: Loss = -9903.03338270505
1
Iteration 19800: Loss = -9903.021605719552
Iteration 19900: Loss = -9903.02167593853
pi: tensor([[0.4916, 0.5084],
        [0.0792, 0.9208]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1903, 0.8097], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0878, 0.1202],
         [0.5216, 0.1498]],

        [[0.6264, 0.0977],
         [0.5999, 0.7220]],

        [[0.7091, 0.1225],
         [0.5557, 0.6985]],

        [[0.6102, 0.0929],
         [0.5442, 0.6842]],

        [[0.7081, 0.0999],
         [0.5594, 0.6034]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.01357925919519104
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.008732110438009916
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0056450870649491815
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.01701445012943135
Global Adjusted Rand Index: 0.004688245061556436
Average Adjusted Rand Index: 0.0034071435369841946
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21855.65447521547
Iteration 100: Loss = -9912.38291991596
Iteration 200: Loss = -9910.850586255086
Iteration 300: Loss = -9910.263159604578
Iteration 400: Loss = -9909.978408778801
Iteration 500: Loss = -9909.807390151578
Iteration 600: Loss = -9909.664861650368
Iteration 700: Loss = -9909.488461083305
Iteration 800: Loss = -9909.302739492767
Iteration 900: Loss = -9909.150664086967
Iteration 1000: Loss = -9909.00916954294
Iteration 1100: Loss = -9908.869130735435
Iteration 1200: Loss = -9908.725667640749
Iteration 1300: Loss = -9908.564509881802
Iteration 1400: Loss = -9908.392081209036
Iteration 1500: Loss = -9908.201397818693
Iteration 1600: Loss = -9908.001520479384
Iteration 1700: Loss = -9907.810561734284
Iteration 1800: Loss = -9907.646949349399
Iteration 1900: Loss = -9907.525220113923
Iteration 2000: Loss = -9907.440802917992
Iteration 2100: Loss = -9907.370145727915
Iteration 2200: Loss = -9907.300501234804
Iteration 2300: Loss = -9907.21820326396
Iteration 2400: Loss = -9907.098111569496
Iteration 2500: Loss = -9906.887460238131
Iteration 2600: Loss = -9906.547444317608
Iteration 2700: Loss = -9906.260857962749
Iteration 2800: Loss = -9906.098513312274
Iteration 2900: Loss = -9906.010523351244
Iteration 3000: Loss = -9905.968618858093
Iteration 3100: Loss = -9905.942462869523
Iteration 3200: Loss = -9905.92475262206
Iteration 3300: Loss = -9905.912405757355
Iteration 3400: Loss = -9905.903413981001
Iteration 3500: Loss = -9905.89621381119
Iteration 3600: Loss = -9905.890428945379
Iteration 3700: Loss = -9905.886133207949
Iteration 3800: Loss = -9905.882466299941
Iteration 3900: Loss = -9905.879134300949
Iteration 4000: Loss = -9905.876565961422
Iteration 4100: Loss = -9905.874642920755
Iteration 4200: Loss = -9905.873071866297
Iteration 4300: Loss = -9905.88280472083
1
Iteration 4400: Loss = -9905.870257006436
Iteration 4500: Loss = -9905.869140308872
Iteration 4600: Loss = -9905.868326627922
Iteration 4700: Loss = -9905.867527514956
Iteration 4800: Loss = -9905.86886430024
1
Iteration 4900: Loss = -9905.866394043445
Iteration 5000: Loss = -9905.865855733957
Iteration 5100: Loss = -9905.865398367894
Iteration 5200: Loss = -9905.865023110639
Iteration 5300: Loss = -9905.877313267196
1
Iteration 5400: Loss = -9905.864310060197
Iteration 5500: Loss = -9905.864011949103
Iteration 5600: Loss = -9905.86385804976
Iteration 5700: Loss = -9905.863461625344
Iteration 5800: Loss = -9905.863239988656
Iteration 5900: Loss = -9905.863022801885
Iteration 6000: Loss = -9905.862875557097
Iteration 6100: Loss = -9905.863225233898
1
Iteration 6200: Loss = -9905.862480225767
Iteration 6300: Loss = -9905.862343957171
Iteration 6400: Loss = -9905.862188600911
Iteration 6500: Loss = -9905.862067602935
Iteration 6600: Loss = -9905.861925562931
Iteration 6700: Loss = -9905.861819749227
Iteration 6800: Loss = -9905.861915450156
Iteration 6900: Loss = -9905.861584010747
Iteration 7000: Loss = -9905.861589531025
Iteration 7100: Loss = -9905.86142127127
Iteration 7200: Loss = -9905.861361921689
Iteration 7300: Loss = -9905.861659273154
1
Iteration 7400: Loss = -9905.861202425707
Iteration 7500: Loss = -9905.861173211364
Iteration 7600: Loss = -9905.86111437241
Iteration 7700: Loss = -9905.861128943623
Iteration 7800: Loss = -9905.8610470121
Iteration 7900: Loss = -9905.861037768302
Iteration 8000: Loss = -9905.860916354857
Iteration 8100: Loss = -9905.860904089755
Iteration 8200: Loss = -9905.863945592308
1
Iteration 8300: Loss = -9905.860850236017
Iteration 8400: Loss = -9905.860865491451
Iteration 8500: Loss = -9905.86079751902
Iteration 8600: Loss = -9905.860852553267
Iteration 8700: Loss = -9905.860753299707
Iteration 8800: Loss = -9905.863852023764
1
Iteration 8900: Loss = -9905.860669132238
Iteration 9000: Loss = -9905.967052838665
1
Iteration 9100: Loss = -9905.860642622643
Iteration 9200: Loss = -9905.860609150584
Iteration 9300: Loss = -9905.862337330644
1
Iteration 9400: Loss = -9905.860582051862
Iteration 9500: Loss = -9906.056120050856
1
Iteration 9600: Loss = -9905.860563289072
Iteration 9700: Loss = -9905.860557553562
Iteration 9800: Loss = -9905.86206840444
1
Iteration 9900: Loss = -9905.860566974967
Iteration 10000: Loss = -9906.167512892594
1
Iteration 10100: Loss = -9905.860575862682
Iteration 10200: Loss = -9905.860532216386
Iteration 10300: Loss = -9905.860549725552
Iteration 10400: Loss = -9905.860547000226
Iteration 10500: Loss = -9906.24554964588
1
Iteration 10600: Loss = -9905.860525809543
Iteration 10700: Loss = -9905.86050316684
Iteration 10800: Loss = -9905.860964902364
1
Iteration 10900: Loss = -9905.86048414471
Iteration 11000: Loss = -9905.861260902131
1
Iteration 11100: Loss = -9905.860517558389
Iteration 11200: Loss = -9905.860465498448
Iteration 11300: Loss = -9905.91872692091
1
Iteration 11400: Loss = -9905.860447183068
Iteration 11500: Loss = -9905.860371690067
Iteration 11600: Loss = -9905.860680746819
1
Iteration 11700: Loss = -9905.860364816248
Iteration 11800: Loss = -9905.906134752995
1
Iteration 11900: Loss = -9905.860326091217
Iteration 12000: Loss = -9905.947950703334
1
Iteration 12100: Loss = -9905.860317186667
Iteration 12200: Loss = -9905.86054265385
1
Iteration 12300: Loss = -9905.860204332963
Iteration 12400: Loss = -9905.861560140725
1
Iteration 12500: Loss = -9905.860085796747
Iteration 12600: Loss = -9905.886872098657
1
Iteration 12700: Loss = -9905.859669373109
Iteration 12800: Loss = -9905.953606431758
1
Iteration 12900: Loss = -9905.857578505396
Iteration 13000: Loss = -9905.873811093154
1
Iteration 13100: Loss = -9905.682695113588
Iteration 13200: Loss = -9903.08912238538
Iteration 13300: Loss = -9903.021648757254
Iteration 13400: Loss = -9903.02949031087
1
Iteration 13500: Loss = -9903.02164714194
Iteration 13600: Loss = -9903.047598647468
1
Iteration 13700: Loss = -9903.021628208506
Iteration 13800: Loss = -9903.0216060557
Iteration 13900: Loss = -9903.02226210362
1
Iteration 14000: Loss = -9903.021583360593
Iteration 14100: Loss = -9903.021606403692
Iteration 14200: Loss = -9903.031561748008
1
Iteration 14300: Loss = -9903.02159544964
Iteration 14400: Loss = -9903.02161681807
Iteration 14500: Loss = -9903.021604786287
Iteration 14600: Loss = -9903.021607922192
Iteration 14700: Loss = -9903.02159254827
Iteration 14800: Loss = -9903.02161227275
Iteration 14900: Loss = -9903.02196505635
1
Iteration 15000: Loss = -9903.021614477124
Iteration 15100: Loss = -9903.021612662415
Iteration 15200: Loss = -9903.021608759685
Iteration 15300: Loss = -9903.021639563762
Iteration 15400: Loss = -9903.021594042319
Iteration 15500: Loss = -9903.02544760219
1
Iteration 15600: Loss = -9903.021598169184
Iteration 15700: Loss = -9903.022542342895
1
Iteration 15800: Loss = -9903.021623392828
Iteration 15900: Loss = -9903.021721593303
Iteration 16000: Loss = -9903.021637796548
Iteration 16100: Loss = -9903.025740653246
1
Iteration 16200: Loss = -9903.02162167255
Iteration 16300: Loss = -9903.48884095065
1
Iteration 16400: Loss = -9903.02162794256
Iteration 16500: Loss = -9903.02159797872
Iteration 16600: Loss = -9903.021865554098
1
Iteration 16700: Loss = -9903.021599168602
Iteration 16800: Loss = -9903.025282876386
1
Iteration 16900: Loss = -9903.02161977296
Iteration 17000: Loss = -9903.3235440767
1
Iteration 17100: Loss = -9903.021617548016
Iteration 17200: Loss = -9903.036434259076
1
Iteration 17300: Loss = -9903.021608857689
Iteration 17400: Loss = -9903.021614357025
Iteration 17500: Loss = -9903.021995713363
1
Iteration 17600: Loss = -9903.021628401024
Iteration 17700: Loss = -9903.022151668498
1
Iteration 17800: Loss = -9903.021596144847
Iteration 17900: Loss = -9903.075301364142
1
Iteration 18000: Loss = -9903.021623697206
Iteration 18100: Loss = -9903.091220441025
1
Iteration 18200: Loss = -9903.021607697676
Iteration 18300: Loss = -9903.021616524211
Iteration 18400: Loss = -9903.023684004038
1
Iteration 18500: Loss = -9903.281741967527
2
Iteration 18600: Loss = -9903.021626798141
Iteration 18700: Loss = -9903.026193390016
1
Iteration 18800: Loss = -9903.021604935475
Iteration 18900: Loss = -9903.02163444951
Iteration 19000: Loss = -9903.021686324932
Iteration 19100: Loss = -9903.021626461874
Iteration 19200: Loss = -9903.0903565341
1
Iteration 19300: Loss = -9903.021620877918
Iteration 19400: Loss = -9903.021590720513
Iteration 19500: Loss = -9903.021828919478
1
Iteration 19600: Loss = -9903.237360173232
2
Iteration 19700: Loss = -9903.021628402614
Iteration 19800: Loss = -9903.029447408782
1
Iteration 19900: Loss = -9903.021621435595
pi: tensor([[0.4849, 0.5151],
        [0.0770, 0.9230]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1864, 0.8136], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0884, 0.1200],
         [0.6436, 0.1482]],

        [[0.6676, 0.0970],
         [0.6562, 0.6464]],

        [[0.5646, 0.1226],
         [0.5545, 0.6568]],

        [[0.5877, 0.0917],
         [0.5172, 0.5183]],

        [[0.7059, 0.0987],
         [0.5952, 0.6371]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.01357925919519104
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.008732110438009916
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0056450870649491815
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.01701445012943135
Global Adjusted Rand Index: 0.004688245061556436
Average Adjusted Rand Index: 0.0034071435369841946
10036.306352043353
[0.004688245061556436, 0.004688245061556436] [0.0034071435369841946, 0.0034071435369841946] [9903.06950730635, 9903.367286253659]
-------------------------------------
This iteration is 66
True Objective function: Loss = -10190.787920498846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23202.50546346491
Iteration 100: Loss = -10135.434547060278
Iteration 200: Loss = -10132.319998484698
Iteration 300: Loss = -10131.417993949843
Iteration 400: Loss = -10131.181316323982
Iteration 500: Loss = -10131.037950300113
Iteration 600: Loss = -10130.923364140039
Iteration 700: Loss = -10130.823079979407
Iteration 800: Loss = -10130.730852495602
Iteration 900: Loss = -10130.654693145874
Iteration 1000: Loss = -10130.601724037568
Iteration 1100: Loss = -10130.565555096728
Iteration 1200: Loss = -10130.541843005478
Iteration 1300: Loss = -10130.523142838874
Iteration 1400: Loss = -10130.508812485687
Iteration 1500: Loss = -10130.497375396311
Iteration 1600: Loss = -10130.486663939342
Iteration 1700: Loss = -10130.473337542147
Iteration 1800: Loss = -10130.452408947971
Iteration 1900: Loss = -10130.413104364508
Iteration 2000: Loss = -10130.349341285524
Iteration 2100: Loss = -10130.307359754293
Iteration 2200: Loss = -10130.292050710057
Iteration 2300: Loss = -10130.28271772769
Iteration 2400: Loss = -10130.274023091839
Iteration 2500: Loss = -10130.263124480174
Iteration 2600: Loss = -10130.246095665027
Iteration 2700: Loss = -10130.210882746987
Iteration 2800: Loss = -10130.0955605584
Iteration 2900: Loss = -10129.552567492303
Iteration 3000: Loss = -10129.39447747471
Iteration 3100: Loss = -10129.37752576948
Iteration 3200: Loss = -10129.369279716822
Iteration 3300: Loss = -10129.363766866561
Iteration 3400: Loss = -10129.360360342822
Iteration 3500: Loss = -10129.358352577128
Iteration 3600: Loss = -10129.357224195253
Iteration 3700: Loss = -10129.356598224356
Iteration 3800: Loss = -10129.356238802618
Iteration 3900: Loss = -10129.356028085816
Iteration 4000: Loss = -10129.355864221923
Iteration 4100: Loss = -10129.355749644687
Iteration 4200: Loss = -10129.355647224633
Iteration 4300: Loss = -10129.355517577895
Iteration 4400: Loss = -10129.355412108858
Iteration 4500: Loss = -10129.355327776817
Iteration 4600: Loss = -10129.355212261382
Iteration 4700: Loss = -10129.355149862951
Iteration 4800: Loss = -10129.355046088513
Iteration 4900: Loss = -10129.354982260505
Iteration 5000: Loss = -10129.354940241496
Iteration 5100: Loss = -10129.35488555854
Iteration 5200: Loss = -10129.354842125987
Iteration 5300: Loss = -10129.354821424844
Iteration 5400: Loss = -10129.354784111547
Iteration 5500: Loss = -10129.3547833452
Iteration 5600: Loss = -10129.354768742523
Iteration 5700: Loss = -10129.354756066588
Iteration 5800: Loss = -10129.354706389693
Iteration 5900: Loss = -10129.355673329017
1
Iteration 6000: Loss = -10129.355510108218
2
Iteration 6100: Loss = -10129.354644528406
Iteration 6200: Loss = -10129.35467939542
Iteration 6300: Loss = -10129.354605692813
Iteration 6400: Loss = -10129.354681558041
Iteration 6500: Loss = -10129.354663053033
Iteration 6600: Loss = -10129.354846973964
1
Iteration 6700: Loss = -10129.354541764795
Iteration 6800: Loss = -10129.355155938509
1
Iteration 6900: Loss = -10129.35452553819
Iteration 7000: Loss = -10129.354514098046
Iteration 7100: Loss = -10129.35448423597
Iteration 7200: Loss = -10129.354487550967
Iteration 7300: Loss = -10129.354473623158
Iteration 7400: Loss = -10129.354446233105
Iteration 7500: Loss = -10129.354516646783
Iteration 7600: Loss = -10129.354556273978
Iteration 7700: Loss = -10129.354447038755
Iteration 7800: Loss = -10129.354724576146
1
Iteration 7900: Loss = -10129.354434234114
Iteration 8000: Loss = -10129.354458404976
Iteration 8100: Loss = -10129.354648478708
1
Iteration 8200: Loss = -10129.355405489567
2
Iteration 8300: Loss = -10129.354421165875
Iteration 8400: Loss = -10129.354459636257
Iteration 8500: Loss = -10129.354417762239
Iteration 8600: Loss = -10129.354565745089
1
Iteration 8700: Loss = -10129.354415262475
Iteration 8800: Loss = -10129.356821068079
1
Iteration 8900: Loss = -10129.35446479571
Iteration 9000: Loss = -10129.354432701572
Iteration 9100: Loss = -10129.682033470233
1
Iteration 9200: Loss = -10129.35441307236
Iteration 9300: Loss = -10129.35444666748
Iteration 9400: Loss = -10129.354588412913
1
Iteration 9500: Loss = -10129.354450166325
Iteration 9600: Loss = -10129.354439355258
Iteration 9700: Loss = -10129.411072676474
1
Iteration 9800: Loss = -10129.354430822323
Iteration 9900: Loss = -10129.354427165792
Iteration 10000: Loss = -10129.374420832124
1
Iteration 10100: Loss = -10129.354434319233
Iteration 10200: Loss = -10129.354431376782
Iteration 10300: Loss = -10129.367964395342
1
Iteration 10400: Loss = -10129.354435180385
Iteration 10500: Loss = -10129.354426226873
Iteration 10600: Loss = -10129.367845044395
1
Iteration 10700: Loss = -10129.35444769266
Iteration 10800: Loss = -10129.5406730447
1
Iteration 10900: Loss = -10129.354447985776
Iteration 11000: Loss = -10129.354443383852
Iteration 11100: Loss = -10129.355070657017
1
Iteration 11200: Loss = -10129.35443574039
Iteration 11300: Loss = -10129.354430646115
Iteration 11400: Loss = -10129.355146923617
1
Iteration 11500: Loss = -10129.354408932168
Iteration 11600: Loss = -10129.354776349377
1
Iteration 11700: Loss = -10129.451309930966
2
Iteration 11800: Loss = -10129.354387027908
Iteration 11900: Loss = -10129.357815120984
1
Iteration 12000: Loss = -10129.354383165839
Iteration 12100: Loss = -10129.356586028625
1
Iteration 12200: Loss = -10129.354405717597
Iteration 12300: Loss = -10129.375364734551
1
Iteration 12400: Loss = -10129.354435002728
Iteration 12500: Loss = -10129.456602945003
1
Iteration 12600: Loss = -10129.354443330601
Iteration 12700: Loss = -10129.354416597169
Iteration 12800: Loss = -10129.354670791201
1
Iteration 12900: Loss = -10129.35442316533
Iteration 13000: Loss = -10129.37700743196
1
Iteration 13100: Loss = -10129.354441499258
Iteration 13200: Loss = -10129.354849552657
1
Iteration 13300: Loss = -10129.354424426401
Iteration 13400: Loss = -10129.35502443309
1
Iteration 13500: Loss = -10129.359761060336
2
Iteration 13600: Loss = -10129.354578119335
3
Iteration 13700: Loss = -10129.390380025492
4
Iteration 13800: Loss = -10129.354457954254
Iteration 13900: Loss = -10129.35653281708
1
Iteration 14000: Loss = -10129.36461508996
2
Iteration 14100: Loss = -10129.354434566525
Iteration 14200: Loss = -10129.358141407996
1
Iteration 14300: Loss = -10129.354545829268
2
Iteration 14400: Loss = -10129.354623556632
3
Iteration 14500: Loss = -10129.355762811461
4
Iteration 14600: Loss = -10129.449123885024
5
Iteration 14700: Loss = -10129.354463768752
Iteration 14800: Loss = -10129.355618326257
1
Iteration 14900: Loss = -10129.417927064133
2
Iteration 15000: Loss = -10129.354490392308
Iteration 15100: Loss = -10129.397028486348
1
Iteration 15200: Loss = -10129.356129243335
2
Iteration 15300: Loss = -10129.359389751513
3
Iteration 15400: Loss = -10129.357068228544
4
Iteration 15500: Loss = -10129.354442858314
Iteration 15600: Loss = -10129.564539041763
1
Iteration 15700: Loss = -10129.354435625803
Iteration 15800: Loss = -10129.363989792004
1
Iteration 15900: Loss = -10129.428202558362
2
Iteration 16000: Loss = -10129.354495924465
Iteration 16100: Loss = -10129.354508838933
Iteration 16200: Loss = -10129.453145324316
1
Iteration 16300: Loss = -10129.35463195591
2
Iteration 16400: Loss = -10129.358942381039
3
Iteration 16500: Loss = -10129.355967992231
4
Iteration 16600: Loss = -10129.357980238845
5
Iteration 16700: Loss = -10129.400590437612
6
Iteration 16800: Loss = -10129.35448823059
Iteration 16900: Loss = -10129.355674508035
1
Iteration 17000: Loss = -10129.354521941817
Iteration 17100: Loss = -10129.354429506897
Iteration 17200: Loss = -10129.354414911588
Iteration 17300: Loss = -10129.355068243214
1
Iteration 17400: Loss = -10129.354534544196
2
Iteration 17500: Loss = -10129.356626798824
3
Iteration 17600: Loss = -10129.354504737628
Iteration 17700: Loss = -10129.354685891982
1
Iteration 17800: Loss = -10129.411225276655
2
Iteration 17900: Loss = -10129.354707989514
3
Iteration 18000: Loss = -10129.355343642748
4
Iteration 18100: Loss = -10129.366769860611
5
Iteration 18200: Loss = -10129.354448756285
Iteration 18300: Loss = -10129.35705073415
1
Iteration 18400: Loss = -10129.354562384926
2
Iteration 18500: Loss = -10129.356794231177
3
Iteration 18600: Loss = -10129.354422920314
Iteration 18700: Loss = -10129.355669639594
1
Iteration 18800: Loss = -10129.354463002375
Iteration 18900: Loss = -10129.35451069787
Iteration 19000: Loss = -10129.35445887128
Iteration 19100: Loss = -10129.40635168883
1
Iteration 19200: Loss = -10129.354429215287
Iteration 19300: Loss = -10129.35443075734
Iteration 19400: Loss = -10129.354914979918
1
Iteration 19500: Loss = -10129.35445170176
Iteration 19600: Loss = -10129.373147139302
1
Iteration 19700: Loss = -10129.354420020809
Iteration 19800: Loss = -10129.354426934917
Iteration 19900: Loss = -10129.355060360233
1
pi: tensor([[0.9838, 0.0162],
        [0.9255, 0.0745]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0204, 0.9796], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1402, 0.1946],
         [0.7077, 0.1334]],

        [[0.5572, 0.2332],
         [0.5637, 0.6996]],

        [[0.5806, 0.1056],
         [0.7091, 0.5755]],

        [[0.6522, 0.1958],
         [0.7089, 0.5586]],

        [[0.7088, 0.2455],
         [0.5996, 0.5881]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: -0.047482343824301235
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: -0.0021032804481535407
Average Adjusted Rand Index: -0.008985699237562988
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23497.523264217234
Iteration 100: Loss = -10136.511736940569
Iteration 200: Loss = -10132.74182219011
Iteration 300: Loss = -10129.230693028809
Iteration 400: Loss = -10126.927832313453
Iteration 500: Loss = -10125.527090341537
Iteration 600: Loss = -10124.419480221904
Iteration 700: Loss = -10123.74283484807
Iteration 800: Loss = -10122.921567963049
Iteration 900: Loss = -10122.100154291624
Iteration 1000: Loss = -10121.146367237701
Iteration 1100: Loss = -10119.839765102208
Iteration 1200: Loss = -10114.201173302252
Iteration 1300: Loss = -10109.168847748875
Iteration 1400: Loss = -10108.094132935936
Iteration 1500: Loss = -10107.854071727194
Iteration 1600: Loss = -10107.779396996822
Iteration 1700: Loss = -10107.759026293503
Iteration 1800: Loss = -10107.747737410948
Iteration 1900: Loss = -10107.740383586352
Iteration 2000: Loss = -10107.735383377605
Iteration 2100: Loss = -10107.732153312849
Iteration 2200: Loss = -10107.730055278991
Iteration 2300: Loss = -10107.728446559944
Iteration 2400: Loss = -10107.727317063789
Iteration 2500: Loss = -10107.725870866769
Iteration 2600: Loss = -10107.724818763318
Iteration 2700: Loss = -10107.725017050674
1
Iteration 2800: Loss = -10107.723267937132
Iteration 2900: Loss = -10107.722729805153
Iteration 3000: Loss = -10107.726396891068
1
Iteration 3100: Loss = -10107.722072208546
Iteration 3200: Loss = -10107.721838528847
Iteration 3300: Loss = -10107.721705632568
Iteration 3400: Loss = -10107.721586613436
Iteration 3500: Loss = -10107.72143895254
Iteration 3600: Loss = -10107.726651808855
1
Iteration 3700: Loss = -10107.72127011483
Iteration 3800: Loss = -10107.721219919062
Iteration 3900: Loss = -10107.730627642584
1
Iteration 4000: Loss = -10107.721071423837
Iteration 4100: Loss = -10107.721403963034
1
Iteration 4200: Loss = -10107.721041962364
Iteration 4300: Loss = -10107.720940784582
Iteration 4400: Loss = -10107.720900671677
Iteration 4500: Loss = -10107.721395754892
1
Iteration 4600: Loss = -10107.720851452215
Iteration 4700: Loss = -10107.724024359582
1
Iteration 4800: Loss = -10107.72080581651
Iteration 4900: Loss = -10107.7207848364
Iteration 5000: Loss = -10107.720770724942
Iteration 5100: Loss = -10107.72075338327
Iteration 5200: Loss = -10107.720934049188
1
Iteration 5300: Loss = -10107.72105166855
2
Iteration 5400: Loss = -10107.722760306116
3
Iteration 5500: Loss = -10107.720710727455
Iteration 5600: Loss = -10107.723440644864
1
Iteration 5700: Loss = -10107.725890110347
2
Iteration 5800: Loss = -10107.720796712903
Iteration 5900: Loss = -10107.720713551656
Iteration 6000: Loss = -10107.721229333094
1
Iteration 6100: Loss = -10107.720685316122
Iteration 6200: Loss = -10107.725487867103
1
Iteration 6300: Loss = -10107.722530901578
2
Iteration 6400: Loss = -10107.720992925491
3
Iteration 6500: Loss = -10107.720681517321
Iteration 6600: Loss = -10107.720814258593
1
Iteration 6700: Loss = -10107.721627293322
2
Iteration 6800: Loss = -10107.720639083902
Iteration 6900: Loss = -10107.721368492137
1
Iteration 7000: Loss = -10107.720986675808
2
Iteration 7100: Loss = -10107.727565118374
3
Iteration 7200: Loss = -10107.720633724935
Iteration 7300: Loss = -10107.720598946673
Iteration 7400: Loss = -10107.722080625826
1
Iteration 7500: Loss = -10107.730863750116
2
Iteration 7600: Loss = -10107.720621086804
Iteration 7700: Loss = -10107.723782439589
1
Iteration 7800: Loss = -10107.720658482749
Iteration 7900: Loss = -10107.720588988448
Iteration 8000: Loss = -10107.720603117868
Iteration 8100: Loss = -10107.720655587536
Iteration 8200: Loss = -10107.723825357305
1
Iteration 8300: Loss = -10107.725300180091
2
Iteration 8400: Loss = -10107.720596232308
Iteration 8500: Loss = -10107.720599148779
Iteration 8600: Loss = -10107.730398224288
1
Iteration 8700: Loss = -10107.720581744765
Iteration 8800: Loss = -10107.720924544754
1
Iteration 8900: Loss = -10107.72057810273
Iteration 9000: Loss = -10107.720815540026
1
Iteration 9100: Loss = -10107.720605868166
Iteration 9200: Loss = -10107.725888494779
1
Iteration 9300: Loss = -10107.720634212257
Iteration 9400: Loss = -10107.723498689631
1
Iteration 9500: Loss = -10107.720595899973
Iteration 9600: Loss = -10107.722703178912
1
Iteration 9700: Loss = -10107.720575194646
Iteration 9800: Loss = -10107.72381045992
1
Iteration 9900: Loss = -10107.720605602259
Iteration 10000: Loss = -10107.722389910608
1
Iteration 10100: Loss = -10107.720553148025
Iteration 10200: Loss = -10107.740245053952
1
Iteration 10300: Loss = -10107.720630375115
Iteration 10400: Loss = -10107.72057041757
Iteration 10500: Loss = -10107.720675264147
1
Iteration 10600: Loss = -10107.720600769357
Iteration 10700: Loss = -10108.017515120886
1
Iteration 10800: Loss = -10107.720562528451
Iteration 10900: Loss = -10107.721069139337
1
Iteration 11000: Loss = -10107.720615238102
Iteration 11100: Loss = -10107.7206108627
Iteration 11200: Loss = -10107.72088042161
1
Iteration 11300: Loss = -10107.720579073353
Iteration 11400: Loss = -10107.735596216024
1
Iteration 11500: Loss = -10107.720587809254
Iteration 11600: Loss = -10107.739445484402
1
Iteration 11700: Loss = -10107.720570120246
Iteration 11800: Loss = -10107.720534444887
Iteration 11900: Loss = -10107.720547933022
Iteration 12000: Loss = -10107.720572886752
Iteration 12100: Loss = -10107.722970276436
1
Iteration 12200: Loss = -10107.720558567931
Iteration 12300: Loss = -10107.720564224528
Iteration 12400: Loss = -10107.720639790183
Iteration 12500: Loss = -10107.72059113819
Iteration 12600: Loss = -10107.778961782255
1
Iteration 12700: Loss = -10107.720567583647
Iteration 12800: Loss = -10107.825737103916
1
Iteration 12900: Loss = -10107.720579588806
Iteration 13000: Loss = -10107.724337671889
1
Iteration 13100: Loss = -10107.720629023761
Iteration 13200: Loss = -10107.720593208502
Iteration 13300: Loss = -10107.720629160032
Iteration 13400: Loss = -10107.72056653941
Iteration 13500: Loss = -10107.720967519686
1
Iteration 13600: Loss = -10107.72055135926
Iteration 13700: Loss = -10107.725423473497
1
Iteration 13800: Loss = -10107.720581192765
Iteration 13900: Loss = -10107.854040344484
1
Iteration 14000: Loss = -10107.720570351672
Iteration 14100: Loss = -10107.720604109307
Iteration 14200: Loss = -10107.720793990438
1
Iteration 14300: Loss = -10107.72058148013
Iteration 14400: Loss = -10107.949061097872
1
Iteration 14500: Loss = -10107.720594639817
Iteration 14600: Loss = -10107.72058792346
Iteration 14700: Loss = -10107.7206278642
Iteration 14800: Loss = -10107.720573552377
Iteration 14900: Loss = -10107.720812024549
1
Iteration 15000: Loss = -10107.720557854791
Iteration 15100: Loss = -10107.720899474936
1
Iteration 15200: Loss = -10107.720577808352
Iteration 15300: Loss = -10107.721176966426
1
Iteration 15400: Loss = -10107.720607106654
Iteration 15500: Loss = -10107.733708221316
1
Iteration 15600: Loss = -10107.720578659591
Iteration 15700: Loss = -10107.842584477112
1
Iteration 15800: Loss = -10107.720566548716
Iteration 15900: Loss = -10107.721556416685
1
Iteration 16000: Loss = -10107.720648036051
Iteration 16100: Loss = -10107.720570336816
Iteration 16200: Loss = -10107.72133990088
1
Iteration 16300: Loss = -10107.72056176782
Iteration 16400: Loss = -10107.725186978478
1
Iteration 16500: Loss = -10107.720577557731
Iteration 16600: Loss = -10107.738850584907
1
Iteration 16700: Loss = -10107.72060113711
Iteration 16800: Loss = -10107.765330899747
1
Iteration 16900: Loss = -10107.720542632429
Iteration 17000: Loss = -10107.82263602225
1
Iteration 17100: Loss = -10107.720600987892
Iteration 17200: Loss = -10107.788857781043
1
Iteration 17300: Loss = -10107.720578575061
Iteration 17400: Loss = -10107.85270805446
1
Iteration 17500: Loss = -10107.72060060575
Iteration 17600: Loss = -10107.721632367902
1
Iteration 17700: Loss = -10107.720635467833
Iteration 17800: Loss = -10107.721185561411
1
Iteration 17900: Loss = -10107.720628094392
Iteration 18000: Loss = -10107.723355954124
1
Iteration 18100: Loss = -10107.720612709567
Iteration 18200: Loss = -10107.720594312605
Iteration 18300: Loss = -10107.720839960928
1
Iteration 18400: Loss = -10107.720574619512
Iteration 18500: Loss = -10107.732357498098
1
Iteration 18600: Loss = -10107.72061427417
Iteration 18700: Loss = -10107.861189578765
1
Iteration 18800: Loss = -10107.720579995737
Iteration 18900: Loss = -10107.720586921312
Iteration 19000: Loss = -10107.720917445979
1
Iteration 19100: Loss = -10107.720562837456
Iteration 19200: Loss = -10107.723318677781
1
Iteration 19300: Loss = -10107.720569476098
Iteration 19400: Loss = -10107.754851991287
1
Iteration 19500: Loss = -10107.72058173789
Iteration 19600: Loss = -10107.805413927475
1
Iteration 19700: Loss = -10107.720582875907
Iteration 19800: Loss = -10108.07052665643
1
Iteration 19900: Loss = -10107.720595806386
pi: tensor([[0.6326, 0.3674],
        [0.0446, 0.9554]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4915, 0.5085], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2315, 0.0891],
         [0.7018, 0.1312]],

        [[0.5780, 0.1606],
         [0.5764, 0.6790]],

        [[0.7221, 0.1378],
         [0.5959, 0.5174]],

        [[0.6798, 0.1724],
         [0.5609, 0.5396]],

        [[0.7175, 0.1582],
         [0.5203, 0.6306]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026262626262626
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 37
Adjusted Rand Index: 0.04271047227926078
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.007262881945936654
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.016979724985067522
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01999651827394162
Global Adjusted Rand Index: 0.0812341752502973
Average Adjusted Rand Index: 0.15112328202806682
10190.787920498846
[-0.0021032804481535407, 0.0812341752502973] [-0.008985699237562988, 0.15112328202806682] [10129.364948992641, 10107.896291485864]
-------------------------------------
This iteration is 67
True Objective function: Loss = -10183.446956519521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23687.56373020885
Iteration 100: Loss = -10109.44462003546
Iteration 200: Loss = -10104.53840459124
Iteration 300: Loss = -10102.368634418188
Iteration 400: Loss = -10100.385518861365
Iteration 500: Loss = -10096.825629150206
Iteration 600: Loss = -10095.036571599565
Iteration 700: Loss = -10094.044059643315
Iteration 800: Loss = -10092.792926776912
Iteration 900: Loss = -10091.701557561535
Iteration 1000: Loss = -10090.5192601798
Iteration 1100: Loss = -10077.276535209994
Iteration 1200: Loss = -10071.351059843304
Iteration 1300: Loss = -10071.16013335262
Iteration 1400: Loss = -10071.119007251005
Iteration 1500: Loss = -10071.091540475489
Iteration 1600: Loss = -10071.055432472804
Iteration 1700: Loss = -10071.035924739363
Iteration 1800: Loss = -10071.027905866556
Iteration 1900: Loss = -10071.021049813784
Iteration 2000: Loss = -10071.015104546319
Iteration 2100: Loss = -10071.00993530057
Iteration 2200: Loss = -10071.008072043955
Iteration 2300: Loss = -10071.006579230483
Iteration 2400: Loss = -10071.005262222605
Iteration 2500: Loss = -10071.004187592269
Iteration 2600: Loss = -10071.003153058551
Iteration 2700: Loss = -10071.0018122989
Iteration 2800: Loss = -10070.999579710111
Iteration 2900: Loss = -10070.998314698856
Iteration 3000: Loss = -10070.997663837039
Iteration 3100: Loss = -10070.99732990026
Iteration 3200: Loss = -10070.997004350915
Iteration 3300: Loss = -10070.996720699188
Iteration 3400: Loss = -10070.996421692993
Iteration 3500: Loss = -10070.996169742932
Iteration 3600: Loss = -10071.00436405015
1
Iteration 3700: Loss = -10070.995462475003
Iteration 3800: Loss = -10070.99890513288
1
Iteration 3900: Loss = -10070.994589716094
Iteration 4000: Loss = -10070.993822434264
Iteration 4100: Loss = -10070.993441336868
Iteration 4200: Loss = -10070.99563182572
1
Iteration 4300: Loss = -10070.993155629616
Iteration 4400: Loss = -10070.993390094372
1
Iteration 4500: Loss = -10070.992859369262
Iteration 4600: Loss = -10071.002206551533
1
Iteration 4700: Loss = -10070.992647239344
Iteration 4800: Loss = -10070.992568021498
Iteration 4900: Loss = -10070.99246704915
Iteration 5000: Loss = -10070.992711592762
1
Iteration 5100: Loss = -10070.99228256617
Iteration 5200: Loss = -10070.992226040918
Iteration 5300: Loss = -10070.99215331629
Iteration 5400: Loss = -10070.992265778756
1
Iteration 5500: Loss = -10070.99198146539
Iteration 5600: Loss = -10070.99200308853
Iteration 5700: Loss = -10070.991738675215
Iteration 5800: Loss = -10071.002495965788
1
Iteration 5900: Loss = -10070.991376424738
Iteration 6000: Loss = -10070.991300837084
Iteration 6100: Loss = -10070.996763040586
1
Iteration 6200: Loss = -10070.99125160476
Iteration 6300: Loss = -10070.991259802517
Iteration 6400: Loss = -10070.991522677203
1
Iteration 6500: Loss = -10070.991262508598
Iteration 6600: Loss = -10070.99164909349
1
Iteration 6700: Loss = -10070.992210655153
2
Iteration 6800: Loss = -10070.991277246429
Iteration 6900: Loss = -10070.991269374457
Iteration 7000: Loss = -10070.991344863127
Iteration 7100: Loss = -10070.991239865509
Iteration 7200: Loss = -10070.992137070732
1
Iteration 7300: Loss = -10070.991394067074
2
Iteration 7400: Loss = -10070.991395883528
3
Iteration 7500: Loss = -10070.991276059543
Iteration 7600: Loss = -10070.991472966083
1
Iteration 7700: Loss = -10070.991280512379
Iteration 7800: Loss = -10070.99123467032
Iteration 7900: Loss = -10071.00316272542
1
Iteration 8000: Loss = -10070.991172867887
Iteration 8100: Loss = -10070.991252067259
Iteration 8200: Loss = -10070.998253302823
1
Iteration 8300: Loss = -10071.022781686013
2
Iteration 8400: Loss = -10071.021728678998
3
Iteration 8500: Loss = -10070.991203960037
Iteration 8600: Loss = -10070.99121854153
Iteration 8700: Loss = -10071.01777805528
1
Iteration 8800: Loss = -10070.991183591115
Iteration 8900: Loss = -10070.994280780369
1
Iteration 9000: Loss = -10070.991175029056
Iteration 9100: Loss = -10070.998761453668
1
Iteration 9200: Loss = -10070.991184145447
Iteration 9300: Loss = -10071.042116337874
1
Iteration 9400: Loss = -10070.991186552603
Iteration 9500: Loss = -10070.991152536344
Iteration 9600: Loss = -10070.991478642774
1
Iteration 9700: Loss = -10070.991139455631
Iteration 9800: Loss = -10071.234061615687
1
Iteration 9900: Loss = -10070.991155871887
Iteration 10000: Loss = -10070.991154791123
Iteration 10100: Loss = -10070.994058155597
1
Iteration 10200: Loss = -10070.991142176172
Iteration 10300: Loss = -10071.149889050308
1
Iteration 10400: Loss = -10070.991144899943
Iteration 10500: Loss = -10070.991984672033
1
Iteration 10600: Loss = -10070.991148211357
Iteration 10700: Loss = -10070.998596344058
1
Iteration 10800: Loss = -10070.991134829259
Iteration 10900: Loss = -10071.005930164296
1
Iteration 11000: Loss = -10070.991151214334
Iteration 11100: Loss = -10070.997601121038
1
Iteration 11200: Loss = -10070.991155904743
Iteration 11300: Loss = -10070.991177438973
Iteration 11400: Loss = -10070.991200047838
Iteration 11500: Loss = -10070.991139448986
Iteration 11600: Loss = -10070.99199912063
1
Iteration 11700: Loss = -10070.991152580034
Iteration 11800: Loss = -10070.991329754677
1
Iteration 11900: Loss = -10070.99115398309
Iteration 12000: Loss = -10070.991561135843
1
Iteration 12100: Loss = -10070.991153587418
Iteration 12200: Loss = -10071.059784077353
1
Iteration 12300: Loss = -10070.991170768635
Iteration 12400: Loss = -10070.99114121408
Iteration 12500: Loss = -10071.223579110598
1
Iteration 12600: Loss = -10070.991158604438
Iteration 12700: Loss = -10070.991143822017
Iteration 12800: Loss = -10071.011346477988
1
Iteration 12900: Loss = -10070.99113843289
Iteration 13000: Loss = -10070.991162275253
Iteration 13100: Loss = -10070.991806785689
1
Iteration 13200: Loss = -10070.991156028387
Iteration 13300: Loss = -10071.090652446255
1
Iteration 13400: Loss = -10070.991163603534
Iteration 13500: Loss = -10070.99114391366
Iteration 13600: Loss = -10070.991408395686
1
Iteration 13700: Loss = -10070.991144181138
Iteration 13800: Loss = -10070.992537676131
1
Iteration 13900: Loss = -10070.991137852783
Iteration 14000: Loss = -10071.010620715686
1
Iteration 14100: Loss = -10070.991143581083
Iteration 14200: Loss = -10070.99114399898
Iteration 14300: Loss = -10070.99216931594
1
Iteration 14400: Loss = -10070.991150906919
Iteration 14500: Loss = -10071.589301350654
1
Iteration 14600: Loss = -10070.991178608714
Iteration 14700: Loss = -10070.991140065058
Iteration 14800: Loss = -10071.174731763505
1
Iteration 14900: Loss = -10070.991131390088
Iteration 15000: Loss = -10070.99113882898
Iteration 15100: Loss = -10070.996641766078
1
Iteration 15200: Loss = -10070.991146575874
Iteration 15300: Loss = -10071.13133388499
1
Iteration 15400: Loss = -10070.991171548903
Iteration 15500: Loss = -10070.991152812163
Iteration 15600: Loss = -10070.992005754175
1
Iteration 15700: Loss = -10070.991157716098
Iteration 15800: Loss = -10070.99204576434
1
Iteration 15900: Loss = -10070.991188868733
Iteration 16000: Loss = -10070.992247857052
1
Iteration 16100: Loss = -10070.991130638598
Iteration 16200: Loss = -10071.002512213125
1
Iteration 16300: Loss = -10070.991136162998
Iteration 16400: Loss = -10070.99122542781
Iteration 16500: Loss = -10070.991219150192
Iteration 16600: Loss = -10070.991146242188
Iteration 16700: Loss = -10070.99115385527
Iteration 16800: Loss = -10070.99112256315
Iteration 16900: Loss = -10070.99373960899
1
Iteration 17000: Loss = -10070.991141811146
Iteration 17100: Loss = -10070.991161817079
Iteration 17200: Loss = -10070.994392029608
1
Iteration 17300: Loss = -10070.99115629571
Iteration 17400: Loss = -10070.991178438755
Iteration 17500: Loss = -10070.991211107952
Iteration 17600: Loss = -10070.99114826918
Iteration 17700: Loss = -10071.003153893633
1
Iteration 17800: Loss = -10070.991138683272
Iteration 17900: Loss = -10070.99115606391
Iteration 18000: Loss = -10070.991240018731
Iteration 18100: Loss = -10070.991131599201
Iteration 18200: Loss = -10071.177630700993
1
Iteration 18300: Loss = -10070.991127741385
Iteration 18400: Loss = -10070.991155645113
Iteration 18500: Loss = -10070.994817690334
1
Iteration 18600: Loss = -10070.991145085734
Iteration 18700: Loss = -10070.991522696362
1
Iteration 18800: Loss = -10070.991191048839
Iteration 18900: Loss = -10070.991150053227
Iteration 19000: Loss = -10070.99114576055
Iteration 19100: Loss = -10070.991237530792
Iteration 19200: Loss = -10070.991183667142
Iteration 19300: Loss = -10071.330101504196
1
Iteration 19400: Loss = -10070.991168751325
Iteration 19500: Loss = -10070.991131069042
Iteration 19600: Loss = -10070.99690552268
1
Iteration 19700: Loss = -10070.991156519127
Iteration 19800: Loss = -10070.991143670606
Iteration 19900: Loss = -10070.99533843375
1
pi: tensor([[0.8497, 0.1503],
        [0.2896, 0.7104]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3823, 0.6177], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1362, 0.1007],
         [0.5365, 0.2173]],

        [[0.6795, 0.1053],
         [0.6298, 0.7297]],

        [[0.7258, 0.1037],
         [0.5359, 0.5545]],

        [[0.6105, 0.0854],
         [0.7207, 0.6872]],

        [[0.6782, 0.1563],
         [0.6707, 0.5692]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 91
Adjusted Rand Index: 0.6685064576664091
time is 1
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 88
Adjusted Rand Index: 0.5732022762545267
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 77
Adjusted Rand Index: 0.28423411838189366
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 85
Adjusted Rand Index: 0.48515105906537936
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.06859081239579141
Global Adjusted Rand Index: 0.3831140883365723
Average Adjusted Rand Index: 0.41593694475280013
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23399.815013324787
Iteration 100: Loss = -10111.34956490316
Iteration 200: Loss = -10107.71384136847
Iteration 300: Loss = -10104.705179999843
Iteration 400: Loss = -10102.2978716324
Iteration 500: Loss = -10101.68309449525
Iteration 600: Loss = -10101.399159742146
Iteration 700: Loss = -10100.953101133675
Iteration 800: Loss = -10097.78662763753
Iteration 900: Loss = -10094.492897652059
Iteration 1000: Loss = -10092.883667278145
Iteration 1100: Loss = -10091.523112677462
Iteration 1200: Loss = -10089.014505040825
Iteration 1300: Loss = -10075.506120898819
Iteration 1400: Loss = -10071.823717739255
Iteration 1500: Loss = -10071.399179421538
Iteration 1600: Loss = -10071.261893855806
Iteration 1700: Loss = -10071.166224765188
Iteration 1800: Loss = -10071.127957378983
Iteration 1900: Loss = -10071.110176058839
Iteration 2000: Loss = -10071.10104325707
Iteration 2100: Loss = -10071.094029804288
Iteration 2200: Loss = -10071.088132077277
Iteration 2300: Loss = -10071.082018355462
Iteration 2400: Loss = -10071.074498024447
Iteration 2500: Loss = -10071.065035601703
Iteration 2600: Loss = -10071.056368150146
Iteration 2700: Loss = -10071.051684359134
Iteration 2800: Loss = -10071.045101793086
Iteration 2900: Loss = -10071.035105292485
Iteration 3000: Loss = -10071.03063866501
Iteration 3100: Loss = -10071.02873462783
Iteration 3200: Loss = -10071.027175954985
Iteration 3300: Loss = -10071.025445942116
Iteration 3400: Loss = -10071.023471748873
Iteration 3500: Loss = -10071.02148634892
Iteration 3600: Loss = -10071.019459363282
Iteration 3700: Loss = -10071.01603635816
Iteration 3800: Loss = -10071.009860054097
Iteration 3900: Loss = -10071.003866929392
Iteration 4000: Loss = -10071.003264496016
Iteration 4100: Loss = -10071.002952721105
Iteration 4200: Loss = -10071.002627523727
Iteration 4300: Loss = -10071.002128826565
Iteration 4400: Loss = -10071.001431910827
Iteration 4500: Loss = -10071.000857576124
Iteration 4600: Loss = -10071.000657870363
Iteration 4700: Loss = -10071.000463784603
Iteration 4800: Loss = -10071.000386581287
Iteration 4900: Loss = -10071.00025041052
Iteration 5000: Loss = -10071.00013333914
Iteration 5100: Loss = -10071.000075263737
Iteration 5200: Loss = -10070.999896267043
Iteration 5300: Loss = -10070.99978882752
Iteration 5400: Loss = -10071.020348815455
1
Iteration 5500: Loss = -10071.000050982775
2
Iteration 5600: Loss = -10070.998923441763
Iteration 5700: Loss = -10070.998377434335
Iteration 5800: Loss = -10070.99853455689
1
Iteration 5900: Loss = -10070.997728351127
Iteration 6000: Loss = -10071.000472153839
1
Iteration 6100: Loss = -10070.9966046661
Iteration 6200: Loss = -10070.996392937292
Iteration 6300: Loss = -10070.996627098752
1
Iteration 6400: Loss = -10071.004771258573
2
Iteration 6500: Loss = -10070.995906085502
Iteration 6600: Loss = -10070.995733477375
Iteration 6700: Loss = -10070.99625292576
1
Iteration 6800: Loss = -10070.995657441857
Iteration 6900: Loss = -10070.99562199097
Iteration 7000: Loss = -10070.995537084804
Iteration 7100: Loss = -10070.997420982272
1
Iteration 7200: Loss = -10070.995335452688
Iteration 7300: Loss = -10070.996542500257
1
Iteration 7400: Loss = -10070.995114623149
Iteration 7500: Loss = -10071.020371532766
1
Iteration 7600: Loss = -10070.995008238795
Iteration 7700: Loss = -10070.99651245964
1
Iteration 7800: Loss = -10070.99482390406
Iteration 7900: Loss = -10070.99470205458
Iteration 8000: Loss = -10070.995305038176
1
Iteration 8100: Loss = -10070.994396221178
Iteration 8200: Loss = -10071.019081557937
1
Iteration 8300: Loss = -10070.994184071385
Iteration 8400: Loss = -10071.00107296816
1
Iteration 8500: Loss = -10070.994035804042
Iteration 8600: Loss = -10070.993835815776
Iteration 8700: Loss = -10070.994130807068
1
Iteration 8800: Loss = -10070.993502736199
Iteration 8900: Loss = -10070.993514465341
Iteration 9000: Loss = -10070.993532087297
Iteration 9100: Loss = -10070.9935059746
Iteration 9200: Loss = -10071.003604948995
1
Iteration 9300: Loss = -10070.993506723444
Iteration 9400: Loss = -10070.993478291624
Iteration 9500: Loss = -10070.994618960074
1
Iteration 9600: Loss = -10070.993498887276
Iteration 9700: Loss = -10070.993468734485
Iteration 9800: Loss = -10070.993558029355
Iteration 9900: Loss = -10070.993467709179
Iteration 10000: Loss = -10071.002006392313
1
Iteration 10100: Loss = -10070.993470697798
Iteration 10200: Loss = -10070.993491339448
Iteration 10300: Loss = -10070.993508153231
Iteration 10400: Loss = -10070.993378378951
Iteration 10500: Loss = -10071.000385914478
1
Iteration 10600: Loss = -10070.99318101948
Iteration 10700: Loss = -10070.993173138937
Iteration 10800: Loss = -10070.994131473264
1
Iteration 10900: Loss = -10070.99313954058
Iteration 11000: Loss = -10070.995211082738
1
Iteration 11100: Loss = -10070.993166754206
Iteration 11200: Loss = -10070.993072613815
Iteration 11300: Loss = -10071.03078033887
1
Iteration 11400: Loss = -10070.992285897655
Iteration 11500: Loss = -10070.992108752454
Iteration 11600: Loss = -10070.99418004366
1
Iteration 11700: Loss = -10070.992047646394
Iteration 11800: Loss = -10070.992074498909
Iteration 11900: Loss = -10070.993031767253
1
Iteration 12000: Loss = -10070.99202934077
Iteration 12100: Loss = -10070.993695558825
1
Iteration 12200: Loss = -10070.991776012
Iteration 12300: Loss = -10071.000441654132
1
Iteration 12400: Loss = -10070.991549378534
Iteration 12500: Loss = -10070.995213193559
1
Iteration 12600: Loss = -10070.991555162975
Iteration 12700: Loss = -10070.991543440807
Iteration 12800: Loss = -10070.9981034524
1
Iteration 12900: Loss = -10070.991544132887
Iteration 13000: Loss = -10070.991557239813
Iteration 13100: Loss = -10070.991975508648
1
Iteration 13200: Loss = -10070.991531781458
Iteration 13300: Loss = -10071.005992078039
1
Iteration 13400: Loss = -10070.991545001974
Iteration 13500: Loss = -10071.071309448962
1
Iteration 13600: Loss = -10070.991528483835
Iteration 13700: Loss = -10071.023533445654
1
Iteration 13800: Loss = -10070.991516974547
Iteration 13900: Loss = -10070.99517031464
1
Iteration 14000: Loss = -10070.991532303176
Iteration 14100: Loss = -10070.991525018546
Iteration 14200: Loss = -10070.991632595056
1
Iteration 14300: Loss = -10070.991502028262
Iteration 14400: Loss = -10071.354438850509
1
Iteration 14500: Loss = -10070.991503562831
Iteration 14600: Loss = -10070.993776688001
1
Iteration 14700: Loss = -10070.991517157368
Iteration 14800: Loss = -10070.991508495925
Iteration 14900: Loss = -10070.99241548522
1
Iteration 15000: Loss = -10070.99151105879
Iteration 15100: Loss = -10071.008918490268
1
Iteration 15200: Loss = -10070.99148124487
Iteration 15300: Loss = -10070.991408005817
Iteration 15400: Loss = -10071.063899962059
1
Iteration 15500: Loss = -10070.991162183123
Iteration 15600: Loss = -10070.991258640213
Iteration 15700: Loss = -10070.991185953695
Iteration 15800: Loss = -10070.991134236108
Iteration 15900: Loss = -10070.997360330164
1
Iteration 16000: Loss = -10070.991131598203
Iteration 16100: Loss = -10071.037398450453
1
Iteration 16200: Loss = -10070.991148492203
Iteration 16300: Loss = -10071.429230972137
1
Iteration 16400: Loss = -10070.991157803886
Iteration 16500: Loss = -10070.991132667928
Iteration 16600: Loss = -10070.99770376012
1
Iteration 16700: Loss = -10070.9911605278
Iteration 16800: Loss = -10070.99116020009
Iteration 16900: Loss = -10070.994653913196
1
Iteration 17000: Loss = -10070.99118185156
Iteration 17100: Loss = -10070.991197591818
Iteration 17200: Loss = -10070.99124364846
Iteration 17300: Loss = -10070.991152496388
Iteration 17400: Loss = -10071.112213087326
1
Iteration 17500: Loss = -10070.991155636795
Iteration 17600: Loss = -10070.9911472664
Iteration 17700: Loss = -10070.992326968872
1
Iteration 17800: Loss = -10070.991140439879
Iteration 17900: Loss = -10070.991148272238
Iteration 18000: Loss = -10070.991212411114
Iteration 18100: Loss = -10070.991138335181
Iteration 18200: Loss = -10070.99187043522
1
Iteration 18300: Loss = -10070.991135883154
Iteration 18400: Loss = -10071.00057542907
1
Iteration 18500: Loss = -10070.991157984761
Iteration 18600: Loss = -10070.99117476674
Iteration 18700: Loss = -10070.992106422313
1
Iteration 18800: Loss = -10070.991155754858
Iteration 18900: Loss = -10071.020118470262
1
Iteration 19000: Loss = -10070.991186710196
Iteration 19100: Loss = -10070.991128460686
Iteration 19200: Loss = -10070.995685669914
1
Iteration 19300: Loss = -10070.99112401881
Iteration 19400: Loss = -10070.991186079937
Iteration 19500: Loss = -10070.991206381557
Iteration 19600: Loss = -10070.991132600235
Iteration 19700: Loss = -10070.99205811695
1
Iteration 19800: Loss = -10070.99119797583
Iteration 19900: Loss = -10070.991162757986
pi: tensor([[0.7108, 0.2892],
        [0.1505, 0.8495]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6182, 0.3818], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2171, 0.1006],
         [0.6486, 0.1364]],

        [[0.5044, 0.1052],
         [0.5240, 0.6802]],

        [[0.5425, 0.1036],
         [0.6782, 0.5562]],

        [[0.5461, 0.0852],
         [0.6075, 0.6838]],

        [[0.5861, 0.1562],
         [0.6253, 0.6361]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 9
Adjusted Rand Index: 0.6685064576664091
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 12
Adjusted Rand Index: 0.5732022762545267
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 23
Adjusted Rand Index: 0.28423411838189366
time is 3
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 15
Adjusted Rand Index: 0.48515105906537936
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.06859081239579141
Global Adjusted Rand Index: 0.3831140883365723
Average Adjusted Rand Index: 0.41593694475280013
10183.446956519521
[0.3831140883365723, 0.3831140883365723] [0.41593694475280013, 0.41593694475280013] [10070.99113035907, 10071.020449965272]
-------------------------------------
This iteration is 68
True Objective function: Loss = -10043.546668513682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23987.61802858943
Iteration 100: Loss = -9941.347018785114
Iteration 200: Loss = -9939.68584768088
Iteration 300: Loss = -9939.199948254583
Iteration 400: Loss = -9938.981943589946
Iteration 500: Loss = -9938.872550247153
Iteration 600: Loss = -9938.799720065956
Iteration 700: Loss = -9938.731653406487
Iteration 800: Loss = -9938.651987235515
Iteration 900: Loss = -9938.539433518874
Iteration 1000: Loss = -9938.362411403552
Iteration 1100: Loss = -9938.094244654769
Iteration 1200: Loss = -9937.768434871732
Iteration 1300: Loss = -9937.37911412513
Iteration 1400: Loss = -9936.885930010014
Iteration 1500: Loss = -9936.39271817308
Iteration 1600: Loss = -9936.113832034694
Iteration 1700: Loss = -9935.982389065493
Iteration 1800: Loss = -9935.876838862936
Iteration 1900: Loss = -9935.790174893496
Iteration 2000: Loss = -9935.705161893775
Iteration 2100: Loss = -9935.655847068907
Iteration 2200: Loss = -9935.629885741439
Iteration 2300: Loss = -9935.614335744056
Iteration 2400: Loss = -9935.606283077494
Iteration 2500: Loss = -9935.603011058569
Iteration 2600: Loss = -9935.601793285696
Iteration 2700: Loss = -9935.601173186617
Iteration 2800: Loss = -9935.600703449452
Iteration 2900: Loss = -9935.600372829716
Iteration 3000: Loss = -9935.600248439334
Iteration 3100: Loss = -9935.600240776963
Iteration 3200: Loss = -9935.600240769025
Iteration 3300: Loss = -9935.600168851524
Iteration 3400: Loss = -9935.600193327811
Iteration 3500: Loss = -9935.600174794698
Iteration 3600: Loss = -9935.600173065695
Iteration 3700: Loss = -9935.601199709228
1
Iteration 3800: Loss = -9935.600139952996
Iteration 3900: Loss = -9935.600163294388
Iteration 4000: Loss = -9935.600284039294
1
Iteration 4100: Loss = -9935.600145792825
Iteration 4200: Loss = -9935.600124725019
Iteration 4300: Loss = -9935.60016760849
Iteration 4400: Loss = -9935.600174486874
Iteration 4500: Loss = -9935.600401155547
1
Iteration 4600: Loss = -9935.600526679715
2
Iteration 4700: Loss = -9935.600111023105
Iteration 4800: Loss = -9935.600201160403
Iteration 4900: Loss = -9935.600144904316
Iteration 5000: Loss = -9935.600247876662
1
Iteration 5100: Loss = -9935.60017684851
Iteration 5200: Loss = -9935.601141528658
1
Iteration 5300: Loss = -9935.600155580354
Iteration 5400: Loss = -9935.600187639753
Iteration 5500: Loss = -9935.600182118344
Iteration 5600: Loss = -9935.600176001011
Iteration 5700: Loss = -9935.604319147298
1
Iteration 5800: Loss = -9935.600149934478
Iteration 5900: Loss = -9935.604371464873
1
Iteration 6000: Loss = -9935.60036040082
2
Iteration 6100: Loss = -9935.600345104976
3
Iteration 6200: Loss = -9935.601253483506
4
Iteration 6300: Loss = -9935.600145440829
Iteration 6400: Loss = -9935.600166658243
Iteration 6500: Loss = -9935.614045411452
1
Iteration 6600: Loss = -9935.600605514004
2
Iteration 6700: Loss = -9935.600451068023
3
Iteration 6800: Loss = -9935.601004497388
4
Iteration 6900: Loss = -9935.60015979034
Iteration 7000: Loss = -9935.600509336966
1
Iteration 7100: Loss = -9935.613845051064
2
Iteration 7200: Loss = -9935.60036019176
3
Iteration 7300: Loss = -9935.60022959816
Iteration 7400: Loss = -9935.600569092745
1
Iteration 7500: Loss = -9935.6002446193
Iteration 7600: Loss = -9935.600178842222
Iteration 7700: Loss = -9935.600385167982
1
Iteration 7800: Loss = -9935.600155650474
Iteration 7900: Loss = -9935.600194291477
Iteration 8000: Loss = -9935.600179436347
Iteration 8100: Loss = -9935.60019789718
Iteration 8200: Loss = -9935.600166635455
Iteration 8300: Loss = -9935.600253004277
Iteration 8400: Loss = -9935.60015096092
Iteration 8500: Loss = -9935.60059318321
1
Iteration 8600: Loss = -9935.600173429151
Iteration 8700: Loss = -9935.60025686193
Iteration 8800: Loss = -9935.607236196614
1
Iteration 8900: Loss = -9935.600163849154
Iteration 9000: Loss = -9935.60022254069
Iteration 9100: Loss = -9935.600176885702
Iteration 9200: Loss = -9935.636628067965
1
Iteration 9300: Loss = -9935.600150074264
Iteration 9400: Loss = -9935.600209287422
Iteration 9500: Loss = -9935.600188568893
Iteration 9600: Loss = -9935.600148113843
Iteration 9700: Loss = -9935.600238740932
Iteration 9800: Loss = -9935.600261163592
Iteration 9900: Loss = -9935.600127548401
Iteration 10000: Loss = -9935.679310888068
1
Iteration 10100: Loss = -9935.60015948833
Iteration 10200: Loss = -9935.60019276025
Iteration 10300: Loss = -9935.600204229413
Iteration 10400: Loss = -9935.670887522047
1
Iteration 10500: Loss = -9935.600148840453
Iteration 10600: Loss = -9935.656026696457
1
Iteration 10700: Loss = -9935.600172936884
Iteration 10800: Loss = -9935.623686188823
1
Iteration 10900: Loss = -9935.600155085329
Iteration 11000: Loss = -9935.600326623877
1
Iteration 11100: Loss = -9935.600158154173
Iteration 11200: Loss = -9935.600297634002
1
Iteration 11300: Loss = -9935.60793680397
2
Iteration 11400: Loss = -9935.600590238982
3
Iteration 11500: Loss = -9935.600286538864
4
Iteration 11600: Loss = -9935.600373103776
5
Iteration 11700: Loss = -9935.600411551477
6
Iteration 11800: Loss = -9935.601908408336
7
Iteration 11900: Loss = -9935.605133376248
8
Iteration 12000: Loss = -9935.604853545687
9
Iteration 12100: Loss = -9935.600203048261
Iteration 12200: Loss = -9935.600433657657
1
Iteration 12300: Loss = -9935.610619775007
2
Iteration 12400: Loss = -9935.600159976388
Iteration 12500: Loss = -9935.600600542657
1
Iteration 12600: Loss = -9935.600148200332
Iteration 12700: Loss = -9935.602000062914
1
Iteration 12800: Loss = -9935.600171418113
Iteration 12900: Loss = -9935.698876116026
1
Iteration 13000: Loss = -9935.651888055392
2
Iteration 13100: Loss = -9935.60367749926
3
Iteration 13200: Loss = -9935.605850168835
4
Iteration 13300: Loss = -9935.72723355005
5
Iteration 13400: Loss = -9935.600234551059
Iteration 13500: Loss = -9935.600183031736
Iteration 13600: Loss = -9935.601628885208
1
Iteration 13700: Loss = -9935.621513478845
2
Iteration 13800: Loss = -9935.601024806441
3
Iteration 13900: Loss = -9935.605123425066
4
Iteration 14000: Loss = -9935.60495387875
5
Iteration 14100: Loss = -9935.620412008231
6
Iteration 14200: Loss = -9935.601936272245
7
Iteration 14300: Loss = -9935.600218742493
Iteration 14400: Loss = -9935.60300738856
1
Iteration 14500: Loss = -9935.665765843349
2
Iteration 14600: Loss = -9935.63175624506
3
Iteration 14700: Loss = -9935.606499415353
4
Iteration 14800: Loss = -9935.603836628363
5
Iteration 14900: Loss = -9935.60063066212
6
Iteration 15000: Loss = -9935.600339378365
7
Iteration 15100: Loss = -9935.647742386258
8
Iteration 15200: Loss = -9935.6024181203
9
Iteration 15300: Loss = -9935.601647776346
10
Iteration 15400: Loss = -9935.60219660171
11
Iteration 15500: Loss = -9935.620555313935
12
Iteration 15600: Loss = -9935.632797851524
13
Iteration 15700: Loss = -9935.600151915574
Iteration 15800: Loss = -9935.641488377152
1
Iteration 15900: Loss = -9935.62279385119
2
Iteration 16000: Loss = -9935.606284507823
3
Iteration 16100: Loss = -9935.600874499862
4
Iteration 16200: Loss = -9935.620137811407
5
Iteration 16300: Loss = -9935.60084637686
6
Iteration 16400: Loss = -9935.601525796808
7
Iteration 16500: Loss = -9935.638955170438
8
Iteration 16600: Loss = -9935.601917452692
9
Iteration 16700: Loss = -9935.620963483461
10
Iteration 16800: Loss = -9935.608814410272
11
Iteration 16900: Loss = -9935.620318653222
12
Iteration 17000: Loss = -9935.600568405174
13
Iteration 17100: Loss = -9935.60020523018
Iteration 17200: Loss = -9935.602963437948
1
Iteration 17300: Loss = -9935.601803870491
2
Iteration 17400: Loss = -9935.607051488116
3
Iteration 17500: Loss = -9935.698909661343
4
Iteration 17600: Loss = -9935.64099367934
5
Iteration 17700: Loss = -9935.67293821877
6
Iteration 17800: Loss = -9935.603364653734
7
Iteration 17900: Loss = -9935.603539421949
8
Iteration 18000: Loss = -9935.600705401475
9
Iteration 18100: Loss = -9935.600211809071
Iteration 18200: Loss = -9935.6027772441
1
Iteration 18300: Loss = -9935.625223679224
2
Iteration 18400: Loss = -9935.628798013653
3
Iteration 18500: Loss = -9935.600349847837
4
Iteration 18600: Loss = -9935.612827285362
5
Iteration 18700: Loss = -9935.603150918394
6
Iteration 18800: Loss = -9935.603100175136
7
Iteration 18900: Loss = -9935.600208776348
Iteration 19000: Loss = -9935.601138915059
1
Iteration 19100: Loss = -9935.605031018282
2
Iteration 19200: Loss = -9935.721701013288
3
Iteration 19300: Loss = -9935.606243187487
4
Iteration 19400: Loss = -9935.61954218434
5
Iteration 19500: Loss = -9935.600214510081
Iteration 19600: Loss = -9935.600567906387
1
Iteration 19700: Loss = -9935.602285434485
2
Iteration 19800: Loss = -9935.603541646973
3
Iteration 19900: Loss = -9935.600307740131
pi: tensor([[0.3372, 0.6628],
        [0.0555, 0.9445]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2650, 0.7350], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2012, 0.1546],
         [0.6529, 0.1305]],

        [[0.6089, 0.1657],
         [0.5171, 0.6456]],

        [[0.6306, 0.1683],
         [0.6419, 0.5942]],

        [[0.6783, 0.1684],
         [0.7078, 0.6738]],

        [[0.6409, 0.1347],
         [0.5007, 0.7189]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.004586397878790981
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0056699812046734
Average Adjusted Rand Index: 0.002914367882850736
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21737.50333876329
Iteration 100: Loss = -9940.663797595571
Iteration 200: Loss = -9939.567412749302
Iteration 300: Loss = -9939.071412807896
Iteration 400: Loss = -9938.695185368259
Iteration 500: Loss = -9938.32134999602
Iteration 600: Loss = -9937.999738348017
Iteration 700: Loss = -9937.706659507816
Iteration 800: Loss = -9937.495533583655
Iteration 900: Loss = -9937.301863002449
Iteration 1000: Loss = -9937.149397512903
Iteration 1100: Loss = -9937.04332299408
Iteration 1200: Loss = -9936.95119178048
Iteration 1300: Loss = -9936.854573783014
Iteration 1400: Loss = -9936.733409857034
Iteration 1500: Loss = -9936.588902891364
Iteration 1600: Loss = -9936.457096912975
Iteration 1700: Loss = -9936.367387156442
Iteration 1800: Loss = -9936.30265978282
Iteration 1900: Loss = -9936.24732673331
Iteration 2000: Loss = -9936.193221031433
Iteration 2100: Loss = -9936.139186436543
Iteration 2200: Loss = -9936.089322547701
Iteration 2300: Loss = -9936.047939128672
Iteration 2400: Loss = -9936.015766773802
Iteration 2500: Loss = -9935.990718472218
Iteration 2600: Loss = -9935.97050186724
Iteration 2700: Loss = -9935.95511343231
Iteration 2800: Loss = -9935.9437112587
Iteration 2900: Loss = -9935.935261910608
Iteration 3000: Loss = -9935.928742334878
Iteration 3100: Loss = -9935.923794661658
Iteration 3200: Loss = -9935.920351000137
Iteration 3300: Loss = -9935.91755536171
Iteration 3400: Loss = -9935.915124208068
Iteration 3500: Loss = -9935.912922247251
Iteration 3600: Loss = -9935.910957958455
Iteration 3700: Loss = -9935.909132927696
Iteration 3800: Loss = -9935.907562693495
Iteration 3900: Loss = -9935.906171024444
Iteration 4000: Loss = -9935.904889851496
Iteration 4100: Loss = -9935.903721341863
Iteration 4200: Loss = -9935.902638405383
Iteration 4300: Loss = -9935.901673127437
Iteration 4400: Loss = -9935.90074346033
Iteration 4500: Loss = -9935.899931524531
Iteration 4600: Loss = -9935.899116163217
Iteration 4700: Loss = -9935.898368833985
Iteration 4800: Loss = -9935.897701081418
Iteration 4900: Loss = -9935.897119588059
Iteration 5000: Loss = -9935.896534726415
Iteration 5100: Loss = -9935.89604468961
Iteration 5200: Loss = -9935.895567049516
Iteration 5300: Loss = -9935.895045753232
Iteration 5400: Loss = -9935.894641568768
Iteration 5500: Loss = -9935.894214949476
Iteration 5600: Loss = -9935.893852862147
Iteration 5700: Loss = -9935.893508400366
Iteration 5800: Loss = -9935.893139301017
Iteration 5900: Loss = -9935.892826079293
Iteration 6000: Loss = -9935.89250053281
Iteration 6100: Loss = -9935.892240618097
Iteration 6200: Loss = -9935.891951452897
Iteration 6300: Loss = -9935.891675817022
Iteration 6400: Loss = -9935.891448854625
Iteration 6500: Loss = -9935.891188982518
Iteration 6600: Loss = -9935.890985631326
Iteration 6700: Loss = -9935.890763493095
Iteration 6800: Loss = -9935.89054079979
Iteration 6900: Loss = -9935.890363564637
Iteration 7000: Loss = -9935.890155374665
Iteration 7100: Loss = -9935.890011971776
Iteration 7200: Loss = -9935.890686401954
1
Iteration 7300: Loss = -9935.889711312875
Iteration 7400: Loss = -9935.894979077315
1
Iteration 7500: Loss = -9935.889385613089
Iteration 7600: Loss = -9935.889268688868
Iteration 7700: Loss = -9935.889261001703
Iteration 7800: Loss = -9935.892990219221
1
Iteration 7900: Loss = -9935.888929798548
Iteration 8000: Loss = -9935.889086462459
1
Iteration 8100: Loss = -9935.888674537182
Iteration 8200: Loss = -9935.889543434054
1
Iteration 8300: Loss = -9935.888431959673
Iteration 8400: Loss = -9935.88834257869
Iteration 8500: Loss = -9935.888240191993
Iteration 8600: Loss = -9935.888191802178
Iteration 8700: Loss = -9935.888123312623
Iteration 8800: Loss = -9935.888261824572
1
Iteration 8900: Loss = -9935.88799658922
Iteration 9000: Loss = -9935.887933696673
Iteration 9100: Loss = -9935.888047143259
1
Iteration 9200: Loss = -9935.887804827866
Iteration 9300: Loss = -9935.887780789577
Iteration 9400: Loss = -9935.888687526036
1
Iteration 9500: Loss = -9935.887655327773
Iteration 9600: Loss = -9935.88758731305
Iteration 9700: Loss = -9935.887758167471
1
Iteration 9800: Loss = -9935.88751650421
Iteration 9900: Loss = -9935.8874745504
Iteration 10000: Loss = -9935.889022717474
1
Iteration 10100: Loss = -9935.887396049005
Iteration 10200: Loss = -9935.887344245803
Iteration 10300: Loss = -9935.887419262417
Iteration 10400: Loss = -9935.887288143358
Iteration 10500: Loss = -9935.887281005755
Iteration 10600: Loss = -9935.91522804373
1
Iteration 10700: Loss = -9935.887225063336
Iteration 10800: Loss = -9935.887181518308
Iteration 10900: Loss = -9935.887839993366
1
Iteration 11000: Loss = -9935.887888762165
2
Iteration 11100: Loss = -9935.892912343697
3
Iteration 11200: Loss = -9935.887312508556
4
Iteration 11300: Loss = -9935.887155594075
Iteration 11400: Loss = -9935.940342716129
1
Iteration 11500: Loss = -9935.88937121938
2
Iteration 11600: Loss = -9935.89374538242
3
Iteration 11700: Loss = -9935.895913266357
4
Iteration 11800: Loss = -9935.887365676372
5
Iteration 11900: Loss = -9935.887186625569
Iteration 12000: Loss = -9935.91294403367
1
Iteration 12100: Loss = -9935.886929649363
Iteration 12200: Loss = -9935.942792248437
1
Iteration 12300: Loss = -9935.886920883566
Iteration 12400: Loss = -9935.897485585921
1
Iteration 12500: Loss = -9935.983139682417
2
Iteration 12600: Loss = -9935.886909057415
Iteration 12700: Loss = -9935.887791889618
1
Iteration 12800: Loss = -9935.886863003176
Iteration 12900: Loss = -9935.887054397519
1
Iteration 13000: Loss = -9935.886862777568
Iteration 13100: Loss = -9935.887053412722
1
Iteration 13200: Loss = -9935.886839429013
Iteration 13300: Loss = -9935.886888104276
Iteration 13400: Loss = -9935.886861578927
Iteration 13500: Loss = -9935.887030346401
1
Iteration 13600: Loss = -9935.886828325447
Iteration 13700: Loss = -9935.887029896752
1
Iteration 13800: Loss = -9935.905139937682
2
Iteration 13900: Loss = -9935.886798836253
Iteration 14000: Loss = -9935.888995576392
1
Iteration 14100: Loss = -9935.88725695263
2
Iteration 14200: Loss = -9935.888168029294
3
Iteration 14300: Loss = -9935.886767744718
Iteration 14400: Loss = -9936.103720622212
1
Iteration 14500: Loss = -9935.896778821803
2
Iteration 14600: Loss = -9935.887387406778
3
Iteration 14700: Loss = -9935.887046887594
4
Iteration 14800: Loss = -9935.892944699137
5
Iteration 14900: Loss = -9935.898769978732
6
Iteration 15000: Loss = -9935.886974978856
7
Iteration 15100: Loss = -9935.886786232813
Iteration 15200: Loss = -9935.998459312086
1
Iteration 15300: Loss = -9935.886764713832
Iteration 15400: Loss = -9935.99036164793
1
Iteration 15500: Loss = -9935.889322618532
2
Iteration 15600: Loss = -9935.88674289366
Iteration 15700: Loss = -9935.893940309106
1
Iteration 15800: Loss = -9935.886710180937
Iteration 15900: Loss = -9935.893803109682
1
Iteration 16000: Loss = -9935.887105020052
2
Iteration 16100: Loss = -9936.002657092844
3
Iteration 16200: Loss = -9935.886740287233
Iteration 16300: Loss = -9935.924625538037
1
Iteration 16400: Loss = -9935.886754744004
Iteration 16500: Loss = -9935.923648161248
1
Iteration 16600: Loss = -9935.886762116275
Iteration 16700: Loss = -9935.886791303006
Iteration 16800: Loss = -9935.90386326407
1
Iteration 16900: Loss = -9935.888304912707
2
Iteration 17000: Loss = -9935.88708975612
3
Iteration 17100: Loss = -9935.890076340409
4
Iteration 17200: Loss = -9935.886740190916
Iteration 17300: Loss = -9935.887009537344
1
Iteration 17400: Loss = -9935.891459226528
2
Iteration 17500: Loss = -9935.89302084891
3
Iteration 17600: Loss = -9935.890226421081
4
Iteration 17700: Loss = -9935.887259114146
5
Iteration 17800: Loss = -9935.944708561205
6
Iteration 17900: Loss = -9935.887903600315
7
Iteration 18000: Loss = -9935.88772516266
8
Iteration 18100: Loss = -9935.88772486702
9
Iteration 18200: Loss = -9935.887373187537
10
Iteration 18300: Loss = -9935.887261908592
11
Iteration 18400: Loss = -9935.886818502677
Iteration 18500: Loss = -9935.88781440126
1
Iteration 18600: Loss = -9935.887716296958
2
Iteration 18700: Loss = -9935.886754130994
Iteration 18800: Loss = -9935.888066238378
1
Iteration 18900: Loss = -9935.88733060233
2
Iteration 19000: Loss = -9935.887718795395
3
Iteration 19100: Loss = -9935.887375259588
4
Iteration 19200: Loss = -9935.899980308162
5
Iteration 19300: Loss = -9935.886944716362
6
Iteration 19400: Loss = -9935.887297296473
7
Iteration 19500: Loss = -9935.892267003726
8
Iteration 19600: Loss = -9935.887240875289
9
Iteration 19700: Loss = -9935.894849101895
10
Iteration 19800: Loss = -9935.932984729458
11
Iteration 19900: Loss = -9935.895445303518
12
pi: tensor([[1.0000e+00, 1.2590e-07],
        [6.9416e-01, 3.0584e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8678, 0.1322], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1358, 0.1646],
         [0.5197, 0.2279]],

        [[0.5690, 0.2011],
         [0.5976, 0.5585]],

        [[0.6023, 0.0707],
         [0.5522, 0.5494]],

        [[0.6390, 0.1038],
         [0.7021, 0.5747]],

        [[0.6456, 0.1403],
         [0.6829, 0.5546]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0006184227628479228
Average Adjusted Rand Index: -0.00012062544077000013
10043.546668513682
[0.0056699812046734, 0.0006184227628479228] [0.002914367882850736, -0.00012062544077000013] [9935.602057025631, 9935.886766767264]
-------------------------------------
This iteration is 69
True Objective function: Loss = -10069.684055968703
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24854.500479279803
Iteration 100: Loss = -9953.640827571018
Iteration 200: Loss = -9952.282728857193
Iteration 300: Loss = -9951.862592534548
Iteration 400: Loss = -9951.67605093282
Iteration 500: Loss = -9951.5645368784
Iteration 600: Loss = -9951.474477073703
Iteration 700: Loss = -9951.386311591537
Iteration 800: Loss = -9951.290665760798
Iteration 900: Loss = -9951.175360014071
Iteration 1000: Loss = -9951.016247458076
Iteration 1100: Loss = -9950.765210688925
Iteration 1200: Loss = -9950.329905082986
Iteration 1300: Loss = -9949.60750655506
Iteration 1400: Loss = -9948.690782252866
Iteration 1500: Loss = -9947.893248483577
Iteration 1600: Loss = -9947.430705710954
Iteration 1700: Loss = -9947.213847583153
Iteration 1800: Loss = -9947.142235298717
Iteration 1900: Loss = -9947.126137618769
Iteration 2000: Loss = -9947.119062694774
Iteration 2100: Loss = -9947.114638037041
Iteration 2200: Loss = -9947.111533923295
Iteration 2300: Loss = -9947.109253427634
Iteration 2400: Loss = -9947.107430309648
Iteration 2500: Loss = -9947.105868004433
Iteration 2600: Loss = -9947.104317757226
Iteration 2700: Loss = -9947.112793709235
1
Iteration 2800: Loss = -9947.100732834268
Iteration 2900: Loss = -9947.098231979995
Iteration 3000: Loss = -9947.094679495323
Iteration 3100: Loss = -9947.088621550927
Iteration 3200: Loss = -9947.078093871396
Iteration 3300: Loss = -9947.050359612173
Iteration 3400: Loss = -9947.01069548284
Iteration 3500: Loss = -9946.975535558524
Iteration 3600: Loss = -9946.92663197178
Iteration 3700: Loss = -9946.83873851783
Iteration 3800: Loss = -9946.540597927764
Iteration 3900: Loss = -9945.008924970714
Iteration 4000: Loss = -9942.485500929719
Iteration 4100: Loss = -9942.471154770767
Iteration 4200: Loss = -9942.465606836377
Iteration 4300: Loss = -9942.462463263719
Iteration 4400: Loss = -9942.460331835371
Iteration 4500: Loss = -9942.495925297731
1
Iteration 4600: Loss = -9942.457796172981
Iteration 4700: Loss = -9942.45690996849
Iteration 4800: Loss = -9942.456240284142
Iteration 4900: Loss = -9942.455717615057
Iteration 5000: Loss = -9942.455576422217
Iteration 5100: Loss = -9942.454840981753
Iteration 5200: Loss = -9942.459473484514
1
Iteration 5300: Loss = -9942.454267503654
Iteration 5400: Loss = -9942.454721416463
1
Iteration 5500: Loss = -9942.453791834701
Iteration 5600: Loss = -9942.453613554848
Iteration 5700: Loss = -9942.45384479019
1
Iteration 5800: Loss = -9942.45331750045
Iteration 5900: Loss = -9942.454736833979
1
Iteration 6000: Loss = -9942.453066172307
Iteration 6100: Loss = -9942.452959077957
Iteration 6200: Loss = -9942.453872891489
1
Iteration 6300: Loss = -9942.45279293058
Iteration 6400: Loss = -9942.452722500182
Iteration 6500: Loss = -9942.452620463146
Iteration 6600: Loss = -9942.479870591369
1
Iteration 6700: Loss = -9942.452469834583
Iteration 6800: Loss = -9942.453055641994
1
Iteration 6900: Loss = -9942.45242252658
Iteration 7000: Loss = -9942.452299546258
Iteration 7100: Loss = -9942.452957746358
1
Iteration 7200: Loss = -9942.479467426178
2
Iteration 7300: Loss = -9942.452187164137
Iteration 7400: Loss = -9942.456986709283
1
Iteration 7500: Loss = -9942.452120275
Iteration 7600: Loss = -9942.46030458223
1
Iteration 7700: Loss = -9942.45204975804
Iteration 7800: Loss = -9942.452046188617
Iteration 7900: Loss = -9942.452069955896
Iteration 8000: Loss = -9942.45198350378
Iteration 8100: Loss = -9942.452350678175
1
Iteration 8200: Loss = -9942.45190487827
Iteration 8300: Loss = -9942.451906911583
Iteration 8400: Loss = -9942.452162959104
1
Iteration 8500: Loss = -9942.45184313088
Iteration 8600: Loss = -9942.463189350467
1
Iteration 8700: Loss = -9942.451814716494
Iteration 8800: Loss = -9942.46239447039
1
Iteration 8900: Loss = -9942.451811369592
Iteration 9000: Loss = -9942.472219704627
1
Iteration 9100: Loss = -9942.451801562645
Iteration 9200: Loss = -9942.451801212213
Iteration 9300: Loss = -9942.451778824707
Iteration 9400: Loss = -9942.451716038053
Iteration 9500: Loss = -9942.477100263312
1
Iteration 9600: Loss = -9942.451755276343
Iteration 9700: Loss = -9942.451708100965
Iteration 9800: Loss = -9942.451868109687
1
Iteration 9900: Loss = -9942.451760402657
Iteration 10000: Loss = -9942.452935797459
1
Iteration 10100: Loss = -9942.451696828086
Iteration 10200: Loss = -9942.458139597373
1
Iteration 10300: Loss = -9942.451712441463
Iteration 10400: Loss = -9942.451687108143
Iteration 10500: Loss = -9942.452808554324
1
Iteration 10600: Loss = -9942.451700499347
Iteration 10700: Loss = -9942.451687692972
Iteration 10800: Loss = -9942.452601257815
1
Iteration 10900: Loss = -9942.45172133732
Iteration 11000: Loss = -9942.452542223473
1
Iteration 11100: Loss = -9942.451696404802
Iteration 11200: Loss = -9942.451673377936
Iteration 11300: Loss = -9942.452227669106
1
Iteration 11400: Loss = -9942.451666307046
Iteration 11500: Loss = -9942.991359895117
1
Iteration 11600: Loss = -9942.451694854904
Iteration 11700: Loss = -9942.451664234286
Iteration 11800: Loss = -9942.700887044755
1
Iteration 11900: Loss = -9942.451725773331
Iteration 12000: Loss = -9942.451699126665
Iteration 12100: Loss = -9942.452209315863
1
Iteration 12200: Loss = -9942.451672996687
Iteration 12300: Loss = -9942.451684401422
Iteration 12400: Loss = -9942.451879824512
1
Iteration 12500: Loss = -9942.451676022432
Iteration 12600: Loss = -9942.881634406782
1
Iteration 12700: Loss = -9942.45166390722
Iteration 12800: Loss = -9942.45168407312
Iteration 12900: Loss = -9942.468540971726
1
Iteration 13000: Loss = -9942.451670773222
Iteration 13100: Loss = -9942.451718650602
Iteration 13200: Loss = -9942.451737172407
Iteration 13300: Loss = -9942.451671883726
Iteration 13400: Loss = -9942.45339825887
1
Iteration 13500: Loss = -9942.45164861747
Iteration 13600: Loss = -9942.52590713761
1
Iteration 13700: Loss = -9942.451673465604
Iteration 13800: Loss = -9942.451705236037
Iteration 13900: Loss = -9942.45168538265
Iteration 14000: Loss = -9942.451656386163
Iteration 14100: Loss = -9942.453949138908
1
Iteration 14200: Loss = -9942.451641946813
Iteration 14300: Loss = -9942.451684511643
Iteration 14400: Loss = -9942.481832955727
1
Iteration 14500: Loss = -9942.451672590103
Iteration 14600: Loss = -9942.451668022626
Iteration 14700: Loss = -9942.45957558
1
Iteration 14800: Loss = -9942.451701531962
Iteration 14900: Loss = -9942.453780642249
1
Iteration 15000: Loss = -9942.451654731727
Iteration 15100: Loss = -9942.453743418919
1
Iteration 15200: Loss = -9942.451653348768
Iteration 15300: Loss = -9942.480379380506
1
Iteration 15400: Loss = -9942.451709046383
Iteration 15500: Loss = -9942.451655147717
Iteration 15600: Loss = -9942.460526950372
1
Iteration 15700: Loss = -9942.45165037905
Iteration 15800: Loss = -9942.45168083176
Iteration 15900: Loss = -9942.4518256348
1
Iteration 16000: Loss = -9942.451663661552
Iteration 16100: Loss = -9942.454564937627
1
Iteration 16200: Loss = -9942.455932675732
2
Iteration 16300: Loss = -9942.457547325059
3
Iteration 16400: Loss = -9942.50184247832
4
Iteration 16500: Loss = -9942.451679887427
Iteration 16600: Loss = -9942.485257800754
1
Iteration 16700: Loss = -9942.451651881318
Iteration 16800: Loss = -9942.451670166072
Iteration 16900: Loss = -9942.459767478764
1
Iteration 17000: Loss = -9942.451668318947
Iteration 17100: Loss = -9942.45462239116
1
Iteration 17200: Loss = -9942.45167685871
Iteration 17300: Loss = -9942.451792535987
1
Iteration 17400: Loss = -9942.451609894155
Iteration 17500: Loss = -9942.452003870265
1
Iteration 17600: Loss = -9942.451676462593
Iteration 17700: Loss = -9942.45484654028
1
Iteration 17800: Loss = -9942.451698961291
Iteration 17900: Loss = -9942.72769003166
1
Iteration 18000: Loss = -9942.4516717912
Iteration 18100: Loss = -9942.451676719731
Iteration 18200: Loss = -9942.451971407658
1
Iteration 18300: Loss = -9942.451677969997
Iteration 18400: Loss = -9942.459933027361
1
Iteration 18500: Loss = -9942.45167862038
Iteration 18600: Loss = -9942.451721701187
Iteration 18700: Loss = -9942.451741871317
Iteration 18800: Loss = -9942.451671908302
Iteration 18900: Loss = -9942.45169559281
Iteration 19000: Loss = -9942.45166304586
Iteration 19100: Loss = -9942.454626226941
1
Iteration 19200: Loss = -9942.451655631881
Iteration 19300: Loss = -9942.94029763177
1
Iteration 19400: Loss = -9942.45168829034
Iteration 19500: Loss = -9942.451670148828
Iteration 19600: Loss = -9942.453609877573
1
Iteration 19700: Loss = -9942.451671185678
Iteration 19800: Loss = -9942.451925710666
1
Iteration 19900: Loss = -9942.451700735406
pi: tensor([[0.7973, 0.2027],
        [0.4016, 0.5984]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0011, 0.9989], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1913, 0.1625],
         [0.7071, 0.1292]],

        [[0.6950, 0.1420],
         [0.7046, 0.5740]],

        [[0.5690, 0.0941],
         [0.6788, 0.5948]],

        [[0.5482, 0.1038],
         [0.6578, 0.7239]],

        [[0.7028, 0.1069],
         [0.5840, 0.6921]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 27
Adjusted Rand Index: 0.20291496141962828
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 23
Adjusted Rand Index: 0.28378781795909125
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 30
Adjusted Rand Index: 0.15239186941991315
Global Adjusted Rand Index: 0.2026479111011882
Average Adjusted Rand Index: 0.2752128691536659
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22042.658379110002
Iteration 100: Loss = -9953.483119820747
Iteration 200: Loss = -9952.361724789153
Iteration 300: Loss = -9951.947656223394
Iteration 400: Loss = -9951.738260047803
Iteration 500: Loss = -9951.610504622124
Iteration 600: Loss = -9951.501793528703
Iteration 700: Loss = -9951.376659880887
Iteration 800: Loss = -9951.209382867692
Iteration 900: Loss = -9950.979447686166
Iteration 1000: Loss = -9950.671138696174
Iteration 1100: Loss = -9950.256728577497
Iteration 1200: Loss = -9949.71247200045
Iteration 1300: Loss = -9949.087927450142
Iteration 1400: Loss = -9948.48582807243
Iteration 1500: Loss = -9948.010090152278
Iteration 1600: Loss = -9947.735796347946
Iteration 1700: Loss = -9947.508735287416
Iteration 1800: Loss = -9947.34426875897
Iteration 1900: Loss = -9947.259222309232
Iteration 2000: Loss = -9947.218511886327
Iteration 2100: Loss = -9947.194808351216
Iteration 2200: Loss = -9947.180555617366
Iteration 2300: Loss = -9947.16581501521
Iteration 2400: Loss = -9947.156282974285
Iteration 2500: Loss = -9947.148904012538
Iteration 2600: Loss = -9947.142756475236
Iteration 2700: Loss = -9947.138151727515
Iteration 2800: Loss = -9947.133737252885
Iteration 2900: Loss = -9947.130247318775
Iteration 3000: Loss = -9947.128039429712
Iteration 3100: Loss = -9947.124621267081
Iteration 3200: Loss = -9947.122351902393
Iteration 3300: Loss = -9947.120488061402
Iteration 3400: Loss = -9947.118568134207
Iteration 3500: Loss = -9947.128324857049
1
Iteration 3600: Loss = -9947.11556460908
Iteration 3700: Loss = -9947.115395773735
Iteration 3800: Loss = -9947.11318658886
Iteration 3900: Loss = -9947.112301685887
Iteration 4000: Loss = -9947.111099362995
Iteration 4100: Loss = -9947.110197035992
Iteration 4200: Loss = -9947.109299710988
Iteration 4300: Loss = -9947.108438940502
Iteration 4400: Loss = -9947.107613864287
Iteration 4500: Loss = -9947.1067257631
Iteration 4600: Loss = -9947.109027857145
1
Iteration 4700: Loss = -9947.104613082052
Iteration 4800: Loss = -9947.104285076295
Iteration 4900: Loss = -9947.102122985916
Iteration 5000: Loss = -9947.100706430861
Iteration 5100: Loss = -9947.102115000802
1
Iteration 5200: Loss = -9947.090585740643
Iteration 5300: Loss = -9947.059423062401
Iteration 5400: Loss = -9947.03004973097
Iteration 5500: Loss = -9946.9960527927
Iteration 5600: Loss = -9946.939811341326
Iteration 5700: Loss = -9946.790729891265
Iteration 5800: Loss = -9942.964622313331
Iteration 5900: Loss = -9942.471076872924
Iteration 6000: Loss = -9942.463934757163
Iteration 6100: Loss = -9942.462820722316
Iteration 6200: Loss = -9942.458628764729
Iteration 6300: Loss = -9942.457321999293
Iteration 6400: Loss = -9942.45640687619
Iteration 6500: Loss = -9942.455688364225
Iteration 6600: Loss = -9942.455303921428
Iteration 6700: Loss = -9942.454703061667
Iteration 6800: Loss = -9942.464149267415
1
Iteration 6900: Loss = -9942.454098123204
Iteration 7000: Loss = -9942.453802366439
Iteration 7100: Loss = -9942.45361478073
Iteration 7200: Loss = -9942.465228108063
1
Iteration 7300: Loss = -9942.454322313846
2
Iteration 7400: Loss = -9942.453486637938
Iteration 7500: Loss = -9942.45298584822
Iteration 7600: Loss = -9942.475070441562
1
Iteration 7700: Loss = -9942.45275321405
Iteration 7800: Loss = -9942.452879740818
1
Iteration 7900: Loss = -9942.452639199983
Iteration 8000: Loss = -9942.452519790722
Iteration 8100: Loss = -9942.457383432147
1
Iteration 8200: Loss = -9942.45238588966
Iteration 8300: Loss = -9942.452399507883
Iteration 8400: Loss = -9942.452347287257
Iteration 8500: Loss = -9942.452245617435
Iteration 8600: Loss = -9942.456875956139
1
Iteration 8700: Loss = -9942.452162848705
Iteration 8800: Loss = -9942.455181051584
1
Iteration 8900: Loss = -9942.452118464704
Iteration 9000: Loss = -9942.452047633664
Iteration 9100: Loss = -9942.45450532765
1
Iteration 9200: Loss = -9942.451987257997
Iteration 9300: Loss = -9942.45197628729
Iteration 9400: Loss = -9942.452250329754
1
Iteration 9500: Loss = -9942.451927078984
Iteration 9600: Loss = -9942.454392812711
1
Iteration 9700: Loss = -9942.45190680136
Iteration 9800: Loss = -9942.4518811879
Iteration 9900: Loss = -9942.491411472407
1
Iteration 10000: Loss = -9942.451800483164
Iteration 10100: Loss = -9942.451839676169
Iteration 10200: Loss = -9942.484759486933
1
Iteration 10300: Loss = -9942.451809985183
Iteration 10400: Loss = -9942.451752905257
Iteration 10500: Loss = -9942.452843600893
1
Iteration 10600: Loss = -9942.451784538944
Iteration 10700: Loss = -9942.474996528128
1
Iteration 10800: Loss = -9942.451756470422
Iteration 10900: Loss = -9942.452420578918
1
Iteration 11000: Loss = -9942.476378400703
2
Iteration 11100: Loss = -9942.451732326845
Iteration 11200: Loss = -9942.452983920044
1
Iteration 11300: Loss = -9942.451739647911
Iteration 11400: Loss = -9942.45171179532
Iteration 11500: Loss = -9942.453621203891
1
Iteration 11600: Loss = -9942.451687146791
Iteration 11700: Loss = -9942.744118616381
1
Iteration 11800: Loss = -9942.451691848739
Iteration 11900: Loss = -9942.859233832954
1
Iteration 12000: Loss = -9942.45169321706
Iteration 12100: Loss = -9942.451684140959
Iteration 12200: Loss = -9942.451973377532
1
Iteration 12300: Loss = -9942.45167044745
Iteration 12400: Loss = -9942.52163637233
1
Iteration 12500: Loss = -9942.451703649802
Iteration 12600: Loss = -9942.451667225605
Iteration 12700: Loss = -9942.478327639961
1
Iteration 12800: Loss = -9942.451699206043
Iteration 12900: Loss = -9942.457720647699
1
Iteration 13000: Loss = -9942.451713813392
Iteration 13100: Loss = -9942.452160058412
1
Iteration 13200: Loss = -9942.451672936191
Iteration 13300: Loss = -9942.458170630403
1
Iteration 13400: Loss = -9942.451661167503
Iteration 13500: Loss = -9942.45165492176
Iteration 13600: Loss = -9942.452210084382
1
Iteration 13700: Loss = -9942.451658210886
Iteration 13800: Loss = -9942.460593582158
1
Iteration 13900: Loss = -9942.451669697344
Iteration 14000: Loss = -9942.451663435215
Iteration 14100: Loss = -9942.451793460044
1
Iteration 14200: Loss = -9942.451662672658
Iteration 14300: Loss = -9942.596569356096
1
Iteration 14400: Loss = -9942.451662869624
Iteration 14500: Loss = -9942.451673125523
Iteration 14600: Loss = -9942.460207330329
1
Iteration 14700: Loss = -9942.451632708973
Iteration 14800: Loss = -9942.458511488949
1
Iteration 14900: Loss = -9942.451658584143
Iteration 15000: Loss = -9942.451661693542
Iteration 15100: Loss = -9942.452209980462
1
Iteration 15200: Loss = -9942.451694003663
Iteration 15300: Loss = -9942.512163659594
1
Iteration 15400: Loss = -9942.451661269346
Iteration 15500: Loss = -9942.45163919267
Iteration 15600: Loss = -9942.452284120754
1
Iteration 15700: Loss = -9942.451651919413
Iteration 15800: Loss = -9942.452684248457
1
Iteration 15900: Loss = -9942.45165385647
Iteration 16000: Loss = -9942.451676470871
Iteration 16100: Loss = -9942.451896800023
1
Iteration 16200: Loss = -9942.451663565222
Iteration 16300: Loss = -9942.523876406958
1
Iteration 16400: Loss = -9942.451666890764
Iteration 16500: Loss = -9942.451878283373
1
Iteration 16600: Loss = -9942.451732074944
Iteration 16700: Loss = -9942.451668541004
Iteration 16800: Loss = -9942.453359057317
1
Iteration 16900: Loss = -9942.451696231194
Iteration 17000: Loss = -9942.577220522015
1
Iteration 17100: Loss = -9942.451672618716
Iteration 17200: Loss = -9942.451682227376
Iteration 17300: Loss = -9942.4519991793
1
Iteration 17400: Loss = -9942.451668050848
Iteration 17500: Loss = -9942.453331093002
1
Iteration 17600: Loss = -9942.45163709553
Iteration 17700: Loss = -9942.466458405126
1
Iteration 17800: Loss = -9942.451676724088
Iteration 17900: Loss = -9942.46438455679
1
Iteration 18000: Loss = -9942.45167974962
Iteration 18100: Loss = -9942.451664301305
Iteration 18200: Loss = -9942.451807827058
1
Iteration 18300: Loss = -9942.451646954429
Iteration 18400: Loss = -9942.461408649557
1
Iteration 18500: Loss = -9942.451678452178
Iteration 18600: Loss = -9942.451699950208
Iteration 18700: Loss = -9942.451782531587
Iteration 18800: Loss = -9942.451693660856
Iteration 18900: Loss = -9942.481339224343
1
Iteration 19000: Loss = -9942.451649082122
Iteration 19100: Loss = -9942.451682411926
Iteration 19200: Loss = -9942.457768453305
1
Iteration 19300: Loss = -9942.451658120148
Iteration 19400: Loss = -9942.733803373636
1
Iteration 19500: Loss = -9942.451646884589
Iteration 19600: Loss = -9942.451665162942
Iteration 19700: Loss = -9942.463482323592
1
Iteration 19800: Loss = -9942.451645063124
Iteration 19900: Loss = -9942.451678150032
pi: tensor([[0.5984, 0.4016],
        [0.2026, 0.7974]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9989, 0.0011], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1292, 0.1624],
         [0.7008, 0.1913]],

        [[0.6747, 0.1421],
         [0.5695, 0.5952]],

        [[0.5475, 0.0941],
         [0.5439, 0.6099]],

        [[0.5211, 0.1038],
         [0.5912, 0.6051]],

        [[0.5329, 0.1069],
         [0.5828, 0.6336]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 73
Adjusted Rand Index: 0.20291496141962828
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 77
Adjusted Rand Index: 0.28378781795909125
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 70
Adjusted Rand Index: 0.15239186941991315
Global Adjusted Rand Index: 0.2026479111011882
Average Adjusted Rand Index: 0.2752128691536659
10069.684055968703
[0.2026479111011882, 0.2026479111011882] [0.2752128691536659, 0.2752128691536659] [9942.451650312185, 9942.45174503528]
-------------------------------------
This iteration is 70
True Objective function: Loss = -10172.849423412514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22362.826809300615
Iteration 100: Loss = -10070.491972315125
Iteration 200: Loss = -10066.16744840714
Iteration 300: Loss = -10064.111109362799
Iteration 400: Loss = -10063.29117134239
Iteration 500: Loss = -10062.673438831933
Iteration 600: Loss = -10062.17643927658
Iteration 700: Loss = -10061.774512420596
Iteration 800: Loss = -10061.489871717156
Iteration 900: Loss = -10061.27380389905
Iteration 1000: Loss = -10061.096027964524
Iteration 1100: Loss = -10060.93852163272
Iteration 1200: Loss = -10060.77681774781
Iteration 1300: Loss = -10060.58928985936
Iteration 1400: Loss = -10060.430794168644
Iteration 1500: Loss = -10060.319948613313
Iteration 1600: Loss = -10060.229683315134
Iteration 1700: Loss = -10060.151553285259
Iteration 1800: Loss = -10060.076022287885
Iteration 1900: Loss = -10059.991783475321
Iteration 2000: Loss = -10059.880171507124
Iteration 2100: Loss = -10059.711797014545
Iteration 2200: Loss = -10059.425457467789
Iteration 2300: Loss = -10058.609585259392
Iteration 2400: Loss = -10056.615354407824
Iteration 2500: Loss = -10053.91683687586
Iteration 2600: Loss = -10052.683545804286
Iteration 2700: Loss = -10052.470707835013
Iteration 2800: Loss = -10052.377041726686
Iteration 2900: Loss = -10052.335150821207
Iteration 3000: Loss = -10052.306364276072
Iteration 3100: Loss = -10052.285091607513
Iteration 3200: Loss = -10052.268694079754
Iteration 3300: Loss = -10052.255557291495
Iteration 3400: Loss = -10052.244808022697
Iteration 3500: Loss = -10052.235869193693
Iteration 3600: Loss = -10052.228323429754
Iteration 3700: Loss = -10052.22189219631
Iteration 3800: Loss = -10052.216307987097
Iteration 3900: Loss = -10052.211425442114
Iteration 4000: Loss = -10052.20718921743
Iteration 4100: Loss = -10052.203408804357
Iteration 4200: Loss = -10052.20004241688
Iteration 4300: Loss = -10052.197067727355
Iteration 4400: Loss = -10052.194390485023
Iteration 4500: Loss = -10052.191927917147
Iteration 4600: Loss = -10052.18970792462
Iteration 4700: Loss = -10052.187689310802
Iteration 4800: Loss = -10052.18593943429
Iteration 4900: Loss = -10052.184222366619
Iteration 5000: Loss = -10052.182680738408
Iteration 5100: Loss = -10052.181271511681
Iteration 5200: Loss = -10052.179930563225
Iteration 5300: Loss = -10052.182543468653
1
Iteration 5400: Loss = -10052.177634931355
Iteration 5500: Loss = -10052.176644625479
Iteration 5600: Loss = -10052.175670682209
Iteration 5700: Loss = -10052.174763278159
Iteration 5800: Loss = -10052.174658935463
Iteration 5900: Loss = -10052.17318123679
Iteration 6000: Loss = -10052.172754922336
Iteration 6100: Loss = -10052.171970744099
Iteration 6200: Loss = -10052.171210682052
Iteration 6300: Loss = -10052.174809355014
1
Iteration 6400: Loss = -10052.171975493267
2
Iteration 6500: Loss = -10052.16967108853
Iteration 6600: Loss = -10052.169286011269
Iteration 6700: Loss = -10052.16890885136
Iteration 6800: Loss = -10052.16822980069
Iteration 6900: Loss = -10052.167698950168
Iteration 7000: Loss = -10052.167867814293
1
Iteration 7100: Loss = -10052.167010246758
Iteration 7200: Loss = -10052.167668576027
1
Iteration 7300: Loss = -10052.166469181511
Iteration 7400: Loss = -10052.16683516324
1
Iteration 7500: Loss = -10052.16601473406
Iteration 7600: Loss = -10052.165509523358
Iteration 7700: Loss = -10052.165268527862
Iteration 7800: Loss = -10052.164994676377
Iteration 7900: Loss = -10052.1649468986
Iteration 8000: Loss = -10052.164590972747
Iteration 8100: Loss = -10052.164414178458
Iteration 8200: Loss = -10052.174532050258
1
Iteration 8300: Loss = -10052.183461433424
2
Iteration 8400: Loss = -10052.182898542835
3
Iteration 8500: Loss = -10052.168033573413
4
Iteration 8600: Loss = -10052.180513203866
5
Iteration 8700: Loss = -10052.163434980243
Iteration 8800: Loss = -10052.16330675058
Iteration 8900: Loss = -10052.163106058832
Iteration 9000: Loss = -10052.163046907242
Iteration 9100: Loss = -10052.162889257508
Iteration 9200: Loss = -10052.173687599161
1
Iteration 9300: Loss = -10052.162664397158
Iteration 9400: Loss = -10052.162556316414
Iteration 9500: Loss = -10052.174139961753
1
Iteration 9600: Loss = -10052.162360449589
Iteration 9700: Loss = -10052.162297778159
Iteration 9800: Loss = -10052.162310679969
Iteration 9900: Loss = -10052.16213429117
Iteration 10000: Loss = -10052.162082421471
Iteration 10100: Loss = -10052.164170526738
1
Iteration 10200: Loss = -10052.16193991476
Iteration 10300: Loss = -10052.161885499756
Iteration 10400: Loss = -10052.162451740367
1
Iteration 10500: Loss = -10052.161759523804
Iteration 10600: Loss = -10052.161692582382
Iteration 10700: Loss = -10052.161831859165
1
Iteration 10800: Loss = -10052.161597280778
Iteration 10900: Loss = -10052.208743723559
1
Iteration 11000: Loss = -10052.181366180088
2
Iteration 11100: Loss = -10052.194504460118
3
Iteration 11200: Loss = -10052.161436479506
Iteration 11300: Loss = -10052.161455040594
Iteration 11400: Loss = -10052.173849265739
1
Iteration 11500: Loss = -10052.217560857385
2
Iteration 11600: Loss = -10052.168915769036
3
Iteration 11700: Loss = -10052.161915670698
4
Iteration 11800: Loss = -10052.161301508477
Iteration 11900: Loss = -10052.163715316683
1
Iteration 12000: Loss = -10052.161204217568
Iteration 12100: Loss = -10052.16164466717
1
Iteration 12200: Loss = -10052.161453301303
2
Iteration 12300: Loss = -10052.161795743976
3
Iteration 12400: Loss = -10052.353267718858
4
Iteration 12500: Loss = -10052.16112017634
Iteration 12600: Loss = -10052.16140934499
1
Iteration 12700: Loss = -10052.161100221096
Iteration 12800: Loss = -10052.161039278197
Iteration 12900: Loss = -10052.164721398764
1
Iteration 13000: Loss = -10052.164434162527
2
Iteration 13100: Loss = -10052.16465674714
3
Iteration 13200: Loss = -10052.161294454225
4
Iteration 13300: Loss = -10052.274748417018
5
Iteration 13400: Loss = -10052.160930015903
Iteration 13500: Loss = -10052.161089746673
1
Iteration 13600: Loss = -10052.22373807438
2
Iteration 13700: Loss = -10052.200152417598
3
Iteration 13800: Loss = -10052.160904247618
Iteration 13900: Loss = -10052.161324979415
1
Iteration 14000: Loss = -10052.18067476231
2
Iteration 14100: Loss = -10052.164684075933
3
Iteration 14200: Loss = -10052.161571443068
4
Iteration 14300: Loss = -10052.162307063158
5
Iteration 14400: Loss = -10052.283170275467
6
Iteration 14500: Loss = -10052.160845040184
Iteration 14600: Loss = -10052.161402437283
1
Iteration 14700: Loss = -10052.162582388655
2
Iteration 14800: Loss = -10052.162273684906
3
Iteration 14900: Loss = -10052.161203592856
4
Iteration 15000: Loss = -10052.1681679732
5
Iteration 15100: Loss = -10052.161692735066
6
Iteration 15200: Loss = -10052.160860416167
Iteration 15300: Loss = -10052.160896124542
Iteration 15400: Loss = -10052.16190719887
1
Iteration 15500: Loss = -10052.16152616455
2
Iteration 15600: Loss = -10052.160777257232
Iteration 15700: Loss = -10052.232048513373
1
Iteration 15800: Loss = -10052.31907963777
2
Iteration 15900: Loss = -10052.17316765691
3
Iteration 16000: Loss = -10052.190843155026
4
Iteration 16100: Loss = -10052.183306290406
5
Iteration 16200: Loss = -10052.174084337694
6
Iteration 16300: Loss = -10052.16346683346
7
Iteration 16400: Loss = -10052.167744071312
8
Iteration 16500: Loss = -10052.161435895368
9
Iteration 16600: Loss = -10052.16095181216
10
Iteration 16700: Loss = -10052.162472176946
11
Iteration 16800: Loss = -10052.161808834122
12
Iteration 16900: Loss = -10052.160785351298
Iteration 17000: Loss = -10052.22149035113
1
Iteration 17100: Loss = -10052.160873007391
Iteration 17200: Loss = -10052.167873194825
1
Iteration 17300: Loss = -10052.160772271027
Iteration 17400: Loss = -10052.16183348307
1
Iteration 17500: Loss = -10052.166453855285
2
Iteration 17600: Loss = -10052.161207118535
3
Iteration 17700: Loss = -10052.1607933845
Iteration 17800: Loss = -10052.161440984686
1
Iteration 17900: Loss = -10052.160718721847
Iteration 18000: Loss = -10052.161936416898
1
Iteration 18100: Loss = -10052.163858529899
2
Iteration 18200: Loss = -10052.185476175762
3
Iteration 18300: Loss = -10052.179838775455
4
Iteration 18400: Loss = -10052.194181536091
5
Iteration 18500: Loss = -10052.161690601302
6
Iteration 18600: Loss = -10052.160730577863
Iteration 18700: Loss = -10052.232548572181
1
Iteration 18800: Loss = -10052.194664882403
2
Iteration 18900: Loss = -10052.160687325279
Iteration 19000: Loss = -10052.160877943617
1
Iteration 19100: Loss = -10052.160796215237
2
Iteration 19200: Loss = -10052.160778450274
Iteration 19300: Loss = -10052.160750605159
Iteration 19400: Loss = -10052.161182274964
1
Iteration 19500: Loss = -10052.160920715854
2
Iteration 19600: Loss = -10052.170380805994
3
Iteration 19700: Loss = -10052.171883505283
4
Iteration 19800: Loss = -10052.312319669867
5
Iteration 19900: Loss = -10052.160918668325
6
pi: tensor([[4.7715e-01, 5.2285e-01],
        [1.4730e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2174, 0.7826], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3513, 0.1123],
         [0.6779, 0.1367]],

        [[0.5666, 0.2048],
         [0.6779, 0.6145]],

        [[0.6125, 0.1508],
         [0.5444, 0.6553]],

        [[0.5258, 0.1702],
         [0.7010, 0.5981]],

        [[0.6362, 0.1687],
         [0.5547, 0.6150]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 32
Adjusted Rand Index: 0.12360995370539879
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.019406386485148783
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002385170646104761
Average Adjusted Rand Index: 0.02038776688210715
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23900.866346855208
Iteration 100: Loss = -10064.311051659512
Iteration 200: Loss = -10062.892173903532
Iteration 300: Loss = -10062.467466098387
Iteration 400: Loss = -10062.089203791713
Iteration 500: Loss = -10061.606430583442
Iteration 600: Loss = -10060.83205739106
Iteration 700: Loss = -10060.265653603608
Iteration 800: Loss = -10060.11467972309
Iteration 900: Loss = -10060.064285750981
Iteration 1000: Loss = -10060.03428342813
Iteration 1100: Loss = -10060.014236768848
Iteration 1200: Loss = -10060.000007905392
Iteration 1300: Loss = -10059.989514448891
Iteration 1400: Loss = -10059.981539479237
Iteration 1500: Loss = -10059.975361981407
Iteration 1600: Loss = -10059.970527758784
Iteration 1700: Loss = -10059.966578170393
Iteration 1800: Loss = -10059.963451072692
Iteration 1900: Loss = -10059.960853365053
Iteration 2000: Loss = -10059.958640255803
Iteration 2100: Loss = -10059.956858922938
Iteration 2200: Loss = -10059.955339412658
Iteration 2300: Loss = -10059.953979387305
Iteration 2400: Loss = -10059.952871430009
Iteration 2500: Loss = -10059.951797444675
Iteration 2600: Loss = -10059.95093191767
Iteration 2700: Loss = -10059.950157035837
Iteration 2800: Loss = -10059.949358299531
Iteration 2900: Loss = -10059.94876386185
Iteration 3000: Loss = -10059.948160206935
Iteration 3100: Loss = -10059.947626178773
Iteration 3200: Loss = -10059.947087861521
Iteration 3300: Loss = -10059.946633038448
Iteration 3400: Loss = -10059.946160942538
Iteration 3500: Loss = -10059.94575386261
Iteration 3600: Loss = -10059.945359327601
Iteration 3700: Loss = -10059.94493051884
Iteration 3800: Loss = -10059.944474039212
Iteration 3900: Loss = -10059.943945584748
Iteration 4000: Loss = -10059.942991217877
Iteration 4100: Loss = -10059.940240846901
Iteration 4200: Loss = -10059.922318135288
Iteration 4300: Loss = -10059.822709784039
Iteration 4400: Loss = -10059.794305570442
Iteration 4500: Loss = -10059.791031509962
Iteration 4600: Loss = -10059.789823752766
Iteration 4700: Loss = -10059.789109756392
Iteration 4800: Loss = -10059.788659144
Iteration 4900: Loss = -10059.788360978517
Iteration 5000: Loss = -10059.788054722838
Iteration 5100: Loss = -10059.787816650125
Iteration 5200: Loss = -10059.787612159282
Iteration 5300: Loss = -10059.787880222273
1
Iteration 5400: Loss = -10059.787240155742
Iteration 5500: Loss = -10059.787137821519
Iteration 5600: Loss = -10059.78695865192
Iteration 5700: Loss = -10059.786824992729
Iteration 5800: Loss = -10059.786773384818
Iteration 5900: Loss = -10059.786582006207
Iteration 6000: Loss = -10059.795623061447
1
Iteration 6100: Loss = -10059.786411854137
Iteration 6200: Loss = -10059.786327315525
Iteration 6300: Loss = -10059.786233700679
Iteration 6400: Loss = -10059.78617847339
Iteration 6500: Loss = -10059.786241115991
Iteration 6600: Loss = -10059.78601453925
Iteration 6700: Loss = -10059.786016623517
Iteration 6800: Loss = -10059.785934031026
Iteration 6900: Loss = -10059.785868962317
Iteration 7000: Loss = -10059.785818083456
Iteration 7100: Loss = -10059.785760460887
Iteration 7200: Loss = -10059.785738229977
Iteration 7300: Loss = -10059.789398351406
1
Iteration 7400: Loss = -10059.78565091354
Iteration 7500: Loss = -10059.785600811229
Iteration 7600: Loss = -10059.786248904247
1
Iteration 7700: Loss = -10059.785551485864
Iteration 7800: Loss = -10059.785508673072
Iteration 7900: Loss = -10059.785558573982
Iteration 8000: Loss = -10059.785430284346
Iteration 8100: Loss = -10059.785411668372
Iteration 8200: Loss = -10059.785381444672
Iteration 8300: Loss = -10059.785381146066
Iteration 8400: Loss = -10059.785392841655
Iteration 8500: Loss = -10059.792149015526
1
Iteration 8600: Loss = -10059.787430870554
2
Iteration 8700: Loss = -10059.785293029921
Iteration 8800: Loss = -10059.80135064324
1
Iteration 8900: Loss = -10059.785252762353
Iteration 9000: Loss = -10059.85318406415
1
Iteration 9100: Loss = -10059.785243393055
Iteration 9200: Loss = -10059.785187974276
Iteration 9300: Loss = -10059.789578236254
1
Iteration 9400: Loss = -10059.785173564334
Iteration 9500: Loss = -10059.785123634261
Iteration 9600: Loss = -10059.788833754625
1
Iteration 9700: Loss = -10059.785115722108
Iteration 9800: Loss = -10059.785103593815
Iteration 9900: Loss = -10059.79426491097
1
Iteration 10000: Loss = -10059.785113157192
Iteration 10100: Loss = -10059.785067747225
Iteration 10200: Loss = -10059.86855228969
1
Iteration 10300: Loss = -10059.785063412512
Iteration 10400: Loss = -10059.785031683568
Iteration 10500: Loss = -10059.798818991223
1
Iteration 10600: Loss = -10059.785042999541
Iteration 10700: Loss = -10059.785034283244
Iteration 10800: Loss = -10059.792362475677
1
Iteration 10900: Loss = -10059.785027901344
Iteration 11000: Loss = -10059.785002754363
Iteration 11100: Loss = -10059.785122055777
1
Iteration 11200: Loss = -10059.785000568756
Iteration 11300: Loss = -10059.784974586672
Iteration 11400: Loss = -10059.784941194734
Iteration 11500: Loss = -10059.784962681932
Iteration 11600: Loss = -10059.785627933734
1
Iteration 11700: Loss = -10059.785007305029
Iteration 11800: Loss = -10059.793576764214
1
Iteration 11900: Loss = -10059.789356821813
2
Iteration 12000: Loss = -10059.78524074253
3
Iteration 12100: Loss = -10059.785366945232
4
Iteration 12200: Loss = -10059.791591174982
5
Iteration 12300: Loss = -10059.826925796317
6
Iteration 12400: Loss = -10059.78496716962
Iteration 12500: Loss = -10059.788676203716
1
Iteration 12600: Loss = -10059.784931283275
Iteration 12700: Loss = -10059.785171663123
1
Iteration 12800: Loss = -10059.78492066828
Iteration 12900: Loss = -10059.933112075078
1
Iteration 13000: Loss = -10059.784936509112
Iteration 13100: Loss = -10059.784943853765
Iteration 13200: Loss = -10059.78836521413
1
Iteration 13300: Loss = -10059.784927598897
Iteration 13400: Loss = -10059.785033510667
1
Iteration 13500: Loss = -10059.92774979028
2
Iteration 13600: Loss = -10059.784950433
Iteration 13700: Loss = -10059.784953608105
Iteration 13800: Loss = -10059.78963894953
1
Iteration 13900: Loss = -10059.784901541643
Iteration 14000: Loss = -10059.803493892448
1
Iteration 14100: Loss = -10059.784909213713
Iteration 14200: Loss = -10059.785698111535
1
Iteration 14300: Loss = -10059.78501726176
2
Iteration 14400: Loss = -10059.78535768905
3
Iteration 14500: Loss = -10059.791977240611
4
Iteration 14600: Loss = -10059.78524835479
5
Iteration 14700: Loss = -10059.784909282627
Iteration 14800: Loss = -10059.78853956593
1
Iteration 14900: Loss = -10059.78534321985
2
Iteration 15000: Loss = -10059.785581819444
3
Iteration 15100: Loss = -10059.791088729544
4
Iteration 15200: Loss = -10059.784905564895
Iteration 15300: Loss = -10059.784982264006
Iteration 15400: Loss = -10059.785000908943
Iteration 15500: Loss = -10059.793258058107
1
Iteration 15600: Loss = -10059.784892854175
Iteration 15700: Loss = -10059.78498783057
Iteration 15800: Loss = -10059.805564545788
1
Iteration 15900: Loss = -10059.78488585205
Iteration 16000: Loss = -10059.785132664207
1
Iteration 16100: Loss = -10059.784991410324
2
Iteration 16200: Loss = -10059.785129060208
3
Iteration 16300: Loss = -10059.825762098002
4
Iteration 16400: Loss = -10059.784888275773
Iteration 16500: Loss = -10059.96949424407
1
Iteration 16600: Loss = -10059.784889803073
Iteration 16700: Loss = -10059.812522133849
1
Iteration 16800: Loss = -10059.802299961942
2
Iteration 16900: Loss = -10059.784966469932
Iteration 17000: Loss = -10059.786058777483
1
Iteration 17100: Loss = -10059.784891192106
Iteration 17200: Loss = -10059.785331614494
1
Iteration 17300: Loss = -10059.784922469647
Iteration 17400: Loss = -10059.787686221727
1
Iteration 17500: Loss = -10059.78524958007
2
Iteration 17600: Loss = -10059.784886010608
Iteration 17700: Loss = -10059.788127697642
1
Iteration 17800: Loss = -10059.784901748924
Iteration 17900: Loss = -10059.786310195766
1
Iteration 18000: Loss = -10059.785161735703
2
Iteration 18100: Loss = -10059.881775327234
3
Iteration 18200: Loss = -10059.784884646851
Iteration 18300: Loss = -10059.786497711046
1
Iteration 18400: Loss = -10059.784879288114
Iteration 18500: Loss = -10059.78538552977
1
Iteration 18600: Loss = -10059.784866335332
Iteration 18700: Loss = -10059.784959593011
Iteration 18800: Loss = -10059.788502340129
1
Iteration 18900: Loss = -10059.7863783862
2
Iteration 19000: Loss = -10059.785144542668
3
Iteration 19100: Loss = -10059.785231822103
4
Iteration 19200: Loss = -10059.784897856407
Iteration 19300: Loss = -10059.784924419304
Iteration 19400: Loss = -10059.815911146181
1
Iteration 19500: Loss = -10059.785185512566
2
Iteration 19600: Loss = -10059.784968327178
Iteration 19700: Loss = -10059.784996990178
Iteration 19800: Loss = -10059.7930612286
1
Iteration 19900: Loss = -10059.785259032482
2
pi: tensor([[1.1310e-07, 1.0000e+00],
        [4.1415e-02, 9.5858e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0170, 0.9830], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2318, 0.0686],
         [0.6598, 0.1395]],

        [[0.7137, 0.2359],
         [0.7020, 0.5112]],

        [[0.5182, 0.0762],
         [0.6295, 0.5374]],

        [[0.6191, 0.1516],
         [0.6340, 0.7204]],

        [[0.6436, 0.1703],
         [0.5066, 0.7028]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.01717781179455718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009033746065289061
Average Adjusted Rand Index: -0.0030817158592678583
10172.849423412514
[-0.002385170646104761, -0.0009033746065289061] [0.02038776688210715, -0.0030817158592678583] [10052.162501802102, 10059.784964494402]
-------------------------------------
This iteration is 71
True Objective function: Loss = -10184.320208632842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23261.658961028817
Iteration 100: Loss = -10083.15022423384
Iteration 200: Loss = -10082.508752020647
Iteration 300: Loss = -10081.890055311795
Iteration 400: Loss = -10079.369945811202
Iteration 500: Loss = -10078.607116024401
Iteration 600: Loss = -10077.600829861842
Iteration 700: Loss = -10073.092234592223
Iteration 800: Loss = -10071.04565156939
Iteration 900: Loss = -10070.434505697891
Iteration 1000: Loss = -10062.283275111766
Iteration 1100: Loss = -10061.919490738517
Iteration 1200: Loss = -10061.871891082523
Iteration 1300: Loss = -10061.859035967205
Iteration 1400: Loss = -10061.853949569157
Iteration 1500: Loss = -10061.851445407261
Iteration 1600: Loss = -10061.850051247784
Iteration 1700: Loss = -10061.849170242302
Iteration 1800: Loss = -10061.848554927443
Iteration 1900: Loss = -10061.84806682869
Iteration 2000: Loss = -10061.847566156268
Iteration 2100: Loss = -10061.847207558072
Iteration 2200: Loss = -10061.847063756415
Iteration 2300: Loss = -10061.84694660957
Iteration 2400: Loss = -10061.846884484186
Iteration 2500: Loss = -10061.846810911396
Iteration 2600: Loss = -10061.846801683225
Iteration 2700: Loss = -10061.846751512905
Iteration 2800: Loss = -10061.846723671373
Iteration 2900: Loss = -10061.846685431443
Iteration 3000: Loss = -10061.846668253242
Iteration 3100: Loss = -10061.846636617885
Iteration 3200: Loss = -10061.846679711976
Iteration 3300: Loss = -10061.846603358868
Iteration 3400: Loss = -10061.846626149185
Iteration 3500: Loss = -10061.846631098515
Iteration 3600: Loss = -10061.846616692625
Iteration 3700: Loss = -10061.84916240057
1
Iteration 3800: Loss = -10061.846627717188
Iteration 3900: Loss = -10061.846618911766
Iteration 4000: Loss = -10061.846725566968
1
Iteration 4100: Loss = -10061.846582337124
Iteration 4200: Loss = -10061.850670502108
1
Iteration 4300: Loss = -10061.846607819336
Iteration 4400: Loss = -10061.846585853416
Iteration 4500: Loss = -10061.846622065605
Iteration 4600: Loss = -10061.846585432528
Iteration 4700: Loss = -10061.84659548039
Iteration 4800: Loss = -10061.846573097611
Iteration 4900: Loss = -10061.846592815746
Iteration 5000: Loss = -10061.846576260532
Iteration 5100: Loss = -10061.846574643781
Iteration 5200: Loss = -10061.846588469776
Iteration 5300: Loss = -10061.846624550964
Iteration 5400: Loss = -10061.84660441269
Iteration 5500: Loss = -10061.846600124476
Iteration 5600: Loss = -10061.846579999958
Iteration 5700: Loss = -10061.846559092217
Iteration 5800: Loss = -10061.846582057864
Iteration 5900: Loss = -10061.84660552391
Iteration 6000: Loss = -10061.84664636553
Iteration 6100: Loss = -10061.847028647644
1
Iteration 6200: Loss = -10061.865804892595
2
Iteration 6300: Loss = -10061.846607507889
Iteration 6400: Loss = -10061.847140247306
1
Iteration 6500: Loss = -10061.846615489996
Iteration 6600: Loss = -10061.846625622986
Iteration 6700: Loss = -10061.846620645783
Iteration 6800: Loss = -10061.84663277618
Iteration 6900: Loss = -10061.846914276555
1
Iteration 7000: Loss = -10061.848107049465
2
Iteration 7100: Loss = -10061.847089239802
3
Iteration 7200: Loss = -10061.846600702858
Iteration 7300: Loss = -10061.850834673794
1
Iteration 7400: Loss = -10061.846892633996
2
Iteration 7500: Loss = -10061.846860594635
3
Iteration 7600: Loss = -10061.846696959028
Iteration 7700: Loss = -10061.846956640225
1
Iteration 7800: Loss = -10061.846636309188
Iteration 7900: Loss = -10061.846938533194
1
Iteration 8000: Loss = -10061.847216547914
2
Iteration 8100: Loss = -10061.846899082779
3
Iteration 8200: Loss = -10061.846602752265
Iteration 8300: Loss = -10061.84731647586
1
Iteration 8400: Loss = -10061.847081874934
2
Iteration 8500: Loss = -10061.848261041563
3
Iteration 8600: Loss = -10061.846602127844
Iteration 8700: Loss = -10061.846585865413
Iteration 8800: Loss = -10061.846601863925
Iteration 8900: Loss = -10061.846596400555
Iteration 9000: Loss = -10061.85312085064
1
Iteration 9100: Loss = -10061.847722952583
2
Iteration 9200: Loss = -10061.8486529103
3
Iteration 9300: Loss = -10061.846631304194
Iteration 9400: Loss = -10061.846622674408
Iteration 9500: Loss = -10061.859499400327
1
Iteration 9600: Loss = -10061.846591273485
Iteration 9700: Loss = -10061.847564046948
1
Iteration 9800: Loss = -10061.846597447424
Iteration 9900: Loss = -10061.846915163327
1
Iteration 10000: Loss = -10061.84663560658
Iteration 10100: Loss = -10061.847020850842
1
Iteration 10200: Loss = -10061.846577752396
Iteration 10300: Loss = -10061.846796682414
1
Iteration 10400: Loss = -10061.846605165329
Iteration 10500: Loss = -10061.846811606425
1
Iteration 10600: Loss = -10061.846603858661
Iteration 10700: Loss = -10062.071626025532
1
Iteration 10800: Loss = -10061.846614980495
Iteration 10900: Loss = -10061.846561840626
Iteration 11000: Loss = -10061.848783907802
1
Iteration 11100: Loss = -10061.946211166289
2
Iteration 11200: Loss = -10061.846589130724
Iteration 11300: Loss = -10061.848721117945
1
Iteration 11400: Loss = -10061.84663190855
Iteration 11500: Loss = -10061.84662719777
Iteration 11600: Loss = -10061.848007330782
1
Iteration 11700: Loss = -10061.8465833939
Iteration 11800: Loss = -10061.912422474503
1
Iteration 11900: Loss = -10061.846630439466
Iteration 12000: Loss = -10061.846607820376
Iteration 12100: Loss = -10061.849999178863
1
Iteration 12200: Loss = -10061.846577824093
Iteration 12300: Loss = -10061.994711091656
1
Iteration 12400: Loss = -10061.846596644593
Iteration 12500: Loss = -10061.847328335853
1
Iteration 12600: Loss = -10061.846600886745
Iteration 12700: Loss = -10061.850478621076
1
Iteration 12800: Loss = -10061.846614076814
Iteration 12900: Loss = -10062.130432367814
1
Iteration 13000: Loss = -10061.846582330132
Iteration 13100: Loss = -10061.846608468144
Iteration 13200: Loss = -10061.848317076261
1
Iteration 13300: Loss = -10061.84658963606
Iteration 13400: Loss = -10062.128739694404
1
Iteration 13500: Loss = -10061.846641588349
Iteration 13600: Loss = -10061.851070710698
1
Iteration 13700: Loss = -10061.848673034794
2
Iteration 13800: Loss = -10061.846611433939
Iteration 13900: Loss = -10061.846581434393
Iteration 14000: Loss = -10062.13486657431
1
Iteration 14100: Loss = -10061.846577156864
Iteration 14200: Loss = -10062.154519301806
1
Iteration 14300: Loss = -10061.846599017552
Iteration 14400: Loss = -10061.846594320328
Iteration 14500: Loss = -10061.847137660769
1
Iteration 14600: Loss = -10061.846579600035
Iteration 14700: Loss = -10061.890838483028
1
Iteration 14800: Loss = -10061.846582948943
Iteration 14900: Loss = -10061.854908579124
1
Iteration 15000: Loss = -10061.846586882382
Iteration 15100: Loss = -10061.854123251846
1
Iteration 15200: Loss = -10061.846568824285
Iteration 15300: Loss = -10061.846567576782
Iteration 15400: Loss = -10061.847413744106
1
Iteration 15500: Loss = -10061.846623784584
Iteration 15600: Loss = -10061.846985660039
1
Iteration 15700: Loss = -10061.847396234352
2
Iteration 15800: Loss = -10061.846783604116
3
Iteration 15900: Loss = -10061.84670516735
Iteration 16000: Loss = -10061.846687814987
Iteration 16100: Loss = -10061.914002169167
1
Iteration 16200: Loss = -10061.84659462685
Iteration 16300: Loss = -10062.086211069643
1
Iteration 16400: Loss = -10061.846615810651
Iteration 16500: Loss = -10061.846645713831
Iteration 16600: Loss = -10061.846644555862
Iteration 16700: Loss = -10062.04400161279
1
Iteration 16800: Loss = -10061.846583499244
Iteration 16900: Loss = -10061.848587768518
1
Iteration 17000: Loss = -10061.848571687777
2
Iteration 17100: Loss = -10061.8466096389
Iteration 17200: Loss = -10061.853625313404
1
Iteration 17300: Loss = -10061.846613184902
Iteration 17400: Loss = -10061.846560371892
Iteration 17500: Loss = -10061.848712641331
1
Iteration 17600: Loss = -10061.846575909325
Iteration 17700: Loss = -10061.882256871319
1
Iteration 17800: Loss = -10061.84658897757
Iteration 17900: Loss = -10061.846603511409
Iteration 18000: Loss = -10061.852679286136
1
Iteration 18100: Loss = -10061.846872580634
2
Iteration 18200: Loss = -10061.846584783822
Iteration 18300: Loss = -10061.94031586217
1
Iteration 18400: Loss = -10061.846574109932
Iteration 18500: Loss = -10061.846589651801
Iteration 18600: Loss = -10061.849884906616
1
Iteration 18700: Loss = -10061.846582068414
Iteration 18800: Loss = -10061.846843374464
1
Iteration 18900: Loss = -10061.846615722887
Iteration 19000: Loss = -10061.846759343227
1
Iteration 19100: Loss = -10061.84658746729
Iteration 19200: Loss = -10061.846855448695
1
Iteration 19300: Loss = -10061.883620867266
2
Iteration 19400: Loss = -10061.846615335087
Iteration 19500: Loss = -10061.84717231047
1
Iteration 19600: Loss = -10061.846572261655
Iteration 19700: Loss = -10061.84814862761
1
Iteration 19800: Loss = -10061.84661901636
Iteration 19900: Loss = -10061.86691805179
1
pi: tensor([[0.8259, 0.1741],
        [0.0388, 0.9612]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8927, 0.1073], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1342, 0.2023],
         [0.6473, 0.2169]],

        [[0.5495, 0.1130],
         [0.6417, 0.5680]],

        [[0.6027, 0.1117],
         [0.6099, 0.5256]],

        [[0.7210, 0.1067],
         [0.5784, 0.6232]],

        [[0.6242, 0.1243],
         [0.6505, 0.5753]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.01669750032904028
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 66
Adjusted Rand Index: 0.09504275242309354
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 75
Adjusted Rand Index: 0.2425844890116912
time is 3
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 88
Adjusted Rand Index: 0.5733276913854508
time is 4
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 80
Adjusted Rand Index: 0.3535723940152557
Global Adjusted Rand Index: 0.18785099228523935
Average Adjusted Rand Index: 0.2495659653012902
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24339.04551650588
Iteration 100: Loss = -10086.918532766946
Iteration 200: Loss = -10084.445360510632
Iteration 300: Loss = -10082.029843071276
Iteration 400: Loss = -10081.638782868278
Iteration 500: Loss = -10081.45815201826
Iteration 600: Loss = -10081.252478267625
Iteration 700: Loss = -10080.288386782217
Iteration 800: Loss = -10078.963262160616
Iteration 900: Loss = -10078.427212578443
Iteration 1000: Loss = -10077.915352901395
Iteration 1100: Loss = -10076.65285125357
Iteration 1200: Loss = -10075.056910025874
Iteration 1300: Loss = -10073.13718571424
Iteration 1400: Loss = -10071.704280548267
Iteration 1500: Loss = -10071.17950869989
Iteration 1600: Loss = -10070.912215186583
Iteration 1700: Loss = -10070.623575886211
Iteration 1800: Loss = -10069.389152546713
Iteration 1900: Loss = -10062.176763913183
Iteration 2000: Loss = -10061.9455280381
Iteration 2100: Loss = -10061.900065440874
Iteration 2200: Loss = -10061.862597582742
Iteration 2300: Loss = -10061.855660734765
Iteration 2400: Loss = -10061.852573510927
Iteration 2500: Loss = -10061.850852968499
Iteration 2600: Loss = -10061.84974262504
Iteration 2700: Loss = -10061.849034425206
Iteration 2800: Loss = -10061.848521193619
Iteration 2900: Loss = -10061.848153325509
Iteration 3000: Loss = -10061.847829320455
Iteration 3100: Loss = -10061.847642057794
Iteration 3200: Loss = -10061.847410454797
Iteration 3300: Loss = -10061.847276292774
Iteration 3400: Loss = -10061.847141223985
Iteration 3500: Loss = -10061.84709207159
Iteration 3600: Loss = -10061.846940242442
Iteration 3700: Loss = -10061.846996391221
Iteration 3800: Loss = -10061.846870158894
Iteration 3900: Loss = -10061.846794054642
Iteration 4000: Loss = -10061.84682145751
Iteration 4100: Loss = -10061.846740731005
Iteration 4200: Loss = -10061.846722436741
Iteration 4300: Loss = -10061.846702520254
Iteration 4400: Loss = -10061.846696400176
Iteration 4500: Loss = -10061.847319538163
1
Iteration 4600: Loss = -10061.846700403896
Iteration 4700: Loss = -10061.846648687528
Iteration 4800: Loss = -10061.847631932522
1
Iteration 4900: Loss = -10061.846655744697
Iteration 5000: Loss = -10061.846638779674
Iteration 5100: Loss = -10061.846643288018
Iteration 5200: Loss = -10061.846594375022
Iteration 5300: Loss = -10061.846990209493
1
Iteration 5400: Loss = -10061.846613519869
Iteration 5500: Loss = -10061.846585055428
Iteration 5600: Loss = -10061.847343412473
1
Iteration 5700: Loss = -10061.84658568464
Iteration 5800: Loss = -10061.84658907577
Iteration 5900: Loss = -10061.846841130955
1
Iteration 6000: Loss = -10061.846612667441
Iteration 6100: Loss = -10061.873915629649
1
Iteration 6200: Loss = -10061.846580857078
Iteration 6300: Loss = -10061.846589278779
Iteration 6400: Loss = -10061.846717515884
1
Iteration 6500: Loss = -10061.84658764881
Iteration 6600: Loss = -10061.847769709506
1
Iteration 6700: Loss = -10061.8465682374
Iteration 6800: Loss = -10061.846607813564
Iteration 6900: Loss = -10061.846562418008
Iteration 7000: Loss = -10061.846605409883
Iteration 7100: Loss = -10061.847224290248
1
Iteration 7200: Loss = -10061.846565910677
Iteration 7300: Loss = -10061.879032706369
1
Iteration 7400: Loss = -10061.846582761456
Iteration 7500: Loss = -10061.848449727873
1
Iteration 7600: Loss = -10061.848882675855
2
Iteration 7700: Loss = -10061.846602164394
Iteration 7800: Loss = -10061.846617207691
Iteration 7900: Loss = -10061.868314786521
1
Iteration 8000: Loss = -10061.846653245735
Iteration 8100: Loss = -10061.848693827507
1
Iteration 8200: Loss = -10061.846704066169
Iteration 8300: Loss = -10061.8466112658
Iteration 8400: Loss = -10061.846629959444
Iteration 8500: Loss = -10061.846589152585
Iteration 8600: Loss = -10061.846617452757
Iteration 8700: Loss = -10061.846670822855
Iteration 8800: Loss = -10061.846580872485
Iteration 8900: Loss = -10061.847811367797
1
Iteration 9000: Loss = -10061.846726568241
2
Iteration 9100: Loss = -10061.846594901606
Iteration 9200: Loss = -10061.848904689177
1
Iteration 9300: Loss = -10061.84658255893
Iteration 9400: Loss = -10061.902723376814
1
Iteration 9500: Loss = -10061.846594024044
Iteration 9600: Loss = -10061.849481747666
1
Iteration 9700: Loss = -10061.84657361363
Iteration 9800: Loss = -10061.847349606174
1
Iteration 9900: Loss = -10061.84657273352
Iteration 10000: Loss = -10061.84674510767
1
Iteration 10100: Loss = -10061.846595339124
Iteration 10200: Loss = -10061.846835680486
1
Iteration 10300: Loss = -10061.846600393294
Iteration 10400: Loss = -10061.846868750737
1
Iteration 10500: Loss = -10061.846560150683
Iteration 10600: Loss = -10061.848449522073
1
Iteration 10700: Loss = -10061.84661444157
Iteration 10800: Loss = -10061.983571273006
1
Iteration 10900: Loss = -10061.846575265396
Iteration 11000: Loss = -10061.846576626926
Iteration 11100: Loss = -10061.846896107254
1
Iteration 11200: Loss = -10061.846581164556
Iteration 11300: Loss = -10061.846815783025
1
Iteration 11400: Loss = -10061.846594123961
Iteration 11500: Loss = -10061.846570949989
Iteration 11600: Loss = -10061.886912812672
1
Iteration 11700: Loss = -10061.846586163369
Iteration 11800: Loss = -10061.846989513997
1
Iteration 11900: Loss = -10061.84678542239
2
Iteration 12000: Loss = -10061.8466208294
Iteration 12100: Loss = -10061.846580922584
Iteration 12200: Loss = -10061.84671820079
1
Iteration 12300: Loss = -10061.846605763865
Iteration 12400: Loss = -10061.970648196168
1
Iteration 12500: Loss = -10061.846606049208
Iteration 12600: Loss = -10061.846593508397
Iteration 12700: Loss = -10061.897082601663
1
Iteration 12800: Loss = -10061.846589733392
Iteration 12900: Loss = -10061.846574623227
Iteration 13000: Loss = -10061.885073230069
1
Iteration 13100: Loss = -10061.846574096042
Iteration 13200: Loss = -10061.849059936583
1
Iteration 13300: Loss = -10061.849967476172
2
Iteration 13400: Loss = -10061.84657315455
Iteration 13500: Loss = -10061.846628092253
Iteration 13600: Loss = -10061.846667132695
Iteration 13700: Loss = -10061.846583383458
Iteration 13800: Loss = -10061.849273829523
1
Iteration 13900: Loss = -10061.846578783758
Iteration 14000: Loss = -10061.8470120594
1
Iteration 14100: Loss = -10061.846620876373
Iteration 14200: Loss = -10061.85016202266
1
Iteration 14300: Loss = -10061.84660037878
Iteration 14400: Loss = -10061.84888795552
1
Iteration 14500: Loss = -10061.846605181818
Iteration 14600: Loss = -10061.855293448732
1
Iteration 14700: Loss = -10061.846594450735
Iteration 14800: Loss = -10061.846606780973
Iteration 14900: Loss = -10061.846895125005
1
Iteration 15000: Loss = -10061.846567182836
Iteration 15100: Loss = -10062.200438055068
1
Iteration 15200: Loss = -10061.846597671229
Iteration 15300: Loss = -10061.858957324075
1
Iteration 15400: Loss = -10061.850549302128
2
Iteration 15500: Loss = -10061.931216724395
3
Iteration 15600: Loss = -10061.846594078608
Iteration 15700: Loss = -10061.848390472538
1
Iteration 15800: Loss = -10061.846586190288
Iteration 15900: Loss = -10061.847294610161
1
Iteration 16000: Loss = -10061.86649130605
2
Iteration 16100: Loss = -10061.846598933238
Iteration 16200: Loss = -10061.855586039557
1
Iteration 16300: Loss = -10061.846573782468
Iteration 16400: Loss = -10061.846600379242
Iteration 16500: Loss = -10061.846675416022
Iteration 16600: Loss = -10061.846607166219
Iteration 16700: Loss = -10061.862373901777
1
Iteration 16800: Loss = -10061.932579530192
2
Iteration 16900: Loss = -10061.846567190334
Iteration 17000: Loss = -10061.848221784647
1
Iteration 17100: Loss = -10061.84662681824
Iteration 17200: Loss = -10061.848870018002
1
Iteration 17300: Loss = -10061.846589037712
Iteration 17400: Loss = -10061.847139405943
1
Iteration 17500: Loss = -10061.860536932514
2
Iteration 17600: Loss = -10061.8465903808
Iteration 17700: Loss = -10061.848126726376
1
Iteration 17800: Loss = -10061.846607053782
Iteration 17900: Loss = -10061.847010958252
1
Iteration 18000: Loss = -10061.846591609363
Iteration 18100: Loss = -10061.846813809163
1
Iteration 18200: Loss = -10061.846596139596
Iteration 18300: Loss = -10061.848656811298
1
Iteration 18400: Loss = -10061.846577251144
Iteration 18500: Loss = -10062.128084043305
1
Iteration 18600: Loss = -10061.846594310651
Iteration 18700: Loss = -10061.897141429534
1
Iteration 18800: Loss = -10061.846596627114
Iteration 18900: Loss = -10061.85510261013
1
Iteration 19000: Loss = -10061.84660554118
Iteration 19100: Loss = -10061.899458276646
1
Iteration 19200: Loss = -10061.846579434374
Iteration 19300: Loss = -10061.846816335588
1
Iteration 19400: Loss = -10061.846611660625
Iteration 19500: Loss = -10061.846594572899
Iteration 19600: Loss = -10061.853904882582
1
Iteration 19700: Loss = -10061.860727209545
2
Iteration 19800: Loss = -10061.899782396396
3
Iteration 19900: Loss = -10061.84660864986
pi: tensor([[0.8262, 0.1738],
        [0.0389, 0.9611]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8925, 0.1075], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1341, 0.2023],
         [0.5677, 0.2170]],

        [[0.6330, 0.1132],
         [0.6175, 0.5920]],

        [[0.5408, 0.1118],
         [0.6109, 0.6929]],

        [[0.7189, 0.1068],
         [0.6314, 0.6511]],

        [[0.6010, 0.1244],
         [0.5156, 0.5388]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.01669750032904028
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 66
Adjusted Rand Index: 0.09504275242309354
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 75
Adjusted Rand Index: 0.2425844890116912
time is 3
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 88
Adjusted Rand Index: 0.5733276913854508
time is 4
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 80
Adjusted Rand Index: 0.3535723940152557
Global Adjusted Rand Index: 0.18785099228523935
Average Adjusted Rand Index: 0.2495659653012902
10184.320208632842
[0.18785099228523935, 0.18785099228523935] [0.2495659653012902, 0.2495659653012902] [10061.848541901036, 10061.846718636056]
-------------------------------------
This iteration is 72
True Objective function: Loss = -10198.969314837863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22149.456234541827
Iteration 100: Loss = -10110.716665436721
Iteration 200: Loss = -10109.556964003266
Iteration 300: Loss = -10107.823829963894
Iteration 400: Loss = -10105.608276554482
Iteration 500: Loss = -10105.064552995991
Iteration 600: Loss = -10104.672330438258
Iteration 700: Loss = -10104.297862783053
Iteration 800: Loss = -10103.857398689293
Iteration 900: Loss = -10103.240125595048
Iteration 1000: Loss = -10102.33220448471
Iteration 1100: Loss = -10101.577456909607
Iteration 1200: Loss = -10101.133649931751
Iteration 1300: Loss = -10100.840403974435
Iteration 1400: Loss = -10100.625298479386
Iteration 1500: Loss = -10100.46417185306
Iteration 1600: Loss = -10100.341098750316
Iteration 1700: Loss = -10100.243979416355
Iteration 1800: Loss = -10100.172127143442
Iteration 1900: Loss = -10100.125483815456
Iteration 2000: Loss = -10100.09557839604
Iteration 2100: Loss = -10100.07473815434
Iteration 2200: Loss = -10100.059559582502
Iteration 2300: Loss = -10100.04816988682
Iteration 2400: Loss = -10100.039129884815
Iteration 2500: Loss = -10100.031341830247
Iteration 2600: Loss = -10100.024443189046
Iteration 2700: Loss = -10100.019075053615
Iteration 2800: Loss = -10100.012551526806
Iteration 2900: Loss = -10100.007091994781
Iteration 3000: Loss = -10100.007199633223
1
Iteration 3100: Loss = -10099.996762619863
Iteration 3200: Loss = -10099.991920179325
Iteration 3300: Loss = -10100.000378435412
1
Iteration 3400: Loss = -10099.982731892744
Iteration 3500: Loss = -10099.978385706216
Iteration 3600: Loss = -10099.990742197882
1
Iteration 3700: Loss = -10099.971073700903
Iteration 3800: Loss = -10099.96662060892
Iteration 3900: Loss = -10099.96314592499
Iteration 4000: Loss = -10099.959784308066
Iteration 4100: Loss = -10099.956659689362
Iteration 4200: Loss = -10099.953746688665
Iteration 4300: Loss = -10099.951130966463
Iteration 4400: Loss = -10099.94876721039
Iteration 4500: Loss = -10099.94645767167
Iteration 4600: Loss = -10099.943921005179
Iteration 4700: Loss = -10099.942105863382
Iteration 4800: Loss = -10099.942746647028
1
Iteration 4900: Loss = -10099.938193752469
Iteration 5000: Loss = -10099.93652008522
Iteration 5100: Loss = -10099.93504036586
Iteration 5200: Loss = -10099.93398517548
Iteration 5300: Loss = -10099.93231075675
Iteration 5400: Loss = -10099.931171328824
Iteration 5500: Loss = -10099.93052520661
Iteration 5600: Loss = -10099.929981048217
Iteration 5700: Loss = -10099.927944260508
Iteration 5800: Loss = -10099.92712767089
Iteration 5900: Loss = -10099.92613398811
Iteration 6000: Loss = -10099.925644162262
Iteration 6100: Loss = -10099.925136176866
Iteration 6200: Loss = -10099.923914591727
Iteration 6300: Loss = -10099.92329523197
Iteration 6400: Loss = -10099.923861380068
1
Iteration 6500: Loss = -10099.922119259534
Iteration 6600: Loss = -10099.921831898444
Iteration 6700: Loss = -10099.9211317032
Iteration 6800: Loss = -10099.920714311917
Iteration 6900: Loss = -10099.920262321277
Iteration 7000: Loss = -10099.924341926082
1
Iteration 7100: Loss = -10099.919465801902
Iteration 7200: Loss = -10099.919123631862
Iteration 7300: Loss = -10099.918775929093
Iteration 7400: Loss = -10099.91857327016
Iteration 7500: Loss = -10099.91825025141
Iteration 7600: Loss = -10099.917897808302
Iteration 7700: Loss = -10099.917659067478
Iteration 7800: Loss = -10099.917408988453
Iteration 7900: Loss = -10099.917255239197
Iteration 8000: Loss = -10099.916980532777
Iteration 8100: Loss = -10099.916904746993
Iteration 8200: Loss = -10099.91655328525
Iteration 8300: Loss = -10099.929804713329
1
Iteration 8400: Loss = -10099.916226058114
Iteration 8500: Loss = -10099.91623333309
Iteration 8600: Loss = -10099.915931196612
Iteration 8700: Loss = -10099.91579973551
Iteration 8800: Loss = -10099.91565159189
Iteration 8900: Loss = -10099.916527062533
1
Iteration 9000: Loss = -10099.915400950811
Iteration 9100: Loss = -10099.915291328236
Iteration 9200: Loss = -10099.91600434795
1
Iteration 9300: Loss = -10099.91506727868
Iteration 9400: Loss = -10099.915121205559
Iteration 9500: Loss = -10099.914915115114
Iteration 9600: Loss = -10099.91481639405
Iteration 9700: Loss = -10099.927017138543
1
Iteration 9800: Loss = -10099.914652426842
Iteration 9900: Loss = -10099.97637256514
1
Iteration 10000: Loss = -10099.915367382682
2
Iteration 10100: Loss = -10099.914503396625
Iteration 10200: Loss = -10099.914791093424
1
Iteration 10300: Loss = -10099.914845464922
2
Iteration 10400: Loss = -10099.918143176516
3
Iteration 10500: Loss = -10099.914790482837
4
Iteration 10600: Loss = -10099.914834017314
5
Iteration 10700: Loss = -10099.915155039762
6
Iteration 10800: Loss = -10099.914125171565
Iteration 10900: Loss = -10099.914625512782
1
Iteration 11000: Loss = -10099.922341906818
2
Iteration 11100: Loss = -10099.915197103159
3
Iteration 11200: Loss = -10099.917196640994
4
Iteration 11300: Loss = -10099.920467106158
5
Iteration 11400: Loss = -10099.918352577164
6
Iteration 11500: Loss = -10099.922228831909
7
Iteration 11600: Loss = -10099.923295487286
8
Iteration 11700: Loss = -10099.918405794555
9
Iteration 11800: Loss = -10099.913895072248
Iteration 11900: Loss = -10099.920216492948
1
Iteration 12000: Loss = -10099.91863306115
2
Iteration 12100: Loss = -10099.915789347777
3
Iteration 12200: Loss = -10099.914558936956
4
Iteration 12300: Loss = -10099.942523298965
5
Iteration 12400: Loss = -10099.913938958092
Iteration 12500: Loss = -10099.913621964444
Iteration 12600: Loss = -10099.913857080928
1
Iteration 12700: Loss = -10099.91504565416
2
Iteration 12800: Loss = -10099.934584432383
3
Iteration 12900: Loss = -10099.952341760569
4
Iteration 13000: Loss = -10099.917727755643
5
Iteration 13100: Loss = -10099.914580829518
6
Iteration 13200: Loss = -10099.913573254144
Iteration 13300: Loss = -10099.915536781471
1
Iteration 13400: Loss = -10099.978828415746
2
Iteration 13500: Loss = -10099.980587834378
3
Iteration 13600: Loss = -10099.975495672528
4
Iteration 13700: Loss = -10099.917872588845
5
Iteration 13800: Loss = -10099.913774644107
6
Iteration 13900: Loss = -10099.913673330064
7
Iteration 14000: Loss = -10099.966300507307
8
Iteration 14100: Loss = -10099.913443146312
Iteration 14200: Loss = -10100.023337352544
1
Iteration 14300: Loss = -10099.913427546757
Iteration 14400: Loss = -10099.91348358819
Iteration 14500: Loss = -10099.91415665517
1
Iteration 14600: Loss = -10099.913528000314
Iteration 14700: Loss = -10099.913766738548
1
Iteration 14800: Loss = -10099.917449859006
2
Iteration 14900: Loss = -10099.921054750937
3
Iteration 15000: Loss = -10099.914081099474
4
Iteration 15100: Loss = -10099.913626113677
Iteration 15200: Loss = -10099.913606979237
Iteration 15300: Loss = -10099.914302164943
1
Iteration 15400: Loss = -10099.914794314516
2
Iteration 15500: Loss = -10099.922721430972
3
Iteration 15600: Loss = -10099.913955102556
4
Iteration 15700: Loss = -10099.916814359054
5
Iteration 15800: Loss = -10099.968655065268
6
Iteration 15900: Loss = -10099.92489524729
7
Iteration 16000: Loss = -10099.914599767886
8
Iteration 16100: Loss = -10099.913882709789
9
Iteration 16200: Loss = -10099.95134673481
10
Iteration 16300: Loss = -10099.916975335556
11
Iteration 16400: Loss = -10099.918751680247
12
Iteration 16500: Loss = -10099.913423702183
Iteration 16600: Loss = -10099.961603033398
1
Iteration 16700: Loss = -10099.957392120019
2
Iteration 16800: Loss = -10099.93574631653
3
Iteration 16900: Loss = -10100.00966535125
4
Iteration 17000: Loss = -10099.91518186183
5
Iteration 17100: Loss = -10099.914376075809
6
Iteration 17200: Loss = -10099.91439750112
7
Iteration 17300: Loss = -10099.91574629435
8
Iteration 17400: Loss = -10099.914044648485
9
Iteration 17500: Loss = -10099.937142295877
10
Iteration 17600: Loss = -10099.914072110245
11
Iteration 17700: Loss = -10099.927608566668
12
Iteration 17800: Loss = -10099.930317461562
13
Iteration 17900: Loss = -10099.916751042574
14
Iteration 18000: Loss = -10100.047384490408
15
Stopping early at iteration 18000 due to no improvement.
pi: tensor([[1.0000e+00, 1.8713e-06],
        [5.1327e-01, 4.8673e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6148, 0.3852], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1348, 0.1607],
         [0.5010, 0.1935]],

        [[0.6630, 0.1552],
         [0.5662, 0.7222]],

        [[0.5744, 0.1754],
         [0.7201, 0.5876]],

        [[0.7011, 0.0759],
         [0.5824, 0.7148]],

        [[0.6424, 0.1559],
         [0.6298, 0.7061]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.06298202224924737
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.024851437378340484
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.009579846823721254
Average Adjusted Rand Index: 0.007243852494330715
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22686.529428101425
Iteration 100: Loss = -10111.113860808156
Iteration 200: Loss = -10110.008441425545
Iteration 300: Loss = -10109.672841997635
Iteration 400: Loss = -10107.970474068085
Iteration 500: Loss = -10104.566332724573
Iteration 600: Loss = -10104.31536222887
Iteration 700: Loss = -10104.108250897341
Iteration 800: Loss = -10103.850983412398
Iteration 900: Loss = -10102.975009732672
Iteration 1000: Loss = -10101.409972748328
Iteration 1100: Loss = -10100.616274989829
Iteration 1200: Loss = -10100.288059673703
Iteration 1300: Loss = -10100.163214847913
Iteration 1400: Loss = -10100.09740747683
Iteration 1500: Loss = -10100.050613840634
Iteration 1600: Loss = -10100.020181581456
Iteration 1700: Loss = -10099.99998392168
Iteration 1800: Loss = -10099.9851855182
Iteration 1900: Loss = -10099.979094208558
Iteration 2000: Loss = -10099.966094717742
Iteration 2100: Loss = -10099.959978355615
Iteration 2200: Loss = -10099.955063635618
Iteration 2300: Loss = -10099.95092535605
Iteration 2400: Loss = -10099.947665976131
Iteration 2500: Loss = -10099.944992802888
Iteration 2600: Loss = -10099.942609752537
Iteration 2700: Loss = -10099.942694416417
Iteration 2800: Loss = -10099.938440809976
Iteration 2900: Loss = -10099.93666065753
Iteration 3000: Loss = -10099.93506430325
Iteration 3100: Loss = -10099.933459559554
Iteration 3200: Loss = -10099.932026027021
Iteration 3300: Loss = -10099.930621421376
Iteration 3400: Loss = -10099.929330156447
Iteration 3500: Loss = -10099.931230811493
1
Iteration 3600: Loss = -10099.92705266586
Iteration 3700: Loss = -10099.926048029007
Iteration 3800: Loss = -10099.925592508154
Iteration 3900: Loss = -10099.924333559231
Iteration 4000: Loss = -10099.923622305014
Iteration 4100: Loss = -10099.922921246116
Iteration 4200: Loss = -10099.92235121419
Iteration 4300: Loss = -10099.927557602517
1
Iteration 4400: Loss = -10099.92124943953
Iteration 4500: Loss = -10099.920782245787
Iteration 4600: Loss = -10099.920323740616
Iteration 4700: Loss = -10099.91988721094
Iteration 4800: Loss = -10099.919511771572
Iteration 4900: Loss = -10099.919095916737
Iteration 5000: Loss = -10099.9187284296
Iteration 5100: Loss = -10099.918443261427
Iteration 5200: Loss = -10099.918333525247
Iteration 5300: Loss = -10099.917824831178
Iteration 5400: Loss = -10099.918106268204
1
Iteration 5500: Loss = -10099.917312839856
Iteration 5600: Loss = -10099.917216202055
Iteration 5700: Loss = -10099.916855921087
Iteration 5800: Loss = -10099.916777987202
Iteration 5900: Loss = -10099.916483239307
Iteration 6000: Loss = -10099.918747803351
1
Iteration 6100: Loss = -10099.916120318841
Iteration 6200: Loss = -10099.916761055787
1
Iteration 6300: Loss = -10099.915829141033
Iteration 6400: Loss = -10099.915671462979
Iteration 6500: Loss = -10099.91584698331
1
Iteration 6600: Loss = -10099.91541736821
Iteration 6700: Loss = -10099.932165954237
1
Iteration 6800: Loss = -10099.915192647723
Iteration 6900: Loss = -10099.915096911896
Iteration 7000: Loss = -10099.915356740508
1
Iteration 7100: Loss = -10099.914895448566
Iteration 7200: Loss = -10099.930658877176
1
Iteration 7300: Loss = -10099.914749598942
Iteration 7400: Loss = -10099.914628680388
Iteration 7500: Loss = -10099.914647567439
Iteration 7600: Loss = -10099.9145312738
Iteration 7700: Loss = -10099.914524283597
Iteration 7800: Loss = -10099.926399865724
1
Iteration 7900: Loss = -10099.91432104285
Iteration 8000: Loss = -10099.916262907247
1
Iteration 8100: Loss = -10099.914238365476
Iteration 8200: Loss = -10099.930643970849
1
Iteration 8300: Loss = -10099.91416637669
Iteration 8400: Loss = -10099.967505894147
1
Iteration 8500: Loss = -10099.91404942193
Iteration 8600: Loss = -10099.914089217622
Iteration 8700: Loss = -10099.913984612831
Iteration 8800: Loss = -10099.950080112456
1
Iteration 8900: Loss = -10099.913928971126
Iteration 9000: Loss = -10099.919964518904
1
Iteration 9100: Loss = -10099.913871636958
Iteration 9200: Loss = -10099.915298245578
1
Iteration 9300: Loss = -10099.914031844166
2
Iteration 9400: Loss = -10099.931329252182
3
Iteration 9500: Loss = -10099.91695680785
4
Iteration 9600: Loss = -10099.939503249221
5
Iteration 9700: Loss = -10099.913717893047
Iteration 9800: Loss = -10099.914280297464
1
Iteration 9900: Loss = -10099.917929336245
2
Iteration 10000: Loss = -10099.918556230356
3
Iteration 10100: Loss = -10099.913848886465
4
Iteration 10200: Loss = -10099.914974608324
5
Iteration 10300: Loss = -10099.931793570036
6
Iteration 10400: Loss = -10099.919488414647
7
Iteration 10500: Loss = -10099.91370790906
Iteration 10600: Loss = -10099.9326579513
1
Iteration 10700: Loss = -10099.914052866525
2
Iteration 10800: Loss = -10099.918229652534
3
Iteration 10900: Loss = -10099.917666075713
4
Iteration 11000: Loss = -10099.913750740392
Iteration 11100: Loss = -10099.935557880297
1
Iteration 11200: Loss = -10099.916017824544
2
Iteration 11300: Loss = -10099.920542379881
3
Iteration 11400: Loss = -10099.913696704594
Iteration 11500: Loss = -10099.923493038323
1
Iteration 11600: Loss = -10099.95660455641
2
Iteration 11700: Loss = -10099.913452041925
Iteration 11800: Loss = -10099.915454806724
1
Iteration 11900: Loss = -10099.914298135598
2
Iteration 12000: Loss = -10099.918440208896
3
Iteration 12100: Loss = -10099.936734522653
4
Iteration 12200: Loss = -10099.929697679529
5
Iteration 12300: Loss = -10099.935212460101
6
Iteration 12400: Loss = -10099.921299643796
7
Iteration 12500: Loss = -10099.913423127464
Iteration 12600: Loss = -10099.916087468378
1
Iteration 12700: Loss = -10099.91337052767
Iteration 12800: Loss = -10099.913534894575
1
Iteration 12900: Loss = -10099.988358159588
2
Iteration 13000: Loss = -10100.004884501483
3
Iteration 13100: Loss = -10099.913830894042
4
Iteration 13200: Loss = -10099.928685548488
5
Iteration 13300: Loss = -10099.913409388671
Iteration 13400: Loss = -10099.914123658382
1
Iteration 13500: Loss = -10100.040473580457
2
Iteration 13600: Loss = -10099.98664798058
3
Iteration 13700: Loss = -10099.914310876915
4
Iteration 13800: Loss = -10099.913894636691
5
Iteration 13900: Loss = -10099.94284685051
6
Iteration 14000: Loss = -10099.913322206854
Iteration 14100: Loss = -10099.914453206562
1
Iteration 14200: Loss = -10099.915852194481
2
Iteration 14300: Loss = -10099.914300009516
3
Iteration 14400: Loss = -10100.036435167662
4
Iteration 14500: Loss = -10099.915428936343
5
Iteration 14600: Loss = -10099.920560657565
6
Iteration 14700: Loss = -10099.917164306293
7
Iteration 14800: Loss = -10099.92833229055
8
Iteration 14900: Loss = -10099.947624648961
9
Iteration 15000: Loss = -10099.914909254096
10
Iteration 15100: Loss = -10099.914347465012
11
Iteration 15200: Loss = -10099.939396426165
12
Iteration 15300: Loss = -10099.914675660184
13
Iteration 15400: Loss = -10099.964310181924
14
Iteration 15500: Loss = -10099.947654948575
15
Stopping early at iteration 15500 due to no improvement.
pi: tensor([[1.0000e+00, 1.9552e-06],
        [5.0901e-01, 4.9099e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6129, 0.3871], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1361, 0.1591],
         [0.5050, 0.1913]],

        [[0.6284, 0.1533],
         [0.5687, 0.7074]],

        [[0.6487, 0.1735],
         [0.6468, 0.5675]],

        [[0.6116, 0.0764],
         [0.5162, 0.5170]],

        [[0.6277, 0.1541],
         [0.6514, 0.5496]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.06298202224924737
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.024851437378340484
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.003937327268695544
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.009458856967018675
Average Adjusted Rand Index: 0.006385704958499416
10198.969314837863
[-0.009579846823721254, -0.009458856967018675] [0.007243852494330715, 0.006385704958499416] [10100.047384490408, 10099.947654948575]
-------------------------------------
This iteration is 73
True Objective function: Loss = -10046.909621382372
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24372.182487869777
Iteration 100: Loss = -9893.153830809051
Iteration 200: Loss = -9891.653785971428
Iteration 300: Loss = -9891.31352818617
Iteration 400: Loss = -9891.15481459519
Iteration 500: Loss = -9891.052126992998
Iteration 600: Loss = -9890.941023271489
Iteration 700: Loss = -9890.668212285418
Iteration 800: Loss = -9890.416328585125
Iteration 900: Loss = -9890.339537213875
Iteration 1000: Loss = -9890.277273798562
Iteration 1100: Loss = -9890.209507722246
Iteration 1200: Loss = -9890.084089909506
Iteration 1300: Loss = -9889.539662605197
Iteration 1400: Loss = -9889.35708625995
Iteration 1500: Loss = -9889.298557360182
Iteration 1600: Loss = -9889.246278349287
Iteration 1700: Loss = -9889.196753489923
Iteration 1800: Loss = -9889.159410680186
Iteration 1900: Loss = -9889.135875608192
Iteration 2000: Loss = -9889.121653860257
Iteration 2100: Loss = -9889.112622815634
Iteration 2200: Loss = -9889.106471494415
Iteration 2300: Loss = -9889.10197711229
Iteration 2400: Loss = -9889.098497261664
Iteration 2500: Loss = -9889.095745663979
Iteration 2600: Loss = -9889.093550720016
Iteration 2700: Loss = -9889.091698678985
Iteration 2800: Loss = -9889.090131665447
Iteration 2900: Loss = -9889.088713115312
Iteration 3000: Loss = -9889.087603877348
Iteration 3100: Loss = -9889.086559115613
Iteration 3200: Loss = -9889.08567806714
Iteration 3300: Loss = -9889.084902755008
Iteration 3400: Loss = -9889.084182827255
Iteration 3500: Loss = -9889.08353201123
Iteration 3600: Loss = -9889.082954143483
Iteration 3700: Loss = -9889.082463260966
Iteration 3800: Loss = -9889.08200015681
Iteration 3900: Loss = -9889.081547237975
Iteration 4000: Loss = -9889.081125628962
Iteration 4100: Loss = -9889.08076684245
Iteration 4200: Loss = -9889.080384739984
Iteration 4300: Loss = -9889.080128647804
Iteration 4400: Loss = -9889.079836338422
Iteration 4500: Loss = -9889.079528810018
Iteration 4600: Loss = -9889.079335529745
Iteration 4700: Loss = -9889.079076230895
Iteration 4800: Loss = -9889.078836046348
Iteration 4900: Loss = -9889.078670964476
Iteration 5000: Loss = -9889.078496794007
Iteration 5100: Loss = -9889.078292709397
Iteration 5200: Loss = -9889.078130977065
Iteration 5300: Loss = -9889.077951077472
Iteration 5400: Loss = -9889.077824095026
Iteration 5500: Loss = -9889.077670565428
Iteration 5600: Loss = -9889.077549388738
Iteration 5700: Loss = -9889.077432532533
Iteration 5800: Loss = -9889.077272843544
Iteration 5900: Loss = -9889.077209040363
Iteration 6000: Loss = -9889.077110521735
Iteration 6100: Loss = -9889.076999766865
Iteration 6200: Loss = -9889.076883994792
Iteration 6300: Loss = -9889.076816570549
Iteration 6400: Loss = -9889.076739675898
Iteration 6500: Loss = -9889.076800465135
Iteration 6600: Loss = -9889.07657267338
Iteration 6700: Loss = -9889.076508280816
Iteration 6800: Loss = -9889.077804117544
1
Iteration 6900: Loss = -9889.076377124718
Iteration 7000: Loss = -9889.076552738075
1
Iteration 7100: Loss = -9889.076238783075
Iteration 7200: Loss = -9889.07640217228
1
Iteration 7300: Loss = -9889.07615456631
Iteration 7400: Loss = -9889.076595013494
1
Iteration 7500: Loss = -9889.076037314502
Iteration 7600: Loss = -9889.07695718747
1
Iteration 7700: Loss = -9889.075961429397
Iteration 7800: Loss = -9889.075983183136
Iteration 7900: Loss = -9889.075890508357
Iteration 8000: Loss = -9889.075872241574
Iteration 8100: Loss = -9889.07584092994
Iteration 8200: Loss = -9889.075753209792
Iteration 8300: Loss = -9889.075882542058
1
Iteration 8400: Loss = -9889.080390987869
2
Iteration 8500: Loss = -9889.075741606288
Iteration 8600: Loss = -9889.07568190249
Iteration 8700: Loss = -9889.078226712052
1
Iteration 8800: Loss = -9889.075622956974
Iteration 8900: Loss = -9889.104749999704
1
Iteration 9000: Loss = -9889.075550889724
Iteration 9100: Loss = -9889.075526202967
Iteration 9200: Loss = -9889.075553333061
Iteration 9300: Loss = -9889.075500972947
Iteration 9400: Loss = -9889.171472906866
1
Iteration 9500: Loss = -9889.075498188387
Iteration 9600: Loss = -9889.075454380161
Iteration 9700: Loss = -9889.07766575861
1
Iteration 9800: Loss = -9889.075462706578
Iteration 9900: Loss = -9889.075397883145
Iteration 10000: Loss = -9889.075413927272
Iteration 10100: Loss = -9889.075524402408
1
Iteration 10200: Loss = -9889.075370380498
Iteration 10300: Loss = -9889.075370305622
Iteration 10400: Loss = -9889.075367672334
Iteration 10500: Loss = -9889.075340438763
Iteration 10600: Loss = -9889.075362856267
Iteration 10700: Loss = -9889.077892191592
1
Iteration 10800: Loss = -9889.075314320238
Iteration 10900: Loss = -9889.075291028425
Iteration 11000: Loss = -9889.102851589265
1
Iteration 11100: Loss = -9889.075286700026
Iteration 11200: Loss = -9889.075256065438
Iteration 11300: Loss = -9889.206572590881
1
Iteration 11400: Loss = -9889.075258911827
Iteration 11500: Loss = -9889.075263489172
Iteration 11600: Loss = -9889.24861387513
1
Iteration 11700: Loss = -9889.075260316993
Iteration 11800: Loss = -9889.075232874768
Iteration 11900: Loss = -9889.075246538747
Iteration 12000: Loss = -9889.075268464168
Iteration 12100: Loss = -9889.07591428251
1
Iteration 12200: Loss = -9889.075209294335
Iteration 12300: Loss = -9889.142015410469
1
Iteration 12400: Loss = -9889.075213443957
Iteration 12500: Loss = -9889.0752035277
Iteration 12600: Loss = -9889.099581396618
1
Iteration 12700: Loss = -9889.075191434747
Iteration 12800: Loss = -9889.075195237589
Iteration 12900: Loss = -9889.080164801904
1
Iteration 13000: Loss = -9889.075171332384
Iteration 13100: Loss = -9889.075230029099
Iteration 13200: Loss = -9889.075177271729
Iteration 13300: Loss = -9889.075309656384
1
Iteration 13400: Loss = -9889.075254457495
Iteration 13500: Loss = -9889.075170516264
Iteration 13600: Loss = -9889.075529656502
1
Iteration 13700: Loss = -9889.0760798315
2
Iteration 13800: Loss = -9889.098327771855
3
Iteration 13900: Loss = -9889.075620885751
4
Iteration 14000: Loss = -9889.075163319158
Iteration 14100: Loss = -9889.07518070967
Iteration 14200: Loss = -9889.07607700989
1
Iteration 14300: Loss = -9889.075241003344
Iteration 14400: Loss = -9889.101514938695
1
Iteration 14500: Loss = -9889.075213768803
Iteration 14600: Loss = -9889.14127227179
1
Iteration 14700: Loss = -9889.075135845856
Iteration 14800: Loss = -9889.07888193921
1
Iteration 14900: Loss = -9889.075261643713
2
Iteration 15000: Loss = -9889.075137918895
Iteration 15100: Loss = -9889.07539430168
1
Iteration 15200: Loss = -9889.075164199925
Iteration 15300: Loss = -9889.124426259654
1
Iteration 15400: Loss = -9889.075169871612
Iteration 15500: Loss = -9889.076084206661
1
Iteration 15600: Loss = -9889.075579675431
2
Iteration 15700: Loss = -9889.07521880047
Iteration 15800: Loss = -9889.075249897574
Iteration 15900: Loss = -9889.07516528385
Iteration 16000: Loss = -9889.076315935863
1
Iteration 16100: Loss = -9889.075126324022
Iteration 16200: Loss = -9889.075153749656
Iteration 16300: Loss = -9889.075206503301
Iteration 16400: Loss = -9889.075232893443
Iteration 16500: Loss = -9889.105591914684
1
Iteration 16600: Loss = -9889.07513288618
Iteration 16700: Loss = -9889.119817920939
1
Iteration 16800: Loss = -9889.07539395611
2
Iteration 16900: Loss = -9889.075277197819
3
Iteration 17000: Loss = -9889.075619672341
4
Iteration 17100: Loss = -9889.075672681793
5
Iteration 17200: Loss = -9889.07772196938
6
Iteration 17300: Loss = -9889.075160125694
Iteration 17400: Loss = -9889.077428737877
1
Iteration 17500: Loss = -9889.076341356005
2
Iteration 17600: Loss = -9889.077257540603
3
Iteration 17700: Loss = -9889.075138384773
Iteration 17800: Loss = -9889.075119860127
Iteration 17900: Loss = -9889.075880649141
1
Iteration 18000: Loss = -9889.075153817497
Iteration 18100: Loss = -9889.084147918416
1
Iteration 18200: Loss = -9889.075129068548
Iteration 18300: Loss = -9889.075121689422
Iteration 18400: Loss = -9889.075235473301
1
Iteration 18500: Loss = -9889.075093926402
Iteration 18600: Loss = -9889.085798309734
1
Iteration 18700: Loss = -9889.075201327541
2
Iteration 18800: Loss = -9889.076846833508
3
Iteration 18900: Loss = -9889.07709464679
4
Iteration 19000: Loss = -9889.075168121743
Iteration 19100: Loss = -9889.075260605054
Iteration 19200: Loss = -9889.076297336334
1
Iteration 19300: Loss = -9889.075247532175
Iteration 19400: Loss = -9889.075920689056
1
Iteration 19500: Loss = -9889.07516071171
Iteration 19600: Loss = -9889.075167912377
Iteration 19700: Loss = -9889.07510780451
Iteration 19800: Loss = -9889.075803388054
1
Iteration 19900: Loss = -9889.07569144977
2
pi: tensor([[9.8567e-01, 1.4325e-02],
        [1.0000e+00, 4.4212e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9635, 0.0365], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1352, 0.2063],
         [0.6223, 0.2482]],

        [[0.5800, 0.1733],
         [0.5121, 0.6142]],

        [[0.7167, 0.0640],
         [0.7235, 0.6709]],

        [[0.5748, 0.2415],
         [0.6939, 0.7251]],

        [[0.6567, 0.2239],
         [0.7048, 0.6607]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.0002897541819023893
Average Adjusted Rand Index: -0.001180785825352918
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20091.035751075517
Iteration 100: Loss = -9892.772539401994
Iteration 200: Loss = -9891.870333734036
Iteration 300: Loss = -9891.58758037657
Iteration 400: Loss = -9891.452736414773
Iteration 500: Loss = -9891.372542878751
Iteration 600: Loss = -9891.311500682572
Iteration 700: Loss = -9891.243100783528
Iteration 800: Loss = -9891.110691860395
Iteration 900: Loss = -9890.854651007161
Iteration 1000: Loss = -9890.44338162373
Iteration 1100: Loss = -9890.037755717274
Iteration 1200: Loss = -9889.848518236422
Iteration 1300: Loss = -9889.623189953592
Iteration 1400: Loss = -9889.47523439574
Iteration 1500: Loss = -9889.406886208091
Iteration 1600: Loss = -9889.372491804912
Iteration 1700: Loss = -9889.350811293281
Iteration 1800: Loss = -9889.333959934344
Iteration 1900: Loss = -9889.319293191846
Iteration 2000: Loss = -9889.306195736504
Iteration 2100: Loss = -9889.294730858468
Iteration 2200: Loss = -9889.285169165874
Iteration 2300: Loss = -9889.277366903589
Iteration 2400: Loss = -9889.271165116148
Iteration 2500: Loss = -9889.266197209192
Iteration 2600: Loss = -9889.26218073105
Iteration 2700: Loss = -9889.258844564154
Iteration 2800: Loss = -9889.256018831084
Iteration 2900: Loss = -9889.253593132318
Iteration 3000: Loss = -9889.25146696275
Iteration 3100: Loss = -9889.249607552425
Iteration 3200: Loss = -9889.247880373377
Iteration 3300: Loss = -9889.246351357617
Iteration 3400: Loss = -9889.244960333017
Iteration 3500: Loss = -9889.243690556941
Iteration 3600: Loss = -9889.242493381182
Iteration 3700: Loss = -9889.24141372153
Iteration 3800: Loss = -9889.24044904638
Iteration 3900: Loss = -9889.239487060195
Iteration 4000: Loss = -9889.238593607002
Iteration 4100: Loss = -9889.237774457837
Iteration 4200: Loss = -9889.237034033033
Iteration 4300: Loss = -9889.236297409157
Iteration 4400: Loss = -9889.235638337013
Iteration 4500: Loss = -9889.235018611651
Iteration 4600: Loss = -9889.234412755022
Iteration 4700: Loss = -9889.233855715704
Iteration 4800: Loss = -9889.23336502494
Iteration 4900: Loss = -9889.232855403046
Iteration 5000: Loss = -9889.232404292721
Iteration 5100: Loss = -9889.231979567521
Iteration 5200: Loss = -9889.231520148267
Iteration 5300: Loss = -9889.23115218912
Iteration 5400: Loss = -9889.230797186765
Iteration 5500: Loss = -9889.230430608697
Iteration 5600: Loss = -9889.230127328692
Iteration 5700: Loss = -9889.229791569434
Iteration 5800: Loss = -9889.229496419186
Iteration 5900: Loss = -9889.229211796845
Iteration 6000: Loss = -9889.228950950825
Iteration 6100: Loss = -9889.228730599449
Iteration 6200: Loss = -9889.228465295551
Iteration 6300: Loss = -9889.228270198677
Iteration 6400: Loss = -9889.228058342822
Iteration 6500: Loss = -9889.227849782934
Iteration 6600: Loss = -9889.22767341536
Iteration 6700: Loss = -9889.227525638962
Iteration 6800: Loss = -9889.227312405836
Iteration 6900: Loss = -9889.227108740377
Iteration 7000: Loss = -9889.226990038878
Iteration 7100: Loss = -9889.226817108354
Iteration 7200: Loss = -9889.226686974602
Iteration 7300: Loss = -9889.226585527831
Iteration 7400: Loss = -9889.226440066637
Iteration 7500: Loss = -9889.226302928571
Iteration 7600: Loss = -9889.245744968863
1
Iteration 7700: Loss = -9889.3035790672
2
Iteration 7800: Loss = -9889.226036991719
Iteration 7900: Loss = -9889.225931886025
Iteration 8000: Loss = -9889.359611451766
1
Iteration 8100: Loss = -9889.225715588305
Iteration 8200: Loss = -9889.428432855273
1
Iteration 8300: Loss = -9889.225560490182
Iteration 8400: Loss = -9889.225484359447
Iteration 8500: Loss = -9889.225702831282
1
Iteration 8600: Loss = -9889.22533476371
Iteration 8700: Loss = -9889.225287288675
Iteration 8800: Loss = -9889.225326711769
Iteration 8900: Loss = -9889.225159378788
Iteration 9000: Loss = -9889.225102369342
Iteration 9100: Loss = -9889.225528466262
1
Iteration 9200: Loss = -9889.224982618176
Iteration 9300: Loss = -9889.224950004726
Iteration 9400: Loss = -9889.224922903279
Iteration 9500: Loss = -9889.22503317411
1
Iteration 9600: Loss = -9889.224805278058
Iteration 9700: Loss = -9889.224776380313
Iteration 9800: Loss = -9889.227797899512
1
Iteration 9900: Loss = -9889.224720264865
Iteration 10000: Loss = -9889.224687236934
Iteration 10100: Loss = -9889.22483232462
1
Iteration 10200: Loss = -9889.224594986503
Iteration 10300: Loss = -9889.224618278939
Iteration 10400: Loss = -9889.228984150453
1
Iteration 10500: Loss = -9889.22454782203
Iteration 10600: Loss = -9889.224502124925
Iteration 10700: Loss = -9889.226595986007
1
Iteration 10800: Loss = -9889.22445804444
Iteration 10900: Loss = -9889.224405983166
Iteration 11000: Loss = -9889.22841221021
1
Iteration 11100: Loss = -9889.224371401275
Iteration 11200: Loss = -9889.224417927142
Iteration 11300: Loss = -9889.22504365379
1
Iteration 11400: Loss = -9889.224336839881
Iteration 11500: Loss = -9889.224325711068
Iteration 11600: Loss = -9889.42662621416
1
Iteration 11700: Loss = -9889.224297124643
Iteration 11800: Loss = -9889.22429664552
Iteration 11900: Loss = -9889.285363864943
1
Iteration 12000: Loss = -9889.224265863959
Iteration 12100: Loss = -9889.224271902507
Iteration 12200: Loss = -9889.258455792715
1
Iteration 12300: Loss = -9889.224272619063
Iteration 12400: Loss = -9889.224224885143
Iteration 12500: Loss = -9889.225230972688
1
Iteration 12600: Loss = -9889.224222920213
Iteration 12700: Loss = -9889.278270109073
1
Iteration 12800: Loss = -9889.224170248894
Iteration 12900: Loss = -9889.224183323717
Iteration 13000: Loss = -9889.224211067278
Iteration 13100: Loss = -9889.227475193024
1
Iteration 13200: Loss = -9889.373515587391
2
Iteration 13300: Loss = -9889.224155264197
Iteration 13400: Loss = -9889.224129095199
Iteration 13500: Loss = -9889.224177525806
Iteration 13600: Loss = -9889.224134189748
Iteration 13700: Loss = -9889.228712332426
1
Iteration 13800: Loss = -9889.224102518943
Iteration 13900: Loss = -9889.226539148718
1
Iteration 14000: Loss = -9889.22450315654
2
Iteration 14100: Loss = -9889.31871039794
3
Iteration 14200: Loss = -9889.224124664985
Iteration 14300: Loss = -9889.275778849544
1
Iteration 14400: Loss = -9889.224106245912
Iteration 14500: Loss = -9889.224104882807
Iteration 14600: Loss = -9889.224795077356
1
Iteration 14700: Loss = -9889.224097579588
Iteration 14800: Loss = -9889.22737341654
1
Iteration 14900: Loss = -9889.224091492033
Iteration 15000: Loss = -9889.225179010255
1
Iteration 15100: Loss = -9889.224084542007
Iteration 15200: Loss = -9889.22969607019
1
Iteration 15300: Loss = -9889.224100421838
Iteration 15400: Loss = -9889.226250552236
1
Iteration 15500: Loss = -9889.224086271865
Iteration 15600: Loss = -9889.230590978063
1
Iteration 15700: Loss = -9889.224067880617
Iteration 15800: Loss = -9889.224084956822
Iteration 15900: Loss = -9889.224388014301
1
Iteration 16000: Loss = -9889.22406160911
Iteration 16100: Loss = -9889.256249364465
1
Iteration 16200: Loss = -9889.224069655218
Iteration 16300: Loss = -9889.340111869311
1
Iteration 16400: Loss = -9889.224105656236
Iteration 16500: Loss = -9889.22548769696
1
Iteration 16600: Loss = -9889.224079426662
Iteration 16700: Loss = -9889.224076307848
Iteration 16800: Loss = -9889.22421069446
1
Iteration 16900: Loss = -9889.224041697036
Iteration 17000: Loss = -9889.635686258553
1
Iteration 17100: Loss = -9889.224373244755
2
Iteration 17200: Loss = -9889.22406307085
Iteration 17300: Loss = -9889.228605785018
1
Iteration 17400: Loss = -9889.224350735363
2
Iteration 17500: Loss = -9889.224085475927
Iteration 17600: Loss = -9889.462282930826
1
Iteration 17700: Loss = -9889.22541819974
2
Iteration 17800: Loss = -9889.240715573029
3
Iteration 17900: Loss = -9889.224883854798
4
Iteration 18000: Loss = -9889.224043677554
Iteration 18100: Loss = -9889.231141570484
1
Iteration 18200: Loss = -9889.224029703044
Iteration 18300: Loss = -9889.250833188602
1
Iteration 18400: Loss = -9889.224031164458
Iteration 18500: Loss = -9889.230228141701
1
Iteration 18600: Loss = -9889.22517107008
2
Iteration 18700: Loss = -9889.224081219578
Iteration 18800: Loss = -9889.224347457352
1
Iteration 18900: Loss = -9889.224127221923
Iteration 19000: Loss = -9889.224058560543
Iteration 19100: Loss = -9889.224051223644
Iteration 19200: Loss = -9889.235543583663
1
Iteration 19300: Loss = -9889.224056148114
Iteration 19400: Loss = -9889.224069943548
Iteration 19500: Loss = -9889.22403939982
Iteration 19600: Loss = -9889.22427499464
1
Iteration 19700: Loss = -9889.22405992719
Iteration 19800: Loss = -9889.304465849013
1
Iteration 19900: Loss = -9889.22404215492
pi: tensor([[9.8290e-01, 1.7098e-02],
        [1.0000e+00, 1.9467e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9611, 0.0389], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1348, 0.2059],
         [0.5794, 0.2569]],

        [[0.7042, 0.1740],
         [0.6523, 0.6017]],

        [[0.6573, 0.1372],
         [0.5058, 0.6631]],

        [[0.5782, 0.2364],
         [0.6766, 0.6664]],

        [[0.5407, 0.2214],
         [0.6066, 0.6712]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.0006432713465041949
Average Adjusted Rand Index: -0.001019041575361005
10046.909621382372
[0.0002897541819023893, 0.0006432713465041949] [-0.001180785825352918, -0.001019041575361005] [9889.07662350418, 9889.242662238785]
-------------------------------------
This iteration is 74
True Objective function: Loss = -10043.741816823027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21417.46881437501
Iteration 100: Loss = -9924.01078734643
Iteration 200: Loss = -9923.06044723057
Iteration 300: Loss = -9922.713988812267
Iteration 400: Loss = -9922.569230328805
Iteration 500: Loss = -9922.494815565607
Iteration 600: Loss = -9922.448272444462
Iteration 700: Loss = -9922.412325631529
Iteration 800: Loss = -9922.377951662087
Iteration 900: Loss = -9922.338435284548
Iteration 1000: Loss = -9922.28481880953
Iteration 1100: Loss = -9922.199621737798
Iteration 1200: Loss = -9922.03769399771
Iteration 1300: Loss = -9921.686652382117
Iteration 1400: Loss = -9921.085759714168
Iteration 1500: Loss = -9920.30546647394
Iteration 1600: Loss = -9919.448491645086
Iteration 1700: Loss = -9919.175844608388
Iteration 1800: Loss = -9919.092870138944
Iteration 1900: Loss = -9919.054075167056
Iteration 2000: Loss = -9919.028182905544
Iteration 2100: Loss = -9919.011598197558
Iteration 2200: Loss = -9918.999277697565
Iteration 2300: Loss = -9918.989714241667
Iteration 2400: Loss = -9918.982090666803
Iteration 2500: Loss = -9918.976798303776
Iteration 2600: Loss = -9918.973782055918
Iteration 2700: Loss = -9918.971633199006
Iteration 2800: Loss = -9918.969883133179
Iteration 2900: Loss = -9918.968371736119
Iteration 3000: Loss = -9918.967149958606
Iteration 3100: Loss = -9918.96600690414
Iteration 3200: Loss = -9918.964889390862
Iteration 3300: Loss = -9918.963834060269
Iteration 3400: Loss = -9918.962842390765
Iteration 3500: Loss = -9918.96195232322
Iteration 3600: Loss = -9918.961124541998
Iteration 3700: Loss = -9918.960367332467
Iteration 3800: Loss = -9918.959745531596
Iteration 3900: Loss = -9918.95916672577
Iteration 4000: Loss = -9918.95861907042
Iteration 4100: Loss = -9918.958204012675
Iteration 4200: Loss = -9918.957817360548
Iteration 4300: Loss = -9918.957463164437
Iteration 4400: Loss = -9918.95786215443
1
Iteration 4500: Loss = -9918.956856438552
Iteration 4600: Loss = -9918.956610154131
Iteration 4700: Loss = -9918.956348578777
Iteration 4800: Loss = -9918.956134664468
Iteration 4900: Loss = -9918.955880315816
Iteration 5000: Loss = -9918.955721752185
Iteration 5100: Loss = -9918.955518570941
Iteration 5200: Loss = -9918.955330817726
Iteration 5300: Loss = -9918.955108347205
Iteration 5400: Loss = -9918.954950230946
Iteration 5500: Loss = -9918.961051346987
1
Iteration 5600: Loss = -9918.954555259706
Iteration 5700: Loss = -9918.954390939947
Iteration 5800: Loss = -9918.954250447585
Iteration 5900: Loss = -9918.954095528663
Iteration 6000: Loss = -9918.954045061786
Iteration 6100: Loss = -9918.953902735775
Iteration 6200: Loss = -9918.953817164796
Iteration 6300: Loss = -9918.953681366134
Iteration 6400: Loss = -9918.953561530443
Iteration 6500: Loss = -9918.953783213454
1
Iteration 6600: Loss = -9918.953449926605
Iteration 6700: Loss = -9918.961514557897
1
Iteration 6800: Loss = -9918.95323473083
Iteration 6900: Loss = -9918.95350650103
1
Iteration 7000: Loss = -9918.953116136343
Iteration 7100: Loss = -9918.954293004746
1
Iteration 7200: Loss = -9918.953000965512
Iteration 7300: Loss = -9918.953106167313
1
Iteration 7400: Loss = -9918.952878059397
Iteration 7500: Loss = -9918.952838098823
Iteration 7600: Loss = -9918.952763361018
Iteration 7700: Loss = -9918.952708393545
Iteration 7800: Loss = -9918.952699704156
Iteration 7900: Loss = -9918.952623060944
Iteration 8000: Loss = -9918.952678294709
Iteration 8100: Loss = -9918.9525737504
Iteration 8200: Loss = -9918.952540771792
Iteration 8300: Loss = -9918.977098639776
1
Iteration 8400: Loss = -9918.952576126641
Iteration 8500: Loss = -9918.952428937291
Iteration 8600: Loss = -9918.971297820433
1
Iteration 8700: Loss = -9918.952373131244
Iteration 8800: Loss = -9918.952359371304
Iteration 8900: Loss = -9918.952312045643
Iteration 9000: Loss = -9918.95228782992
Iteration 9100: Loss = -9918.953632492474
1
Iteration 9200: Loss = -9918.952259744268
Iteration 9300: Loss = -9918.952195677492
Iteration 9400: Loss = -9919.156325499896
1
Iteration 9500: Loss = -9918.95216372411
Iteration 9600: Loss = -9918.952142836555
Iteration 9700: Loss = -9918.960407551276
1
Iteration 9800: Loss = -9918.952149473704
Iteration 9900: Loss = -9918.95210390899
Iteration 10000: Loss = -9919.159215817133
1
Iteration 10100: Loss = -9918.952054513442
Iteration 10200: Loss = -9918.95742766406
1
Iteration 10300: Loss = -9918.952035970804
Iteration 10400: Loss = -9918.952626528897
1
Iteration 10500: Loss = -9918.95381873846
2
Iteration 10600: Loss = -9918.952295343071
3
Iteration 10700: Loss = -9918.954226733424
4
Iteration 10800: Loss = -9918.95659288495
5
Iteration 10900: Loss = -9918.952090421537
Iteration 11000: Loss = -9918.95239823615
1
Iteration 11100: Loss = -9918.952345728014
2
Iteration 11200: Loss = -9918.954742572563
3
Iteration 11300: Loss = -9918.952336841752
4
Iteration 11400: Loss = -9918.998746777548
5
Iteration 11500: Loss = -9918.95219719813
6
Iteration 11600: Loss = -9918.970938644068
7
Iteration 11700: Loss = -9919.006121674507
8
Iteration 11800: Loss = -9918.958587648149
9
Iteration 11900: Loss = -9918.952919903286
10
Iteration 12000: Loss = -9918.952446518615
11
Iteration 12100: Loss = -9918.957551852045
12
Iteration 12200: Loss = -9918.962088122697
13
Iteration 12300: Loss = -9918.955518144927
14
Iteration 12400: Loss = -9918.958255780279
15
Stopping early at iteration 12400 due to no improvement.
pi: tensor([[0.5802, 0.4198],
        [0.0602, 0.9398]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.5783e-05, 9.9998e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1935, 0.1123],
         [0.5087, 0.1357]],

        [[0.6974, 0.0696],
         [0.5810, 0.7051]],

        [[0.7057, 0.1421],
         [0.6499, 0.7297]],

        [[0.7289, 0.1715],
         [0.6298, 0.5999]],

        [[0.6368, 0.1651],
         [0.5736, 0.7038]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.01871047777462865
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.004283318471253273
Average Adjusted Rand Index: 0.004474061416160939
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21196.27048301779
Iteration 100: Loss = -9923.95870692461
Iteration 200: Loss = -9923.25556379065
Iteration 300: Loss = -9922.939035951506
Iteration 400: Loss = -9922.77069788226
Iteration 500: Loss = -9922.670522891418
Iteration 600: Loss = -9922.60619511753
Iteration 700: Loss = -9922.562568601565
Iteration 800: Loss = -9922.531393145746
Iteration 900: Loss = -9922.507961940786
Iteration 1000: Loss = -9922.489249803197
Iteration 1100: Loss = -9922.473247306023
Iteration 1200: Loss = -9922.458408887367
Iteration 1300: Loss = -9922.44335629684
Iteration 1400: Loss = -9922.426777735078
Iteration 1500: Loss = -9922.406899308402
Iteration 1600: Loss = -9922.380984878506
Iteration 1700: Loss = -9922.344064408735
Iteration 1800: Loss = -9922.281870556686
Iteration 1900: Loss = -9922.137521041319
Iteration 2000: Loss = -9921.651162672042
Iteration 2100: Loss = -9920.78287317202
Iteration 2200: Loss = -9919.684841796294
Iteration 2300: Loss = -9919.371623862135
Iteration 2400: Loss = -9919.19097901238
Iteration 2500: Loss = -9919.090040590552
Iteration 2600: Loss = -9919.047604059368
Iteration 2700: Loss = -9919.023601286362
Iteration 2800: Loss = -9919.005846252014
Iteration 2900: Loss = -9918.992058346024
Iteration 3000: Loss = -9918.984156319411
Iteration 3100: Loss = -9918.97916312821
Iteration 3200: Loss = -9918.976420782923
Iteration 3300: Loss = -9918.974484994354
Iteration 3400: Loss = -9918.972602832171
Iteration 3500: Loss = -9918.97105874275
Iteration 3600: Loss = -9918.96979550368
Iteration 3700: Loss = -9918.968662833666
Iteration 3800: Loss = -9918.967547964501
Iteration 3900: Loss = -9918.966283852043
Iteration 4000: Loss = -9918.965052130616
Iteration 4100: Loss = -9918.963775860831
Iteration 4200: Loss = -9918.962656649668
Iteration 4300: Loss = -9918.96198730968
Iteration 4400: Loss = -9918.96082171317
Iteration 4500: Loss = -9918.960404793379
Iteration 4600: Loss = -9918.959290567953
Iteration 4700: Loss = -9918.958689782325
Iteration 4800: Loss = -9918.95824961307
Iteration 4900: Loss = -9918.957851685222
Iteration 5000: Loss = -9918.957463570718
Iteration 5100: Loss = -9918.957122059364
Iteration 5200: Loss = -9918.957549953735
1
Iteration 5300: Loss = -9918.956494373093
Iteration 5400: Loss = -9918.958104887945
1
Iteration 5500: Loss = -9918.956004425587
Iteration 5600: Loss = -9918.955794848265
Iteration 5700: Loss = -9918.955586088905
Iteration 5800: Loss = -9918.955376556141
Iteration 5900: Loss = -9918.955440867478
Iteration 6000: Loss = -9918.954978552882
Iteration 6100: Loss = -9918.958566713392
1
Iteration 6200: Loss = -9918.954570405796
Iteration 6300: Loss = -9918.9543972432
Iteration 6400: Loss = -9918.954115837296
Iteration 6500: Loss = -9918.953958295091
Iteration 6600: Loss = -9918.95438407309
1
Iteration 6700: Loss = -9918.953703288715
Iteration 6800: Loss = -9918.95393391431
1
Iteration 6900: Loss = -9918.953464998069
Iteration 7000: Loss = -9918.96478763314
1
Iteration 7100: Loss = -9918.953285439628
Iteration 7200: Loss = -9918.95743641535
1
Iteration 7300: Loss = -9918.953123897907
Iteration 7400: Loss = -9918.961410368145
1
Iteration 7500: Loss = -9918.953012980934
Iteration 7600: Loss = -9918.952930706211
Iteration 7700: Loss = -9918.952984435162
Iteration 7800: Loss = -9918.952841167647
Iteration 7900: Loss = -9918.953020648934
1
Iteration 8000: Loss = -9918.952720213028
Iteration 8100: Loss = -9918.952675534176
Iteration 8200: Loss = -9919.090061504308
1
Iteration 8300: Loss = -9918.952609633176
Iteration 8400: Loss = -9918.9579694502
1
Iteration 8500: Loss = -9918.952491278858
Iteration 8600: Loss = -9918.974307150782
1
Iteration 8700: Loss = -9918.952375091061
Iteration 8800: Loss = -9918.95234620364
Iteration 8900: Loss = -9918.952571389413
1
Iteration 9000: Loss = -9918.95228565849
Iteration 9100: Loss = -9918.952296962036
Iteration 9200: Loss = -9918.952452930935
1
Iteration 9300: Loss = -9918.95221526055
Iteration 9400: Loss = -9918.956973373974
1
Iteration 9500: Loss = -9918.952167088966
Iteration 9600: Loss = -9918.95217793808
Iteration 9700: Loss = -9918.95552250563
1
Iteration 9800: Loss = -9918.959407291975
2
Iteration 9900: Loss = -9918.990294075991
3
Iteration 10000: Loss = -9918.952090809877
Iteration 10100: Loss = -9918.956175398685
1
Iteration 10200: Loss = -9919.05771254978
2
Iteration 10300: Loss = -9918.952027494464
Iteration 10400: Loss = -9918.953140589912
1
Iteration 10500: Loss = -9918.953569224444
2
Iteration 10600: Loss = -9918.953250722496
3
Iteration 10700: Loss = -9918.952008000677
Iteration 10800: Loss = -9918.952233167161
1
Iteration 10900: Loss = -9918.959940329401
2
Iteration 11000: Loss = -9918.953048702746
3
Iteration 11100: Loss = -9918.961163332375
4
Iteration 11200: Loss = -9918.951940225696
Iteration 11300: Loss = -9918.95272524719
1
Iteration 11400: Loss = -9918.951898896375
Iteration 11500: Loss = -9918.952687882746
1
Iteration 11600: Loss = -9918.951898905041
Iteration 11700: Loss = -9918.95253494251
1
Iteration 11800: Loss = -9918.963395705438
2
Iteration 11900: Loss = -9918.972850100046
3
Iteration 12000: Loss = -9918.971399411801
4
Iteration 12100: Loss = -9918.960016502651
5
Iteration 12200: Loss = -9919.034963869728
6
Iteration 12300: Loss = -9918.958725612998
7
Iteration 12400: Loss = -9918.95246452853
8
Iteration 12500: Loss = -9918.987250774431
9
Iteration 12600: Loss = -9918.954600451803
10
Iteration 12700: Loss = -9918.956242677452
11
Iteration 12800: Loss = -9918.95290985755
12
Iteration 12900: Loss = -9918.951853830631
Iteration 13000: Loss = -9918.955665165895
1
Iteration 13100: Loss = -9918.952044637806
2
Iteration 13200: Loss = -9918.952583045439
3
Iteration 13300: Loss = -9918.952474534124
4
Iteration 13400: Loss = -9918.953365671347
5
Iteration 13500: Loss = -9918.952974892409
6
Iteration 13600: Loss = -9918.959359024435
7
Iteration 13700: Loss = -9918.953944085933
8
Iteration 13800: Loss = -9918.952428451797
9
Iteration 13900: Loss = -9918.956712757772
10
Iteration 14000: Loss = -9918.977257555298
11
Iteration 14100: Loss = -9918.972291419423
12
Iteration 14200: Loss = -9918.952743685906
13
Iteration 14300: Loss = -9918.954371500458
14
Iteration 14400: Loss = -9918.95185455698
Iteration 14500: Loss = -9918.95283880214
1
Iteration 14600: Loss = -9918.952304983104
2
Iteration 14700: Loss = -9918.952113898134
3
Iteration 14800: Loss = -9918.952081372918
4
Iteration 14900: Loss = -9918.975580164068
5
Iteration 15000: Loss = -9918.952547559807
6
Iteration 15100: Loss = -9918.951894949214
Iteration 15200: Loss = -9918.951902625453
Iteration 15300: Loss = -9918.951913788713
Iteration 15400: Loss = -9918.967998942564
1
Iteration 15500: Loss = -9918.961050010175
2
Iteration 15600: Loss = -9918.954723044604
3
Iteration 15700: Loss = -9918.953555645317
4
Iteration 15800: Loss = -9918.951849163823
Iteration 15900: Loss = -9918.95183215888
Iteration 16000: Loss = -9918.962054012252
1
Iteration 16100: Loss = -9918.962192088959
2
Iteration 16200: Loss = -9918.955815247475
3
Iteration 16300: Loss = -9919.002087405162
4
Iteration 16400: Loss = -9918.951897855348
Iteration 16500: Loss = -9918.959435170578
1
Iteration 16600: Loss = -9918.971439622073
2
Iteration 16700: Loss = -9918.951806077059
Iteration 16800: Loss = -9919.024740745219
1
Iteration 16900: Loss = -9918.952048389394
2
Iteration 17000: Loss = -9918.95240016122
3
Iteration 17100: Loss = -9918.967178961066
4
Iteration 17200: Loss = -9918.98263629407
5
Iteration 17300: Loss = -9918.983753074306
6
Iteration 17400: Loss = -9918.953305647388
7
Iteration 17500: Loss = -9918.952075113026
8
Iteration 17600: Loss = -9918.95192343351
9
Iteration 17700: Loss = -9918.952506239833
10
Iteration 17800: Loss = -9918.98970631057
11
Iteration 17900: Loss = -9918.962129985559
12
Iteration 18000: Loss = -9918.953487668128
13
Iteration 18100: Loss = -9918.951972892015
14
Iteration 18200: Loss = -9918.954145985617
15
Stopping early at iteration 18200 due to no improvement.
pi: tensor([[0.9396, 0.0604],
        [0.4192, 0.5808]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 8.6511e-07], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1356, 0.1120],
         [0.5577, 0.1934]],

        [[0.5655, 0.0696],
         [0.7304, 0.7088]],

        [[0.5312, 0.1423],
         [0.5176, 0.6837]],

        [[0.7248, 0.1712],
         [0.5587, 0.5232]],

        [[0.6544, 0.1651],
         [0.5605, 0.7284]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.01871047777462865
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 65
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.004283318471253273
Average Adjusted Rand Index: 0.004474061416160939
10043.741816823027
[0.004283318471253273, 0.004283318471253273] [0.004474061416160939, 0.004474061416160939] [9918.958255780279, 9918.954145985617]
-------------------------------------
This iteration is 75
True Objective function: Loss = -9936.702815894521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22268.101433034684
Iteration 100: Loss = -9772.877774608422
Iteration 200: Loss = -9770.579768927826
Iteration 300: Loss = -9769.99584709948
Iteration 400: Loss = -9769.750699523602
Iteration 500: Loss = -9769.63032221547
Iteration 600: Loss = -9769.557025359833
Iteration 700: Loss = -9769.50945999474
Iteration 800: Loss = -9769.478350606485
Iteration 900: Loss = -9769.456851334644
Iteration 1000: Loss = -9769.44112734243
Iteration 1100: Loss = -9769.4287257949
Iteration 1200: Loss = -9769.417719290675
Iteration 1300: Loss = -9769.405992286067
Iteration 1400: Loss = -9769.389945602357
Iteration 1500: Loss = -9769.36139199644
Iteration 1600: Loss = -9769.307185974967
Iteration 1700: Loss = -9769.25001579942
Iteration 1800: Loss = -9769.231431634918
Iteration 1900: Loss = -9769.225519662941
Iteration 2000: Loss = -9769.221736684047
Iteration 2100: Loss = -9769.218811999423
Iteration 2200: Loss = -9769.216404975054
Iteration 2300: Loss = -9769.214363399862
Iteration 2400: Loss = -9769.212624970636
Iteration 2500: Loss = -9769.211116738115
Iteration 2600: Loss = -9769.20983508506
Iteration 2700: Loss = -9769.208712574857
Iteration 2800: Loss = -9769.207716750694
Iteration 2900: Loss = -9769.206821431264
Iteration 3000: Loss = -9769.206082549408
Iteration 3100: Loss = -9769.205355872398
Iteration 3200: Loss = -9769.204762945496
Iteration 3300: Loss = -9769.204178070946
Iteration 3400: Loss = -9769.203687330411
Iteration 3500: Loss = -9769.20321850058
Iteration 3600: Loss = -9769.202813651027
Iteration 3700: Loss = -9769.202449169195
Iteration 3800: Loss = -9769.202090689694
Iteration 3900: Loss = -9769.201796538913
Iteration 4000: Loss = -9769.201462054038
Iteration 4100: Loss = -9769.201221666852
Iteration 4200: Loss = -9769.200947441619
Iteration 4300: Loss = -9769.200718182698
Iteration 4400: Loss = -9769.200477952607
Iteration 4500: Loss = -9769.20028775874
Iteration 4600: Loss = -9769.200104222082
Iteration 4700: Loss = -9769.19994340996
Iteration 4800: Loss = -9769.199731620154
Iteration 4900: Loss = -9769.19961369432
Iteration 5000: Loss = -9769.199451268047
Iteration 5100: Loss = -9769.19933081983
Iteration 5200: Loss = -9769.199206309868
Iteration 5300: Loss = -9769.199099405963
Iteration 5400: Loss = -9769.19899818745
Iteration 5500: Loss = -9769.198863766711
Iteration 5600: Loss = -9769.198752844883
Iteration 5700: Loss = -9769.198668661453
Iteration 5800: Loss = -9769.198642568526
Iteration 5900: Loss = -9769.198497748752
Iteration 6000: Loss = -9769.19845999088
Iteration 6100: Loss = -9769.198347323618
Iteration 6200: Loss = -9769.198327030172
Iteration 6300: Loss = -9769.198254786665
Iteration 6400: Loss = -9769.19816304562
Iteration 6500: Loss = -9769.198142359459
Iteration 6600: Loss = -9769.1980708699
Iteration 6700: Loss = -9769.199741280325
1
Iteration 6800: Loss = -9769.197989577891
Iteration 6900: Loss = -9769.197939410178
Iteration 7000: Loss = -9769.197892066872
Iteration 7100: Loss = -9769.197846751202
Iteration 7200: Loss = -9769.197804154586
Iteration 7300: Loss = -9769.197757580789
Iteration 7400: Loss = -9769.198193573775
1
Iteration 7500: Loss = -9769.197694652856
Iteration 7600: Loss = -9769.197670515488
Iteration 7700: Loss = -9769.197619530521
Iteration 7800: Loss = -9769.197592051687
Iteration 7900: Loss = -9769.197602584829
Iteration 8000: Loss = -9769.197561605772
Iteration 8100: Loss = -9769.197496999053
Iteration 8200: Loss = -9769.197513652876
Iteration 8300: Loss = -9769.197476652542
Iteration 8400: Loss = -9769.198148541864
1
Iteration 8500: Loss = -9769.197434398293
Iteration 8600: Loss = -9769.200954381071
1
Iteration 8700: Loss = -9769.197414179705
Iteration 8800: Loss = -9769.197446297641
Iteration 8900: Loss = -9769.197649765754
1
Iteration 9000: Loss = -9769.197532410453
Iteration 9100: Loss = -9769.197328719432
Iteration 9200: Loss = -9769.197421357936
Iteration 9300: Loss = -9769.197315325482
Iteration 9400: Loss = -9769.200218048032
1
Iteration 9500: Loss = -9769.197272794047
Iteration 9600: Loss = -9769.445653355242
1
Iteration 9700: Loss = -9769.197247819015
Iteration 9800: Loss = -9769.197231656985
Iteration 9900: Loss = -9769.200390036369
1
Iteration 10000: Loss = -9769.19722412009
Iteration 10100: Loss = -9769.1972025559
Iteration 10200: Loss = -9769.197212959318
Iteration 10300: Loss = -9769.225969280405
1
Iteration 10400: Loss = -9769.197187374468
Iteration 10500: Loss = -9769.197185696683
Iteration 10600: Loss = -9769.197145749962
Iteration 10700: Loss = -9769.197449314996
1
Iteration 10800: Loss = -9769.197151739067
Iteration 10900: Loss = -9769.197171741844
Iteration 11000: Loss = -9769.197134698437
Iteration 11100: Loss = -9769.200231635135
1
Iteration 11200: Loss = -9769.19713211559
Iteration 11300: Loss = -9769.197114322928
Iteration 11400: Loss = -9769.267251045438
1
Iteration 11500: Loss = -9769.197165286841
Iteration 11600: Loss = -9769.197105750656
Iteration 11700: Loss = -9769.197295209451
1
Iteration 11800: Loss = -9769.197130131497
Iteration 11900: Loss = -9769.197093974512
Iteration 12000: Loss = -9769.197124607144
Iteration 12100: Loss = -9769.199712013395
1
Iteration 12200: Loss = -9769.197085892029
Iteration 12300: Loss = -9769.197072065619
Iteration 12400: Loss = -9769.201031975874
1
Iteration 12500: Loss = -9769.197052197693
Iteration 12600: Loss = -9769.197087585791
Iteration 12700: Loss = -9769.210721433494
1
Iteration 12800: Loss = -9769.19708521205
Iteration 12900: Loss = -9769.197078177776
Iteration 13000: Loss = -9769.198511059658
1
Iteration 13100: Loss = -9769.197105849516
Iteration 13200: Loss = -9769.197039927043
Iteration 13300: Loss = -9769.197054016378
Iteration 13400: Loss = -9769.199244600964
1
Iteration 13500: Loss = -9769.197070194215
Iteration 13600: Loss = -9769.197083200006
Iteration 13700: Loss = -9769.224905170106
1
Iteration 13800: Loss = -9769.197041384567
Iteration 13900: Loss = -9769.197284212472
1
Iteration 14000: Loss = -9769.199960706283
2
Iteration 14100: Loss = -9769.19704888397
Iteration 14200: Loss = -9769.225213976677
1
Iteration 14300: Loss = -9769.197044467834
Iteration 14400: Loss = -9769.197050220193
Iteration 14500: Loss = -9769.200993447914
1
Iteration 14600: Loss = -9769.197028061275
Iteration 14700: Loss = -9769.199737180523
1
Iteration 14800: Loss = -9769.197400118968
2
Iteration 14900: Loss = -9769.197050615054
Iteration 15000: Loss = -9769.205356341157
1
Iteration 15100: Loss = -9769.19703713175
Iteration 15200: Loss = -9769.197025396143
Iteration 15300: Loss = -9769.198187789363
1
Iteration 15400: Loss = -9769.197040683703
Iteration 15500: Loss = -9769.197253022963
1
Iteration 15600: Loss = -9769.197223800516
2
Iteration 15700: Loss = -9769.197038760156
Iteration 15800: Loss = -9769.21052476726
1
Iteration 15900: Loss = -9769.197096607584
Iteration 16000: Loss = -9769.197083067582
Iteration 16100: Loss = -9769.197030598927
Iteration 16200: Loss = -9769.197036158585
Iteration 16300: Loss = -9769.19704832276
Iteration 16400: Loss = -9769.197393343138
1
Iteration 16500: Loss = -9769.197051941344
Iteration 16600: Loss = -9769.213172956397
1
Iteration 16700: Loss = -9769.197043270362
Iteration 16800: Loss = -9769.197912411888
1
Iteration 16900: Loss = -9769.19704409008
Iteration 17000: Loss = -9769.197026323774
Iteration 17100: Loss = -9769.35204039205
1
Iteration 17200: Loss = -9769.19703666809
Iteration 17300: Loss = -9769.197034134677
Iteration 17400: Loss = -9769.19786096263
1
Iteration 17500: Loss = -9769.197008076619
Iteration 17600: Loss = -9769.197226540884
1
Iteration 17700: Loss = -9769.197077527604
Iteration 17800: Loss = -9769.19704947359
Iteration 17900: Loss = -9769.208404046738
1
Iteration 18000: Loss = -9769.197012781988
Iteration 18100: Loss = -9769.197040175137
Iteration 18200: Loss = -9769.197451437729
1
Iteration 18300: Loss = -9769.197010602991
Iteration 18400: Loss = -9769.224362579811
1
Iteration 18500: Loss = -9769.197026422258
Iteration 18600: Loss = -9769.217404096518
1
Iteration 18700: Loss = -9769.19702023999
Iteration 18800: Loss = -9769.219546936876
1
Iteration 18900: Loss = -9769.19704559742
Iteration 19000: Loss = -9769.236007548328
1
Iteration 19100: Loss = -9769.197028495611
Iteration 19200: Loss = -9769.201750744367
1
Iteration 19300: Loss = -9769.197017996492
Iteration 19400: Loss = -9769.197275201395
1
Iteration 19500: Loss = -9769.197011024948
Iteration 19600: Loss = -9769.197768733618
1
Iteration 19700: Loss = -9769.197147297591
2
Iteration 19800: Loss = -9769.201315023889
3
Iteration 19900: Loss = -9769.19705259055
pi: tensor([[9.8263e-01, 1.7374e-02],
        [1.0000e+00, 3.7580e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9982, 0.0018], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1323, 0.1571],
         [0.6712, 0.3880]],

        [[0.7142, 0.2285],
         [0.6183, 0.6495]],

        [[0.5364, 0.1806],
         [0.6657, 0.6997]],

        [[0.6686, 0.1690],
         [0.5451, 0.7235]],

        [[0.6729, 0.2596],
         [0.5632, 0.6500]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.0033679068004826202
Average Adjusted Rand Index: 0.0017177239518776325
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20175.415836638495
Iteration 100: Loss = -9773.21152165323
Iteration 200: Loss = -9770.491460335084
Iteration 300: Loss = -9769.723670431866
Iteration 400: Loss = -9769.500596505019
Iteration 500: Loss = -9769.403014160684
Iteration 600: Loss = -9769.350604585607
Iteration 700: Loss = -9769.318054712408
Iteration 800: Loss = -9769.295638216247
Iteration 900: Loss = -9769.279256214591
Iteration 1000: Loss = -9769.266784935324
Iteration 1100: Loss = -9769.256717589322
Iteration 1200: Loss = -9769.248405750073
Iteration 1300: Loss = -9769.241377532979
Iteration 1400: Loss = -9769.23546000089
Iteration 1500: Loss = -9769.230438164454
Iteration 1600: Loss = -9769.226138305437
Iteration 1700: Loss = -9769.22248649826
Iteration 1800: Loss = -9769.219464856858
Iteration 1900: Loss = -9769.21687508001
Iteration 2000: Loss = -9769.214693342741
Iteration 2100: Loss = -9769.212831148474
Iteration 2200: Loss = -9769.211252329613
Iteration 2300: Loss = -9769.209867955617
Iteration 2400: Loss = -9769.208680391903
Iteration 2500: Loss = -9769.207662253173
Iteration 2600: Loss = -9769.206770033845
Iteration 2700: Loss = -9769.205977055433
Iteration 2800: Loss = -9769.205283843416
Iteration 2900: Loss = -9769.204638673613
Iteration 3000: Loss = -9769.204082468927
Iteration 3100: Loss = -9769.20358723777
Iteration 3200: Loss = -9769.203080222702
Iteration 3300: Loss = -9769.20266672109
Iteration 3400: Loss = -9769.202262487222
Iteration 3500: Loss = -9769.201922124963
Iteration 3600: Loss = -9769.201560234389
Iteration 3700: Loss = -9769.201257710627
Iteration 3800: Loss = -9769.200998937533
Iteration 3900: Loss = -9769.200745379734
Iteration 4000: Loss = -9769.200479055633
Iteration 4100: Loss = -9769.200274834659
Iteration 4200: Loss = -9769.20005348288
Iteration 4300: Loss = -9769.199867653317
Iteration 4400: Loss = -9769.19967506222
Iteration 4500: Loss = -9769.199493150005
Iteration 4600: Loss = -9769.199364249842
Iteration 4700: Loss = -9769.199223005096
Iteration 4800: Loss = -9769.199054506438
Iteration 4900: Loss = -9769.198920668028
Iteration 5000: Loss = -9769.198820225938
Iteration 5100: Loss = -9769.198716072673
Iteration 5200: Loss = -9769.198578920646
Iteration 5300: Loss = -9769.198513834632
Iteration 5400: Loss = -9769.198430899527
Iteration 5500: Loss = -9769.198330074632
Iteration 5600: Loss = -9769.198253539222
Iteration 5700: Loss = -9769.198202968326
Iteration 5800: Loss = -9769.198113577206
Iteration 5900: Loss = -9769.198076104274
Iteration 6000: Loss = -9769.198010698075
Iteration 6100: Loss = -9769.197937570274
Iteration 6200: Loss = -9769.197906641035
Iteration 6300: Loss = -9769.1978496669
Iteration 6400: Loss = -9769.197818066114
Iteration 6500: Loss = -9769.19778432486
Iteration 6600: Loss = -9769.197717346398
Iteration 6700: Loss = -9769.197849174365
1
Iteration 6800: Loss = -9769.197646102373
Iteration 6900: Loss = -9769.199513325308
1
Iteration 7000: Loss = -9769.197603493454
Iteration 7100: Loss = -9769.200154077429
1
Iteration 7200: Loss = -9769.197523943703
Iteration 7300: Loss = -9769.197488611762
Iteration 7400: Loss = -9769.197495037117
Iteration 7500: Loss = -9769.197434795298
Iteration 7600: Loss = -9769.19767913535
1
Iteration 7700: Loss = -9769.197439445386
Iteration 7800: Loss = -9769.19744108917
Iteration 7900: Loss = -9769.197381296375
Iteration 8000: Loss = -9769.197354610076
Iteration 8100: Loss = -9769.197358830623
Iteration 8200: Loss = -9769.197589133071
1
Iteration 8300: Loss = -9769.197303120693
Iteration 8400: Loss = -9769.19737096715
Iteration 8500: Loss = -9769.197909164901
1
Iteration 8600: Loss = -9769.205787864885
2
Iteration 8700: Loss = -9769.19725274859
Iteration 8800: Loss = -9769.19873119149
1
Iteration 8900: Loss = -9769.197231412625
Iteration 9000: Loss = -9769.197251265456
Iteration 9100: Loss = -9769.197202029713
Iteration 9200: Loss = -9769.19721380903
Iteration 9300: Loss = -9769.197865268028
1
Iteration 9400: Loss = -9769.197217114826
Iteration 9500: Loss = -9769.19716956753
Iteration 9600: Loss = -9769.197167870998
Iteration 9700: Loss = -9769.197517200308
1
Iteration 9800: Loss = -9769.197165117625
Iteration 9900: Loss = -9769.197139794005
Iteration 10000: Loss = -9769.197112561007
Iteration 10100: Loss = -9769.19715984424
Iteration 10200: Loss = -9769.197119751476
Iteration 10300: Loss = -9769.197105541947
Iteration 10400: Loss = -9769.197114379274
Iteration 10500: Loss = -9769.221518449493
1
Iteration 10600: Loss = -9769.197132353478
Iteration 10700: Loss = -9769.197134835349
Iteration 10800: Loss = -9769.197136505172
Iteration 10900: Loss = -9769.201688621479
1
Iteration 11000: Loss = -9769.197099223258
Iteration 11100: Loss = -9769.197109105682
Iteration 11200: Loss = -9769.197078780733
Iteration 11300: Loss = -9769.24303065437
1
Iteration 11400: Loss = -9769.197089913087
Iteration 11500: Loss = -9769.197084204734
Iteration 11600: Loss = -9769.197068664891
Iteration 11700: Loss = -9769.199430457493
1
Iteration 11800: Loss = -9769.197076096985
Iteration 11900: Loss = -9769.197079627735
Iteration 12000: Loss = -9769.573893355135
1
Iteration 12100: Loss = -9769.197072798106
Iteration 12200: Loss = -9769.197081683827
Iteration 12300: Loss = -9769.197057025043
Iteration 12400: Loss = -9769.197217679306
1
Iteration 12500: Loss = -9769.197076378961
Iteration 12600: Loss = -9769.197037099611
Iteration 12700: Loss = -9769.197069763191
Iteration 12800: Loss = -9769.197032080358
Iteration 12900: Loss = -9769.197054803302
Iteration 13000: Loss = -9769.235876597482
1
Iteration 13100: Loss = -9769.19704837144
Iteration 13200: Loss = -9769.197071529716
Iteration 13300: Loss = -9769.262171568229
1
Iteration 13400: Loss = -9769.197067594692
Iteration 13500: Loss = -9769.197021629674
Iteration 13600: Loss = -9769.282711586446
1
Iteration 13700: Loss = -9769.197034986832
Iteration 13800: Loss = -9769.197047075917
Iteration 13900: Loss = -9769.197595744507
1
Iteration 14000: Loss = -9769.197027370814
Iteration 14100: Loss = -9769.246463484298
1
Iteration 14200: Loss = -9769.197053010595
Iteration 14300: Loss = -9769.197618132239
1
Iteration 14400: Loss = -9769.19726810921
2
Iteration 14500: Loss = -9769.234662935743
3
Iteration 14600: Loss = -9769.197057703916
Iteration 14700: Loss = -9769.203907143448
1
Iteration 14800: Loss = -9769.197024901849
Iteration 14900: Loss = -9769.19831631849
1
Iteration 15000: Loss = -9769.197017717588
Iteration 15100: Loss = -9769.19703605228
Iteration 15200: Loss = -9769.19750029092
1
Iteration 15300: Loss = -9769.197038758131
Iteration 15400: Loss = -9769.197022801256
Iteration 15500: Loss = -9769.198507388677
1
Iteration 15600: Loss = -9769.197029322964
Iteration 15700: Loss = -9769.22787055324
1
Iteration 15800: Loss = -9769.197059783082
Iteration 15900: Loss = -9769.197109852796
Iteration 16000: Loss = -9769.197106129599
Iteration 16100: Loss = -9769.27477222109
1
Iteration 16200: Loss = -9769.197046849702
Iteration 16300: Loss = -9769.202544512525
1
Iteration 16400: Loss = -9769.19703373981
Iteration 16500: Loss = -9769.19705365366
Iteration 16600: Loss = -9769.209026042685
1
Iteration 16700: Loss = -9769.19703508768
Iteration 16800: Loss = -9769.197015031925
Iteration 16900: Loss = -9769.19832087198
1
Iteration 17000: Loss = -9769.197035380565
Iteration 17100: Loss = -9769.197049424163
Iteration 17200: Loss = -9769.197659172894
1
Iteration 17300: Loss = -9769.197033238246
Iteration 17400: Loss = -9769.19702137651
Iteration 17500: Loss = -9769.197032605669
Iteration 17600: Loss = -9769.197225749695
1
Iteration 17700: Loss = -9769.19707028187
Iteration 17800: Loss = -9769.197047905112
Iteration 17900: Loss = -9769.197011077818
Iteration 18000: Loss = -9769.19734255374
1
Iteration 18100: Loss = -9769.197034041601
Iteration 18200: Loss = -9769.201553581597
1
Iteration 18300: Loss = -9769.197067608031
Iteration 18400: Loss = -9769.202437192447
1
Iteration 18500: Loss = -9769.197049650395
Iteration 18600: Loss = -9769.197040341704
Iteration 18700: Loss = -9769.197524537632
1
Iteration 18800: Loss = -9769.388687341538
2
Iteration 18900: Loss = -9769.197010730231
Iteration 19000: Loss = -9769.30739988233
1
Iteration 19100: Loss = -9769.197028252385
Iteration 19200: Loss = -9769.215913013913
1
Iteration 19300: Loss = -9769.19703089936
Iteration 19400: Loss = -9769.19983062695
1
Iteration 19500: Loss = -9769.19730637422
2
Iteration 19600: Loss = -9769.197042870703
Iteration 19700: Loss = -9769.197070655004
Iteration 19800: Loss = -9769.197342192463
1
Iteration 19900: Loss = -9769.199173463798
2
pi: tensor([[9.8264e-01, 1.7363e-02],
        [1.0000e+00, 2.7931e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9982, 0.0018], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1324, 0.1569],
         [0.6000, 0.3879]],

        [[0.6292, 0.2285],
         [0.6969, 0.5549]],

        [[0.7022, 0.1804],
         [0.5499, 0.7002]],

        [[0.6166, 0.1689],
         [0.6146, 0.7224]],

        [[0.5237, 0.2597],
         [0.5809, 0.5997]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.0033679068004826202
Average Adjusted Rand Index: 0.0017177239518776325
9936.702815894521
[0.0033679068004826202, 0.0033679068004826202] [0.0017177239518776325, 0.0017177239518776325] [9769.197486999756, 9769.19707814444]
-------------------------------------
This iteration is 76
True Objective function: Loss = -10040.330889248846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20871.040138493492
Iteration 100: Loss = -9944.375445394784
Iteration 200: Loss = -9943.318946470885
Iteration 300: Loss = -9942.769548553315
Iteration 400: Loss = -9942.113861292522
Iteration 500: Loss = -9939.004692558983
Iteration 600: Loss = -9938.10263658514
Iteration 700: Loss = -9937.190278150016
Iteration 800: Loss = -9936.688192651278
Iteration 900: Loss = -9936.423138215514
Iteration 1000: Loss = -9936.213141747327
Iteration 1100: Loss = -9936.041269500065
Iteration 1200: Loss = -9935.924270970043
Iteration 1300: Loss = -9935.823760879282
Iteration 1400: Loss = -9935.735217497917
Iteration 1500: Loss = -9935.664476774598
Iteration 1600: Loss = -9935.609486732663
Iteration 1700: Loss = -9935.568318473486
Iteration 1800: Loss = -9935.536731693033
Iteration 1900: Loss = -9935.511733721447
Iteration 2000: Loss = -9935.494905394136
Iteration 2100: Loss = -9935.483677164388
Iteration 2200: Loss = -9935.475764711602
Iteration 2300: Loss = -9935.470161946823
Iteration 2400: Loss = -9935.4660388882
Iteration 2500: Loss = -9935.462796140953
Iteration 2600: Loss = -9935.460124061863
Iteration 2700: Loss = -9935.457856567835
Iteration 2800: Loss = -9935.455999085081
Iteration 2900: Loss = -9935.454369599702
Iteration 3000: Loss = -9935.4529672989
Iteration 3100: Loss = -9935.45169926684
Iteration 3200: Loss = -9935.450598286974
Iteration 3300: Loss = -9935.449625188972
Iteration 3400: Loss = -9935.448683229466
Iteration 3500: Loss = -9935.447865158936
Iteration 3600: Loss = -9935.44707603615
Iteration 3700: Loss = -9935.446408912463
Iteration 3800: Loss = -9935.44580299938
Iteration 3900: Loss = -9935.445142045975
Iteration 4000: Loss = -9935.444626424518
Iteration 4100: Loss = -9935.444125504488
Iteration 4200: Loss = -9935.443627944649
Iteration 4300: Loss = -9935.444271031014
1
Iteration 4400: Loss = -9935.44280394737
Iteration 4500: Loss = -9935.442370480376
Iteration 4600: Loss = -9935.442070393574
Iteration 4700: Loss = -9935.441713338589
Iteration 4800: Loss = -9935.443881943778
1
Iteration 4900: Loss = -9935.44115118483
Iteration 5000: Loss = -9935.440907192893
Iteration 5100: Loss = -9935.440685860283
Iteration 5200: Loss = -9935.440465345351
Iteration 5300: Loss = -9935.441856596986
1
Iteration 5400: Loss = -9935.44007582983
Iteration 5500: Loss = -9935.439937237436
Iteration 5600: Loss = -9935.439710322285
Iteration 5700: Loss = -9935.439649042357
Iteration 5800: Loss = -9935.439397386683
Iteration 5900: Loss = -9935.439343579837
Iteration 6000: Loss = -9935.439132018928
Iteration 6100: Loss = -9935.440808601667
1
Iteration 6200: Loss = -9935.438930098013
Iteration 6300: Loss = -9935.438858112955
Iteration 6400: Loss = -9935.438722496103
Iteration 6500: Loss = -9935.4416733668
1
Iteration 6600: Loss = -9935.438503716863
Iteration 6700: Loss = -9935.443449603272
1
Iteration 6800: Loss = -9935.438893309312
2
Iteration 6900: Loss = -9935.43832414399
Iteration 7000: Loss = -9935.438218251164
Iteration 7100: Loss = -9935.443594856584
1
Iteration 7200: Loss = -9935.438037491074
Iteration 7300: Loss = -9935.438429914182
1
Iteration 7400: Loss = -9935.437957765815
Iteration 7500: Loss = -9935.438082913179
1
Iteration 7600: Loss = -9935.437870159234
Iteration 7700: Loss = -9935.437906433328
Iteration 7800: Loss = -9935.437770804425
Iteration 7900: Loss = -9935.437756095038
Iteration 8000: Loss = -9935.437710106418
Iteration 8100: Loss = -9935.437662053571
Iteration 8200: Loss = -9935.43819032905
1
Iteration 8300: Loss = -9935.437612208296
Iteration 8400: Loss = -9935.44455971104
1
Iteration 8500: Loss = -9935.437556435852
Iteration 8600: Loss = -9935.437744825651
1
Iteration 8700: Loss = -9935.437506722066
Iteration 8800: Loss = -9935.43947039885
1
Iteration 8900: Loss = -9935.43748219103
Iteration 9000: Loss = -9935.507243079875
1
Iteration 9100: Loss = -9935.437452876164
Iteration 9200: Loss = -9935.437433005089
Iteration 9300: Loss = -9935.437705012708
1
Iteration 9400: Loss = -9935.437412366588
Iteration 9500: Loss = -9935.437370530852
Iteration 9600: Loss = -9935.43740646962
Iteration 9700: Loss = -9935.43737082291
Iteration 9800: Loss = -9935.918290817963
1
Iteration 9900: Loss = -9935.4373181042
Iteration 10000: Loss = -9935.437293127681
Iteration 10100: Loss = -9935.44532801147
1
Iteration 10200: Loss = -9935.437283762221
Iteration 10300: Loss = -9935.437281008919
Iteration 10400: Loss = -9935.437362129327
Iteration 10500: Loss = -9935.43722338569
Iteration 10600: Loss = -9935.758985224133
1
Iteration 10700: Loss = -9935.437176914042
Iteration 10800: Loss = -9935.437128842043
Iteration 10900: Loss = -9935.442699219528
1
Iteration 11000: Loss = -9935.437059655424
Iteration 11100: Loss = -9935.437012223078
Iteration 11200: Loss = -9935.532941459074
1
Iteration 11300: Loss = -9935.436822494965
Iteration 11400: Loss = -9935.436701055476
Iteration 11500: Loss = -9935.436506129405
Iteration 11600: Loss = -9935.436010529385
Iteration 11700: Loss = -9935.43759571931
1
Iteration 11800: Loss = -9935.431945105187
Iteration 11900: Loss = -9935.425237793599
Iteration 12000: Loss = -9935.477797471643
1
Iteration 12100: Loss = -9932.147996618043
Iteration 12200: Loss = -9932.11869198772
Iteration 12300: Loss = -9932.113026147212
Iteration 12400: Loss = -9932.110240405775
Iteration 12500: Loss = -9932.108583456326
Iteration 12600: Loss = -9932.107435118103
Iteration 12700: Loss = -9932.106607369265
Iteration 12800: Loss = -9932.105984329464
Iteration 12900: Loss = -9932.105512007325
Iteration 13000: Loss = -9932.105076899534
Iteration 13100: Loss = -9932.104777312361
Iteration 13200: Loss = -9932.10450647482
Iteration 13300: Loss = -9932.104499257763
Iteration 13400: Loss = -9932.104065636646
Iteration 13500: Loss = -9932.10490068543
1
Iteration 13600: Loss = -9932.10372153384
Iteration 13700: Loss = -9932.704487839577
1
Iteration 13800: Loss = -9932.103466115332
Iteration 13900: Loss = -9932.103364621658
Iteration 14000: Loss = -9932.114363761451
1
Iteration 14100: Loss = -9932.103549734713
2
Iteration 14200: Loss = -9932.103610488875
3
Iteration 14300: Loss = -9932.105350411568
4
Iteration 14400: Loss = -9932.102969946945
Iteration 14500: Loss = -9932.103102513742
1
Iteration 14600: Loss = -9932.102869905593
Iteration 14700: Loss = -9932.104253424943
1
Iteration 14800: Loss = -9932.10278192985
Iteration 14900: Loss = -9932.102965351063
1
Iteration 15000: Loss = -9932.102668859938
Iteration 15100: Loss = -9932.102675610033
Iteration 15200: Loss = -9932.102729086986
Iteration 15300: Loss = -9932.102592231928
Iteration 15400: Loss = -9932.125275666975
1
Iteration 15500: Loss = -9932.102538395582
Iteration 15600: Loss = -9932.36432485686
1
Iteration 15700: Loss = -9932.102479284995
Iteration 15800: Loss = -9932.102451511162
Iteration 15900: Loss = -9932.102568115299
1
Iteration 16000: Loss = -9932.102409791489
Iteration 16100: Loss = -9932.123849753714
1
Iteration 16200: Loss = -9932.102367030906
Iteration 16300: Loss = -9932.102333300343
Iteration 16400: Loss = -9932.103886573383
1
Iteration 16500: Loss = -9932.123922866496
2
Iteration 16600: Loss = -9932.150427746994
3
Iteration 16700: Loss = -9932.102300499831
Iteration 16800: Loss = -9932.108904708168
1
Iteration 16900: Loss = -9932.102270296218
Iteration 17000: Loss = -9932.114723477967
1
Iteration 17100: Loss = -9932.161300367889
2
Iteration 17200: Loss = -9932.10225045237
Iteration 17300: Loss = -9932.102676626715
1
Iteration 17400: Loss = -9932.102357163016
2
Iteration 17500: Loss = -9932.102175904158
Iteration 17600: Loss = -9932.102209543362
Iteration 17700: Loss = -9932.102502859494
1
Iteration 17800: Loss = -9932.102135020477
Iteration 17900: Loss = -9932.10242406983
1
Iteration 18000: Loss = -9932.102208548096
Iteration 18100: Loss = -9932.24776932409
1
Iteration 18200: Loss = -9932.102189673371
Iteration 18300: Loss = -9932.102184175797
Iteration 18400: Loss = -9932.10543529333
1
Iteration 18500: Loss = -9932.102146287676
Iteration 18600: Loss = -9932.1021775651
Iteration 18700: Loss = -9932.102213687254
Iteration 18800: Loss = -9932.2342763622
1
Iteration 18900: Loss = -9932.102142182848
Iteration 19000: Loss = -9932.105559030042
1
Iteration 19100: Loss = -9932.10214073821
Iteration 19200: Loss = -9932.225979349249
1
Iteration 19300: Loss = -9932.102090364848
Iteration 19400: Loss = -9932.102138487031
Iteration 19500: Loss = -9932.11432232353
1
Iteration 19600: Loss = -9932.102104986812
Iteration 19700: Loss = -9932.102452550607
1
Iteration 19800: Loss = -9932.115609985573
2
Iteration 19900: Loss = -9932.10208990558
pi: tensor([[1.0000e+00, 8.7439e-07],
        [2.1318e-01, 7.8682e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.4305e-05, 9.9997e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1932, 0.1506],
         [0.6081, 0.1303]],

        [[0.5932, 0.1430],
         [0.5205, 0.6901]],

        [[0.5024, 0.1380],
         [0.6854, 0.6464]],

        [[0.5665, 0.1242],
         [0.5616, 0.5449]],

        [[0.7293, 0.0960],
         [0.7244, 0.6763]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.010344451541541386
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 33
Adjusted Rand Index: 0.10685562944258625
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 19
Adjusted Rand Index: 0.3783133485807961
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 10
Adjusted Rand Index: 0.6355210470028131
Global Adjusted Rand Index: 0.1382116749758509
Average Adjusted Rand Index: 0.22620689531354737
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22857.66055645226
Iteration 100: Loss = -9945.144694466866
Iteration 200: Loss = -9942.022975793745
Iteration 300: Loss = -9940.850298809997
Iteration 400: Loss = -9940.02104119306
Iteration 500: Loss = -9939.389692489282
Iteration 600: Loss = -9938.604921659944
Iteration 700: Loss = -9937.68900523331
Iteration 800: Loss = -9937.36781712805
Iteration 900: Loss = -9937.107961548394
Iteration 1000: Loss = -9936.868697988077
Iteration 1100: Loss = -9936.661099497232
Iteration 1200: Loss = -9936.456030167385
Iteration 1300: Loss = -9936.25921994276
Iteration 1400: Loss = -9936.12556273784
Iteration 1500: Loss = -9936.011705586194
Iteration 1600: Loss = -9935.939394021409
Iteration 1700: Loss = -9935.883700240465
Iteration 1800: Loss = -9935.830429569593
Iteration 1900: Loss = -9935.779278321572
Iteration 2000: Loss = -9935.727495912222
Iteration 2100: Loss = -9935.675549224372
Iteration 2200: Loss = -9935.632862688763
Iteration 2300: Loss = -9935.592590912567
Iteration 2400: Loss = -9935.551247391222
Iteration 2500: Loss = -9935.518663647661
Iteration 2600: Loss = -9935.492143817977
Iteration 2700: Loss = -9935.472074331772
Iteration 2800: Loss = -9935.45962873296
Iteration 2900: Loss = -9935.452135573278
Iteration 3000: Loss = -9935.447495998016
Iteration 3100: Loss = -9935.444456148714
Iteration 3200: Loss = -9935.442207188775
Iteration 3300: Loss = -9935.440502587557
Iteration 3400: Loss = -9935.439210214368
Iteration 3500: Loss = -9935.43832757151
Iteration 3600: Loss = -9935.437716366234
Iteration 3700: Loss = -9935.437291061446
Iteration 3800: Loss = -9935.43696637836
Iteration 3900: Loss = -9935.436673860795
Iteration 4000: Loss = -9935.437151776974
1
Iteration 4100: Loss = -9935.43630779539
Iteration 4200: Loss = -9935.43613443816
Iteration 4300: Loss = -9935.436189095888
Iteration 4400: Loss = -9935.435881764432
Iteration 4500: Loss = -9935.435760010858
Iteration 4600: Loss = -9935.435648927118
Iteration 4700: Loss = -9935.4355380843
Iteration 4800: Loss = -9935.43544091903
Iteration 4900: Loss = -9935.435370122761
Iteration 5000: Loss = -9935.435220219906
Iteration 5100: Loss = -9935.435370126006
1
Iteration 5200: Loss = -9935.435017855185
Iteration 5300: Loss = -9935.43488431247
Iteration 5400: Loss = -9935.434725918532
Iteration 5500: Loss = -9935.434551334545
Iteration 5600: Loss = -9935.435439605122
1
Iteration 5700: Loss = -9935.434144605102
Iteration 5800: Loss = -9935.433855843992
Iteration 5900: Loss = -9935.433557152923
Iteration 6000: Loss = -9935.433172409528
Iteration 6100: Loss = -9935.432783471486
Iteration 6200: Loss = -9935.432047615526
Iteration 6300: Loss = -9935.431277644571
Iteration 6400: Loss = -9935.430266300937
Iteration 6500: Loss = -9935.429058225942
Iteration 6600: Loss = -9935.427574884727
Iteration 6700: Loss = -9935.425592499627
Iteration 6800: Loss = -9935.42332142221
Iteration 6900: Loss = -9935.420807632283
Iteration 7000: Loss = -9935.408121934026
Iteration 7100: Loss = -9935.286698773438
Iteration 7200: Loss = -9928.540626113618
Iteration 7300: Loss = -9928.118188777215
Iteration 7400: Loss = -9928.117471512722
Iteration 7500: Loss = -9928.117328457176
Iteration 7600: Loss = -9928.11723496253
Iteration 7700: Loss = -9928.11717424697
Iteration 7800: Loss = -9928.117194395645
Iteration 7900: Loss = -9928.117135720962
Iteration 8000: Loss = -9928.117143986905
Iteration 8100: Loss = -9928.117807343246
1
Iteration 8200: Loss = -9928.117095142727
Iteration 8300: Loss = -9928.117106379303
Iteration 8400: Loss = -9928.1185970216
1
Iteration 8500: Loss = -9928.117142630468
Iteration 8600: Loss = -9928.117102218515
Iteration 8700: Loss = -9928.164174915104
1
Iteration 8800: Loss = -9928.117084157799
Iteration 8900: Loss = -9928.117072626497
Iteration 9000: Loss = -9928.136191863725
1
Iteration 9100: Loss = -9928.117085252481
Iteration 9200: Loss = -9928.117075157606
Iteration 9300: Loss = -9928.117189173407
1
Iteration 9400: Loss = -9928.117091765369
Iteration 9500: Loss = -9928.117894306071
1
Iteration 9600: Loss = -9928.117102669608
Iteration 9700: Loss = -9928.132600082476
1
Iteration 9800: Loss = -9928.119268925759
2
Iteration 9900: Loss = -9928.147131334628
3
Iteration 10000: Loss = -9928.12828047174
4
Iteration 10100: Loss = -9928.11710262646
Iteration 10200: Loss = -9928.122689189999
1
Iteration 10300: Loss = -9928.117360543209
2
Iteration 10400: Loss = -9928.117425587023
3
Iteration 10500: Loss = -9928.117524120518
4
Iteration 10600: Loss = -9928.13074714885
5
Iteration 10700: Loss = -9928.125808403584
6
Iteration 10800: Loss = -9928.126558462747
7
Iteration 10900: Loss = -9928.11823063602
8
Iteration 11000: Loss = -9928.11979493327
9
Iteration 11100: Loss = -9928.155224933324
10
Iteration 11200: Loss = -9928.136524310168
11
Iteration 11300: Loss = -9928.124061164246
12
Iteration 11400: Loss = -9928.131445427261
13
Iteration 11500: Loss = -9928.117167955454
Iteration 11600: Loss = -9928.120357444108
1
Iteration 11700: Loss = -9928.135093676337
2
Iteration 11800: Loss = -9928.158982780875
3
Iteration 11900: Loss = -9928.12258823905
4
Iteration 12000: Loss = -9928.197722370303
5
Iteration 12100: Loss = -9928.205676983958
6
Iteration 12200: Loss = -9928.120862980752
7
Iteration 12300: Loss = -9928.120240524142
8
Iteration 12400: Loss = -9928.125318851135
9
Iteration 12500: Loss = -9928.119392837827
10
Iteration 12600: Loss = -9928.122600793717
11
Iteration 12700: Loss = -9928.120088416217
12
Iteration 12800: Loss = -9928.132808119866
13
Iteration 12900: Loss = -9928.120930147883
14
Iteration 13000: Loss = -9928.230013944232
15
Stopping early at iteration 13000 due to no improvement.
pi: tensor([[0.9659, 0.0341],
        [0.2688, 0.7312]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6009, 0.3991], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1652, 0.1094],
         [0.5794, 0.1568]],

        [[0.5786, 0.0916],
         [0.7278, 0.6858]],

        [[0.5199, 0.0898],
         [0.5521, 0.6617]],

        [[0.6674, 0.1032],
         [0.6874, 0.6841]],

        [[0.6125, 0.0915],
         [0.7024, 0.6034]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 34
Adjusted Rand Index: 0.09377345560160559
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 26
Adjusted Rand Index: 0.22342228449760465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 24
Adjusted Rand Index: 0.2638512204571871
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 21
Adjusted Rand Index: 0.31561972623288953
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 21
Adjusted Rand Index: 0.31554945158442005
Global Adjusted Rand Index: 0.242141157148657
Average Adjusted Rand Index: 0.24244322767474139
10040.330889248846
[0.1382116749758509, 0.242141157148657] [0.22620689531354737, 0.24244322767474139] [9932.102132329384, 9928.230013944232]
-------------------------------------
This iteration is 77
True Objective function: Loss = -10133.247553055035
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23272.37545242637
Iteration 100: Loss = -10017.042685548073
Iteration 200: Loss = -10015.856015047659
Iteration 300: Loss = -10014.570576634964
Iteration 400: Loss = -10013.21052513935
Iteration 500: Loss = -10011.565576492698
Iteration 600: Loss = -10008.868919697401
Iteration 700: Loss = -10003.91664978675
Iteration 800: Loss = -9994.757810290434
Iteration 900: Loss = -9992.728912517157
Iteration 1000: Loss = -9991.894459780046
Iteration 1100: Loss = -9991.590491269506
Iteration 1200: Loss = -9991.48166466556
Iteration 1300: Loss = -9991.430040951338
Iteration 1400: Loss = -9991.368070407076
Iteration 1500: Loss = -9991.222916845098
Iteration 1600: Loss = -9991.168774789117
Iteration 1700: Loss = -9991.136587571338
Iteration 1800: Loss = -9991.12599100499
Iteration 1900: Loss = -9991.11834305375
Iteration 2000: Loss = -9991.110934042981
Iteration 2100: Loss = -9991.10180817063
Iteration 2200: Loss = -9991.09428198023
Iteration 2300: Loss = -9991.088288452807
Iteration 2400: Loss = -9991.081071085744
Iteration 2500: Loss = -9991.065406277277
Iteration 2600: Loss = -9991.034289568868
Iteration 2700: Loss = -9990.892092024957
Iteration 2800: Loss = -9990.855456308367
Iteration 2900: Loss = -9990.850922508189
Iteration 3000: Loss = -9990.848161792594
Iteration 3100: Loss = -9990.845901294333
Iteration 3200: Loss = -9990.844503691484
Iteration 3300: Loss = -9990.843650590767
Iteration 3400: Loss = -9990.842840755851
Iteration 3500: Loss = -9990.84215570652
Iteration 3600: Loss = -9990.841621567492
Iteration 3700: Loss = -9990.84073253449
Iteration 3800: Loss = -9990.840000750928
Iteration 3900: Loss = -9990.839209865166
Iteration 4000: Loss = -9990.838065475962
Iteration 4100: Loss = -9990.836649257086
Iteration 4200: Loss = -9990.835404643805
Iteration 4300: Loss = -9990.834219803042
Iteration 4400: Loss = -9990.838280148128
1
Iteration 4500: Loss = -9990.831946461041
Iteration 4600: Loss = -9990.831339496608
Iteration 4700: Loss = -9990.831069555297
Iteration 4800: Loss = -9990.830853499405
Iteration 4900: Loss = -9990.832655588863
1
Iteration 5000: Loss = -9990.830720208169
Iteration 5100: Loss = -9990.83063719238
Iteration 5200: Loss = -9990.831007421893
1
Iteration 5300: Loss = -9990.830583322533
Iteration 5400: Loss = -9990.830589437439
Iteration 5500: Loss = -9990.830510054762
Iteration 5600: Loss = -9990.830462737042
Iteration 5700: Loss = -9990.831011220596
1
Iteration 5800: Loss = -9990.83045092474
Iteration 5900: Loss = -9990.830408939246
Iteration 6000: Loss = -9990.83038599727
Iteration 6100: Loss = -9990.83034897728
Iteration 6200: Loss = -9990.833781142306
1
Iteration 6300: Loss = -9990.83030129888
Iteration 6400: Loss = -9990.83028439871
Iteration 6500: Loss = -9990.830339026112
Iteration 6600: Loss = -9990.830282948267
Iteration 6700: Loss = -9990.8304789918
1
Iteration 6800: Loss = -9990.830259970846
Iteration 6900: Loss = -9990.85354123037
1
Iteration 7000: Loss = -9990.830242575885
Iteration 7100: Loss = -9990.8379775515
1
Iteration 7200: Loss = -9990.83022281736
Iteration 7300: Loss = -9990.841377291941
1
Iteration 7400: Loss = -9990.83021820378
Iteration 7500: Loss = -9990.858602917715
1
Iteration 7600: Loss = -9990.830192212148
Iteration 7700: Loss = -9990.852003309277
1
Iteration 7800: Loss = -9990.830188334914
Iteration 7900: Loss = -9990.830282251121
Iteration 8000: Loss = -9990.841179453324
1
Iteration 8100: Loss = -9990.832550469007
2
Iteration 8200: Loss = -9990.834027342173
3
Iteration 8300: Loss = -9990.830992597263
4
Iteration 8400: Loss = -9990.830203252755
Iteration 8500: Loss = -9990.830208477017
Iteration 8600: Loss = -9990.834730104492
1
Iteration 8700: Loss = -9990.830116711086
Iteration 8800: Loss = -9990.832735887958
1
Iteration 8900: Loss = -9990.830143244846
Iteration 9000: Loss = -9990.830137121453
Iteration 9100: Loss = -9990.830132432948
Iteration 9200: Loss = -9990.830106179888
Iteration 9300: Loss = -9990.830326843401
1
Iteration 9400: Loss = -9990.830086363303
Iteration 9500: Loss = -9991.021382949408
1
Iteration 9600: Loss = -9990.830103559696
Iteration 9700: Loss = -9990.830104425371
Iteration 9800: Loss = -9990.830248699127
1
Iteration 9900: Loss = -9990.830078257039
Iteration 10000: Loss = -9990.831940336933
1
Iteration 10100: Loss = -9990.830103558452
Iteration 10200: Loss = -9990.914924468087
1
Iteration 10300: Loss = -9990.830085266154
Iteration 10400: Loss = -9990.830107335576
Iteration 10500: Loss = -9990.830124328058
Iteration 10600: Loss = -9990.8300401217
Iteration 10700: Loss = -9990.838489776841
1
Iteration 10800: Loss = -9990.830038471833
Iteration 10900: Loss = -9990.83002611653
Iteration 11000: Loss = -9990.830100411065
Iteration 11100: Loss = -9990.829973647818
Iteration 11200: Loss = -9990.914533044333
1
Iteration 11300: Loss = -9990.830006410642
Iteration 11400: Loss = -9990.829952358468
Iteration 11500: Loss = -9990.831849995653
1
Iteration 11600: Loss = -9990.833270341423
2
Iteration 11700: Loss = -9990.829919969165
Iteration 11800: Loss = -9990.832316254096
1
Iteration 11900: Loss = -9990.829891422509
Iteration 12000: Loss = -9990.85906624231
1
Iteration 12100: Loss = -9990.829829686416
Iteration 12200: Loss = -9990.82983858701
Iteration 12300: Loss = -9990.838210940925
1
Iteration 12400: Loss = -9991.000692774454
2
Iteration 12500: Loss = -9990.830303024559
3
Iteration 12600: Loss = -9990.829732056165
Iteration 12700: Loss = -9990.838070506277
1
Iteration 12800: Loss = -9990.829654713409
Iteration 12900: Loss = -9990.838201655408
1
Iteration 13000: Loss = -9990.876037197224
2
Iteration 13100: Loss = -9990.829639643443
Iteration 13200: Loss = -9990.829654205338
Iteration 13300: Loss = -9990.843368585804
1
Iteration 13400: Loss = -9990.829492352423
Iteration 13500: Loss = -9991.004488322995
1
Iteration 13600: Loss = -9990.829525983736
Iteration 13700: Loss = -9990.830180968474
1
Iteration 13800: Loss = -9990.829493139201
Iteration 13900: Loss = -9990.831217984574
1
Iteration 14000: Loss = -9990.829536671556
Iteration 14100: Loss = -9990.91298064629
1
Iteration 14200: Loss = -9990.829535064284
Iteration 14300: Loss = -9990.829486025743
Iteration 14400: Loss = -9990.829641800572
1
Iteration 14500: Loss = -9990.829503804096
Iteration 14600: Loss = -9990.842773638758
1
Iteration 14700: Loss = -9990.829537931828
Iteration 14800: Loss = -9990.82991258855
1
Iteration 14900: Loss = -9990.829572649323
Iteration 15000: Loss = -9990.829496039687
Iteration 15100: Loss = -9990.84080651911
1
Iteration 15200: Loss = -9990.830779858079
2
Iteration 15300: Loss = -9990.830153796916
3
Iteration 15400: Loss = -9990.82950513034
Iteration 15500: Loss = -9990.829995352065
1
Iteration 15600: Loss = -9990.829625106602
2
Iteration 15700: Loss = -9990.844086026304
3
Iteration 15800: Loss = -9990.829497197536
Iteration 15900: Loss = -9990.829577386126
Iteration 16000: Loss = -9990.829501084436
Iteration 16100: Loss = -9990.830098464663
1
Iteration 16200: Loss = -9990.912546919137
2
Iteration 16300: Loss = -9990.82953709161
Iteration 16400: Loss = -9990.945827048592
1
Iteration 16500: Loss = -9990.829519894642
Iteration 16600: Loss = -9990.829499631405
Iteration 16700: Loss = -9990.831013580026
1
Iteration 16800: Loss = -9990.829929558175
2
Iteration 16900: Loss = -9990.829731445096
3
Iteration 17000: Loss = -9990.829518974511
Iteration 17100: Loss = -9990.865899111452
1
Iteration 17200: Loss = -9990.831127271342
2
Iteration 17300: Loss = -9990.829573833806
Iteration 17400: Loss = -9990.835403624103
1
Iteration 17500: Loss = -9990.829530347344
Iteration 17600: Loss = -9990.844656625259
1
Iteration 17700: Loss = -9990.831354975611
2
Iteration 17800: Loss = -9990.829743031656
3
Iteration 17900: Loss = -9990.836216542786
4
Iteration 18000: Loss = -9990.829480829621
Iteration 18100: Loss = -9990.833818717578
1
Iteration 18200: Loss = -9990.896227531266
2
Iteration 18300: Loss = -9990.829473595071
Iteration 18400: Loss = -9990.829626885978
1
Iteration 18500: Loss = -9990.829530329665
Iteration 18600: Loss = -9990.829574475027
Iteration 18700: Loss = -9990.830024030533
1
Iteration 18800: Loss = -9990.82952740252
Iteration 18900: Loss = -9990.895342650476
1
Iteration 19000: Loss = -9990.82954438784
Iteration 19100: Loss = -9990.829496927034
Iteration 19200: Loss = -9990.853893768608
1
Iteration 19300: Loss = -9990.82952189442
Iteration 19400: Loss = -9990.829494173548
Iteration 19500: Loss = -9990.830200557088
1
Iteration 19600: Loss = -9990.829530562623
Iteration 19700: Loss = -9990.836055652237
1
Iteration 19800: Loss = -9990.82950817715
Iteration 19900: Loss = -9990.881457367948
1
pi: tensor([[0.9969, 0.0031],
        [0.2270, 0.7730]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3592, 0.6408], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1290, 0.1005],
         [0.6289, 0.2043]],

        [[0.5151, 0.1255],
         [0.5101, 0.6540]],

        [[0.6304, 0.1244],
         [0.5454, 0.5381]],

        [[0.5959, 0.1193],
         [0.7137, 0.6918]],

        [[0.6842, 0.1372],
         [0.7082, 0.7134]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 77
Adjusted Rand Index: 0.2831074738956079
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 79
Adjusted Rand Index: 0.3298282552545568
time is 2
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 79
Adjusted Rand Index: 0.3299962257445343
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.12223872153522168
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.013080348971996827
Global Adjusted Rand Index: 0.19180685425529617
Average Adjusted Rand Index: 0.21565020508038352
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22738.36918497106
Iteration 100: Loss = -10017.598647170773
Iteration 200: Loss = -10015.792097258916
Iteration 300: Loss = -10014.69737412772
Iteration 400: Loss = -10013.115603742079
Iteration 500: Loss = -10010.36254266967
Iteration 600: Loss = -10004.956174867933
Iteration 700: Loss = -9993.729019831813
Iteration 800: Loss = -9991.826990702924
Iteration 900: Loss = -9991.459509421027
Iteration 1000: Loss = -9991.26077500217
Iteration 1100: Loss = -9991.202474644168
Iteration 1200: Loss = -9991.171243375631
Iteration 1300: Loss = -9991.147086054398
Iteration 1400: Loss = -9991.126654961865
Iteration 1500: Loss = -9991.111187291179
Iteration 1600: Loss = -9991.098197212154
Iteration 1700: Loss = -9991.083021069457
Iteration 1800: Loss = -9991.05122441669
Iteration 1900: Loss = -9990.926625247035
Iteration 2000: Loss = -9990.860102714669
Iteration 2100: Loss = -9990.850067354413
Iteration 2200: Loss = -9990.84536048872
Iteration 2300: Loss = -9990.841913482069
Iteration 2400: Loss = -9990.839169079716
Iteration 2500: Loss = -9990.836849894908
Iteration 2600: Loss = -9990.837209507188
1
Iteration 2700: Loss = -9990.834504836694
Iteration 2800: Loss = -9990.833891653967
Iteration 2900: Loss = -9990.833791734385
Iteration 3000: Loss = -9990.832939827953
Iteration 3100: Loss = -9990.832601754408
Iteration 3200: Loss = -9990.83233128284
Iteration 3300: Loss = -9990.832023952511
Iteration 3400: Loss = -9990.83182525502
Iteration 3500: Loss = -9990.831655095984
Iteration 3600: Loss = -9990.831482286452
Iteration 3700: Loss = -9990.831502688052
Iteration 3800: Loss = -9990.831251541204
Iteration 3900: Loss = -9990.83112749174
Iteration 4000: Loss = -9990.831106090438
Iteration 4100: Loss = -9990.83097195801
Iteration 4200: Loss = -9990.830866918559
Iteration 4300: Loss = -9990.830828570848
Iteration 4400: Loss = -9990.83077137115
Iteration 4500: Loss = -9990.83799430551
1
Iteration 4600: Loss = -9990.830630688404
Iteration 4700: Loss = -9990.830594210905
Iteration 4800: Loss = -9990.83054000101
Iteration 4900: Loss = -9990.830468052209
Iteration 5000: Loss = -9990.837556035498
1
Iteration 5100: Loss = -9990.830386939095
Iteration 5200: Loss = -9990.83042707675
Iteration 5300: Loss = -9990.831154546115
1
Iteration 5400: Loss = -9990.83029868793
Iteration 5500: Loss = -9990.837534346541
1
Iteration 5600: Loss = -9990.830223540715
Iteration 5700: Loss = -9990.83018795001
Iteration 5800: Loss = -9990.830148322515
Iteration 5900: Loss = -9990.830124456184
Iteration 6000: Loss = -9990.83032799571
1
Iteration 6100: Loss = -9990.830092068309
Iteration 6200: Loss = -9990.830520321371
1
Iteration 6300: Loss = -9990.830059399483
Iteration 6400: Loss = -9990.832694022642
1
Iteration 6500: Loss = -9990.830014858717
Iteration 6600: Loss = -9990.83142778655
1
Iteration 6700: Loss = -9990.829992560626
Iteration 6800: Loss = -9990.83253021615
1
Iteration 6900: Loss = -9990.8299769205
Iteration 7000: Loss = -9990.829935429627
Iteration 7100: Loss = -9990.857662534188
1
Iteration 7200: Loss = -9990.829940569818
Iteration 7300: Loss = -9990.829949557145
Iteration 7400: Loss = -9990.829916155824
Iteration 7500: Loss = -9990.829939477237
Iteration 7600: Loss = -9990.829914549355
Iteration 7700: Loss = -9990.829889776396
Iteration 7800: Loss = -9990.829917648844
Iteration 7900: Loss = -9990.829840836554
Iteration 8000: Loss = -9990.829910426366
Iteration 8100: Loss = -9990.829852591409
Iteration 8200: Loss = -9990.9934142567
1
Iteration 8300: Loss = -9990.829867735509
Iteration 8400: Loss = -9990.829887416256
Iteration 8500: Loss = -9990.829829586253
Iteration 8600: Loss = -9990.82984015683
Iteration 8700: Loss = -9990.840686380063
1
Iteration 8800: Loss = -9990.829862089633
Iteration 8900: Loss = -9990.829835555469
Iteration 9000: Loss = -9990.829867555409
Iteration 9100: Loss = -9990.829760229484
Iteration 9200: Loss = -9990.835789672972
1
Iteration 9300: Loss = -9990.829784905573
Iteration 9400: Loss = -9990.829817628533
Iteration 9500: Loss = -9990.829819304714
Iteration 9600: Loss = -9990.829772035659
Iteration 9700: Loss = -9990.89196746755
1
Iteration 9800: Loss = -9990.829777248
Iteration 9900: Loss = -9990.829755273766
Iteration 10000: Loss = -9990.8313673455
1
Iteration 10100: Loss = -9990.829770255566
Iteration 10200: Loss = -9990.829750866878
Iteration 10300: Loss = -9990.863196379813
1
Iteration 10400: Loss = -9990.829715456528
Iteration 10500: Loss = -9990.829706589
Iteration 10600: Loss = -9990.837495745343
1
Iteration 10700: Loss = -9990.829657059374
Iteration 10800: Loss = -9990.829682682559
Iteration 10900: Loss = -9990.82972334231
Iteration 11000: Loss = -9990.829630865237
Iteration 11100: Loss = -9990.829607673639
Iteration 11200: Loss = -9990.829632809815
Iteration 11300: Loss = -9990.829609433518
Iteration 11400: Loss = -9990.829602304975
Iteration 11500: Loss = -9990.83113857916
1
Iteration 11600: Loss = -9990.829593493756
Iteration 11700: Loss = -9990.829583487228
Iteration 11800: Loss = -9990.82999730807
1
Iteration 11900: Loss = -9990.82959708282
Iteration 12000: Loss = -9990.859858082393
1
Iteration 12100: Loss = -9990.829562688967
Iteration 12200: Loss = -9990.872580634168
1
Iteration 12300: Loss = -9990.829583665096
Iteration 12400: Loss = -9990.837069488653
1
Iteration 12500: Loss = -9990.836805662324
2
Iteration 12600: Loss = -9990.835313208112
3
Iteration 12700: Loss = -9990.837537178804
4
Iteration 12800: Loss = -9990.829574762198
Iteration 12900: Loss = -9990.830019844107
1
Iteration 13000: Loss = -9990.829770402863
2
Iteration 13100: Loss = -9990.829609099674
Iteration 13200: Loss = -9990.82953769276
Iteration 13300: Loss = -9990.829595001298
Iteration 13400: Loss = -9990.829503273488
Iteration 13500: Loss = -9990.831951880864
1
Iteration 13600: Loss = -9990.829525974143
Iteration 13700: Loss = -9990.830576037817
1
Iteration 13800: Loss = -9990.829524691671
Iteration 13900: Loss = -9990.838300695534
1
Iteration 14000: Loss = -9990.829568002815
Iteration 14100: Loss = -9990.829526854068
Iteration 14200: Loss = -9990.84899060983
1
Iteration 14300: Loss = -9990.83747780822
2
Iteration 14400: Loss = -9990.829537129666
Iteration 14500: Loss = -9990.833763864908
1
Iteration 14600: Loss = -9990.829513294982
Iteration 14700: Loss = -9990.829796878925
1
Iteration 14800: Loss = -9990.829477755518
Iteration 14900: Loss = -9990.830708211859
1
Iteration 15000: Loss = -9990.845648468026
2
Iteration 15100: Loss = -9990.829525721805
Iteration 15200: Loss = -9990.829819809182
1
Iteration 15300: Loss = -9990.829532426536
Iteration 15400: Loss = -9990.82952548785
Iteration 15500: Loss = -9990.840822299324
1
Iteration 15600: Loss = -9990.829505260303
Iteration 15700: Loss = -9990.858270431832
1
Iteration 15800: Loss = -9990.832377702536
2
Iteration 15900: Loss = -9990.878248647232
3
Iteration 16000: Loss = -9990.829511800548
Iteration 16100: Loss = -9990.830222170045
1
Iteration 16200: Loss = -9990.829670969932
2
Iteration 16300: Loss = -9990.829527526279
Iteration 16400: Loss = -9990.831490476814
1
Iteration 16500: Loss = -9990.829536488081
Iteration 16600: Loss = -9990.829512077102
Iteration 16700: Loss = -9990.829511159935
Iteration 16800: Loss = -9990.831326126125
1
Iteration 16900: Loss = -9990.829523166294
Iteration 17000: Loss = -9990.83089775577
1
Iteration 17100: Loss = -9990.829495499436
Iteration 17200: Loss = -9990.830526157877
1
Iteration 17300: Loss = -9990.829522997465
Iteration 17400: Loss = -9990.83990776346
1
Iteration 17500: Loss = -9990.831522776085
2
Iteration 17600: Loss = -9990.856606820931
3
Iteration 17700: Loss = -9990.829507565993
Iteration 17800: Loss = -9990.830177774049
1
Iteration 17900: Loss = -9990.829475964116
Iteration 18000: Loss = -9990.83182512209
1
Iteration 18100: Loss = -9990.831730255939
2
Iteration 18200: Loss = -9990.82953570985
Iteration 18300: Loss = -9990.829907513018
1
Iteration 18400: Loss = -9990.837438754666
2
Iteration 18500: Loss = -9990.829502529832
Iteration 18600: Loss = -9990.830954561261
1
Iteration 18700: Loss = -9990.829649761925
2
Iteration 18800: Loss = -9990.830276093035
3
Iteration 18900: Loss = -9990.829524848756
Iteration 19000: Loss = -9990.831049451108
1
Iteration 19100: Loss = -9990.829537234455
Iteration 19200: Loss = -9990.829481301937
Iteration 19300: Loss = -9990.829693308884
1
Iteration 19400: Loss = -9990.82948927146
Iteration 19500: Loss = -9990.831560851739
1
Iteration 19600: Loss = -9990.829492278315
Iteration 19700: Loss = -9991.218310296834
1
Iteration 19800: Loss = -9990.829506836144
Iteration 19900: Loss = -9990.829529229317
pi: tensor([[0.7734, 0.2266],
        [0.0031, 0.9969]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6409, 0.3591], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2041, 0.1006],
         [0.7006, 0.1288]],

        [[0.6407, 0.1256],
         [0.5005, 0.6583]],

        [[0.6881, 0.1244],
         [0.7004, 0.6041]],

        [[0.6470, 0.1195],
         [0.5899, 0.6102]],

        [[0.5732, 0.1373],
         [0.6226, 0.7246]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 22
Adjusted Rand Index: 0.3050984480679263
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 21
Adjusted Rand Index: 0.3298282552545568
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 21
Adjusted Rand Index: 0.3299962257445343
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 32
Adjusted Rand Index: 0.12223872153522168
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.013080348971996827
Global Adjusted Rand Index: 0.19535806907185357
Average Adjusted Rand Index: 0.22004839991484718
10133.247553055035
[0.19180685425529617, 0.19535806907185357] [0.21565020508038352, 0.22004839991484718] [9990.829524969362, 9990.835122504372]
-------------------------------------
This iteration is 78
True Objective function: Loss = -10043.40895267735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25775.678888496197
Iteration 100: Loss = -9928.972948994615
Iteration 200: Loss = -9924.689527972001
Iteration 300: Loss = -9924.084637548614
Iteration 400: Loss = -9923.786269068818
Iteration 500: Loss = -9923.580062772338
Iteration 600: Loss = -9923.429903152399
Iteration 700: Loss = -9923.319576273792
Iteration 800: Loss = -9923.242028420726
Iteration 900: Loss = -9923.190068495998
Iteration 1000: Loss = -9923.153618928945
Iteration 1100: Loss = -9923.125741314348
Iteration 1200: Loss = -9923.102820009013
Iteration 1300: Loss = -9923.082984587732
Iteration 1400: Loss = -9923.065243320176
Iteration 1500: Loss = -9923.048780909841
Iteration 1600: Loss = -9923.032841402883
Iteration 1700: Loss = -9923.0167873699
Iteration 1800: Loss = -9922.999355723716
Iteration 1900: Loss = -9922.965356168883
Iteration 2000: Loss = -9922.933185807147
Iteration 2100: Loss = -9922.913937160545
Iteration 2200: Loss = -9922.890551399641
Iteration 2300: Loss = -9922.853537166506
Iteration 2400: Loss = -9922.806489221515
Iteration 2500: Loss = -9922.652535431427
Iteration 2600: Loss = -9921.345421976293
Iteration 2700: Loss = -9920.874330523036
Iteration 2800: Loss = -9920.717955985434
Iteration 2900: Loss = -9920.61959230003
Iteration 3000: Loss = -9920.553966031768
Iteration 3100: Loss = -9920.516424023186
Iteration 3200: Loss = -9920.488763100204
Iteration 3300: Loss = -9920.465820978896
Iteration 3400: Loss = -9920.44348112529
Iteration 3500: Loss = -9920.419765286833
Iteration 3600: Loss = -9920.376369601896
Iteration 3700: Loss = -9920.258547668094
Iteration 3800: Loss = -9919.909119300071
Iteration 3900: Loss = -9919.091055001341
Iteration 4000: Loss = -9918.502948734653
Iteration 4100: Loss = -9918.30275905506
Iteration 4200: Loss = -9918.25417941818
Iteration 4300: Loss = -9918.233541978987
Iteration 4400: Loss = -9918.220996361368
Iteration 4500: Loss = -9918.212594619752
Iteration 4600: Loss = -9918.207701458052
Iteration 4700: Loss = -9918.204648992956
Iteration 4800: Loss = -9918.202646973916
Iteration 4900: Loss = -9918.201032925595
Iteration 5000: Loss = -9918.199736623126
Iteration 5100: Loss = -9918.198557302723
Iteration 5200: Loss = -9918.197523537185
Iteration 5300: Loss = -9918.196593103303
Iteration 5400: Loss = -9918.195734368664
Iteration 5500: Loss = -9918.194964381111
Iteration 5600: Loss = -9918.194277698061
Iteration 5700: Loss = -9918.193620170496
Iteration 5800: Loss = -9918.194016631514
1
Iteration 5900: Loss = -9918.192482918637
Iteration 6000: Loss = -9918.19196403423
Iteration 6100: Loss = -9918.192088043477
1
Iteration 6200: Loss = -9918.191004878398
Iteration 6300: Loss = -9918.190599677488
Iteration 6400: Loss = -9918.191663595762
1
Iteration 6500: Loss = -9918.189844095936
Iteration 6600: Loss = -9918.189503530615
Iteration 6700: Loss = -9918.189680785816
1
Iteration 6800: Loss = -9918.188897262775
Iteration 6900: Loss = -9918.188579382058
Iteration 7000: Loss = -9918.18833894817
Iteration 7100: Loss = -9918.188077139064
Iteration 7200: Loss = -9918.188853308595
1
Iteration 7300: Loss = -9918.18758821138
Iteration 7400: Loss = -9918.187381044458
Iteration 7500: Loss = -9918.187201360333
Iteration 7600: Loss = -9918.187011684637
Iteration 7700: Loss = -9918.186826327783
Iteration 7800: Loss = -9918.18665271762
Iteration 7900: Loss = -9918.186514216142
Iteration 8000: Loss = -9918.18689368538
1
Iteration 8100: Loss = -9918.186226407654
Iteration 8200: Loss = -9918.186071507937
Iteration 8300: Loss = -9918.186009023544
Iteration 8400: Loss = -9918.185851890159
Iteration 8500: Loss = -9918.185747422602
Iteration 8600: Loss = -9918.185635170235
Iteration 8700: Loss = -9918.18552614367
Iteration 8800: Loss = -9918.195098143167
1
Iteration 8900: Loss = -9918.18684837802
2
Iteration 9000: Loss = -9918.18761384895
3
Iteration 9100: Loss = -9918.253409690033
4
Iteration 9200: Loss = -9918.185087134032
Iteration 9300: Loss = -9918.21685711874
1
Iteration 9400: Loss = -9918.184931419111
Iteration 9500: Loss = -9918.184870362154
Iteration 9600: Loss = -9918.18484880634
Iteration 9700: Loss = -9918.184749112923
Iteration 9800: Loss = -9918.184712958911
Iteration 9900: Loss = -9918.184644086576
Iteration 10000: Loss = -9918.184575061898
Iteration 10100: Loss = -9918.18459988416
Iteration 10200: Loss = -9918.184552758905
Iteration 10300: Loss = -9918.184420760083
Iteration 10400: Loss = -9918.184444713437
Iteration 10500: Loss = -9918.184401476998
Iteration 10600: Loss = -9918.184331836614
Iteration 10700: Loss = -9918.184282765345
Iteration 10800: Loss = -9918.18457243204
1
Iteration 10900: Loss = -9918.184261849432
Iteration 11000: Loss = -9918.184195799118
Iteration 11100: Loss = -9918.184442421141
1
Iteration 11200: Loss = -9918.184128469828
Iteration 11300: Loss = -9918.184110194008
Iteration 11400: Loss = -9918.19257445013
1
Iteration 11500: Loss = -9918.184068322209
Iteration 11600: Loss = -9918.184041081939
Iteration 11700: Loss = -9918.230538193384
1
Iteration 11800: Loss = -9918.184009708317
Iteration 11900: Loss = -9918.183971177881
Iteration 12000: Loss = -9918.254858437012
1
Iteration 12100: Loss = -9918.183953435895
Iteration 12200: Loss = -9918.183883685615
Iteration 12300: Loss = -9918.192820522281
1
Iteration 12400: Loss = -9918.183890433258
Iteration 12500: Loss = -9918.183925221192
Iteration 12600: Loss = -9918.183888883297
Iteration 12700: Loss = -9918.183854779578
Iteration 12800: Loss = -9918.18382169666
Iteration 12900: Loss = -9918.187354265303
1
Iteration 13000: Loss = -9918.198777989479
2
Iteration 13100: Loss = -9918.184441385809
3
Iteration 13200: Loss = -9918.183792700318
Iteration 13300: Loss = -9918.186445245376
1
Iteration 13400: Loss = -9918.185104436257
2
Iteration 13500: Loss = -9918.183891130895
Iteration 13600: Loss = -9918.185175306984
1
Iteration 13700: Loss = -9918.191287451338
2
Iteration 13800: Loss = -9918.183791787895
Iteration 13900: Loss = -9918.183901078415
1
Iteration 14000: Loss = -9918.185783072495
2
Iteration 14100: Loss = -9918.18910618843
3
Iteration 14200: Loss = -9918.18386782859
Iteration 14300: Loss = -9918.183708850423
Iteration 14400: Loss = -9918.189544245417
1
Iteration 14500: Loss = -9918.188831635123
2
Iteration 14600: Loss = -9918.200322575722
3
Iteration 14700: Loss = -9918.183792146297
Iteration 14800: Loss = -9918.183798207674
Iteration 14900: Loss = -9918.185531110896
1
Iteration 15000: Loss = -9918.216397830069
2
Iteration 15100: Loss = -9918.184352226472
3
Iteration 15200: Loss = -9918.183957212876
4
Iteration 15300: Loss = -9918.191393762763
5
Iteration 15400: Loss = -9918.185863455514
6
Iteration 15500: Loss = -9918.184457981488
7
Iteration 15600: Loss = -9918.19848185969
8
Iteration 15700: Loss = -9918.184957623289
9
Iteration 15800: Loss = -9918.18370544359
Iteration 15900: Loss = -9918.18365409539
Iteration 16000: Loss = -9918.183824039705
1
Iteration 16100: Loss = -9918.186211348155
2
Iteration 16200: Loss = -9918.185112772704
3
Iteration 16300: Loss = -9918.186166069196
4
Iteration 16400: Loss = -9918.183653487196
Iteration 16500: Loss = -9918.187521200625
1
Iteration 16600: Loss = -9918.183982827431
2
Iteration 16700: Loss = -9918.18366920262
Iteration 16800: Loss = -9918.279182297736
1
Iteration 16900: Loss = -9918.184836967393
2
Iteration 17000: Loss = -9918.183941041521
3
Iteration 17100: Loss = -9918.183914005998
4
Iteration 17200: Loss = -9918.26784794371
5
Iteration 17300: Loss = -9918.18387354437
6
Iteration 17400: Loss = -9918.183658403732
Iteration 17500: Loss = -9918.184336744753
1
Iteration 17600: Loss = -9918.237298172126
2
Iteration 17700: Loss = -9918.183655452942
Iteration 17800: Loss = -9918.184975280185
1
Iteration 17900: Loss = -9918.183884775397
2
Iteration 18000: Loss = -9918.183744677583
Iteration 18100: Loss = -9918.19248757058
1
Iteration 18200: Loss = -9918.18419027146
2
Iteration 18300: Loss = -9918.197300148146
3
Iteration 18400: Loss = -9918.18368707809
Iteration 18500: Loss = -9918.18482886925
1
Iteration 18600: Loss = -9918.184045852202
2
Iteration 18700: Loss = -9918.185526889478
3
Iteration 18800: Loss = -9918.184858253297
4
Iteration 18900: Loss = -9918.184133998873
5
Iteration 19000: Loss = -9918.18914062722
6
Iteration 19100: Loss = -9918.186268103547
7
Iteration 19200: Loss = -9918.298502528945
8
Iteration 19300: Loss = -9918.183690288224
Iteration 19400: Loss = -9918.183748094147
Iteration 19500: Loss = -9918.185269657282
1
Iteration 19600: Loss = -9918.192983294137
2
Iteration 19700: Loss = -9918.186053254447
3
Iteration 19800: Loss = -9918.184179645736
4
Iteration 19900: Loss = -9918.187211511631
5
pi: tensor([[1.0000e+00, 1.6163e-08],
        [4.8630e-01, 5.1370e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8936, 0.1064], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1387, 0.1017],
         [0.6084, 0.0688]],

        [[0.6673, 0.1765],
         [0.7151, 0.6948]],

        [[0.6119, 0.0716],
         [0.5758, 0.5829]],

        [[0.5604, 0.2815],
         [0.6405, 0.6363]],

        [[0.6927, 0.2230],
         [0.6674, 0.7112]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0009975514204148314
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.005431979218977636
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 3.863438608745782e-05
Average Adjusted Rand Index: 0.0014950762130082284
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21672.53693461973
Iteration 100: Loss = -9928.785528898316
Iteration 200: Loss = -9926.738077631955
Iteration 300: Loss = -9924.736974606774
Iteration 400: Loss = -9924.12412022129
Iteration 500: Loss = -9923.794086745198
Iteration 600: Loss = -9923.538655496206
Iteration 700: Loss = -9923.354230979781
Iteration 800: Loss = -9923.234899277724
Iteration 900: Loss = -9923.159119834874
Iteration 1000: Loss = -9923.113670510093
Iteration 1100: Loss = -9923.082743982639
Iteration 1200: Loss = -9923.057852212078
Iteration 1300: Loss = -9923.034125931334
Iteration 1400: Loss = -9923.007730556328
Iteration 1500: Loss = -9922.978619390922
Iteration 1600: Loss = -9922.945327689407
Iteration 1700: Loss = -9922.891282259501
Iteration 1800: Loss = -9922.82875930536
Iteration 1900: Loss = -9922.762524584112
Iteration 2000: Loss = -9922.689694549052
Iteration 2100: Loss = -9922.606685081386
Iteration 2200: Loss = -9922.527997642608
Iteration 2300: Loss = -9922.345861813446
Iteration 2400: Loss = -9921.194343468032
Iteration 2500: Loss = -9920.70979199963
Iteration 2600: Loss = -9920.446022302198
Iteration 2700: Loss = -9920.254248321375
Iteration 2800: Loss = -9919.167325415867
Iteration 2900: Loss = -9918.54976175893
Iteration 3000: Loss = -9918.35927173925
Iteration 3100: Loss = -9918.306643233873
Iteration 3200: Loss = -9918.28115899546
Iteration 3300: Loss = -9918.264265233296
Iteration 3400: Loss = -9918.25244679033
Iteration 3500: Loss = -9918.24396184109
Iteration 3600: Loss = -9918.237361414283
Iteration 3700: Loss = -9918.231815261928
Iteration 3800: Loss = -9918.227250189686
Iteration 3900: Loss = -9918.223250834175
Iteration 4000: Loss = -9918.220195991886
Iteration 4100: Loss = -9918.21672282962
Iteration 4200: Loss = -9918.214068367071
Iteration 4300: Loss = -9918.211746692234
Iteration 4400: Loss = -9918.209494700368
Iteration 4500: Loss = -9918.207623580545
Iteration 4600: Loss = -9918.20589750207
Iteration 4700: Loss = -9918.204269782591
Iteration 4800: Loss = -9918.202855102225
Iteration 4900: Loss = -9918.20153200709
Iteration 5000: Loss = -9918.200336347685
Iteration 5100: Loss = -9918.199205712584
Iteration 5200: Loss = -9918.198228080071
Iteration 5300: Loss = -9918.197317873735
Iteration 5400: Loss = -9918.19645960481
Iteration 5500: Loss = -9918.195638061643
Iteration 5600: Loss = -9918.194900998353
Iteration 5700: Loss = -9918.194251831708
Iteration 5800: Loss = -9918.19363731701
Iteration 5900: Loss = -9918.193044475622
Iteration 6000: Loss = -9918.19249582966
Iteration 6100: Loss = -9918.191968052037
Iteration 6200: Loss = -9918.191489032275
Iteration 6300: Loss = -9918.191627434067
1
Iteration 6400: Loss = -9918.190641054844
Iteration 6500: Loss = -9918.190256528962
Iteration 6600: Loss = -9918.189879982641
Iteration 6700: Loss = -9918.189539886755
Iteration 6800: Loss = -9918.189202507783
Iteration 6900: Loss = -9918.188892934488
Iteration 7000: Loss = -9918.18863261879
Iteration 7100: Loss = -9918.188374338473
Iteration 7200: Loss = -9918.18810439274
Iteration 7300: Loss = -9918.187853982015
Iteration 7400: Loss = -9918.18762540766
Iteration 7500: Loss = -9918.187449067573
Iteration 7600: Loss = -9918.187239018347
Iteration 7700: Loss = -9918.187046140181
Iteration 7800: Loss = -9918.186835548766
Iteration 7900: Loss = -9918.267937501483
1
Iteration 8000: Loss = -9918.186561061128
Iteration 8100: Loss = -9918.188305174132
1
Iteration 8200: Loss = -9918.186220207133
Iteration 8300: Loss = -9918.512992655767
1
Iteration 8400: Loss = -9918.185977404444
Iteration 8500: Loss = -9918.185850453357
Iteration 8600: Loss = -9918.187244245511
1
Iteration 8700: Loss = -9918.185597390711
Iteration 8800: Loss = -9918.185533156467
Iteration 8900: Loss = -9918.186991097573
1
Iteration 9000: Loss = -9918.185318567186
Iteration 9100: Loss = -9918.185273909048
Iteration 9200: Loss = -9918.186593681427
1
Iteration 9300: Loss = -9918.185095381075
Iteration 9400: Loss = -9918.185010110095
Iteration 9500: Loss = -9918.184946131594
Iteration 9600: Loss = -9918.222349937725
1
Iteration 9700: Loss = -9918.18483781067
Iteration 9800: Loss = -9918.184735076538
Iteration 9900: Loss = -9918.243585306513
1
Iteration 10000: Loss = -9918.184627449795
Iteration 10100: Loss = -9918.184574793102
Iteration 10200: Loss = -9918.255955029988
1
Iteration 10300: Loss = -9918.18449425598
Iteration 10400: Loss = -9918.184459424243
Iteration 10500: Loss = -9918.433263843854
1
Iteration 10600: Loss = -9918.184362025499
Iteration 10700: Loss = -9918.184344123429
Iteration 10800: Loss = -9918.184645252977
1
Iteration 10900: Loss = -9918.184270341393
Iteration 11000: Loss = -9918.184197662626
Iteration 11100: Loss = -9918.184204750485
Iteration 11200: Loss = -9918.184331807985
1
Iteration 11300: Loss = -9918.184136011409
Iteration 11400: Loss = -9918.184113110288
Iteration 11500: Loss = -9918.184148283104
Iteration 11600: Loss = -9918.184043684487
Iteration 11700: Loss = -9918.184045451837
Iteration 11800: Loss = -9918.184422630191
1
Iteration 11900: Loss = -9918.184005103562
Iteration 12000: Loss = -9918.183953554708
Iteration 12100: Loss = -9918.184416761309
1
Iteration 12200: Loss = -9918.183945760717
Iteration 12300: Loss = -9918.183950298404
Iteration 12400: Loss = -9918.183903217827
Iteration 12500: Loss = -9918.183899455398
Iteration 12600: Loss = -9918.183887196368
Iteration 12700: Loss = -9918.186636675802
1
Iteration 12800: Loss = -9918.183838675086
Iteration 12900: Loss = -9918.183882955615
Iteration 13000: Loss = -9918.18381034185
Iteration 13100: Loss = -9918.185752036696
1
Iteration 13200: Loss = -9918.183802495
Iteration 13300: Loss = -9918.183847018716
Iteration 13400: Loss = -9918.183855663061
Iteration 13500: Loss = -9918.183949595383
Iteration 13600: Loss = -9918.184165381288
1
Iteration 13700: Loss = -9918.183801304745
Iteration 13800: Loss = -9918.18388780689
Iteration 13900: Loss = -9918.18373951652
Iteration 14000: Loss = -9918.192394315007
1
Iteration 14100: Loss = -9918.183736025107
Iteration 14200: Loss = -9918.18372312929
Iteration 14300: Loss = -9918.184171195906
1
Iteration 14400: Loss = -9918.18385067467
2
Iteration 14500: Loss = -9918.183725263263
Iteration 14600: Loss = -9918.185717822842
1
Iteration 14700: Loss = -9918.183710782423
Iteration 14800: Loss = -9918.184632630739
1
Iteration 14900: Loss = -9918.18377701226
Iteration 15000: Loss = -9918.18387664142
Iteration 15100: Loss = -9918.195752472293
1
Iteration 15200: Loss = -9918.183691657789
Iteration 15300: Loss = -9918.185563545563
1
Iteration 15400: Loss = -9918.188072715795
2
Iteration 15500: Loss = -9918.183770489231
Iteration 15600: Loss = -9918.183893958292
1
Iteration 15700: Loss = -9918.18393448942
2
Iteration 15800: Loss = -9918.183711960182
Iteration 15900: Loss = -9918.184171426696
1
Iteration 16000: Loss = -9918.203684917591
2
Iteration 16100: Loss = -9918.183880410697
3
Iteration 16200: Loss = -9918.18412515157
4
Iteration 16300: Loss = -9918.190695846013
5
Iteration 16400: Loss = -9918.185387582445
6
Iteration 16500: Loss = -9918.18369838406
Iteration 16600: Loss = -9918.184267486267
1
Iteration 16700: Loss = -9918.183704063576
Iteration 16800: Loss = -9918.198752935114
1
Iteration 16900: Loss = -9918.184689492758
2
Iteration 17000: Loss = -9918.19193576908
3
Iteration 17100: Loss = -9918.18972962937
4
Iteration 17200: Loss = -9918.183825328146
5
Iteration 17300: Loss = -9918.1864979149
6
Iteration 17400: Loss = -9918.183702512993
Iteration 17500: Loss = -9918.184428130306
1
Iteration 17600: Loss = -9918.183808630822
2
Iteration 17700: Loss = -9918.189584231919
3
Iteration 17800: Loss = -9918.183641653597
Iteration 17900: Loss = -9918.184815756314
1
Iteration 18000: Loss = -9918.183704859046
Iteration 18100: Loss = -9918.18376808816
Iteration 18200: Loss = -9918.183881614606
1
Iteration 18300: Loss = -9918.183751119926
Iteration 18400: Loss = -9918.18391957476
1
Iteration 18500: Loss = -9918.184367103893
2
Iteration 18600: Loss = -9918.202097312856
3
Iteration 18700: Loss = -9918.188248101449
4
Iteration 18800: Loss = -9918.184463293073
5
Iteration 18900: Loss = -9918.18383385557
Iteration 19000: Loss = -9918.240641378026
1
Iteration 19100: Loss = -9918.183842381348
Iteration 19200: Loss = -9918.183748995725
Iteration 19300: Loss = -9918.18538473089
1
Iteration 19400: Loss = -9918.18388817783
2
Iteration 19500: Loss = -9918.209413865054
3
Iteration 19600: Loss = -9918.183649843477
Iteration 19700: Loss = -9918.201259611096
1
Iteration 19800: Loss = -9918.183732100484
Iteration 19900: Loss = -9918.184664716538
1
pi: tensor([[1.0000e+00, 1.7865e-08],
        [4.8736e-01, 5.1264e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8933, 0.1067], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1387, 0.1019],
         [0.6511, 0.0690]],

        [[0.5030, 0.1765],
         [0.5503, 0.5916]],

        [[0.7137, 0.0715],
         [0.7146, 0.5739]],

        [[0.6877, 0.2824],
         [0.6360, 0.6103]],

        [[0.5289, 0.2223],
         [0.6748, 0.7056]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0009975514204148314
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.005431979218977636
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 3.863438608745782e-05
Average Adjusted Rand Index: 0.0014950762130082284
10043.40895267735
[3.863438608745782e-05, 3.863438608745782e-05] [0.0014950762130082284, 0.0014950762130082284] [9918.184243117483, 9918.18413950259]
-------------------------------------
This iteration is 79
True Objective function: Loss = -9861.737684810876
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22492.82984192727
Iteration 100: Loss = -9694.739396133446
Iteration 200: Loss = -9693.613400983286
Iteration 300: Loss = -9693.122285403262
Iteration 400: Loss = -9692.8490098705
Iteration 500: Loss = -9692.681925286493
Iteration 600: Loss = -9692.566055727231
Iteration 700: Loss = -9692.458046675549
Iteration 800: Loss = -9692.294142688597
Iteration 900: Loss = -9691.991706223864
Iteration 1000: Loss = -9691.77109338462
Iteration 1100: Loss = -9691.648622947116
Iteration 1200: Loss = -9691.551065633077
Iteration 1300: Loss = -9691.454410217417
Iteration 1400: Loss = -9691.352395516575
Iteration 1500: Loss = -9691.248827735391
Iteration 1600: Loss = -9691.152307072593
Iteration 1700: Loss = -9691.069463601547
Iteration 1800: Loss = -9691.000878197552
Iteration 1900: Loss = -9690.94471106513
Iteration 2000: Loss = -9690.898717819546
Iteration 2100: Loss = -9690.859283289328
Iteration 2200: Loss = -9690.824569531953
Iteration 2300: Loss = -9690.794788391617
Iteration 2400: Loss = -9690.76990331676
Iteration 2500: Loss = -9690.74994065939
Iteration 2600: Loss = -9690.73451993517
Iteration 2700: Loss = -9690.722757569578
Iteration 2800: Loss = -9690.713815703724
Iteration 2900: Loss = -9690.706924579015
Iteration 3000: Loss = -9690.701574861183
Iteration 3100: Loss = -9690.697302479844
Iteration 3200: Loss = -9690.693824330365
Iteration 3300: Loss = -9690.690906556707
Iteration 3400: Loss = -9690.688406803554
Iteration 3500: Loss = -9690.686189486512
Iteration 3600: Loss = -9690.684268958285
Iteration 3700: Loss = -9690.682500427327
Iteration 3800: Loss = -9690.680919473278
Iteration 3900: Loss = -9690.67948840456
Iteration 4000: Loss = -9690.678150033846
Iteration 4100: Loss = -9690.676877464719
Iteration 4200: Loss = -9690.675817367775
Iteration 4300: Loss = -9690.67476736709
Iteration 4400: Loss = -9690.673804828577
Iteration 4500: Loss = -9690.672878978949
Iteration 4600: Loss = -9690.672021457904
Iteration 4700: Loss = -9690.671248558134
Iteration 4800: Loss = -9690.67049253891
Iteration 4900: Loss = -9690.669813018982
Iteration 5000: Loss = -9690.669194321068
Iteration 5100: Loss = -9690.668552367208
Iteration 5200: Loss = -9690.667977087098
Iteration 5300: Loss = -9690.667406821161
Iteration 5400: Loss = -9690.666951170779
Iteration 5500: Loss = -9690.666410627127
Iteration 5600: Loss = -9690.665983172881
Iteration 5700: Loss = -9690.665569062958
Iteration 5800: Loss = -9690.665143397837
Iteration 5900: Loss = -9690.664766897393
Iteration 6000: Loss = -9690.66435099919
Iteration 6100: Loss = -9690.664014498509
Iteration 6200: Loss = -9690.66371797449
Iteration 6300: Loss = -9690.663415002547
Iteration 6400: Loss = -9690.663988816093
1
Iteration 6500: Loss = -9690.662819871131
Iteration 6600: Loss = -9690.662544580562
Iteration 6700: Loss = -9690.6623511791
Iteration 6800: Loss = -9690.662081331973
Iteration 6900: Loss = -9690.661834458833
Iteration 7000: Loss = -9690.661815504478
Iteration 7100: Loss = -9690.661434908387
Iteration 7200: Loss = -9690.661247151724
Iteration 7300: Loss = -9690.661054067268
Iteration 7400: Loss = -9690.660876718439
Iteration 7500: Loss = -9690.660768182948
Iteration 7600: Loss = -9690.660560725599
Iteration 7700: Loss = -9690.660497291345
Iteration 7800: Loss = -9690.66028740618
Iteration 7900: Loss = -9690.664349358305
1
Iteration 8000: Loss = -9690.659995938307
Iteration 8100: Loss = -9690.674214104969
1
Iteration 8200: Loss = -9690.736994236157
2
Iteration 8300: Loss = -9690.661941306531
3
Iteration 8400: Loss = -9690.659579300513
Iteration 8500: Loss = -9690.659501755877
Iteration 8600: Loss = -9690.659375923095
Iteration 8700: Loss = -9690.65930135365
Iteration 8800: Loss = -9690.65919383166
Iteration 8900: Loss = -9690.705765531351
1
Iteration 9000: Loss = -9690.659047416671
Iteration 9100: Loss = -9690.658984458976
Iteration 9200: Loss = -9690.663323183846
1
Iteration 9300: Loss = -9690.658843425997
Iteration 9400: Loss = -9690.658774177005
Iteration 9500: Loss = -9690.658952177171
1
Iteration 9600: Loss = -9690.658632631146
Iteration 9700: Loss = -9690.658612184236
Iteration 9800: Loss = -9690.727774610661
1
Iteration 9900: Loss = -9690.658543600906
Iteration 10000: Loss = -9690.658457495385
Iteration 10100: Loss = -9690.658401079672
Iteration 10200: Loss = -9690.661852952942
1
Iteration 10300: Loss = -9690.658339090854
Iteration 10400: Loss = -9690.658304669567
Iteration 10500: Loss = -9690.660000379365
1
Iteration 10600: Loss = -9690.658232019183
Iteration 10700: Loss = -9690.65819670765
Iteration 10800: Loss = -9690.660371207014
1
Iteration 10900: Loss = -9690.658155473522
Iteration 11000: Loss = -9690.658125988575
Iteration 11100: Loss = -9690.658283791525
1
Iteration 11200: Loss = -9690.658077017033
Iteration 11300: Loss = -9690.658051985569
Iteration 11400: Loss = -9690.658102381716
Iteration 11500: Loss = -9690.657998500872
Iteration 11600: Loss = -9690.658023308935
Iteration 11700: Loss = -9690.658077115917
Iteration 11800: Loss = -9690.657963496924
Iteration 11900: Loss = -9690.659947327962
1
Iteration 12000: Loss = -9690.657948184751
Iteration 12100: Loss = -9690.657914507834
Iteration 12200: Loss = -9690.657895212891
Iteration 12300: Loss = -9690.65848182685
1
Iteration 12400: Loss = -9690.657853357343
Iteration 12500: Loss = -9690.657827337245
Iteration 12600: Loss = -9690.67919714201
1
Iteration 12700: Loss = -9690.657800990222
Iteration 12800: Loss = -9690.657844730114
Iteration 12900: Loss = -9690.657831682149
Iteration 13000: Loss = -9690.657798678081
Iteration 13100: Loss = -9690.6680514048
1
Iteration 13200: Loss = -9690.657763807623
Iteration 13300: Loss = -9690.701757485624
1
Iteration 13400: Loss = -9690.657751359115
Iteration 13500: Loss = -9690.657755820344
Iteration 13600: Loss = -9690.657791826014
Iteration 13700: Loss = -9690.657722711463
Iteration 13800: Loss = -9690.66043500755
1
Iteration 13900: Loss = -9690.657754450393
Iteration 14000: Loss = -9690.65972985671
1
Iteration 14100: Loss = -9690.65777568557
Iteration 14200: Loss = -9690.65768438417
Iteration 14300: Loss = -9690.65787650159
1
Iteration 14400: Loss = -9690.657684772583
Iteration 14500: Loss = -9690.660591941725
1
Iteration 14600: Loss = -9690.65767529655
Iteration 14700: Loss = -9690.658931021739
1
Iteration 14800: Loss = -9690.657654264198
Iteration 14900: Loss = -9690.6622948399
1
Iteration 15000: Loss = -9690.65766183706
Iteration 15100: Loss = -9690.65763372057
Iteration 15200: Loss = -9690.65870112256
1
Iteration 15300: Loss = -9690.65765503118
Iteration 15400: Loss = -9690.975719491043
1
Iteration 15500: Loss = -9690.657641069267
Iteration 15600: Loss = -9690.657631647297
Iteration 15700: Loss = -9690.658251309627
1
Iteration 15800: Loss = -9690.666385300621
2
Iteration 15900: Loss = -9690.657654976669
Iteration 16000: Loss = -9690.693328812482
1
Iteration 16100: Loss = -9690.657617226992
Iteration 16200: Loss = -9690.666588605953
1
Iteration 16300: Loss = -9690.657631695112
Iteration 16400: Loss = -9690.657852909299
1
Iteration 16500: Loss = -9690.657764269805
2
Iteration 16600: Loss = -9690.657610335418
Iteration 16700: Loss = -9690.664290830362
1
Iteration 16800: Loss = -9690.65762960287
Iteration 16900: Loss = -9690.657604841703
Iteration 17000: Loss = -9690.657887176303
1
Iteration 17100: Loss = -9690.65759923243
Iteration 17200: Loss = -9690.657620520311
Iteration 17300: Loss = -9690.657645032301
Iteration 17400: Loss = -9690.65766782456
Iteration 17500: Loss = -9690.65799940194
1
Iteration 17600: Loss = -9690.657998987872
2
Iteration 17700: Loss = -9690.65844277422
3
Iteration 17800: Loss = -9690.729456803223
4
Iteration 17900: Loss = -9690.657629015805
Iteration 18000: Loss = -9690.68186710177
1
Iteration 18100: Loss = -9690.657597633524
Iteration 18200: Loss = -9690.65804754127
1
Iteration 18300: Loss = -9690.66045030884
2
Iteration 18400: Loss = -9690.66771552036
3
Iteration 18500: Loss = -9690.657633220753
Iteration 18600: Loss = -9690.716239435786
1
Iteration 18700: Loss = -9690.657588918148
Iteration 18800: Loss = -9690.669653389376
1
Iteration 18900: Loss = -9690.658102346288
2
Iteration 19000: Loss = -9690.65760007724
Iteration 19100: Loss = -9690.658103622316
1
Iteration 19200: Loss = -9690.657619250826
Iteration 19300: Loss = -9690.658155104433
1
Iteration 19400: Loss = -9690.667205110356
2
Iteration 19500: Loss = -9690.657574438992
Iteration 19600: Loss = -9690.658124752588
1
Iteration 19700: Loss = -9690.661107413647
2
Iteration 19800: Loss = -9690.65865625671
3
Iteration 19900: Loss = -9690.657599037306
pi: tensor([[1.0946e-06, 1.0000e+00],
        [3.1527e-02, 9.6847e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0271, 0.9729], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1992, 0.2036],
         [0.6061, 0.1299]],

        [[0.7214, 0.2154],
         [0.6842, 0.5688]],

        [[0.6887, 0.1623],
         [0.6547, 0.7223]],

        [[0.5275, 0.1507],
         [0.5322, 0.5377]],

        [[0.6821, 0.1282],
         [0.7200, 0.5774]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.0207567131845433
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.003617863505838146
Average Adjusted Rand Index: 0.0056741769466282154
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23245.701947628822
Iteration 100: Loss = -9694.124675374786
Iteration 200: Loss = -9693.03231462713
Iteration 300: Loss = -9692.679065178332
Iteration 400: Loss = -9692.434631495631
Iteration 500: Loss = -9692.243875560485
Iteration 600: Loss = -9692.160274096788
Iteration 700: Loss = -9692.11535883157
Iteration 800: Loss = -9692.084774627201
Iteration 900: Loss = -9692.06196034653
Iteration 1000: Loss = -9692.043394484377
Iteration 1100: Loss = -9692.026425328435
Iteration 1200: Loss = -9692.006937326976
Iteration 1300: Loss = -9691.970875108285
Iteration 1400: Loss = -9691.815742762801
Iteration 1500: Loss = -9691.383167206674
Iteration 1600: Loss = -9691.151967067535
Iteration 1700: Loss = -9691.05110436517
Iteration 1800: Loss = -9690.98064912235
Iteration 1900: Loss = -9690.925076393767
Iteration 2000: Loss = -9690.879693672492
Iteration 2100: Loss = -9690.841819010639
Iteration 2200: Loss = -9690.809525556293
Iteration 2300: Loss = -9690.781768928635
Iteration 2400: Loss = -9690.757824194887
Iteration 2500: Loss = -9690.737609820346
Iteration 2600: Loss = -9690.721168831777
Iteration 2700: Loss = -9690.707995359982
Iteration 2800: Loss = -9690.697833425427
Iteration 2900: Loss = -9690.690536235736
Iteration 3000: Loss = -9690.685679598495
Iteration 3100: Loss = -9690.682388415895
Iteration 3200: Loss = -9690.679997394358
Iteration 3300: Loss = -9690.6781155476
Iteration 3400: Loss = -9690.676601825611
Iteration 3500: Loss = -9690.675245733559
Iteration 3600: Loss = -9690.674075138875
Iteration 3700: Loss = -9690.67300216146
Iteration 3800: Loss = -9690.672033486646
Iteration 3900: Loss = -9690.67115237294
Iteration 4000: Loss = -9690.670354554351
Iteration 4100: Loss = -9690.669565780829
Iteration 4200: Loss = -9690.668859587411
Iteration 4300: Loss = -9690.668195827915
Iteration 4400: Loss = -9690.667610681605
Iteration 4500: Loss = -9690.66703934835
Iteration 4600: Loss = -9690.666528409007
Iteration 4700: Loss = -9690.666010404844
Iteration 4800: Loss = -9690.665543473979
Iteration 4900: Loss = -9690.665143821994
Iteration 5000: Loss = -9690.664677646115
Iteration 5100: Loss = -9690.664420187832
Iteration 5200: Loss = -9690.663953364498
Iteration 5300: Loss = -9690.66360866965
Iteration 5400: Loss = -9690.66329439864
Iteration 5500: Loss = -9690.66297512354
Iteration 5600: Loss = -9690.670691327443
1
Iteration 5700: Loss = -9690.66245052581
Iteration 5800: Loss = -9690.662178734585
Iteration 5900: Loss = -9690.662831315074
1
Iteration 6000: Loss = -9690.661714152624
Iteration 6100: Loss = -9690.661507070718
Iteration 6200: Loss = -9690.667809855997
1
Iteration 6300: Loss = -9690.661113526769
Iteration 6400: Loss = -9690.660930020402
Iteration 6500: Loss = -9690.660754466051
Iteration 6600: Loss = -9690.660600562689
Iteration 6700: Loss = -9690.660480413824
Iteration 6800: Loss = -9690.660314740868
Iteration 6900: Loss = -9690.661894261273
1
Iteration 7000: Loss = -9690.660072396795
Iteration 7100: Loss = -9690.659906461977
Iteration 7200: Loss = -9690.659786522012
Iteration 7300: Loss = -9690.659680860932
Iteration 7400: Loss = -9690.65957405171
Iteration 7500: Loss = -9690.659487870269
Iteration 7600: Loss = -9690.668710956761
1
Iteration 7700: Loss = -9690.659291490188
Iteration 7800: Loss = -9690.659228877465
Iteration 7900: Loss = -9690.659126287866
Iteration 8000: Loss = -9690.659052420291
Iteration 8100: Loss = -9690.658988787789
Iteration 8200: Loss = -9690.658927758319
Iteration 8300: Loss = -9690.658828740536
Iteration 8400: Loss = -9690.658774495403
Iteration 8500: Loss = -9690.658708564852
Iteration 8600: Loss = -9690.658877134676
1
Iteration 8700: Loss = -9690.66044440495
2
Iteration 8800: Loss = -9690.658609529071
Iteration 8900: Loss = -9690.658591795558
Iteration 9000: Loss = -9690.658450890047
Iteration 9100: Loss = -9690.675838778308
1
Iteration 9200: Loss = -9690.658383667227
Iteration 9300: Loss = -9690.658334222997
Iteration 9400: Loss = -9690.663319953344
1
Iteration 9500: Loss = -9690.658248463338
Iteration 9600: Loss = -9690.65825565782
Iteration 9700: Loss = -9690.70909995924
1
Iteration 9800: Loss = -9690.658183620384
Iteration 9900: Loss = -9690.658153541886
Iteration 10000: Loss = -9690.658110310342
Iteration 10100: Loss = -9690.658151352096
Iteration 10200: Loss = -9690.658088009312
Iteration 10300: Loss = -9690.658049265716
Iteration 10400: Loss = -9690.659005905027
1
Iteration 10500: Loss = -9690.657993556913
Iteration 10600: Loss = -9690.657948930164
Iteration 10700: Loss = -9690.657932528587
Iteration 10800: Loss = -9690.657954453236
Iteration 10900: Loss = -9690.657928315226
Iteration 11000: Loss = -9690.658336285109
1
Iteration 11100: Loss = -9690.657897759545
Iteration 11200: Loss = -9690.657904359292
Iteration 11300: Loss = -9690.658516865702
1
Iteration 11400: Loss = -9690.657861360703
Iteration 11500: Loss = -9690.65781616514
Iteration 11600: Loss = -9690.65782165944
Iteration 11700: Loss = -9690.657810079114
Iteration 11800: Loss = -9691.050653688775
1
Iteration 11900: Loss = -9690.657793596933
Iteration 12000: Loss = -9690.657801612257
Iteration 12100: Loss = -9690.658022216658
1
Iteration 12200: Loss = -9690.657762553048
Iteration 12300: Loss = -9690.657759683929
Iteration 12400: Loss = -9690.668369119838
1
Iteration 12500: Loss = -9690.65773875438
Iteration 12600: Loss = -9690.657747038298
Iteration 12700: Loss = -9690.657758307436
Iteration 12800: Loss = -9690.657732137071
Iteration 12900: Loss = -9690.660381805816
1
Iteration 13000: Loss = -9690.657729636054
Iteration 13100: Loss = -9690.657672949057
Iteration 13200: Loss = -9690.657832102575
1
Iteration 13300: Loss = -9690.657653417034
Iteration 13400: Loss = -9690.66239904069
1
Iteration 13500: Loss = -9690.657667011446
Iteration 13600: Loss = -9690.664512561356
1
Iteration 13700: Loss = -9690.657662533193
Iteration 13800: Loss = -9690.734590419204
1
Iteration 13900: Loss = -9690.65767159209
Iteration 14000: Loss = -9690.816105695432
1
Iteration 14100: Loss = -9690.657621500972
Iteration 14200: Loss = -9690.662846945143
1
Iteration 14300: Loss = -9690.657650741796
Iteration 14400: Loss = -9690.659443758717
1
Iteration 14500: Loss = -9690.657638333494
Iteration 14600: Loss = -9690.671749083634
1
Iteration 14700: Loss = -9690.657653951163
Iteration 14800: Loss = -9690.701083998418
1
Iteration 14900: Loss = -9690.659917504272
2
Iteration 15000: Loss = -9690.657619141315
Iteration 15100: Loss = -9690.658406820356
1
Iteration 15200: Loss = -9690.667559059624
2
Iteration 15300: Loss = -9690.658213851395
3
Iteration 15400: Loss = -9690.65767395372
Iteration 15500: Loss = -9690.660008837629
1
Iteration 15600: Loss = -9690.657635259495
Iteration 15700: Loss = -9690.657978944319
1
Iteration 15800: Loss = -9690.657607904963
Iteration 15900: Loss = -9690.658429016574
1
Iteration 16000: Loss = -9690.657605564184
Iteration 16100: Loss = -9690.657615422248
Iteration 16200: Loss = -9690.657694263167
Iteration 16300: Loss = -9690.657624265137
Iteration 16400: Loss = -9690.666678601938
1
Iteration 16500: Loss = -9690.65761175342
Iteration 16600: Loss = -9690.657621352273
Iteration 16700: Loss = -9690.658069568859
1
Iteration 16800: Loss = -9690.657953353813
2
Iteration 16900: Loss = -9690.658272804425
3
Iteration 17000: Loss = -9690.65760886038
Iteration 17100: Loss = -9690.657793456212
1
Iteration 17200: Loss = -9690.660741073498
2
Iteration 17300: Loss = -9690.65761479629
Iteration 17400: Loss = -9690.670898046918
1
Iteration 17500: Loss = -9690.657624225421
Iteration 17600: Loss = -9690.791493514405
1
Iteration 17700: Loss = -9690.657611992643
Iteration 17800: Loss = -9690.6582418961
1
Iteration 17900: Loss = -9690.657604330472
Iteration 18000: Loss = -9690.807250754864
1
Iteration 18100: Loss = -9690.657613551044
Iteration 18200: Loss = -9690.66417002043
1
Iteration 18300: Loss = -9690.657641578999
Iteration 18400: Loss = -9690.751623885646
1
Iteration 18500: Loss = -9690.657605032346
Iteration 18600: Loss = -9690.65764541371
Iteration 18700: Loss = -9690.65763665611
Iteration 18800: Loss = -9690.657607094501
Iteration 18900: Loss = -9690.657701315904
Iteration 19000: Loss = -9690.657611127594
Iteration 19100: Loss = -9690.830963764063
1
Iteration 19200: Loss = -9690.657576088972
Iteration 19300: Loss = -9690.702540536236
1
Iteration 19400: Loss = -9690.657579630739
Iteration 19500: Loss = -9690.663784412736
1
Iteration 19600: Loss = -9690.657606804494
Iteration 19700: Loss = -9690.657599650296
Iteration 19800: Loss = -9690.657715973313
1
Iteration 19900: Loss = -9690.657575318952
pi: tensor([[6.1597e-07, 1.0000e+00],
        [3.1699e-02, 9.6830e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0275, 0.9725], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1993, 0.2016],
         [0.5594, 0.1301]],

        [[0.7056, 0.2156],
         [0.6065, 0.5755]],

        [[0.5750, 0.1623],
         [0.6728, 0.6382]],

        [[0.5040, 0.1506],
         [0.6683, 0.5297]],

        [[0.5393, 0.1286],
         [0.6134, 0.5253]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.0207567131845433
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.003617863505838146
Average Adjusted Rand Index: 0.0056741769466282154
9861.737684810876
[0.003617863505838146, 0.003617863505838146] [0.0056741769466282154, 0.0056741769466282154] [9690.659274470128, 9690.674596589022]
-------------------------------------
This iteration is 80
True Objective function: Loss = -10174.012735112165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22674.912458898296
Iteration 100: Loss = -10105.52204823237
Iteration 200: Loss = -10104.307285626732
Iteration 300: Loss = -10103.754714600285
Iteration 400: Loss = -10102.81673904903
Iteration 500: Loss = -10102.13308080799
Iteration 600: Loss = -10101.056902578573
Iteration 700: Loss = -10098.759969849654
Iteration 800: Loss = -10097.570262111174
Iteration 900: Loss = -10096.806683516756
Iteration 1000: Loss = -10096.25562636099
Iteration 1100: Loss = -10095.76956373819
Iteration 1200: Loss = -10095.300566807678
Iteration 1300: Loss = -10094.826730190876
Iteration 1400: Loss = -10094.351339537548
Iteration 1500: Loss = -10093.882118731952
Iteration 1600: Loss = -10093.465781202627
Iteration 1700: Loss = -10093.19747697257
Iteration 1800: Loss = -10093.099109572393
Iteration 1900: Loss = -10093.078254329736
Iteration 2000: Loss = -10093.075015519664
Iteration 2100: Loss = -10093.074594054524
Iteration 2200: Loss = -10093.074513005778
Iteration 2300: Loss = -10093.07452726794
Iteration 2400: Loss = -10093.074531023554
Iteration 2500: Loss = -10093.07456347897
Iteration 2600: Loss = -10093.0763385372
1
Iteration 2700: Loss = -10093.074508196207
Iteration 2800: Loss = -10093.07454040728
Iteration 2900: Loss = -10093.074886709448
1
Iteration 3000: Loss = -10093.074514885006
Iteration 3100: Loss = -10093.074539185345
Iteration 3200: Loss = -10093.074822527764
1
Iteration 3300: Loss = -10093.074678251203
2
Iteration 3400: Loss = -10093.07454408338
Iteration 3500: Loss = -10093.074540557183
Iteration 3600: Loss = -10093.074536005732
Iteration 3700: Loss = -10093.074549358342
Iteration 3800: Loss = -10093.07452116542
Iteration 3900: Loss = -10093.07548976473
1
Iteration 4000: Loss = -10093.074518364723
Iteration 4100: Loss = -10093.07478883958
1
Iteration 4200: Loss = -10093.089100705181
2
Iteration 4300: Loss = -10093.074598947926
Iteration 4400: Loss = -10093.074555440031
Iteration 4500: Loss = -10093.074719317117
1
Iteration 4600: Loss = -10093.074534627889
Iteration 4700: Loss = -10093.074536545073
Iteration 4800: Loss = -10093.07453890941
Iteration 4900: Loss = -10093.074557016393
Iteration 5000: Loss = -10093.074561427968
Iteration 5100: Loss = -10093.074568779411
Iteration 5200: Loss = -10093.074557419688
Iteration 5300: Loss = -10093.074550832791
Iteration 5400: Loss = -10093.074524622501
Iteration 5500: Loss = -10093.075210096373
1
Iteration 5600: Loss = -10093.074530248734
Iteration 5700: Loss = -10093.088089220348
1
Iteration 5800: Loss = -10093.07454682477
Iteration 5900: Loss = -10093.074530046008
Iteration 6000: Loss = -10093.074568414382
Iteration 6100: Loss = -10093.074518246438
Iteration 6200: Loss = -10093.07458809358
Iteration 6300: Loss = -10093.080596219557
1
Iteration 6400: Loss = -10093.074530979264
Iteration 6500: Loss = -10093.074743948928
1
Iteration 6600: Loss = -10093.074670004771
2
Iteration 6700: Loss = -10093.076401074988
3
Iteration 6800: Loss = -10093.074804354324
4
Iteration 6900: Loss = -10093.074649522445
5
Iteration 7000: Loss = -10093.07457912274
Iteration 7100: Loss = -10093.074881029073
1
Iteration 7200: Loss = -10093.074690397778
2
Iteration 7300: Loss = -10093.074668175772
Iteration 7400: Loss = -10093.080897411786
1
Iteration 7500: Loss = -10093.075623383864
2
Iteration 7600: Loss = -10093.074554310337
Iteration 7700: Loss = -10093.074592687584
Iteration 7800: Loss = -10093.074736477172
1
Iteration 7900: Loss = -10093.074530177235
Iteration 8000: Loss = -10093.116020276624
1
Iteration 8100: Loss = -10093.074527859128
Iteration 8200: Loss = -10093.088401700335
1
Iteration 8300: Loss = -10093.074537481481
Iteration 8400: Loss = -10093.089014100447
1
Iteration 8500: Loss = -10093.07453268417
Iteration 8600: Loss = -10093.074538282317
Iteration 8700: Loss = -10093.09441988732
1
Iteration 8800: Loss = -10093.07456598094
Iteration 8900: Loss = -10093.074580717146
Iteration 9000: Loss = -10093.074573301808
Iteration 9100: Loss = -10093.07454666178
Iteration 9200: Loss = -10093.076120209664
1
Iteration 9300: Loss = -10093.074549445557
Iteration 9400: Loss = -10093.07451989796
Iteration 9500: Loss = -10093.080663257744
1
Iteration 9600: Loss = -10093.07452859114
Iteration 9700: Loss = -10093.074596338474
Iteration 9800: Loss = -10093.074674369785
Iteration 9900: Loss = -10093.07456290772
Iteration 10000: Loss = -10093.124280504377
1
Iteration 10100: Loss = -10093.07454906522
Iteration 10200: Loss = -10093.074527427669
Iteration 10300: Loss = -10093.117401733152
1
Iteration 10400: Loss = -10093.074520124126
Iteration 10500: Loss = -10093.074554049263
Iteration 10600: Loss = -10093.108893182047
1
Iteration 10700: Loss = -10093.074523404053
Iteration 10800: Loss = -10093.074579051881
Iteration 10900: Loss = -10093.077183465428
1
Iteration 11000: Loss = -10093.074544872487
Iteration 11100: Loss = -10093.336135824251
1
Iteration 11200: Loss = -10093.074541134421
Iteration 11300: Loss = -10093.074556893289
Iteration 11400: Loss = -10093.108968496892
1
Iteration 11500: Loss = -10093.074550416342
Iteration 11600: Loss = -10093.074552923918
Iteration 11700: Loss = -10093.074763351533
1
Iteration 11800: Loss = -10093.074527704184
Iteration 11900: Loss = -10093.137874179165
1
Iteration 12000: Loss = -10093.074538500096
Iteration 12100: Loss = -10093.074525476106
Iteration 12200: Loss = -10093.077049986854
1
Iteration 12300: Loss = -10093.074540048725
Iteration 12400: Loss = -10093.074642589158
1
Iteration 12500: Loss = -10093.07456725702
Iteration 12600: Loss = -10093.074520271573
Iteration 12700: Loss = -10093.144036041549
1
Iteration 12800: Loss = -10093.074555918616
Iteration 12900: Loss = -10093.074556368972
Iteration 13000: Loss = -10093.07468222616
1
Iteration 13100: Loss = -10093.074571048954
Iteration 13200: Loss = -10093.074906818018
1
Iteration 13300: Loss = -10093.074569233428
Iteration 13400: Loss = -10093.372427730334
1
Iteration 13500: Loss = -10093.074526702572
Iteration 13600: Loss = -10093.074499013728
Iteration 13700: Loss = -10093.078758911153
1
Iteration 13800: Loss = -10093.074532638142
Iteration 13900: Loss = -10093.109602057031
1
Iteration 14000: Loss = -10093.074531505406
Iteration 14100: Loss = -10093.074541388147
Iteration 14200: Loss = -10093.365652108123
1
Iteration 14300: Loss = -10093.074571446068
Iteration 14400: Loss = -10093.07456611003
Iteration 14500: Loss = -10093.078159588513
1
Iteration 14600: Loss = -10093.074544943336
Iteration 14700: Loss = -10093.07457680518
Iteration 14800: Loss = -10093.074750416843
1
Iteration 14900: Loss = -10093.074548295335
Iteration 15000: Loss = -10093.102085786606
1
Iteration 15100: Loss = -10093.07455747035
Iteration 15200: Loss = -10093.074542779044
Iteration 15300: Loss = -10093.107038904851
1
Iteration 15400: Loss = -10093.074534587988
Iteration 15500: Loss = -10093.074536420658
Iteration 15600: Loss = -10093.07611630432
1
Iteration 15700: Loss = -10093.074554669522
Iteration 15800: Loss = -10093.130085295687
1
Iteration 15900: Loss = -10093.074549502991
Iteration 16000: Loss = -10093.074528490512
Iteration 16100: Loss = -10093.084696322767
1
Iteration 16200: Loss = -10093.074565091534
Iteration 16300: Loss = -10093.074531871343
Iteration 16400: Loss = -10093.074574874807
Iteration 16500: Loss = -10093.07454657341
Iteration 16600: Loss = -10093.47628732077
1
Iteration 16700: Loss = -10093.074585790693
Iteration 16800: Loss = -10093.074544325093
Iteration 16900: Loss = -10093.078459761893
1
Iteration 17000: Loss = -10093.074549298928
Iteration 17100: Loss = -10093.538538226847
1
Iteration 17200: Loss = -10093.074540169659
Iteration 17300: Loss = -10093.074536062391
Iteration 17400: Loss = -10093.074771432617
1
Iteration 17500: Loss = -10093.074551223683
Iteration 17600: Loss = -10093.27080534128
1
Iteration 17700: Loss = -10093.07456876518
Iteration 17800: Loss = -10093.074513515941
Iteration 17900: Loss = -10093.076821403092
1
Iteration 18000: Loss = -10093.074546252075
Iteration 18100: Loss = -10093.074554503663
Iteration 18200: Loss = -10093.075055324112
1
Iteration 18300: Loss = -10093.074531553104
Iteration 18400: Loss = -10093.10355973275
1
Iteration 18500: Loss = -10093.07454289323
Iteration 18600: Loss = -10093.074581971687
Iteration 18700: Loss = -10093.075503451053
1
Iteration 18800: Loss = -10093.074558739949
Iteration 18900: Loss = -10093.10895361673
1
Iteration 19000: Loss = -10093.07453550843
Iteration 19100: Loss = -10093.074539169646
Iteration 19200: Loss = -10093.075281208638
1
Iteration 19300: Loss = -10093.074529783087
Iteration 19400: Loss = -10093.094406883985
1
Iteration 19500: Loss = -10093.074575424085
Iteration 19600: Loss = -10093.074536831493
Iteration 19700: Loss = -10093.074811401942
1
Iteration 19800: Loss = -10093.074556609161
Iteration 19900: Loss = -10093.115672276921
1
pi: tensor([[0.6740, 0.3260],
        [0.2273, 0.7727]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3677, 0.6323], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1059, 0.1304],
         [0.6056, 0.1766]],

        [[0.5273, 0.1160],
         [0.6176, 0.7125]],

        [[0.5213, 0.1268],
         [0.7107, 0.6535]],

        [[0.5344, 0.1303],
         [0.7227, 0.5314]],

        [[0.5022, 0.1331],
         [0.5785, 0.7264]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 78
Adjusted Rand Index: 0.30727461959311375
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 76
Adjusted Rand Index: 0.2631380844038566
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 66
Adjusted Rand Index: 0.09356106675127351
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 68
Adjusted Rand Index: 0.12102848284874512
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 72
Adjusted Rand Index: 0.18499472980899417
Global Adjusted Rand Index: 0.19206216489430955
Average Adjusted Rand Index: 0.19399939668119665
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20546.60579387408
Iteration 100: Loss = -10105.644759886458
Iteration 200: Loss = -10104.660754701552
Iteration 300: Loss = -10104.310552953435
Iteration 400: Loss = -10103.98837924576
Iteration 500: Loss = -10103.357867569126
Iteration 600: Loss = -10102.952180472945
Iteration 700: Loss = -10102.318707169876
Iteration 800: Loss = -10100.870041178363
Iteration 900: Loss = -10099.543678141084
Iteration 1000: Loss = -10098.57440680066
Iteration 1100: Loss = -10097.838943790684
Iteration 1200: Loss = -10097.119248917641
Iteration 1300: Loss = -10096.539837363947
Iteration 1400: Loss = -10095.944969720014
Iteration 1500: Loss = -10095.413111721258
Iteration 1600: Loss = -10094.886059931643
Iteration 1700: Loss = -10094.358058363456
Iteration 1800: Loss = -10093.79306934719
Iteration 1900: Loss = -10093.339837861437
Iteration 2000: Loss = -10093.132175434659
Iteration 2100: Loss = -10093.083140866393
Iteration 2200: Loss = -10093.076695980642
Iteration 2300: Loss = -10093.075967685812
Iteration 2400: Loss = -10093.075837050414
Iteration 2500: Loss = -10093.075710914438
Iteration 2600: Loss = -10093.075788915265
Iteration 2700: Loss = -10093.075351227648
Iteration 2800: Loss = -10093.075100734386
Iteration 2900: Loss = -10093.074875322136
Iteration 3000: Loss = -10093.07459568851
Iteration 3100: Loss = -10093.074627362885
Iteration 3200: Loss = -10093.074544543599
Iteration 3300: Loss = -10093.075859026481
1
Iteration 3400: Loss = -10093.082757525777
2
Iteration 3500: Loss = -10093.07463613401
Iteration 3600: Loss = -10093.074575811506
Iteration 3700: Loss = -10093.074597466126
Iteration 3800: Loss = -10093.0958879872
1
Iteration 3900: Loss = -10093.07453531057
Iteration 4000: Loss = -10093.074624824674
Iteration 4100: Loss = -10093.07623563767
1
Iteration 4200: Loss = -10093.074514988391
Iteration 4300: Loss = -10093.07462819541
1
Iteration 4400: Loss = -10093.077857617409
2
Iteration 4500: Loss = -10093.074529015874
Iteration 4600: Loss = -10093.074530555696
Iteration 4700: Loss = -10093.074562187558
Iteration 4800: Loss = -10093.132456882637
1
Iteration 4900: Loss = -10093.074554175686
Iteration 5000: Loss = -10093.074535044607
Iteration 5100: Loss = -10093.080224428664
1
Iteration 5200: Loss = -10093.074530545906
Iteration 5300: Loss = -10093.075070384519
1
Iteration 5400: Loss = -10093.074539143578
Iteration 5500: Loss = -10093.07455883796
Iteration 5600: Loss = -10093.074613524088
Iteration 5700: Loss = -10093.074511026032
Iteration 5800: Loss = -10093.07611818894
1
Iteration 5900: Loss = -10093.074558468696
Iteration 6000: Loss = -10093.074613812292
Iteration 6100: Loss = -10093.07851045463
1
Iteration 6200: Loss = -10093.07458586491
Iteration 6300: Loss = -10093.07579488163
1
Iteration 6400: Loss = -10093.083369420108
2
Iteration 6500: Loss = -10093.07769508507
3
Iteration 6600: Loss = -10093.08046424125
4
Iteration 6700: Loss = -10093.075263585364
5
Iteration 6800: Loss = -10093.074598019442
Iteration 6900: Loss = -10093.075140214913
1
Iteration 7000: Loss = -10093.075863423337
2
Iteration 7100: Loss = -10093.07563846704
3
Iteration 7200: Loss = -10093.075286589246
4
Iteration 7300: Loss = -10093.074823314066
5
Iteration 7400: Loss = -10093.077042649755
6
Iteration 7500: Loss = -10093.075210801466
7
Iteration 7600: Loss = -10093.07457722833
Iteration 7700: Loss = -10093.085534125627
1
Iteration 7800: Loss = -10093.074550174004
Iteration 7900: Loss = -10093.074740181823
1
Iteration 8000: Loss = -10093.074547801018
Iteration 8100: Loss = -10093.076144789411
1
Iteration 8200: Loss = -10093.074533733696
Iteration 8300: Loss = -10093.074561045798
Iteration 8400: Loss = -10093.074594308555
Iteration 8500: Loss = -10093.074514980586
Iteration 8600: Loss = -10093.082809754362
1
Iteration 8700: Loss = -10093.074542669065
Iteration 8800: Loss = -10093.075386243792
1
Iteration 8900: Loss = -10093.074578752883
Iteration 9000: Loss = -10093.074523631441
Iteration 9100: Loss = -10093.08273789671
1
Iteration 9200: Loss = -10093.074580462287
Iteration 9300: Loss = -10093.07452598233
Iteration 9400: Loss = -10093.080731897364
1
Iteration 9500: Loss = -10093.074535848084
Iteration 9600: Loss = -10093.074580106339
Iteration 9700: Loss = -10093.074600459544
Iteration 9800: Loss = -10093.074538575238
Iteration 9900: Loss = -10093.088020865674
1
Iteration 10000: Loss = -10093.074533336161
Iteration 10100: Loss = -10093.074643198952
1
Iteration 10200: Loss = -10093.074672081711
2
Iteration 10300: Loss = -10093.074512475798
Iteration 10400: Loss = -10093.077627205384
1
Iteration 10500: Loss = -10093.074510363915
Iteration 10600: Loss = -10093.074561894386
Iteration 10700: Loss = -10093.125598283244
1
Iteration 10800: Loss = -10093.07453604791
Iteration 10900: Loss = -10093.074527689936
Iteration 11000: Loss = -10093.07457307187
Iteration 11100: Loss = -10093.07452368185
Iteration 11200: Loss = -10093.180833288145
1
Iteration 11300: Loss = -10093.07456273333
Iteration 11400: Loss = -10093.074497003807
Iteration 11500: Loss = -10093.08013304679
1
Iteration 11600: Loss = -10093.074532666127
Iteration 11700: Loss = -10093.074523819016
Iteration 11800: Loss = -10093.074779124452
1
Iteration 11900: Loss = -10093.074535045072
Iteration 12000: Loss = -10093.227033077328
1
Iteration 12100: Loss = -10093.074537715926
Iteration 12200: Loss = -10093.074557072478
Iteration 12300: Loss = -10093.07468930245
1
Iteration 12400: Loss = -10093.074585894805
Iteration 12500: Loss = -10093.122269827467
1
Iteration 12600: Loss = -10093.07455423532
Iteration 12700: Loss = -10093.07452695028
Iteration 12800: Loss = -10093.074821478984
1
Iteration 12900: Loss = -10093.074577033045
Iteration 13000: Loss = -10093.07695541904
1
Iteration 13100: Loss = -10093.07454466135
Iteration 13200: Loss = -10093.074532525749
Iteration 13300: Loss = -10093.078032048807
1
Iteration 13400: Loss = -10093.074541987891
Iteration 13500: Loss = -10093.07455218615
Iteration 13600: Loss = -10093.074681644948
1
Iteration 13700: Loss = -10093.074538040504
Iteration 13800: Loss = -10093.131440179262
1
Iteration 13900: Loss = -10093.074556906762
Iteration 14000: Loss = -10093.07454434273
Iteration 14100: Loss = -10093.076399182666
1
Iteration 14200: Loss = -10093.074549559118
Iteration 14300: Loss = -10093.15270366672
1
Iteration 14400: Loss = -10093.074596800334
Iteration 14500: Loss = -10093.074526630551
Iteration 14600: Loss = -10093.075093595913
1
Iteration 14700: Loss = -10093.074553920234
Iteration 14800: Loss = -10093.22040057167
1
Iteration 14900: Loss = -10093.074522206794
Iteration 15000: Loss = -10093.074507165595
Iteration 15100: Loss = -10093.075145233892
1
Iteration 15200: Loss = -10093.074536267433
Iteration 15300: Loss = -10093.229242337104
1
Iteration 15400: Loss = -10093.074538461904
Iteration 15500: Loss = -10093.07455196623
Iteration 15600: Loss = -10093.092145701576
1
Iteration 15700: Loss = -10093.074545380825
Iteration 15800: Loss = -10093.07452672466
Iteration 15900: Loss = -10093.077946308935
1
Iteration 16000: Loss = -10093.074548800521
Iteration 16100: Loss = -10093.074551551032
Iteration 16200: Loss = -10093.07492032625
1
Iteration 16300: Loss = -10093.074541019378
Iteration 16400: Loss = -10093.074778088923
1
Iteration 16500: Loss = -10093.074584887585
Iteration 16600: Loss = -10093.074535013478
Iteration 16700: Loss = -10093.07610902964
1
Iteration 16800: Loss = -10093.074543592924
Iteration 16900: Loss = -10093.150975004131
1
Iteration 17000: Loss = -10093.074529136047
Iteration 17100: Loss = -10093.0745501397
Iteration 17200: Loss = -10093.074620116902
Iteration 17300: Loss = -10093.074514940337
Iteration 17400: Loss = -10093.082480210956
1
Iteration 17500: Loss = -10093.074544483132
Iteration 17600: Loss = -10093.075013499953
1
Iteration 17700: Loss = -10093.074542310522
Iteration 17800: Loss = -10093.074521899931
Iteration 17900: Loss = -10093.096522184109
1
Iteration 18000: Loss = -10093.07454928912
Iteration 18100: Loss = -10093.07454077668
Iteration 18200: Loss = -10093.076341161082
1
Iteration 18300: Loss = -10093.074545264435
Iteration 18400: Loss = -10093.074537581191
Iteration 18500: Loss = -10093.074669098629
1
Iteration 18600: Loss = -10093.074536239124
Iteration 18700: Loss = -10093.10259587976
1
Iteration 18800: Loss = -10093.074558869774
Iteration 18900: Loss = -10093.074559412307
Iteration 19000: Loss = -10093.075020865543
1
Iteration 19100: Loss = -10093.074537672306
Iteration 19200: Loss = -10093.074915108878
1
Iteration 19300: Loss = -10093.074625949537
Iteration 19400: Loss = -10093.074541699438
Iteration 19500: Loss = -10093.331435217242
1
Iteration 19600: Loss = -10093.074559168095
Iteration 19700: Loss = -10093.074539483432
Iteration 19800: Loss = -10093.075938561744
1
Iteration 19900: Loss = -10093.074551034835
pi: tensor([[0.6735, 0.3265],
        [0.2270, 0.7730]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3673, 0.6327], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1058, 0.1304],
         [0.7092, 0.1764]],

        [[0.7195, 0.1159],
         [0.5017, 0.6316]],

        [[0.6761, 0.1266],
         [0.5620, 0.5358]],

        [[0.6405, 0.1302],
         [0.5450, 0.6608]],

        [[0.5165, 0.1330],
         [0.7243, 0.6402]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 78
Adjusted Rand Index: 0.30727461959311375
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 76
Adjusted Rand Index: 0.2631380844038566
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 66
Adjusted Rand Index: 0.09356106675127351
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 68
Adjusted Rand Index: 0.12102848284874512
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 72
Adjusted Rand Index: 0.18499472980899417
Global Adjusted Rand Index: 0.19206216489430955
Average Adjusted Rand Index: 0.19399939668119665
10174.012735112165
[0.19206216489430955, 0.19206216489430955] [0.19399939668119665, 0.19399939668119665] [10093.074540374106, 10093.0759315667]
-------------------------------------
This iteration is 81
True Objective function: Loss = -10007.924475058191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22889.118077442563
Iteration 100: Loss = -9891.402505811579
Iteration 200: Loss = -9890.166195288703
Iteration 300: Loss = -9889.810438607634
Iteration 400: Loss = -9889.645789692579
Iteration 500: Loss = -9889.558938348653
Iteration 600: Loss = -9889.509513034025
Iteration 700: Loss = -9889.47977674919
Iteration 800: Loss = -9889.4608624155
Iteration 900: Loss = -9889.44819390066
Iteration 1000: Loss = -9889.439082704414
Iteration 1100: Loss = -9889.43205448291
Iteration 1200: Loss = -9889.426254815731
Iteration 1300: Loss = -9889.42102160231
Iteration 1400: Loss = -9889.415948327856
Iteration 1500: Loss = -9889.410496199835
Iteration 1600: Loss = -9889.404203030397
Iteration 1700: Loss = -9889.396309857433
Iteration 1800: Loss = -9889.385712613202
Iteration 1900: Loss = -9889.37039161338
Iteration 2000: Loss = -9889.346901610239
Iteration 2100: Loss = -9889.309965744395
Iteration 2200: Loss = -9889.256495893516
Iteration 2300: Loss = -9889.196591715754
Iteration 2400: Loss = -9889.146395640235
Iteration 2500: Loss = -9889.104707037091
Iteration 2600: Loss = -9889.064848877591
Iteration 2700: Loss = -9889.023401518847
Iteration 2800: Loss = -9888.980851704597
Iteration 2900: Loss = -9888.93596682692
Iteration 3000: Loss = -9888.884012531866
Iteration 3100: Loss = -9888.818682205441
Iteration 3200: Loss = -9888.749552865682
Iteration 3300: Loss = -9888.705700148626
Iteration 3400: Loss = -9888.685472821704
Iteration 3500: Loss = -9888.675272194267
Iteration 3600: Loss = -9888.669101436037
Iteration 3700: Loss = -9888.66411767675
Iteration 3800: Loss = -9888.658326868708
Iteration 3900: Loss = -9888.646819401112
Iteration 4000: Loss = -9888.576695461958
Iteration 4100: Loss = -9888.207576257211
Iteration 4200: Loss = -9888.10485424394
Iteration 4300: Loss = -9887.65142035793
Iteration 4400: Loss = -9884.316874249309
Iteration 4500: Loss = -9883.948714011016
Iteration 4600: Loss = -9883.896606404618
Iteration 4700: Loss = -9883.886298383013
Iteration 4800: Loss = -9883.858177464213
Iteration 4900: Loss = -9883.854304191147
Iteration 5000: Loss = -9883.852683176017
Iteration 5100: Loss = -9883.851379813612
Iteration 5200: Loss = -9883.850677324308
Iteration 5300: Loss = -9883.850044113564
Iteration 5400: Loss = -9883.84929741092
Iteration 5500: Loss = -9883.847133285079
Iteration 5600: Loss = -9883.846727118038
Iteration 5700: Loss = -9883.846620780374
Iteration 5800: Loss = -9883.846501448297
Iteration 5900: Loss = -9883.846332901778
Iteration 6000: Loss = -9883.846299241062
Iteration 6100: Loss = -9883.846122720022
Iteration 6200: Loss = -9883.846741189978
1
Iteration 6300: Loss = -9883.845143057137
Iteration 6400: Loss = -9883.84479148145
Iteration 6500: Loss = -9883.84473351787
Iteration 6600: Loss = -9883.84467669027
Iteration 6700: Loss = -9883.844626205306
Iteration 6800: Loss = -9883.844625267011
Iteration 6900: Loss = -9883.844619947778
Iteration 7000: Loss = -9883.844635498468
Iteration 7100: Loss = -9883.844610899992
Iteration 7200: Loss = -9883.845016294668
1
Iteration 7300: Loss = -9883.844602167612
Iteration 7400: Loss = -9883.844566097014
Iteration 7500: Loss = -9883.844501300438
Iteration 7600: Loss = -9883.844473027211
Iteration 7700: Loss = -9883.844451747666
Iteration 7800: Loss = -9883.844383543048
Iteration 7900: Loss = -9883.844324447635
Iteration 8000: Loss = -9883.844235767554
Iteration 8100: Loss = -9883.844252712726
Iteration 8200: Loss = -9883.844375909688
1
Iteration 8300: Loss = -9883.844246467843
Iteration 8400: Loss = -9883.844211655687
Iteration 8500: Loss = -9883.844220734418
Iteration 8600: Loss = -9883.84703370194
1
Iteration 8700: Loss = -9883.844269810303
Iteration 8800: Loss = -9883.84422121573
Iteration 8900: Loss = -9883.872620731698
1
Iteration 9000: Loss = -9883.8441974302
Iteration 9100: Loss = -9883.847987604127
1
Iteration 9200: Loss = -9883.844174439213
Iteration 9300: Loss = -9883.844397389323
1
Iteration 9400: Loss = -9883.844180179678
Iteration 9500: Loss = -9883.84612949799
1
Iteration 9600: Loss = -9883.84413279426
Iteration 9700: Loss = -9883.853864090142
1
Iteration 9800: Loss = -9883.844126081138
Iteration 9900: Loss = -9883.844139904953
Iteration 10000: Loss = -9883.85187170134
1
Iteration 10100: Loss = -9883.844153405722
Iteration 10200: Loss = -9883.844117153663
Iteration 10300: Loss = -9883.858228212872
1
Iteration 10400: Loss = -9883.844095232598
Iteration 10500: Loss = -9883.844105553097
Iteration 10600: Loss = -9883.854896894076
1
Iteration 10700: Loss = -9883.844074769713
Iteration 10800: Loss = -9883.844094697988
Iteration 10900: Loss = -9883.846289470535
1
Iteration 11000: Loss = -9883.844100282593
Iteration 11100: Loss = -9883.844095031882
Iteration 11200: Loss = -9883.844827436755
1
Iteration 11300: Loss = -9883.84409135867
Iteration 11400: Loss = -9883.844088439548
Iteration 11500: Loss = -9883.845490898017
1
Iteration 11600: Loss = -9883.844071769798
Iteration 11700: Loss = -9883.84410134391
Iteration 11800: Loss = -9883.853509759425
1
Iteration 11900: Loss = -9883.84408139305
Iteration 12000: Loss = -9883.844078092809
Iteration 12100: Loss = -9883.844304976416
1
Iteration 12200: Loss = -9883.84409347134
Iteration 12300: Loss = -9883.84407570146
Iteration 12400: Loss = -9884.16388758841
1
Iteration 12500: Loss = -9883.844089188799
Iteration 12600: Loss = -9883.844074046689
Iteration 12700: Loss = -9883.89311305821
1
Iteration 12800: Loss = -9883.844094551165
Iteration 12900: Loss = -9883.844118365083
Iteration 13000: Loss = -9884.04461617002
1
Iteration 13100: Loss = -9883.844120829712
Iteration 13200: Loss = -9883.844098714359
Iteration 13300: Loss = -9883.84443544166
1
Iteration 13400: Loss = -9883.844096660663
Iteration 13500: Loss = -9883.844082866173
Iteration 13600: Loss = -9883.852198173152
1
Iteration 13700: Loss = -9883.844093122803
Iteration 13800: Loss = -9883.844104182866
Iteration 13900: Loss = -9883.85959400515
1
Iteration 14000: Loss = -9883.844083101056
Iteration 14100: Loss = -9884.084416728625
1
Iteration 14200: Loss = -9883.844110795253
Iteration 14300: Loss = -9883.844077395479
Iteration 14400: Loss = -9883.848033981229
1
Iteration 14500: Loss = -9883.84410301784
Iteration 14600: Loss = -9883.84406099818
Iteration 14700: Loss = -9883.844100035461
Iteration 14800: Loss = -9883.844129230609
Iteration 14900: Loss = -9883.84409457881
Iteration 15000: Loss = -9883.844107446497
Iteration 15100: Loss = -9883.848040996247
1
Iteration 15200: Loss = -9883.845075548736
2
Iteration 15300: Loss = -9883.844116199356
Iteration 15400: Loss = -9883.898298102387
1
Iteration 15500: Loss = -9883.844060607014
Iteration 15600: Loss = -9883.934818652386
1
Iteration 15700: Loss = -9883.84409813005
Iteration 15800: Loss = -9883.844573889013
1
Iteration 15900: Loss = -9883.844127254111
Iteration 16000: Loss = -9883.844068384811
Iteration 16100: Loss = -9883.847908650061
1
Iteration 16200: Loss = -9883.844067921413
Iteration 16300: Loss = -9883.853605797744
1
Iteration 16400: Loss = -9883.844066290156
Iteration 16500: Loss = -9884.241226958635
1
Iteration 16600: Loss = -9883.844094510034
Iteration 16700: Loss = -9883.844074949006
Iteration 16800: Loss = -9883.858697990603
1
Iteration 16900: Loss = -9883.844081780368
Iteration 17000: Loss = -9883.84408803692
Iteration 17100: Loss = -9883.881523467957
1
Iteration 17200: Loss = -9883.844065614016
Iteration 17300: Loss = -9883.844084525012
Iteration 17400: Loss = -9883.845123038755
1
Iteration 17500: Loss = -9883.844093296148
Iteration 17600: Loss = -9883.847663454935
1
Iteration 17700: Loss = -9883.844090176903
Iteration 17800: Loss = -9883.844094964543
Iteration 17900: Loss = -9883.845651601587
1
Iteration 18000: Loss = -9883.844097369474
Iteration 18100: Loss = -9883.844082774462
Iteration 18200: Loss = -9883.844241556944
1
Iteration 18300: Loss = -9883.844135159068
Iteration 18400: Loss = -9883.844072099091
Iteration 18500: Loss = -9883.844766827006
1
Iteration 18600: Loss = -9883.844129467148
Iteration 18700: Loss = -9883.844077582837
Iteration 18800: Loss = -9883.846187641511
1
Iteration 18900: Loss = -9883.844064786927
Iteration 19000: Loss = -9883.844054017964
Iteration 19100: Loss = -9883.84411481125
Iteration 19200: Loss = -9883.8440720334
Iteration 19300: Loss = -9883.844075233184
Iteration 19400: Loss = -9883.844527350095
1
Iteration 19500: Loss = -9883.844093206384
Iteration 19600: Loss = -9883.844129734045
Iteration 19700: Loss = -9883.84409620142
Iteration 19800: Loss = -9883.844091237994
Iteration 19900: Loss = -9883.922703037719
1
pi: tensor([[0.8503, 0.1497],
        [0.0097, 0.9903]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0742, 0.9258], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0420, 0.0858],
         [0.7155, 0.1428]],

        [[0.5918, 0.1018],
         [0.5254, 0.7253]],

        [[0.6784, 0.1316],
         [0.6603, 0.6092]],

        [[0.5009, 0.1087],
         [0.5690, 0.6583]],

        [[0.6877, 0.0893],
         [0.6127, 0.5188]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006438680677883739
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.009618156350865796
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.035323271006983556
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.04056271981242673
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 66
Adjusted Rand Index: 0.0639954496618846
Global Adjusted Rand Index: 0.026448068751413516
Average Adjusted Rand Index: 0.029771145752874463
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21355.351689835963
Iteration 100: Loss = -9891.576951100487
Iteration 200: Loss = -9890.38050228388
Iteration 300: Loss = -9889.90680173825
Iteration 400: Loss = -9889.715194051125
Iteration 500: Loss = -9889.615556821944
Iteration 600: Loss = -9889.553817473387
Iteration 700: Loss = -9889.511607220997
Iteration 800: Loss = -9889.480588313165
Iteration 900: Loss = -9889.456446164422
Iteration 1000: Loss = -9889.436392840542
Iteration 1100: Loss = -9889.41877510665
Iteration 1200: Loss = -9889.402369506513
Iteration 1300: Loss = -9889.386066461802
Iteration 1400: Loss = -9889.36878443059
Iteration 1500: Loss = -9889.349449365765
Iteration 1600: Loss = -9889.326828672965
Iteration 1700: Loss = -9889.299562317052
Iteration 1800: Loss = -9889.266240769908
Iteration 1900: Loss = -9889.22413800151
Iteration 2000: Loss = -9889.166458686772
Iteration 2100: Loss = -9889.077074999404
Iteration 2200: Loss = -9888.933969091766
Iteration 2300: Loss = -9888.701646812238
Iteration 2400: Loss = -9888.342648593241
Iteration 2500: Loss = -9887.818131151238
Iteration 2600: Loss = -9887.45069936592
Iteration 2700: Loss = -9887.261997338124
Iteration 2800: Loss = -9887.142778603595
Iteration 2900: Loss = -9887.058054232977
Iteration 3000: Loss = -9886.98793690698
Iteration 3100: Loss = -9886.926908469082
Iteration 3200: Loss = -9886.861106785685
Iteration 3300: Loss = -9886.739547838444
Iteration 3400: Loss = -9886.336796305994
Iteration 3500: Loss = -9884.553324535138
Iteration 3600: Loss = -9883.71692716074
Iteration 3700: Loss = -9883.144716864037
Iteration 3800: Loss = -9882.820573062258
Iteration 3900: Loss = -9882.512925307126
Iteration 4000: Loss = -9882.407271598098
Iteration 4100: Loss = -9882.323490900222
Iteration 4200: Loss = -9882.256280931779
Iteration 4300: Loss = -9882.218839188405
Iteration 4400: Loss = -9882.190561230147
Iteration 4500: Loss = -9882.166927066854
Iteration 4600: Loss = -9882.139595919776
Iteration 4700: Loss = -9881.986027838526
Iteration 4800: Loss = -9881.602018336194
Iteration 4900: Loss = -9881.561887387921
Iteration 5000: Loss = -9881.531683339586
Iteration 5100: Loss = -9881.44711904585
Iteration 5200: Loss = -9881.424002776146
Iteration 5300: Loss = -9881.410841818675
Iteration 5400: Loss = -9881.383825679777
Iteration 5500: Loss = -9881.362783643022
Iteration 5600: Loss = -9881.358226571747
Iteration 5700: Loss = -9881.35325967658
Iteration 5800: Loss = -9881.345368174185
Iteration 5900: Loss = -9881.34115679608
Iteration 6000: Loss = -9881.33703500656
Iteration 6100: Loss = -9881.315127104197
Iteration 6200: Loss = -9881.300922657514
Iteration 6300: Loss = -9881.290031065697
Iteration 6400: Loss = -9881.263441041392
Iteration 6500: Loss = -9881.258928042924
Iteration 6600: Loss = -9881.255516033665
Iteration 6700: Loss = -9881.25549297867
Iteration 6800: Loss = -9881.248352940494
Iteration 6900: Loss = -9881.247567784712
Iteration 7000: Loss = -9881.247336402259
Iteration 7100: Loss = -9881.246114100448
Iteration 7200: Loss = -9881.244898288485
Iteration 7300: Loss = -9881.242470700663
Iteration 7400: Loss = -9881.241722497261
Iteration 7500: Loss = -9881.248703661056
1
Iteration 7600: Loss = -9881.240943762024
Iteration 7700: Loss = -9881.240629629783
Iteration 7800: Loss = -9881.240349757569
Iteration 7900: Loss = -9881.240054075297
Iteration 8000: Loss = -9881.239877219594
Iteration 8100: Loss = -9881.23958236856
Iteration 8200: Loss = -9881.239388858583
Iteration 8300: Loss = -9881.239312650736
Iteration 8400: Loss = -9881.238961410534
Iteration 8500: Loss = -9881.238827194622
Iteration 8600: Loss = -9881.239816598494
1
Iteration 8700: Loss = -9881.23824715936
Iteration 8800: Loss = -9881.237747012989
Iteration 8900: Loss = -9881.240102631897
1
Iteration 9000: Loss = -9881.231140090651
Iteration 9100: Loss = -9881.246564754856
1
Iteration 9200: Loss = -9881.230694841757
Iteration 9300: Loss = -9881.242504178423
1
Iteration 9400: Loss = -9881.230462725984
Iteration 9500: Loss = -9881.268874337862
1
Iteration 9600: Loss = -9881.230210952634
Iteration 9700: Loss = -9881.231976188974
1
Iteration 9800: Loss = -9881.230034398333
Iteration 9900: Loss = -9881.229820330309
Iteration 10000: Loss = -9881.241305707166
1
Iteration 10100: Loss = -9881.227999130875
Iteration 10200: Loss = -9881.228094781552
Iteration 10300: Loss = -9881.227903753655
Iteration 10400: Loss = -9881.23027134068
1
Iteration 10500: Loss = -9881.227609645259
Iteration 10600: Loss = -9881.227564128954
Iteration 10700: Loss = -9881.227836825177
1
Iteration 10800: Loss = -9881.232404856819
2
Iteration 10900: Loss = -9881.227522929808
Iteration 11000: Loss = -9881.227436996138
Iteration 11100: Loss = -9881.243371151253
1
Iteration 11200: Loss = -9881.317203408822
2
Iteration 11300: Loss = -9881.230808051212
3
Iteration 11400: Loss = -9881.227143670996
Iteration 11500: Loss = -9881.227310709231
1
Iteration 11600: Loss = -9881.228597068774
2
Iteration 11700: Loss = -9881.227316999772
3
Iteration 11800: Loss = -9881.239650916343
4
Iteration 11900: Loss = -9881.228808264454
5
Iteration 12000: Loss = -9881.227051383985
Iteration 12100: Loss = -9881.236956744082
1
Iteration 12200: Loss = -9881.266608935666
2
Iteration 12300: Loss = -9881.232244181001
3
Iteration 12400: Loss = -9881.22700771598
Iteration 12500: Loss = -9881.233925032611
1
Iteration 12600: Loss = -9881.226754008298
Iteration 12700: Loss = -9881.226859737026
1
Iteration 12800: Loss = -9881.227240499935
2
Iteration 12900: Loss = -9881.227649623253
3
Iteration 13000: Loss = -9881.226752645618
Iteration 13100: Loss = -9881.233812154984
1
Iteration 13200: Loss = -9881.232749376682
2
Iteration 13300: Loss = -9881.225788201535
Iteration 13400: Loss = -9881.271325259262
1
Iteration 13500: Loss = -9881.228394176193
2
Iteration 13600: Loss = -9881.232305951144
3
Iteration 13700: Loss = -9881.225521905011
Iteration 13800: Loss = -9881.225526276607
Iteration 13900: Loss = -9881.226691108643
1
Iteration 14000: Loss = -9881.227923819584
2
Iteration 14100: Loss = -9881.230970754319
3
Iteration 14200: Loss = -9881.22867432576
4
Iteration 14300: Loss = -9881.24534237608
5
Iteration 14400: Loss = -9881.225675710251
6
Iteration 14500: Loss = -9881.360049674795
7
Iteration 14600: Loss = -9881.225254768182
Iteration 14700: Loss = -9881.228358060745
1
Iteration 14800: Loss = -9881.225251113676
Iteration 14900: Loss = -9881.226577307993
1
Iteration 15000: Loss = -9881.225238987023
Iteration 15100: Loss = -9881.225773955373
1
Iteration 15200: Loss = -9881.225263076522
Iteration 15300: Loss = -9881.234361877727
1
Iteration 15400: Loss = -9881.230715723974
2
Iteration 15500: Loss = -9881.225236417771
Iteration 15600: Loss = -9881.232119358947
1
Iteration 15700: Loss = -9881.225602168342
2
Iteration 15800: Loss = -9881.229893813519
3
Iteration 15900: Loss = -9881.229873431284
4
Iteration 16000: Loss = -9881.236277481601
5
Iteration 16100: Loss = -9881.228381041858
6
Iteration 16200: Loss = -9881.22514784501
Iteration 16300: Loss = -9881.229800575855
1
Iteration 16400: Loss = -9881.225783796954
2
Iteration 16500: Loss = -9881.225576464481
3
Iteration 16600: Loss = -9881.3052457028
4
Iteration 16700: Loss = -9881.229192554061
5
Iteration 16800: Loss = -9881.225125298954
Iteration 16900: Loss = -9881.226428074566
1
Iteration 17000: Loss = -9881.225131557892
Iteration 17100: Loss = -9881.225457905455
1
Iteration 17200: Loss = -9881.307124077617
2
Iteration 17300: Loss = -9881.227274319446
3
Iteration 17400: Loss = -9881.237464174737
4
Iteration 17500: Loss = -9881.225209408882
Iteration 17600: Loss = -9881.225160279462
Iteration 17700: Loss = -9881.228480926036
1
Iteration 17800: Loss = -9881.225150557286
Iteration 17900: Loss = -9881.230442743321
1
Iteration 18000: Loss = -9881.225152324572
Iteration 18100: Loss = -9881.228163373555
1
Iteration 18200: Loss = -9881.251162317447
2
Iteration 18300: Loss = -9881.225734334655
3
Iteration 18400: Loss = -9881.278986587111
4
Iteration 18500: Loss = -9881.228595162553
5
Iteration 18600: Loss = -9881.259635044482
6
Iteration 18700: Loss = -9881.234179111778
7
Iteration 18800: Loss = -9881.245294161694
8
Iteration 18900: Loss = -9881.248701831695
9
Iteration 19000: Loss = -9881.225317124707
10
Iteration 19100: Loss = -9881.228208718041
11
Iteration 19200: Loss = -9881.227892144001
12
Iteration 19300: Loss = -9881.243147590307
13
Iteration 19400: Loss = -9881.254166720322
14
Iteration 19500: Loss = -9881.225097756687
Iteration 19600: Loss = -9881.22561537461
1
Iteration 19700: Loss = -9881.378625372587
2
Iteration 19800: Loss = -9881.225222030847
3
Iteration 19900: Loss = -9881.227943164922
4
pi: tensor([[1.0000e+00, 1.0172e-06],
        [1.7126e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5758, 0.4242], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1240, 0.1216],
         [0.6055, 0.1824]],

        [[0.5275, 0.1310],
         [0.5217, 0.7041]],

        [[0.6083, 0.1271],
         [0.5541, 0.6387]],

        [[0.5682, 0.1375],
         [0.6715, 0.6539]],

        [[0.5829, 0.1293],
         [0.6754, 0.6791]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.022913377152719128
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.03897614180264656
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 70
Adjusted Rand Index: 0.15140086206896552
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 69
Adjusted Rand Index: 0.13550354557813132
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 65
Adjusted Rand Index: 0.07972640017651601
Global Adjusted Rand Index: 0.08547997445319659
Average Adjusted Rand Index: 0.08570406535579571
10007.924475058191
[0.026448068751413516, 0.08547997445319659] [0.029771145752874463, 0.08570406535579571] [9883.844101717199, 9881.231527247359]
-------------------------------------
This iteration is 82
True Objective function: Loss = -9935.044715388682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21999.99903080466
Iteration 100: Loss = -9787.50394978008
Iteration 200: Loss = -9786.370618352676
Iteration 300: Loss = -9785.84101555101
Iteration 400: Loss = -9785.583491910322
Iteration 500: Loss = -9785.414332382525
Iteration 600: Loss = -9785.291932531156
Iteration 700: Loss = -9785.199542441247
Iteration 800: Loss = -9785.112122012903
Iteration 900: Loss = -9785.012956551582
Iteration 1000: Loss = -9784.886435369777
Iteration 1100: Loss = -9784.710052245528
Iteration 1200: Loss = -9784.45368167978
Iteration 1300: Loss = -9784.160652242437
Iteration 1400: Loss = -9783.926194220885
Iteration 1500: Loss = -9783.737153555823
Iteration 1600: Loss = -9783.578342368017
Iteration 1700: Loss = -9783.445376937512
Iteration 1800: Loss = -9783.334591589395
Iteration 1900: Loss = -9783.241083969062
Iteration 2000: Loss = -9783.160223352896
Iteration 2100: Loss = -9783.08705632708
Iteration 2200: Loss = -9783.016529462982
Iteration 2300: Loss = -9782.943013187443
Iteration 2400: Loss = -9782.8603420698
Iteration 2500: Loss = -9782.765295429912
Iteration 2600: Loss = -9782.664167403047
Iteration 2700: Loss = -9782.570033706217
Iteration 2800: Loss = -9782.487137984797
Iteration 2900: Loss = -9782.405837563976
Iteration 3000: Loss = -9782.296252808124
Iteration 3100: Loss = -9782.034238712433
Iteration 3200: Loss = -9781.034402033169
Iteration 3300: Loss = -9779.338621387218
Iteration 3400: Loss = -9778.848756051126
Iteration 3500: Loss = -9776.451605350223
Iteration 3600: Loss = -9775.96986207277
Iteration 3700: Loss = -9775.860532108663
Iteration 3800: Loss = -9775.81253113845
Iteration 3900: Loss = -9775.784825310478
Iteration 4000: Loss = -9775.766169324925
Iteration 4100: Loss = -9775.752402277885
Iteration 4200: Loss = -9775.741333408636
Iteration 4300: Loss = -9775.73230529014
Iteration 4400: Loss = -9775.725860952984
Iteration 4500: Loss = -9775.720834441221
Iteration 4600: Loss = -9775.71663118091
Iteration 4700: Loss = -9775.713019512803
Iteration 4800: Loss = -9775.70988195042
Iteration 4900: Loss = -9775.707157717601
Iteration 5000: Loss = -9775.704648689696
Iteration 5100: Loss = -9775.70250146488
Iteration 5200: Loss = -9775.700574080529
Iteration 5300: Loss = -9775.698898739285
Iteration 5400: Loss = -9775.697395417874
Iteration 5500: Loss = -9775.696027630856
Iteration 5600: Loss = -9775.694807647878
Iteration 5700: Loss = -9775.693662745509
Iteration 5800: Loss = -9775.692643009133
Iteration 5900: Loss = -9775.691699935378
Iteration 6000: Loss = -9775.690819524294
Iteration 6100: Loss = -9775.68998892479
Iteration 6200: Loss = -9775.689221904318
Iteration 6300: Loss = -9775.688478679185
Iteration 6400: Loss = -9775.687822791002
Iteration 6500: Loss = -9775.687204465357
Iteration 6600: Loss = -9775.686634398658
Iteration 6700: Loss = -9775.686096400033
Iteration 6800: Loss = -9775.685583344515
Iteration 6900: Loss = -9775.685085705516
Iteration 7000: Loss = -9775.684608693038
Iteration 7100: Loss = -9775.684147523023
Iteration 7200: Loss = -9775.683654600687
Iteration 7300: Loss = -9775.683023464439
Iteration 7400: Loss = -9775.682405270014
Iteration 7500: Loss = -9775.682045813204
Iteration 7600: Loss = -9775.681810663824
Iteration 7700: Loss = -9775.68143726739
Iteration 7800: Loss = -9775.68119294017
Iteration 7900: Loss = -9775.685925689842
1
Iteration 8000: Loss = -9775.680727144336
Iteration 8100: Loss = -9775.68049550316
Iteration 8200: Loss = -9775.967922647174
1
Iteration 8300: Loss = -9775.680092428696
Iteration 8400: Loss = -9775.679941837478
Iteration 8500: Loss = -9775.67976859127
Iteration 8600: Loss = -9775.67965144806
Iteration 8700: Loss = -9775.679440775959
Iteration 8800: Loss = -9775.679320648118
Iteration 8900: Loss = -9775.682122645449
1
Iteration 9000: Loss = -9775.679014113022
Iteration 9100: Loss = -9775.678904540928
Iteration 9200: Loss = -9775.69934913896
1
Iteration 9300: Loss = -9775.678735818274
Iteration 9400: Loss = -9775.67862352675
Iteration 9500: Loss = -9775.678501385353
Iteration 9600: Loss = -9775.678442524802
Iteration 9700: Loss = -9775.678327861107
Iteration 9800: Loss = -9775.678237498689
Iteration 9900: Loss = -9775.678362314144
1
Iteration 10000: Loss = -9775.67808620659
Iteration 10100: Loss = -9775.678005902913
Iteration 10200: Loss = -9775.678065456508
Iteration 10300: Loss = -9775.783333387413
1
Iteration 10400: Loss = -9775.6777829841
Iteration 10500: Loss = -9775.680047087866
1
Iteration 10600: Loss = -9775.677706795963
Iteration 10700: Loss = -9775.678870205127
1
Iteration 10800: Loss = -9775.677589010285
Iteration 10900: Loss = -9775.892259051
1
Iteration 11000: Loss = -9775.67748917559
Iteration 11100: Loss = -9775.677457148497
Iteration 11200: Loss = -9775.687235208461
1
Iteration 11300: Loss = -9775.67736011073
Iteration 11400: Loss = -9775.677361361311
Iteration 11500: Loss = -9775.685482070257
1
Iteration 11600: Loss = -9775.677843750682
2
Iteration 11700: Loss = -9775.677655871792
3
Iteration 11800: Loss = -9775.679100686504
4
Iteration 11900: Loss = -9775.680427811076
5
Iteration 12000: Loss = -9775.680319437792
6
Iteration 12100: Loss = -9775.704836175804
7
Iteration 12200: Loss = -9775.698637380352
8
Iteration 12300: Loss = -9775.678033154403
9
Iteration 12400: Loss = -9775.68068961988
10
Iteration 12500: Loss = -9775.678852290335
11
Iteration 12600: Loss = -9775.679998863272
12
Iteration 12700: Loss = -9775.680098092698
13
Iteration 12800: Loss = -9775.67965046685
14
Iteration 12900: Loss = -9775.72371522455
15
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[2.2766e-05, 9.9998e-01],
        [1.0000e+00, 3.6723e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0252, 0.9748], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1237, 0.1337],
         [0.6395, 0.1427]],

        [[0.5601, 0.2171],
         [0.5560, 0.7268]],

        [[0.5850, 0.1130],
         [0.5725, 0.7088]],

        [[0.7183, 0.0862],
         [0.6588, 0.5088]],

        [[0.6740, 0.0915],
         [0.7124, 0.6889]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0007748402262652058
time is 3
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 83%|████████▎ | 83/100 [28:03:01<5:41:13, 1204.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 84%|████████▍ | 84/100 [28:24:25<5:27:28, 1228.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 85%|████████▌ | 85/100 [28:45:54<5:11:35, 1246.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 86%|████████▌ | 86/100 [29:07:21<4:53:40, 1258.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 87%|████████▋ | 87/100 [29:28:17<4:32:30, 1257.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 88%|████████▊ | 88/100 [29:49:52<4:13:49, 1269.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 89%|████████▉ | 89/100 [30:11:14<3:53:22, 1272.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 90%|█████████ | 90/100 [30:33:29<3:35:16, 1291.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 91%|█████████ | 91/100 [30:55:11<3:14:12, 1294.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 92%|█████████▏| 92/100 [31:16:36<2:52:14, 1291.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 93%|█████████▎| 93/100 [31:38:00<2:30:26, 1289.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 94%|█████████▍| 94/100 [31:56:18<2:03:11, 1231.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 95%|█████████▌| 95/100 [32:20:31<1:48:11, 1298.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 96%|█████████▌| 96/100 [32:38:52<1:22:36, 1239.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 97%|█████████▋| 97/100 [33:00:19<1:02:40, 1253.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 98%|█████████▊| 98/100 [33:21:45<42:06, 1263.10s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 99%|█████████▉| 99/100 [33:43:18<21:12, 1272.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
100%|██████████| 100/100 [34:04:45<00:00, 1276.62s/it]100%|██████████| 100/100 [34:04:45<00:00, 1226.86s/it]
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -0.0010177607850258056
Average Adjusted Rand Index: -0.0015999903951959935
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21746.683723913877
Iteration 100: Loss = -9787.865122371068
Iteration 200: Loss = -9786.507144248395
Iteration 300: Loss = -9785.918030901852
Iteration 400: Loss = -9785.552721340624
Iteration 500: Loss = -9785.329147532255
Iteration 600: Loss = -9785.148999274541
Iteration 700: Loss = -9784.924539150546
Iteration 800: Loss = -9784.506777141574
Iteration 900: Loss = -9783.608088324114
Iteration 1000: Loss = -9782.86359512361
Iteration 1100: Loss = -9782.50591768507
Iteration 1200: Loss = -9782.2605036091
Iteration 1300: Loss = -9782.067077206493
Iteration 1400: Loss = -9781.908011208654
Iteration 1500: Loss = -9781.761530814674
Iteration 1600: Loss = -9781.596870929241
Iteration 1700: Loss = -9781.415113598423
Iteration 1800: Loss = -9781.20742632096
Iteration 1900: Loss = -9781.037496044266
Iteration 2000: Loss = -9780.833677725694
Iteration 2100: Loss = -9780.512766647505
Iteration 2200: Loss = -9780.40577525795
Iteration 2300: Loss = -9780.336284908395
Iteration 2400: Loss = -9780.284222926866
Iteration 2500: Loss = -9780.24005794451
Iteration 2600: Loss = -9780.20556589515
Iteration 2700: Loss = -9780.182001059546
Iteration 2800: Loss = -9780.165848694753
Iteration 2900: Loss = -9780.152859478914
Iteration 3000: Loss = -9780.1424552581
Iteration 3100: Loss = -9780.134083376464
Iteration 3200: Loss = -9780.127006661822
Iteration 3300: Loss = -9780.120748641755
Iteration 3400: Loss = -9780.114894195076
Iteration 3500: Loss = -9780.108799127658
Iteration 3600: Loss = -9780.102058743218
Iteration 3700: Loss = -9780.094223567308
Iteration 3800: Loss = -9780.085370910343
Iteration 3900: Loss = -9780.073421517352
Iteration 4000: Loss = -9780.05983247741
Iteration 4100: Loss = -9780.054504914311
Iteration 4200: Loss = -9780.051141809217
Iteration 4300: Loss = -9780.047723161812
Iteration 4400: Loss = -9780.0437428851
Iteration 4500: Loss = -9780.040444257433
Iteration 4600: Loss = -9780.038367902423
Iteration 4700: Loss = -9780.036659533902
Iteration 4800: Loss = -9780.034945364087
Iteration 4900: Loss = -9780.032877100317
Iteration 5000: Loss = -9780.02985321016
Iteration 5100: Loss = -9780.027295995225
Iteration 5200: Loss = -9780.026017823764
Iteration 5300: Loss = -9780.02503880434
Iteration 5400: Loss = -9780.024137249351
Iteration 5500: Loss = -9780.02325216041
Iteration 5600: Loss = -9780.022318732243
Iteration 5700: Loss = -9780.021147464207
Iteration 5800: Loss = -9780.019197667618
Iteration 5900: Loss = -9780.015868713095
Iteration 6000: Loss = -9780.01367797253
Iteration 6100: Loss = -9780.012637020996
Iteration 6200: Loss = -9780.012005203296
Iteration 6300: Loss = -9780.011438525851
Iteration 6400: Loss = -9780.010841985999
Iteration 6500: Loss = -9780.010219907064
Iteration 6600: Loss = -9780.009338456739
Iteration 6700: Loss = -9780.008337694415
Iteration 6800: Loss = -9780.007763293765
Iteration 6900: Loss = -9780.007399195238
Iteration 7000: Loss = -9780.007031392137
Iteration 7100: Loss = -9780.00673496945
Iteration 7200: Loss = -9780.006510712305
Iteration 7300: Loss = -9780.006203731255
Iteration 7400: Loss = -9780.006496037957
1
Iteration 7500: Loss = -9780.005692957422
Iteration 7600: Loss = -9780.005392172676
Iteration 7700: Loss = -9780.005144934212
Iteration 7800: Loss = -9780.004772758057
Iteration 7900: Loss = -9780.004497443724
Iteration 8000: Loss = -9780.004304315707
Iteration 8100: Loss = -9780.004132462478
Iteration 8200: Loss = -9780.003891035863
Iteration 8300: Loss = -9780.004311291039
1
Iteration 8400: Loss = -9780.00348901391
Iteration 8500: Loss = -9780.002493799353
Iteration 8600: Loss = -9779.999364361069
Iteration 8700: Loss = -9779.99921302768
Iteration 8800: Loss = -9779.999056719435
Iteration 8900: Loss = -9779.999189541979
1
Iteration 9000: Loss = -9779.99885977243
Iteration 9100: Loss = -9780.002801462668
1
Iteration 9200: Loss = -9779.998637676124
Iteration 9300: Loss = -9779.99852103369
Iteration 9400: Loss = -9779.998529029795
Iteration 9500: Loss = -9779.997911265084
Iteration 9600: Loss = -9779.995854668632
Iteration 9700: Loss = -9779.995658084814
Iteration 9800: Loss = -9779.995364435568
Iteration 9900: Loss = -9779.99461506255
Iteration 10000: Loss = -9779.994102413171
Iteration 10100: Loss = -9779.996647146438
1
Iteration 10200: Loss = -9779.99400456942
Iteration 10300: Loss = -9779.993893053628
Iteration 10400: Loss = -9780.018525736852
1
Iteration 10500: Loss = -9779.99380232394
Iteration 10600: Loss = -9779.99383491065
Iteration 10700: Loss = -9779.993715897695
Iteration 10800: Loss = -9780.003717019858
1
Iteration 10900: Loss = -9779.991863239364
Iteration 11000: Loss = -9779.991577057603
Iteration 11100: Loss = -9780.000281852148
1
Iteration 11200: Loss = -9779.988492454078
Iteration 11300: Loss = -9779.987707909448
Iteration 11400: Loss = -9780.160638964195
1
Iteration 11500: Loss = -9779.991776346556
2
Iteration 11600: Loss = -9779.989260124436
3
Iteration 11700: Loss = -9779.989987229452
4
Iteration 11800: Loss = -9780.016074910576
5
Iteration 11900: Loss = -9779.987113522993
Iteration 12000: Loss = -9779.99992783647
1
Iteration 12100: Loss = -9779.987575141096
2
Iteration 12200: Loss = -9779.986623948806
Iteration 12300: Loss = -9779.987519630648
1
Iteration 12400: Loss = -9779.99291259826
2
Iteration 12500: Loss = -9779.99146428872
3
Iteration 12600: Loss = -9779.989280444604
4
Iteration 12700: Loss = -9780.039424536426
5
Iteration 12800: Loss = -9779.993393322993
6
Iteration 12900: Loss = -9779.995296805459
7
Iteration 13000: Loss = -9779.992356757
8
Iteration 13100: Loss = -9779.986641570027
Iteration 13200: Loss = -9779.98689600516
1
Iteration 13300: Loss = -9779.987748142566
2
Iteration 13400: Loss = -9779.98646091696
Iteration 13500: Loss = -9779.987014003593
1
Iteration 13600: Loss = -9780.015024125812
2
Iteration 13700: Loss = -9779.997138017241
3
Iteration 13800: Loss = -9779.989506310745
4
Iteration 13900: Loss = -9779.986708444982
5
Iteration 14000: Loss = -9779.985810908764
Iteration 14100: Loss = -9779.986083797194
1
Iteration 14200: Loss = -9779.986959638603
2
Iteration 14300: Loss = -9780.019195091365
3
Iteration 14400: Loss = -9779.985782861566
Iteration 14500: Loss = -9779.985716401497
Iteration 14600: Loss = -9779.987397675255
1
Iteration 14700: Loss = -9779.987736703735
2
Iteration 14800: Loss = -9779.985935377548
3
Iteration 14900: Loss = -9779.98506930028
Iteration 15000: Loss = -9779.985032316707
Iteration 15100: Loss = -9779.987068423283
1
Iteration 15200: Loss = -9779.995612059438
2
Iteration 15300: Loss = -9780.009160064223
3
Iteration 15400: Loss = -9779.986861476029
4
Iteration 15500: Loss = -9779.98631060083
5
Iteration 15600: Loss = -9779.986549817568
6
Iteration 15700: Loss = -9779.986702200304
7
Iteration 15800: Loss = -9779.985050977095
Iteration 15900: Loss = -9779.985345811361
1
Iteration 16000: Loss = -9779.986034888963
2
Iteration 16100: Loss = -9780.022819694288
3
Iteration 16200: Loss = -9779.988238984735
4
Iteration 16300: Loss = -9779.987248765605
5
Iteration 16400: Loss = -9779.98778072842
6
Iteration 16500: Loss = -9779.98607099137
7
Iteration 16600: Loss = -9780.00395982082
8
Iteration 16700: Loss = -9779.985339002327
9
Iteration 16800: Loss = -9779.985137745358
Iteration 16900: Loss = -9779.993210264087
1
Iteration 17000: Loss = -9779.986631221896
2
Iteration 17100: Loss = -9779.9963244839
3
Iteration 17200: Loss = -9779.987314798123
4
Iteration 17300: Loss = -9779.985076396473
Iteration 17400: Loss = -9779.984944251017
Iteration 17500: Loss = -9779.98635438164
1
Iteration 17600: Loss = -9779.990933783574
2
Iteration 17700: Loss = -9779.98568735288
3
Iteration 17800: Loss = -9779.985297150184
4
Iteration 17900: Loss = -9779.98494659296
Iteration 18000: Loss = -9779.985306482038
1
Iteration 18100: Loss = -9780.007553689837
2
Iteration 18200: Loss = -9779.984895213138
Iteration 18300: Loss = -9779.984961246135
Iteration 18400: Loss = -9779.98592401626
1
Iteration 18500: Loss = -9779.985833253977
2
Iteration 18600: Loss = -9779.98622555431
3
Iteration 18700: Loss = -9779.98547954279
4
Iteration 18800: Loss = -9779.984671429047
Iteration 18900: Loss = -9779.985372870913
1
Iteration 19000: Loss = -9779.985552271208
2
Iteration 19100: Loss = -9779.989206764621
3
Iteration 19200: Loss = -9779.986724075223
4
Iteration 19300: Loss = -9779.988974234711
5
Iteration 19400: Loss = -9780.006370820944
6
Iteration 19500: Loss = -9779.989711416363
7
Iteration 19600: Loss = -9780.006893107426
8
Iteration 19700: Loss = -9779.984677817223
Iteration 19800: Loss = -9779.985254235022
1
Iteration 19900: Loss = -9779.985168581901
2
pi: tensor([[1.0000e+00, 9.4745e-08],
        [9.0069e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8415, 0.1585], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1315, 0.1720],
         [0.7276, 0.1341]],

        [[0.5966, 0.0894],
         [0.6401, 0.6054]],

        [[0.6863, 0.1684],
         [0.5653, 0.6781]],

        [[0.5711, 0.1372],
         [0.5322, 0.5139]],

        [[0.5415, 0.1494],
         [0.5299, 0.6107]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.017592125485411263
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.006605112803125404
time is 2
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.033977340236559704
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.017734840702916317
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.001542858998928119
Global Adjusted Rand Index: 0.0005496162883111499
Average Adjusted Rand Index: 0.012848410524137998
9935.044715388682
[-0.0010177607850258056, 0.0005496162883111499] [-0.0015999903951959935, 0.012848410524137998] [9775.72371522455, 9779.987908126583]
-------------------------------------
This iteration is 83
True Objective function: Loss = -9907.448220415668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22707.643868668685
Iteration 100: Loss = -9776.773693837595
Iteration 200: Loss = -9775.513018533587
Iteration 300: Loss = -9774.969013020234
Iteration 400: Loss = -9774.637844917639
Iteration 500: Loss = -9774.252839558274
Iteration 600: Loss = -9773.314288189502
Iteration 700: Loss = -9772.73301355643
Iteration 800: Loss = -9772.44148473472
Iteration 900: Loss = -9772.295937481287
Iteration 1000: Loss = -9772.176584680146
Iteration 1100: Loss = -9772.056144704527
Iteration 1200: Loss = -9771.963280364289
Iteration 1300: Loss = -9771.901664157902
Iteration 1400: Loss = -9771.845328411575
Iteration 1500: Loss = -9771.781560489166
Iteration 1600: Loss = -9771.665152664309
Iteration 1700: Loss = -9771.067647561573
Iteration 1800: Loss = -9770.272723050823
Iteration 1900: Loss = -9770.098515784572
Iteration 2000: Loss = -9770.026255424167
Iteration 2100: Loss = -9769.936116319668
Iteration 2200: Loss = -9769.905192740747
Iteration 2300: Loss = -9769.888744506823
Iteration 2400: Loss = -9769.877740307893
Iteration 2500: Loss = -9769.868649445474
Iteration 2600: Loss = -9769.860658274622
Iteration 2700: Loss = -9769.853264769723
Iteration 2800: Loss = -9769.84633267561
Iteration 2900: Loss = -9769.840385516905
Iteration 3000: Loss = -9769.835670804554
Iteration 3100: Loss = -9769.831478869855
Iteration 3200: Loss = -9769.827188531162
Iteration 3300: Loss = -9769.822098233688
Iteration 3400: Loss = -9769.817253411044
Iteration 3500: Loss = -9769.814216816432
Iteration 3600: Loss = -9769.81182978819
Iteration 3700: Loss = -9769.809564106063
Iteration 3800: Loss = -9769.806949747466
Iteration 3900: Loss = -9769.801925731532
Iteration 4000: Loss = -9769.799503261736
Iteration 4100: Loss = -9769.79846532331
Iteration 4200: Loss = -9769.797553022785
Iteration 4300: Loss = -9769.796724335105
Iteration 4400: Loss = -9769.79591233104
Iteration 4500: Loss = -9769.795209827333
Iteration 4600: Loss = -9769.79452361024
Iteration 4700: Loss = -9769.79390426322
Iteration 4800: Loss = -9769.793342543124
Iteration 4900: Loss = -9769.792785775007
Iteration 5000: Loss = -9769.792299868415
Iteration 5100: Loss = -9769.791809600101
Iteration 5200: Loss = -9769.791399044678
Iteration 5300: Loss = -9769.791001597283
Iteration 5400: Loss = -9769.790571213445
Iteration 5500: Loss = -9769.790253478965
Iteration 5600: Loss = -9769.789883056346
Iteration 5700: Loss = -9769.789546792377
Iteration 5800: Loss = -9769.789243341998
Iteration 5900: Loss = -9769.78892913547
Iteration 6000: Loss = -9769.788646949128
Iteration 6100: Loss = -9769.78837508369
Iteration 6200: Loss = -9769.788093843445
Iteration 6300: Loss = -9769.78783899582
Iteration 6400: Loss = -9769.787636018811
Iteration 6500: Loss = -9769.787361804
Iteration 6600: Loss = -9769.787174484403
Iteration 6700: Loss = -9769.787501845081
1
Iteration 6800: Loss = -9769.786745460058
Iteration 6900: Loss = -9769.786606943866
Iteration 7000: Loss = -9769.786402037595
Iteration 7100: Loss = -9769.786211355668
Iteration 7200: Loss = -9769.786066932264
Iteration 7300: Loss = -9769.786556571846
1
Iteration 7400: Loss = -9769.785800935484
Iteration 7500: Loss = -9769.785692545367
Iteration 7600: Loss = -9769.785526987347
Iteration 7700: Loss = -9769.785853089594
1
Iteration 7800: Loss = -9769.785279417029
Iteration 7900: Loss = -9769.785180791096
Iteration 8000: Loss = -9769.785087179365
Iteration 8100: Loss = -9769.784956737018
Iteration 8200: Loss = -9769.784975044007
Iteration 8300: Loss = -9769.787437596413
1
Iteration 8400: Loss = -9769.786719633963
2
Iteration 8500: Loss = -9769.784603363896
Iteration 8600: Loss = -9769.817037315861
1
Iteration 8700: Loss = -9769.784435135292
Iteration 8800: Loss = -9769.784392998465
Iteration 8900: Loss = -9769.784302222617
Iteration 9000: Loss = -9769.784246426258
Iteration 9100: Loss = -9769.807071252451
1
Iteration 9200: Loss = -9769.784179263637
Iteration 9300: Loss = -9769.784107585281
Iteration 9400: Loss = -9769.784202303772
Iteration 9500: Loss = -9769.783980418431
Iteration 9600: Loss = -9769.784851950872
1
Iteration 9700: Loss = -9769.783889785776
Iteration 9800: Loss = -9769.783999666906
1
Iteration 9900: Loss = -9769.783807042224
Iteration 10000: Loss = -9769.786609667288
1
Iteration 10100: Loss = -9769.783739539154
Iteration 10200: Loss = -9769.788971107899
1
Iteration 10300: Loss = -9769.783659673672
Iteration 10400: Loss = -9769.783623553569
Iteration 10500: Loss = -9769.784202493132
1
Iteration 10600: Loss = -9769.785609113706
2
Iteration 10700: Loss = -9769.787679569
3
Iteration 10800: Loss = -9769.78351804337
Iteration 10900: Loss = -9769.783776650116
1
Iteration 11000: Loss = -9769.783474265425
Iteration 11100: Loss = -9769.783435432557
Iteration 11200: Loss = -9769.783468060097
Iteration 11300: Loss = -9769.783619387243
1
Iteration 11400: Loss = -9769.783389022936
Iteration 11500: Loss = -9770.077937313854
1
Iteration 11600: Loss = -9769.783823290758
2
Iteration 11700: Loss = -9769.783606717208
3
Iteration 11800: Loss = -9769.783340031967
Iteration 11900: Loss = -9769.785998602523
1
Iteration 12000: Loss = -9769.783300021014
Iteration 12100: Loss = -9769.788495318626
1
Iteration 12200: Loss = -9769.783240322045
Iteration 12300: Loss = -9769.784368435487
1
Iteration 12400: Loss = -9769.783248358764
Iteration 12500: Loss = -9769.783673891512
1
Iteration 12600: Loss = -9769.783261600192
Iteration 12700: Loss = -9769.784875833222
1
Iteration 12800: Loss = -9769.783262296836
Iteration 12900: Loss = -9769.783990956557
1
Iteration 13000: Loss = -9769.78319582862
Iteration 13100: Loss = -9769.783165124838
Iteration 13200: Loss = -9769.783159481909
Iteration 13300: Loss = -9769.786115834248
1
Iteration 13400: Loss = -9769.783600724244
2
Iteration 13500: Loss = -9769.783174790553
Iteration 13600: Loss = -9769.792470603868
1
Iteration 13700: Loss = -9769.783162386688
Iteration 13800: Loss = -9770.134845210581
1
Iteration 13900: Loss = -9769.783106047686
Iteration 14000: Loss = -9769.783127623938
Iteration 14100: Loss = -9769.793081768625
1
Iteration 14200: Loss = -9769.783103285481
Iteration 14300: Loss = -9769.784157769347
1
Iteration 14400: Loss = -9769.783097311516
Iteration 14500: Loss = -9769.783094250757
Iteration 14600: Loss = -9769.795601755524
1
Iteration 14700: Loss = -9769.783092515127
Iteration 14800: Loss = -9769.783083968294
Iteration 14900: Loss = -9769.783568436695
1
Iteration 15000: Loss = -9769.783074540865
Iteration 15100: Loss = -9769.783078060396
Iteration 15200: Loss = -9769.783094829376
Iteration 15300: Loss = -9769.78397902194
1
Iteration 15400: Loss = -9769.78309805538
Iteration 15500: Loss = -9769.783067537484
Iteration 15600: Loss = -9769.78338197723
1
Iteration 15700: Loss = -9769.784344587613
2
Iteration 15800: Loss = -9769.847251439367
3
Iteration 15900: Loss = -9769.783066971158
Iteration 16000: Loss = -9769.783463811127
1
Iteration 16100: Loss = -9769.78316225763
Iteration 16200: Loss = -9769.789879721524
1
Iteration 16300: Loss = -9769.790468778723
2
Iteration 16400: Loss = -9769.78366632849
3
Iteration 16500: Loss = -9769.783095646073
Iteration 16600: Loss = -9769.783075682977
Iteration 16700: Loss = -9769.783400943974
1
Iteration 16800: Loss = -9769.786229414456
2
Iteration 16900: Loss = -9769.784350092792
3
Iteration 17000: Loss = -9769.783052647852
Iteration 17100: Loss = -9769.784632925404
1
Iteration 17200: Loss = -9769.783047333702
Iteration 17300: Loss = -9769.783225509153
1
Iteration 17400: Loss = -9769.783048763726
Iteration 17500: Loss = -9769.783049357593
Iteration 17600: Loss = -9769.791724459192
1
Iteration 17700: Loss = -9769.783052032442
Iteration 17800: Loss = -9769.78302923699
Iteration 17900: Loss = -9769.78311097456
Iteration 18000: Loss = -9769.78303422092
Iteration 18100: Loss = -9769.784222365864
1
Iteration 18200: Loss = -9769.78303690936
Iteration 18300: Loss = -9769.783132018507
Iteration 18400: Loss = -9769.783063081508
Iteration 18500: Loss = -9769.78308330583
Iteration 18600: Loss = -9769.783070565196
Iteration 18700: Loss = -9769.783096303727
Iteration 18800: Loss = -9769.783058679712
Iteration 18900: Loss = -9769.7833427599
1
Iteration 19000: Loss = -9769.783067030035
Iteration 19100: Loss = -9769.78709991893
1
Iteration 19200: Loss = -9769.783044049651
Iteration 19300: Loss = -9769.785244427816
1
Iteration 19400: Loss = -9769.783036097637
Iteration 19500: Loss = -9769.819624314163
1
Iteration 19600: Loss = -9769.783069218709
Iteration 19700: Loss = -9769.783431753456
1
Iteration 19800: Loss = -9769.783130330985
Iteration 19900: Loss = -9769.783048120678
pi: tensor([[1.0000e+00, 8.6919e-08],
        [3.5796e-01, 6.4204e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8975, 0.1025], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1310, 0.1607],
         [0.5960, 0.2814]],

        [[0.6955, 0.1866],
         [0.5900, 0.5894]],

        [[0.5710, 0.1668],
         [0.5763, 0.6717]],

        [[0.5820, 0.1681],
         [0.7263, 0.6247]],

        [[0.6668, 0.0695],
         [0.6903, 0.5967]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.03878787878787879
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.018772080923839488
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.007493599095377873
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.007792617437691116
Average Adjusted Rand Index: 0.01226111024573991
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20766.452406823253
Iteration 100: Loss = -9775.931600108599
Iteration 200: Loss = -9775.17277453277
Iteration 300: Loss = -9774.835292276199
Iteration 400: Loss = -9774.565762314267
Iteration 500: Loss = -9774.293840011287
Iteration 600: Loss = -9773.57751770842
Iteration 700: Loss = -9772.572725035312
Iteration 800: Loss = -9772.237951390514
Iteration 900: Loss = -9771.989425663209
Iteration 1000: Loss = -9771.690071293371
Iteration 1100: Loss = -9770.793459283646
Iteration 1200: Loss = -9770.298687380739
Iteration 1300: Loss = -9770.159089925246
Iteration 1400: Loss = -9770.047935347724
Iteration 1500: Loss = -9769.9878799766
Iteration 1600: Loss = -9769.955569662348
Iteration 1700: Loss = -9769.929372625898
Iteration 1800: Loss = -9769.906633674404
Iteration 1900: Loss = -9769.890444531451
Iteration 2000: Loss = -9769.879296810725
Iteration 2100: Loss = -9769.870122109529
Iteration 2200: Loss = -9769.861697285087
Iteration 2300: Loss = -9769.853871791523
Iteration 2400: Loss = -9769.846267399314
Iteration 2500: Loss = -9769.83787191882
Iteration 2600: Loss = -9769.828836940176
Iteration 2700: Loss = -9769.822376618482
Iteration 2800: Loss = -9769.818186685492
Iteration 2900: Loss = -9769.815000984257
Iteration 3000: Loss = -9769.81246174241
Iteration 3100: Loss = -9769.810440672234
Iteration 3200: Loss = -9769.808720693665
Iteration 3300: Loss = -9769.807135393818
Iteration 3400: Loss = -9769.805737449533
Iteration 3500: Loss = -9769.804390161753
Iteration 3600: Loss = -9769.803088390512
Iteration 3700: Loss = -9769.801871661342
Iteration 3800: Loss = -9769.800684284135
Iteration 3900: Loss = -9769.799564650555
Iteration 4000: Loss = -9769.798548235382
Iteration 4100: Loss = -9769.797562776783
Iteration 4200: Loss = -9769.796657327177
Iteration 4300: Loss = -9769.795842856285
Iteration 4400: Loss = -9769.79510604832
Iteration 4500: Loss = -9769.794413082665
Iteration 4600: Loss = -9769.793764383587
Iteration 4700: Loss = -9769.793176995843
Iteration 4800: Loss = -9769.792614975822
Iteration 4900: Loss = -9769.792082639773
Iteration 5000: Loss = -9769.791565429774
Iteration 5100: Loss = -9769.791082858941
Iteration 5200: Loss = -9769.790666310946
Iteration 5300: Loss = -9769.790231393312
Iteration 5400: Loss = -9769.78983845069
Iteration 5500: Loss = -9769.789475164753
Iteration 5600: Loss = -9769.789072441094
Iteration 5700: Loss = -9769.788790690814
Iteration 5800: Loss = -9769.78845339246
Iteration 5900: Loss = -9769.78816610822
Iteration 6000: Loss = -9769.787874618012
Iteration 6100: Loss = -9769.787638456364
Iteration 6200: Loss = -9769.787379380414
Iteration 6300: Loss = -9769.78713216484
Iteration 6400: Loss = -9769.787976528947
1
Iteration 6500: Loss = -9769.786748420513
Iteration 6600: Loss = -9769.786534531224
Iteration 6700: Loss = -9769.786388306837
Iteration 6800: Loss = -9769.78620657047
Iteration 6900: Loss = -9769.786019682424
Iteration 7000: Loss = -9769.785870210168
Iteration 7100: Loss = -9769.785742940909
Iteration 7200: Loss = -9769.785800284742
Iteration 7300: Loss = -9769.785444259209
Iteration 7400: Loss = -9769.79609114908
1
Iteration 7500: Loss = -9769.785221323413
Iteration 7600: Loss = -9769.785153561274
Iteration 7700: Loss = -9769.785614405495
1
Iteration 7800: Loss = -9769.785192559511
Iteration 7900: Loss = -9769.785286781707
Iteration 8000: Loss = -9769.78477951025
Iteration 8100: Loss = -9769.784645810772
Iteration 8200: Loss = -9769.804684524452
1
Iteration 8300: Loss = -9769.784498157644
Iteration 8400: Loss = -9769.791180486302
1
Iteration 8500: Loss = -9769.784638936157
2
Iteration 8600: Loss = -9769.818462620331
3
Iteration 8700: Loss = -9769.784241883484
Iteration 8800: Loss = -9769.7876343733
1
Iteration 8900: Loss = -9769.784120666454
Iteration 9000: Loss = -9769.817633944673
1
Iteration 9100: Loss = -9769.78401392157
Iteration 9200: Loss = -9769.784344662572
1
Iteration 9300: Loss = -9769.78394639573
Iteration 9400: Loss = -9769.926783772211
1
Iteration 9500: Loss = -9769.783851776372
Iteration 9600: Loss = -9769.806928724443
1
Iteration 9700: Loss = -9769.783766043498
Iteration 9800: Loss = -9769.784043687876
1
Iteration 9900: Loss = -9769.783672274676
Iteration 10000: Loss = -9769.783951251047
1
Iteration 10100: Loss = -9769.783774222002
2
Iteration 10200: Loss = -9769.788413818536
3
Iteration 10300: Loss = -9769.783553894258
Iteration 10400: Loss = -9769.783543122678
Iteration 10500: Loss = -9769.783543156773
Iteration 10600: Loss = -9769.78350477974
Iteration 10700: Loss = -9769.799131374364
1
Iteration 10800: Loss = -9769.785793898793
2
Iteration 10900: Loss = -9769.783398054024
Iteration 11000: Loss = -9769.7860770258
1
Iteration 11100: Loss = -9769.783387143581
Iteration 11200: Loss = -9769.78340786037
Iteration 11300: Loss = -9769.783347284207
Iteration 11400: Loss = -9769.783322209507
Iteration 11500: Loss = -9769.783787315522
1
Iteration 11600: Loss = -9769.783312582615
Iteration 11700: Loss = -9769.819604656794
1
Iteration 11800: Loss = -9769.783696472725
2
Iteration 11900: Loss = -9769.78327913075
Iteration 12000: Loss = -9769.78416292175
1
Iteration 12100: Loss = -9769.783251633238
Iteration 12200: Loss = -9769.783352896104
1
Iteration 12300: Loss = -9769.893734164289
2
Iteration 12400: Loss = -9769.785386231368
3
Iteration 12500: Loss = -9769.783197942426
Iteration 12600: Loss = -9769.78346209193
1
Iteration 12700: Loss = -9769.783173879612
Iteration 12800: Loss = -9769.792336176073
1
Iteration 12900: Loss = -9769.783208242183
Iteration 13000: Loss = -9769.797815199383
1
Iteration 13100: Loss = -9769.783175828084
Iteration 13200: Loss = -9769.789043043218
1
Iteration 13300: Loss = -9769.78320867392
Iteration 13400: Loss = -9769.783233864047
Iteration 13500: Loss = -9769.783463713877
1
Iteration 13600: Loss = -9769.809668755464
2
Iteration 13700: Loss = -9769.783138161563
Iteration 13800: Loss = -9769.783145058882
Iteration 13900: Loss = -9769.783097997723
Iteration 14000: Loss = -9769.783310315715
1
Iteration 14100: Loss = -9769.783104146165
Iteration 14200: Loss = -9769.785201024235
1
Iteration 14300: Loss = -9769.783121082366
Iteration 14400: Loss = -9769.804679224162
1
Iteration 14500: Loss = -9769.783099221604
Iteration 14600: Loss = -9769.783552338291
1
Iteration 14700: Loss = -9769.783116241102
Iteration 14800: Loss = -9769.783147258559
Iteration 14900: Loss = -9769.784003221695
1
Iteration 15000: Loss = -9769.783150860807
Iteration 15100: Loss = -9769.788456246568
1
Iteration 15200: Loss = -9769.783447328444
2
Iteration 15300: Loss = -9769.784075492676
3
Iteration 15400: Loss = -9769.926710177531
4
Iteration 15500: Loss = -9769.783072553304
Iteration 15600: Loss = -9769.821903092075
1
Iteration 15700: Loss = -9769.783040869008
Iteration 15800: Loss = -9769.856151887885
1
Iteration 15900: Loss = -9769.786271863966
2
Iteration 16000: Loss = -9769.783279973319
3
Iteration 16100: Loss = -9769.783100853125
Iteration 16200: Loss = -9769.783064179961
Iteration 16300: Loss = -9769.787976087217
1
Iteration 16400: Loss = -9769.783025423121
Iteration 16500: Loss = -9769.783469429864
1
Iteration 16600: Loss = -9769.78309627297
Iteration 16700: Loss = -9769.783063283134
Iteration 16800: Loss = -9769.85243525748
1
Iteration 16900: Loss = -9769.783049383008
Iteration 17000: Loss = -9769.834137481545
1
Iteration 17100: Loss = -9769.783041661556
Iteration 17200: Loss = -9769.783030646407
Iteration 17300: Loss = -9769.783385251312
1
Iteration 17400: Loss = -9769.783039845217
Iteration 17500: Loss = -9769.783608257989
1
Iteration 17600: Loss = -9769.783037435132
Iteration 17700: Loss = -9769.789813473364
1
Iteration 17800: Loss = -9769.784193786136
2
Iteration 17900: Loss = -9769.847329662682
3
Iteration 18000: Loss = -9769.783332784727
4
Iteration 18100: Loss = -9769.797487719305
5
Iteration 18200: Loss = -9769.783025945351
Iteration 18300: Loss = -9769.791261096052
1
Iteration 18400: Loss = -9769.783010963118
Iteration 18500: Loss = -9769.79502777425
1
Iteration 18600: Loss = -9769.783288212233
2
Iteration 18700: Loss = -9769.783091203004
Iteration 18800: Loss = -9769.792834419408
1
Iteration 18900: Loss = -9769.783279236806
2
Iteration 19000: Loss = -9769.783089863256
Iteration 19100: Loss = -9769.887441300165
1
Iteration 19200: Loss = -9769.783007337346
Iteration 19300: Loss = -9770.010387411601
1
Iteration 19400: Loss = -9769.783044267668
Iteration 19500: Loss = -9769.783633186722
1
Iteration 19600: Loss = -9769.783052668345
Iteration 19700: Loss = -9769.817650881114
1
Iteration 19800: Loss = -9769.783034811431
Iteration 19900: Loss = -9769.8013232502
1
pi: tensor([[1.0000e+00, 7.3604e-08],
        [3.5624e-01, 6.4376e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8977, 0.1023], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1311, 0.1606],
         [0.6121, 0.2812]],

        [[0.5190, 0.1866],
         [0.5598, 0.5384]],

        [[0.6461, 0.1667],
         [0.5123, 0.6121]],

        [[0.7268, 0.1681],
         [0.5617, 0.7304]],

        [[0.5296, 0.0696],
         [0.5544, 0.5762]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.03878787878787879
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.018772080923839488
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.007493599095377873
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.007792617437691116
Average Adjusted Rand Index: 0.01226111024573991
9907.448220415668
[0.007792617437691116, 0.007792617437691116] [0.01226111024573991, 0.01226111024573991] [9769.784831028473, 9769.783262740471]
-------------------------------------
This iteration is 84
True Objective function: Loss = -10076.105992540199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23266.993496832678
Iteration 100: Loss = -9921.36335848111
Iteration 200: Loss = -9920.74110226136
Iteration 300: Loss = -9920.62693382984
Iteration 400: Loss = -9920.555508187745
Iteration 500: Loss = -9920.499416178944
Iteration 600: Loss = -9920.4498703408
Iteration 700: Loss = -9920.400012959812
Iteration 800: Loss = -9920.3470622021
Iteration 900: Loss = -9920.280770519697
Iteration 1000: Loss = -9920.122212389593
Iteration 1100: Loss = -9919.638296823867
Iteration 1200: Loss = -9919.226740836177
Iteration 1300: Loss = -9919.003553264469
Iteration 1400: Loss = -9918.918876552256
Iteration 1500: Loss = -9918.875530806969
Iteration 1600: Loss = -9918.844558202101
Iteration 1700: Loss = -9918.818805934223
Iteration 1800: Loss = -9918.79618043641
Iteration 1900: Loss = -9918.775637094435
Iteration 2000: Loss = -9918.756926951792
Iteration 2100: Loss = -9918.73971256884
Iteration 2200: Loss = -9918.723948369121
Iteration 2300: Loss = -9918.709496675512
Iteration 2400: Loss = -9918.696362079037
Iteration 2500: Loss = -9918.684378894295
Iteration 2600: Loss = -9918.673559278968
Iteration 2700: Loss = -9918.663820117212
Iteration 2800: Loss = -9918.65505476366
Iteration 2900: Loss = -9918.647231315963
Iteration 3000: Loss = -9918.640228024467
Iteration 3100: Loss = -9918.634021349975
Iteration 3200: Loss = -9918.628481028261
Iteration 3300: Loss = -9918.623611054782
Iteration 3400: Loss = -9918.61933726513
Iteration 3500: Loss = -9918.615512627593
Iteration 3600: Loss = -9918.612159965693
Iteration 3700: Loss = -9918.609242808601
Iteration 3800: Loss = -9918.60668835166
Iteration 3900: Loss = -9918.604461041237
Iteration 4000: Loss = -9918.602477555267
Iteration 4100: Loss = -9918.60078159168
Iteration 4200: Loss = -9918.599312750079
Iteration 4300: Loss = -9918.598000089572
Iteration 4400: Loss = -9918.596887098909
Iteration 4500: Loss = -9918.595928539262
Iteration 4600: Loss = -9918.595076252399
Iteration 4700: Loss = -9918.594362780705
Iteration 4800: Loss = -9918.593758383733
Iteration 4900: Loss = -9918.59325013579
Iteration 5000: Loss = -9918.5927372779
Iteration 5100: Loss = -9918.59238142673
Iteration 5200: Loss = -9918.591996483572
Iteration 5300: Loss = -9918.591689935954
Iteration 5400: Loss = -9918.591419705326
Iteration 5500: Loss = -9918.591164281801
Iteration 5600: Loss = -9918.591004106698
Iteration 5700: Loss = -9918.590816439058
Iteration 5800: Loss = -9918.590657403709
Iteration 5900: Loss = -9918.590481223366
Iteration 6000: Loss = -9918.590393938413
Iteration 6100: Loss = -9918.590271725318
Iteration 6200: Loss = -9918.590158539642
Iteration 6300: Loss = -9918.590081069664
Iteration 6400: Loss = -9918.589975536213
Iteration 6500: Loss = -9918.589883581692
Iteration 6600: Loss = -9918.58980700851
Iteration 6700: Loss = -9918.589744608389
Iteration 6800: Loss = -9918.589658494233
Iteration 6900: Loss = -9918.589630136175
Iteration 7000: Loss = -9918.58954039722
Iteration 7100: Loss = -9918.589514520176
Iteration 7200: Loss = -9918.58952849556
Iteration 7300: Loss = -9918.589373009929
Iteration 7400: Loss = -9918.589378404808
Iteration 7500: Loss = -9918.589365304353
Iteration 7600: Loss = -9918.589272723915
Iteration 7700: Loss = -9918.616089819747
1
Iteration 7800: Loss = -9918.592930526082
2
Iteration 7900: Loss = -9918.591193139335
3
Iteration 8000: Loss = -9918.734663293628
4
Iteration 8100: Loss = -9918.589088174589
Iteration 8200: Loss = -9918.590389940427
1
Iteration 8300: Loss = -9918.589040158502
Iteration 8400: Loss = -9918.596404425216
1
Iteration 8500: Loss = -9918.588987408957
Iteration 8600: Loss = -9918.589064772243
Iteration 8700: Loss = -9918.588995637252
Iteration 8800: Loss = -9918.58890591567
Iteration 8900: Loss = -9918.649373670582
1
Iteration 9000: Loss = -9918.588854469046
Iteration 9100: Loss = -9918.588855337666
Iteration 9200: Loss = -9918.697430966728
1
Iteration 9300: Loss = -9918.5888367803
Iteration 9400: Loss = -9918.588805255968
Iteration 9500: Loss = -9918.922388394403
1
Iteration 9600: Loss = -9918.588789438007
Iteration 9700: Loss = -9918.588778533982
Iteration 9800: Loss = -9918.588800301755
Iteration 9900: Loss = -9918.588777557812
Iteration 10000: Loss = -9918.58876724465
Iteration 10100: Loss = -9918.588744461122
Iteration 10200: Loss = -9918.589388312048
1
Iteration 10300: Loss = -9918.58870747541
Iteration 10400: Loss = -9918.588718533816
Iteration 10500: Loss = -9918.58869685783
Iteration 10600: Loss = -9918.588701584122
Iteration 10700: Loss = -9918.588685721
Iteration 10800: Loss = -9918.588636411949
Iteration 10900: Loss = -9918.590345517925
1
Iteration 11000: Loss = -9918.588607326117
Iteration 11100: Loss = -9918.588660405956
Iteration 11200: Loss = -9918.616384413106
1
Iteration 11300: Loss = -9918.588669395886
Iteration 11400: Loss = -9918.58863210364
Iteration 11500: Loss = -9919.003084565747
1
Iteration 11600: Loss = -9918.588621622326
Iteration 11700: Loss = -9918.58863107179
Iteration 11800: Loss = -9918.588936159707
1
Iteration 11900: Loss = -9918.588630078244
Iteration 12000: Loss = -9918.588585281806
Iteration 12100: Loss = -9918.588590855385
Iteration 12200: Loss = -9918.588604314284
Iteration 12300: Loss = -9918.588584408224
Iteration 12400: Loss = -9918.588583949097
Iteration 12500: Loss = -9918.588597972152
Iteration 12600: Loss = -9918.588689079003
Iteration 12700: Loss = -9918.588579293546
Iteration 12800: Loss = -9918.588564410815
Iteration 12900: Loss = -9918.656401394153
1
Iteration 13000: Loss = -9918.588554717513
Iteration 13100: Loss = -9918.588559623899
Iteration 13200: Loss = -9918.58855354228
Iteration 13300: Loss = -9918.588837050936
1
Iteration 13400: Loss = -9918.588572004348
Iteration 13500: Loss = -9918.588569542235
Iteration 13600: Loss = -9918.589353892545
1
Iteration 13700: Loss = -9918.588545237924
Iteration 13800: Loss = -9918.588566924536
Iteration 13900: Loss = -9918.80435869417
1
Iteration 14000: Loss = -9918.588580760232
Iteration 14100: Loss = -9918.588565502932
Iteration 14200: Loss = -9918.588871153375
1
Iteration 14300: Loss = -9918.58861295374
Iteration 14400: Loss = -9918.58852488088
Iteration 14500: Loss = -9918.592055359486
1
Iteration 14600: Loss = -9918.590451951679
2
Iteration 14700: Loss = -9918.588562087383
Iteration 14800: Loss = -9918.588923600799
1
Iteration 14900: Loss = -9918.58856118557
Iteration 15000: Loss = -9918.58857191057
Iteration 15100: Loss = -9918.58948469871
1
Iteration 15200: Loss = -9918.588690013616
2
Iteration 15300: Loss = -9918.588562632634
Iteration 15400: Loss = -9918.58858727228
Iteration 15500: Loss = -9918.588582289629
Iteration 15600: Loss = -9918.588572990251
Iteration 15700: Loss = -9918.588967157504
1
Iteration 15800: Loss = -9918.588644915495
Iteration 15900: Loss = -9918.588508698038
Iteration 16000: Loss = -9918.72617249611
1
Iteration 16100: Loss = -9918.588591590998
Iteration 16200: Loss = -9918.592304638027
1
Iteration 16300: Loss = -9918.601112185414
2
Iteration 16400: Loss = -9918.588711169408
3
Iteration 16500: Loss = -9918.588850782457
4
Iteration 16600: Loss = -9918.819479073092
5
Iteration 16700: Loss = -9918.588968898213
6
Iteration 16800: Loss = -9918.614287693475
7
Iteration 16900: Loss = -9918.589205393235
8
Iteration 17000: Loss = -9918.588840395381
9
Iteration 17100: Loss = -9918.588693965075
10
Iteration 17200: Loss = -9918.589086122152
11
Iteration 17300: Loss = -9918.588605896668
Iteration 17400: Loss = -9918.58854607456
Iteration 17500: Loss = -9918.589708561407
1
Iteration 17600: Loss = -9918.58855857768
Iteration 17700: Loss = -9918.590035904455
1
Iteration 17800: Loss = -9918.588570223676
Iteration 17900: Loss = -9918.590413759637
1
Iteration 18000: Loss = -9918.59112432879
2
Iteration 18100: Loss = -9918.589487640014
3
Iteration 18200: Loss = -9918.588636135264
Iteration 18300: Loss = -9918.588682338976
Iteration 18400: Loss = -9918.588600201449
Iteration 18500: Loss = -9918.593104484644
1
Iteration 18600: Loss = -9918.588627845187
Iteration 18700: Loss = -9918.589067950643
1
Iteration 18800: Loss = -9918.588604696206
Iteration 18900: Loss = -9918.588712987656
1
Iteration 19000: Loss = -9918.588717708635
2
Iteration 19100: Loss = -9918.588741107575
3
Iteration 19200: Loss = -9918.69679391816
4
Iteration 19300: Loss = -9918.588639762951
Iteration 19400: Loss = -9918.589461511088
1
Iteration 19500: Loss = -9918.62103921171
2
Iteration 19600: Loss = -9918.588555695176
Iteration 19700: Loss = -9918.58875850783
1
Iteration 19800: Loss = -9918.590539660889
2
Iteration 19900: Loss = -9918.588591404112
pi: tensor([[1.0000e+00, 2.7346e-08],
        [2.2352e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9760, 0.0240], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1366, 0.1319],
         [0.5545, 0.0634]],

        [[0.7043, 0.2215],
         [0.6443, 0.5628]],

        [[0.7048, 0.1452],
         [0.5510, 0.5098]],

        [[0.6950, 0.2059],
         [0.6989, 0.6077]],

        [[0.6902, 0.1312],
         [0.5552, 0.6807]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.560954192727771e-05
Average Adjusted Rand Index: -0.0002760504335195928
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23657.378282308815
Iteration 100: Loss = -9924.004633003955
Iteration 200: Loss = -9922.08042928022
Iteration 300: Loss = -9921.33729990197
Iteration 400: Loss = -9921.036889660934
Iteration 500: Loss = -9920.900197281897
Iteration 600: Loss = -9920.826361141033
Iteration 700: Loss = -9920.78092294651
Iteration 800: Loss = -9920.750141755883
Iteration 900: Loss = -9920.727779754057
Iteration 1000: Loss = -9920.71065634339
Iteration 1100: Loss = -9920.697078017492
Iteration 1200: Loss = -9920.686007490853
Iteration 1300: Loss = -9920.676719059127
Iteration 1400: Loss = -9920.668807290407
Iteration 1500: Loss = -9920.661990133061
Iteration 1600: Loss = -9920.655860878613
Iteration 1700: Loss = -9920.650417856137
Iteration 1800: Loss = -9920.645307087636
Iteration 1900: Loss = -9920.640460129458
Iteration 2000: Loss = -9920.635811745604
Iteration 2100: Loss = -9920.631211106627
Iteration 2200: Loss = -9920.626547585722
Iteration 2300: Loss = -9920.621639760006
Iteration 2400: Loss = -9920.616333578266
Iteration 2500: Loss = -9920.610316511116
Iteration 2600: Loss = -9920.603265434946
Iteration 2700: Loss = -9920.594371493191
Iteration 2800: Loss = -9920.582300405833
Iteration 2900: Loss = -9920.564250718355
Iteration 3000: Loss = -9920.534884673418
Iteration 3100: Loss = -9920.492343799815
Iteration 3200: Loss = -9920.451947657719
Iteration 3300: Loss = -9920.41701994983
Iteration 3400: Loss = -9920.386615209642
Iteration 3500: Loss = -9920.360190538573
Iteration 3600: Loss = -9920.336722210539
Iteration 3700: Loss = -9920.31491220052
Iteration 3800: Loss = -9920.29365281491
Iteration 3900: Loss = -9920.271826533934
Iteration 4000: Loss = -9920.248429722233
Iteration 4100: Loss = -9920.222785330583
Iteration 4200: Loss = -9920.195216032776
Iteration 4300: Loss = -9920.166851177417
Iteration 4400: Loss = -9920.139841893699
Iteration 4500: Loss = -9920.115784630814
Iteration 4600: Loss = -9920.095520244633
Iteration 4700: Loss = -9920.07896336961
Iteration 4800: Loss = -9920.067335501235
Iteration 4900: Loss = -9920.058063798198
Iteration 5000: Loss = -9920.052130463375
Iteration 5100: Loss = -9920.04554984607
Iteration 5200: Loss = -9920.044954914956
Iteration 5300: Loss = -9920.039815325468
Iteration 5400: Loss = -9920.03798814453
Iteration 5500: Loss = -9920.036744370103
Iteration 5600: Loss = -9920.035643175015
Iteration 5700: Loss = -9920.046652241383
1
Iteration 5800: Loss = -9920.034041313142
Iteration 5900: Loss = -9920.033386159863
Iteration 6000: Loss = -9920.032843286112
Iteration 6100: Loss = -9920.032383489697
Iteration 6200: Loss = -9920.031954246704
Iteration 6300: Loss = -9920.031449368355
Iteration 6400: Loss = -9920.031079012722
Iteration 6500: Loss = -9920.030720131928
Iteration 6600: Loss = -9920.030344785016
Iteration 6700: Loss = -9920.030036161317
Iteration 6800: Loss = -9920.029785069773
Iteration 6900: Loss = -9920.029454940086
Iteration 7000: Loss = -9920.052591903192
1
Iteration 7100: Loss = -9920.028941384251
Iteration 7200: Loss = -9920.02869696144
Iteration 7300: Loss = -9920.02853184218
Iteration 7400: Loss = -9920.028302928356
Iteration 7500: Loss = -9920.02808517742
Iteration 7600: Loss = -9920.027919381673
Iteration 7700: Loss = -9920.027725746104
Iteration 7800: Loss = -9920.027708340845
Iteration 7900: Loss = -9920.027375089057
Iteration 8000: Loss = -9920.027245631089
Iteration 8100: Loss = -9920.027364036503
1
Iteration 8200: Loss = -9920.026978754271
Iteration 8300: Loss = -9920.02683993637
Iteration 8400: Loss = -9920.026713636831
Iteration 8500: Loss = -9920.027011793212
1
Iteration 8600: Loss = -9920.026500241152
Iteration 8700: Loss = -9920.036986692845
1
Iteration 8800: Loss = -9920.026260096625
Iteration 8900: Loss = -9920.139629931971
1
Iteration 9000: Loss = -9920.026073486637
Iteration 9100: Loss = -9920.026020121772
Iteration 9200: Loss = -9920.0284104921
1
Iteration 9300: Loss = -9920.025876934984
Iteration 9400: Loss = -9920.025778648007
Iteration 9500: Loss = -9920.025796277858
Iteration 9600: Loss = -9920.02563921588
Iteration 9700: Loss = -9920.1106434714
1
Iteration 9800: Loss = -9920.025521949623
Iteration 9900: Loss = -9920.025442987046
Iteration 10000: Loss = -9920.054863438125
1
Iteration 10100: Loss = -9920.025460278543
Iteration 10200: Loss = -9920.02529550687
Iteration 10300: Loss = -9920.037303070909
1
Iteration 10400: Loss = -9920.025200317003
Iteration 10500: Loss = -9920.030787733898
1
Iteration 10600: Loss = -9920.025165450717
Iteration 10700: Loss = -9920.025382838534
1
Iteration 10800: Loss = -9920.025166748128
Iteration 10900: Loss = -9920.02502151338
Iteration 11000: Loss = -9920.025130379088
1
Iteration 11100: Loss = -9920.024937403306
Iteration 11200: Loss = -9920.025242353608
1
Iteration 11300: Loss = -9920.02588976438
2
Iteration 11400: Loss = -9920.062002863033
3
Iteration 11500: Loss = -9920.024884137745
Iteration 11600: Loss = -9920.076090336948
1
Iteration 11700: Loss = -9920.024855547106
Iteration 11800: Loss = -9920.033444301245
1
Iteration 11900: Loss = -9920.024774012061
Iteration 12000: Loss = -9920.056316399641
1
Iteration 12100: Loss = -9920.024739431892
Iteration 12200: Loss = -9920.031044735935
1
Iteration 12300: Loss = -9920.024722169162
Iteration 12400: Loss = -9920.027383903633
1
Iteration 12500: Loss = -9920.05226495699
2
Iteration 12600: Loss = -9920.024667776834
Iteration 12700: Loss = -9920.024772945642
1
Iteration 12800: Loss = -9920.026449526858
2
Iteration 12900: Loss = -9920.024684555148
Iteration 13000: Loss = -9920.032937087375
1
Iteration 13100: Loss = -9920.024664474025
Iteration 13200: Loss = -9920.327080099023
1
Iteration 13300: Loss = -9920.024623924468
Iteration 13400: Loss = -9920.028085293665
1
Iteration 13500: Loss = -9920.024593980635
Iteration 13600: Loss = -9920.025350294458
1
Iteration 13700: Loss = -9920.028007232446
2
Iteration 13800: Loss = -9920.040440458335
3
Iteration 13900: Loss = -9920.049684164125
4
Iteration 14000: Loss = -9920.024556739769
Iteration 14100: Loss = -9920.024922364497
1
Iteration 14200: Loss = -9920.025288319881
2
Iteration 14300: Loss = -9920.024530458199
Iteration 14400: Loss = -9920.02563473115
1
Iteration 14500: Loss = -9920.028320299043
2
Iteration 14600: Loss = -9920.024560858406
Iteration 14700: Loss = -9920.029423794194
1
Iteration 14800: Loss = -9920.027577366833
2
Iteration 14900: Loss = -9920.024772260831
3
Iteration 15000: Loss = -9920.024861595335
4
Iteration 15100: Loss = -9920.040678813559
5
Iteration 15200: Loss = -9920.129021149061
6
Iteration 15300: Loss = -9920.045190068686
7
Iteration 15400: Loss = -9920.027109680675
8
Iteration 15500: Loss = -9920.024846627623
9
Iteration 15600: Loss = -9920.075697094217
10
Iteration 15700: Loss = -9920.024428953413
Iteration 15800: Loss = -9920.024791011858
1
Iteration 15900: Loss = -9920.029595256023
2
Iteration 16000: Loss = -9920.024517213262
Iteration 16100: Loss = -9920.039489497609
1
Iteration 16200: Loss = -9920.026016378642
2
Iteration 16300: Loss = -9920.02463964792
3
Iteration 16400: Loss = -9920.02500131508
4
Iteration 16500: Loss = -9920.03089700085
5
Iteration 16600: Loss = -9920.024445324407
Iteration 16700: Loss = -9920.03334793362
1
Iteration 16800: Loss = -9920.032041460332
2
Iteration 16900: Loss = -9920.033783767003
3
Iteration 17000: Loss = -9920.025879289024
4
Iteration 17100: Loss = -9920.036696399462
5
Iteration 17200: Loss = -9920.08009161974
6
Iteration 17300: Loss = -9920.027851316743
7
Iteration 17400: Loss = -9920.02445967543
Iteration 17500: Loss = -9920.025333051368
1
Iteration 17600: Loss = -9920.025779148986
2
Iteration 17700: Loss = -9920.024874897601
3
Iteration 17800: Loss = -9920.024506975176
Iteration 17900: Loss = -9920.026180002591
1
Iteration 18000: Loss = -9920.096220161398
2
Iteration 18100: Loss = -9920.072836194577
3
Iteration 18200: Loss = -9920.024455361823
Iteration 18300: Loss = -9920.024888253865
1
Iteration 18400: Loss = -9920.285961046227
2
Iteration 18500: Loss = -9920.02444790089
Iteration 18600: Loss = -9920.024790817017
1
Iteration 18700: Loss = -9920.024469820544
Iteration 18800: Loss = -9920.077062880768
1
Iteration 18900: Loss = -9920.029715184348
2
Iteration 19000: Loss = -9920.024425057316
Iteration 19100: Loss = -9920.024875556961
1
Iteration 19200: Loss = -9920.03093062097
2
Iteration 19300: Loss = -9920.02724163072
3
Iteration 19400: Loss = -9920.028697388914
4
Iteration 19500: Loss = -9920.024695660559
5
Iteration 19600: Loss = -9920.024454358048
Iteration 19700: Loss = -9920.025613569718
1
Iteration 19800: Loss = -9920.025321642097
2
Iteration 19900: Loss = -9920.024711189813
3
pi: tensor([[9.0836e-01, 9.1643e-02],
        [9.9998e-01, 2.2386e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9914, 0.0086], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1366, 0.0565],
         [0.5041, 0.1560]],

        [[0.5138, 0.1473],
         [0.5522, 0.6259]],

        [[0.6362, 0.1344],
         [0.6199, 0.5225]],

        [[0.7186, 0.1581],
         [0.5505, 0.6114]],

        [[0.6316, 0.1447],
         [0.6444, 0.5539]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00021790095408848675
Average Adjusted Rand Index: -0.00015692302765368048
10076.105992540199
[-9.560954192727771e-05, -0.00021790095408848675] [-0.0002760504335195928, -0.00015692302765368048] [9918.598598897035, 9920.028640080778]
-------------------------------------
This iteration is 85
True Objective function: Loss = -10110.611975058191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20968.10725142488
Iteration 100: Loss = -9959.20629016414
Iteration 200: Loss = -9958.044838907817
Iteration 300: Loss = -9957.580678401018
Iteration 400: Loss = -9957.363839900405
Iteration 500: Loss = -9957.268946317177
Iteration 600: Loss = -9957.22068469935
Iteration 700: Loss = -9957.189417937863
Iteration 800: Loss = -9957.164382289075
Iteration 900: Loss = -9957.141492591642
Iteration 1000: Loss = -9957.118821197237
Iteration 1100: Loss = -9957.095359496663
Iteration 1200: Loss = -9957.070707679683
Iteration 1300: Loss = -9957.044652801045
Iteration 1400: Loss = -9957.017262732503
Iteration 1500: Loss = -9956.98849154091
Iteration 1600: Loss = -9956.958587265684
Iteration 1700: Loss = -9956.927581937236
Iteration 1800: Loss = -9956.895800070914
Iteration 1900: Loss = -9956.863654325256
Iteration 2000: Loss = -9956.831734414514
Iteration 2100: Loss = -9956.80078164849
Iteration 2200: Loss = -9956.771651487466
Iteration 2300: Loss = -9956.745238784388
Iteration 2400: Loss = -9956.72229016462
Iteration 2500: Loss = -9956.70308811386
Iteration 2600: Loss = -9956.687331236883
Iteration 2700: Loss = -9956.674258476414
Iteration 2800: Loss = -9956.662766542104
Iteration 2900: Loss = -9956.652058756943
Iteration 3000: Loss = -9956.64138807827
Iteration 3100: Loss = -9956.630347123797
Iteration 3200: Loss = -9956.618397667205
Iteration 3300: Loss = -9956.605033496704
Iteration 3400: Loss = -9956.589717269699
Iteration 3500: Loss = -9956.57156438155
Iteration 3600: Loss = -9956.549298806647
Iteration 3700: Loss = -9956.520121255213
Iteration 3800: Loss = -9956.47488706592
Iteration 3900: Loss = -9956.351586293711
Iteration 4000: Loss = -9955.9887389576
Iteration 4100: Loss = -9955.551702495333
Iteration 4200: Loss = -9955.247833441626
Iteration 4300: Loss = -9955.131175196504
Iteration 4400: Loss = -9955.080485707918
Iteration 4500: Loss = -9955.052940780044
Iteration 4600: Loss = -9955.035732966337
Iteration 4700: Loss = -9955.023952965174
Iteration 4800: Loss = -9955.015375797353
Iteration 4900: Loss = -9955.008903471646
Iteration 5000: Loss = -9955.003751692
Iteration 5100: Loss = -9954.999563637928
Iteration 5200: Loss = -9954.996160154167
Iteration 5300: Loss = -9954.99327153143
Iteration 5400: Loss = -9954.990810338231
Iteration 5500: Loss = -9954.988678820831
Iteration 5600: Loss = -9954.986816133132
Iteration 5700: Loss = -9954.985188842493
Iteration 5800: Loss = -9954.983735332664
Iteration 5900: Loss = -9954.982412101239
Iteration 6000: Loss = -9954.981292153921
Iteration 6100: Loss = -9954.980191179293
Iteration 6200: Loss = -9954.979237071777
Iteration 6300: Loss = -9954.978372118454
Iteration 6400: Loss = -9954.977576113313
Iteration 6500: Loss = -9954.976875441504
Iteration 6600: Loss = -9954.976180422405
Iteration 6700: Loss = -9954.975579588689
Iteration 6800: Loss = -9954.974996158691
Iteration 6900: Loss = -9954.974464020901
Iteration 7000: Loss = -9954.973973578719
Iteration 7100: Loss = -9954.973502066383
Iteration 7200: Loss = -9954.97303327981
Iteration 7300: Loss = -9954.972635811162
Iteration 7400: Loss = -9954.972229258501
Iteration 7500: Loss = -9954.97183975498
Iteration 7600: Loss = -9954.971499619349
Iteration 7700: Loss = -9954.97113690012
Iteration 7800: Loss = -9954.970788898821
Iteration 7900: Loss = -9954.970545148084
Iteration 8000: Loss = -9954.970265189786
Iteration 8100: Loss = -9954.972105272036
1
Iteration 8200: Loss = -9954.970592025666
2
Iteration 8300: Loss = -9954.999263888942
3
Iteration 8400: Loss = -9954.97166814
4
Iteration 8500: Loss = -9954.969226484365
Iteration 8600: Loss = -9954.969350676658
1
Iteration 8700: Loss = -9954.969086732892
Iteration 8800: Loss = -9954.968761449407
Iteration 8900: Loss = -9954.96856550515
Iteration 9000: Loss = -9954.968576619585
Iteration 9100: Loss = -9954.968308155372
Iteration 9200: Loss = -9954.968234764114
Iteration 9300: Loss = -9954.968441032494
1
Iteration 9400: Loss = -9954.967990536858
Iteration 9500: Loss = -9954.967895655212
Iteration 9600: Loss = -9954.968810421675
1
Iteration 9700: Loss = -9954.967708979113
Iteration 9800: Loss = -9954.967621350479
Iteration 9900: Loss = -9954.967475580022
Iteration 10000: Loss = -9954.967638659615
1
Iteration 10100: Loss = -9954.967352650407
Iteration 10200: Loss = -9954.96728939137
Iteration 10300: Loss = -9954.9707360992
1
Iteration 10400: Loss = -9954.967126122117
Iteration 10500: Loss = -9954.967097191651
Iteration 10600: Loss = -9954.977436851213
1
Iteration 10700: Loss = -9954.966875369766
Iteration 10800: Loss = -9954.966739796178
Iteration 10900: Loss = -9955.346555425569
1
Iteration 11000: Loss = -9954.966654538805
Iteration 11100: Loss = -9954.966613122258
Iteration 11200: Loss = -9954.966565812732
Iteration 11300: Loss = -9954.966611060148
Iteration 11400: Loss = -9954.966463030709
Iteration 11500: Loss = -9954.966469641453
Iteration 11600: Loss = -9954.966423583983
Iteration 11700: Loss = -9954.966367577006
Iteration 11800: Loss = -9954.966355113296
Iteration 11900: Loss = -9954.96648866596
1
Iteration 12000: Loss = -9954.966276544676
Iteration 12100: Loss = -9954.966270865534
Iteration 12200: Loss = -9954.966357490888
Iteration 12300: Loss = -9954.966186895554
Iteration 12400: Loss = -9954.96620772191
Iteration 12500: Loss = -9954.966181560747
Iteration 12600: Loss = -9954.966154010166
Iteration 12700: Loss = -9954.9890812954
1
Iteration 12800: Loss = -9954.966092340777
Iteration 12900: Loss = -9954.967813010462
1
Iteration 13000: Loss = -9954.969555304817
2
Iteration 13100: Loss = -9954.967257506863
3
Iteration 13200: Loss = -9954.966059596956
Iteration 13300: Loss = -9954.972726610036
1
Iteration 13400: Loss = -9954.966017468425
Iteration 13500: Loss = -9954.966855776172
1
Iteration 13600: Loss = -9954.966023481675
Iteration 13700: Loss = -9954.966006359547
Iteration 13800: Loss = -9954.966029489253
Iteration 13900: Loss = -9954.965944756423
Iteration 14000: Loss = -9954.966941938317
1
Iteration 14100: Loss = -9954.96649044156
2
Iteration 14200: Loss = -9955.089264146873
3
Iteration 14300: Loss = -9954.965910379999
Iteration 14400: Loss = -9954.966954091164
1
Iteration 14500: Loss = -9954.970272032457
2
Iteration 14600: Loss = -9954.965923140615
Iteration 14700: Loss = -9954.976642416386
1
Iteration 14800: Loss = -9954.965909089087
Iteration 14900: Loss = -9954.967512064604
1
Iteration 15000: Loss = -9954.997614469377
2
Iteration 15100: Loss = -9954.965884934385
Iteration 15200: Loss = -9954.966139675633
1
Iteration 15300: Loss = -9955.109759901261
2
Iteration 15400: Loss = -9954.965877076127
Iteration 15500: Loss = -9954.966671036953
1
Iteration 15600: Loss = -9954.970697946885
2
Iteration 15700: Loss = -9954.965856187184
Iteration 15800: Loss = -9954.966824172063
1
Iteration 15900: Loss = -9954.974988080108
2
Iteration 16000: Loss = -9954.966092486018
3
Iteration 16100: Loss = -9954.99040703949
4
Iteration 16200: Loss = -9954.965817450584
Iteration 16300: Loss = -9954.965807817332
Iteration 16400: Loss = -9954.966453933472
1
Iteration 16500: Loss = -9954.965800130829
Iteration 16600: Loss = -9955.006989719275
1
Iteration 16700: Loss = -9954.965832626898
Iteration 16800: Loss = -9954.965938915226
1
Iteration 16900: Loss = -9954.974353141897
2
Iteration 17000: Loss = -9954.96581568945
Iteration 17100: Loss = -9954.969262771689
1
Iteration 17200: Loss = -9954.965972146563
2
Iteration 17300: Loss = -9954.966471405969
3
Iteration 17400: Loss = -9954.965848146188
Iteration 17500: Loss = -9954.975508878822
1
Iteration 17600: Loss = -9954.965801510594
Iteration 17700: Loss = -9954.965881772194
Iteration 17800: Loss = -9954.965781549143
Iteration 17900: Loss = -9954.998181874142
1
Iteration 18000: Loss = -9954.967662726034
2
Iteration 18100: Loss = -9954.966850026329
3
Iteration 18200: Loss = -9954.977705295005
4
Iteration 18300: Loss = -9954.965771815396
Iteration 18400: Loss = -9954.984899861685
1
Iteration 18500: Loss = -9954.965783011647
Iteration 18600: Loss = -9954.968042627852
1
Iteration 18700: Loss = -9954.965776166428
Iteration 18800: Loss = -9954.966558972414
1
Iteration 18900: Loss = -9954.96578803695
Iteration 19000: Loss = -9954.978664573713
1
Iteration 19100: Loss = -9954.965762732765
Iteration 19200: Loss = -9955.012356112342
1
Iteration 19300: Loss = -9954.965770132489
Iteration 19400: Loss = -9954.996508712433
1
Iteration 19500: Loss = -9954.965883035813
2
Iteration 19600: Loss = -9954.965966814123
3
Iteration 19700: Loss = -9954.965809961686
Iteration 19800: Loss = -9954.96600319404
1
Iteration 19900: Loss = -9954.972017061777
2
pi: tensor([[9.9998e-01, 1.7336e-05],
        [4.9791e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0364, 0.9636], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1161, 0.1062],
         [0.6195, 0.1369]],

        [[0.5699, 0.1946],
         [0.6910, 0.5157]],

        [[0.7214, 0.1434],
         [0.5391, 0.6069]],

        [[0.7069, 0.2023],
         [0.5935, 0.6284]],

        [[0.6084, 0.1618],
         [0.6849, 0.5751]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.007493599095377873
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.01721950551250514
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.0077756655156993705
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
Global Adjusted Rand Index: 0.011847937261732756
Average Adjusted Rand Index: 0.008175734463970373
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21075.54861935729
Iteration 100: Loss = -9958.408409279627
Iteration 200: Loss = -9957.607903174556
Iteration 300: Loss = -9957.361337182303
Iteration 400: Loss = -9957.246762030765
Iteration 500: Loss = -9957.186297844835
Iteration 600: Loss = -9957.146971828091
Iteration 700: Loss = -9957.116552952784
Iteration 800: Loss = -9957.088093194554
Iteration 900: Loss = -9957.057710614676
Iteration 1000: Loss = -9957.02494924181
Iteration 1100: Loss = -9956.992185992025
Iteration 1200: Loss = -9956.963216614939
Iteration 1300: Loss = -9956.940009511423
Iteration 1400: Loss = -9956.922611255477
Iteration 1500: Loss = -9956.909652737704
Iteration 1600: Loss = -9956.899477574432
Iteration 1700: Loss = -9956.890940700681
Iteration 1800: Loss = -9956.883489067515
Iteration 1900: Loss = -9956.876793572901
Iteration 2000: Loss = -9956.870522365223
Iteration 2100: Loss = -9956.864577947432
Iteration 2200: Loss = -9956.858700425328
Iteration 2300: Loss = -9956.85280505111
Iteration 2400: Loss = -9956.846784639383
Iteration 2500: Loss = -9956.840619031658
Iteration 2600: Loss = -9956.834279207842
Iteration 2700: Loss = -9956.82794813857
Iteration 2800: Loss = -9956.821915251048
Iteration 2900: Loss = -9956.816542102822
Iteration 3000: Loss = -9956.812247680431
Iteration 3100: Loss = -9956.80919378413
Iteration 3200: Loss = -9956.807232780091
Iteration 3300: Loss = -9956.806047800439
Iteration 3400: Loss = -9956.805322550608
Iteration 3500: Loss = -9956.804823456836
Iteration 3600: Loss = -9956.804485136947
Iteration 3700: Loss = -9956.804231161603
Iteration 3800: Loss = -9956.804083907942
Iteration 3900: Loss = -9956.803924505799
Iteration 4000: Loss = -9956.803787060639
Iteration 4100: Loss = -9956.803718652842
Iteration 4200: Loss = -9956.80363977299
Iteration 4300: Loss = -9956.803537082285
Iteration 4400: Loss = -9956.803524298177
Iteration 4500: Loss = -9956.813183985107
1
Iteration 4600: Loss = -9956.803424332995
Iteration 4700: Loss = -9956.803388324563
Iteration 4800: Loss = -9956.803361254182
Iteration 4900: Loss = -9956.803322189131
Iteration 5000: Loss = -9956.803496369408
1
Iteration 5100: Loss = -9956.803276473933
Iteration 5200: Loss = -9956.80327945929
Iteration 5300: Loss = -9956.803256373436
Iteration 5400: Loss = -9956.803240796693
Iteration 5500: Loss = -9956.804864341093
1
Iteration 5600: Loss = -9956.803212154698
Iteration 5700: Loss = -9956.803933757934
1
Iteration 5800: Loss = -9956.803169661962
Iteration 5900: Loss = -9956.80318204022
Iteration 6000: Loss = -9956.803210225746
Iteration 6100: Loss = -9956.803223678855
Iteration 6200: Loss = -9956.808252990317
1
Iteration 6300: Loss = -9956.803153605939
Iteration 6400: Loss = -9956.803150359006
Iteration 6500: Loss = -9956.803187113897
Iteration 6600: Loss = -9956.803182678008
Iteration 6700: Loss = -9956.803364570507
1
Iteration 6800: Loss = -9956.803164903238
Iteration 6900: Loss = -9956.803156732061
Iteration 7000: Loss = -9956.803171248843
Iteration 7100: Loss = -9956.80449726618
1
Iteration 7200: Loss = -9956.803277980584
2
Iteration 7300: Loss = -9956.803185954383
Iteration 7400: Loss = -9956.804789752106
1
Iteration 7500: Loss = -9956.803193660144
Iteration 7600: Loss = -9956.80317072579
Iteration 7700: Loss = -9956.803393850776
1
Iteration 7800: Loss = -9956.803124779593
Iteration 7900: Loss = -9956.803281877113
1
Iteration 8000: Loss = -9956.803112219204
Iteration 8100: Loss = -9956.803146158107
Iteration 8200: Loss = -9956.803610829262
1
Iteration 8300: Loss = -9956.80360859382
2
Iteration 8400: Loss = -9956.803141821847
Iteration 8500: Loss = -9956.817202818323
1
Iteration 8600: Loss = -9956.803099427774
Iteration 8700: Loss = -9956.806918630442
1
Iteration 8800: Loss = -9956.803113825936
Iteration 8900: Loss = -9956.80712191208
1
Iteration 9000: Loss = -9956.803106481324
Iteration 9100: Loss = -9956.827320211962
1
Iteration 9200: Loss = -9956.803061177108
Iteration 9300: Loss = -9956.803081760572
Iteration 9400: Loss = -9956.821572052299
1
Iteration 9500: Loss = -9956.803006113325
Iteration 9600: Loss = -9956.802989308326
Iteration 9700: Loss = -9956.804473631506
1
Iteration 9800: Loss = -9956.802963626013
Iteration 9900: Loss = -9956.802936458116
Iteration 10000: Loss = -9956.802922879386
Iteration 10100: Loss = -9956.802851484348
Iteration 10200: Loss = -9956.802733584942
Iteration 10300: Loss = -9956.802738339018
Iteration 10400: Loss = -9956.802506484553
Iteration 10500: Loss = -9956.802668290105
1
Iteration 10600: Loss = -9956.80729002935
2
Iteration 10700: Loss = -9956.839559758635
3
Iteration 10800: Loss = -9956.801967482159
Iteration 10900: Loss = -9956.814865076383
1
Iteration 11000: Loss = -9956.803838453301
2
Iteration 11100: Loss = -9956.802228045999
3
Iteration 11200: Loss = -9956.805385908636
4
Iteration 11300: Loss = -9956.80474726539
5
Iteration 11400: Loss = -9956.80218544075
6
Iteration 11500: Loss = -9956.802415718932
7
Iteration 11600: Loss = -9956.80223682897
8
Iteration 11700: Loss = -9956.802718680077
9
Iteration 11800: Loss = -9956.805906711801
10
Iteration 11900: Loss = -9956.803775946439
11
Iteration 12000: Loss = -9956.801779301668
Iteration 12100: Loss = -9956.800370588608
Iteration 12200: Loss = -9956.809154825236
1
Iteration 12300: Loss = -9956.818599940347
2
Iteration 12400: Loss = -9956.81289169678
3
Iteration 12500: Loss = -9956.7996948795
Iteration 12600: Loss = -9956.800088424674
1
Iteration 12700: Loss = -9956.810001476038
2
Iteration 12800: Loss = -9956.799781075264
Iteration 12900: Loss = -9956.811537902147
1
Iteration 13000: Loss = -9956.953628435664
2
Iteration 13100: Loss = -9956.80787126474
3
Iteration 13200: Loss = -9956.798996605618
Iteration 13300: Loss = -9956.798837741973
Iteration 13400: Loss = -9956.79906773677
1
Iteration 13500: Loss = -9956.797861750487
Iteration 13600: Loss = -9956.798193196635
1
Iteration 13700: Loss = -9956.796781779372
Iteration 13800: Loss = -9956.79661503076
Iteration 13900: Loss = -9956.795823104532
Iteration 14000: Loss = -9956.794871684282
Iteration 14100: Loss = -9956.810738833088
1
Iteration 14200: Loss = -9956.806354771654
2
Iteration 14300: Loss = -9956.789824162615
Iteration 14400: Loss = -9956.883392343712
1
Iteration 14500: Loss = -9956.794214201364
2
Iteration 14600: Loss = -9956.8017527713
3
Iteration 14700: Loss = -9956.747027950727
Iteration 14800: Loss = -9955.000393745688
Iteration 14900: Loss = -9954.977382197925
Iteration 15000: Loss = -9954.973537812048
Iteration 15100: Loss = -9954.971751399868
Iteration 15200: Loss = -9954.970553818994
Iteration 15300: Loss = -9954.970479387865
Iteration 15400: Loss = -9954.96912907746
Iteration 15500: Loss = -9954.973218970365
1
Iteration 15600: Loss = -9954.97235772926
2
Iteration 15700: Loss = -9954.96809401681
Iteration 15800: Loss = -9954.96783479695
Iteration 15900: Loss = -9954.967590393053
Iteration 16000: Loss = -9954.967879537857
1
Iteration 16100: Loss = -9954.967279739933
Iteration 16200: Loss = -9955.042272368644
1
Iteration 16300: Loss = -9954.970852862161
2
Iteration 16400: Loss = -9954.983201410398
3
Iteration 16500: Loss = -9954.966828682327
Iteration 16600: Loss = -9955.117821259575
1
Iteration 16700: Loss = -9954.967915383595
2
Iteration 16800: Loss = -9954.967561031228
3
Iteration 16900: Loss = -9954.966552120133
Iteration 17000: Loss = -9954.966980120165
1
Iteration 17100: Loss = -9954.97190850779
2
Iteration 17200: Loss = -9954.966391723856
Iteration 17300: Loss = -9955.003317809045
1
Iteration 17400: Loss = -9954.966976525668
2
Iteration 17500: Loss = -9954.966319948911
Iteration 17600: Loss = -9955.040398391902
1
Iteration 17700: Loss = -9954.96621919503
Iteration 17800: Loss = -9954.967683010396
1
Iteration 17900: Loss = -9954.977436542793
2
Iteration 18000: Loss = -9954.966184610053
Iteration 18100: Loss = -9954.966783830909
1
Iteration 18200: Loss = -9955.12229063949
2
Iteration 18300: Loss = -9954.9660588186
Iteration 18400: Loss = -9954.96636843835
1
Iteration 18500: Loss = -9954.967688769042
2
Iteration 18600: Loss = -9954.968891445276
3
Iteration 18700: Loss = -9954.966648199888
4
Iteration 18800: Loss = -9954.96897316996
5
Iteration 18900: Loss = -9954.975653107414
6
Iteration 19000: Loss = -9954.966640996823
7
Iteration 19100: Loss = -9954.990011121157
8
Iteration 19200: Loss = -9954.965978426562
Iteration 19300: Loss = -9955.0115326393
1
Iteration 19400: Loss = -9954.965959477699
Iteration 19500: Loss = -9954.96596933982
Iteration 19600: Loss = -9955.007155461837
1
Iteration 19700: Loss = -9954.965905417943
Iteration 19800: Loss = -9954.966052357573
1
Iteration 19900: Loss = -9955.106553791165
2
pi: tensor([[1.0000e+00, 5.1686e-07],
        [4.4166e-04, 9.9956e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9635, 0.0365], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1368, 0.1063],
         [0.6055, 0.1161]],

        [[0.5089, 0.1945],
         [0.6661, 0.6765]],

        [[0.6219, 0.1444],
         [0.5496, 0.6901]],

        [[0.6729, 0.2020],
         [0.5285, 0.6263]],

        [[0.7199, 0.1618],
         [0.6456, 0.6768]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.007493599095377873
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.01721950551250514
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.0077756655156993705
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
Global Adjusted Rand Index: 0.011847937261732756
Average Adjusted Rand Index: 0.008175734463970373
10110.611975058191
[0.011847937261732756, 0.011847937261732756] [0.008175734463970373, 0.008175734463970373] [9954.966454277328, 9954.967419735816]
-------------------------------------
This iteration is 86
True Objective function: Loss = -10054.045589332021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23956.63371262411
Iteration 100: Loss = -9950.05344052457
Iteration 200: Loss = -9948.71832583043
Iteration 300: Loss = -9948.336703901765
Iteration 400: Loss = -9948.137135102252
Iteration 500: Loss = -9948.01209352923
Iteration 600: Loss = -9947.925311825276
Iteration 700: Loss = -9947.858940305987
Iteration 800: Loss = -9947.802358211127
Iteration 900: Loss = -9947.748085350055
Iteration 1000: Loss = -9947.69001265113
Iteration 1100: Loss = -9947.621803400029
Iteration 1200: Loss = -9947.536598467268
Iteration 1300: Loss = -9947.432061007656
Iteration 1400: Loss = -9947.338280804232
Iteration 1500: Loss = -9947.304513135063
Iteration 1600: Loss = -9947.293651025797
Iteration 1700: Loss = -9947.285757564272
Iteration 1800: Loss = -9947.279274457089
Iteration 1900: Loss = -9947.27345209154
Iteration 2000: Loss = -9947.267510279962
Iteration 2100: Loss = -9947.260411181047
Iteration 2200: Loss = -9947.250149005102
Iteration 2300: Loss = -9947.233309688501
Iteration 2400: Loss = -9947.21410181728
Iteration 2500: Loss = -9947.204236930786
Iteration 2600: Loss = -9947.198715848179
Iteration 2700: Loss = -9947.194017464913
Iteration 2800: Loss = -9947.189150076161
Iteration 2900: Loss = -9947.183346328464
Iteration 3000: Loss = -9947.175853194072
Iteration 3100: Loss = -9947.165127313698
Iteration 3200: Loss = -9947.146778403127
Iteration 3300: Loss = -9947.097495193228
Iteration 3400: Loss = -9944.128832466491
Iteration 3500: Loss = -9940.33262049495
Iteration 3600: Loss = -9938.135802491495
Iteration 3700: Loss = -9924.303090895333
Iteration 3800: Loss = -9923.450826831828
Iteration 3900: Loss = -9923.39717763085
Iteration 4000: Loss = -9923.393705347426
Iteration 4100: Loss = -9923.390516232043
Iteration 4200: Loss = -9923.387459899153
Iteration 4300: Loss = -9923.381641295524
Iteration 4400: Loss = -9923.370718231183
Iteration 4500: Loss = -9921.925269996205
Iteration 4600: Loss = -9921.372595702122
Iteration 4700: Loss = -9921.12192509016
Iteration 4800: Loss = -9920.80217285867
Iteration 4900: Loss = -9920.750682986183
Iteration 5000: Loss = -9920.50862962673
Iteration 5100: Loss = -9920.493624380313
Iteration 5200: Loss = -9920.486491054362
Iteration 5300: Loss = -9920.481707540916
Iteration 5400: Loss = -9920.473939456157
Iteration 5500: Loss = -9920.468497944017
Iteration 5600: Loss = -9920.46772998931
Iteration 5700: Loss = -9920.461159254597
Iteration 5800: Loss = -9920.457399131477
Iteration 5900: Loss = -9920.413051454974
Iteration 6000: Loss = -9920.411292942474
Iteration 6100: Loss = -9920.406506694784
Iteration 6200: Loss = -9920.36250193077
Iteration 6300: Loss = -9920.349708710175
Iteration 6400: Loss = -9920.34112090595
Iteration 6500: Loss = -9920.328982512174
Iteration 6600: Loss = -9920.328702924457
Iteration 6700: Loss = -9920.29071236562
Iteration 6800: Loss = -9920.290280727104
Iteration 6900: Loss = -9920.290781469812
1
Iteration 7000: Loss = -9920.287766226187
Iteration 7100: Loss = -9920.276367283665
Iteration 7200: Loss = -9920.275590479649
Iteration 7300: Loss = -9920.270820463244
Iteration 7400: Loss = -9920.284793179379
1
Iteration 7500: Loss = -9920.22394398623
Iteration 7600: Loss = -9920.217538729126
Iteration 7700: Loss = -9920.214795213064
Iteration 7800: Loss = -9920.181797783276
Iteration 7900: Loss = -9920.09987719696
Iteration 8000: Loss = -9920.067345557258
Iteration 8100: Loss = -9920.067251545608
Iteration 8200: Loss = -9920.06718123601
Iteration 8300: Loss = -9920.066956058494
Iteration 8400: Loss = -9920.059340931148
Iteration 8500: Loss = -9919.984046078029
Iteration 8600: Loss = -9919.983368967485
Iteration 8700: Loss = -9919.981146963724
Iteration 8800: Loss = -9919.951499382576
Iteration 8900: Loss = -9919.975285280389
1
Iteration 9000: Loss = -9919.900420299276
Iteration 9100: Loss = -9919.873626853532
Iteration 9200: Loss = -9919.859062905065
Iteration 9300: Loss = -9919.854501992366
Iteration 9400: Loss = -9919.754075941157
Iteration 9500: Loss = -9919.741614950228
Iteration 9600: Loss = -9919.754853102506
1
Iteration 9700: Loss = -9919.739591921769
Iteration 9800: Loss = -9919.739904461529
1
Iteration 9900: Loss = -9919.736736505982
Iteration 10000: Loss = -9919.815128613036
1
Iteration 10100: Loss = -9919.73337778934
Iteration 10200: Loss = -9919.733653354951
1
Iteration 10300: Loss = -9919.717838205926
Iteration 10400: Loss = -9919.718538960768
1
Iteration 10500: Loss = -9919.71767436043
Iteration 10600: Loss = -9919.64893066621
Iteration 10700: Loss = -9919.62511030771
Iteration 10800: Loss = -9919.598363081299
Iteration 10900: Loss = -9919.577170169381
Iteration 11000: Loss = -9919.573456089449
Iteration 11100: Loss = -9919.569441546068
Iteration 11200: Loss = -9919.529871996723
Iteration 11300: Loss = -9919.599934560156
1
Iteration 11400: Loss = -9919.487049715613
Iteration 11500: Loss = -9919.49248422261
1
Iteration 11600: Loss = -9919.464344476999
Iteration 11700: Loss = -9919.416885676019
Iteration 11800: Loss = -9919.34482382743
Iteration 11900: Loss = -9919.345352315437
1
Iteration 12000: Loss = -9919.344138484523
Iteration 12100: Loss = -9919.33121200901
Iteration 12200: Loss = -9919.314897049415
Iteration 12300: Loss = -9919.29530367404
Iteration 12400: Loss = -9919.257392412772
Iteration 12500: Loss = -9919.235324761896
Iteration 12600: Loss = -9919.226704647735
Iteration 12700: Loss = -9919.182502212636
Iteration 12800: Loss = -9919.214395348205
1
Iteration 12900: Loss = -9919.381542303192
2
Iteration 13000: Loss = -9919.145761438023
Iteration 13100: Loss = -9919.134949562906
Iteration 13200: Loss = -9919.134263920294
Iteration 13300: Loss = -9919.135291735309
1
Iteration 13400: Loss = -9919.134335030412
Iteration 13500: Loss = -9919.132981830491
Iteration 13600: Loss = -9919.136579605742
1
Iteration 13700: Loss = -9919.132154430246
Iteration 13800: Loss = -9919.12766523902
Iteration 13900: Loss = -9919.131197271161
1
Iteration 14000: Loss = -9919.141657772137
2
Iteration 14100: Loss = -9919.194049841622
3
Iteration 14200: Loss = -9919.109757501135
Iteration 14300: Loss = -9919.109820798603
Iteration 14400: Loss = -9919.108358122608
Iteration 14500: Loss = -9918.914530592787
Iteration 14600: Loss = -9918.903092171271
Iteration 14700: Loss = -9918.90302061225
Iteration 14800: Loss = -9918.890254926235
Iteration 14900: Loss = -9918.88243904523
Iteration 15000: Loss = -9918.87766370976
Iteration 15100: Loss = -9918.865737852264
Iteration 15200: Loss = -9919.164206751773
1
Iteration 15300: Loss = -9918.864521867714
Iteration 15400: Loss = -9918.92941943507
1
Iteration 15500: Loss = -9918.856296124837
Iteration 15600: Loss = -9918.856432442855
1
Iteration 15700: Loss = -9918.8630721129
2
Iteration 15800: Loss = -9918.851625102263
Iteration 15900: Loss = -9918.78497793806
Iteration 16000: Loss = -9918.783944008665
Iteration 16100: Loss = -9918.910024021825
1
Iteration 16200: Loss = -9918.767839173399
Iteration 16300: Loss = -9918.768046437266
1
Iteration 16400: Loss = -9918.767771968045
Iteration 16500: Loss = -9918.767296202523
Iteration 16600: Loss = -9918.754599227646
Iteration 16700: Loss = -9918.993578843343
1
Iteration 16800: Loss = -9918.748753426844
Iteration 16900: Loss = -9918.759607258202
1
Iteration 17000: Loss = -9918.747456465746
Iteration 17100: Loss = -9918.75628598393
1
Iteration 17200: Loss = -9918.683931202911
Iteration 17300: Loss = -9918.75694874842
1
Iteration 17400: Loss = -9918.64248936733
Iteration 17500: Loss = -9918.662145664424
1
Iteration 17600: Loss = -9918.643386703017
2
Iteration 17700: Loss = -9918.587073488252
Iteration 17800: Loss = -9918.605321565095
1
Iteration 17900: Loss = -9918.318968697135
Iteration 18000: Loss = -9918.376198821801
1
Iteration 18100: Loss = -9918.284895049565
Iteration 18200: Loss = -9918.26798539316
Iteration 18300: Loss = -9918.265741029083
Iteration 18400: Loss = -9918.266281452323
1
Iteration 18500: Loss = -9918.268390968598
2
Iteration 18600: Loss = -9918.265513311782
Iteration 18700: Loss = -9918.266312926164
1
Iteration 18800: Loss = -9918.265282875893
Iteration 18900: Loss = -9918.267400737188
1
Iteration 19000: Loss = -9918.25630620404
Iteration 19100: Loss = -9918.256497334802
1
Iteration 19200: Loss = -9918.26772571006
2
Iteration 19300: Loss = -9918.250472907761
Iteration 19400: Loss = -9918.250348719772
Iteration 19500: Loss = -9918.552389331231
1
Iteration 19600: Loss = -9918.250966500764
2
Iteration 19700: Loss = -9918.46754601792
3
Iteration 19800: Loss = -9918.249575153543
Iteration 19900: Loss = -9918.277722702673
1
pi: tensor([[0.7380, 0.2620],
        [0.1568, 0.8432]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9731, 0.0269], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1351, 0.2119],
         [0.5406, 0.2194]],

        [[0.5350, 0.1232],
         [0.6516, 0.5738]],

        [[0.7067, 0.0872],
         [0.5507, 0.6076]],

        [[0.5364, 0.1026],
         [0.6388, 0.7175]],

        [[0.6543, 0.1074],
         [0.6661, 0.5731]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 74
Adjusted Rand Index: 0.2217164307904729
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 82
Adjusted Rand Index: 0.4036705334790734
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 87
Adjusted Rand Index: 0.5431180154210982
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 83
Adjusted Rand Index: 0.42987888645476574
Global Adjusted Rand Index: 0.2688454910798694
Average Adjusted Rand Index: 0.31967677322908206
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21023.06235856956
Iteration 100: Loss = -9948.618913555843
Iteration 200: Loss = -9947.267137719275
Iteration 300: Loss = -9946.198935179693
Iteration 400: Loss = -9945.627770667286
Iteration 500: Loss = -9944.89215141023
Iteration 600: Loss = -9943.753266001371
Iteration 700: Loss = -9942.658477813655
Iteration 800: Loss = -9941.85621069572
Iteration 900: Loss = -9941.257712483928
Iteration 1000: Loss = -9940.844874431663
Iteration 1100: Loss = -9940.599009229507
Iteration 1200: Loss = -9940.413451735212
Iteration 1300: Loss = -9940.180339649576
Iteration 1400: Loss = -9940.012868692864
Iteration 1500: Loss = -9939.9186204851
Iteration 1600: Loss = -9939.89169431268
Iteration 1700: Loss = -9939.883798195873
Iteration 1800: Loss = -9939.881157201185
Iteration 1900: Loss = -9939.879921758335
Iteration 2000: Loss = -9939.879099491121
Iteration 2100: Loss = -9939.878358558026
Iteration 2200: Loss = -9939.877689659343
Iteration 2300: Loss = -9939.876933990687
Iteration 2400: Loss = -9939.876232184723
Iteration 2500: Loss = -9939.900475721903
1
Iteration 2600: Loss = -9939.874789043348
Iteration 2700: Loss = -9939.874033205826
Iteration 2800: Loss = -9939.873532129184
Iteration 2900: Loss = -9939.872636951115
Iteration 3000: Loss = -9939.871947253354
Iteration 3100: Loss = -9939.871357271024
Iteration 3200: Loss = -9939.870713823084
Iteration 3300: Loss = -9939.870485879916
Iteration 3400: Loss = -9939.869553053655
Iteration 3500: Loss = -9939.869021911658
Iteration 3600: Loss = -9939.868525946473
Iteration 3700: Loss = -9939.868114301356
Iteration 3800: Loss = -9939.867640517006
Iteration 3900: Loss = -9939.867216271477
Iteration 4000: Loss = -9939.866846773863
Iteration 4100: Loss = -9939.866438559626
Iteration 4200: Loss = -9939.872048908892
1
Iteration 4300: Loss = -9939.86578878163
Iteration 4400: Loss = -9939.86548168316
Iteration 4500: Loss = -9939.865171567017
Iteration 4600: Loss = -9939.864936637603
Iteration 4700: Loss = -9939.864677001433
Iteration 4800: Loss = -9939.864440646006
Iteration 4900: Loss = -9939.86421700205
Iteration 5000: Loss = -9939.868082934077
1
Iteration 5100: Loss = -9939.863825780028
Iteration 5200: Loss = -9939.86370685521
Iteration 5300: Loss = -9939.863483951583
Iteration 5400: Loss = -9939.863327329316
Iteration 5500: Loss = -9939.867308304163
1
Iteration 5600: Loss = -9939.862984031166
Iteration 5700: Loss = -9939.862915674757
Iteration 5800: Loss = -9939.862782530308
Iteration 5900: Loss = -9939.862664848333
Iteration 6000: Loss = -9939.907308385933
1
Iteration 6100: Loss = -9939.862454151968
Iteration 6200: Loss = -9939.86238242555
Iteration 6300: Loss = -9939.862342893573
Iteration 6400: Loss = -9939.862180299328
Iteration 6500: Loss = -9939.868865846718
1
Iteration 6600: Loss = -9939.862022165698
Iteration 6700: Loss = -9939.86195785885
Iteration 6800: Loss = -9939.86192902802
Iteration 6900: Loss = -9939.861822268258
Iteration 7000: Loss = -9939.862334675327
1
Iteration 7100: Loss = -9939.86171964035
Iteration 7200: Loss = -9939.923917735541
1
Iteration 7300: Loss = -9939.86160903034
Iteration 7400: Loss = -9939.861589031885
Iteration 7500: Loss = -9939.872990147293
1
Iteration 7600: Loss = -9939.861437014226
Iteration 7700: Loss = -9939.861471592378
Iteration 7800: Loss = -9939.861436290243
Iteration 7900: Loss = -9939.86134999196
Iteration 8000: Loss = -9939.868900931377
1
Iteration 8100: Loss = -9939.861253827701
Iteration 8200: Loss = -9939.86127520504
Iteration 8300: Loss = -9939.861268213368
Iteration 8400: Loss = -9939.861211069647
Iteration 8500: Loss = -9939.864736057336
1
Iteration 8600: Loss = -9939.861151364681
Iteration 8700: Loss = -9939.914390244943
1
Iteration 8800: Loss = -9939.876246587095
2
Iteration 8900: Loss = -9939.92493360316
3
Iteration 9000: Loss = -9939.86343060565
4
Iteration 9100: Loss = -9939.940757984828
5
Iteration 9200: Loss = -9939.86848315322
6
Iteration 9300: Loss = -9939.909960281215
7
Iteration 9400: Loss = -9939.866733984547
8
Iteration 9500: Loss = -9940.007368881103
9
Iteration 9600: Loss = -9939.88216033566
10
Iteration 9700: Loss = -9939.861126938871
Iteration 9800: Loss = -9939.866778353718
1
Iteration 9900: Loss = -9939.864754971559
2
Iteration 10000: Loss = -9939.876999200902
3
Iteration 10100: Loss = -9939.887999818098
4
Iteration 10200: Loss = -9939.86829857547
5
Iteration 10300: Loss = -9939.971296448772
6
Iteration 10400: Loss = -9939.864617951993
7
Iteration 10500: Loss = -9939.912590223734
8
Iteration 10600: Loss = -9939.862220936939
9
Iteration 10700: Loss = -9939.862535252025
10
Iteration 10800: Loss = -9939.861343835633
11
Iteration 10900: Loss = -9939.86119335813
Iteration 11000: Loss = -9939.889164264467
1
Iteration 11100: Loss = -9939.889779890778
2
Iteration 11200: Loss = -9940.033827137846
3
Iteration 11300: Loss = -9939.86196305147
4
Iteration 11400: Loss = -9939.876739382162
5
Iteration 11500: Loss = -9939.861654636934
6
Iteration 11600: Loss = -9939.860834225603
Iteration 11700: Loss = -9939.862524960809
1
Iteration 11800: Loss = -9939.860865418052
Iteration 11900: Loss = -9939.861049574854
1
Iteration 12000: Loss = -9939.93751873938
2
Iteration 12100: Loss = -9939.860789378785
Iteration 12200: Loss = -9939.861016305958
1
Iteration 12300: Loss = -9939.860948881123
2
Iteration 12400: Loss = -9939.86124487872
3
Iteration 12500: Loss = -9939.938836885194
4
Iteration 12600: Loss = -9939.971560737584
5
Iteration 12700: Loss = -9939.860846666114
Iteration 12800: Loss = -9939.86083203072
Iteration 12900: Loss = -9939.905215794592
1
Iteration 13000: Loss = -9939.933530915638
2
Iteration 13100: Loss = -9939.863706680395
3
Iteration 13200: Loss = -9939.868786939513
4
Iteration 13300: Loss = -9939.865421136617
5
Iteration 13400: Loss = -9939.86086880384
Iteration 13500: Loss = -9939.862124535834
1
Iteration 13600: Loss = -9939.860970963582
2
Iteration 13700: Loss = -9939.862075832967
3
Iteration 13800: Loss = -9939.877521498218
4
Iteration 13900: Loss = -9939.904841687494
5
Iteration 14000: Loss = -9939.902691589165
6
Iteration 14100: Loss = -9939.86166272684
7
Iteration 14200: Loss = -9939.874838738053
8
Iteration 14300: Loss = -9939.870214418672
9
Iteration 14400: Loss = -9939.878321595952
10
Iteration 14500: Loss = -9939.861058743516
11
Iteration 14600: Loss = -9939.860759699854
Iteration 14700: Loss = -9939.861882046267
1
Iteration 14800: Loss = -9939.860907953658
2
Iteration 14900: Loss = -9939.862189992673
3
Iteration 15000: Loss = -9939.86155802573
4
Iteration 15100: Loss = -9939.863067658223
5
Iteration 15200: Loss = -9939.862592491743
6
Iteration 15300: Loss = -9939.868276263198
7
Iteration 15400: Loss = -9939.867069751082
8
Iteration 15500: Loss = -9939.910316253528
9
Iteration 15600: Loss = -9939.863315893368
10
Iteration 15700: Loss = -9939.871911644168
11
Iteration 15800: Loss = -9939.886621492056
12
Iteration 15900: Loss = -9939.860810027247
Iteration 16000: Loss = -9939.865793633844
1
Iteration 16100: Loss = -9939.861470089176
2
Iteration 16200: Loss = -9939.902919710268
3
Iteration 16300: Loss = -9939.884339818776
4
Iteration 16400: Loss = -9939.868325696158
5
Iteration 16500: Loss = -9939.867868694808
6
Iteration 16600: Loss = -9939.874854759973
7
Iteration 16700: Loss = -9939.870579472064
8
Iteration 16800: Loss = -9939.860732259558
Iteration 16900: Loss = -9939.861074348375
1
Iteration 17000: Loss = -9939.860814661322
Iteration 17100: Loss = -9939.861002337688
1
Iteration 17200: Loss = -9939.86317960123
2
Iteration 17300: Loss = -9939.889891429277
3
Iteration 17400: Loss = -9939.860885447093
Iteration 17500: Loss = -9939.868633994607
1
Iteration 17600: Loss = -9939.8645515299
2
Iteration 17700: Loss = -9939.868865711762
3
Iteration 17800: Loss = -9939.88135497082
4
Iteration 17900: Loss = -9939.894756854477
5
Iteration 18000: Loss = -9939.861861072177
6
Iteration 18100: Loss = -9939.866917732334
7
Iteration 18200: Loss = -9939.999094690513
8
Iteration 18300: Loss = -9939.862301984793
9
Iteration 18400: Loss = -9939.8717206347
10
Iteration 18500: Loss = -9939.869138688964
11
Iteration 18600: Loss = -9939.862234619675
12
Iteration 18700: Loss = -9939.862624958407
13
Iteration 18800: Loss = -9939.910685609535
14
Iteration 18900: Loss = -9939.978099737013
15
Stopping early at iteration 18900 due to no improvement.
pi: tensor([[3.2565e-06, 1.0000e+00],
        [3.6915e-01, 6.3085e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0342, 0.9658], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1676, 0.2208],
         [0.5633, 0.1292]],

        [[0.6635, 0.1361],
         [0.5964, 0.5409]],

        [[0.5845, 0.1271],
         [0.6912, 0.5671]],

        [[0.6295, 0.1547],
         [0.7210, 0.5795]],

        [[0.5885, 0.1644],
         [0.7178, 0.5735]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.012965964343598054
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.02104340201665936
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.015172214275382118
Global Adjusted Rand Index: 0.014276010832021391
Average Adjusted Rand Index: 0.011389682339655583
10054.045589332021
[0.2688454910798694, 0.014276010832021391] [0.31967677322908206, 0.011389682339655583] [9918.20811898125, 9939.978099737013]
-------------------------------------
This iteration is 87
True Objective function: Loss = -10036.431938653048
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22284.437154528903
Iteration 100: Loss = -9887.063661085795
Iteration 200: Loss = -9886.033134454143
Iteration 300: Loss = -9885.574602558063
Iteration 400: Loss = -9885.19532171304
Iteration 500: Loss = -9884.960829424323
Iteration 600: Loss = -9884.771736082517
Iteration 700: Loss = -9884.602176604916
Iteration 800: Loss = -9884.446818562603
Iteration 900: Loss = -9884.291261174245
Iteration 1000: Loss = -9884.124351226474
Iteration 1100: Loss = -9883.941808962178
Iteration 1200: Loss = -9883.751742633862
Iteration 1300: Loss = -9883.560721269645
Iteration 1400: Loss = -9883.36063232711
Iteration 1500: Loss = -9883.115049136293
Iteration 1600: Loss = -9882.671855401626
Iteration 1700: Loss = -9881.808088151101
Iteration 1800: Loss = -9880.710276895123
Iteration 1900: Loss = -9880.06469669024
Iteration 2000: Loss = -9879.373160374258
Iteration 2100: Loss = -9878.414095906754
Iteration 2200: Loss = -9877.888347556278
Iteration 2300: Loss = -9877.672289595517
Iteration 2400: Loss = -9877.51518999562
Iteration 2500: Loss = -9877.411845692224
Iteration 2600: Loss = -9877.27823413409
Iteration 2700: Loss = -9877.20878049009
Iteration 2800: Loss = -9877.174521683557
Iteration 2900: Loss = -9877.148478168792
Iteration 3000: Loss = -9877.12845710331
Iteration 3100: Loss = -9877.111979909523
Iteration 3200: Loss = -9877.097250355637
Iteration 3300: Loss = -9877.083924106286
Iteration 3400: Loss = -9877.071927856381
Iteration 3500: Loss = -9877.0614649264
Iteration 3600: Loss = -9877.053250384663
Iteration 3700: Loss = -9877.046556476813
Iteration 3800: Loss = -9877.040832431825
Iteration 3900: Loss = -9877.03574763739
Iteration 4000: Loss = -9877.031135342266
Iteration 4100: Loss = -9877.026988002282
Iteration 4200: Loss = -9877.023217161759
Iteration 4300: Loss = -9877.019779264918
Iteration 4400: Loss = -9877.016576554337
Iteration 4500: Loss = -9877.013696992592
Iteration 4600: Loss = -9877.011060658753
Iteration 4700: Loss = -9877.008615002076
Iteration 4800: Loss = -9877.006373635744
Iteration 4900: Loss = -9877.004566973343
Iteration 5000: Loss = -9877.002427417045
Iteration 5100: Loss = -9877.000670798649
Iteration 5200: Loss = -9877.00029596157
Iteration 5300: Loss = -9876.997453274098
Iteration 5400: Loss = -9876.996035946388
Iteration 5500: Loss = -9876.995843937902
Iteration 5600: Loss = -9876.993406166135
Iteration 5700: Loss = -9876.99224866558
Iteration 5800: Loss = -9876.992266839536
Iteration 5900: Loss = -9876.990126874216
Iteration 6000: Loss = -9876.989185211154
Iteration 6100: Loss = -9876.988325452607
Iteration 6200: Loss = -9876.987452347657
Iteration 6300: Loss = -9876.986658135342
Iteration 6400: Loss = -9876.985903369807
Iteration 6500: Loss = -9876.985185310174
Iteration 6600: Loss = -9876.984513828176
Iteration 6700: Loss = -9876.98388053447
Iteration 6800: Loss = -9876.983266948182
Iteration 6900: Loss = -9876.986400675767
1
Iteration 7000: Loss = -9876.982054550434
Iteration 7100: Loss = -9876.981497035684
Iteration 7200: Loss = -9876.980995457889
Iteration 7300: Loss = -9876.980439926592
Iteration 7400: Loss = -9876.980006481599
Iteration 7500: Loss = -9876.979557050476
Iteration 7600: Loss = -9876.979193398625
Iteration 7700: Loss = -9876.978938298258
Iteration 7800: Loss = -9876.978459384636
Iteration 7900: Loss = -9876.978196642416
Iteration 8000: Loss = -9876.977786327403
Iteration 8100: Loss = -9876.977541454155
Iteration 8200: Loss = -9876.97726808208
Iteration 8300: Loss = -9876.97700593369
Iteration 8400: Loss = -9876.976762412276
Iteration 8500: Loss = -9876.976518662985
Iteration 8600: Loss = -9876.976751683467
1
Iteration 8700: Loss = -9876.976068643009
Iteration 8800: Loss = -9876.975855808474
Iteration 8900: Loss = -9876.977924799538
1
Iteration 9000: Loss = -9876.975435647371
Iteration 9100: Loss = -9876.975251958685
Iteration 9200: Loss = -9876.983188774278
1
Iteration 9300: Loss = -9876.974901155525
Iteration 9400: Loss = -9876.974724784714
Iteration 9500: Loss = -9877.02082972596
1
Iteration 9600: Loss = -9876.974421495957
Iteration 9700: Loss = -9876.97432605775
Iteration 9800: Loss = -9876.980885619823
1
Iteration 9900: Loss = -9876.974037666127
Iteration 10000: Loss = -9876.973940133179
Iteration 10100: Loss = -9876.973876131728
Iteration 10200: Loss = -9876.973752257198
Iteration 10300: Loss = -9876.973672262688
Iteration 10400: Loss = -9876.974150894883
1
Iteration 10500: Loss = -9876.973888059047
2
Iteration 10600: Loss = -9876.973407098458
Iteration 10700: Loss = -9876.98490222427
1
Iteration 10800: Loss = -9876.973300056889
Iteration 10900: Loss = -9876.97316662071
Iteration 11000: Loss = -9876.977554458594
1
Iteration 11100: Loss = -9876.973465770243
2
Iteration 11200: Loss = -9877.017868432786
3
Iteration 11300: Loss = -9876.972885373514
Iteration 11400: Loss = -9876.97276158913
Iteration 11500: Loss = -9877.023862180858
1
Iteration 11600: Loss = -9876.972650463957
Iteration 11700: Loss = -9876.9733069486
1
Iteration 11800: Loss = -9876.972982991601
2
Iteration 11900: Loss = -9876.97265479193
Iteration 12000: Loss = -9876.981668632676
1
Iteration 12100: Loss = -9876.974240231317
2
Iteration 12200: Loss = -9876.973432391644
3
Iteration 12300: Loss = -9876.972968329192
4
Iteration 12400: Loss = -9876.972602457583
Iteration 12500: Loss = -9876.972438934106
Iteration 12600: Loss = -9876.972381383373
Iteration 12700: Loss = -9877.014714426303
1
Iteration 12800: Loss = -9876.994612280318
2
Iteration 12900: Loss = -9877.002175838195
3
Iteration 13000: Loss = -9876.98878307195
4
Iteration 13100: Loss = -9876.972231911286
Iteration 13200: Loss = -9876.975923256421
1
Iteration 13300: Loss = -9876.972165266383
Iteration 13400: Loss = -9876.972155039794
Iteration 13500: Loss = -9876.972124184895
Iteration 13600: Loss = -9876.972730382002
1
Iteration 13700: Loss = -9876.973006117156
2
Iteration 13800: Loss = -9876.982331928251
3
Iteration 13900: Loss = -9876.972074444602
Iteration 14000: Loss = -9876.973663322544
1
Iteration 14100: Loss = -9876.973681282541
2
Iteration 14200: Loss = -9876.97398336822
3
Iteration 14300: Loss = -9876.982148377187
4
Iteration 14400: Loss = -9876.989260605473
5
Iteration 14500: Loss = -9876.972106436913
Iteration 14600: Loss = -9877.109830642272
1
Iteration 14700: Loss = -9876.97197020145
Iteration 14800: Loss = -9876.973460858993
1
Iteration 14900: Loss = -9876.971952533488
Iteration 15000: Loss = -9876.972048114585
Iteration 15100: Loss = -9876.972178068077
1
Iteration 15200: Loss = -9876.985602849107
2
Iteration 15300: Loss = -9877.021409724632
3
Iteration 15400: Loss = -9876.97437088151
4
Iteration 15500: Loss = -9876.978590139088
5
Iteration 15600: Loss = -9876.971980126027
Iteration 15700: Loss = -9876.996725331594
1
Iteration 15800: Loss = -9876.979806266407
2
Iteration 15900: Loss = -9876.971797946195
Iteration 16000: Loss = -9876.97415255765
1
Iteration 16100: Loss = -9876.97238647853
2
Iteration 16200: Loss = -9876.971844408747
Iteration 16300: Loss = -9876.977090932307
1
Iteration 16400: Loss = -9877.012749952946
2
Iteration 16500: Loss = -9876.97168425851
Iteration 16600: Loss = -9876.97195450441
1
Iteration 16700: Loss = -9876.992563136091
2
Iteration 16800: Loss = -9876.978091097355
3
Iteration 16900: Loss = -9876.971919813515
4
Iteration 17000: Loss = -9876.971661312935
Iteration 17100: Loss = -9876.972699458818
1
Iteration 17200: Loss = -9876.975872214089
2
Iteration 17300: Loss = -9876.97171169883
Iteration 17400: Loss = -9877.056379460135
1
Iteration 17500: Loss = -9876.977930792356
2
Iteration 17600: Loss = -9876.971738757733
Iteration 17700: Loss = -9876.971687334799
Iteration 17800: Loss = -9876.97160924188
Iteration 17900: Loss = -9876.97440734523
1
Iteration 18000: Loss = -9876.971614613327
Iteration 18100: Loss = -9876.971811428763
1
Iteration 18200: Loss = -9876.988064956899
2
Iteration 18300: Loss = -9876.972022257121
3
Iteration 18400: Loss = -9876.97241595328
4
Iteration 18500: Loss = -9876.972308292095
5
Iteration 18600: Loss = -9876.976094134005
6
Iteration 18700: Loss = -9876.974942017981
7
Iteration 18800: Loss = -9876.971643055522
Iteration 18900: Loss = -9876.971912073172
1
Iteration 19000: Loss = -9876.9718439281
2
Iteration 19100: Loss = -9876.97171674097
Iteration 19200: Loss = -9876.972453025399
1
Iteration 19300: Loss = -9876.976928530792
2
Iteration 19400: Loss = -9876.972018878088
3
Iteration 19500: Loss = -9876.97304435852
4
Iteration 19600: Loss = -9876.971733849101
Iteration 19700: Loss = -9876.973684134944
1
Iteration 19800: Loss = -9876.994071040419
2
Iteration 19900: Loss = -9876.972981974912
3
pi: tensor([[1.0000e+00, 4.9935e-06],
        [3.8434e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0927, 0.9073], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2194, 0.1429],
         [0.5690, 0.1354]],

        [[0.7277, 0.1572],
         [0.5466, 0.5854]],

        [[0.5825, 0.1154],
         [0.6227, 0.6630]],

        [[0.5324, 0.1998],
         [0.5960, 0.6800]],

        [[0.7118, 0.0877],
         [0.7054, 0.5728]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.012100586122140291
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.000855298619494572
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.015287316523713311
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.004021803333230736
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.00932995567938315
Global Adjusted Rand Index: 0.012669014218733052
Average Adjusted Rand Index: 0.006368151274502289
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22783.186571315655
Iteration 100: Loss = -9887.566552561337
Iteration 200: Loss = -9886.267092163198
Iteration 300: Loss = -9885.88059432526
Iteration 400: Loss = -9885.697901297848
Iteration 500: Loss = -9885.587009837212
Iteration 600: Loss = -9885.502430879531
Iteration 700: Loss = -9885.418180779609
Iteration 800: Loss = -9885.31728789885
Iteration 900: Loss = -9885.191302167648
Iteration 1000: Loss = -9885.02336287371
Iteration 1100: Loss = -9884.802459964018
Iteration 1200: Loss = -9884.55960358478
Iteration 1300: Loss = -9884.33022236323
Iteration 1400: Loss = -9884.135719983431
Iteration 1500: Loss = -9883.945661421181
Iteration 1600: Loss = -9883.683492010803
Iteration 1700: Loss = -9883.062884649822
Iteration 1800: Loss = -9882.231090518653
Iteration 1900: Loss = -9881.938993283724
Iteration 2000: Loss = -9881.829483235244
Iteration 2100: Loss = -9881.77706345359
Iteration 2200: Loss = -9881.747546708773
Iteration 2300: Loss = -9881.72738410942
Iteration 2400: Loss = -9881.716619916155
Iteration 2500: Loss = -9881.710790987163
Iteration 2600: Loss = -9881.70747047295
Iteration 2700: Loss = -9881.705703907322
Iteration 2800: Loss = -9881.704451198091
Iteration 2900: Loss = -9881.703545457807
Iteration 3000: Loss = -9881.702703421399
Iteration 3100: Loss = -9881.7019809112
Iteration 3200: Loss = -9881.701253889845
Iteration 3300: Loss = -9881.700549061816
Iteration 3400: Loss = -9881.699754661575
Iteration 3500: Loss = -9881.698464176288
Iteration 3600: Loss = -9881.694697788604
Iteration 3700: Loss = -9881.665847318221
Iteration 3800: Loss = -9881.421794357302
Iteration 3900: Loss = -9881.061829690267
Iteration 4000: Loss = -9880.740661900343
Iteration 4100: Loss = -9878.867014568239
Iteration 4200: Loss = -9878.699567708489
Iteration 4300: Loss = -9878.641054854528
Iteration 4400: Loss = -9878.616441383763
Iteration 4500: Loss = -9878.602197004435
Iteration 4600: Loss = -9878.592778410899
Iteration 4700: Loss = -9878.585758351608
Iteration 4800: Loss = -9878.58040912714
Iteration 4900: Loss = -9878.576673995998
Iteration 5000: Loss = -9878.573743768662
Iteration 5100: Loss = -9878.571277487621
Iteration 5200: Loss = -9878.568200202477
Iteration 5300: Loss = -9878.540773166635
Iteration 5400: Loss = -9878.534849920106
Iteration 5500: Loss = -9878.531063255534
Iteration 5600: Loss = -9878.53564752002
1
Iteration 5700: Loss = -9878.529030403019
Iteration 5800: Loss = -9878.527027577973
Iteration 5900: Loss = -9878.519736476252
Iteration 6000: Loss = -9878.519035438381
Iteration 6100: Loss = -9878.520809786614
1
Iteration 6200: Loss = -9878.518044116268
Iteration 6300: Loss = -9878.517581095542
Iteration 6400: Loss = -9878.517094829373
Iteration 6500: Loss = -9878.516512496566
Iteration 6600: Loss = -9878.51631483864
Iteration 6700: Loss = -9878.515441943844
Iteration 6800: Loss = -9878.514596544494
Iteration 6900: Loss = -9878.510688222274
Iteration 7000: Loss = -9878.506091543253
Iteration 7100: Loss = -9878.505048222949
Iteration 7200: Loss = -9878.50475733347
Iteration 7300: Loss = -9878.505266679244
1
Iteration 7400: Loss = -9878.504355711815
Iteration 7500: Loss = -9878.506126911596
1
Iteration 7600: Loss = -9878.503997714955
Iteration 7700: Loss = -9878.50376169106
Iteration 7800: Loss = -9878.503509012628
Iteration 7900: Loss = -9878.50322668337
Iteration 8000: Loss = -9878.503040579752
Iteration 8100: Loss = -9878.502469972218
Iteration 8200: Loss = -9878.502123232262
Iteration 8300: Loss = -9878.502870544622
1
Iteration 8400: Loss = -9878.50186699691
Iteration 8500: Loss = -9878.501960465324
Iteration 8600: Loss = -9878.504861441595
1
Iteration 8700: Loss = -9878.501307771745
Iteration 8800: Loss = -9878.511402733973
1
Iteration 8900: Loss = -9878.500342928617
Iteration 9000: Loss = -9878.539272666038
1
Iteration 9100: Loss = -9878.110116394815
Iteration 9200: Loss = -9878.083361701767
Iteration 9300: Loss = -9878.073875606684
Iteration 9400: Loss = -9878.070673404782
Iteration 9500: Loss = -9878.2022156814
1
Iteration 9600: Loss = -9878.068815143162
Iteration 9700: Loss = -9878.066179129448
Iteration 9800: Loss = -9878.145444022446
1
Iteration 9900: Loss = -9878.06547829615
Iteration 10000: Loss = -9878.064322277274
Iteration 10100: Loss = -9878.064428999023
1
Iteration 10200: Loss = -9878.063932788731
Iteration 10300: Loss = -9878.063224321435
Iteration 10400: Loss = -9876.639567757094
Iteration 10500: Loss = -9876.451504432689
Iteration 10600: Loss = -9876.423791150812
Iteration 10700: Loss = -9876.417903574567
Iteration 10800: Loss = -9876.415355852556
Iteration 10900: Loss = -9876.643774039929
1
Iteration 11000: Loss = -9876.41332748128
Iteration 11100: Loss = -9876.412928639562
Iteration 11200: Loss = -9876.445141278715
1
Iteration 11300: Loss = -9876.412534009054
Iteration 11400: Loss = -9876.412409716215
Iteration 11500: Loss = -9876.538111724472
1
Iteration 11600: Loss = -9876.412328681148
Iteration 11700: Loss = -9876.41225888611
Iteration 11800: Loss = -9876.412255541516
Iteration 11900: Loss = -9876.412448260058
1
Iteration 12000: Loss = -9876.41221217823
Iteration 12100: Loss = -9876.412181120726
Iteration 12200: Loss = -9876.413008977996
1
Iteration 12300: Loss = -9876.412150146221
Iteration 12400: Loss = -9876.412114572679
Iteration 12500: Loss = -9876.413328906394
1
Iteration 12600: Loss = -9876.41209236444
Iteration 12700: Loss = -9876.411932608979
Iteration 12800: Loss = -9876.411338552602
Iteration 12900: Loss = -9876.413520945374
1
Iteration 13000: Loss = -9876.41141009667
Iteration 13100: Loss = -9876.418579135083
1
Iteration 13200: Loss = -9876.411713585288
2
Iteration 13300: Loss = -9876.411307870872
Iteration 13400: Loss = -9876.411335328494
Iteration 13500: Loss = -9876.411372527786
Iteration 13600: Loss = -9876.42307866367
1
Iteration 13700: Loss = -9876.411226260852
Iteration 13800: Loss = -9876.411218924428
Iteration 13900: Loss = -9876.457136190247
1
Iteration 14000: Loss = -9876.411237733038
Iteration 14100: Loss = -9876.411235104888
Iteration 14200: Loss = -9876.417034048942
1
Iteration 14300: Loss = -9876.411262912103
Iteration 14400: Loss = -9876.42302025237
1
Iteration 14500: Loss = -9876.411247857193
Iteration 14600: Loss = -9876.435151514099
1
Iteration 14700: Loss = -9876.411252618944
Iteration 14800: Loss = -9876.411250756006
Iteration 14900: Loss = -9876.414614488674
1
Iteration 15000: Loss = -9876.411245376945
Iteration 15100: Loss = -9876.411264431603
Iteration 15200: Loss = -9876.411331874942
Iteration 15300: Loss = -9876.411248652508
Iteration 15400: Loss = -9876.425208244676
1
Iteration 15500: Loss = -9876.411242474323
Iteration 15600: Loss = -9876.569061602258
1
Iteration 15700: Loss = -9876.41121737432
Iteration 15800: Loss = -9876.413270313715
1
Iteration 15900: Loss = -9876.411559977541
2
Iteration 16000: Loss = -9876.411488143069
3
Iteration 16100: Loss = -9876.657311330262
4
Iteration 16200: Loss = -9876.412332449041
5
Iteration 16300: Loss = -9876.43669230219
6
Iteration 16400: Loss = -9876.41133018013
7
Iteration 16500: Loss = -9876.4118152159
8
Iteration 16600: Loss = -9876.411331297106
9
Iteration 16700: Loss = -9876.41202435282
10
Iteration 16800: Loss = -9876.41176137821
11
Iteration 16900: Loss = -9876.41142548102
12
Iteration 17000: Loss = -9876.57625343537
13
Iteration 17100: Loss = -9876.41122011514
Iteration 17200: Loss = -9876.41124479484
Iteration 17300: Loss = -9876.411543245953
1
Iteration 17400: Loss = -9876.411240063957
Iteration 17500: Loss = -9876.429377120121
1
Iteration 17600: Loss = -9876.411198897373
Iteration 17700: Loss = -9876.411152597668
Iteration 17800: Loss = -9876.411255973246
1
Iteration 17900: Loss = -9876.41121688096
Iteration 18000: Loss = -9876.424365770326
1
Iteration 18100: Loss = -9876.411045897335
Iteration 18200: Loss = -9876.411694082712
1
Iteration 18300: Loss = -9876.411084060988
Iteration 18400: Loss = -9876.411100940702
Iteration 18500: Loss = -9876.412116730991
1
Iteration 18600: Loss = -9876.41104961454
Iteration 18700: Loss = -9876.411052723235
Iteration 18800: Loss = -9876.41107371266
Iteration 18900: Loss = -9876.411044947372
Iteration 19000: Loss = -9876.415431307696
1
Iteration 19100: Loss = -9876.411038580696
Iteration 19200: Loss = -9876.411231545408
1
Iteration 19300: Loss = -9876.41104092013
Iteration 19400: Loss = -9876.411713872172
1
Iteration 19500: Loss = -9876.411170869727
2
Iteration 19600: Loss = -9876.411346241259
3
Iteration 19700: Loss = -9876.41316560434
4
Iteration 19800: Loss = -9876.411024069126
Iteration 19900: Loss = -9876.412725038532
1
pi: tensor([[9.9592e-01, 4.0783e-03],
        [1.7979e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9550, 0.0450], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.4137e-01, 1.2448e-01],
         [6.5724e-01, 5.2106e-04]],

        [[5.5306e-01, 9.7833e-02],
         [6.8050e-01, 6.3821e-01]],

        [[5.4498e-01, 6.4934e-02],
         [6.2428e-01, 5.1851e-01]],

        [[5.7149e-01, 1.1507e-01],
         [5.3527e-01, 7.3046e-01]],

        [[7.0072e-01, 1.0601e-01],
         [5.4522e-01, 7.0686e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0035158395898187145
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.010020462578284864
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.0019209323236274677
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0008263300397341679
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.01405713152211082
Global Adjusted Rand Index: -0.0027833502564380444
Average Adjusted Rand Index: -0.0020599541794012613
10036.431938653048
[0.012669014218733052, -0.0027833502564380444] [0.006368151274502289, -0.0020599541794012613] [9876.97252715406, 9876.410955776451]
-------------------------------------
This iteration is 88
True Objective function: Loss = -10096.191343430504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21726.162584481513
Iteration 100: Loss = -9981.508219134936
Iteration 200: Loss = -9980.024979986953
Iteration 300: Loss = -9979.32061831755
Iteration 400: Loss = -9978.868265989393
Iteration 500: Loss = -9978.56096165247
Iteration 600: Loss = -9978.341649223426
Iteration 700: Loss = -9978.160650329917
Iteration 800: Loss = -9977.996891233488
Iteration 900: Loss = -9977.827134446066
Iteration 1000: Loss = -9977.634271581685
Iteration 1100: Loss = -9977.41457762526
Iteration 1200: Loss = -9977.201836388287
Iteration 1300: Loss = -9977.005477948927
Iteration 1400: Loss = -9976.841309002108
Iteration 1500: Loss = -9976.668804636594
Iteration 1600: Loss = -9976.402661344095
Iteration 1700: Loss = -9976.017323697515
Iteration 1800: Loss = -9975.740854339325
Iteration 1900: Loss = -9975.561415615364
Iteration 2000: Loss = -9975.425583046901
Iteration 2100: Loss = -9975.311074807209
Iteration 2200: Loss = -9975.21229968373
Iteration 2300: Loss = -9975.128849506124
Iteration 2400: Loss = -9975.032392339173
Iteration 2500: Loss = -9974.917831272453
Iteration 2600: Loss = -9974.509932482746
Iteration 2700: Loss = -9974.004778909579
Iteration 2800: Loss = -9973.738830251057
Iteration 2900: Loss = -9973.662346680221
Iteration 3000: Loss = -9973.318169453021
Iteration 3100: Loss = -9973.161648911135
Iteration 3200: Loss = -9972.72161855917
Iteration 3300: Loss = -9972.207048785223
Iteration 3400: Loss = -9971.950865389363
Iteration 3500: Loss = -9971.092531049486
Iteration 3600: Loss = -9970.413667985003
Iteration 3700: Loss = -9970.091001350638
Iteration 3800: Loss = -9969.880964546697
Iteration 3900: Loss = -9969.77970126871
Iteration 4000: Loss = -9969.389755456077
Iteration 4100: Loss = -9969.184852411609
Iteration 4200: Loss = -9968.94041994322
Iteration 4300: Loss = -9967.929092855113
Iteration 4400: Loss = -9966.305446051765
Iteration 4500: Loss = -9965.886796607778
Iteration 4600: Loss = -9965.063483146096
Iteration 4700: Loss = -9964.980317944002
Iteration 4800: Loss = -9964.973846258881
Iteration 4900: Loss = -9964.970354429428
Iteration 5000: Loss = -9964.969131338392
Iteration 5100: Loss = -9964.9823174877
1
Iteration 5200: Loss = -9964.968007711403
Iteration 5300: Loss = -9964.96778581783
Iteration 5400: Loss = -9964.967478876782
Iteration 5500: Loss = -9964.967281997731
Iteration 5600: Loss = -9964.967189596917
Iteration 5700: Loss = -9964.96695093751
Iteration 5800: Loss = -9964.966798363863
Iteration 5900: Loss = -9964.966672342638
Iteration 6000: Loss = -9964.966367421934
Iteration 6100: Loss = -9964.972205431139
1
Iteration 6200: Loss = -9964.965659034653
Iteration 6300: Loss = -9964.965532889599
Iteration 6400: Loss = -9964.965527869072
Iteration 6500: Loss = -9964.965521609112
Iteration 6600: Loss = -9964.96908411344
1
Iteration 6700: Loss = -9964.965473888042
Iteration 6800: Loss = -9964.965451123962
Iteration 6900: Loss = -9964.96562135225
1
Iteration 7000: Loss = -9964.965462465569
Iteration 7100: Loss = -9964.965462508522
Iteration 7200: Loss = -9964.965445308484
Iteration 7300: Loss = -9964.965446398082
Iteration 7400: Loss = -9964.973638374811
1
Iteration 7500: Loss = -9964.965421590083
Iteration 7600: Loss = -9964.965432202229
Iteration 7700: Loss = -9964.966161856046
1
Iteration 7800: Loss = -9964.965398199816
Iteration 7900: Loss = -9964.982715057076
1
Iteration 8000: Loss = -9964.965378788092
Iteration 8100: Loss = -9964.970206170394
1
Iteration 8200: Loss = -9964.965791385866
2
Iteration 8300: Loss = -9964.965528154673
3
Iteration 8400: Loss = -9964.965474466535
Iteration 8500: Loss = -9964.965392032931
Iteration 8600: Loss = -9964.96537152982
Iteration 8700: Loss = -9964.965388449078
Iteration 8800: Loss = -9964.965357683237
Iteration 8900: Loss = -9964.96530590493
Iteration 9000: Loss = -9964.96535829772
Iteration 9100: Loss = -9964.965420891876
Iteration 9200: Loss = -9964.9699509827
1
Iteration 9300: Loss = -9964.965340831885
Iteration 9400: Loss = -9964.965545357709
1
Iteration 9500: Loss = -9964.965337917176
Iteration 9600: Loss = -9964.965379855135
Iteration 9700: Loss = -9964.969194903091
1
Iteration 9800: Loss = -9964.965328944285
Iteration 9900: Loss = -9964.966062277055
1
Iteration 10000: Loss = -9964.965357755758
Iteration 10100: Loss = -9964.965429334734
Iteration 10200: Loss = -9964.965348571384
Iteration 10300: Loss = -9964.965435672713
Iteration 10400: Loss = -9964.975995725608
1
Iteration 10500: Loss = -9964.965331372348
Iteration 10600: Loss = -9964.97050032089
1
Iteration 10700: Loss = -9964.965383307328
Iteration 10800: Loss = -9964.965336759806
Iteration 10900: Loss = -9964.96565002637
1
Iteration 11000: Loss = -9964.965461090444
2
Iteration 11100: Loss = -9965.207768227065
3
Iteration 11200: Loss = -9964.965336204774
Iteration 11300: Loss = -9964.974153075813
1
Iteration 11400: Loss = -9964.96534067349
Iteration 11500: Loss = -9964.984590691583
1
Iteration 11600: Loss = -9964.965328246779
Iteration 11700: Loss = -9964.968248273672
1
Iteration 11800: Loss = -9964.965386284233
Iteration 11900: Loss = -9964.965351674438
Iteration 12000: Loss = -9964.966005832906
1
Iteration 12100: Loss = -9964.965830349336
2
Iteration 12200: Loss = -9964.965365384285
Iteration 12300: Loss = -9964.969581413015
1
Iteration 12400: Loss = -9964.965362189876
Iteration 12500: Loss = -9964.965533282768
1
Iteration 12600: Loss = -9964.965410651725
Iteration 12700: Loss = -9964.965630724899
1
Iteration 12800: Loss = -9965.26303268624
2
Iteration 12900: Loss = -9964.965353055424
Iteration 13000: Loss = -9964.974619191826
1
Iteration 13100: Loss = -9964.966489867238
2
Iteration 13200: Loss = -9964.965940762275
3
Iteration 13300: Loss = -9965.009285460847
4
Iteration 13400: Loss = -9964.96540070311
Iteration 13500: Loss = -9964.969666533172
1
Iteration 13600: Loss = -9964.965736840408
2
Iteration 13700: Loss = -9964.965404042898
Iteration 13800: Loss = -9964.976339479317
1
Iteration 13900: Loss = -9964.965320630577
Iteration 14000: Loss = -9964.980139015504
1
Iteration 14100: Loss = -9964.978137352084
2
Iteration 14200: Loss = -9964.965377089626
Iteration 14300: Loss = -9964.966879460553
1
Iteration 14400: Loss = -9964.965340494755
Iteration 14500: Loss = -9964.965678154269
1
Iteration 14600: Loss = -9964.965416043675
Iteration 14700: Loss = -9964.965785350316
1
Iteration 14800: Loss = -9964.967057279384
2
Iteration 14900: Loss = -9964.966925115968
3
Iteration 15000: Loss = -9964.965341873303
Iteration 15100: Loss = -9964.971556116596
1
Iteration 15200: Loss = -9964.965342372074
Iteration 15300: Loss = -9964.998150602507
1
Iteration 15400: Loss = -9964.965408372229
Iteration 15500: Loss = -9964.966110933135
1
Iteration 15600: Loss = -9964.9657782382
2
Iteration 15700: Loss = -9964.966474181389
3
Iteration 15800: Loss = -9965.00800420019
4
Iteration 15900: Loss = -9964.96533334929
Iteration 16000: Loss = -9964.965584585632
1
Iteration 16100: Loss = -9964.965700714234
2
Iteration 16200: Loss = -9964.96620345579
3
Iteration 16300: Loss = -9964.978467228393
4
Iteration 16400: Loss = -9965.107229237925
5
Iteration 16500: Loss = -9964.972858594369
6
Iteration 16600: Loss = -9964.965390720838
Iteration 16700: Loss = -9964.969336296548
1
Iteration 16800: Loss = -9964.967341784468
2
Iteration 16900: Loss = -9964.965406796502
Iteration 17000: Loss = -9964.965768886601
1
Iteration 17100: Loss = -9964.975007994028
2
Iteration 17200: Loss = -9964.965321254482
Iteration 17300: Loss = -9965.288507364605
1
Iteration 17400: Loss = -9964.965339175173
Iteration 17500: Loss = -9964.965343674116
Iteration 17600: Loss = -9964.965642532567
1
Iteration 17700: Loss = -9964.96536963068
Iteration 17800: Loss = -9965.179009262287
1
Iteration 17900: Loss = -9964.965339590919
Iteration 18000: Loss = -9964.965338029668
Iteration 18100: Loss = -9964.973694282542
1
Iteration 18200: Loss = -9964.965335681038
Iteration 18300: Loss = -9965.0161412727
1
Iteration 18400: Loss = -9964.965345589291
Iteration 18500: Loss = -9964.965343097798
Iteration 18600: Loss = -9964.972077829436
1
Iteration 18700: Loss = -9964.965327362495
Iteration 18800: Loss = -9964.965364384729
Iteration 18900: Loss = -9965.040776861291
1
Iteration 19000: Loss = -9964.965328593446
Iteration 19100: Loss = -9964.965355880544
Iteration 19200: Loss = -9965.29008971924
1
Iteration 19300: Loss = -9964.965344958831
Iteration 19400: Loss = -9964.965403574974
Iteration 19500: Loss = -9964.986652916583
1
Iteration 19600: Loss = -9964.965336519439
Iteration 19700: Loss = -9964.966106135933
1
Iteration 19800: Loss = -9964.965342104066
Iteration 19900: Loss = -9964.966438695394
1
pi: tensor([[0.8092, 0.1908],
        [0.2390, 0.7610]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6177, 0.3823], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1378, 0.0991],
         [0.6002, 0.2266]],

        [[0.5170, 0.1055],
         [0.5648, 0.6775]],

        [[0.6100, 0.1258],
         [0.5253, 0.5872]],

        [[0.6710, 0.0985],
         [0.6474, 0.5364]],

        [[0.5166, 0.1057],
         [0.6078, 0.5601]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 80
Adjusted Rand Index: 0.3536974138593714
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 87
Adjusted Rand Index: 0.543005236161102
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 88
Adjusted Rand Index: 0.5733333333333334
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 78
Adjusted Rand Index: 0.30667787187277784
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 68
Adjusted Rand Index: 0.12102891321335613
Global Adjusted Rand Index: 0.3635677089384681
Average Adjusted Rand Index: 0.3795485536879881
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24163.09287659868
Iteration 100: Loss = -9983.482937039349
Iteration 200: Loss = -9981.025829176233
Iteration 300: Loss = -9979.593394181013
Iteration 400: Loss = -9978.997917915001
Iteration 500: Loss = -9978.625739949164
Iteration 600: Loss = -9978.364116866145
Iteration 700: Loss = -9978.155678060724
Iteration 800: Loss = -9977.964201407362
Iteration 900: Loss = -9977.77240294153
Iteration 1000: Loss = -9977.572815833095
Iteration 1100: Loss = -9977.36732708137
Iteration 1200: Loss = -9977.160482534477
Iteration 1300: Loss = -9976.951600695764
Iteration 1400: Loss = -9976.726929034985
Iteration 1500: Loss = -9976.468266824893
Iteration 1600: Loss = -9976.19864532883
Iteration 1700: Loss = -9975.933449259692
Iteration 1800: Loss = -9975.697057914693
Iteration 1900: Loss = -9975.517119158612
Iteration 2000: Loss = -9975.343669253798
Iteration 2100: Loss = -9975.085921772912
Iteration 2200: Loss = -9974.529314289364
Iteration 2300: Loss = -9974.084741053197
Iteration 2400: Loss = -9973.687906252697
Iteration 2500: Loss = -9973.46907054587
Iteration 2600: Loss = -9972.803095326277
Iteration 2700: Loss = -9971.595368341155
Iteration 2800: Loss = -9970.844116096152
Iteration 2900: Loss = -9970.409033601989
Iteration 3000: Loss = -9969.793397433978
Iteration 3100: Loss = -9969.43631668457
Iteration 3200: Loss = -9968.962100448702
Iteration 3300: Loss = -9968.331628510368
Iteration 3400: Loss = -9967.666204562098
Iteration 3500: Loss = -9967.226007780235
Iteration 3600: Loss = -9967.089285326692
Iteration 3700: Loss = -9966.857408953203
Iteration 3800: Loss = -9966.44311705758
Iteration 3900: Loss = -9966.091880664555
Iteration 4000: Loss = -9965.94399659171
Iteration 4100: Loss = -9965.843122379187
Iteration 4200: Loss = -9965.669985944285
Iteration 4300: Loss = -9965.022545687169
Iteration 4400: Loss = -9964.980347488054
Iteration 4500: Loss = -9964.978866890895
Iteration 4600: Loss = -9964.974851849613
Iteration 4700: Loss = -9964.968449024966
Iteration 4800: Loss = -9964.9673067189
Iteration 4900: Loss = -9964.966641912439
Iteration 5000: Loss = -9964.966592464232
Iteration 5100: Loss = -9964.968335027314
1
Iteration 5200: Loss = -9964.966064619855
Iteration 5300: Loss = -9964.967460779757
1
Iteration 5400: Loss = -9964.966140265615
Iteration 5500: Loss = -9964.965907793625
Iteration 5600: Loss = -9964.966965251082
1
Iteration 5700: Loss = -9964.965721188284
Iteration 5800: Loss = -9964.96581482582
Iteration 5900: Loss = -9964.96568434553
Iteration 6000: Loss = -9964.967656088653
1
Iteration 6100: Loss = -9964.97541049486
2
Iteration 6200: Loss = -9964.965530347135
Iteration 6300: Loss = -9964.965537660153
Iteration 6400: Loss = -9964.965788012925
1
Iteration 6500: Loss = -9964.965486474523
Iteration 6600: Loss = -9964.96551623454
Iteration 6700: Loss = -9964.992806558184
1
Iteration 6800: Loss = -9964.96548698063
Iteration 6900: Loss = -9964.966554804781
1
Iteration 7000: Loss = -9964.965451864735
Iteration 7100: Loss = -9964.973931935123
1
Iteration 7200: Loss = -9964.965451093858
Iteration 7300: Loss = -9964.965421621262
Iteration 7400: Loss = -9964.965405945233
Iteration 7500: Loss = -9964.965381224214
Iteration 7600: Loss = -9964.965652029236
1
Iteration 7700: Loss = -9964.965401815743
Iteration 7800: Loss = -9964.96536633509
Iteration 7900: Loss = -9965.01596762291
1
Iteration 8000: Loss = -9964.965376498125
Iteration 8100: Loss = -9964.965387211743
Iteration 8200: Loss = -9964.965402799618
Iteration 8300: Loss = -9964.965364510103
Iteration 8400: Loss = -9964.974837391317
1
Iteration 8500: Loss = -9964.965346938128
Iteration 8600: Loss = -9964.965663653249
1
Iteration 8700: Loss = -9965.023068539316
2
Iteration 8800: Loss = -9964.965381349419
Iteration 8900: Loss = -9964.968011820367
1
Iteration 9000: Loss = -9964.965364420454
Iteration 9100: Loss = -9964.965397562475
Iteration 9200: Loss = -9964.965388790579
Iteration 9300: Loss = -9965.008194921063
1
Iteration 9400: Loss = -9964.965340342273
Iteration 9500: Loss = -9964.968020227501
1
Iteration 9600: Loss = -9964.965364851194
Iteration 9700: Loss = -9964.965895224605
1
Iteration 9800: Loss = -9964.984052121958
2
Iteration 9900: Loss = -9964.965344483875
Iteration 10000: Loss = -9964.966371572906
1
Iteration 10100: Loss = -9964.965380709513
Iteration 10200: Loss = -9964.96541644235
Iteration 10300: Loss = -9964.965374805945
Iteration 10400: Loss = -9964.966023200599
1
Iteration 10500: Loss = -9964.965326820662
Iteration 10600: Loss = -9964.965531850718
1
Iteration 10700: Loss = -9964.965363644778
Iteration 10800: Loss = -9964.965482336873
1
Iteration 10900: Loss = -9964.965331315421
Iteration 11000: Loss = -9964.96934825836
1
Iteration 11100: Loss = -9964.965373449097
Iteration 11200: Loss = -9964.966312974077
1
Iteration 11300: Loss = -9964.965339433878
Iteration 11400: Loss = -9964.96701859421
1
Iteration 11500: Loss = -9964.965335489456
Iteration 11600: Loss = -9964.991643932035
1
Iteration 11700: Loss = -9964.965375845515
Iteration 11800: Loss = -9964.965337594609
Iteration 11900: Loss = -9964.965677961076
1
Iteration 12000: Loss = -9964.965360952818
Iteration 12100: Loss = -9965.029796009043
1
Iteration 12200: Loss = -9964.965389713
Iteration 12300: Loss = -9964.96534745656
Iteration 12400: Loss = -9965.025102628813
1
Iteration 12500: Loss = -9964.965352485606
Iteration 12600: Loss = -9964.965349453685
Iteration 12700: Loss = -9964.9800607633
1
Iteration 12800: Loss = -9964.965343904165
Iteration 12900: Loss = -9964.965349266637
Iteration 13000: Loss = -9964.966943337584
1
Iteration 13100: Loss = -9964.965377883875
Iteration 13200: Loss = -9964.989652507926
1
Iteration 13300: Loss = -9964.965359216354
Iteration 13400: Loss = -9964.975662318506
1
Iteration 13500: Loss = -9964.965371527693
Iteration 13600: Loss = -9964.965329872844
Iteration 13700: Loss = -9964.965569092692
1
Iteration 13800: Loss = -9964.965352215364
Iteration 13900: Loss = -9964.978288964187
1
Iteration 14000: Loss = -9964.965348399546
Iteration 14100: Loss = -9964.965418698797
Iteration 14200: Loss = -9964.965415659122
Iteration 14300: Loss = -9965.05649192016
1
Iteration 14400: Loss = -9964.965362451385
Iteration 14500: Loss = -9964.966628152017
1
Iteration 14600: Loss = -9964.965342121393
Iteration 14700: Loss = -9964.965348904601
Iteration 14800: Loss = -9964.96541123744
Iteration 14900: Loss = -9964.96718456614
1
Iteration 15000: Loss = -9964.96555338047
2
Iteration 15100: Loss = -9964.965401089947
Iteration 15200: Loss = -9964.96536435865
Iteration 15300: Loss = -9964.965845729737
1
Iteration 15400: Loss = -9964.96574287128
2
Iteration 15500: Loss = -9964.965572237146
3
Iteration 15600: Loss = -9964.965619639062
4
Iteration 15700: Loss = -9964.965388043234
Iteration 15800: Loss = -9964.9669048337
1
Iteration 15900: Loss = -9964.966176342577
2
Iteration 16000: Loss = -9964.965401838463
Iteration 16100: Loss = -9964.98299269032
1
Iteration 16200: Loss = -9965.047149759312
2
Iteration 16300: Loss = -9964.981615791361
3
Iteration 16400: Loss = -9964.965399785704
Iteration 16500: Loss = -9964.967249454394
1
Iteration 16600: Loss = -9964.965608547247
2
Iteration 16700: Loss = -9965.035218079362
3
Iteration 16800: Loss = -9964.965326409494
Iteration 16900: Loss = -9964.965604677534
1
Iteration 17000: Loss = -9964.965415574236
Iteration 17100: Loss = -9964.965406783604
Iteration 17200: Loss = -9964.96531893655
Iteration 17300: Loss = -9964.972144897398
1
Iteration 17400: Loss = -9964.965337956131
Iteration 17500: Loss = -9965.00530349572
1
Iteration 17600: Loss = -9964.965338517824
Iteration 17700: Loss = -9965.374551493187
1
Iteration 17800: Loss = -9964.965328934824
Iteration 17900: Loss = -9964.96538322907
Iteration 18000: Loss = -9964.965906016634
1
Iteration 18100: Loss = -9964.965342567368
Iteration 18200: Loss = -9964.967030017779
1
Iteration 18300: Loss = -9964.965353106227
Iteration 18400: Loss = -9964.96536623757
Iteration 18500: Loss = -9964.965411156629
Iteration 18600: Loss = -9964.965346049283
Iteration 18700: Loss = -9965.131102917629
1
Iteration 18800: Loss = -9964.9653195195
Iteration 18900: Loss = -9964.965343718643
Iteration 19000: Loss = -9964.96990680096
1
Iteration 19100: Loss = -9964.965354481694
Iteration 19200: Loss = -9964.966645472407
1
Iteration 19300: Loss = -9964.965973346194
2
Iteration 19400: Loss = -9964.965448922056
Iteration 19500: Loss = -9964.97149464786
1
Iteration 19600: Loss = -9964.965370335876
Iteration 19700: Loss = -9964.965530331196
1
Iteration 19800: Loss = -9965.031767327388
2
Iteration 19900: Loss = -9964.966504514729
3
pi: tensor([[0.8086, 0.1914],
        [0.2361, 0.7639]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6150, 0.3850], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1383, 0.0988],
         [0.5290, 0.2255]],

        [[0.5683, 0.1050],
         [0.5758, 0.6692]],

        [[0.5033, 0.1253],
         [0.6455, 0.5735]],

        [[0.6483, 0.0980],
         [0.7118, 0.6230]],

        [[0.6331, 0.1056],
         [0.5613, 0.5731]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 81
Adjusted Rand Index: 0.378375035026354
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 87
Adjusted Rand Index: 0.543005236161102
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 88
Adjusted Rand Index: 0.5733333333333334
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 78
Adjusted Rand Index: 0.30667787187277784
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 68
Adjusted Rand Index: 0.12102891321335613
Global Adjusted Rand Index: 0.3684267095385055
Average Adjusted Rand Index: 0.38448407792138467
10096.191343430504
[0.3635677089384681, 0.3684267095385055] [0.3795485536879881, 0.38448407792138467] [9964.965335890822, 9965.008996737724]
-------------------------------------
This iteration is 89
True Objective function: Loss = -9956.438311123846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22492.23679703705
Iteration 100: Loss = -9871.337548428184
Iteration 200: Loss = -9869.977107858344
Iteration 300: Loss = -9869.566266399077
Iteration 400: Loss = -9869.390286991633
Iteration 500: Loss = -9869.299432261578
Iteration 600: Loss = -9869.247219171128
Iteration 700: Loss = -9869.214658608384
Iteration 800: Loss = -9869.192796984802
Iteration 900: Loss = -9869.177142048708
Iteration 1000: Loss = -9869.165398092506
Iteration 1100: Loss = -9869.156164610486
Iteration 1200: Loss = -9869.148586380348
Iteration 1300: Loss = -9869.142072507191
Iteration 1400: Loss = -9869.136233431062
Iteration 1500: Loss = -9869.130841100863
Iteration 1600: Loss = -9869.125677002208
Iteration 1700: Loss = -9869.120610932043
Iteration 1800: Loss = -9869.115361760201
Iteration 1900: Loss = -9869.109931923866
Iteration 2000: Loss = -9869.104086501733
Iteration 2100: Loss = -9869.097701443998
Iteration 2200: Loss = -9869.090432969371
Iteration 2300: Loss = -9869.082061268082
Iteration 2400: Loss = -9869.072008213501
Iteration 2500: Loss = -9869.058971158107
Iteration 2600: Loss = -9869.039518038597
Iteration 2700: Loss = -9868.997868020027
Iteration 2800: Loss = -9868.725316630329
Iteration 2900: Loss = -9867.76740678406
Iteration 3000: Loss = -9867.50639275436
Iteration 3100: Loss = -9867.31735569358
Iteration 3200: Loss = -9867.231443106124
Iteration 3300: Loss = -9867.164098069132
Iteration 3400: Loss = -9867.10723917246
Iteration 3500: Loss = -9867.007575738595
Iteration 3600: Loss = -9866.872925841973
Iteration 3700: Loss = -9866.787493083073
Iteration 3800: Loss = -9866.750706584875
Iteration 3900: Loss = -9866.720588784501
Iteration 4000: Loss = -9866.691511358642
Iteration 4100: Loss = -9866.668381361113
Iteration 4200: Loss = -9866.654320548127
Iteration 4300: Loss = -9866.64514029979
Iteration 4400: Loss = -9866.639078640572
Iteration 4500: Loss = -9866.634513260226
Iteration 4600: Loss = -9866.631308075988
Iteration 4700: Loss = -9866.62847458022
Iteration 4800: Loss = -9866.62843974434
Iteration 4900: Loss = -9866.624492390878
Iteration 5000: Loss = -9866.622934788162
Iteration 5100: Loss = -9866.621672831783
Iteration 5200: Loss = -9866.620434328375
Iteration 5300: Loss = -9866.619565066765
Iteration 5400: Loss = -9866.618537818526
Iteration 5500: Loss = -9866.617694008011
Iteration 5600: Loss = -9866.616947323839
Iteration 5700: Loss = -9866.616249736724
Iteration 5800: Loss = -9866.615683210824
Iteration 5900: Loss = -9866.615046362162
Iteration 6000: Loss = -9866.615284582249
1
Iteration 6100: Loss = -9866.61398228643
Iteration 6200: Loss = -9866.62053721262
1
Iteration 6300: Loss = -9866.61290172267
Iteration 6400: Loss = -9866.612246075501
Iteration 6500: Loss = -9866.611512385505
Iteration 6600: Loss = -9866.610463442668
Iteration 6700: Loss = -9866.609021060787
Iteration 6800: Loss = -9866.607232804432
Iteration 6900: Loss = -9866.608399096012
1
Iteration 7000: Loss = -9866.604534949445
Iteration 7100: Loss = -9866.603939157261
Iteration 7200: Loss = -9866.603599103999
Iteration 7300: Loss = -9866.603379729517
Iteration 7400: Loss = -9866.603250676944
Iteration 7500: Loss = -9866.60313170649
Iteration 7600: Loss = -9866.603118072708
Iteration 7700: Loss = -9866.602930072937
Iteration 7800: Loss = -9866.60285887018
Iteration 7900: Loss = -9866.602799349399
Iteration 8000: Loss = -9866.632613649446
1
Iteration 8100: Loss = -9866.602670146302
Iteration 8200: Loss = -9866.60260565127
Iteration 8300: Loss = -9866.60268443694
Iteration 8400: Loss = -9866.603320012853
1
Iteration 8500: Loss = -9866.611196125716
2
Iteration 8600: Loss = -9866.602406135498
Iteration 8700: Loss = -9866.604123536472
1
Iteration 8800: Loss = -9866.602327288776
Iteration 8900: Loss = -9866.602346270276
Iteration 9000: Loss = -9866.622096975341
1
Iteration 9100: Loss = -9866.614042490513
2
Iteration 9200: Loss = -9866.73429589199
3
Iteration 9300: Loss = -9866.602124508167
Iteration 9400: Loss = -9866.612201949416
1
Iteration 9500: Loss = -9866.602082359048
Iteration 9600: Loss = -9866.603959083319
1
Iteration 9700: Loss = -9866.602056913061
Iteration 9800: Loss = -9866.602050951158
Iteration 9900: Loss = -9866.60195295854
Iteration 10000: Loss = -9866.601976073174
Iteration 10100: Loss = -9866.60465982797
1
Iteration 10200: Loss = -9866.604997155328
2
Iteration 10300: Loss = -9866.60192386408
Iteration 10400: Loss = -9866.60680844651
1
Iteration 10500: Loss = -9866.60189226108
Iteration 10600: Loss = -9866.862425281692
1
Iteration 10700: Loss = -9866.601858466847
Iteration 10800: Loss = -9866.615258705764
1
Iteration 10900: Loss = -9866.601796058827
Iteration 11000: Loss = -9866.605260513048
1
Iteration 11100: Loss = -9866.601735883245
Iteration 11200: Loss = -9866.603770087651
1
Iteration 11300: Loss = -9866.601755623984
Iteration 11400: Loss = -9866.614092302912
1
Iteration 11500: Loss = -9866.601700498151
Iteration 11600: Loss = -9866.664021782193
1
Iteration 11700: Loss = -9866.601688269002
Iteration 11800: Loss = -9866.601857237549
1
Iteration 11900: Loss = -9866.60166465204
Iteration 12000: Loss = -9866.60180160607
1
Iteration 12100: Loss = -9866.60166590567
Iteration 12200: Loss = -9866.602173384312
1
Iteration 12300: Loss = -9866.6016502128
Iteration 12400: Loss = -9866.769148826626
1
Iteration 12500: Loss = -9866.6016405718
Iteration 12600: Loss = -9866.602068170843
1
Iteration 12700: Loss = -9866.604839065561
2
Iteration 12800: Loss = -9866.601620039786
Iteration 12900: Loss = -9866.602242439645
1
Iteration 13000: Loss = -9866.614299522464
2
Iteration 13100: Loss = -9866.601609837975
Iteration 13200: Loss = -9866.606622018151
1
Iteration 13300: Loss = -9866.601628425567
Iteration 13400: Loss = -9866.72223292514
1
Iteration 13500: Loss = -9866.603392171226
2
Iteration 13600: Loss = -9866.60163301427
Iteration 13700: Loss = -9866.64267114647
1
Iteration 13800: Loss = -9866.601562050362
Iteration 13900: Loss = -9866.602531628016
1
Iteration 14000: Loss = -9866.639518077647
2
Iteration 14100: Loss = -9866.601598629222
Iteration 14200: Loss = -9866.605447547174
1
Iteration 14300: Loss = -9866.601534237532
Iteration 14400: Loss = -9866.602467461402
1
Iteration 14500: Loss = -9866.61272354432
2
Iteration 14600: Loss = -9866.606665363157
3
Iteration 14700: Loss = -9866.601528478137
Iteration 14800: Loss = -9866.601893377803
1
Iteration 14900: Loss = -9866.601550055731
Iteration 15000: Loss = -9866.602070363451
1
Iteration 15100: Loss = -9866.601715438987
2
Iteration 15200: Loss = -9866.601584981636
Iteration 15300: Loss = -9866.619707263879
1
Iteration 15400: Loss = -9866.602037473003
2
Iteration 15500: Loss = -9866.670204854792
3
Iteration 15600: Loss = -9866.601514503302
Iteration 15700: Loss = -9866.60206380715
1
Iteration 15800: Loss = -9866.617375577966
2
Iteration 15900: Loss = -9866.697662079503
3
Iteration 16000: Loss = -9866.601535728963
Iteration 16100: Loss = -9866.601560963221
Iteration 16200: Loss = -9866.656746506786
1
Iteration 16300: Loss = -9866.602218108248
2
Iteration 16400: Loss = -9866.601559100725
Iteration 16500: Loss = -9866.602639960245
1
Iteration 16600: Loss = -9866.601498585274
Iteration 16700: Loss = -9866.60212540184
1
Iteration 16800: Loss = -9866.757234615992
2
Iteration 16900: Loss = -9866.601501043711
Iteration 17000: Loss = -9866.60151515582
Iteration 17100: Loss = -9866.601784584562
1
Iteration 17200: Loss = -9866.601540601987
Iteration 17300: Loss = -9866.601560914829
Iteration 17400: Loss = -9866.601500540424
Iteration 17500: Loss = -9866.602126061316
1
Iteration 17600: Loss = -9866.601504229351
Iteration 17700: Loss = -9866.60828030355
1
Iteration 17800: Loss = -9866.60170190049
2
Iteration 17900: Loss = -9866.611105489967
3
Iteration 18000: Loss = -9866.611613912595
4
Iteration 18100: Loss = -9866.60153068353
Iteration 18200: Loss = -9866.6014994972
Iteration 18300: Loss = -9866.716017083136
1
Iteration 18400: Loss = -9866.601502090694
Iteration 18500: Loss = -9866.60168462579
1
Iteration 18600: Loss = -9866.602371519159
2
Iteration 18700: Loss = -9866.601507375823
Iteration 18800: Loss = -9866.601594798138
Iteration 18900: Loss = -9866.601510252398
Iteration 19000: Loss = -9866.6014881603
Iteration 19100: Loss = -9866.601817871233
1
Iteration 19200: Loss = -9866.601504685937
Iteration 19300: Loss = -9866.602859344779
1
Iteration 19400: Loss = -9866.601504351742
Iteration 19500: Loss = -9866.622505494563
1
Iteration 19600: Loss = -9866.601501284791
Iteration 19700: Loss = -9866.952475514174
1
Iteration 19800: Loss = -9866.601508403695
Iteration 19900: Loss = -9866.60147281309
pi: tensor([[3.2808e-01, 6.7192e-01],
        [5.2366e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9845, 0.0155], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1376, 0.1552],
         [0.7239, 0.1373]],

        [[0.6346, 0.1373],
         [0.6722, 0.5529]],

        [[0.5798, 0.1173],
         [0.7040, 0.7186]],

        [[0.6965, 0.1663],
         [0.5457, 0.6072]],

        [[0.6397, 0.0379],
         [0.5061, 0.6485]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: -0.0022281630157266236
Average Adjusted Rand Index: -0.00022637680438637088
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23839.46854146687
Iteration 100: Loss = -9871.082797102903
Iteration 200: Loss = -9869.835239067785
Iteration 300: Loss = -9869.538020374714
Iteration 400: Loss = -9869.389388819718
Iteration 500: Loss = -9869.301393227523
Iteration 600: Loss = -9869.244056012569
Iteration 700: Loss = -9869.204869853304
Iteration 800: Loss = -9869.177727268934
Iteration 900: Loss = -9869.158536837162
Iteration 1000: Loss = -9869.144227802572
Iteration 1100: Loss = -9869.13300056407
Iteration 1200: Loss = -9869.123670599804
Iteration 1300: Loss = -9869.115596920767
Iteration 1400: Loss = -9869.108389038867
Iteration 1500: Loss = -9869.101698341254
Iteration 1600: Loss = -9869.095109038159
Iteration 1700: Loss = -9869.088330838205
Iteration 1800: Loss = -9869.080817901824
Iteration 1900: Loss = -9869.071509013356
Iteration 2000: Loss = -9869.057844113839
Iteration 2100: Loss = -9869.030635011348
Iteration 2200: Loss = -9868.92670653133
Iteration 2300: Loss = -9868.316775041216
Iteration 2400: Loss = -9868.014471425377
Iteration 2500: Loss = -9867.852805035964
Iteration 2600: Loss = -9867.690063291908
Iteration 2700: Loss = -9867.519746821341
Iteration 2800: Loss = -9867.37098001707
Iteration 2900: Loss = -9867.256980709193
Iteration 3000: Loss = -9867.172024953421
Iteration 3100: Loss = -9867.116306255737
Iteration 3200: Loss = -9867.01800190754
Iteration 3300: Loss = -9866.847481868504
Iteration 3400: Loss = -9866.772919565525
Iteration 3500: Loss = -9866.744877775134
Iteration 3600: Loss = -9866.725524891302
Iteration 3700: Loss = -9866.704242845997
Iteration 3800: Loss = -9866.676132700424
Iteration 3900: Loss = -9866.653165198668
Iteration 4000: Loss = -9866.6401353892
Iteration 4100: Loss = -9866.633552645368
Iteration 4200: Loss = -9866.628871520268
Iteration 4300: Loss = -9866.630452476671
1
Iteration 4400: Loss = -9866.623173867802
Iteration 4500: Loss = -9866.621370745233
Iteration 4600: Loss = -9866.620060836085
Iteration 4700: Loss = -9866.618803300877
Iteration 4800: Loss = -9866.654476680213
1
Iteration 4900: Loss = -9866.61699422146
Iteration 5000: Loss = -9866.616241352585
Iteration 5100: Loss = -9866.615578647235
Iteration 5200: Loss = -9866.614938879653
Iteration 5300: Loss = -9866.636898766701
1
Iteration 5400: Loss = -9866.613873446708
Iteration 5500: Loss = -9866.61335940328
Iteration 5600: Loss = -9866.613046150149
Iteration 5700: Loss = -9866.612380969422
Iteration 5800: Loss = -9866.61176907336
Iteration 5900: Loss = -9866.611164014776
Iteration 6000: Loss = -9866.610204267692
Iteration 6100: Loss = -9866.609127127605
Iteration 6200: Loss = -9866.60690981301
Iteration 6300: Loss = -9866.623081065509
1
Iteration 6400: Loss = -9866.604160662027
Iteration 6500: Loss = -9866.603623819416
Iteration 6600: Loss = -9866.603318140289
Iteration 6700: Loss = -9866.603101284445
Iteration 6800: Loss = -9866.603085811737
Iteration 6900: Loss = -9866.602905753587
Iteration 7000: Loss = -9866.603322045929
1
Iteration 7100: Loss = -9866.602769830899
Iteration 7200: Loss = -9866.602717617878
Iteration 7300: Loss = -9866.60273460356
Iteration 7400: Loss = -9866.602530949829
Iteration 7500: Loss = -9866.603365722964
1
Iteration 7600: Loss = -9866.602463792024
Iteration 7700: Loss = -9866.602412448015
Iteration 7800: Loss = -9866.602328352898
Iteration 7900: Loss = -9866.602282686395
Iteration 8000: Loss = -9866.620078568116
1
Iteration 8100: Loss = -9866.60221731611
Iteration 8200: Loss = -9866.602199120392
Iteration 8300: Loss = -9866.602198286279
Iteration 8400: Loss = -9866.602077042216
Iteration 8500: Loss = -9866.602082449994
Iteration 8600: Loss = -9866.602051881116
Iteration 8700: Loss = -9866.602044824833
Iteration 8800: Loss = -9866.603009029823
1
Iteration 8900: Loss = -9866.601971970142
Iteration 9000: Loss = -9866.602247129797
1
Iteration 9100: Loss = -9866.601904352876
Iteration 9200: Loss = -9866.602091424307
1
Iteration 9300: Loss = -9866.601890070115
Iteration 9400: Loss = -9866.602030818014
1
Iteration 9500: Loss = -9866.601846665915
Iteration 9600: Loss = -9866.601942249265
Iteration 9700: Loss = -9866.606287650964
1
Iteration 9800: Loss = -9866.601768137334
Iteration 9900: Loss = -9866.605766452565
1
Iteration 10000: Loss = -9866.601743677014
Iteration 10100: Loss = -9866.603349942361
1
Iteration 10200: Loss = -9866.636928827053
2
Iteration 10300: Loss = -9866.60567431385
3
Iteration 10400: Loss = -9866.601778705803
Iteration 10500: Loss = -9866.603874311608
1
Iteration 10600: Loss = -9866.626865586151
2
Iteration 10700: Loss = -9866.603594035605
3
Iteration 10800: Loss = -9866.646849312823
4
Iteration 10900: Loss = -9866.705754595698
5
Iteration 11000: Loss = -9866.609701805442
6
Iteration 11100: Loss = -9866.601716184485
Iteration 11200: Loss = -9866.601968748644
1
Iteration 11300: Loss = -9866.67999596847
2
Iteration 11400: Loss = -9866.601623376091
Iteration 11500: Loss = -9866.623125177923
1
Iteration 11600: Loss = -9866.824620163185
2
Iteration 11700: Loss = -9866.601600727454
Iteration 11800: Loss = -9866.60463270238
1
Iteration 11900: Loss = -9866.603493371396
2
Iteration 12000: Loss = -9866.60157295225
Iteration 12100: Loss = -9866.619022925235
1
Iteration 12200: Loss = -9866.601596007149
Iteration 12300: Loss = -9866.614979708746
1
Iteration 12400: Loss = -9866.601557478545
Iteration 12500: Loss = -9866.697616692662
1
Iteration 12600: Loss = -9866.601574365508
Iteration 12700: Loss = -9866.63353305496
1
Iteration 12800: Loss = -9866.601559015106
Iteration 12900: Loss = -9866.62347695571
1
Iteration 13000: Loss = -9866.603591721747
2
Iteration 13100: Loss = -9866.601597186502
Iteration 13200: Loss = -9866.60192294525
1
Iteration 13300: Loss = -9866.611222461986
2
Iteration 13400: Loss = -9866.611393677174
3
Iteration 13500: Loss = -9866.601525199652
Iteration 13600: Loss = -9866.61197385565
1
Iteration 13700: Loss = -9866.601512939122
Iteration 13800: Loss = -9866.614364385592
1
Iteration 13900: Loss = -9866.601511325813
Iteration 14000: Loss = -9866.60164205492
1
Iteration 14100: Loss = -9866.604091781535
2
Iteration 14200: Loss = -9866.601536178325
Iteration 14300: Loss = -9866.624629031534
1
Iteration 14400: Loss = -9866.60152769817
Iteration 14500: Loss = -9866.60571043306
1
Iteration 14600: Loss = -9866.602177356772
2
Iteration 14700: Loss = -9866.601560238387
Iteration 14800: Loss = -9866.601706610063
1
Iteration 14900: Loss = -9866.84726340402
2
Iteration 15000: Loss = -9866.601515203845
Iteration 15100: Loss = -9866.613641317508
1
Iteration 15200: Loss = -9866.601476679532
Iteration 15300: Loss = -9866.601530393227
Iteration 15400: Loss = -9866.601509210917
Iteration 15500: Loss = -9866.643089020741
1
Iteration 15600: Loss = -9866.601570103596
Iteration 15700: Loss = -9866.601558530843
Iteration 15800: Loss = -9866.786874821692
1
Iteration 15900: Loss = -9866.601496767176
Iteration 16000: Loss = -9866.603037643496
1
Iteration 16100: Loss = -9866.602710908694
2
Iteration 16200: Loss = -9866.601858530064
3
Iteration 16300: Loss = -9866.605491727341
4
Iteration 16400: Loss = -9866.693286813332
5
Iteration 16500: Loss = -9866.601524966078
Iteration 16600: Loss = -9866.658146241409
1
Iteration 16700: Loss = -9866.601503471347
Iteration 16800: Loss = -9866.609138674336
1
Iteration 16900: Loss = -9866.601472019189
Iteration 17000: Loss = -9866.61584357687
1
Iteration 17100: Loss = -9866.601531283233
Iteration 17200: Loss = -9866.601504128703
Iteration 17300: Loss = -9866.601519821666
Iteration 17400: Loss = -9866.601505598275
Iteration 17500: Loss = -9866.606671869546
1
Iteration 17600: Loss = -9866.601489333221
Iteration 17700: Loss = -9866.605918240504
1
Iteration 17800: Loss = -9866.601499074683
Iteration 17900: Loss = -9866.606184157292
1
Iteration 18000: Loss = -9866.601496260304
Iteration 18100: Loss = -9866.607701803458
1
Iteration 18200: Loss = -9866.633993900032
2
Iteration 18300: Loss = -9866.601505158262
Iteration 18400: Loss = -9866.60344988853
1
Iteration 18500: Loss = -9866.611985475169
2
Iteration 18600: Loss = -9866.601566910223
Iteration 18700: Loss = -9866.60155615692
Iteration 18800: Loss = -9866.602911595577
1
Iteration 18900: Loss = -9866.601547417336
Iteration 19000: Loss = -9866.61296917506
1
Iteration 19100: Loss = -9866.608427081746
2
Iteration 19200: Loss = -9866.602131850987
3
Iteration 19300: Loss = -9866.602011826844
4
Iteration 19400: Loss = -9866.60157247979
Iteration 19500: Loss = -9866.657769953901
1
Iteration 19600: Loss = -9866.626666815306
2
Iteration 19700: Loss = -9866.601520591434
Iteration 19800: Loss = -9866.601997739293
1
Iteration 19900: Loss = -9866.657718782342
2
pi: tensor([[1.0000e+00, 2.9277e-08],
        [6.7201e-01, 3.2799e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0157, 0.9843], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1373, 0.1552],
         [0.5456, 0.1375]],

        [[0.6714, 0.1373],
         [0.7108, 0.5946]],

        [[0.6486, 0.1173],
         [0.5904, 0.5916]],

        [[0.6999, 0.1663],
         [0.5454, 0.5063]],

        [[0.7122, 0.0379],
         [0.5423, 0.7275]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: -0.0022281630157266236
Average Adjusted Rand Index: -0.00022637680438637088
9956.438311123846
[-0.0022281630157266236, -0.0022281630157266236] [-0.00022637680438637088, -0.00022637680438637088] [9866.601701854117, 9866.601489719313]
-------------------------------------
This iteration is 90
True Objective function: Loss = -9972.318924212863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20970.331366630435
Iteration 100: Loss = -9827.650104426215
Iteration 200: Loss = -9826.8538560047
Iteration 300: Loss = -9826.658515393896
Iteration 400: Loss = -9826.58400586252
Iteration 500: Loss = -9826.544974235661
Iteration 600: Loss = -9826.514365412584
Iteration 700: Loss = -9826.485640993287
Iteration 800: Loss = -9826.45552385357
Iteration 900: Loss = -9826.42128225116
Iteration 1000: Loss = -9826.37931994192
Iteration 1100: Loss = -9826.322122127194
Iteration 1200: Loss = -9826.222251234449
Iteration 1300: Loss = -9825.791677865083
Iteration 1400: Loss = -9824.90921987496
Iteration 1500: Loss = -9824.731626328965
Iteration 1600: Loss = -9824.597304511995
Iteration 1700: Loss = -9824.18913682725
Iteration 1800: Loss = -9822.074142673939
Iteration 1900: Loss = -9821.405808462629
Iteration 2000: Loss = -9821.148459499493
Iteration 2100: Loss = -9820.991167271059
Iteration 2200: Loss = -9820.877973852086
Iteration 2300: Loss = -9820.792475864693
Iteration 2400: Loss = -9820.727493234253
Iteration 2500: Loss = -9820.678491463204
Iteration 2600: Loss = -9820.642171071731
Iteration 2700: Loss = -9820.61552671403
Iteration 2800: Loss = -9820.596347495824
Iteration 2900: Loss = -9820.58262247557
Iteration 3000: Loss = -9820.572812440161
Iteration 3100: Loss = -9820.565786922512
Iteration 3200: Loss = -9820.56075851308
Iteration 3300: Loss = -9820.556995345507
Iteration 3400: Loss = -9820.554169877278
Iteration 3500: Loss = -9820.55194959073
Iteration 3600: Loss = -9820.550149262343
Iteration 3700: Loss = -9820.548612912407
Iteration 3800: Loss = -9820.547321293558
Iteration 3900: Loss = -9820.546184778403
Iteration 4000: Loss = -9820.545182728198
Iteration 4100: Loss = -9820.544261569092
Iteration 4200: Loss = -9820.543434527966
Iteration 4300: Loss = -9820.542724336328
Iteration 4400: Loss = -9820.542003479579
Iteration 4500: Loss = -9820.541356643535
Iteration 4600: Loss = -9820.540776373524
Iteration 4700: Loss = -9820.540224559625
Iteration 4800: Loss = -9820.539715058136
Iteration 4900: Loss = -9820.539290264518
Iteration 5000: Loss = -9820.538880001966
Iteration 5100: Loss = -9820.538491560745
Iteration 5200: Loss = -9820.538142451836
Iteration 5300: Loss = -9820.53783412046
Iteration 5400: Loss = -9820.53751202593
Iteration 5500: Loss = -9820.537265418894
Iteration 5600: Loss = -9820.53699380407
Iteration 5700: Loss = -9820.536748528684
Iteration 5800: Loss = -9820.536493320009
Iteration 5900: Loss = -9820.536308875298
Iteration 6000: Loss = -9820.536103823506
Iteration 6100: Loss = -9820.535929837823
Iteration 6200: Loss = -9820.535727812618
Iteration 6300: Loss = -9820.535578487275
Iteration 6400: Loss = -9820.5354169355
Iteration 6500: Loss = -9820.535285672946
Iteration 6600: Loss = -9820.535129944485
Iteration 6700: Loss = -9820.534974584098
Iteration 6800: Loss = -9820.534855425922
Iteration 6900: Loss = -9820.534787123901
Iteration 7000: Loss = -9820.534660171792
Iteration 7100: Loss = -9820.534515289926
Iteration 7200: Loss = -9820.534427030681
Iteration 7300: Loss = -9820.53434635652
Iteration 7400: Loss = -9820.534284508645
Iteration 7500: Loss = -9820.534194545164
Iteration 7600: Loss = -9820.534113600552
Iteration 7700: Loss = -9820.53403759715
Iteration 7800: Loss = -9820.53396626622
Iteration 7900: Loss = -9820.533919721644
Iteration 8000: Loss = -9820.533840870694
Iteration 8100: Loss = -9820.533750485065
Iteration 8200: Loss = -9820.53386354082
1
Iteration 8300: Loss = -9820.533713441195
Iteration 8400: Loss = -9820.533612192849
Iteration 8500: Loss = -9820.533555610544
Iteration 8600: Loss = -9820.541079552959
1
Iteration 8700: Loss = -9820.533484911764
Iteration 8800: Loss = -9820.533404533307
Iteration 8900: Loss = -9820.5333993206
Iteration 9000: Loss = -9820.535145908625
1
Iteration 9100: Loss = -9820.533322475756
Iteration 9200: Loss = -9820.533257298615
Iteration 9300: Loss = -9820.533325544913
Iteration 9400: Loss = -9820.533239937442
Iteration 9500: Loss = -9820.533182638394
Iteration 9600: Loss = -9820.533173139853
Iteration 9700: Loss = -9820.534813306193
1
Iteration 9800: Loss = -9820.533108537902
Iteration 9900: Loss = -9820.533073970297
Iteration 10000: Loss = -9820.533041326345
Iteration 10100: Loss = -9820.542257352212
1
Iteration 10200: Loss = -9820.533014034309
Iteration 10300: Loss = -9820.532941585321
Iteration 10400: Loss = -9820.533101906802
1
Iteration 10500: Loss = -9820.53222429778
Iteration 10600: Loss = -9820.531875499706
Iteration 10700: Loss = -9820.531605388163
Iteration 10800: Loss = -9820.53158362685
Iteration 10900: Loss = -9820.533508337194
1
Iteration 11000: Loss = -9820.531563484827
Iteration 11100: Loss = -9820.531564109406
Iteration 11200: Loss = -9820.594239934348
1
Iteration 11300: Loss = -9820.531550682748
Iteration 11400: Loss = -9820.531516819372
Iteration 11500: Loss = -9820.531520325556
Iteration 11600: Loss = -9820.544645489237
1
Iteration 11700: Loss = -9820.531525391592
Iteration 11800: Loss = -9820.53148249166
Iteration 11900: Loss = -9820.531502324113
Iteration 12000: Loss = -9820.536040388684
1
Iteration 12100: Loss = -9820.531501361334
Iteration 12200: Loss = -9820.531471580003
Iteration 12300: Loss = -9820.531456349696
Iteration 12400: Loss = -9820.53164373512
1
Iteration 12500: Loss = -9820.531440006274
Iteration 12600: Loss = -9820.531442957224
Iteration 12700: Loss = -9820.531427223077
Iteration 12800: Loss = -9820.531440131133
Iteration 12900: Loss = -9820.539702019647
1
Iteration 13000: Loss = -9820.531404488274
Iteration 13100: Loss = -9820.53142029336
Iteration 13200: Loss = -9820.53141743314
Iteration 13300: Loss = -9820.543489117079
1
Iteration 13400: Loss = -9820.5313944364
Iteration 13500: Loss = -9820.531398842882
Iteration 13600: Loss = -9820.531403663665
Iteration 13700: Loss = -9820.531581672534
1
Iteration 13800: Loss = -9820.535260047269
2
Iteration 13900: Loss = -9820.5318104646
3
Iteration 14000: Loss = -9820.532443362345
4
Iteration 14100: Loss = -9820.53145356055
Iteration 14200: Loss = -9820.531614069523
1
Iteration 14300: Loss = -9820.531779259003
2
Iteration 14400: Loss = -9820.531363514829
Iteration 14500: Loss = -9820.545970684461
1
Iteration 14600: Loss = -9820.531572189699
2
Iteration 14700: Loss = -9820.533455503284
3
Iteration 14800: Loss = -9820.531369252434
Iteration 14900: Loss = -9820.531629327377
1
Iteration 15000: Loss = -9820.532768149988
2
Iteration 15100: Loss = -9820.547078579964
3
Iteration 15200: Loss = -9820.53170227217
4
Iteration 15300: Loss = -9820.532219245264
5
Iteration 15400: Loss = -9820.531348747798
Iteration 15500: Loss = -9820.54110905381
1
Iteration 15600: Loss = -9820.532092364105
2
Iteration 15700: Loss = -9820.532795336174
3
Iteration 15800: Loss = -9820.53141378985
Iteration 15900: Loss = -9820.552368975372
1
Iteration 16000: Loss = -9820.53135034135
Iteration 16100: Loss = -9820.53136361039
Iteration 16200: Loss = -9820.531625952151
1
Iteration 16300: Loss = -9820.531338890092
Iteration 16400: Loss = -9820.531377221378
Iteration 16500: Loss = -9820.531374755645
Iteration 16600: Loss = -9820.531388585901
Iteration 16700: Loss = -9820.531451281493
Iteration 16800: Loss = -9820.647548896006
1
Iteration 16900: Loss = -9820.53128986894
Iteration 17000: Loss = -9820.525557347422
Iteration 17100: Loss = -9820.525575690192
Iteration 17200: Loss = -9820.525522607517
Iteration 17300: Loss = -9820.547258582359
1
Iteration 17400: Loss = -9820.52642147699
2
Iteration 17500: Loss = -9820.525612562727
Iteration 17600: Loss = -9820.526045800094
1
Iteration 17700: Loss = -9820.625605006582
2
Iteration 17800: Loss = -9820.530797477619
3
Iteration 17900: Loss = -9820.525763602865
4
Iteration 18000: Loss = -9820.525546066832
Iteration 18100: Loss = -9820.526552850935
1
Iteration 18200: Loss = -9820.525579647443
Iteration 18300: Loss = -9820.527501755649
1
Iteration 18400: Loss = -9820.525526071
Iteration 18500: Loss = -9820.525867018636
1
Iteration 18600: Loss = -9820.525561094842
Iteration 18700: Loss = -9820.525583532364
Iteration 18800: Loss = -9820.526241970176
1
Iteration 18900: Loss = -9820.525755660636
2
Iteration 19000: Loss = -9820.544612330568
3
Iteration 19100: Loss = -9820.52556494685
Iteration 19200: Loss = -9820.526139236354
1
Iteration 19300: Loss = -9820.551241551224
2
Iteration 19400: Loss = -9820.52602358663
3
Iteration 19500: Loss = -9820.525501595628
Iteration 19600: Loss = -9820.525815057932
1
Iteration 19700: Loss = -9820.53622243657
2
Iteration 19800: Loss = -9820.528733456407
3
Iteration 19900: Loss = -9820.52571853057
4
pi: tensor([[1.0000e+00, 7.7275e-09],
        [7.6023e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9800, 0.0200], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1364, 0.1879],
         [0.6436, 0.3872]],

        [[0.5695, 0.0469],
         [0.7212, 0.5938]],

        [[0.5385, 0.0919],
         [0.6296, 0.6902]],

        [[0.5233, 0.1026],
         [0.5296, 0.5855]],

        [[0.7089, 0.1731],
         [0.6210, 0.5242]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -0.0011592778456692542
Average Adjusted Rand Index: -0.002116864058417607
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21414.698575954204
Iteration 100: Loss = -9827.807595301247
Iteration 200: Loss = -9827.14625855315
Iteration 300: Loss = -9826.911544104178
Iteration 400: Loss = -9826.781058470713
Iteration 500: Loss = -9826.69472520927
Iteration 600: Loss = -9826.630619114403
Iteration 700: Loss = -9826.580267576705
Iteration 800: Loss = -9826.540276733012
Iteration 900: Loss = -9826.507328854166
Iteration 1000: Loss = -9826.477567282396
Iteration 1100: Loss = -9826.447716360291
Iteration 1200: Loss = -9826.415551897966
Iteration 1300: Loss = -9826.379695011497
Iteration 1400: Loss = -9826.339778863014
Iteration 1500: Loss = -9826.29580937514
Iteration 1600: Loss = -9826.24770296686
Iteration 1700: Loss = -9826.194594170813
Iteration 1800: Loss = -9826.13455935666
Iteration 1900: Loss = -9826.064199306395
Iteration 2000: Loss = -9825.976183323137
Iteration 2100: Loss = -9825.855890063593
Iteration 2200: Loss = -9825.68539613146
Iteration 2300: Loss = -9825.462335948101
Iteration 2400: Loss = -9825.198856534977
Iteration 2500: Loss = -9824.884616523095
Iteration 2600: Loss = -9824.159901440973
Iteration 2700: Loss = -9821.754125615256
Iteration 2800: Loss = -9821.036759712139
Iteration 2900: Loss = -9820.908166387644
Iteration 3000: Loss = -9819.890810328152
Iteration 3100: Loss = -9818.436177584152
Iteration 3200: Loss = -9818.38752669889
Iteration 3300: Loss = -9818.380675199041
Iteration 3400: Loss = -9818.377738988154
Iteration 3500: Loss = -9818.375909933784
Iteration 3600: Loss = -9818.374606817126
Iteration 3700: Loss = -9818.373728661578
Iteration 3800: Loss = -9818.373058867153
Iteration 3900: Loss = -9818.372529945551
Iteration 4000: Loss = -9818.372111282955
Iteration 4100: Loss = -9818.371812206591
Iteration 4200: Loss = -9818.371586254712
Iteration 4300: Loss = -9818.37138109836
Iteration 4400: Loss = -9818.371207362152
Iteration 4500: Loss = -9818.371085952134
Iteration 4600: Loss = -9818.370954109347
Iteration 4700: Loss = -9818.370898208595
Iteration 4800: Loss = -9818.370834753528
Iteration 4900: Loss = -9818.370810061162
Iteration 5000: Loss = -9818.37072083893
Iteration 5100: Loss = -9818.370698217397
Iteration 5200: Loss = -9818.370684436779
Iteration 5300: Loss = -9818.370646949204
Iteration 5400: Loss = -9818.370607822671
Iteration 5500: Loss = -9818.3706175358
Iteration 5600: Loss = -9818.371969706825
1
Iteration 5700: Loss = -9818.37056350909
Iteration 5800: Loss = -9818.370595650093
Iteration 5900: Loss = -9818.370561644368
Iteration 6000: Loss = -9818.370591244378
Iteration 6100: Loss = -9818.371337115974
1
Iteration 6200: Loss = -9818.370588102658
Iteration 6300: Loss = -9818.37058870945
Iteration 6400: Loss = -9818.370588908045
Iteration 6500: Loss = -9818.370560015144
Iteration 6600: Loss = -9818.370580947309
Iteration 6700: Loss = -9818.370573280195
Iteration 6800: Loss = -9818.37059309924
Iteration 6900: Loss = -9818.372406089706
1
Iteration 7000: Loss = -9818.370575701541
Iteration 7100: Loss = -9818.370602411864
Iteration 7200: Loss = -9818.37060149004
Iteration 7300: Loss = -9818.370557346323
Iteration 7400: Loss = -9818.371326811448
1
Iteration 7500: Loss = -9818.370589297996
Iteration 7600: Loss = -9818.37056645213
Iteration 7700: Loss = -9818.370563932367
Iteration 7800: Loss = -9818.370535730084
Iteration 7900: Loss = -9818.37062373413
Iteration 8000: Loss = -9818.370580393737
Iteration 8100: Loss = -9818.370714096698
1
Iteration 8200: Loss = -9818.370575939927
Iteration 8300: Loss = -9818.370545764348
Iteration 8400: Loss = -9818.370624145587
Iteration 8500: Loss = -9818.370557720023
Iteration 8600: Loss = -9818.377309256703
1
Iteration 8700: Loss = -9818.373909366059
2
Iteration 8800: Loss = -9818.395002049268
3
Iteration 8900: Loss = -9818.3709048339
4
Iteration 9000: Loss = -9818.371234579152
5
Iteration 9100: Loss = -9818.376530346275
6
Iteration 9200: Loss = -9818.381635035339
7
Iteration 9300: Loss = -9818.370562549399
Iteration 9400: Loss = -9818.370642567394
Iteration 9500: Loss = -9818.480161431344
1
Iteration 9600: Loss = -9818.370897800634
2
Iteration 9700: Loss = -9818.372967902651
3
Iteration 9800: Loss = -9818.37058376969
Iteration 9900: Loss = -9818.371065696794
1
Iteration 10000: Loss = -9818.375076080029
2
Iteration 10100: Loss = -9818.370655107206
Iteration 10200: Loss = -9818.39378868437
1
Iteration 10300: Loss = -9818.461372658077
2
Iteration 10400: Loss = -9818.37505452413
3
Iteration 10500: Loss = -9818.400337991436
4
Iteration 10600: Loss = -9818.427806157515
5
Iteration 10700: Loss = -9818.37147326961
6
Iteration 10800: Loss = -9818.370624830823
Iteration 10900: Loss = -9818.371157081188
1
Iteration 11000: Loss = -9818.370933912802
2
Iteration 11100: Loss = -9818.376210899694
3
Iteration 11200: Loss = -9818.37095190584
4
Iteration 11300: Loss = -9818.392131868784
5
Iteration 11400: Loss = -9818.384931357454
6
Iteration 11500: Loss = -9818.371587691136
7
Iteration 11600: Loss = -9818.421842120293
8
Iteration 11700: Loss = -9818.370602113639
Iteration 11800: Loss = -9818.37115978428
1
Iteration 11900: Loss = -9818.376277672405
2
Iteration 12000: Loss = -9818.3723298879
3
Iteration 12100: Loss = -9818.370729598148
4
Iteration 12200: Loss = -9818.381324669985
5
Iteration 12300: Loss = -9818.39672525444
6
Iteration 12400: Loss = -9818.374835027073
7
Iteration 12500: Loss = -9818.370672306628
Iteration 12600: Loss = -9818.371536651577
1
Iteration 12700: Loss = -9818.370707549982
Iteration 12800: Loss = -9818.371035645498
1
Iteration 12900: Loss = -9818.370908319139
2
Iteration 13000: Loss = -9818.395287002339
3
Iteration 13100: Loss = -9818.37507110822
4
Iteration 13200: Loss = -9818.374014643243
5
Iteration 13300: Loss = -9818.38433194301
6
Iteration 13400: Loss = -9818.393401648658
7
Iteration 13500: Loss = -9818.370903591676
8
Iteration 13600: Loss = -9818.4198111492
9
Iteration 13700: Loss = -9818.374624526976
10
Iteration 13800: Loss = -9818.432931830079
11
Iteration 13900: Loss = -9818.40912029113
12
Iteration 14000: Loss = -9818.378927958509
13
Iteration 14100: Loss = -9818.370611340548
Iteration 14200: Loss = -9818.415125901325
1
Iteration 14300: Loss = -9818.419939807072
2
Iteration 14400: Loss = -9818.370651442097
Iteration 14500: Loss = -9818.37545875461
1
Iteration 14600: Loss = -9818.379753333029
2
Iteration 14700: Loss = -9818.379647419277
3
Iteration 14800: Loss = -9818.370930585092
4
Iteration 14900: Loss = -9818.371234665501
5
Iteration 15000: Loss = -9818.38185002236
6
Iteration 15100: Loss = -9818.370766206403
7
Iteration 15200: Loss = -9818.381481604923
8
Iteration 15300: Loss = -9818.442810416673
9
Iteration 15400: Loss = -9818.3809603192
10
Iteration 15500: Loss = -9818.44895121857
11
Iteration 15600: Loss = -9818.431979268684
12
Iteration 15700: Loss = -9818.370814439708
13
Iteration 15800: Loss = -9818.370595497907
Iteration 15900: Loss = -9818.373659680312
1
Iteration 16000: Loss = -9818.370689426269
Iteration 16100: Loss = -9818.371239184784
1
Iteration 16200: Loss = -9818.38723516453
2
Iteration 16300: Loss = -9818.373258239979
3
Iteration 16400: Loss = -9818.404538752806
4
Iteration 16500: Loss = -9818.380756467874
5
Iteration 16600: Loss = -9818.417273906554
6
Iteration 16700: Loss = -9818.37056975941
Iteration 16800: Loss = -9818.370728884114
1
Iteration 16900: Loss = -9818.379926554851
2
Iteration 17000: Loss = -9818.37059893989
Iteration 17100: Loss = -9818.372143073828
1
Iteration 17200: Loss = -9818.370673078085
Iteration 17300: Loss = -9818.37672252153
1
Iteration 17400: Loss = -9818.416160017992
2
Iteration 17500: Loss = -9818.370565409265
Iteration 17600: Loss = -9818.380276715276
1
Iteration 17700: Loss = -9818.371977518955
2
Iteration 17800: Loss = -9818.371753706338
3
Iteration 17900: Loss = -9818.37064888428
Iteration 18000: Loss = -9818.39322544602
1
Iteration 18100: Loss = -9818.372671271587
2
Iteration 18200: Loss = -9818.371433286391
3
Iteration 18300: Loss = -9818.386232217554
4
Iteration 18400: Loss = -9818.37426202013
5
Iteration 18500: Loss = -9818.422258342742
6
Iteration 18600: Loss = -9818.39569708821
7
Iteration 18700: Loss = -9818.382777729345
8
Iteration 18800: Loss = -9818.382045263921
9
Iteration 18900: Loss = -9818.382034958666
10
Iteration 19000: Loss = -9818.371259021722
11
Iteration 19100: Loss = -9818.370998009434
12
Iteration 19200: Loss = -9818.37066635293
Iteration 19300: Loss = -9818.371884576996
1
Iteration 19400: Loss = -9818.371139604244
2
Iteration 19500: Loss = -9818.372125802629
3
Iteration 19600: Loss = -9818.371031223918
4
Iteration 19700: Loss = -9818.379024936246
5
Iteration 19800: Loss = -9818.375011810469
6
Iteration 19900: Loss = -9818.390059436386
7
pi: tensor([[0.9907, 0.0093],
        [0.0282, 0.9718]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6366, 0.3634], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1250, 0.1348],
         [0.5482, 0.1934]],

        [[0.5485, 0.1217],
         [0.6794, 0.6758]],

        [[0.6690, 0.1311],
         [0.5896, 0.6569]],

        [[0.5138, 0.1148],
         [0.6024, 0.6697]],

        [[0.6964, 0.1456],
         [0.6953, 0.7087]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 65
Adjusted Rand Index: 0.08136235645024954
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 72
Adjusted Rand Index: 0.1851165197145752
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 70
Adjusted Rand Index: 0.15231431646932186
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 72
Adjusted Rand Index: 0.1862961946431166
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.031304993622757876
Global Adjusted Rand Index: 0.12499190842474309
Average Adjusted Rand Index: 0.12727887618000422
9972.318924212863
[-0.0011592778456692542, 0.12499190842474309] [-0.002116864058417607, 0.12727887618000422] [9820.52657224841, 9818.370807117433]
-------------------------------------
This iteration is 91
True Objective function: Loss = -9890.645055040199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24989.933308884672
Iteration 100: Loss = -9745.754565507765
Iteration 200: Loss = -9744.615266674722
Iteration 300: Loss = -9744.27707678273
Iteration 400: Loss = -9744.09223258474
Iteration 500: Loss = -9743.944413421572
Iteration 600: Loss = -9743.778079330647
Iteration 700: Loss = -9743.423379246013
Iteration 800: Loss = -9741.947120832187
Iteration 900: Loss = -9741.374968119373
Iteration 1000: Loss = -9741.231194278558
Iteration 1100: Loss = -9741.180086392767
Iteration 1200: Loss = -9741.15804591131
Iteration 1300: Loss = -9741.1463249708
Iteration 1400: Loss = -9741.138860522316
Iteration 1500: Loss = -9741.13325749285
Iteration 1600: Loss = -9741.128468543679
Iteration 1700: Loss = -9741.124188444735
Iteration 1800: Loss = -9741.12020957644
Iteration 1900: Loss = -9741.116435287757
Iteration 2000: Loss = -9741.112876031722
Iteration 2100: Loss = -9741.109357755196
Iteration 2200: Loss = -9741.106060153397
Iteration 2300: Loss = -9741.102905735324
Iteration 2400: Loss = -9741.099898764094
Iteration 2500: Loss = -9741.097041737086
Iteration 2600: Loss = -9741.09437575159
Iteration 2700: Loss = -9741.091874838876
Iteration 2800: Loss = -9741.089569810227
Iteration 2900: Loss = -9741.087396653164
Iteration 3000: Loss = -9741.085398257437
Iteration 3100: Loss = -9741.083531555116
Iteration 3200: Loss = -9741.081848707607
Iteration 3300: Loss = -9741.080268028916
Iteration 3400: Loss = -9741.078816649508
Iteration 3500: Loss = -9741.07748056104
Iteration 3600: Loss = -9741.07626275156
Iteration 3700: Loss = -9741.075125952386
Iteration 3800: Loss = -9741.074125950368
Iteration 3900: Loss = -9741.073164608473
Iteration 4000: Loss = -9741.072286766044
Iteration 4100: Loss = -9741.07148281508
Iteration 4200: Loss = -9741.07072925848
Iteration 4300: Loss = -9741.070030942674
Iteration 4400: Loss = -9741.069393618973
Iteration 4500: Loss = -9741.068802810996
Iteration 4600: Loss = -9741.068240424343
Iteration 4700: Loss = -9741.067728826134
Iteration 4800: Loss = -9741.067272524655
Iteration 4900: Loss = -9741.066829285253
Iteration 5000: Loss = -9741.066375108683
Iteration 5100: Loss = -9741.069669044211
1
Iteration 5200: Loss = -9741.065647863781
Iteration 5300: Loss = -9741.065321473734
Iteration 5400: Loss = -9741.065048722285
Iteration 5500: Loss = -9741.064675434876
Iteration 5600: Loss = -9741.064858956877
1
Iteration 5700: Loss = -9741.064144544014
Iteration 5800: Loss = -9741.063890590041
Iteration 5900: Loss = -9741.063675542599
Iteration 6000: Loss = -9741.063418053722
Iteration 6100: Loss = -9741.063225602427
Iteration 6200: Loss = -9741.063037409063
Iteration 6300: Loss = -9741.063367613091
1
Iteration 6400: Loss = -9741.062659243536
Iteration 6500: Loss = -9741.069865141044
1
Iteration 6600: Loss = -9741.062328779575
Iteration 6700: Loss = -9741.06215502582
Iteration 6800: Loss = -9741.061985687316
Iteration 6900: Loss = -9741.061841074219
Iteration 7000: Loss = -9741.061685224588
Iteration 7100: Loss = -9741.061522758531
Iteration 7200: Loss = -9741.06135519831
Iteration 7300: Loss = -9741.0611616092
Iteration 7400: Loss = -9741.0609910642
Iteration 7500: Loss = -9741.060819030576
Iteration 7600: Loss = -9741.060643207173
Iteration 7700: Loss = -9741.060455278734
Iteration 7800: Loss = -9741.060307798776
Iteration 7900: Loss = -9741.06229369104
1
Iteration 8000: Loss = -9741.059927153361
Iteration 8100: Loss = -9741.060398400374
1
Iteration 8200: Loss = -9741.05958563046
Iteration 8300: Loss = -9741.059499406709
Iteration 8400: Loss = -9741.059424659396
Iteration 8500: Loss = -9741.059334881356
Iteration 8600: Loss = -9741.060183613372
1
Iteration 8700: Loss = -9741.059204700196
Iteration 8800: Loss = -9741.059514521576
1
Iteration 8900: Loss = -9741.05912363105
Iteration 9000: Loss = -9741.059276705382
1
Iteration 9100: Loss = -9741.059349209541
2
Iteration 9200: Loss = -9741.059059071815
Iteration 9300: Loss = -9741.059112637033
Iteration 9400: Loss = -9741.058999027782
Iteration 9500: Loss = -9741.058956907533
Iteration 9600: Loss = -9741.060580982621
1
Iteration 9700: Loss = -9741.058932676618
Iteration 9800: Loss = -9741.05891018262
Iteration 9900: Loss = -9741.058915966078
Iteration 10000: Loss = -9741.05887794762
Iteration 10100: Loss = -9741.123584018322
1
Iteration 10200: Loss = -9741.058853766646
Iteration 10300: Loss = -9741.058823370022
Iteration 10400: Loss = -9741.059370969855
1
Iteration 10500: Loss = -9741.058835701688
Iteration 10600: Loss = -9741.058811685833
Iteration 10700: Loss = -9741.059165983985
1
Iteration 10800: Loss = -9741.05880518291
Iteration 10900: Loss = -9741.099004944164
1
Iteration 11000: Loss = -9741.058797611877
Iteration 11100: Loss = -9741.0590214485
1
Iteration 11200: Loss = -9741.0589413597
2
Iteration 11300: Loss = -9741.058758846704
Iteration 11400: Loss = -9741.074633097422
1
Iteration 11500: Loss = -9741.058757071853
Iteration 11600: Loss = -9741.058713907343
Iteration 11700: Loss = -9741.061678836517
1
Iteration 11800: Loss = -9741.058750847169
Iteration 11900: Loss = -9741.059615416892
1
Iteration 12000: Loss = -9741.058692656326
Iteration 12100: Loss = -9741.267435783268
1
Iteration 12200: Loss = -9741.058718731949
Iteration 12300: Loss = -9741.058672989062
Iteration 12400: Loss = -9741.058801531351
1
Iteration 12500: Loss = -9741.058673006663
Iteration 12600: Loss = -9741.058656624513
Iteration 12700: Loss = -9741.064283310105
1
Iteration 12800: Loss = -9741.058663770618
Iteration 12900: Loss = -9741.058666049204
Iteration 13000: Loss = -9741.059203566238
1
Iteration 13100: Loss = -9741.058657785894
Iteration 13200: Loss = -9741.06208144702
1
Iteration 13300: Loss = -9741.058663454989
Iteration 13400: Loss = -9741.058660381692
Iteration 13500: Loss = -9741.058888780151
1
Iteration 13600: Loss = -9741.058687688548
Iteration 13700: Loss = -9741.072844270397
1
Iteration 13800: Loss = -9741.058652056654
Iteration 13900: Loss = -9741.058672767474
Iteration 14000: Loss = -9741.062422994559
1
Iteration 14100: Loss = -9741.058643695489
Iteration 14200: Loss = -9741.05975878369
1
Iteration 14300: Loss = -9741.058622492192
Iteration 14400: Loss = -9741.06445721998
1
Iteration 14500: Loss = -9741.058640511133
Iteration 14600: Loss = -9741.075993240327
1
Iteration 14700: Loss = -9741.058678620811
Iteration 14800: Loss = -9741.05863379273
Iteration 14900: Loss = -9741.058881134662
1
Iteration 15000: Loss = -9741.058629002577
Iteration 15100: Loss = -9741.152437531504
1
Iteration 15200: Loss = -9741.05867558849
Iteration 15300: Loss = -9741.058645597146
Iteration 15400: Loss = -9741.0955364856
1
Iteration 15500: Loss = -9741.05864090527
Iteration 15600: Loss = -9741.058632634431
Iteration 15700: Loss = -9741.058781008365
1
Iteration 15800: Loss = -9741.058618459696
Iteration 15900: Loss = -9741.059163872886
1
Iteration 16000: Loss = -9741.058633819843
Iteration 16100: Loss = -9741.066933750357
1
Iteration 16200: Loss = -9741.058599407686
Iteration 16300: Loss = -9741.058646410598
Iteration 16400: Loss = -9741.058658474196
Iteration 16500: Loss = -9741.328880908688
1
Iteration 16600: Loss = -9741.058618038296
Iteration 16700: Loss = -9741.18574362241
1
Iteration 16800: Loss = -9741.058619964866
Iteration 16900: Loss = -9741.058630777947
Iteration 17000: Loss = -9741.059678305252
1
Iteration 17100: Loss = -9741.058605027943
Iteration 17200: Loss = -9741.059099610693
1
Iteration 17300: Loss = -9741.058620971504
Iteration 17400: Loss = -9741.058944054283
1
Iteration 17500: Loss = -9741.058690306647
Iteration 17600: Loss = -9741.058629487408
Iteration 17700: Loss = -9741.06450527472
1
Iteration 17800: Loss = -9741.060040750917
2
Iteration 17900: Loss = -9741.216682742366
3
Iteration 18000: Loss = -9741.058670000175
Iteration 18100: Loss = -9741.05881323413
1
Iteration 18200: Loss = -9741.172726084264
2
Iteration 18300: Loss = -9741.058601313274
Iteration 18400: Loss = -9741.059354618123
1
Iteration 18500: Loss = -9741.058596406858
Iteration 18600: Loss = -9741.05866920638
Iteration 18700: Loss = -9741.058631472171
Iteration 18800: Loss = -9741.059679750819
1
Iteration 18900: Loss = -9741.058633941788
Iteration 19000: Loss = -9741.059024325832
1
Iteration 19100: Loss = -9741.058616555663
Iteration 19200: Loss = -9741.05952502219
1
Iteration 19300: Loss = -9741.058628815197
Iteration 19400: Loss = -9741.070232901111
1
Iteration 19500: Loss = -9741.058602951696
Iteration 19600: Loss = -9741.05899694738
1
Iteration 19700: Loss = -9741.058790180754
2
Iteration 19800: Loss = -9741.058707220434
3
Iteration 19900: Loss = -9741.058638102155
pi: tensor([[9.3977e-01, 6.0228e-02],
        [1.0000e+00, 1.6663e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9934, 0.0066], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1299, 0.1138],
         [0.5839, 0.1667]],

        [[0.5086, 0.2008],
         [0.6264, 0.7120]],

        [[0.6498, 0.1592],
         [0.6286, 0.6288]],

        [[0.6843, 0.1278],
         [0.7056, 0.7249]],

        [[0.7209, 0.2135],
         [0.6402, 0.5218]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.004267232452421997
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.009618156350865796
Global Adjusted Rand Index: 0.0030671300492161446
Average Adjusted Rand Index: 0.0010701847796887598
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24228.965660609276
Iteration 100: Loss = -9747.24204533158
Iteration 200: Loss = -9745.488022994972
Iteration 300: Loss = -9745.016476818157
Iteration 400: Loss = -9744.7115787174
Iteration 500: Loss = -9744.241689764589
Iteration 600: Loss = -9743.67557353445
Iteration 700: Loss = -9743.279708220462
Iteration 800: Loss = -9742.92781869394
Iteration 900: Loss = -9742.607140856466
Iteration 1000: Loss = -9742.283892972091
Iteration 1100: Loss = -9741.970941762253
Iteration 1200: Loss = -9741.690540730415
Iteration 1300: Loss = -9741.46326762828
Iteration 1400: Loss = -9741.293005247931
Iteration 1500: Loss = -9741.202063112245
Iteration 1600: Loss = -9741.159952196856
Iteration 1700: Loss = -9741.141549085367
Iteration 1800: Loss = -9741.1319970989
Iteration 1900: Loss = -9741.126059941684
Iteration 2000: Loss = -9741.121685124213
Iteration 2100: Loss = -9741.117915848763
Iteration 2200: Loss = -9741.114563875213
Iteration 2300: Loss = -9741.11135695126
Iteration 2400: Loss = -9741.108369871357
Iteration 2500: Loss = -9741.105470546818
Iteration 2600: Loss = -9741.102614856396
Iteration 2700: Loss = -9741.099870552718
Iteration 2800: Loss = -9741.097230266123
Iteration 2900: Loss = -9741.094758316622
Iteration 3000: Loss = -9741.0923528954
Iteration 3100: Loss = -9741.090045088664
Iteration 3200: Loss = -9741.087892721533
Iteration 3300: Loss = -9741.085825548642
Iteration 3400: Loss = -9741.08392373359
Iteration 3500: Loss = -9741.08213570202
Iteration 3600: Loss = -9741.080443511284
Iteration 3700: Loss = -9741.078898558659
Iteration 3800: Loss = -9741.077425407424
Iteration 3900: Loss = -9741.076062526281
Iteration 4000: Loss = -9741.07482403374
Iteration 4100: Loss = -9741.073760708477
Iteration 4200: Loss = -9741.072638715938
Iteration 4300: Loss = -9741.071664857594
Iteration 4400: Loss = -9741.071102163916
Iteration 4500: Loss = -9741.069942068021
Iteration 4600: Loss = -9741.069193696956
Iteration 4700: Loss = -9741.06868783548
Iteration 4800: Loss = -9741.067872272823
Iteration 4900: Loss = -9741.067304779011
Iteration 5000: Loss = -9741.066752544895
Iteration 5100: Loss = -9741.066260515068
Iteration 5200: Loss = -9741.065969778581
Iteration 5300: Loss = -9741.065334664854
Iteration 5400: Loss = -9741.064921741108
Iteration 5500: Loss = -9741.064551321959
Iteration 5600: Loss = -9741.064151224686
Iteration 5700: Loss = -9741.063851932004
Iteration 5800: Loss = -9741.063534267298
Iteration 5900: Loss = -9741.063217798712
Iteration 6000: Loss = -9741.063782713632
1
Iteration 6100: Loss = -9741.062686616226
Iteration 6200: Loss = -9741.062411158013
Iteration 6300: Loss = -9741.062281296036
Iteration 6400: Loss = -9741.062006897604
Iteration 6500: Loss = -9741.061785895094
Iteration 6600: Loss = -9741.06161832406
Iteration 6700: Loss = -9741.061453426108
Iteration 6800: Loss = -9741.061273026298
Iteration 6900: Loss = -9741.061108690436
Iteration 7000: Loss = -9741.061213922912
1
Iteration 7100: Loss = -9741.06083907532
Iteration 7200: Loss = -9741.060715151843
Iteration 7300: Loss = -9741.060608062737
Iteration 7400: Loss = -9741.060465326784
Iteration 7500: Loss = -9741.0603820123
Iteration 7600: Loss = -9741.060313591886
Iteration 7700: Loss = -9741.06020224753
Iteration 7800: Loss = -9741.060104474094
Iteration 7900: Loss = -9741.061607096892
1
Iteration 8000: Loss = -9741.059952113737
Iteration 8100: Loss = -9741.067087185216
1
Iteration 8200: Loss = -9741.059796714602
Iteration 8300: Loss = -9741.060413216886
1
Iteration 8400: Loss = -9741.059690400443
Iteration 8500: Loss = -9741.060653852832
1
Iteration 8600: Loss = -9741.059589625806
Iteration 8700: Loss = -9741.060548234467
1
Iteration 8800: Loss = -9741.05948452537
Iteration 8900: Loss = -9741.063637481539
1
Iteration 9000: Loss = -9741.06061700262
2
Iteration 9100: Loss = -9741.059359689647
Iteration 9200: Loss = -9741.100030955724
1
Iteration 9300: Loss = -9741.059285114796
Iteration 9400: Loss = -9741.059246202756
Iteration 9500: Loss = -9741.059668198252
1
Iteration 9600: Loss = -9741.059186379456
Iteration 9700: Loss = -9741.059149762308
Iteration 9800: Loss = -9741.0594779079
1
Iteration 9900: Loss = -9741.059106007238
Iteration 10000: Loss = -9741.059181572415
Iteration 10100: Loss = -9741.059072793985
Iteration 10200: Loss = -9741.059005827672
Iteration 10300: Loss = -9741.059033116806
Iteration 10400: Loss = -9741.058971802142
Iteration 10500: Loss = -9741.058984616338
Iteration 10600: Loss = -9741.470162758917
1
Iteration 10700: Loss = -9741.058925682439
Iteration 10800: Loss = -9741.05891568332
Iteration 10900: Loss = -9741.103627902667
1
Iteration 11000: Loss = -9741.058870435665
Iteration 11100: Loss = -9741.05888619457
Iteration 11200: Loss = -9741.063884938178
1
Iteration 11300: Loss = -9741.058849084142
Iteration 11400: Loss = -9741.058836821581
Iteration 11500: Loss = -9741.05892150724
Iteration 11600: Loss = -9741.058808113363
Iteration 11700: Loss = -9741.058811102244
Iteration 11800: Loss = -9741.058906084849
Iteration 11900: Loss = -9741.058772427263
Iteration 12000: Loss = -9741.10618698992
1
Iteration 12100: Loss = -9741.058812361656
Iteration 12200: Loss = -9741.058762300998
Iteration 12300: Loss = -9741.05921284191
1
Iteration 12400: Loss = -9741.058774814494
Iteration 12500: Loss = -9741.18689314664
1
Iteration 12600: Loss = -9741.058765001242
Iteration 12700: Loss = -9741.058734757662
Iteration 12800: Loss = -9741.072727938863
1
Iteration 12900: Loss = -9741.058723425509
Iteration 13000: Loss = -9741.059068374547
1
Iteration 13100: Loss = -9741.05871433492
Iteration 13200: Loss = -9741.058698574268
Iteration 13300: Loss = -9741.059079613393
1
Iteration 13400: Loss = -9741.058739223263
Iteration 13500: Loss = -9741.0586833959
Iteration 13600: Loss = -9741.060940438123
1
Iteration 13700: Loss = -9741.05898162981
2
Iteration 13800: Loss = -9741.05884959556
3
Iteration 13900: Loss = -9741.064781196115
4
Iteration 14000: Loss = -9741.058690898757
Iteration 14100: Loss = -9741.058698529536
Iteration 14200: Loss = -9741.078865471787
1
Iteration 14300: Loss = -9741.058677180206
Iteration 14400: Loss = -9741.080473400556
1
Iteration 14500: Loss = -9741.058648713226
Iteration 14600: Loss = -9741.069600141094
1
Iteration 14700: Loss = -9741.058648966482
Iteration 14800: Loss = -9741.062330987925
1
Iteration 14900: Loss = -9741.05864277663
Iteration 15000: Loss = -9741.065126764033
1
Iteration 15100: Loss = -9741.058626697268
Iteration 15200: Loss = -9741.092474416784
1
Iteration 15300: Loss = -9741.058657006426
Iteration 15400: Loss = -9741.124418167128
1
Iteration 15500: Loss = -9741.058665287232
Iteration 15600: Loss = -9741.066596517043
1
Iteration 15700: Loss = -9741.05865636509
Iteration 15800: Loss = -9741.0680360419
1
Iteration 15900: Loss = -9741.058602900852
Iteration 16000: Loss = -9741.07018703276
1
Iteration 16100: Loss = -9741.058650917246
Iteration 16200: Loss = -9741.061636746912
1
Iteration 16300: Loss = -9741.05862162321
Iteration 16400: Loss = -9741.064708063652
1
Iteration 16500: Loss = -9741.058637562603
Iteration 16600: Loss = -9741.136541805232
1
Iteration 16700: Loss = -9741.058632207621
Iteration 16800: Loss = -9741.072344900824
1
Iteration 16900: Loss = -9741.058651180003
Iteration 17000: Loss = -9741.06048299419
1
Iteration 17100: Loss = -9741.058660304525
Iteration 17200: Loss = -9741.05860511773
Iteration 17300: Loss = -9741.058771297887
1
Iteration 17400: Loss = -9741.058624769334
Iteration 17500: Loss = -9741.279434163844
1
Iteration 17600: Loss = -9741.058616986196
Iteration 17700: Loss = -9741.0586056731
Iteration 17800: Loss = -9741.10293015467
1
Iteration 17900: Loss = -9741.058611814195
Iteration 18000: Loss = -9741.058645246512
Iteration 18100: Loss = -9741.060179876476
1
Iteration 18200: Loss = -9741.058646879677
Iteration 18300: Loss = -9741.058652742262
Iteration 18400: Loss = -9741.058730529334
Iteration 18500: Loss = -9741.058624145708
Iteration 18600: Loss = -9741.060166215646
1
Iteration 18700: Loss = -9741.118521062668
2
Iteration 18800: Loss = -9741.058645150206
Iteration 18900: Loss = -9741.084185212449
1
Iteration 19000: Loss = -9741.05861347623
Iteration 19100: Loss = -9741.074305632665
1
Iteration 19200: Loss = -9741.05860244791
Iteration 19300: Loss = -9741.059233981667
1
Iteration 19400: Loss = -9741.058644409002
Iteration 19500: Loss = -9741.06709441768
1
Iteration 19600: Loss = -9741.058613116867
Iteration 19700: Loss = -9741.117222616318
1
Iteration 19800: Loss = -9741.058607307234
Iteration 19900: Loss = -9741.058738485375
1
pi: tensor([[9.3967e-01, 6.0333e-02],
        [1.0000e+00, 3.0436e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9934, 0.0066], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1298, 0.1139],
         [0.6441, 0.1668]],

        [[0.5101, 0.2006],
         [0.5697, 0.5182]],

        [[0.6783, 0.1592],
         [0.7226, 0.7108]],

        [[0.6212, 0.1279],
         [0.5418, 0.6871]],

        [[0.6469, 0.2133],
         [0.6796, 0.6328]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.004267232452421997
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.009618156350865796
Global Adjusted Rand Index: 0.0030671300492161446
Average Adjusted Rand Index: 0.0010701847796887598
9890.645055040199
[0.0030671300492161446, 0.0030671300492161446] [0.0010701847796887598, 0.0010701847796887598] [9741.058608005747, 9741.059116191866]
-------------------------------------
This iteration is 92
True Objective function: Loss = -9944.979100986695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22456.020971101538
Iteration 100: Loss = -9794.914083302712
Iteration 200: Loss = -9790.497506053427
Iteration 300: Loss = -9788.709227982994
Iteration 400: Loss = -9787.934358163808
Iteration 500: Loss = -9787.45656337
Iteration 600: Loss = -9787.056550851361
Iteration 700: Loss = -9786.653209283886
Iteration 800: Loss = -9785.79733675163
Iteration 900: Loss = -9785.515213148668
Iteration 1000: Loss = -9785.295305935977
Iteration 1100: Loss = -9785.040937745833
Iteration 1200: Loss = -9784.887652591027
Iteration 1300: Loss = -9784.684782752478
Iteration 1400: Loss = -9784.432003123527
Iteration 1500: Loss = -9784.205526881751
Iteration 1600: Loss = -9784.084430433373
Iteration 1700: Loss = -9784.005373997628
Iteration 1800: Loss = -9783.952971786961
Iteration 1900: Loss = -9783.916055564587
Iteration 2000: Loss = -9783.888623857425
Iteration 2100: Loss = -9783.866652427034
Iteration 2200: Loss = -9783.846988359983
Iteration 2300: Loss = -9783.830754332908
Iteration 2400: Loss = -9783.819090983976
Iteration 2500: Loss = -9783.809879067887
Iteration 2600: Loss = -9783.802145271844
Iteration 2700: Loss = -9783.795563529473
Iteration 2800: Loss = -9783.790036346332
Iteration 2900: Loss = -9783.785444480412
Iteration 3000: Loss = -9783.781566321906
Iteration 3100: Loss = -9783.778230199847
Iteration 3200: Loss = -9783.775400311135
Iteration 3300: Loss = -9783.772921737294
Iteration 3400: Loss = -9783.770743282492
Iteration 3500: Loss = -9783.768817221417
Iteration 3600: Loss = -9783.767080416623
Iteration 3700: Loss = -9783.765595145105
Iteration 3800: Loss = -9783.76424094253
Iteration 3900: Loss = -9783.763049648705
Iteration 4000: Loss = -9783.761899092933
Iteration 4100: Loss = -9783.760929432883
Iteration 4200: Loss = -9783.760042674243
Iteration 4300: Loss = -9783.759212470386
Iteration 4400: Loss = -9783.75849611268
Iteration 4500: Loss = -9783.757808532171
Iteration 4600: Loss = -9783.757226840236
Iteration 4700: Loss = -9783.756692553648
Iteration 4800: Loss = -9783.756181606675
Iteration 4900: Loss = -9783.755707076869
Iteration 5000: Loss = -9783.75525105857
Iteration 5100: Loss = -9783.754850950158
Iteration 5200: Loss = -9783.754473342537
Iteration 5300: Loss = -9783.754052828872
Iteration 5400: Loss = -9783.753595520908
Iteration 5500: Loss = -9783.753045031
Iteration 5600: Loss = -9783.75254173766
Iteration 5700: Loss = -9783.752184062268
Iteration 5800: Loss = -9783.751833750197
Iteration 5900: Loss = -9783.751467429298
Iteration 6000: Loss = -9783.751163870364
Iteration 6100: Loss = -9783.750855197635
Iteration 6200: Loss = -9783.750567146988
Iteration 6300: Loss = -9783.750408982809
Iteration 6400: Loss = -9783.75024871478
Iteration 6500: Loss = -9783.750072656716
Iteration 6600: Loss = -9783.749970260029
Iteration 6700: Loss = -9783.74982736992
Iteration 6800: Loss = -9783.749673492128
Iteration 6900: Loss = -9783.749589653376
Iteration 7000: Loss = -9783.749436042533
Iteration 7100: Loss = -9783.749354342483
Iteration 7200: Loss = -9783.749242539845
Iteration 7300: Loss = -9783.749147966784
Iteration 7400: Loss = -9783.749045233897
Iteration 7500: Loss = -9783.748962885658
Iteration 7600: Loss = -9783.748851237968
Iteration 7700: Loss = -9783.748757560805
Iteration 7800: Loss = -9783.748641627037
Iteration 7900: Loss = -9783.75053901004
1
Iteration 8000: Loss = -9783.748334443322
Iteration 8100: Loss = -9783.74847302194
1
Iteration 8200: Loss = -9783.748186583138
Iteration 8300: Loss = -9783.761037716677
1
Iteration 8400: Loss = -9783.748094446182
Iteration 8500: Loss = -9783.748034112954
Iteration 8600: Loss = -9783.74810968012
Iteration 8700: Loss = -9783.74796418516
Iteration 8800: Loss = -9783.748008756555
Iteration 8900: Loss = -9783.747942530352
Iteration 9000: Loss = -9783.747883299138
Iteration 9100: Loss = -9783.764789720419
1
Iteration 9200: Loss = -9783.74785673113
Iteration 9300: Loss = -9783.74778307159
Iteration 9400: Loss = -9784.116916734736
1
Iteration 9500: Loss = -9783.747755562217
Iteration 9600: Loss = -9783.747728414184
Iteration 9700: Loss = -9783.748172870875
1
Iteration 9800: Loss = -9783.747708946625
Iteration 9900: Loss = -9783.74767208348
Iteration 10000: Loss = -9783.747665000628
Iteration 10100: Loss = -9783.780901746344
1
Iteration 10200: Loss = -9783.747640694235
Iteration 10300: Loss = -9783.747660237566
Iteration 10400: Loss = -9783.752502992436
1
Iteration 10500: Loss = -9783.747625703747
Iteration 10600: Loss = -9783.747621034263
Iteration 10700: Loss = -9783.747627147904
Iteration 10800: Loss = -9783.748386866451
1
Iteration 10900: Loss = -9783.747590509991
Iteration 11000: Loss = -9783.747584256911
Iteration 11100: Loss = -9783.758584100962
1
Iteration 11200: Loss = -9783.747559573858
Iteration 11300: Loss = -9783.74758465303
Iteration 11400: Loss = -9783.747561746432
Iteration 11500: Loss = -9783.747686564675
1
Iteration 11600: Loss = -9783.747569649526
Iteration 11700: Loss = -9783.74754904442
Iteration 11800: Loss = -9783.77533570921
1
Iteration 11900: Loss = -9783.747553900197
Iteration 12000: Loss = -9783.747561877619
Iteration 12100: Loss = -9783.763255168744
1
Iteration 12200: Loss = -9783.747573314136
Iteration 12300: Loss = -9783.747556881131
Iteration 12400: Loss = -9783.788339415994
1
Iteration 12500: Loss = -9783.747522891343
Iteration 12600: Loss = -9783.747543067082
Iteration 12700: Loss = -9783.763098199333
1
Iteration 12800: Loss = -9783.74757205452
Iteration 12900: Loss = -9783.750048738342
1
Iteration 13000: Loss = -9783.747516153697
Iteration 13100: Loss = -9783.783527482803
1
Iteration 13200: Loss = -9783.749891438367
2
Iteration 13300: Loss = -9783.747532846439
Iteration 13400: Loss = -9783.747660938305
1
Iteration 13500: Loss = -9783.747521085266
Iteration 13600: Loss = -9783.747794351571
1
Iteration 13700: Loss = -9783.751960132935
2
Iteration 13800: Loss = -9783.7475288451
Iteration 13900: Loss = -9783.747698640896
1
Iteration 14000: Loss = -9783.748119215443
2
Iteration 14100: Loss = -9783.747552538425
Iteration 14200: Loss = -9783.74751229865
Iteration 14300: Loss = -9783.747648708591
1
Iteration 14400: Loss = -9783.747551249928
Iteration 14500: Loss = -9783.752399603249
1
Iteration 14600: Loss = -9783.747608994057
Iteration 14700: Loss = -9783.747511931499
Iteration 14800: Loss = -9783.976689067431
1
Iteration 14900: Loss = -9783.747511761574
Iteration 15000: Loss = -9783.752976514927
1
Iteration 15100: Loss = -9783.747554384188
Iteration 15200: Loss = -9783.74753297696
Iteration 15300: Loss = -9783.747825010849
1
Iteration 15400: Loss = -9783.747512257482
Iteration 15500: Loss = -9783.754537947905
1
Iteration 15600: Loss = -9783.747477537294
Iteration 15700: Loss = -9783.747498610499
Iteration 15800: Loss = -9783.74760228403
1
Iteration 15900: Loss = -9783.74750072364
Iteration 16000: Loss = -9783.74750726845
Iteration 16100: Loss = -9783.747628659437
1
Iteration 16200: Loss = -9783.74752940121
Iteration 16300: Loss = -9783.781552121562
1
Iteration 16400: Loss = -9783.747513719703
Iteration 16500: Loss = -9783.747964157354
1
Iteration 16600: Loss = -9783.74753649315
Iteration 16700: Loss = -9783.748309888699
1
Iteration 16800: Loss = -9783.749092458562
2
Iteration 16900: Loss = -9783.747733770138
3
Iteration 17000: Loss = -9783.749018911682
4
Iteration 17100: Loss = -9783.74798881504
5
Iteration 17200: Loss = -9783.747549649695
Iteration 17300: Loss = -9783.747527682359
Iteration 17400: Loss = -9783.747879055903
1
Iteration 17500: Loss = -9783.747521660824
Iteration 17600: Loss = -9783.747743573771
1
Iteration 17700: Loss = -9783.7476420415
2
Iteration 17800: Loss = -9783.747507294976
Iteration 17900: Loss = -9783.747573194716
Iteration 18000: Loss = -9783.747598937525
Iteration 18100: Loss = -9783.748009066603
1
Iteration 18200: Loss = -9783.747601309708
Iteration 18300: Loss = -9783.747546631643
Iteration 18400: Loss = -9783.752476839234
1
Iteration 18500: Loss = -9783.747505786345
Iteration 18600: Loss = -9783.747508134726
Iteration 18700: Loss = -9783.747608448899
1
Iteration 18800: Loss = -9783.747703094474
2
Iteration 18900: Loss = -9783.789116760881
3
Iteration 19000: Loss = -9783.747624112242
4
Iteration 19100: Loss = -9783.74783299996
5
Iteration 19200: Loss = -9783.747489622441
Iteration 19300: Loss = -9783.87687352518
1
Iteration 19400: Loss = -9783.747597806567
2
Iteration 19500: Loss = -9783.752284232747
3
Iteration 19600: Loss = -9783.747521496321
Iteration 19700: Loss = -9783.747963085565
1
Iteration 19800: Loss = -9783.747630500115
2
Iteration 19900: Loss = -9783.805325015555
3
pi: tensor([[0.9920, 0.0080],
        [0.3213, 0.6787]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9768, 0.0232], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1321, 0.1780],
         [0.5674, 0.1208]],

        [[0.5281, 0.2728],
         [0.5799, 0.7176]],

        [[0.6477, 0.1333],
         [0.6406, 0.5763]],

        [[0.6617, 0.2454],
         [0.6637, 0.6674]],

        [[0.6797, 0.1530],
         [0.7226, 0.6774]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0038434178376523637
Average Adjusted Rand Index: 0.0027776557968514004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23065.211372185844
Iteration 100: Loss = -9792.638923292265
Iteration 200: Loss = -9789.470771273634
Iteration 300: Loss = -9788.28195462689
Iteration 400: Loss = -9787.532271434413
Iteration 500: Loss = -9786.28526055062
Iteration 600: Loss = -9785.525843767322
Iteration 700: Loss = -9785.151157429964
Iteration 800: Loss = -9784.892675570667
Iteration 900: Loss = -9784.573502346646
Iteration 1000: Loss = -9784.336234541923
Iteration 1100: Loss = -9784.22760006373
Iteration 1200: Loss = -9784.146305751221
Iteration 1300: Loss = -9784.082089211252
Iteration 1400: Loss = -9784.029828199391
Iteration 1500: Loss = -9783.987161481675
Iteration 1600: Loss = -9783.95205761081
Iteration 1700: Loss = -9783.922641075518
Iteration 1800: Loss = -9783.897934174995
Iteration 1900: Loss = -9783.877640013214
Iteration 2000: Loss = -9783.860942784622
Iteration 2100: Loss = -9783.846938649303
Iteration 2200: Loss = -9783.834964401321
Iteration 2300: Loss = -9783.824707634965
Iteration 2400: Loss = -9783.815801331979
Iteration 2500: Loss = -9783.80811928155
Iteration 2600: Loss = -9783.8014398348
Iteration 2700: Loss = -9783.795545741092
Iteration 2800: Loss = -9783.790401320115
Iteration 2900: Loss = -9783.785915484785
Iteration 3000: Loss = -9783.781924865643
Iteration 3100: Loss = -9783.778373545534
Iteration 3200: Loss = -9783.775270455764
Iteration 3300: Loss = -9783.772478143912
Iteration 3400: Loss = -9783.76999961616
Iteration 3500: Loss = -9783.767814922056
Iteration 3600: Loss = -9783.765834619251
Iteration 3700: Loss = -9783.764113945606
Iteration 3800: Loss = -9783.762515952274
Iteration 3900: Loss = -9783.761128680588
Iteration 4000: Loss = -9783.759876322922
Iteration 4100: Loss = -9783.758797723702
Iteration 4200: Loss = -9783.757783925028
Iteration 4300: Loss = -9783.75687767202
Iteration 4400: Loss = -9783.756091208643
Iteration 4500: Loss = -9783.755373061917
Iteration 4600: Loss = -9783.754731049898
Iteration 4700: Loss = -9783.754131461732
Iteration 4800: Loss = -9783.753600710048
Iteration 4900: Loss = -9783.753171475453
Iteration 5000: Loss = -9783.752697979236
Iteration 5100: Loss = -9783.752335743558
Iteration 5200: Loss = -9783.751964318559
Iteration 5300: Loss = -9783.751633462978
Iteration 5400: Loss = -9783.751364996582
Iteration 5500: Loss = -9783.751054927252
Iteration 5600: Loss = -9783.750809573517
Iteration 5700: Loss = -9783.750598200018
Iteration 5800: Loss = -9783.750338073925
Iteration 5900: Loss = -9783.750137564219
Iteration 6000: Loss = -9783.749948794686
Iteration 6100: Loss = -9783.74975400992
Iteration 6200: Loss = -9783.74960099354
Iteration 6300: Loss = -9783.749437516948
Iteration 6400: Loss = -9783.74930720979
Iteration 6500: Loss = -9783.749141785898
Iteration 6600: Loss = -9783.749003711993
Iteration 6700: Loss = -9783.74891649531
Iteration 6800: Loss = -9783.748796075224
Iteration 6900: Loss = -9783.748718669503
Iteration 7000: Loss = -9783.748637382008
Iteration 7100: Loss = -9783.748587994723
Iteration 7200: Loss = -9783.748454376544
Iteration 7300: Loss = -9783.748393492246
Iteration 7400: Loss = -9783.748345675585
Iteration 7500: Loss = -9783.748299831721
Iteration 7600: Loss = -9783.748190196568
Iteration 7700: Loss = -9783.748197424486
Iteration 7800: Loss = -9783.74812560047
Iteration 7900: Loss = -9783.74809721145
Iteration 8000: Loss = -9783.748056645749
Iteration 8100: Loss = -9783.747991189903
Iteration 8200: Loss = -9783.748282732078
1
Iteration 8300: Loss = -9783.74929372085
2
Iteration 8400: Loss = -9783.748434859184
3
Iteration 8500: Loss = -9783.74787063474
Iteration 8600: Loss = -9783.750080460688
1
Iteration 8700: Loss = -9783.74784483224
Iteration 8800: Loss = -9783.747929219082
Iteration 8900: Loss = -9783.747802563857
Iteration 9000: Loss = -9783.74779213293
Iteration 9100: Loss = -9783.747755060136
Iteration 9200: Loss = -9783.757147308253
1
Iteration 9300: Loss = -9783.747725585476
Iteration 9400: Loss = -9783.74771476322
Iteration 9500: Loss = -9783.850763558255
1
Iteration 9600: Loss = -9783.747681828085
Iteration 9700: Loss = -9783.747644868261
Iteration 9800: Loss = -9783.994779461627
1
Iteration 9900: Loss = -9783.747641458585
Iteration 10000: Loss = -9783.74764521423
Iteration 10100: Loss = -9783.747637466035
Iteration 10200: Loss = -9783.747676336741
Iteration 10300: Loss = -9783.747612759724
Iteration 10400: Loss = -9783.747601278284
Iteration 10500: Loss = -9783.84015404736
1
Iteration 10600: Loss = -9783.747638935913
Iteration 10700: Loss = -9783.747577212067
Iteration 10800: Loss = -9783.74758303284
Iteration 10900: Loss = -9783.766248531052
1
Iteration 11000: Loss = -9783.747589285604
Iteration 11100: Loss = -9783.74759996618
Iteration 11200: Loss = -9783.747579027584
Iteration 11300: Loss = -9783.750837885787
1
Iteration 11400: Loss = -9783.747535568811
Iteration 11500: Loss = -9783.74752979318
Iteration 11600: Loss = -9783.747654086137
1
Iteration 11700: Loss = -9783.74752282654
Iteration 11800: Loss = -9783.747553002011
Iteration 11900: Loss = -9783.747546259776
Iteration 12000: Loss = -9783.907219198027
1
Iteration 12100: Loss = -9783.747547563216
Iteration 12200: Loss = -9783.747531577901
Iteration 12300: Loss = -9783.747529055217
Iteration 12400: Loss = -9783.747998155643
1
Iteration 12500: Loss = -9783.747509414618
Iteration 12600: Loss = -9783.747537539746
Iteration 12700: Loss = -9783.755817603147
1
Iteration 12800: Loss = -9783.747540572576
Iteration 12900: Loss = -9783.747522572217
Iteration 13000: Loss = -9783.747519141714
Iteration 13100: Loss = -9783.747547315865
Iteration 13200: Loss = -9783.7475102312
Iteration 13300: Loss = -9783.747539217196
Iteration 13400: Loss = -9783.748351697779
1
Iteration 13500: Loss = -9783.747511942558
Iteration 13600: Loss = -9783.754386959625
1
Iteration 13700: Loss = -9783.747517645761
Iteration 13800: Loss = -9783.764869829436
1
Iteration 13900: Loss = -9783.7475061084
Iteration 14000: Loss = -9783.747522805977
Iteration 14100: Loss = -9783.748644910267
1
Iteration 14200: Loss = -9783.747516131132
Iteration 14300: Loss = -9783.75204894733
1
Iteration 14400: Loss = -9783.747531570967
Iteration 14500: Loss = -9783.74754689713
Iteration 14600: Loss = -9783.747604232558
Iteration 14700: Loss = -9783.747548555693
Iteration 14800: Loss = -9783.90544903272
1
Iteration 14900: Loss = -9783.74749526385
Iteration 15000: Loss = -9783.74752608437
Iteration 15100: Loss = -9783.753739479323
1
Iteration 15200: Loss = -9783.747493532077
Iteration 15300: Loss = -9783.747515291037
Iteration 15400: Loss = -9783.749730224194
1
Iteration 15500: Loss = -9783.747535337136
Iteration 15600: Loss = -9783.747537726498
Iteration 15700: Loss = -9783.74753819512
Iteration 15800: Loss = -9783.749822564196
1
Iteration 15900: Loss = -9783.74753972183
Iteration 16000: Loss = -9783.747887887577
1
Iteration 16100: Loss = -9783.74754277769
Iteration 16200: Loss = -9783.903615641802
1
Iteration 16300: Loss = -9783.747523202084
Iteration 16400: Loss = -9783.747630302494
1
Iteration 16500: Loss = -9783.747707203771
2
Iteration 16600: Loss = -9783.74754987585
Iteration 16700: Loss = -9783.747599985312
Iteration 16800: Loss = -9783.74782411294
1
Iteration 16900: Loss = -9783.748667198313
2
Iteration 17000: Loss = -9783.747554278845
Iteration 17100: Loss = -9783.752301370878
1
Iteration 17200: Loss = -9783.747710811569
2
Iteration 17300: Loss = -9783.747535890107
Iteration 17400: Loss = -9783.752696347094
1
Iteration 17500: Loss = -9783.748447895146
2
Iteration 17600: Loss = -9783.747671358331
3
Iteration 17700: Loss = -9783.747525393565
Iteration 17800: Loss = -9783.74903197922
1
Iteration 17900: Loss = -9783.806939049193
2
Iteration 18000: Loss = -9783.74752641249
Iteration 18100: Loss = -9783.801941469805
1
Iteration 18200: Loss = -9783.747953244021
2
Iteration 18300: Loss = -9783.748037045756
3
Iteration 18400: Loss = -9783.747552253384
Iteration 18500: Loss = -9783.747501030697
Iteration 18600: Loss = -9783.747746754123
1
Iteration 18700: Loss = -9783.748252953179
2
Iteration 18800: Loss = -9783.747524508919
Iteration 18900: Loss = -9783.747624177753
Iteration 19000: Loss = -9783.758087662369
1
Iteration 19100: Loss = -9783.748029262078
2
Iteration 19200: Loss = -9783.750058465455
3
Iteration 19300: Loss = -9783.747512511103
Iteration 19400: Loss = -9783.759058111376
1
Iteration 19500: Loss = -9783.749774973881
2
Iteration 19600: Loss = -9783.748960257779
3
Iteration 19700: Loss = -9783.747499989737
Iteration 19800: Loss = -9783.74803032815
1
Iteration 19900: Loss = -9783.747609465212
2
pi: tensor([[0.6768, 0.3232],
        [0.0080, 0.9920]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0230, 0.9770], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1210, 0.1782],
         [0.5342, 0.1326]],

        [[0.6861, 0.2739],
         [0.6476, 0.6061]],

        [[0.6950, 0.1328],
         [0.6497, 0.6630]],

        [[0.7153, 0.2462],
         [0.7138, 0.6748]],

        [[0.5739, 0.1521],
         [0.7007, 0.7091]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0038434178376523637
Average Adjusted Rand Index: 0.0027776557968514004
9944.979100986695
[0.0038434178376523637, 0.0038434178376523637] [0.0027776557968514004, 0.0027776557968514004] [9783.747518250071, 9783.777912280964]
-------------------------------------
This iteration is 93
True Objective function: Loss = -10200.30510867104
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25241.926290904004
Iteration 100: Loss = -10073.865119356557
Iteration 200: Loss = -10072.455998663876
Iteration 300: Loss = -10072.137785695428
Iteration 400: Loss = -10072.022374752953
Iteration 500: Loss = -10071.961084608341
Iteration 600: Loss = -10071.918096146093
Iteration 700: Loss = -10071.880203934537
Iteration 800: Loss = -10071.840161519489
Iteration 900: Loss = -10071.791786769956
Iteration 1000: Loss = -10071.728024863394
Iteration 1100: Loss = -10071.641831046247
Iteration 1200: Loss = -10071.53468479377
Iteration 1300: Loss = -10071.419060597025
Iteration 1400: Loss = -10071.315079945598
Iteration 1500: Loss = -10071.216629153692
Iteration 1600: Loss = -10071.138195739419
Iteration 1700: Loss = -10071.09159962525
Iteration 1800: Loss = -10071.057166609586
Iteration 1900: Loss = -10071.01143387425
Iteration 2000: Loss = -10070.928436631797
Iteration 2100: Loss = -10070.800568326771
Iteration 2200: Loss = -10070.683758558487
Iteration 2300: Loss = -10070.61248526543
Iteration 2400: Loss = -10070.574039095221
Iteration 2500: Loss = -10070.55279686131
Iteration 2600: Loss = -10070.540202273405
Iteration 2700: Loss = -10070.532128087485
Iteration 2800: Loss = -10070.52656022051
Iteration 2900: Loss = -10070.522519661024
Iteration 3000: Loss = -10070.519467711403
Iteration 3100: Loss = -10070.517027731124
Iteration 3200: Loss = -10070.515007572962
Iteration 3300: Loss = -10070.513379362896
Iteration 3400: Loss = -10070.511976615988
Iteration 3500: Loss = -10070.510798986756
Iteration 3600: Loss = -10070.509765616474
Iteration 3700: Loss = -10070.508843921936
Iteration 3800: Loss = -10070.508056406808
Iteration 3900: Loss = -10070.507339104604
Iteration 4000: Loss = -10070.50673253792
Iteration 4100: Loss = -10070.50619972535
Iteration 4200: Loss = -10070.505694446738
Iteration 4300: Loss = -10070.505211734686
Iteration 4400: Loss = -10070.504766832977
Iteration 4500: Loss = -10070.504408715213
Iteration 4600: Loss = -10070.504079599617
Iteration 4700: Loss = -10070.503738398174
Iteration 4800: Loss = -10070.503436719457
Iteration 4900: Loss = -10070.50673885654
1
Iteration 5000: Loss = -10070.50291934709
Iteration 5100: Loss = -10070.502707173337
Iteration 5200: Loss = -10070.502525870374
Iteration 5300: Loss = -10070.502259051504
Iteration 5400: Loss = -10070.502121468857
Iteration 5500: Loss = -10070.50200804045
Iteration 5600: Loss = -10070.501734324069
Iteration 5700: Loss = -10070.501622586642
Iteration 5800: Loss = -10070.501457789362
Iteration 5900: Loss = -10070.501383741703
Iteration 6000: Loss = -10070.503769229184
1
Iteration 6100: Loss = -10070.501094571506
Iteration 6200: Loss = -10070.501131207975
Iteration 6300: Loss = -10070.501044606417
Iteration 6400: Loss = -10070.501020963127
Iteration 6500: Loss = -10070.50074241057
Iteration 6600: Loss = -10070.500666638576
Iteration 6700: Loss = -10070.50187464281
1
Iteration 6800: Loss = -10070.50073347011
Iteration 6900: Loss = -10070.501192494039
1
Iteration 7000: Loss = -10070.500386806198
Iteration 7100: Loss = -10070.500785484634
1
Iteration 7200: Loss = -10070.500250912537
Iteration 7300: Loss = -10070.505355206047
1
Iteration 7400: Loss = -10070.500141912102
Iteration 7500: Loss = -10070.500097989545
Iteration 7600: Loss = -10070.502710322144
1
Iteration 7700: Loss = -10070.499979787694
Iteration 7800: Loss = -10070.499933036866
Iteration 7900: Loss = -10070.500029248984
Iteration 8000: Loss = -10070.499920551258
Iteration 8100: Loss = -10070.500078265759
1
Iteration 8200: Loss = -10070.499785702848
Iteration 8300: Loss = -10070.500284551365
1
Iteration 8400: Loss = -10070.499726885593
Iteration 8500: Loss = -10070.49976341056
Iteration 8600: Loss = -10070.499701594325
Iteration 8700: Loss = -10070.499663526407
Iteration 8800: Loss = -10070.499598678713
Iteration 8900: Loss = -10070.499669437597
Iteration 9000: Loss = -10070.521725268347
1
Iteration 9100: Loss = -10070.499556901712
Iteration 9200: Loss = -10070.500263791013
1
Iteration 9300: Loss = -10070.49955314648
Iteration 9400: Loss = -10070.51150226838
1
Iteration 9500: Loss = -10070.499470526645
Iteration 9600: Loss = -10070.499461282707
Iteration 9700: Loss = -10070.499452140502
Iteration 9800: Loss = -10070.49942979893
Iteration 9900: Loss = -10070.559141756352
1
Iteration 10000: Loss = -10070.499404140892
Iteration 10100: Loss = -10070.499387904358
Iteration 10200: Loss = -10070.518495978176
1
Iteration 10300: Loss = -10070.499372388329
Iteration 10400: Loss = -10070.49935498788
Iteration 10500: Loss = -10070.504233201056
1
Iteration 10600: Loss = -10070.499353219666
Iteration 10700: Loss = -10070.499349825826
Iteration 10800: Loss = -10070.499505830508
1
Iteration 10900: Loss = -10070.499320944373
Iteration 11000: Loss = -10070.499318501303
Iteration 11100: Loss = -10070.499906777059
1
Iteration 11200: Loss = -10070.499317547672
Iteration 11300: Loss = -10070.499306591462
Iteration 11400: Loss = -10070.50232657767
1
Iteration 11500: Loss = -10070.499258032783
Iteration 11600: Loss = -10070.499768655038
1
Iteration 11700: Loss = -10070.499470137103
2
Iteration 11800: Loss = -10070.503400176305
3
Iteration 11900: Loss = -10070.49926996409
Iteration 12000: Loss = -10070.504086427392
1
Iteration 12100: Loss = -10070.499244019267
Iteration 12200: Loss = -10070.6022130342
1
Iteration 12300: Loss = -10070.499235898333
Iteration 12400: Loss = -10070.499220193888
Iteration 12500: Loss = -10070.500039009159
1
Iteration 12600: Loss = -10070.49922972263
Iteration 12700: Loss = -10070.699967335453
1
Iteration 12800: Loss = -10070.499230993995
Iteration 12900: Loss = -10070.502171239934
1
Iteration 13000: Loss = -10070.499205352042
Iteration 13100: Loss = -10070.499223662453
Iteration 13200: Loss = -10070.499355216256
1
Iteration 13300: Loss = -10070.499209845515
Iteration 13400: Loss = -10070.512030627959
1
Iteration 13500: Loss = -10070.499290476871
Iteration 13600: Loss = -10070.499208184134
Iteration 13700: Loss = -10070.501216352615
1
Iteration 13800: Loss = -10070.499324611204
2
Iteration 13900: Loss = -10070.603240322504
3
Iteration 14000: Loss = -10070.49925162238
Iteration 14100: Loss = -10070.499254604223
Iteration 14200: Loss = -10070.569861003945
1
Iteration 14300: Loss = -10070.502996459496
2
Iteration 14400: Loss = -10070.499226486332
Iteration 14500: Loss = -10070.501558945038
1
Iteration 14600: Loss = -10070.499856537834
2
Iteration 14700: Loss = -10070.499225988546
Iteration 14800: Loss = -10070.499750033285
1
Iteration 14900: Loss = -10070.499316006582
Iteration 15000: Loss = -10070.502374811298
1
Iteration 15100: Loss = -10070.49916987143
Iteration 15200: Loss = -10070.50206582384
1
Iteration 15300: Loss = -10070.499959755965
2
Iteration 15400: Loss = -10070.499167742937
Iteration 15500: Loss = -10070.499455926249
1
Iteration 15600: Loss = -10070.500513550202
2
Iteration 15700: Loss = -10070.499361377293
3
Iteration 15800: Loss = -10070.587712668548
4
Iteration 15900: Loss = -10070.49918573993
Iteration 16000: Loss = -10070.502331232314
1
Iteration 16100: Loss = -10070.499204898964
Iteration 16200: Loss = -10070.760086728784
1
Iteration 16300: Loss = -10070.499124640166
Iteration 16400: Loss = -10070.597854119591
1
Iteration 16500: Loss = -10070.50328283538
2
Iteration 16600: Loss = -10070.499965909163
3
Iteration 16700: Loss = -10070.500257860838
4
Iteration 16800: Loss = -10070.507375547832
5
Iteration 16900: Loss = -10070.502403523733
6
Iteration 17000: Loss = -10070.540102582594
7
Iteration 17100: Loss = -10070.499158686567
Iteration 17200: Loss = -10070.499736815458
1
Iteration 17300: Loss = -10070.49921895094
Iteration 17400: Loss = -10070.499194471242
Iteration 17500: Loss = -10070.499248676519
Iteration 17600: Loss = -10070.499308034825
Iteration 17700: Loss = -10070.510181887708
1
Iteration 17800: Loss = -10070.499204174423
Iteration 17900: Loss = -10070.499379356934
1
Iteration 18000: Loss = -10070.513817049019
2
Iteration 18100: Loss = -10070.499179820397
Iteration 18200: Loss = -10070.501039152849
1
Iteration 18300: Loss = -10070.502041187072
2
Iteration 18400: Loss = -10070.499205752552
Iteration 18500: Loss = -10070.500757220223
1
Iteration 18600: Loss = -10070.500407129888
2
Iteration 18700: Loss = -10070.499216683062
Iteration 18800: Loss = -10070.544953586721
1
Iteration 18900: Loss = -10070.499189174703
Iteration 19000: Loss = -10070.499180596438
Iteration 19100: Loss = -10070.49922212622
Iteration 19200: Loss = -10070.542173361282
1
Iteration 19300: Loss = -10070.527633877318
2
Iteration 19400: Loss = -10070.501071959508
3
Iteration 19500: Loss = -10070.499274114043
Iteration 19600: Loss = -10070.499736168544
1
Iteration 19700: Loss = -10070.577623964298
2
Iteration 19800: Loss = -10070.499181961071
Iteration 19900: Loss = -10070.512874668579
1
pi: tensor([[9.8150e-01, 1.8504e-02],
        [1.0000e+00, 4.7260e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7635, 0.2365], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1417, 0.1070],
         [0.6766, 0.3317]],

        [[0.5807, 0.0511],
         [0.6903, 0.5349]],

        [[0.6354, 0.1708],
         [0.6608, 0.5081]],

        [[0.6820, 0.2090],
         [0.5821, 0.6402]],

        [[0.6925, 0.1006],
         [0.5990, 0.7275]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 68
Adjusted Rand Index: 0.12352309344790548
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0016236859719290819
Average Adjusted Rand Index: 0.02397734596230837
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23919.571533087135
Iteration 100: Loss = -10074.275469897751
Iteration 200: Loss = -10071.256212157112
Iteration 300: Loss = -10070.08822562096
Iteration 400: Loss = -10069.320777870276
Iteration 500: Loss = -10068.544419547254
Iteration 600: Loss = -10068.257562888086
Iteration 700: Loss = -10068.077331310946
Iteration 800: Loss = -10067.951940537843
Iteration 900: Loss = -10067.851120645437
Iteration 1000: Loss = -10067.756469156784
Iteration 1100: Loss = -10067.655994527522
Iteration 1200: Loss = -10067.537755830137
Iteration 1300: Loss = -10067.390079862134
Iteration 1400: Loss = -10067.204632308576
Iteration 1500: Loss = -10066.986521906922
Iteration 1600: Loss = -10066.762165922339
Iteration 1700: Loss = -10066.565510162847
Iteration 1800: Loss = -10066.404956184731
Iteration 1900: Loss = -10066.26316955843
Iteration 2000: Loss = -10066.131581238966
Iteration 2100: Loss = -10066.008578914196
Iteration 2200: Loss = -10065.894606514488
Iteration 2300: Loss = -10065.793015121673
Iteration 2400: Loss = -10065.702697655119
Iteration 2500: Loss = -10065.619529832806
Iteration 2600: Loss = -10065.543138609108
Iteration 2700: Loss = -10065.472466521871
Iteration 2800: Loss = -10065.40672914044
Iteration 2900: Loss = -10065.34296124031
Iteration 3000: Loss = -10065.27778401065
Iteration 3100: Loss = -10065.210564544179
Iteration 3200: Loss = -10065.132184993608
Iteration 3300: Loss = -10065.037336077403
Iteration 3400: Loss = -10064.922749177718
Iteration 3500: Loss = -10064.815496555493
Iteration 3600: Loss = -10064.758980695102
Iteration 3700: Loss = -10064.743261563277
Iteration 3800: Loss = -10064.739193027073
Iteration 3900: Loss = -10064.73698766593
Iteration 4000: Loss = -10064.741944074669
1
Iteration 4100: Loss = -10064.73468857079
Iteration 4200: Loss = -10064.73214189436
Iteration 4300: Loss = -10064.732822362908
1
Iteration 4400: Loss = -10064.729530148234
Iteration 4500: Loss = -10064.729786528116
1
Iteration 4600: Loss = -10064.727343190296
Iteration 4700: Loss = -10064.726383001742
Iteration 4800: Loss = -10064.72977422238
1
Iteration 4900: Loss = -10064.724635417993
Iteration 5000: Loss = -10064.723874696381
Iteration 5100: Loss = -10064.7235099892
Iteration 5200: Loss = -10064.723007372693
Iteration 5300: Loss = -10064.761724388325
1
Iteration 5400: Loss = -10064.721271214577
Iteration 5500: Loss = -10064.720801062871
Iteration 5600: Loss = -10064.720265693752
Iteration 5700: Loss = -10064.719777269784
Iteration 5800: Loss = -10064.719319920458
Iteration 5900: Loss = -10064.719176623013
Iteration 6000: Loss = -10064.718531095568
Iteration 6100: Loss = -10064.726109345944
1
Iteration 6200: Loss = -10064.717812643554
Iteration 6300: Loss = -10064.718642952701
1
Iteration 6400: Loss = -10064.71717961149
Iteration 6500: Loss = -10064.72244804738
1
Iteration 6600: Loss = -10064.71664471127
Iteration 6700: Loss = -10064.717506200988
1
Iteration 6800: Loss = -10064.716170708554
Iteration 6900: Loss = -10064.716319960633
1
Iteration 7000: Loss = -10064.715693559734
Iteration 7100: Loss = -10064.732158494156
1
Iteration 7200: Loss = -10064.715317074653
Iteration 7300: Loss = -10064.715178509034
Iteration 7400: Loss = -10064.71530261154
1
Iteration 7500: Loss = -10064.714831699554
Iteration 7600: Loss = -10064.714731849706
Iteration 7700: Loss = -10064.71474729467
Iteration 7800: Loss = -10064.714418894844
Iteration 7900: Loss = -10064.714285952143
Iteration 8000: Loss = -10064.717293208621
1
Iteration 8100: Loss = -10064.71408445767
Iteration 8200: Loss = -10064.716086482676
1
Iteration 8300: Loss = -10064.714453591962
2
Iteration 8400: Loss = -10064.714913701442
3
Iteration 8500: Loss = -10064.725051168674
4
Iteration 8600: Loss = -10064.71357170168
Iteration 8700: Loss = -10064.713845307286
1
Iteration 8800: Loss = -10064.716798286601
2
Iteration 8900: Loss = -10064.713295957163
Iteration 9000: Loss = -10064.715764709756
1
Iteration 9100: Loss = -10064.713268675012
Iteration 9200: Loss = -10064.766672155556
1
Iteration 9300: Loss = -10064.852565731035
2
Iteration 9400: Loss = -10064.71763479077
3
Iteration 9500: Loss = -10064.713049443622
Iteration 9600: Loss = -10064.712953602138
Iteration 9700: Loss = -10064.76078642712
1
Iteration 9800: Loss = -10064.71300949861
Iteration 9900: Loss = -10064.713674050618
1
Iteration 10000: Loss = -10064.71974265657
2
Iteration 10100: Loss = -10064.713867363258
3
Iteration 10200: Loss = -10064.712902709049
Iteration 10300: Loss = -10064.72601803464
1
Iteration 10400: Loss = -10064.729452753905
2
Iteration 10500: Loss = -10064.722666819944
3
Iteration 10600: Loss = -10064.736073185204
4
Iteration 10700: Loss = -10064.712643288249
Iteration 10800: Loss = -10064.712867300852
1
Iteration 10900: Loss = -10064.71652008054
2
Iteration 11000: Loss = -10064.733099005965
3
Iteration 11100: Loss = -10064.713435922551
4
Iteration 11200: Loss = -10064.712539028082
Iteration 11300: Loss = -10064.815660615779
1
Iteration 11400: Loss = -10064.72171711737
2
Iteration 11500: Loss = -10064.740005841511
3
Iteration 11600: Loss = -10064.76086714579
4
Iteration 11700: Loss = -10064.717107557512
5
Iteration 11800: Loss = -10064.722971034242
6
Iteration 11900: Loss = -10064.717634565417
7
Iteration 12000: Loss = -10064.713926859675
8
Iteration 12100: Loss = -10064.726350777491
9
Iteration 12200: Loss = -10064.713891378098
10
Iteration 12300: Loss = -10064.712235510628
Iteration 12400: Loss = -10064.71609357513
1
Iteration 12500: Loss = -10064.717296517987
2
Iteration 12600: Loss = -10064.726024215359
3
Iteration 12700: Loss = -10064.741791511606
4
Iteration 12800: Loss = -10064.730140220836
5
Iteration 12900: Loss = -10064.729882112151
6
Iteration 13000: Loss = -10064.718875990551
7
Iteration 13100: Loss = -10064.718414173327
8
Iteration 13200: Loss = -10064.715830516321
9
Iteration 13300: Loss = -10064.716282718184
10
Iteration 13400: Loss = -10064.71415102214
11
Iteration 13500: Loss = -10064.81368061314
12
Iteration 13600: Loss = -10064.720636870008
13
Iteration 13700: Loss = -10064.713556329956
14
Iteration 13800: Loss = -10064.758043002073
15
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[5.2418e-01, 4.7582e-01],
        [9.9996e-01, 4.0096e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5168, 0.4832], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1248, 0.1457],
         [0.5115, 0.1780]],

        [[0.6095, 0.1401],
         [0.5365, 0.6308]],

        [[0.5922, 0.1480],
         [0.5444, 0.6661]],

        [[0.6822, 0.1590],
         [0.5899, 0.5174]],

        [[0.7025, 0.1414],
         [0.5757, 0.6075]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 67
Adjusted Rand Index: 0.1067541593722072
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.002789625056224717
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.012706535921033284
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.006132953674296353
Global Adjusted Rand Index: 0.013000107394003618
Average Adjusted Rand Index: 0.01865464649694506
10200.30510867104
[0.0016236859719290819, 0.013000107394003618] [0.02397734596230837, 0.01865464649694506] [10070.526197160736, 10064.758043002073]
-------------------------------------
This iteration is 94
True Objective function: Loss = -10046.354326748846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18670.196633183667
Iteration 100: Loss = -9915.339743221362
Iteration 200: Loss = -9914.995548139621
Iteration 300: Loss = -9914.76309028504
Iteration 400: Loss = -9913.827222945436
Iteration 500: Loss = -9912.122642549855
Iteration 600: Loss = -9911.638944310353
Iteration 700: Loss = -9910.572157019766
Iteration 800: Loss = -9908.670052651029
Iteration 900: Loss = -9908.188227763001
Iteration 1000: Loss = -9907.945284552294
Iteration 1100: Loss = -9907.802529947454
Iteration 1200: Loss = -9907.7136916546
Iteration 1300: Loss = -9907.652652329843
Iteration 1400: Loss = -9907.608685572328
Iteration 1500: Loss = -9907.575741053835
Iteration 1600: Loss = -9907.550248762447
Iteration 1700: Loss = -9907.530168095496
Iteration 1800: Loss = -9907.513967661818
Iteration 1900: Loss = -9907.500478933383
Iteration 2000: Loss = -9907.488992984489
Iteration 2100: Loss = -9907.478997593334
Iteration 2200: Loss = -9907.470452043875
Iteration 2300: Loss = -9907.463308941327
Iteration 2400: Loss = -9907.457239634901
Iteration 2500: Loss = -9907.451969352212
Iteration 2600: Loss = -9907.44732802171
Iteration 2700: Loss = -9907.443133106446
Iteration 2800: Loss = -9907.439337487553
Iteration 2900: Loss = -9907.435821759991
Iteration 3000: Loss = -9907.432524876243
Iteration 3100: Loss = -9907.429450928194
Iteration 3200: Loss = -9907.426533574988
Iteration 3300: Loss = -9907.424058983006
Iteration 3400: Loss = -9907.422006432413
Iteration 3500: Loss = -9907.420280966871
Iteration 3600: Loss = -9907.418735603409
Iteration 3700: Loss = -9907.417369457453
Iteration 3800: Loss = -9907.416105697523
Iteration 3900: Loss = -9907.414949750248
Iteration 4000: Loss = -9907.413917143216
Iteration 4100: Loss = -9907.412930627175
Iteration 4200: Loss = -9907.412064515265
Iteration 4300: Loss = -9907.41120620167
Iteration 4400: Loss = -9907.41046458826
Iteration 4500: Loss = -9907.409736694884
Iteration 4600: Loss = -9907.409068361761
Iteration 4700: Loss = -9907.408438414881
Iteration 4800: Loss = -9907.407833499647
Iteration 4900: Loss = -9907.407315953877
Iteration 5000: Loss = -9907.406768704494
Iteration 5100: Loss = -9907.406299800123
Iteration 5200: Loss = -9907.405877209107
Iteration 5300: Loss = -9907.405419209446
Iteration 5400: Loss = -9907.40514695608
Iteration 5500: Loss = -9907.404659276046
Iteration 5600: Loss = -9907.404304677697
Iteration 5700: Loss = -9907.406274936875
1
Iteration 5800: Loss = -9907.403678672727
Iteration 5900: Loss = -9907.403392824684
Iteration 6000: Loss = -9907.40319095962
Iteration 6100: Loss = -9907.402902876274
Iteration 6200: Loss = -9907.402720555856
Iteration 6300: Loss = -9907.402469576988
Iteration 6400: Loss = -9907.402263629889
Iteration 6500: Loss = -9907.402034990286
Iteration 6600: Loss = -9907.401865381971
Iteration 6700: Loss = -9907.40168133841
Iteration 6800: Loss = -9907.401493326353
Iteration 6900: Loss = -9907.401354625748
Iteration 7000: Loss = -9907.401120070588
Iteration 7100: Loss = -9907.40139022535
1
Iteration 7200: Loss = -9907.400756541649
Iteration 7300: Loss = -9907.4005211066
Iteration 7400: Loss = -9907.400483710913
Iteration 7500: Loss = -9907.396950913186
Iteration 7600: Loss = -9907.396775225532
Iteration 7700: Loss = -9907.396712686257
Iteration 7800: Loss = -9907.39653498376
Iteration 7900: Loss = -9907.39655759891
Iteration 8000: Loss = -9907.396319606141
Iteration 8100: Loss = -9907.396296950337
Iteration 8200: Loss = -9907.402578553732
1
Iteration 8300: Loss = -9907.396037126335
Iteration 8400: Loss = -9907.397277027681
1
Iteration 8500: Loss = -9907.395837296459
Iteration 8600: Loss = -9907.39578578818
Iteration 8700: Loss = -9907.39559741904
Iteration 8800: Loss = -9907.404214951413
1
Iteration 8900: Loss = -9907.39534906357
Iteration 9000: Loss = -9907.395251273092
Iteration 9100: Loss = -9907.409255113324
1
Iteration 9200: Loss = -9907.39515921823
Iteration 9300: Loss = -9907.395062719821
Iteration 9400: Loss = -9907.399937535984
1
Iteration 9500: Loss = -9907.394942841685
Iteration 9600: Loss = -9907.394867241446
Iteration 9700: Loss = -9907.401357560788
1
Iteration 9800: Loss = -9907.394781904428
Iteration 9900: Loss = -9907.394741605844
Iteration 10000: Loss = -9907.394703515336
Iteration 10100: Loss = -9907.404611419872
1
Iteration 10200: Loss = -9907.394505506936
Iteration 10300: Loss = -9907.394460828842
Iteration 10400: Loss = -9907.398140855406
1
Iteration 10500: Loss = -9907.394423880025
Iteration 10600: Loss = -9907.394398160413
Iteration 10700: Loss = -9907.395543632287
1
Iteration 10800: Loss = -9907.394350443335
Iteration 10900: Loss = -9907.394302100263
Iteration 11000: Loss = -9907.418925736287
1
Iteration 11100: Loss = -9907.394264679537
Iteration 11200: Loss = -9907.394242016455
Iteration 11300: Loss = -9907.397749496375
1
Iteration 11400: Loss = -9907.394204163551
Iteration 11500: Loss = -9907.394222499695
Iteration 11600: Loss = -9907.394458628325
1
Iteration 11700: Loss = -9907.394171610213
Iteration 11800: Loss = -9907.394148637548
Iteration 11900: Loss = -9907.394568256295
1
Iteration 12000: Loss = -9907.394119083754
Iteration 12100: Loss = -9907.394120521625
Iteration 12200: Loss = -9907.423704125664
1
Iteration 12300: Loss = -9907.394087613211
Iteration 12400: Loss = -9907.394109550043
Iteration 12500: Loss = -9907.422411803262
1
Iteration 12600: Loss = -9907.39405776311
Iteration 12700: Loss = -9907.401630997798
1
Iteration 12800: Loss = -9907.394176859067
2
Iteration 12900: Loss = -9907.41224684442
3
Iteration 13000: Loss = -9907.39404053043
Iteration 13100: Loss = -9907.394656458522
1
Iteration 13200: Loss = -9907.394041135634
Iteration 13300: Loss = -9907.394798892497
1
Iteration 13400: Loss = -9907.399746846926
2
Iteration 13500: Loss = -9907.394295109016
3
Iteration 13600: Loss = -9907.394332965216
4
Iteration 13700: Loss = -9907.402997796797
5
Iteration 13800: Loss = -9907.397750284497
6
Iteration 13900: Loss = -9907.393912858282
Iteration 14000: Loss = -9907.406185789077
1
Iteration 14100: Loss = -9907.39391607872
Iteration 14200: Loss = -9907.405577841679
1
Iteration 14300: Loss = -9907.402357315388
2
Iteration 14400: Loss = -9907.393933796606
Iteration 14500: Loss = -9907.394146528952
1
Iteration 14600: Loss = -9907.396638346003
2
Iteration 14700: Loss = -9907.403467901579
3
Iteration 14800: Loss = -9907.394563762835
4
Iteration 14900: Loss = -9907.447921738587
5
Iteration 15000: Loss = -9907.393816876767
Iteration 15100: Loss = -9907.400003351411
1
Iteration 15200: Loss = -9907.394142876603
2
Iteration 15300: Loss = -9907.399509158515
3
Iteration 15400: Loss = -9907.393822711336
Iteration 15500: Loss = -9907.396322848539
1
Iteration 15600: Loss = -9907.395890615857
2
Iteration 15700: Loss = -9907.395591447463
3
Iteration 15800: Loss = -9907.393865587212
Iteration 15900: Loss = -9907.394346353214
1
Iteration 16000: Loss = -9907.39858180558
2
Iteration 16100: Loss = -9907.393864768228
Iteration 16200: Loss = -9907.397915192132
1
Iteration 16300: Loss = -9907.394184768591
2
Iteration 16400: Loss = -9907.393846495825
Iteration 16500: Loss = -9907.402286820417
1
Iteration 16600: Loss = -9907.453676691583
2
Iteration 16700: Loss = -9907.393800381906
Iteration 16800: Loss = -9907.395268659908
1
Iteration 16900: Loss = -9907.393809923475
Iteration 17000: Loss = -9907.396769200857
1
Iteration 17100: Loss = -9907.393795366834
Iteration 17200: Loss = -9907.398780658925
1
Iteration 17300: Loss = -9907.393801396072
Iteration 17400: Loss = -9907.394935183731
1
Iteration 17500: Loss = -9907.394026070544
2
Iteration 17600: Loss = -9907.39381123946
Iteration 17700: Loss = -9907.540250369473
1
Iteration 17800: Loss = -9907.393798218052
Iteration 17900: Loss = -9907.538181498878
1
Iteration 18000: Loss = -9907.39381529375
Iteration 18100: Loss = -9907.39509555883
1
Iteration 18200: Loss = -9907.393773270456
Iteration 18300: Loss = -9907.393989629083
1
Iteration 18400: Loss = -9907.469559632607
2
Iteration 18500: Loss = -9907.393776168621
Iteration 18600: Loss = -9907.398791775968
1
Iteration 18700: Loss = -9907.393811057116
Iteration 18800: Loss = -9907.39404135025
1
Iteration 18900: Loss = -9907.394913348155
2
Iteration 19000: Loss = -9907.394006120334
3
Iteration 19100: Loss = -9907.393944059573
4
Iteration 19200: Loss = -9907.395702190437
5
Iteration 19300: Loss = -9907.40037626971
6
Iteration 19400: Loss = -9907.396021731773
7
Iteration 19500: Loss = -9907.422594308904
8
Iteration 19600: Loss = -9907.394048985645
9
Iteration 19700: Loss = -9907.393825834299
Iteration 19800: Loss = -9907.394893129971
1
Iteration 19900: Loss = -9907.407484752563
2
pi: tensor([[1.0000e+00, 2.5808e-07],
        [1.0732e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0559, 0.9441], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2614, 0.1801],
         [0.6029, 0.1349]],

        [[0.5209, 0.1800],
         [0.6367, 0.7076]],

        [[0.5150, 0.1964],
         [0.5006, 0.5294]],

        [[0.6089, 0.1413],
         [0.5849, 0.5759]],

        [[0.6094, 0.0886],
         [0.5462, 0.6637]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.017056499655107544
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.01357925919519104
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.004682108332720131
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
Global Adjusted Rand Index: -0.0006367033707332506
Average Adjusted Rand Index: -0.0019773695016435354
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21640.615368007948
Iteration 100: Loss = -9916.86297877792
Iteration 200: Loss = -9915.74723011095
Iteration 300: Loss = -9915.279198103099
Iteration 400: Loss = -9914.973394160454
Iteration 500: Loss = -9914.732350964774
Iteration 600: Loss = -9914.613191086484
Iteration 700: Loss = -9914.496654064158
Iteration 800: Loss = -9914.16927562153
Iteration 900: Loss = -9912.606046611287
Iteration 1000: Loss = -9912.171923339105
Iteration 1100: Loss = -9911.913658569501
Iteration 1200: Loss = -9911.765337694507
Iteration 1300: Loss = -9911.660161642201
Iteration 1400: Loss = -9911.578155840578
Iteration 1500: Loss = -9911.514270759602
Iteration 1600: Loss = -9911.464119915858
Iteration 1700: Loss = -9911.429877450748
Iteration 1800: Loss = -9911.40907054038
Iteration 1900: Loss = -9911.396140199282
Iteration 2000: Loss = -9911.387140288638
Iteration 2100: Loss = -9911.380550769445
Iteration 2200: Loss = -9911.37564336465
Iteration 2300: Loss = -9911.371822700463
Iteration 2400: Loss = -9911.368857303987
Iteration 2500: Loss = -9911.36649453645
Iteration 2600: Loss = -9911.364644362726
Iteration 2700: Loss = -9911.363135276804
Iteration 2800: Loss = -9911.361978202218
Iteration 2900: Loss = -9911.360898583904
Iteration 3000: Loss = -9911.36007445138
Iteration 3100: Loss = -9911.359298127249
Iteration 3200: Loss = -9911.358627699288
Iteration 3300: Loss = -9911.358064452876
Iteration 3400: Loss = -9911.357566972985
Iteration 3500: Loss = -9911.357148866091
Iteration 3600: Loss = -9911.356766366347
Iteration 3700: Loss = -9911.356397781405
Iteration 3800: Loss = -9911.356042033625
Iteration 3900: Loss = -9911.355830179173
Iteration 4000: Loss = -9911.355575755522
Iteration 4100: Loss = -9911.355334970247
Iteration 4200: Loss = -9911.355110235025
Iteration 4300: Loss = -9911.354934066467
Iteration 4400: Loss = -9911.354781562375
Iteration 4500: Loss = -9911.354652892422
Iteration 4600: Loss = -9911.35449935641
Iteration 4700: Loss = -9911.354359068844
Iteration 4800: Loss = -9911.354229482107
Iteration 4900: Loss = -9911.35827736887
1
Iteration 5000: Loss = -9911.354019671375
Iteration 5100: Loss = -9911.353908477242
Iteration 5200: Loss = -9911.353808192827
Iteration 5300: Loss = -9911.353657574771
Iteration 5400: Loss = -9911.35353770668
Iteration 5500: Loss = -9911.353465808976
Iteration 5600: Loss = -9911.353302582538
Iteration 5700: Loss = -9911.353192020386
Iteration 5800: Loss = -9911.353112083681
Iteration 5900: Loss = -9911.353010965257
Iteration 6000: Loss = -9911.354998154582
1
Iteration 6100: Loss = -9911.352935847734
Iteration 6200: Loss = -9911.353149350358
1
Iteration 6300: Loss = -9911.354002364185
2
Iteration 6400: Loss = -9911.352853628277
Iteration 6500: Loss = -9911.352839783873
Iteration 6600: Loss = -9911.359499865184
1
Iteration 6700: Loss = -9911.352808576468
Iteration 6800: Loss = -9911.352819357095
Iteration 6900: Loss = -9911.352808133108
Iteration 7000: Loss = -9911.352801529889
Iteration 7100: Loss = -9911.355566316071
1
Iteration 7200: Loss = -9911.352766203427
Iteration 7300: Loss = -9911.35276395701
Iteration 7400: Loss = -9911.352766795082
Iteration 7500: Loss = -9911.352767949473
Iteration 7600: Loss = -9911.355341018954
1
Iteration 7700: Loss = -9911.352725138726
Iteration 7800: Loss = -9911.358160588892
1
Iteration 7900: Loss = -9911.35271822263
Iteration 8000: Loss = -9911.355869001954
1
Iteration 8100: Loss = -9911.352685853199
Iteration 8200: Loss = -9911.352757771774
Iteration 8300: Loss = -9911.35582770491
1
Iteration 8400: Loss = -9911.352658349058
Iteration 8500: Loss = -9911.353190076454
1
Iteration 8600: Loss = -9911.352680344695
Iteration 8700: Loss = -9911.352654298174
Iteration 8800: Loss = -9911.35273745923
Iteration 8900: Loss = -9911.352665547589
Iteration 9000: Loss = -9911.352669792257
Iteration 9100: Loss = -9911.352745050055
Iteration 9200: Loss = -9911.352654697366
Iteration 9300: Loss = -9911.50775196325
1
Iteration 9400: Loss = -9911.352664414619
Iteration 9500: Loss = -9911.352650088056
Iteration 9600: Loss = -9911.381953414373
1
Iteration 9700: Loss = -9911.352659479304
Iteration 9800: Loss = -9911.35264495344
Iteration 9900: Loss = -9911.35268271265
Iteration 10000: Loss = -9911.352639132985
Iteration 10100: Loss = -9911.35263527647
Iteration 10200: Loss = -9911.357958535988
1
Iteration 10300: Loss = -9911.352633621733
Iteration 10400: Loss = -9911.3526438255
Iteration 10500: Loss = -9911.352927462218
1
Iteration 10600: Loss = -9911.352627107066
Iteration 10700: Loss = -9911.3526355771
Iteration 10800: Loss = -9911.352708064167
Iteration 10900: Loss = -9911.352634592638
Iteration 11000: Loss = -9911.407210203353
1
Iteration 11100: Loss = -9911.352641309293
Iteration 11200: Loss = -9911.352633975017
Iteration 11300: Loss = -9911.353227633985
1
Iteration 11400: Loss = -9911.352620578738
Iteration 11500: Loss = -9911.352635006298
Iteration 11600: Loss = -9911.353406417193
1
Iteration 11700: Loss = -9911.352629406123
Iteration 11800: Loss = -9911.359359845039
1
Iteration 11900: Loss = -9911.352627738586
Iteration 12000: Loss = -9911.352631678334
Iteration 12100: Loss = -9911.356579092402
1
Iteration 12200: Loss = -9911.352645122346
Iteration 12300: Loss = -9911.352638762583
Iteration 12400: Loss = -9911.352664205135
Iteration 12500: Loss = -9911.49416811555
1
Iteration 12600: Loss = -9911.352624681109
Iteration 12700: Loss = -9911.420401631818
1
Iteration 12800: Loss = -9911.35261838149
Iteration 12900: Loss = -9911.353906876046
1
Iteration 13000: Loss = -9911.352663469652
Iteration 13100: Loss = -9911.352637025991
Iteration 13200: Loss = -9911.352906955903
1
Iteration 13300: Loss = -9911.352630405852
Iteration 13400: Loss = -9911.358033367032
1
Iteration 13500: Loss = -9911.352619753794
Iteration 13600: Loss = -9911.376412224308
1
Iteration 13700: Loss = -9911.352636859952
Iteration 13800: Loss = -9911.35264696873
Iteration 13900: Loss = -9911.352742962858
Iteration 14000: Loss = -9911.352629020112
Iteration 14100: Loss = -9911.357476182338
1
Iteration 14200: Loss = -9911.352642731075
Iteration 14300: Loss = -9911.35282210375
1
Iteration 14400: Loss = -9911.352657219202
Iteration 14500: Loss = -9911.353236156436
1
Iteration 14600: Loss = -9911.353394190113
2
Iteration 14700: Loss = -9911.352634401825
Iteration 14800: Loss = -9911.35261916017
Iteration 14900: Loss = -9911.353050937529
1
Iteration 15000: Loss = -9911.352649757904
Iteration 15100: Loss = -9911.38232904124
1
Iteration 15200: Loss = -9911.352622598291
Iteration 15300: Loss = -9911.352634411876
Iteration 15400: Loss = -9911.352810511187
1
Iteration 15500: Loss = -9911.354667439518
2
Iteration 15600: Loss = -9911.352651256484
Iteration 15700: Loss = -9911.390512313148
1
Iteration 15800: Loss = -9911.352635673658
Iteration 15900: Loss = -9911.357902357524
1
Iteration 16000: Loss = -9911.352651788153
Iteration 16100: Loss = -9911.353710340496
1
Iteration 16200: Loss = -9911.352704235906
Iteration 16300: Loss = -9911.352853982886
1
Iteration 16400: Loss = -9911.370615139138
2
Iteration 16500: Loss = -9911.352656269915
Iteration 16600: Loss = -9911.356088862534
1
Iteration 16700: Loss = -9911.35266359118
Iteration 16800: Loss = -9911.353580170151
1
Iteration 16900: Loss = -9911.352673497157
Iteration 17000: Loss = -9911.36366852206
1
Iteration 17100: Loss = -9911.352636737545
Iteration 17200: Loss = -9911.35269038269
Iteration 17300: Loss = -9911.359700633455
1
Iteration 17400: Loss = -9911.352633147206
Iteration 17500: Loss = -9911.35426092947
1
Iteration 17600: Loss = -9911.352634500852
Iteration 17700: Loss = -9911.353469820277
1
Iteration 17800: Loss = -9911.352668720016
Iteration 17900: Loss = -9911.352988584673
1
Iteration 18000: Loss = -9911.36362170786
2
Iteration 18100: Loss = -9911.352658246497
Iteration 18200: Loss = -9911.643858380505
1
Iteration 18300: Loss = -9911.352645753379
Iteration 18400: Loss = -9911.369914383475
1
Iteration 18500: Loss = -9911.353888972133
2
Iteration 18600: Loss = -9911.42166461581
3
Iteration 18700: Loss = -9911.352625350577
Iteration 18800: Loss = -9911.356841904242
1
Iteration 18900: Loss = -9911.386166211088
2
Iteration 19000: Loss = -9911.35264575658
Iteration 19100: Loss = -9911.353690714326
1
Iteration 19200: Loss = -9911.352587235366
Iteration 19300: Loss = -9911.353123932331
1
Iteration 19400: Loss = -9911.35266063362
Iteration 19500: Loss = -9911.606356781354
1
Iteration 19600: Loss = -9911.352626727668
Iteration 19700: Loss = -9911.50167720534
1
Iteration 19800: Loss = -9911.35263762345
Iteration 19900: Loss = -9911.3610312799
1
pi: tensor([[0.9953, 0.0047],
        [0.6059, 0.3941]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8713, 0.1287], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1333, 0.1661],
         [0.5977, 0.2312]],

        [[0.5402, 0.1803],
         [0.6122, 0.6935]],

        [[0.7264, 0.2204],
         [0.6522, 0.5421]],

        [[0.6577, 0.1609],
         [0.5426, 0.7026]],

        [[0.5874, 0.2156],
         [0.5111, 0.5198]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001891506915500912
Average Adjusted Rand Index: -0.001869303920740405
10046.354326748846
[-0.0006367033707332506, -0.001891506915500912] [-0.0019773695016435354, -0.001869303920740405] [9907.393797372131, 9911.35267607895]
-------------------------------------
This iteration is 95
True Objective function: Loss = -9924.352291528518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20097.752385536187
Iteration 100: Loss = -9823.729630451979
Iteration 200: Loss = -9822.873654124727
Iteration 300: Loss = -9822.44505467152
Iteration 400: Loss = -9821.836709675594
Iteration 500: Loss = -9821.20644109484
Iteration 600: Loss = -9820.690240035017
Iteration 700: Loss = -9820.315960018766
Iteration 800: Loss = -9819.979501097436
Iteration 900: Loss = -9819.730749428381
Iteration 1000: Loss = -9819.512158898628
Iteration 1100: Loss = -9819.2926284461
Iteration 1200: Loss = -9818.956776506207
Iteration 1300: Loss = -9817.56199308738
Iteration 1400: Loss = -9813.156857193717
Iteration 1500: Loss = -9809.122598972423
Iteration 1600: Loss = -9808.910851268978
Iteration 1700: Loss = -9808.856397939082
Iteration 1800: Loss = -9808.819690092509
Iteration 1900: Loss = -9808.800872273521
Iteration 2000: Loss = -9808.789176311178
Iteration 2100: Loss = -9808.780504810855
Iteration 2200: Loss = -9808.773880287563
Iteration 2300: Loss = -9808.768833026224
Iteration 2400: Loss = -9808.764440470293
Iteration 2500: Loss = -9808.760544761362
Iteration 2600: Loss = -9808.75693257444
Iteration 2700: Loss = -9808.753655802084
Iteration 2800: Loss = -9808.755166356754
1
Iteration 2900: Loss = -9808.74836151298
Iteration 3000: Loss = -9808.746256120698
Iteration 3100: Loss = -9808.744439774055
Iteration 3200: Loss = -9808.74279065954
Iteration 3300: Loss = -9808.74130989003
Iteration 3400: Loss = -9808.739934584812
Iteration 3500: Loss = -9808.738684577553
Iteration 3600: Loss = -9808.737507412403
Iteration 3700: Loss = -9808.736412958662
Iteration 3800: Loss = -9808.73541312252
Iteration 3900: Loss = -9808.734602337738
Iteration 4000: Loss = -9808.733683911474
Iteration 4100: Loss = -9808.732887283184
Iteration 4200: Loss = -9808.73249574429
Iteration 4300: Loss = -9808.731475116894
Iteration 4400: Loss = -9808.730849247822
Iteration 4500: Loss = -9808.73042950379
Iteration 4600: Loss = -9808.729621577952
Iteration 4700: Loss = -9808.729107634883
Iteration 4800: Loss = -9808.728594072225
Iteration 4900: Loss = -9808.728069844077
Iteration 5000: Loss = -9808.727671600285
Iteration 5100: Loss = -9808.727195370278
Iteration 5200: Loss = -9808.727209758032
Iteration 5300: Loss = -9808.726395807676
Iteration 5400: Loss = -9808.726017946352
Iteration 5500: Loss = -9808.725951648463
Iteration 5600: Loss = -9808.725392339824
Iteration 5700: Loss = -9808.725085579057
Iteration 5800: Loss = -9808.72489979898
Iteration 5900: Loss = -9808.724544563884
Iteration 6000: Loss = -9808.743648054495
1
Iteration 6100: Loss = -9808.724021097574
Iteration 6200: Loss = -9808.723779566928
Iteration 6300: Loss = -9808.724053902019
1
Iteration 6400: Loss = -9808.723364122758
Iteration 6500: Loss = -9808.723196185912
Iteration 6600: Loss = -9808.72308549463
Iteration 6700: Loss = -9808.722848544
Iteration 6800: Loss = -9808.722690469373
Iteration 6900: Loss = -9808.72250333176
Iteration 7000: Loss = -9808.722370422047
Iteration 7100: Loss = -9808.722639820204
1
Iteration 7200: Loss = -9808.722102781345
Iteration 7300: Loss = -9808.722103650663
Iteration 7400: Loss = -9808.721853753379
Iteration 7500: Loss = -9808.721784359435
Iteration 7600: Loss = -9808.721602685744
Iteration 7700: Loss = -9808.741953846526
1
Iteration 7800: Loss = -9808.721438007988
Iteration 7900: Loss = -9808.721304445531
Iteration 8000: Loss = -9808.723482465955
1
Iteration 8100: Loss = -9808.721152511747
Iteration 8200: Loss = -9808.722332518995
1
Iteration 8300: Loss = -9808.72094804111
Iteration 8400: Loss = -9808.720903514812
Iteration 8500: Loss = -9808.720873388767
Iteration 8600: Loss = -9808.738746476507
1
Iteration 8700: Loss = -9808.720738848244
Iteration 8800: Loss = -9808.721790735783
1
Iteration 8900: Loss = -9808.720805721267
Iteration 9000: Loss = -9808.720541371527
Iteration 9100: Loss = -9808.777394671959
1
Iteration 9200: Loss = -9808.720462254045
Iteration 9300: Loss = -9808.720897596912
1
Iteration 9400: Loss = -9808.720499237801
Iteration 9500: Loss = -9808.72033469736
Iteration 9600: Loss = -9808.864319989512
1
Iteration 9700: Loss = -9808.7202529592
Iteration 9800: Loss = -9808.721347728217
1
Iteration 9900: Loss = -9808.720422007205
2
Iteration 10000: Loss = -9808.815140073571
3
Iteration 10100: Loss = -9808.720093648382
Iteration 10200: Loss = -9808.720825507518
1
Iteration 10300: Loss = -9808.720211242662
2
Iteration 10400: Loss = -9808.722234647921
3
Iteration 10500: Loss = -9808.761290648788
4
Iteration 10600: Loss = -9808.720410815316
5
Iteration 10700: Loss = -9808.720755033104
6
Iteration 10800: Loss = -9808.720905254888
7
Iteration 10900: Loss = -9808.72744689505
8
Iteration 11000: Loss = -9808.724415119666
9
Iteration 11100: Loss = -9808.725709533708
10
Iteration 11200: Loss = -9808.783535758752
11
Iteration 11300: Loss = -9808.727954665992
12
Iteration 11400: Loss = -9808.740398632764
13
Iteration 11500: Loss = -9808.71981983504
Iteration 11600: Loss = -9808.72012188255
1
Iteration 11700: Loss = -9808.736675788032
2
Iteration 11800: Loss = -9808.719762409773
Iteration 11900: Loss = -9808.721638808247
1
Iteration 12000: Loss = -9808.721595700945
2
Iteration 12100: Loss = -9808.724466069274
3
Iteration 12200: Loss = -9808.722607125732
4
Iteration 12300: Loss = -9808.726832668752
5
Iteration 12400: Loss = -9808.728499552346
6
Iteration 12500: Loss = -9808.917674669889
7
Iteration 12600: Loss = -9808.719698470644
Iteration 12700: Loss = -9808.722656937844
1
Iteration 12800: Loss = -9808.722375771136
2
Iteration 12900: Loss = -9808.721432733822
3
Iteration 13000: Loss = -9808.71992595104
4
Iteration 13100: Loss = -9808.719766487653
Iteration 13200: Loss = -9808.719803818785
Iteration 13300: Loss = -9808.860650368688
1
Iteration 13400: Loss = -9808.719599567112
Iteration 13500: Loss = -9808.719766327795
1
Iteration 13600: Loss = -9808.719661157927
Iteration 13700: Loss = -9808.720873812492
1
Iteration 13800: Loss = -9808.72121889975
2
Iteration 13900: Loss = -9808.725078435069
3
Iteration 14000: Loss = -9808.725214294254
4
Iteration 14100: Loss = -9808.719618608206
Iteration 14200: Loss = -9808.725974299117
1
Iteration 14300: Loss = -9808.750701246396
2
Iteration 14400: Loss = -9808.720507765873
3
Iteration 14500: Loss = -9808.756641647064
4
Iteration 14600: Loss = -9808.721537779895
5
Iteration 14700: Loss = -9808.751096358752
6
Iteration 14800: Loss = -9808.72806861022
7
Iteration 14900: Loss = -9808.725890229223
8
Iteration 15000: Loss = -9808.720004924897
9
Iteration 15100: Loss = -9808.720991430546
10
Iteration 15200: Loss = -9808.721233330965
11
Iteration 15300: Loss = -9808.732174601224
12
Iteration 15400: Loss = -9808.719626456736
Iteration 15500: Loss = -9808.720430305204
1
Iteration 15600: Loss = -9808.729730220424
2
Iteration 15700: Loss = -9808.729714824249
3
Iteration 15800: Loss = -9808.740386027184
4
Iteration 15900: Loss = -9808.73732915886
5
Iteration 16000: Loss = -9808.721685878854
6
Iteration 16100: Loss = -9808.719806799629
7
Iteration 16200: Loss = -9808.720744223281
8
Iteration 16300: Loss = -9808.720202047381
9
Iteration 16400: Loss = -9808.71959835028
Iteration 16500: Loss = -9808.719790435573
1
Iteration 16600: Loss = -9808.74307603146
2
Iteration 16700: Loss = -9808.721001161772
3
Iteration 16800: Loss = -9808.71955875566
Iteration 16900: Loss = -9808.719697598375
1
Iteration 17000: Loss = -9808.749955629153
2
Iteration 17100: Loss = -9808.7233478329
3
Iteration 17200: Loss = -9808.721123288042
4
Iteration 17300: Loss = -9808.757181918309
5
Iteration 17400: Loss = -9808.73355936012
6
Iteration 17500: Loss = -9808.720538000585
7
Iteration 17600: Loss = -9808.800768847814
8
Iteration 17700: Loss = -9808.726852286436
9
Iteration 17800: Loss = -9808.719525386932
Iteration 17900: Loss = -9808.719586531122
Iteration 18000: Loss = -9808.720115434313
1
Iteration 18100: Loss = -9808.722512683353
2
Iteration 18200: Loss = -9808.733243596886
3
Iteration 18300: Loss = -9808.720183719877
4
Iteration 18400: Loss = -9808.719567366794
Iteration 18500: Loss = -9808.720096166804
1
Iteration 18600: Loss = -9808.721100161913
2
Iteration 18700: Loss = -9808.725649487067
3
Iteration 18800: Loss = -9808.719785051517
4
Iteration 18900: Loss = -9808.720814986797
5
Iteration 19000: Loss = -9808.728843384572
6
Iteration 19100: Loss = -9808.719989663168
7
Iteration 19200: Loss = -9808.719532442692
Iteration 19300: Loss = -9808.724031833226
1
Iteration 19400: Loss = -9808.773407700479
2
Iteration 19500: Loss = -9808.719865051082
3
Iteration 19600: Loss = -9808.719599474429
Iteration 19700: Loss = -9808.727418266884
1
Iteration 19800: Loss = -9808.721657882594
2
Iteration 19900: Loss = -9808.752022367185
3
pi: tensor([[8.3950e-01, 1.6050e-01],
        [1.7240e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5639, 0.4361], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1993, 0.1235],
         [0.6563, 0.1294]],

        [[0.7021, 0.1011],
         [0.6076, 0.7296]],

        [[0.6449, 0.1082],
         [0.5565, 0.6549]],

        [[0.6354, 0.1336],
         [0.6652, 0.6991]],

        [[0.5316, 0.1182],
         [0.7179, 0.5187]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 23
Adjusted Rand Index: 0.28438556560760186
time is 1
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 14
Adjusted Rand Index: 0.5134936572464123
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 23
Adjusted Rand Index: 0.28486472596453877
time is 3
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.08080808080808081
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 26
Adjusted Rand Index: 0.22316194085694444
Global Adjusted Rand Index: 0.2648021252989963
Average Adjusted Rand Index: 0.27734279409671564
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24179.71156459897
Iteration 100: Loss = -9824.202230874698
Iteration 200: Loss = -9822.782906827224
Iteration 300: Loss = -9821.834257825676
Iteration 400: Loss = -9821.41967498002
Iteration 500: Loss = -9821.18175043815
Iteration 600: Loss = -9821.003403040375
Iteration 700: Loss = -9820.835342182245
Iteration 800: Loss = -9820.618574619099
Iteration 900: Loss = -9819.906982025665
Iteration 1000: Loss = -9818.871526337765
Iteration 1100: Loss = -9816.877041542264
Iteration 1200: Loss = -9813.667142549028
Iteration 1300: Loss = -9809.144219718948
Iteration 1400: Loss = -9806.945347906827
Iteration 1500: Loss = -9805.037852863748
Iteration 1600: Loss = -9804.315021843135
Iteration 1700: Loss = -9804.090504141825
Iteration 1800: Loss = -9804.013980756268
Iteration 1900: Loss = -9803.975286931347
Iteration 2000: Loss = -9803.951871234889
Iteration 2100: Loss = -9803.943207858438
Iteration 2200: Loss = -9803.938988124544
Iteration 2300: Loss = -9803.933034593118
Iteration 2400: Loss = -9803.929781604173
Iteration 2500: Loss = -9803.922049891014
Iteration 2600: Loss = -9803.919321641997
Iteration 2700: Loss = -9803.918885271953
Iteration 2800: Loss = -9803.91857906841
Iteration 2900: Loss = -9803.91828660612
Iteration 3000: Loss = -9803.918042719111
Iteration 3100: Loss = -9803.917855350539
Iteration 3200: Loss = -9803.917645878953
Iteration 3300: Loss = -9803.917430545216
Iteration 3400: Loss = -9803.917164949293
Iteration 3500: Loss = -9803.916934437755
Iteration 3600: Loss = -9803.916374714077
Iteration 3700: Loss = -9803.91537155864
Iteration 3800: Loss = -9803.912041760253
Iteration 3900: Loss = -9803.907613255673
Iteration 4000: Loss = -9803.907405317314
Iteration 4100: Loss = -9803.907337790675
Iteration 4200: Loss = -9803.907064833831
Iteration 4300: Loss = -9803.906963088142
Iteration 4400: Loss = -9803.90686627633
Iteration 4500: Loss = -9803.90992202652
1
Iteration 4600: Loss = -9803.906727561101
Iteration 4700: Loss = -9803.907123703606
1
Iteration 4800: Loss = -9803.90658941919
Iteration 4900: Loss = -9803.906633485118
Iteration 5000: Loss = -9803.906361375548
Iteration 5100: Loss = -9803.90710121686
1
Iteration 5200: Loss = -9803.906184174857
Iteration 5300: Loss = -9803.90620992439
Iteration 5400: Loss = -9803.90611083664
Iteration 5500: Loss = -9803.906029431755
Iteration 5600: Loss = -9803.90675977913
1
Iteration 5700: Loss = -9803.905631346306
Iteration 5800: Loss = -9803.905070127561
Iteration 5900: Loss = -9803.90498599329
Iteration 6000: Loss = -9803.90797731744
1
Iteration 6100: Loss = -9803.905201440848
2
Iteration 6200: Loss = -9803.904983214437
Iteration 6300: Loss = -9803.920086338083
1
Iteration 6400: Loss = -9803.905722068472
2
Iteration 6500: Loss = -9803.905238472606
3
Iteration 6600: Loss = -9803.905834468067
4
Iteration 6700: Loss = -9803.904886715272
Iteration 6800: Loss = -9803.904786427898
Iteration 6900: Loss = -9803.904844011207
Iteration 7000: Loss = -9803.9049441866
1
Iteration 7100: Loss = -9803.904776648722
Iteration 7200: Loss = -9803.904852544976
Iteration 7300: Loss = -9803.907066990874
1
Iteration 7400: Loss = -9803.904674149915
Iteration 7500: Loss = -9803.904700678131
Iteration 7600: Loss = -9803.907896048137
1
Iteration 7700: Loss = -9803.905749424577
2
Iteration 7800: Loss = -9803.90499931579
3
Iteration 7900: Loss = -9803.904675370093
Iteration 8000: Loss = -9803.904583963309
Iteration 8100: Loss = -9803.904640557179
Iteration 8200: Loss = -9803.905814099655
1
Iteration 8300: Loss = -9803.914575871922
2
Iteration 8400: Loss = -9803.904599172318
Iteration 8500: Loss = -9803.908888581913
1
Iteration 8600: Loss = -9803.915044903906
2
Iteration 8700: Loss = -9803.905039612628
3
Iteration 8800: Loss = -9803.905085881079
4
Iteration 8900: Loss = -9803.906125755013
5
Iteration 9000: Loss = -9803.911646400473
6
Iteration 9100: Loss = -9803.90941274771
7
Iteration 9200: Loss = -9803.905367665679
8
Iteration 9300: Loss = -9803.904699303846
9
Iteration 9400: Loss = -9803.97288950577
10
Iteration 9500: Loss = -9803.942070585987
11
Iteration 9600: Loss = -9803.955254567249
12
Iteration 9700: Loss = -9803.911936755505
13
Iteration 9800: Loss = -9803.904613596169
Iteration 9900: Loss = -9803.90460849463
Iteration 10000: Loss = -9803.917865085452
1
Iteration 10100: Loss = -9803.916990362422
2
Iteration 10200: Loss = -9803.904543037665
Iteration 10300: Loss = -9803.905643172939
1
Iteration 10400: Loss = -9804.015725435196
2
Iteration 10500: Loss = -9803.904571926741
Iteration 10600: Loss = -9803.926031928971
1
Iteration 10700: Loss = -9803.904531709532
Iteration 10800: Loss = -9804.043118550224
1
Iteration 10900: Loss = -9803.90453907978
Iteration 11000: Loss = -9803.904566299776
Iteration 11100: Loss = -9803.90468042431
1
Iteration 11200: Loss = -9803.904562807968
Iteration 11300: Loss = -9803.905890188926
1
Iteration 11400: Loss = -9803.90453351832
Iteration 11500: Loss = -9803.904795399038
1
Iteration 11600: Loss = -9803.904548688519
Iteration 11700: Loss = -9803.904612572449
Iteration 11800: Loss = -9803.908970408398
1
Iteration 11900: Loss = -9803.90458345598
Iteration 12000: Loss = -9803.904954173619
1
Iteration 12100: Loss = -9803.935285814525
2
Iteration 12200: Loss = -9803.908130193518
3
Iteration 12300: Loss = -9803.905697380284
4
Iteration 12400: Loss = -9803.904820480813
5
Iteration 12500: Loss = -9803.904760204701
6
Iteration 12600: Loss = -9803.905941987168
7
Iteration 12700: Loss = -9803.905840096626
8
Iteration 12800: Loss = -9803.91476581847
9
Iteration 12900: Loss = -9803.92191047319
10
Iteration 13000: Loss = -9803.906566636531
11
Iteration 13100: Loss = -9803.904794477421
12
Iteration 13200: Loss = -9803.906725430561
13
Iteration 13300: Loss = -9803.914180893493
14
Iteration 13400: Loss = -9803.911579282663
15
Stopping early at iteration 13400 due to no improvement.
pi: tensor([[0.8599, 0.1401],
        [0.1633, 0.8367]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4937, 0.5063], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1425, 0.1256],
         [0.6113, 0.1972]],

        [[0.7134, 0.0919],
         [0.7165, 0.7181]],

        [[0.5988, 0.0896],
         [0.6490, 0.6995]],

        [[0.5918, 0.1046],
         [0.7015, 0.6544]],

        [[0.5890, 0.1036],
         [0.5730, 0.5810]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 76
Adjusted Rand Index: 0.2629609983528295
time is 1
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7025959183673469
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 91
Adjusted Rand Index: 0.6690692994951206
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 76
Adjusted Rand Index: 0.263030303030303
time is 4
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 76
Adjusted Rand Index: 0.2629995025605804
Global Adjusted Rand Index: 0.4135627216924835
Average Adjusted Rand Index: 0.4321312043612361
9924.352291528518
[0.2648021252989963, 0.4135627216924835] [0.27734279409671564, 0.4321312043612361] [9808.75492627966, 9803.911579282663]
-------------------------------------
This iteration is 96
True Objective function: Loss = -9998.008397861695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22574.65354212216
Iteration 100: Loss = -9824.631017986456
Iteration 200: Loss = -9824.387056993657
Iteration 300: Loss = -9824.349536289199
Iteration 400: Loss = -9824.314697592223
Iteration 500: Loss = -9824.280673484096
Iteration 600: Loss = -9824.246077655453
Iteration 700: Loss = -9824.210203910467
Iteration 800: Loss = -9824.173650527204
Iteration 900: Loss = -9824.136915104149
Iteration 1000: Loss = -9824.099738895626
Iteration 1100: Loss = -9824.062383529774
Iteration 1200: Loss = -9824.025165890882
Iteration 1300: Loss = -9823.98828414637
Iteration 1400: Loss = -9823.95106419704
Iteration 1500: Loss = -9823.911284856526
Iteration 1600: Loss = -9823.862985958036
Iteration 1700: Loss = -9823.793089949035
Iteration 1800: Loss = -9823.680428497812
Iteration 1900: Loss = -9823.534670731939
Iteration 2000: Loss = -9823.43152304163
Iteration 2100: Loss = -9823.391905822473
Iteration 2200: Loss = -9823.379811775934
Iteration 2300: Loss = -9823.375222168135
Iteration 2400: Loss = -9823.372772762094
Iteration 2500: Loss = -9823.371034903137
Iteration 2600: Loss = -9823.369750358484
Iteration 2700: Loss = -9823.368683989474
Iteration 2800: Loss = -9823.367786527899
Iteration 2900: Loss = -9823.366951377304
Iteration 3000: Loss = -9823.366156872518
Iteration 3100: Loss = -9823.365196385756
Iteration 3200: Loss = -9823.36390190687
Iteration 3300: Loss = -9823.361010393692
Iteration 3400: Loss = -9823.35072992409
Iteration 3500: Loss = -9823.305479467912
Iteration 3600: Loss = -9823.240251428295
Iteration 3700: Loss = -9823.227359746694
Iteration 3800: Loss = -9823.225805781682
Iteration 3900: Loss = -9823.22511094045
Iteration 4000: Loss = -9823.224730875943
Iteration 4100: Loss = -9823.224443376525
Iteration 4200: Loss = -9823.224205331098
Iteration 4300: Loss = -9823.224048955095
Iteration 4400: Loss = -9823.223883406397
Iteration 4500: Loss = -9823.223794735864
Iteration 4600: Loss = -9823.22369552917
Iteration 4700: Loss = -9823.223553447642
Iteration 4800: Loss = -9823.223476726398
Iteration 4900: Loss = -9823.223393885228
Iteration 5000: Loss = -9823.22333078009
Iteration 5100: Loss = -9823.223276829503
Iteration 5200: Loss = -9823.223200902321
Iteration 5300: Loss = -9823.22317499453
Iteration 5400: Loss = -9823.223093109847
Iteration 5500: Loss = -9823.223115244462
Iteration 5600: Loss = -9823.223046403968
Iteration 5700: Loss = -9823.222995669377
Iteration 5800: Loss = -9823.223216028367
1
Iteration 5900: Loss = -9823.222953525741
Iteration 6000: Loss = -9823.225484764844
1
Iteration 6100: Loss = -9823.222879809837
Iteration 6200: Loss = -9823.222844957414
Iteration 6300: Loss = -9823.222856125429
Iteration 6400: Loss = -9823.222799847119
Iteration 6500: Loss = -9823.222995179298
1
Iteration 6600: Loss = -9823.222772796444
Iteration 6700: Loss = -9823.223811066871
1
Iteration 6800: Loss = -9823.222714174377
Iteration 6900: Loss = -9823.222863372177
1
Iteration 7000: Loss = -9823.222683977865
Iteration 7100: Loss = -9823.225257847069
1
Iteration 7200: Loss = -9823.2226945843
Iteration 7300: Loss = -9823.223769626064
1
Iteration 7400: Loss = -9823.222658204671
Iteration 7500: Loss = -9823.223628352871
1
Iteration 7600: Loss = -9823.222621105773
Iteration 7700: Loss = -9823.22291204432
1
Iteration 7800: Loss = -9823.248113588223
2
Iteration 7900: Loss = -9823.222608306061
Iteration 8000: Loss = -9823.223004589026
1
Iteration 8100: Loss = -9823.22260607925
Iteration 8200: Loss = -9823.222946579526
1
Iteration 8300: Loss = -9823.222579909807
Iteration 8400: Loss = -9823.227221635563
1
Iteration 8500: Loss = -9823.222573979156
Iteration 8600: Loss = -9823.222840667206
1
Iteration 8700: Loss = -9823.222537820213
Iteration 8800: Loss = -9823.222530641853
Iteration 8900: Loss = -9823.22254441297
Iteration 9000: Loss = -9823.222743768652
1
Iteration 9100: Loss = -9823.2225369743
Iteration 9200: Loss = -9823.22252143431
Iteration 9300: Loss = -9823.227719964756
1
Iteration 9400: Loss = -9823.22254667758
Iteration 9500: Loss = -9823.222516755375
Iteration 9600: Loss = -9823.222521317337
Iteration 9700: Loss = -9823.22252648735
Iteration 9800: Loss = -9823.222538267275
Iteration 9900: Loss = -9823.222513164721
Iteration 10000: Loss = -9823.222520783082
Iteration 10100: Loss = -9823.222495148333
Iteration 10200: Loss = -9823.222498851159
Iteration 10300: Loss = -9823.389584874914
1
Iteration 10400: Loss = -9823.222533227265
Iteration 10500: Loss = -9823.22248621738
Iteration 10600: Loss = -9823.655665582952
1
Iteration 10700: Loss = -9823.222534284643
Iteration 10800: Loss = -9823.222510954356
Iteration 10900: Loss = -9823.417201126475
1
Iteration 11000: Loss = -9823.222497192099
Iteration 11100: Loss = -9823.222492726245
Iteration 11200: Loss = -9823.282434775405
1
Iteration 11300: Loss = -9823.22246862263
Iteration 11400: Loss = -9823.222458837261
Iteration 11500: Loss = -9823.741566356153
1
Iteration 11600: Loss = -9823.222479772621
Iteration 11700: Loss = -9823.222452799515
Iteration 11800: Loss = -9823.222481022258
Iteration 11900: Loss = -9823.222630834516
1
Iteration 12000: Loss = -9823.222459322425
Iteration 12100: Loss = -9823.222460480436
Iteration 12200: Loss = -9823.222489970516
Iteration 12300: Loss = -9823.22244908912
Iteration 12400: Loss = -9823.222462019785
Iteration 12500: Loss = -9823.222608718028
1
Iteration 12600: Loss = -9823.222459467252
Iteration 12700: Loss = -9823.222772519724
1
Iteration 12800: Loss = -9823.222477195088
Iteration 12900: Loss = -9823.222672923082
1
Iteration 13000: Loss = -9823.223709534868
2
Iteration 13100: Loss = -9823.229428931174
3
Iteration 13200: Loss = -9823.22245251073
Iteration 13300: Loss = -9823.234486347475
1
Iteration 13400: Loss = -9823.222465492654
Iteration 13500: Loss = -9823.222448149663
Iteration 13600: Loss = -9823.223890785313
1
Iteration 13700: Loss = -9823.22247847304
Iteration 13800: Loss = -9823.22570526054
1
Iteration 13900: Loss = -9823.222483195626
Iteration 14000: Loss = -9823.244490569206
1
Iteration 14100: Loss = -9823.222462342232
Iteration 14200: Loss = -9823.228105099186
1
Iteration 14300: Loss = -9823.22245464021
Iteration 14400: Loss = -9823.259436200187
1
Iteration 14500: Loss = -9823.222459743718
Iteration 14600: Loss = -9823.35334069908
1
Iteration 14700: Loss = -9823.222498399906
Iteration 14800: Loss = -9823.222497640623
Iteration 14900: Loss = -9823.246040026761
1
Iteration 15000: Loss = -9823.222449373863
Iteration 15100: Loss = -9823.29296677476
1
Iteration 15200: Loss = -9823.222474573486
Iteration 15300: Loss = -9823.232446916869
1
Iteration 15400: Loss = -9823.222723354476
2
Iteration 15500: Loss = -9823.222463566615
Iteration 15600: Loss = -9823.228692494447
1
Iteration 15700: Loss = -9823.222465192988
Iteration 15800: Loss = -9823.222841717483
1
Iteration 15900: Loss = -9823.222446487116
Iteration 16000: Loss = -9823.223607384913
1
Iteration 16100: Loss = -9823.222464138547
Iteration 16200: Loss = -9823.225359378277
1
Iteration 16300: Loss = -9823.222445305622
Iteration 16400: Loss = -9823.222443293344
Iteration 16500: Loss = -9823.231657028866
1
Iteration 16600: Loss = -9823.222441090236
Iteration 16700: Loss = -9823.261354598813
1
Iteration 16800: Loss = -9823.22245779345
Iteration 16900: Loss = -9823.227763617468
1
Iteration 17000: Loss = -9823.22252756635
Iteration 17100: Loss = -9823.222601996924
Iteration 17200: Loss = -9823.223851469644
1
Iteration 17300: Loss = -9823.22246578034
Iteration 17400: Loss = -9823.222492488552
Iteration 17500: Loss = -9823.222485258666
Iteration 17600: Loss = -9823.222463020611
Iteration 17700: Loss = -9823.222466510117
Iteration 17800: Loss = -9823.223505512873
1
Iteration 17900: Loss = -9823.222441114469
Iteration 18000: Loss = -9823.22397581144
1
Iteration 18100: Loss = -9823.223045647277
2
Iteration 18200: Loss = -9823.222558543983
3
Iteration 18300: Loss = -9823.226315590975
4
Iteration 18400: Loss = -9823.22245342521
Iteration 18500: Loss = -9823.22729476686
1
Iteration 18600: Loss = -9823.222512241124
Iteration 18700: Loss = -9823.48463479071
1
Iteration 18800: Loss = -9823.222441422864
Iteration 18900: Loss = -9823.287447363946
1
Iteration 19000: Loss = -9823.222511093012
Iteration 19100: Loss = -9823.222473627546
Iteration 19200: Loss = -9823.250464301904
1
Iteration 19300: Loss = -9823.222473974727
Iteration 19400: Loss = -9823.27113654219
1
Iteration 19500: Loss = -9823.222454946568
Iteration 19600: Loss = -9823.256416650813
1
Iteration 19700: Loss = -9823.222442140472
Iteration 19800: Loss = -9823.231836598816
1
Iteration 19900: Loss = -9823.22245702428
pi: tensor([[7.1373e-08, 1.0000e+00],
        [1.7013e-02, 9.8299e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0597, 0.9403], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0815, 0.1061],
         [0.5205, 0.1376]],

        [[0.5313, 0.0655],
         [0.7256, 0.6608]],

        [[0.6007, 0.1520],
         [0.5610, 0.6420]],

        [[0.6938, 0.0462],
         [0.6357, 0.6141]],

        [[0.6579, 0.0964],
         [0.6602, 0.5869]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001579177405897713
Average Adjusted Rand Index: -0.002274891251268548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22388.191611958882
Iteration 100: Loss = -9826.890510036943
Iteration 200: Loss = -9825.459659200182
Iteration 300: Loss = -9825.086874185334
Iteration 400: Loss = -9824.949602494708
Iteration 500: Loss = -9824.883241191852
Iteration 600: Loss = -9824.84479347864
Iteration 700: Loss = -9824.819471946383
Iteration 800: Loss = -9824.800893482832
Iteration 900: Loss = -9824.785813905946
Iteration 1000: Loss = -9824.77247059776
Iteration 1100: Loss = -9824.759638199816
Iteration 1200: Loss = -9824.74652216755
Iteration 1300: Loss = -9824.73238694171
Iteration 1400: Loss = -9824.71650796621
Iteration 1500: Loss = -9824.698506647856
Iteration 1600: Loss = -9824.678402908146
Iteration 1700: Loss = -9824.65504270025
Iteration 1800: Loss = -9824.615453741317
Iteration 1900: Loss = -9824.498219951047
Iteration 2000: Loss = -9824.3839058197
Iteration 2100: Loss = -9824.315364590659
Iteration 2200: Loss = -9824.140129328143
Iteration 2300: Loss = -9823.553384264987
Iteration 2400: Loss = -9823.30932454646
Iteration 2500: Loss = -9823.206008106774
Iteration 2600: Loss = -9823.136090453885
Iteration 2700: Loss = -9823.075336373753
Iteration 2800: Loss = -9823.018212856114
Iteration 2900: Loss = -9822.966972425875
Iteration 3000: Loss = -9822.92360140927
Iteration 3100: Loss = -9822.888848974435
Iteration 3200: Loss = -9822.861871561156
Iteration 3300: Loss = -9822.841420718107
Iteration 3400: Loss = -9822.826050298725
Iteration 3500: Loss = -9822.814516001219
Iteration 3600: Loss = -9822.805784540988
Iteration 3700: Loss = -9822.799156553243
Iteration 3800: Loss = -9822.794005534894
Iteration 3900: Loss = -9822.789965621958
Iteration 4000: Loss = -9822.786773366282
Iteration 4100: Loss = -9822.78422410241
Iteration 4200: Loss = -9822.782079309662
Iteration 4300: Loss = -9822.780303008314
Iteration 4400: Loss = -9822.778785589233
Iteration 4500: Loss = -9822.77752887853
Iteration 4600: Loss = -9822.776425467198
Iteration 4700: Loss = -9822.775421880664
Iteration 4800: Loss = -9822.77456959163
Iteration 4900: Loss = -9822.773802910946
Iteration 5000: Loss = -9822.77310043598
Iteration 5100: Loss = -9822.77249479321
Iteration 5200: Loss = -9822.771949447348
Iteration 5300: Loss = -9822.771421960544
Iteration 5400: Loss = -9822.77095792818
Iteration 5500: Loss = -9822.770567650154
Iteration 5600: Loss = -9822.770171465414
Iteration 5700: Loss = -9822.769813726858
Iteration 5800: Loss = -9822.769493914364
Iteration 5900: Loss = -9822.769192383272
Iteration 6000: Loss = -9822.768917100379
Iteration 6100: Loss = -9822.768637483501
Iteration 6200: Loss = -9822.768421182029
Iteration 6300: Loss = -9822.768227784087
Iteration 6400: Loss = -9822.76799079186
Iteration 6500: Loss = -9822.76780261563
Iteration 6600: Loss = -9822.767637053716
Iteration 6700: Loss = -9822.7674683947
Iteration 6800: Loss = -9822.767325686555
Iteration 6900: Loss = -9822.767181207548
Iteration 7000: Loss = -9822.767028872926
Iteration 7100: Loss = -9822.766898932545
Iteration 7200: Loss = -9822.766810457217
Iteration 7300: Loss = -9822.766674810688
Iteration 7400: Loss = -9822.766552052011
Iteration 7500: Loss = -9822.766470661905
Iteration 7600: Loss = -9822.766402380888
Iteration 7700: Loss = -9822.766316841537
Iteration 7800: Loss = -9822.766223085093
Iteration 7900: Loss = -9822.766112792684
Iteration 8000: Loss = -9822.76606081165
Iteration 8100: Loss = -9822.765989524549
Iteration 8200: Loss = -9822.765927543647
Iteration 8300: Loss = -9822.765835869111
Iteration 8400: Loss = -9822.765752302974
Iteration 8500: Loss = -9822.765749542985
Iteration 8600: Loss = -9822.765818739816
Iteration 8700: Loss = -9822.765691317256
Iteration 8800: Loss = -9822.766086631034
1
Iteration 8900: Loss = -9822.765564761652
Iteration 9000: Loss = -9822.769352691816
1
Iteration 9100: Loss = -9822.766470499975
2
Iteration 9200: Loss = -9822.765447941812
Iteration 9300: Loss = -9822.77299089051
1
Iteration 9400: Loss = -9822.765379315852
Iteration 9500: Loss = -9822.765342032286
Iteration 9600: Loss = -9822.765317638261
Iteration 9700: Loss = -9822.765270317748
Iteration 9800: Loss = -9822.765540354796
1
Iteration 9900: Loss = -9822.765251030352
Iteration 10000: Loss = -9822.76522517787
Iteration 10100: Loss = -9822.765471217366
1
Iteration 10200: Loss = -9822.765173608868
Iteration 10300: Loss = -9822.765127486511
Iteration 10400: Loss = -9823.264989067451
1
Iteration 10500: Loss = -9822.765127923401
Iteration 10600: Loss = -9822.765083306775
Iteration 10700: Loss = -9822.765053250812
Iteration 10800: Loss = -9822.837711568789
1
Iteration 10900: Loss = -9822.765034402757
Iteration 11000: Loss = -9822.765029540386
Iteration 11100: Loss = -9822.765014274766
Iteration 11200: Loss = -9822.765008647739
Iteration 11300: Loss = -9822.764996491196
Iteration 11400: Loss = -9822.765190268663
1
Iteration 11500: Loss = -9822.764958557715
Iteration 11600: Loss = -9822.764947727208
Iteration 11700: Loss = -9822.814415023164
1
Iteration 11800: Loss = -9822.764922907396
Iteration 11900: Loss = -9822.764908125493
Iteration 12000: Loss = -9822.764929927755
Iteration 12100: Loss = -9822.859126672593
1
Iteration 12200: Loss = -9822.764890399525
Iteration 12300: Loss = -9822.764878180567
Iteration 12400: Loss = -9822.76489596588
Iteration 12500: Loss = -9822.80288373188
1
Iteration 12600: Loss = -9822.76489351159
Iteration 12700: Loss = -9822.764872866797
Iteration 12800: Loss = -9822.764866964475
Iteration 12900: Loss = -9822.764871502472
Iteration 13000: Loss = -9822.814066676505
1
Iteration 13100: Loss = -9822.764859332328
Iteration 13200: Loss = -9822.764866664289
Iteration 13300: Loss = -9822.764829197307
Iteration 13400: Loss = -9822.767200581922
1
Iteration 13500: Loss = -9822.764840428646
Iteration 13600: Loss = -9822.991750186724
1
Iteration 13700: Loss = -9822.764821736027
Iteration 13800: Loss = -9822.7648167006
Iteration 13900: Loss = -9822.764922606926
1
Iteration 14000: Loss = -9822.764863274499
Iteration 14100: Loss = -9822.764882322967
Iteration 14200: Loss = -9822.764830508255
Iteration 14300: Loss = -9822.765065732392
1
Iteration 14400: Loss = -9822.76481819228
Iteration 14500: Loss = -9822.789572955158
1
Iteration 14600: Loss = -9822.76483798686
Iteration 14700: Loss = -9822.764795508616
Iteration 14800: Loss = -9822.767099923509
1
Iteration 14900: Loss = -9822.764764858583
Iteration 15000: Loss = -9822.772301862065
1
Iteration 15100: Loss = -9822.764856480615
Iteration 15200: Loss = -9822.803373041404
1
Iteration 15300: Loss = -9822.764799158118
Iteration 15400: Loss = -9822.764793390732
Iteration 15500: Loss = -9822.76488021434
Iteration 15600: Loss = -9822.764791948677
Iteration 15700: Loss = -9822.767150519434
1
Iteration 15800: Loss = -9822.764777012897
Iteration 15900: Loss = -9822.767855997265
1
Iteration 16000: Loss = -9822.764838733152
Iteration 16100: Loss = -9822.77906670959
1
Iteration 16200: Loss = -9822.765253406958
2
Iteration 16300: Loss = -9822.95827731872
3
Iteration 16400: Loss = -9822.764800777984
Iteration 16500: Loss = -9822.764761846702
Iteration 16600: Loss = -9822.764872412372
1
Iteration 16700: Loss = -9822.764874956983
2
Iteration 16800: Loss = -9822.764866535175
3
Iteration 16900: Loss = -9822.764783624296
Iteration 17000: Loss = -9822.764885581462
1
Iteration 17100: Loss = -9822.765073692528
2
Iteration 17200: Loss = -9822.765203220228
3
Iteration 17300: Loss = -9822.765215025334
4
Iteration 17400: Loss = -9822.765858257339
5
Iteration 17500: Loss = -9822.765287497128
6
Iteration 17600: Loss = -9822.765289616636
7
Iteration 17700: Loss = -9822.764828959207
Iteration 17800: Loss = -9822.765682908626
1
Iteration 17900: Loss = -9822.764803224041
Iteration 18000: Loss = -9822.765012506268
1
Iteration 18100: Loss = -9822.765302984528
2
Iteration 18200: Loss = -9822.765294706873
3
Iteration 18300: Loss = -9822.780181762635
4
Iteration 18400: Loss = -9822.765449778339
5
Iteration 18500: Loss = -9822.768725627919
6
Iteration 18600: Loss = -9822.765445854833
7
Iteration 18700: Loss = -9822.768818119086
8
Iteration 18800: Loss = -9822.764787113816
Iteration 18900: Loss = -9822.765694711443
1
Iteration 19000: Loss = -9822.76478907805
Iteration 19100: Loss = -9822.765492341085
1
Iteration 19200: Loss = -9822.764861211737
Iteration 19300: Loss = -9822.768237544858
1
Iteration 19400: Loss = -9822.765459757791
2
Iteration 19500: Loss = -9822.764785961632
Iteration 19600: Loss = -9822.767662431464
1
Iteration 19700: Loss = -9822.764811228983
Iteration 19800: Loss = -9822.794542016702
1
Iteration 19900: Loss = -9822.764771530487
pi: tensor([[1.0000e+00, 2.1700e-07],
        [1.7474e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0278, 0.9722], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0646, 0.1810],
         [0.6836, 0.1327]],

        [[0.5745, 0.1773],
         [0.6743, 0.6686]],

        [[0.6931, 0.2120],
         [0.6921, 0.5544]],

        [[0.7070, 0.1610],
         [0.5078, 0.6778]],

        [[0.6090, 0.1573],
         [0.6295, 0.6852]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.0041478895259491316
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: -0.004294123202807246
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
Global Adjusted Rand Index: 0.0021662910533685594
Average Adjusted Rand Index: 0.00025148989365067194
9998.008397861695
[-0.001579177405897713, 0.0021662910533685594] [-0.002274891251268548, 0.00025148989365067194] [9823.224815611247, 9822.799741132023]
-------------------------------------
This iteration is 97
True Objective function: Loss = -10076.878884478168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24097.74318431674
Iteration 100: Loss = -9968.07470427911
Iteration 200: Loss = -9966.150881110616
Iteration 300: Loss = -9965.400817604264
Iteration 400: Loss = -9965.065355217415
Iteration 500: Loss = -9964.900548024463
Iteration 600: Loss = -9964.809694482656
Iteration 700: Loss = -9964.7532992567
Iteration 800: Loss = -9964.715333890235
Iteration 900: Loss = -9964.6881950578
Iteration 1000: Loss = -9964.66783269029
Iteration 1100: Loss = -9964.651933499761
Iteration 1200: Loss = -9964.639008091195
Iteration 1300: Loss = -9964.62820767907
Iteration 1400: Loss = -9964.618901411697
Iteration 1500: Loss = -9964.610564451057
Iteration 1600: Loss = -9964.602984350233
Iteration 1700: Loss = -9964.595699511414
Iteration 1800: Loss = -9964.588681808264
Iteration 1900: Loss = -9964.581626549936
Iteration 2000: Loss = -9964.57464481857
Iteration 2100: Loss = -9964.567733358865
Iteration 2200: Loss = -9964.561239756733
Iteration 2300: Loss = -9964.555554278812
Iteration 2400: Loss = -9964.55091314114
Iteration 2500: Loss = -9964.547169752086
Iteration 2600: Loss = -9964.54403344571
Iteration 2700: Loss = -9964.541267226665
Iteration 2800: Loss = -9964.538713966233
Iteration 2900: Loss = -9964.536269608425
Iteration 3000: Loss = -9964.533820530163
Iteration 3100: Loss = -9964.531353870756
Iteration 3200: Loss = -9964.528693820705
Iteration 3300: Loss = -9964.525833544232
Iteration 3400: Loss = -9964.522578888265
Iteration 3500: Loss = -9964.51885660828
Iteration 3600: Loss = -9964.514256077537
Iteration 3700: Loss = -9964.508486740502
Iteration 3800: Loss = -9964.500814125464
Iteration 3900: Loss = -9964.490358834339
Iteration 4000: Loss = -9964.475936114259
Iteration 4100: Loss = -9964.456337152465
Iteration 4200: Loss = -9964.431951488845
Iteration 4300: Loss = -9964.404606751208
Iteration 4400: Loss = -9964.376014387044
Iteration 4500: Loss = -9964.346396148565
Iteration 4600: Loss = -9964.315314351821
Iteration 4700: Loss = -9964.283259020836
Iteration 4800: Loss = -9964.252248491313
Iteration 4900: Loss = -9964.225109788174
Iteration 5000: Loss = -9964.20403427266
Iteration 5100: Loss = -9964.18967934563
Iteration 5200: Loss = -9964.180889470079
Iteration 5300: Loss = -9964.175918427769
Iteration 5400: Loss = -9964.172865586235
Iteration 5500: Loss = -9964.170963779517
Iteration 5600: Loss = -9964.169707588057
Iteration 5700: Loss = -9964.168568884468
Iteration 5800: Loss = -9964.167718893692
Iteration 5900: Loss = -9964.167597080183
Iteration 6000: Loss = -9964.166348985615
Iteration 6100: Loss = -9964.165719566108
Iteration 6200: Loss = -9964.1652540324
Iteration 6300: Loss = -9964.164669532807
Iteration 6400: Loss = -9964.164238642396
Iteration 6500: Loss = -9964.163835685551
Iteration 6600: Loss = -9964.1634522913
Iteration 6700: Loss = -9964.163104983934
Iteration 6800: Loss = -9964.16273836299
Iteration 6900: Loss = -9964.162418446986
Iteration 7000: Loss = -9964.162514900032
Iteration 7100: Loss = -9964.161863033398
Iteration 7200: Loss = -9964.161563998488
Iteration 7300: Loss = -9964.161937385696
1
Iteration 7400: Loss = -9964.161078375797
Iteration 7500: Loss = -9964.160852826106
Iteration 7600: Loss = -9964.160652649101
Iteration 7700: Loss = -9964.16044040961
Iteration 7800: Loss = -9964.16757784994
1
Iteration 7900: Loss = -9964.160048883105
Iteration 8000: Loss = -9964.159861462309
Iteration 8100: Loss = -9964.15973491736
Iteration 8200: Loss = -9964.159500061964
Iteration 8300: Loss = -9964.159450432267
Iteration 8400: Loss = -9964.159230606398
Iteration 8500: Loss = -9964.16156867828
1
Iteration 8600: Loss = -9964.158952344133
Iteration 8700: Loss = -9964.158871439999
Iteration 8800: Loss = -9964.158782475535
Iteration 8900: Loss = -9964.15856075484
Iteration 9000: Loss = -9964.158649391951
Iteration 9100: Loss = -9964.158375234456
Iteration 9200: Loss = -9964.158266646706
Iteration 9300: Loss = -9964.158178362446
Iteration 9400: Loss = -9964.158094912635
Iteration 9500: Loss = -9964.157998446748
Iteration 9600: Loss = -9964.157896228151
Iteration 9700: Loss = -9964.158075353918
1
Iteration 9800: Loss = -9964.157762444074
Iteration 9900: Loss = -9964.15764624986
Iteration 10000: Loss = -9964.1576325615
Iteration 10100: Loss = -9964.1575632557
Iteration 10200: Loss = -9964.1575096012
Iteration 10300: Loss = -9964.157544817088
Iteration 10400: Loss = -9964.17697678082
1
Iteration 10500: Loss = -9964.16212814664
2
Iteration 10600: Loss = -9964.170403529854
3
Iteration 10700: Loss = -9964.15728593714
Iteration 10800: Loss = -9964.157582971065
1
Iteration 10900: Loss = -9964.15762756431
2
Iteration 11000: Loss = -9964.170129525672
3
Iteration 11100: Loss = -9964.157067763166
Iteration 11200: Loss = -9964.158552754738
1
Iteration 11300: Loss = -9964.157241965087
2
Iteration 11400: Loss = -9964.157220645993
3
Iteration 11500: Loss = -9964.157038774782
Iteration 11600: Loss = -9964.159770510101
1
Iteration 11700: Loss = -9964.156884189913
Iteration 11800: Loss = -9964.157212980059
1
Iteration 11900: Loss = -9964.160066196131
2
Iteration 12000: Loss = -9964.156835425618
Iteration 12100: Loss = -9964.158428332486
1
Iteration 12200: Loss = -9964.157457901601
2
Iteration 12300: Loss = -9964.200431700068
3
Iteration 12400: Loss = -9964.156765875954
Iteration 12500: Loss = -9964.157267137767
1
Iteration 12600: Loss = -9964.156730439414
Iteration 12700: Loss = -9964.156652159641
Iteration 12800: Loss = -9964.164308991674
1
Iteration 12900: Loss = -9964.156621701306
Iteration 13000: Loss = -9964.156609290358
Iteration 13100: Loss = -9964.170420925742
1
Iteration 13200: Loss = -9964.156602601393
Iteration 13300: Loss = -9964.167342676381
1
Iteration 13400: Loss = -9964.156576454556
Iteration 13500: Loss = -9964.156695531754
1
Iteration 13600: Loss = -9964.156631712522
Iteration 13700: Loss = -9964.160417105742
1
Iteration 13800: Loss = -9964.15938303885
2
Iteration 13900: Loss = -9964.252115996027
3
Iteration 14000: Loss = -9964.156496394724
Iteration 14100: Loss = -9964.156672670182
1
Iteration 14200: Loss = -9964.156556337435
Iteration 14300: Loss = -9964.158866416514
1
Iteration 14400: Loss = -9964.156524935997
Iteration 14500: Loss = -9964.162202635534
1
Iteration 14600: Loss = -9964.15682041346
2
Iteration 14700: Loss = -9964.156513085787
Iteration 14800: Loss = -9964.288757889652
1
Iteration 14900: Loss = -9964.156449572112
Iteration 15000: Loss = -9964.15653141597
Iteration 15100: Loss = -9964.156551475113
Iteration 15200: Loss = -9964.156558198532
Iteration 15300: Loss = -9964.156491168169
Iteration 15400: Loss = -9964.156480062795
Iteration 15500: Loss = -9964.36418960736
1
Iteration 15600: Loss = -9964.15640558741
Iteration 15700: Loss = -9964.159521296273
1
Iteration 15800: Loss = -9964.18633986041
2
Iteration 15900: Loss = -9964.157178713323
3
Iteration 16000: Loss = -9964.156698047766
4
Iteration 16100: Loss = -9964.199234217165
5
Iteration 16200: Loss = -9964.156432493588
Iteration 16300: Loss = -9964.156779292944
1
Iteration 16400: Loss = -9964.259408740783
2
Iteration 16500: Loss = -9964.156408397757
Iteration 16600: Loss = -9964.25616739419
1
Iteration 16700: Loss = -9964.15640020174
Iteration 16800: Loss = -9964.181072342348
1
Iteration 16900: Loss = -9964.156399380194
Iteration 17000: Loss = -9964.156365119337
Iteration 17100: Loss = -9964.156599447688
1
Iteration 17200: Loss = -9964.156478386867
2
Iteration 17300: Loss = -9964.182268533896
3
Iteration 17400: Loss = -9964.156424882793
Iteration 17500: Loss = -9964.157100765267
1
Iteration 17600: Loss = -9964.159376384954
2
Iteration 17700: Loss = -9964.158318424805
3
Iteration 17800: Loss = -9964.175616505701
4
Iteration 17900: Loss = -9964.156458549942
Iteration 18000: Loss = -9964.158567960882
1
Iteration 18100: Loss = -9964.168211205211
2
Iteration 18200: Loss = -9964.223023620916
3
Iteration 18300: Loss = -9964.159073544823
4
Iteration 18400: Loss = -9964.158528607752
5
Iteration 18500: Loss = -9964.156774631108
6
Iteration 18600: Loss = -9964.156429122544
Iteration 18700: Loss = -9964.15694517125
1
Iteration 18800: Loss = -9964.291511042213
2
Iteration 18900: Loss = -9964.15639461286
Iteration 19000: Loss = -9964.156632447697
1
Iteration 19100: Loss = -9964.156465772987
Iteration 19200: Loss = -9964.156389856862
Iteration 19300: Loss = -9964.15636439936
Iteration 19400: Loss = -9964.156598949565
1
Iteration 19500: Loss = -9964.156480175374
2
Iteration 19600: Loss = -9964.158963223299
3
Iteration 19700: Loss = -9964.161756675709
4
Iteration 19800: Loss = -9964.15691540269
5
Iteration 19900: Loss = -9964.156387515128
pi: tensor([[9.6397e-01, 3.6030e-02],
        [9.9999e-01, 1.2882e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9927, 0.0073], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1395, 0.1848],
         [0.5207, 0.1367]],

        [[0.6132, 0.1466],
         [0.5365, 0.5149]],

        [[0.6015, 0.0916],
         [0.6824, 0.6689]],

        [[0.7002, 0.1526],
         [0.6161, 0.5791]],

        [[0.6362, 0.1384],
         [0.6664, 0.5651]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25879.402826434078
Iteration 100: Loss = -9965.78851785581
Iteration 200: Loss = -9965.099370348918
Iteration 300: Loss = -9964.886642328122
Iteration 400: Loss = -9964.778471076454
Iteration 500: Loss = -9964.71605386937
Iteration 600: Loss = -9964.676630195954
Iteration 700: Loss = -9964.64992192494
Iteration 800: Loss = -9964.630789592251
Iteration 900: Loss = -9964.61650439658
Iteration 1000: Loss = -9964.605506291162
Iteration 1100: Loss = -9964.596893880796
Iteration 1200: Loss = -9964.589922907224
Iteration 1300: Loss = -9964.58436878855
Iteration 1400: Loss = -9964.579820198212
Iteration 1500: Loss = -9964.576118979847
Iteration 1600: Loss = -9964.573036417234
Iteration 1700: Loss = -9964.570443086244
Iteration 1800: Loss = -9964.568234229539
Iteration 1900: Loss = -9964.566327652132
Iteration 2000: Loss = -9964.564731012484
Iteration 2100: Loss = -9964.563321261083
Iteration 2200: Loss = -9964.562076602318
Iteration 2300: Loss = -9964.5609101938
Iteration 2400: Loss = -9964.559898367967
Iteration 2500: Loss = -9964.5589353965
Iteration 2600: Loss = -9964.558063397262
Iteration 2700: Loss = -9964.557232677898
Iteration 2800: Loss = -9964.556461837134
Iteration 2900: Loss = -9964.555690703675
Iteration 3000: Loss = -9964.554977057274
Iteration 3100: Loss = -9964.55424431074
Iteration 3200: Loss = -9964.55351326728
Iteration 3300: Loss = -9964.552839527694
Iteration 3400: Loss = -9964.552115273083
Iteration 3500: Loss = -9964.55143850342
Iteration 3600: Loss = -9964.550761596125
Iteration 3700: Loss = -9964.549987131319
Iteration 3800: Loss = -9964.549286061687
Iteration 3900: Loss = -9964.548553339906
Iteration 4000: Loss = -9964.547792187117
Iteration 4100: Loss = -9964.547025139336
Iteration 4200: Loss = -9964.546247257678
Iteration 4300: Loss = -9964.545486574885
Iteration 4400: Loss = -9964.544702437684
Iteration 4500: Loss = -9964.54390034738
Iteration 4600: Loss = -9964.543101019588
Iteration 4700: Loss = -9964.542333203688
Iteration 4800: Loss = -9964.541609412428
Iteration 4900: Loss = -9964.540902661489
Iteration 5000: Loss = -9964.540244314023
Iteration 5100: Loss = -9964.539630019615
Iteration 5200: Loss = -9964.539058518241
Iteration 5300: Loss = -9964.538559772187
Iteration 5400: Loss = -9964.538104678544
Iteration 5500: Loss = -9964.537776098743
Iteration 5600: Loss = -9964.537429424472
Iteration 5700: Loss = -9964.537172037124
Iteration 5800: Loss = -9964.536967471437
Iteration 5900: Loss = -9964.536830453095
Iteration 6000: Loss = -9964.536708799691
Iteration 6100: Loss = -9964.536631854326
Iteration 6200: Loss = -9964.53654825037
Iteration 6300: Loss = -9964.53651697713
Iteration 6400: Loss = -9964.536485379114
Iteration 6500: Loss = -9964.53642773561
Iteration 6600: Loss = -9964.5364254203
Iteration 6700: Loss = -9964.536390594145
Iteration 6800: Loss = -9964.536359294601
Iteration 6900: Loss = -9964.536358679086
Iteration 7000: Loss = -9964.536362783401
Iteration 7100: Loss = -9964.536337488404
Iteration 7200: Loss = -9964.53631301217
Iteration 7300: Loss = -9964.536318897792
Iteration 7400: Loss = -9964.536291254597
Iteration 7500: Loss = -9964.536258609756
Iteration 7600: Loss = -9964.53624461366
Iteration 7700: Loss = -9964.536243298406
Iteration 7800: Loss = -9964.536226063447
Iteration 7900: Loss = -9964.536178501095
Iteration 8000: Loss = -9964.536162221455
Iteration 8100: Loss = -9964.536101649872
Iteration 8200: Loss = -9964.536052978365
Iteration 8300: Loss = -9964.536005934842
Iteration 8400: Loss = -9964.53590970968
Iteration 8500: Loss = -9964.535821966014
Iteration 8600: Loss = -9964.535651254253
Iteration 8700: Loss = -9964.535398915052
Iteration 8800: Loss = -9964.534963443275
Iteration 8900: Loss = -9964.544667020964
1
Iteration 9000: Loss = -9964.530576310775
Iteration 9100: Loss = -9964.39092396728
Iteration 9200: Loss = -9964.162500322815
Iteration 9300: Loss = -9964.281512361247
1
Iteration 9400: Loss = -9964.15818685097
Iteration 9500: Loss = -9964.158320335408
1
Iteration 9600: Loss = -9964.157533832702
Iteration 9700: Loss = -9964.157451579473
Iteration 9800: Loss = -9964.157215131661
Iteration 9900: Loss = -9964.172612233175
1
Iteration 10000: Loss = -9964.156992798524
Iteration 10100: Loss = -9964.156928444185
Iteration 10200: Loss = -9964.15740703564
1
Iteration 10300: Loss = -9964.157133406155
2
Iteration 10400: Loss = -9964.166598218411
3
Iteration 10500: Loss = -9964.156759714264
Iteration 10600: Loss = -9964.15692498848
1
Iteration 10700: Loss = -9964.156672748135
Iteration 10800: Loss = -9964.15666099446
Iteration 10900: Loss = -9964.156655574641
Iteration 11000: Loss = -9964.166170575332
1
Iteration 11100: Loss = -9964.156588437878
Iteration 11200: Loss = -9964.158073200186
1
Iteration 11300: Loss = -9964.156549663576
Iteration 11400: Loss = -9964.157477804372
1
Iteration 11500: Loss = -9964.156516751133
Iteration 11600: Loss = -9964.159330647526
1
Iteration 11700: Loss = -9964.156495868425
Iteration 11800: Loss = -9964.157725183315
1
Iteration 11900: Loss = -9964.205766433866
2
Iteration 12000: Loss = -9964.156503145598
Iteration 12100: Loss = -9964.156480305855
Iteration 12200: Loss = -9964.15781961178
1
Iteration 12300: Loss = -9964.163317078528
2
Iteration 12400: Loss = -9964.156525212811
Iteration 12500: Loss = -9964.173308650927
1
Iteration 12600: Loss = -9964.156441449548
Iteration 12700: Loss = -9964.1732437316
1
Iteration 12800: Loss = -9964.156429220568
Iteration 12900: Loss = -9964.156439798926
Iteration 13000: Loss = -9964.160228833685
1
Iteration 13100: Loss = -9964.15641548186
Iteration 13200: Loss = -9964.157398033049
1
Iteration 13300: Loss = -9964.156496788535
Iteration 13400: Loss = -9964.156387625073
Iteration 13500: Loss = -9964.159303820968
1
Iteration 13600: Loss = -9964.156405602362
Iteration 13700: Loss = -9964.156461600904
Iteration 13800: Loss = -9964.156418378605
Iteration 13900: Loss = -9964.15639379249
Iteration 14000: Loss = -9964.156788360071
1
Iteration 14100: Loss = -9964.156413642324
Iteration 14200: Loss = -9964.15640598654
Iteration 14300: Loss = -9964.460395549866
1
Iteration 14400: Loss = -9964.156388373696
Iteration 14500: Loss = -9964.15638513484
Iteration 14600: Loss = -9964.235948384257
1
Iteration 14700: Loss = -9964.156388990048
Iteration 14800: Loss = -9964.15634585155
Iteration 14900: Loss = -9964.15998330025
1
Iteration 15000: Loss = -9964.15637608417
Iteration 15100: Loss = -9964.156393669069
Iteration 15200: Loss = -9964.156437847267
Iteration 15300: Loss = -9964.160776925894
1
Iteration 15400: Loss = -9964.180106528462
2
Iteration 15500: Loss = -9964.156393780884
Iteration 15600: Loss = -9964.156376736251
Iteration 15700: Loss = -9964.156664004957
1
Iteration 15800: Loss = -9964.16597127291
2
Iteration 15900: Loss = -9964.157012070209
3
Iteration 16000: Loss = -9964.156418028479
Iteration 16100: Loss = -9964.156535436614
1
Iteration 16200: Loss = -9964.156374512155
Iteration 16300: Loss = -9964.195454269611
1
Iteration 16400: Loss = -9964.156386628814
Iteration 16500: Loss = -9964.160743910332
1
Iteration 16600: Loss = -9964.15701178922
2
Iteration 16700: Loss = -9964.15639048699
Iteration 16800: Loss = -9964.282126323049
1
Iteration 16900: Loss = -9964.15672483664
2
Iteration 17000: Loss = -9964.1569238315
3
Iteration 17100: Loss = -9964.160578970435
4
Iteration 17200: Loss = -9964.156327157278
Iteration 17300: Loss = -9964.156675334714
1
Iteration 17400: Loss = -9964.156518536196
2
Iteration 17500: Loss = -9964.157211281214
3
Iteration 17600: Loss = -9964.156600097076
4
Iteration 17700: Loss = -9964.157940974055
5
Iteration 17800: Loss = -9964.249937660154
6
Iteration 17900: Loss = -9964.156402810078
Iteration 18000: Loss = -9964.156525143044
1
Iteration 18100: Loss = -9964.184276290438
2
Iteration 18200: Loss = -9964.156356299105
Iteration 18300: Loss = -9964.21994946708
1
Iteration 18400: Loss = -9964.162342046737
2
Iteration 18500: Loss = -9964.282655229552
3
Iteration 18600: Loss = -9964.156698581231
4
Iteration 18700: Loss = -9964.158580487527
5
Iteration 18800: Loss = -9964.156395827109
Iteration 18900: Loss = -9964.164158612211
1
Iteration 19000: Loss = -9964.157311048992
2
Iteration 19100: Loss = -9964.176476191486
3
Iteration 19200: Loss = -9964.158679388313
4
Iteration 19300: Loss = -9964.172625110308
5
Iteration 19400: Loss = -9964.156584278304
6
Iteration 19500: Loss = -9964.156803732125
7
Iteration 19600: Loss = -9964.156421885871
Iteration 19700: Loss = -9964.157520857234
1
Iteration 19800: Loss = -9964.162528276844
2
Iteration 19900: Loss = -9964.157052962257
3
pi: tensor([[9.6386e-01, 3.6145e-02],
        [1.0000e+00, 2.9311e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9927, 0.0073], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1398, 0.1843],
         [0.5284, 0.1364]],

        [[0.7130, 0.1462],
         [0.7154, 0.5959]],

        [[0.5067, 0.0914],
         [0.6962, 0.6241]],

        [[0.6812, 0.1522],
         [0.6246, 0.6492]],

        [[0.5307, 0.1380],
         [0.5197, 0.6997]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
10076.878884478168
[0.0, 0.0] [0.0, 0.0] [9964.18365089658, 9964.218360787472]
-------------------------------------
This iteration is 98
True Objective function: Loss = -10180.664134765897
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21175.76505230459
Iteration 100: Loss = -10064.162375728969
Iteration 200: Loss = -10063.08393508979
Iteration 300: Loss = -10062.780601255294
Iteration 400: Loss = -10062.602648392081
Iteration 500: Loss = -10062.396241347542
Iteration 600: Loss = -10061.664472304625
Iteration 700: Loss = -10060.391429384581
Iteration 800: Loss = -10059.881319097185
Iteration 900: Loss = -10059.175251357588
Iteration 1000: Loss = -10057.736385525555
Iteration 1100: Loss = -10055.789253516152
Iteration 1200: Loss = -10054.02692196086
Iteration 1300: Loss = -10051.767869303683
Iteration 1400: Loss = -10049.946474054126
Iteration 1500: Loss = -10048.322912737136
Iteration 1600: Loss = -10045.365048504975
Iteration 1700: Loss = -10041.431575513474
Iteration 1800: Loss = -10040.860908018232
Iteration 1900: Loss = -10040.629513840939
Iteration 2000: Loss = -10040.183226021974
Iteration 2100: Loss = -10040.02278857129
Iteration 2200: Loss = -10039.984484200686
Iteration 2300: Loss = -10039.893442168475
Iteration 2400: Loss = -10039.875395637542
Iteration 2500: Loss = -10039.857599323477
Iteration 2600: Loss = -10039.856229879886
Iteration 2700: Loss = -10039.855539449005
Iteration 2800: Loss = -10039.855262215353
Iteration 2900: Loss = -10039.855095701661
Iteration 3000: Loss = -10039.865023734194
1
Iteration 3100: Loss = -10039.85478742914
Iteration 3200: Loss = -10039.854690126558
Iteration 3300: Loss = -10039.854614411546
Iteration 3400: Loss = -10039.854560277388
Iteration 3500: Loss = -10039.854487331671
Iteration 3600: Loss = -10039.855159654306
1
Iteration 3700: Loss = -10039.854362947173
Iteration 3800: Loss = -10039.85438057863
Iteration 3900: Loss = -10039.854370388615
Iteration 4000: Loss = -10039.854471669176
1
Iteration 4100: Loss = -10039.854350736412
Iteration 4200: Loss = -10039.85413678532
Iteration 4300: Loss = -10039.863732674381
1
Iteration 4400: Loss = -10039.85397638211
Iteration 4500: Loss = -10039.853993539828
Iteration 4600: Loss = -10039.853702898401
Iteration 4700: Loss = -10039.854005787824
1
Iteration 4800: Loss = -10039.853613849624
Iteration 4900: Loss = -10039.854471429246
1
Iteration 5000: Loss = -10039.853646263135
Iteration 5100: Loss = -10039.854661391519
1
Iteration 5200: Loss = -10039.853621003906
Iteration 5300: Loss = -10039.854152326123
1
Iteration 5400: Loss = -10039.853585540917
Iteration 5500: Loss = -10039.855382863685
1
Iteration 5600: Loss = -10039.853579743802
Iteration 5700: Loss = -10039.85778036282
1
Iteration 5800: Loss = -10039.853554341855
Iteration 5900: Loss = -10039.853577103333
Iteration 6000: Loss = -10039.853621762337
Iteration 6100: Loss = -10039.853560027965
Iteration 6200: Loss = -10039.853829870748
1
Iteration 6300: Loss = -10039.853572292557
Iteration 6400: Loss = -10039.853591767738
Iteration 6500: Loss = -10039.85355752726
Iteration 6600: Loss = -10039.853545480657
Iteration 6700: Loss = -10039.85537013165
1
Iteration 6800: Loss = -10039.853549242625
Iteration 6900: Loss = -10039.853551142056
Iteration 7000: Loss = -10039.853559753758
Iteration 7100: Loss = -10039.853555162335
Iteration 7200: Loss = -10039.858077604553
1
Iteration 7300: Loss = -10039.853539657293
Iteration 7400: Loss = -10039.853564462428
Iteration 7500: Loss = -10039.853654588294
Iteration 7600: Loss = -10039.853554020643
Iteration 7700: Loss = -10039.854385013587
1
Iteration 7800: Loss = -10039.853545627768
Iteration 7900: Loss = -10039.85355410214
Iteration 8000: Loss = -10039.85356324762
Iteration 8100: Loss = -10039.855154588651
1
Iteration 8200: Loss = -10039.853654779017
Iteration 8300: Loss = -10039.853573412418
Iteration 8400: Loss = -10039.853600516011
Iteration 8500: Loss = -10039.853789385454
1
Iteration 8600: Loss = -10039.853619208974
Iteration 8700: Loss = -10039.853668846617
Iteration 8800: Loss = -10039.854209023113
1
Iteration 8900: Loss = -10039.855330874705
2
Iteration 9000: Loss = -10039.90161778771
3
Iteration 9100: Loss = -10039.898259640611
4
Iteration 9200: Loss = -10039.87469416674
5
Iteration 9300: Loss = -10039.854446568339
6
Iteration 9400: Loss = -10039.854183738247
7
Iteration 9500: Loss = -10039.85781944405
8
Iteration 9600: Loss = -10039.853756764916
Iteration 9700: Loss = -10039.858712124262
1
Iteration 9800: Loss = -10039.928618317677
2
Iteration 9900: Loss = -10039.854380903058
3
Iteration 10000: Loss = -10039.968431330126
4
Iteration 10100: Loss = -10039.85354284875
Iteration 10200: Loss = -10039.868135257955
1
Iteration 10300: Loss = -10039.854895012018
2
Iteration 10400: Loss = -10039.853838463841
3
Iteration 10500: Loss = -10039.85999870389
4
Iteration 10600: Loss = -10039.85445062081
5
Iteration 10700: Loss = -10039.853619693926
Iteration 10800: Loss = -10039.85477400046
1
Iteration 10900: Loss = -10039.854094231023
2
Iteration 11000: Loss = -10039.8537319245
3
Iteration 11100: Loss = -10039.857686156263
4
Iteration 11200: Loss = -10039.87571993231
5
Iteration 11300: Loss = -10039.853580655805
Iteration 11400: Loss = -10039.853680182938
Iteration 11500: Loss = -10039.905557351387
1
Iteration 11600: Loss = -10039.853553217492
Iteration 11700: Loss = -10039.860642141675
1
Iteration 11800: Loss = -10039.85357333795
Iteration 11900: Loss = -10039.86507916183
1
Iteration 12000: Loss = -10039.857866341114
2
Iteration 12100: Loss = -10039.864532793445
3
Iteration 12200: Loss = -10039.856677726408
4
Iteration 12300: Loss = -10039.853525276609
Iteration 12400: Loss = -10039.855332510211
1
Iteration 12500: Loss = -10039.883055634364
2
Iteration 12600: Loss = -10039.853769302907
3
Iteration 12700: Loss = -10039.853593265389
Iteration 12800: Loss = -10039.868433514393
1
Iteration 12900: Loss = -10039.982817496353
2
Iteration 13000: Loss = -10039.85357761193
Iteration 13100: Loss = -10039.853850844398
1
Iteration 13200: Loss = -10039.945752083026
2
Iteration 13300: Loss = -10039.853570793379
Iteration 13400: Loss = -10039.85487303256
1
Iteration 13500: Loss = -10039.862000680625
2
Iteration 13600: Loss = -10039.853573493305
Iteration 13700: Loss = -10039.853629660802
Iteration 13800: Loss = -10039.853523748518
Iteration 13900: Loss = -10039.853639263629
1
Iteration 14000: Loss = -10039.85470820598
2
Iteration 14100: Loss = -10039.853555719772
Iteration 14200: Loss = -10039.854198361692
1
Iteration 14300: Loss = -10039.853567361082
Iteration 14400: Loss = -10039.853966678062
1
Iteration 14500: Loss = -10039.85384935762
2
Iteration 14600: Loss = -10039.855314669428
3
Iteration 14700: Loss = -10039.853582572388
Iteration 14800: Loss = -10039.854858482891
1
Iteration 14900: Loss = -10039.854147016698
2
Iteration 15000: Loss = -10039.855796261358
3
Iteration 15100: Loss = -10039.853577820248
Iteration 15200: Loss = -10039.878654563514
1
Iteration 15300: Loss = -10039.8544208601
2
Iteration 15400: Loss = -10039.85359048523
Iteration 15500: Loss = -10040.125302342653
1
Iteration 15600: Loss = -10039.853571435742
Iteration 15700: Loss = -10039.969365380537
1
Iteration 15800: Loss = -10039.853587367941
Iteration 15900: Loss = -10039.854298951284
1
Iteration 16000: Loss = -10039.853550102289
Iteration 16100: Loss = -10039.85384326754
1
Iteration 16200: Loss = -10039.853563921042
Iteration 16300: Loss = -10039.856185124572
1
Iteration 16400: Loss = -10039.858589142794
2
Iteration 16500: Loss = -10039.944929712065
3
Iteration 16600: Loss = -10039.8535530853
Iteration 16700: Loss = -10039.856434635783
1
Iteration 16800: Loss = -10039.865801621707
2
Iteration 16900: Loss = -10039.85356019276
Iteration 17000: Loss = -10039.854779409672
1
Iteration 17100: Loss = -10039.853569254823
Iteration 17200: Loss = -10039.854062417431
1
Iteration 17300: Loss = -10039.85398360519
2
Iteration 17400: Loss = -10039.854413953204
3
Iteration 17500: Loss = -10039.85369509705
4
Iteration 17600: Loss = -10039.853793446717
5
Iteration 17700: Loss = -10039.856895536357
6
Iteration 17800: Loss = -10039.853538735795
Iteration 17900: Loss = -10039.875588683972
1
Iteration 18000: Loss = -10039.853550455546
Iteration 18100: Loss = -10039.853533788899
Iteration 18200: Loss = -10039.853664474103
1
Iteration 18300: Loss = -10039.853548657738
Iteration 18400: Loss = -10039.919722860266
1
Iteration 18500: Loss = -10039.854157285428
2
Iteration 18600: Loss = -10039.853976498789
3
Iteration 18700: Loss = -10039.85722342934
4
Iteration 18800: Loss = -10039.85358504653
Iteration 18900: Loss = -10039.853787151693
1
Iteration 19000: Loss = -10039.857884161924
2
Iteration 19100: Loss = -10039.935244470433
3
Iteration 19200: Loss = -10039.907569419587
4
Iteration 19300: Loss = -10039.853588613878
Iteration 19400: Loss = -10039.85363252244
Iteration 19500: Loss = -10039.868713719181
1
Iteration 19600: Loss = -10039.853555500524
Iteration 19700: Loss = -10039.85860700463
1
Iteration 19800: Loss = -10039.853540342532
Iteration 19900: Loss = -10039.858963731984
1
pi: tensor([[0.7454, 0.2546],
        [0.2391, 0.7609]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8037, 0.1963], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1353, 0.1557],
         [0.6365, 0.2335]],

        [[0.6833, 0.1163],
         [0.5022, 0.7072]],

        [[0.6881, 0.1087],
         [0.6903, 0.6971]],

        [[0.6566, 0.0951],
         [0.5241, 0.5782]],

        [[0.5404, 0.1035],
         [0.6947, 0.6703]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 77
Adjusted Rand Index: 0.28400682731336985
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 77
Adjusted Rand Index: 0.28432106814373387
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 77
Adjusted Rand Index: 0.28444444444444444
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026076198471212
Global Adjusted Rand Index: 0.25653932699040116
Average Adjusted Rand Index: 0.3103487192224611
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24296.476050429123
Iteration 100: Loss = -10064.72834856421
Iteration 200: Loss = -10062.80517137997
Iteration 300: Loss = -10061.364021433756
Iteration 400: Loss = -10060.490442501583
Iteration 500: Loss = -10059.89854545265
Iteration 600: Loss = -10059.540970636535
Iteration 700: Loss = -10059.230688582582
Iteration 800: Loss = -10059.076381897232
Iteration 900: Loss = -10059.002156915407
Iteration 1000: Loss = -10058.946732168735
Iteration 1100: Loss = -10058.896899394264
Iteration 1200: Loss = -10058.859337197491
Iteration 1300: Loss = -10058.830381719808
Iteration 1400: Loss = -10058.80640548588
Iteration 1500: Loss = -10058.787803696778
Iteration 1600: Loss = -10058.773176066072
Iteration 1700: Loss = -10058.761064816632
Iteration 1800: Loss = -10058.75084853744
Iteration 1900: Loss = -10058.742215155187
Iteration 2000: Loss = -10058.734955452164
Iteration 2100: Loss = -10058.728441828041
Iteration 2200: Loss = -10058.721397097692
Iteration 2300: Loss = -10058.712201699753
Iteration 2400: Loss = -10058.703275536509
Iteration 2500: Loss = -10058.692341051537
Iteration 2600: Loss = -10058.674736168423
Iteration 2700: Loss = -10058.630863087372
Iteration 2800: Loss = -10058.167372985312
Iteration 2900: Loss = -10057.062064647034
Iteration 3000: Loss = -10056.857174946306
Iteration 3100: Loss = -10056.804552498457
Iteration 3200: Loss = -10056.77691907948
Iteration 3300: Loss = -10056.759031029964
Iteration 3400: Loss = -10056.746537392612
Iteration 3500: Loss = -10056.737379441378
Iteration 3600: Loss = -10056.730550153177
Iteration 3700: Loss = -10056.725268971853
Iteration 3800: Loss = -10056.721186910741
Iteration 3900: Loss = -10056.71798067477
Iteration 4000: Loss = -10056.715370396945
Iteration 4100: Loss = -10056.713274508502
Iteration 4200: Loss = -10056.711562508397
Iteration 4300: Loss = -10056.710137523738
Iteration 4400: Loss = -10056.708965978853
Iteration 4500: Loss = -10056.707935460807
Iteration 4600: Loss = -10056.707100079082
Iteration 4700: Loss = -10056.706350440254
Iteration 4800: Loss = -10056.70567395649
Iteration 4900: Loss = -10056.705091707243
Iteration 5000: Loss = -10056.7046044091
Iteration 5100: Loss = -10056.704145103222
Iteration 5200: Loss = -10056.703753590997
Iteration 5300: Loss = -10056.703434372117
Iteration 5400: Loss = -10056.703077577535
Iteration 5500: Loss = -10056.702755414195
Iteration 5600: Loss = -10056.702505538002
Iteration 5700: Loss = -10056.7022488379
Iteration 5800: Loss = -10056.702009295757
Iteration 5900: Loss = -10056.701907145552
Iteration 6000: Loss = -10056.701589067225
Iteration 6100: Loss = -10056.70140583774
Iteration 6200: Loss = -10056.701248219162
Iteration 6300: Loss = -10056.701098112775
Iteration 6400: Loss = -10056.70093729387
Iteration 6500: Loss = -10056.700793829974
Iteration 6600: Loss = -10056.700685074684
Iteration 6700: Loss = -10056.700578067086
Iteration 6800: Loss = -10056.700483131779
Iteration 6900: Loss = -10056.700373979427
Iteration 7000: Loss = -10056.700668164689
1
Iteration 7100: Loss = -10056.700151977313
Iteration 7200: Loss = -10056.700075286035
Iteration 7300: Loss = -10056.700013558007
Iteration 7400: Loss = -10056.699955114582
Iteration 7500: Loss = -10056.699875263908
Iteration 7600: Loss = -10056.699802645924
Iteration 7700: Loss = -10056.70000664314
1
Iteration 7800: Loss = -10056.699677233915
Iteration 7900: Loss = -10056.699678999976
Iteration 8000: Loss = -10056.699548278864
Iteration 8100: Loss = -10056.699508797989
Iteration 8200: Loss = -10056.699480442849
Iteration 8300: Loss = -10056.699442230894
Iteration 8400: Loss = -10056.700855828503
1
Iteration 8500: Loss = -10056.699329234596
Iteration 8600: Loss = -10056.699296661185
Iteration 8700: Loss = -10056.6992731664
Iteration 8800: Loss = -10056.69932872852
Iteration 8900: Loss = -10056.699283344355
Iteration 9000: Loss = -10056.69921551138
Iteration 9100: Loss = -10056.699148708874
Iteration 9200: Loss = -10056.699408562057
1
Iteration 9300: Loss = -10056.69915458913
Iteration 9400: Loss = -10056.700111860147
1
Iteration 9500: Loss = -10056.699126429323
Iteration 9600: Loss = -10056.699160495255
Iteration 9700: Loss = -10056.699070570716
Iteration 9800: Loss = -10056.699115761012
Iteration 9900: Loss = -10056.698998154328
Iteration 10000: Loss = -10056.705427143832
1
Iteration 10100: Loss = -10056.698936417748
Iteration 10200: Loss = -10056.698969903424
Iteration 10300: Loss = -10056.698950990574
Iteration 10400: Loss = -10056.698931892324
Iteration 10500: Loss = -10056.698926766287
Iteration 10600: Loss = -10056.699644105052
1
Iteration 10700: Loss = -10056.698911324813
Iteration 10800: Loss = -10056.6989632397
Iteration 10900: Loss = -10056.698866269247
Iteration 11000: Loss = -10056.698940584529
Iteration 11100: Loss = -10056.698902873724
Iteration 11200: Loss = -10056.698848815835
Iteration 11300: Loss = -10056.69884305287
Iteration 11400: Loss = -10056.699318170338
1
Iteration 11500: Loss = -10056.700004340746
2
Iteration 11600: Loss = -10056.701296219653
3
Iteration 11700: Loss = -10056.700269798022
4
Iteration 11800: Loss = -10056.698785046761
Iteration 11900: Loss = -10056.698751885073
Iteration 12000: Loss = -10056.699010415845
1
Iteration 12100: Loss = -10056.698777037558
Iteration 12200: Loss = -10056.700289245015
1
Iteration 12300: Loss = -10056.698782379612
Iteration 12400: Loss = -10056.698764343533
Iteration 12500: Loss = -10056.722130382068
1
Iteration 12600: Loss = -10056.698752450158
Iteration 12700: Loss = -10056.698754960473
Iteration 12800: Loss = -10056.709174506963
1
Iteration 12900: Loss = -10056.698745403673
Iteration 13000: Loss = -10056.699649656995
1
Iteration 13100: Loss = -10056.707284807044
2
Iteration 13200: Loss = -10056.699360497012
3
Iteration 13300: Loss = -10056.703376442918
4
Iteration 13400: Loss = -10056.700246827744
5
Iteration 13500: Loss = -10056.698751516222
Iteration 13600: Loss = -10056.699695824087
1
Iteration 13700: Loss = -10056.699063166314
2
Iteration 13800: Loss = -10056.698724767979
Iteration 13900: Loss = -10056.69903814563
1
Iteration 14000: Loss = -10056.797343342652
2
Iteration 14100: Loss = -10056.698701264966
Iteration 14200: Loss = -10057.079302921284
1
Iteration 14300: Loss = -10056.698706322815
Iteration 14400: Loss = -10056.698713068608
Iteration 14500: Loss = -10056.699102046845
1
Iteration 14600: Loss = -10056.701277883
2
Iteration 14700: Loss = -10056.708830330237
3
Iteration 14800: Loss = -10056.698718343605
Iteration 14900: Loss = -10056.709196335629
1
Iteration 15000: Loss = -10056.69875718593
Iteration 15100: Loss = -10056.699986972524
1
Iteration 15200: Loss = -10056.698727389425
Iteration 15300: Loss = -10056.699564251463
1
Iteration 15400: Loss = -10056.698722678237
Iteration 15500: Loss = -10056.698890749132
1
Iteration 15600: Loss = -10056.70203934117
2
Iteration 15700: Loss = -10056.698713047643
Iteration 15800: Loss = -10056.76510289358
1
Iteration 15900: Loss = -10056.698715964189
Iteration 16000: Loss = -10056.702183672824
1
Iteration 16100: Loss = -10056.698784636817
Iteration 16200: Loss = -10056.69911661367
1
Iteration 16300: Loss = -10056.699824678368
2
Iteration 16400: Loss = -10056.72051298845
3
Iteration 16500: Loss = -10056.698797940293
Iteration 16600: Loss = -10056.906695110467
1
Iteration 16700: Loss = -10056.698714183794
Iteration 16800: Loss = -10056.698780485029
Iteration 16900: Loss = -10056.705557386716
1
Iteration 17000: Loss = -10056.698691247058
Iteration 17100: Loss = -10056.699014526817
1
Iteration 17200: Loss = -10056.85489687065
2
Iteration 17300: Loss = -10056.698664002997
Iteration 17400: Loss = -10056.703216060985
1
Iteration 17500: Loss = -10056.698908429267
2
Iteration 17600: Loss = -10056.698714486445
Iteration 17700: Loss = -10056.724922472476
1
Iteration 17800: Loss = -10056.704981149813
2
Iteration 17900: Loss = -10056.698740762171
Iteration 18000: Loss = -10056.701550753995
1
Iteration 18100: Loss = -10056.69868106668
Iteration 18200: Loss = -10056.69923376975
1
Iteration 18300: Loss = -10056.69868851449
Iteration 18400: Loss = -10056.698950232825
1
Iteration 18500: Loss = -10056.698737872815
Iteration 18600: Loss = -10056.699286851494
1
Iteration 18700: Loss = -10056.70265025293
2
Iteration 18800: Loss = -10056.698894498999
3
Iteration 18900: Loss = -10056.905502927293
4
Iteration 19000: Loss = -10056.699349352662
5
Iteration 19100: Loss = -10056.741772799032
6
Iteration 19200: Loss = -10056.699382456201
7
Iteration 19300: Loss = -10056.725266986332
8
Iteration 19400: Loss = -10056.698857205945
9
Iteration 19500: Loss = -10056.698888449855
10
Iteration 19600: Loss = -10056.744122490863
11
Iteration 19700: Loss = -10056.698689172359
Iteration 19800: Loss = -10056.698692573638
Iteration 19900: Loss = -10056.702449031698
1
pi: tensor([[6.7101e-01, 3.2899e-01],
        [9.0665e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0629, 0.9371], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0514, 0.1176],
         [0.6143, 0.1405]],

        [[0.5212, 0.2106],
         [0.6377, 0.6692]],

        [[0.5862, 0.2013],
         [0.5275, 0.5027]],

        [[0.5012, 0.0730],
         [0.6337, 0.7265]],

        [[0.6022, 0.0524],
         [0.5015, 0.5971]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.01717781179455718
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: -0.0010352701002318571
Average Adjusted Rand Index: 0.0010097657790142105
10180.664134765897
[0.25653932699040116, -0.0010352701002318571] [0.3103487192224611, 0.0010097657790142105] [10039.861563413846, 10056.698806996916]
-------------------------------------
This iteration is 99
True Objective function: Loss = -9954.026017034359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23434.853114532925
Iteration 100: Loss = -9834.960349178393
Iteration 200: Loss = -9833.310782564362
Iteration 300: Loss = -9832.780369675602
Iteration 400: Loss = -9832.538648759606
Iteration 500: Loss = -9832.413395676007
Iteration 600: Loss = -9832.34136371304
Iteration 700: Loss = -9832.295617196018
Iteration 800: Loss = -9832.263141037653
Iteration 900: Loss = -9832.235411234049
Iteration 1000: Loss = -9832.199652390631
Iteration 1100: Loss = -9832.072092197404
Iteration 1200: Loss = -9831.386341472367
Iteration 1300: Loss = -9831.025890680901
Iteration 1400: Loss = -9830.219418594832
Iteration 1500: Loss = -9829.480428644187
Iteration 1600: Loss = -9829.180373807845
Iteration 1700: Loss = -9829.051134475107
Iteration 1800: Loss = -9828.931087241805
Iteration 1900: Loss = -9828.820189766024
Iteration 2000: Loss = -9828.623942365972
Iteration 2100: Loss = -9828.5114121822
Iteration 2200: Loss = -9828.443732511512
Iteration 2300: Loss = -9828.307079911516
Iteration 2400: Loss = -9828.266815363839
Iteration 2500: Loss = -9828.251554775352
Iteration 2600: Loss = -9828.239661933887
Iteration 2700: Loss = -9828.22951372914
Iteration 2800: Loss = -9828.218971815855
Iteration 2900: Loss = -9828.181654772265
Iteration 3000: Loss = -9828.173254033649
Iteration 3100: Loss = -9828.159530754214
Iteration 3200: Loss = -9828.067312101677
Iteration 3300: Loss = -9828.057992158014
Iteration 3400: Loss = -9828.042306629564
Iteration 3500: Loss = -9828.034384181607
Iteration 3600: Loss = -9828.012149354852
Iteration 3700: Loss = -9827.991097809349
Iteration 3800: Loss = -9827.987940247434
Iteration 3900: Loss = -9827.985340715122
Iteration 4000: Loss = -9827.982911264626
Iteration 4100: Loss = -9827.979633700152
Iteration 4200: Loss = -9827.94938807177
Iteration 4300: Loss = -9827.878938625718
Iteration 4400: Loss = -9827.85561909365
Iteration 4500: Loss = -9827.847959475179
Iteration 4600: Loss = -9827.845082740936
Iteration 4700: Loss = -9827.843389692245
Iteration 4800: Loss = -9827.840430905704
Iteration 4900: Loss = -9827.764228728012
Iteration 5000: Loss = -9827.749876322452
Iteration 5100: Loss = -9827.748718029228
Iteration 5200: Loss = -9827.747758020798
Iteration 5300: Loss = -9827.747012184544
Iteration 5400: Loss = -9827.746176735413
Iteration 5500: Loss = -9827.74556328535
Iteration 5600: Loss = -9827.74481247935
Iteration 5700: Loss = -9827.744155366196
Iteration 5800: Loss = -9827.743566380888
Iteration 5900: Loss = -9827.742888157714
Iteration 6000: Loss = -9827.741845837541
Iteration 6100: Loss = -9827.47320232852
Iteration 6200: Loss = -9827.471714333218
Iteration 6300: Loss = -9827.470132483853
Iteration 6400: Loss = -9827.368348189095
Iteration 6500: Loss = -9827.355294416608
Iteration 6600: Loss = -9827.348421505973
Iteration 6700: Loss = -9827.312291106338
Iteration 6800: Loss = -9827.309530042403
Iteration 6900: Loss = -9827.296889108038
Iteration 7000: Loss = -9827.296415523735
Iteration 7100: Loss = -9827.29599048398
Iteration 7200: Loss = -9827.29521473081
Iteration 7300: Loss = -9827.274494626607
Iteration 7400: Loss = -9827.272462254863
Iteration 7500: Loss = -9827.262858690065
Iteration 7600: Loss = -9827.250142703226
Iteration 7700: Loss = -9827.249405664274
Iteration 7800: Loss = -9827.225577840025
Iteration 7900: Loss = -9827.201264439045
Iteration 8000: Loss = -9827.200910707734
Iteration 8100: Loss = -9827.200785165114
Iteration 8200: Loss = -9827.200594413956
Iteration 8300: Loss = -9827.20043324281
Iteration 8400: Loss = -9827.19958788678
Iteration 8500: Loss = -9827.198026506148
Iteration 8600: Loss = -9827.196798939456
Iteration 8700: Loss = -9827.18496312237
Iteration 8800: Loss = -9827.182388705172
Iteration 8900: Loss = -9827.179984266846
Iteration 9000: Loss = -9827.218521299366
1
Iteration 9100: Loss = -9827.147817808389
Iteration 9200: Loss = -9827.147755865102
Iteration 9300: Loss = -9827.139508749808
Iteration 9400: Loss = -9827.134353413296
Iteration 9500: Loss = -9827.134249953704
Iteration 9600: Loss = -9827.134331688405
Iteration 9700: Loss = -9827.134120955148
Iteration 9800: Loss = -9827.134299150357
1
Iteration 9900: Loss = -9827.133958777955
Iteration 10000: Loss = -9827.127985626526
Iteration 10100: Loss = -9827.068691137512
Iteration 10200: Loss = -9827.068557417922
Iteration 10300: Loss = -9827.068727902355
1
Iteration 10400: Loss = -9827.068484024496
Iteration 10500: Loss = -9827.068457216965
Iteration 10600: Loss = -9827.068338437562
Iteration 10700: Loss = -9827.062086208518
Iteration 10800: Loss = -9827.128277745063
1
Iteration 10900: Loss = -9827.043507950097
Iteration 11000: Loss = -9826.99856230647
Iteration 11100: Loss = -9826.998684455937
1
Iteration 11200: Loss = -9827.027076462486
2
Iteration 11300: Loss = -9826.989130516915
Iteration 11400: Loss = -9826.989533814229
1
Iteration 11500: Loss = -9826.980492384948
Iteration 11600: Loss = -9826.980461517775
Iteration 11700: Loss = -9826.980087762511
Iteration 11800: Loss = -9826.981248885368
1
Iteration 11900: Loss = -9826.980067751525
Iteration 12000: Loss = -9826.987014305214
1
Iteration 12100: Loss = -9826.941937804046
Iteration 12200: Loss = -9826.954016039306
1
Iteration 12300: Loss = -9826.94154654127
Iteration 12400: Loss = -9826.941538070181
Iteration 12500: Loss = -9826.942206446909
1
Iteration 12600: Loss = -9826.941502097636
Iteration 12700: Loss = -9826.941831163009
1
Iteration 12800: Loss = -9827.006552910238
2
Iteration 12900: Loss = -9826.941583307296
Iteration 13000: Loss = -9826.951176708832
1
Iteration 13100: Loss = -9826.921895713816
Iteration 13200: Loss = -9826.924765303045
1
Iteration 13300: Loss = -9826.926352416509
2
Iteration 13400: Loss = -9826.921816423668
Iteration 13500: Loss = -9826.911411235187
Iteration 13600: Loss = -9826.99409935214
1
Iteration 13700: Loss = -9826.901871591932
Iteration 13800: Loss = -9826.926496014747
1
Iteration 13900: Loss = -9826.901371825237
Iteration 14000: Loss = -9826.901245657806
Iteration 14100: Loss = -9827.000478065987
1
Iteration 14200: Loss = -9826.900203168401
Iteration 14300: Loss = -9826.797838629696
Iteration 14400: Loss = -9826.7974340902
Iteration 14500: Loss = -9826.80065985747
1
Iteration 14600: Loss = -9826.797388459887
Iteration 14700: Loss = -9826.797366075132
Iteration 14800: Loss = -9826.798898281228
1
Iteration 14900: Loss = -9826.797360392728
Iteration 15000: Loss = -9826.897850401409
1
Iteration 15100: Loss = -9826.771564767896
Iteration 15200: Loss = -9826.772337255725
1
Iteration 15300: Loss = -9826.780874496228
2
Iteration 15400: Loss = -9826.766161793772
Iteration 15500: Loss = -9826.766333060325
1
Iteration 15600: Loss = -9826.76332199605
Iteration 15700: Loss = -9826.761298394755
Iteration 15800: Loss = -9826.751864628302
Iteration 15900: Loss = -9826.712918082821
Iteration 16000: Loss = -9826.712589796312
Iteration 16100: Loss = -9826.731720481182
1
Iteration 16200: Loss = -9826.712516708238
Iteration 16300: Loss = -9826.714644151165
1
Iteration 16400: Loss = -9826.712525694928
Iteration 16500: Loss = -9826.728493053595
1
Iteration 16600: Loss = -9826.712792826527
2
Iteration 16700: Loss = -9826.712533240805
Iteration 16800: Loss = -9826.713034978085
1
Iteration 16900: Loss = -9826.721239673856
2
Iteration 17000: Loss = -9826.712518833894
Iteration 17100: Loss = -9826.712892407662
1
Iteration 17200: Loss = -9826.712538986954
Iteration 17300: Loss = -9826.712484877089
Iteration 17400: Loss = -9826.75374489739
1
Iteration 17500: Loss = -9826.713552306712
2
Iteration 17600: Loss = -9826.706872577839
Iteration 17700: Loss = -9826.707032314353
1
Iteration 17800: Loss = -9826.800877126154
2
Iteration 17900: Loss = -9826.700804457092
Iteration 18000: Loss = -9826.83686731252
1
Iteration 18100: Loss = -9826.700773383467
Iteration 18200: Loss = -9826.700790823123
Iteration 18300: Loss = -9826.700761877235
Iteration 18400: Loss = -9826.724498620259
1
Iteration 18500: Loss = -9826.67981820922
Iteration 18600: Loss = -9826.677930601354
Iteration 18700: Loss = -9826.679209450176
1
Iteration 18800: Loss = -9826.677861030634
Iteration 18900: Loss = -9826.554852253874
Iteration 19000: Loss = -9826.488913209907
Iteration 19100: Loss = -9826.500611245017
1
Iteration 19200: Loss = -9826.48183787666
Iteration 19300: Loss = -9826.478579129802
Iteration 19400: Loss = -9826.479843478835
1
Iteration 19500: Loss = -9826.496467093808
2
Iteration 19600: Loss = -9826.478485504014
Iteration 19700: Loss = -9826.475560384071
Iteration 19800: Loss = -9826.771311161094
1
Iteration 19900: Loss = -9826.475193303986
pi: tensor([[1.0000e+00, 2.3318e-07],
        [6.6668e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1341, 0.8659], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1852, 0.1684],
         [0.5439, 0.1286]],

        [[0.5939, 0.1744],
         [0.6569, 0.6973]],

        [[0.5765, 0.1485],
         [0.6381, 0.6404]],

        [[0.5781, 0.1357],
         [0.7184, 0.6237]],

        [[0.5934, 0.1638],
         [0.6983, 0.5651]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.021492879445994654
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.03407739186585839
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.01570657266520905
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.02954470050093279
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.06365756619563726
Global Adjusted Rand Index: 0.03711595466040971
Average Adjusted Rand Index: 0.03289582213472643
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20336.04660144731
Iteration 100: Loss = -9833.342499222024
Iteration 200: Loss = -9832.735287235075
Iteration 300: Loss = -9832.455180418752
Iteration 400: Loss = -9832.287053025299
Iteration 500: Loss = -9832.0890326367
Iteration 600: Loss = -9831.561346054832
Iteration 700: Loss = -9831.271422265647
Iteration 800: Loss = -9830.905166600636
Iteration 900: Loss = -9829.984579997066
Iteration 1000: Loss = -9829.221822553274
Iteration 1100: Loss = -9828.91521936901
Iteration 1200: Loss = -9828.547316738215
Iteration 1300: Loss = -9828.420803839348
Iteration 1400: Loss = -9828.323883804444
Iteration 1500: Loss = -9827.956907203026
Iteration 1600: Loss = -9827.75353591601
Iteration 1700: Loss = -9827.586942960392
Iteration 1800: Loss = -9827.553345385022
Iteration 1900: Loss = -9827.488488080924
Iteration 2000: Loss = -9827.471922544597
Iteration 2100: Loss = -9827.457459175135
Iteration 2200: Loss = -9827.443772202532
Iteration 2300: Loss = -9827.41150955183
Iteration 2400: Loss = -9827.366961035328
Iteration 2500: Loss = -9827.305028029445
Iteration 2600: Loss = -9827.29429274645
Iteration 2700: Loss = -9827.288087950203
Iteration 2800: Loss = -9827.27263441952
Iteration 2900: Loss = -9827.255600720195
Iteration 3000: Loss = -9827.248519473646
Iteration 3100: Loss = -9827.239800525862
Iteration 3200: Loss = -9827.236110898517
Iteration 3300: Loss = -9827.232576941658
Iteration 3400: Loss = -9827.222297598672
Iteration 3500: Loss = -9827.17177775748
Iteration 3600: Loss = -9827.117359124168
Iteration 3700: Loss = -9827.111480542751
Iteration 3800: Loss = -9827.107655628926
Iteration 3900: Loss = -9827.099605127856
Iteration 4000: Loss = -9827.08290841265
Iteration 4100: Loss = -9827.080380584588
Iteration 4200: Loss = -9827.074757968985
Iteration 4300: Loss = -9827.05967716398
Iteration 4400: Loss = -9827.058240647882
Iteration 4500: Loss = -9827.057224494938
Iteration 4600: Loss = -9827.056097031706
Iteration 4700: Loss = -9827.054308421182
Iteration 4800: Loss = -9827.052075071068
Iteration 4900: Loss = -9827.051147104226
Iteration 5000: Loss = -9827.050202032608
Iteration 5100: Loss = -9827.048951573333
Iteration 5200: Loss = -9827.043734326176
Iteration 5300: Loss = -9827.02138104321
Iteration 5400: Loss = -9827.01708442159
Iteration 5500: Loss = -9826.9817048984
Iteration 5600: Loss = -9826.971638356981
Iteration 5700: Loss = -9826.96621452963
Iteration 5800: Loss = -9826.925028810974
Iteration 5900: Loss = -9826.900774213333
Iteration 6000: Loss = -9826.887541913933
Iteration 6100: Loss = -9826.776978023076
Iteration 6200: Loss = -9826.775646173002
Iteration 6300: Loss = -9826.771542675191
Iteration 6400: Loss = -9826.655264027082
Iteration 6500: Loss = -9826.630752621633
Iteration 6600: Loss = -9826.60901686264
Iteration 6700: Loss = -9826.597326451834
Iteration 6800: Loss = -9826.58764763016
Iteration 6900: Loss = -9826.544083399658
Iteration 7000: Loss = -9826.542161172732
Iteration 7100: Loss = -9826.541815062834
Iteration 7200: Loss = -9826.541580525196
Iteration 7300: Loss = -9826.539199528299
Iteration 7400: Loss = -9826.535736269463
Iteration 7500: Loss = -9826.525975686747
Iteration 7600: Loss = -9826.507044270864
Iteration 7700: Loss = -9826.50677446453
Iteration 7800: Loss = -9826.506670149118
Iteration 7900: Loss = -9826.50637110571
Iteration 8000: Loss = -9826.506213093562
Iteration 8100: Loss = -9826.490733797158
Iteration 8200: Loss = -9826.492246124528
1
Iteration 8300: Loss = -9826.489858377507
Iteration 8400: Loss = -9826.454528797884
Iteration 8500: Loss = -9826.44456033224
Iteration 8600: Loss = -9826.4273439675
Iteration 8700: Loss = -9826.422052787366
Iteration 8800: Loss = -9826.42079424845
Iteration 8900: Loss = -9826.412197268097
Iteration 9000: Loss = -9826.412213490785
Iteration 9100: Loss = -9826.41208744284
Iteration 9200: Loss = -9826.416725611003
1
Iteration 9300: Loss = -9826.41127481105
Iteration 9400: Loss = -9826.408955374964
Iteration 9500: Loss = -9826.409204315702
1
Iteration 9600: Loss = -9826.408821412031
Iteration 9700: Loss = -9826.408753491842
Iteration 9800: Loss = -9826.406212189157
Iteration 9900: Loss = -9826.401572242441
Iteration 10000: Loss = -9826.426670528843
1
Iteration 10100: Loss = -9826.392259601687
Iteration 10200: Loss = -9826.392084835974
Iteration 10300: Loss = -9826.391977859885
Iteration 10400: Loss = -9826.376206220011
Iteration 10500: Loss = -9826.376149416801
Iteration 10600: Loss = -9826.376262054117
1
Iteration 10700: Loss = -9826.376101362139
Iteration 10800: Loss = -9826.386272980491
1
Iteration 10900: Loss = -9826.376024105732
Iteration 11000: Loss = -9826.375973685153
Iteration 11100: Loss = -9826.37800812507
1
Iteration 11200: Loss = -9826.365806629845
Iteration 11300: Loss = -9826.510743038933
1
Iteration 11400: Loss = -9826.343118362747
Iteration 11500: Loss = -9826.336371399573
Iteration 11600: Loss = -9826.335096210072
Iteration 11700: Loss = -9826.325816265255
Iteration 11800: Loss = -9826.325709116638
Iteration 11900: Loss = -9826.325374146973
Iteration 12000: Loss = -9826.325353884937
Iteration 12100: Loss = -9826.325408538192
Iteration 12200: Loss = -9826.32262442771
Iteration 12300: Loss = -9826.343242360115
1
Iteration 12400: Loss = -9826.31373295565
Iteration 12500: Loss = -9826.314927593672
1
Iteration 12600: Loss = -9826.309194868742
Iteration 12700: Loss = -9826.299170898257
Iteration 12800: Loss = -9826.294331252588
Iteration 12900: Loss = -9826.292986242175
Iteration 13000: Loss = -9826.42431677739
1
Iteration 13100: Loss = -9826.283799332727
Iteration 13200: Loss = -9826.325917565722
1
Iteration 13300: Loss = -9826.280197937978
Iteration 13400: Loss = -9826.28015426254
Iteration 13500: Loss = -9826.273663055077
Iteration 13600: Loss = -9826.273511445654
Iteration 13700: Loss = -9826.274960565292
1
Iteration 13800: Loss = -9826.273514458646
Iteration 13900: Loss = -9826.372508189894
1
Iteration 14000: Loss = -9826.273521174444
Iteration 14100: Loss = -9826.273510235676
Iteration 14200: Loss = -9826.273706704857
1
Iteration 14300: Loss = -9826.27342629684
Iteration 14400: Loss = -9826.287090425498
1
Iteration 14500: Loss = -9826.268011592392
Iteration 14600: Loss = -9826.268151453472
1
Iteration 14700: Loss = -9826.26801715608
Iteration 14800: Loss = -9826.268438866644
1
Iteration 14900: Loss = -9826.267926770834
Iteration 15000: Loss = -9826.383994124513
1
Iteration 15100: Loss = -9826.262562019898
Iteration 15200: Loss = -9826.26484767866
1
Iteration 15300: Loss = -9826.262540799462
Iteration 15400: Loss = -9826.265310965737
1
Iteration 15500: Loss = -9826.251077345567
Iteration 15600: Loss = -9826.253812929013
1
Iteration 15700: Loss = -9826.257851939046
2
Iteration 15800: Loss = -9826.251112669075
Iteration 15900: Loss = -9826.251360546132
1
Iteration 16000: Loss = -9826.281055770254
2
Iteration 16100: Loss = -9826.22874006471
Iteration 16200: Loss = -9826.228883335049
1
Iteration 16300: Loss = -9826.228746251141
Iteration 16400: Loss = -9826.222701254683
Iteration 16500: Loss = -9826.22258427201
Iteration 16600: Loss = -9826.223122826364
1
Iteration 16700: Loss = -9826.22275434123
2
Iteration 16800: Loss = -9826.222620271315
Iteration 16900: Loss = -9826.22259232034
Iteration 17000: Loss = -9826.223246066766
1
Iteration 17100: Loss = -9826.22116839218
Iteration 17200: Loss = -9826.229223464463
1
Iteration 17300: Loss = -9826.22120989232
Iteration 17400: Loss = -9826.221164845887
Iteration 17500: Loss = -9826.21700359718
Iteration 17600: Loss = -9826.21619597861
Iteration 17700: Loss = -9826.21461617948
Iteration 17800: Loss = -9826.214693749602
Iteration 17900: Loss = -9826.459275079796
1
Iteration 18000: Loss = -9826.211505730736
Iteration 18100: Loss = -9826.271816263348
1
Iteration 18200: Loss = -9826.211461968393
Iteration 18300: Loss = -9826.209556109497
Iteration 18400: Loss = -9826.209483614131
Iteration 18500: Loss = -9826.209418123804
Iteration 18600: Loss = -9826.209530333945
1
Iteration 18700: Loss = -9826.209414635212
Iteration 18800: Loss = -9826.21078985679
1
Iteration 18900: Loss = -9826.209429406468
Iteration 19000: Loss = -9826.221455509296
1
Iteration 19100: Loss = -9826.209439846818
Iteration 19200: Loss = -9826.235589373955
1
Iteration 19300: Loss = -9826.209439656066
Iteration 19400: Loss = -9826.209431380363
Iteration 19500: Loss = -9826.210594812863
1
Iteration 19600: Loss = -9826.209440352895
Iteration 19700: Loss = -9826.211925935073
1
Iteration 19800: Loss = -9826.209408015493
Iteration 19900: Loss = -9826.210203946148
1
pi: tensor([[1.0000e+00, 2.0183e-07],
        [8.8633e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1776, 0.8224], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1792, 0.1593],
         [0.5049, 0.1261]],

        [[0.6137, 0.1683],
         [0.5648, 0.6350]],

        [[0.6453, 0.1453],
         [0.6923, 0.7201]],

        [[0.5154, 0.1364],
         [0.6265, 0.5440]],

        [[0.5443, 0.1585],
         [0.5669, 0.7139]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.027469209953202733
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.02586023568974811
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0353071101438919
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.04031496747232086
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 34
Adjusted Rand Index: 0.07900104472414307
Global Adjusted Rand Index: 0.04609642778478347
Average Adjusted Rand Index: 0.04159051359666134
9954.026017034359
[0.03711595466040971, 0.04609642778478347] [0.03289582213472643, 0.04159051359666134] [9826.590857197345, 9826.209404125]
